<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 06 Sep 2025 20:30:45 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Europe enters the exascale supercomputing league with Jupiter]]></title>
            <link>https://ec.europa.eu/commission/presscorner/detail/en/ip_25_2029</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152369</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Greppers – fast CLI cheat sheet with instant copy and shareable search]]></title>
            <link>https://www.greppers.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152086</guid>
            <description><![CDATA[Clean, fast CLI commands with real‑world examples and instant copy.]]></description>
            <content:encoded><![CDATA[
    
      Stop Googling the same command twice.    A tiny, blazing‑fast directory of CLI commands with copy‑ready examples. Offline friendly. No BS.      
      Try:   
    

    

    
        Help grow the community!
        Know a useful command or recipe that's missing? Help other developers by contributing.
        Suggest a Command
      

    
      
    

    
      Built for speed and memory.
      
        Instant search: Runs entirely in your browser.
        Copy‑to‑clipboard: One click, no ceremony.
        Opinionated examples: Real‑world flags and patterns.
        Keyboard first: / focuses search, ↑↓ navigate, ⏎ copies.
      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI hype is crashing into reality. Stay calm]]></title>
            <link>https://www.businessinsider.com/ai-hype-crashing-into-reality-iphone-openai-2025-9</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152001</guid>
            <description><![CDATA[Between Nvidia's recent earnings and OpenAI's underwhelming GPT-5, it's clear AI is entering its "meh" era. That's good news for everyone.]]></description>
            <content:encoded><![CDATA[
  
    
  
    
              
              
                  
                    
                    
                  
              
                
                  
                    Getty Images; Tyler Le/BI
                    
              
          

  
    
    
    
    
          
          
          
          
              
              
                  
                    
                    
                  
              
                
                  
                    Getty Images; Tyler Le/BI
                    
              
          
    
    
    
          
            
            
            
            
            
            
            
              
                  2025-09-04T08:17:01Z
                
              
            
            
                
      
          
      
          
          
          
          
          
          
          
          
          
              A market correction. A wake-up call. A great digestion. Call it what you want: AI is going through it.Two things appear to be happening in tandem. Businesses are starting to finally grasp what AI can — and importantly, can't — do to boost their bottom lines. And the sky-high expectations that have been partly inflated and overhyped by AI firms over the past few years are finally coming down to Earth.In short, it's increasingly looking like both the AI doomers and boomers were both wrong. AI's trajectory is starting to look less like a time machine or space elevator and more akin computers, smartphones, televisions: The technology will get better, it will almost certainly change our lives in the fullness of time, but it will more likely do so incrementally — to the point that if AGI (artificial general intelligence) or superintelligence do in fact one day arrive, it might not seem like much of a leap at all.There's perhaps no better example of this happening than OpenAI's latest and long-anticipated model, GPT-5, which was touted with a bang and landed with a shrug. Ahead of launch, OpenAI's Sam Altman said he'd felt "useless" compared to the model's intelligence, even drawing parallels with the Manhattan Project. When it arrived, users apparently felt less intimidated. "The degree of overhyping was too significant," one person wrote. "In the absence of massive gains, all you have is hype," wrote another.But it may be a glimpse at our new reality, where the breakneck speed of AI progress is simply steadying, where progress cannot run on hype alone, and where we will neither experience an overnight white-collar job wipeout nor reach an AI abundance society overnight.Welcome to AI's "meh" era. Stay calm. We've been here before. It'll all be fine. Probably.When the internet revolution took hold in the late 1990s, companies were minting millions overnight with little more than a website and a savvy sales pitch. By the year 2000, the economic reality caught up to the hype, leaving trillions of dollars wiped out overnight. Not familiar? Go ask your parents what happened to Pets.com.
          It's easy to see why talk of a bubble has once again reared its head. Even Altman recently (and in an unusually measured moment from AI's biggest hype man) said he believes the AI market might be in a bubble.Progress has been such that you probably won't even notice the improvements from now on.Carl Benedikt Frey"If you go back to the 1990s when the dotcom bubble burst, there weren't the profits necessarily to back the investments up, but there were tangible productivity gains," says Carl Benedikt Frey, an economist at Oxford. If that sounds eerily familiar, a check on AI now could prevent history from repeating itself.A recent study published by the Massachusetts Institute of Technology further stirred the pot last month, claiming that just 5% of companies it studied have managed to convert the technology into actual revenue — a revelation scary enough to cause a tech stock sell-off, even if the study had a lot of limitations.
            
            
            
          Other evidence suggests AI is starting to have an impact on businesses that are adopting it. A study from Stanford University researchers that analyzed payroll data concluded that AI was killing off entry-level jobs for people aged 22 to 25, and was especially doing so in fields where AI was more likely to replace, rather than augment, labor. Marc Benioff claims AI agents are replacing thousands of Salesforce's support roles, while other companies are boasting about AI automating more of their work.A study of AI's effects on the demand for foreign translators by Frey and fellow Oxford economist Pedro Llanos-Paredes, published earlier this year, concluded that the technology was having a small but provable impact on these jobs."We seem to be seeing reasonable revenue growth for a handful of firms that are pioneering the AI revolution, but we're not seeing it translate into broader economic growth," Frey tells me. "What I find concerning is that we're still not seeing any hint of it in the productivity statistics, and ultimately that's what matters. It doesn't really matter how well AI performs on tests or on some benchmark. What matters is translating that into real economic growth."For the markets, a more modest uptake may be perfectly OK. Evercore ISI strategists predict AI excitement will buoy US stocks a further 20% by the end of 2026. "AI is 'bigger' than the internet," they wrote in a note published this week. "In three years, its effect has touched all parts of society and industry even as adoption only begins to inflect."Last week's Nvidia earnings were a strong indicator of where we're at. The company, which sells the valuable chips on which AI is trained and run, has become something of a bellwether for the entire artificial intelligence boom and counts some of the biggest tech giants as key customers. (Bloomberg estimates that Microsoft spends about 47% of its capital expenditures on Nvidia's chips.) While it beat Wall Street expectations and its own sales records, its stock still dropped, suggesting investors weren't impressed with the figures they saw. Some analysts warn that the companies buying these services from Nvidia aren't yet seeing the returns. One UBS analyst characterized Nvidia's results in a way that may perfectly sum up the new steady-chug-along paradigm of AI: "good enough."All to say, AI seems to have reached its iPhone 4 moment.When Apple's iPhone 4 arrived in 2010, it was nothing short of a smash hit. Onstage in Cupertino, Steve Jobs boasted that Apple had made the thinnest phone in the world with a laundry list of new must-have features: a squared-off design, high-resolution display, a front-facing camera for FaceTime and selfies, and the debut of Apple's custom silicon chip, the A4. It flew off the shelves — despite the antenna fiasco — and further cemented Apple as the king of the smartphone. One could argue that the market has been chasing the iPhone 4's "sandwich glass" design ever since.A lot of this sort of feeling of disappointment is due to unreasonable levels of hype.David KruegerThen things changed: With the exception of a couple of moderate leaps since, the iPhone has been on a more incremental trajectory. Evidence suggests artificial intelligence might be plotting a similar course. Frontier labs have been rolling out a steady flow of updates and mini-leaps, rather than waiting years between generations, and as a result, each new rollout has started feeling evolutionary, incremental."If you're not the leading expert in the field, I think the progress has been such that you probably won't even notice the improvements from now on," says Frey.Last year, the big topic was whether AI labs were seeing diminishing returns when simply trying to throw more data and compute power at the models. That may go some way to explain how GPT-5 landed, but it's not the only factor.Between the launch of GPT-4 in March 2023 and GPT-5 last month, OpenAI rolled out well over a dozen models, each one focusing on specific jobs or incrementally improving another. It's perhaps no wonder, then, that GPT-5 didn't blow our socks off. (In the same conversation in which Altman claimed AI is in a bubble, he also claimed that OpenAI has more advanced models than GPT-5, but can't deploy them because it doesn't have the capacity.)Google's latest frontier model, Gemini 2.5, is also a bridge model, and the launch of GPT-5 may serve as a healthy warning to temper expectations for Gemini 3, which is expected before the year's end.
            
            
            
          "The progress just feels more continuous," says David Krueger, an assistant professor at the University of Montreal who studies AI safety and risks.Krueger still thinks we'll have the occasional "wow" moment, but said he also believes we'll need more breakthroughs on the technical side to reach any level of artificial intelligence that can go toe-to-toe with a human — and he says he doesn't believe we will reach AGI from large language models alone."I think LLMs and more broadly deep learning are probably a big piece of the puzzle. If I had to bet, the biggest one," he says. "But I think we're maybe missing a couple of puzzle pieces."Krueger also lays blame at the feet of certain AI figureheads who have created "unreasonable levels of hype," and who are finally getting a reality check. Altman may be the worst offender, but he's not the only one.In March, Anthropic CEO Dario Amodei predicted that AI would be writing 90% of software developers' code in three to six months. The actual gains appear to be much more modest: During Alphabet's Q1 2025 earnings call, CEO Sundar Pichai said that more than 30% of code written at Google was being generated by AI."I think a lot of this sort of feeling of disappointment is due to unreasonable levels of hype from the companies," said Krueger. As the rubber hits the road and the breakneck speed of AI potentially slows, expectations for the future of AI — and the possible, maybe, one day, arrival of AGI — are finally being put in check.You can see how we got here. In a January interview with Bloomberg, Altman predicted AGI would arrive during Trump's second presidency. Elon Musk once predicted it could be here by the year's end. According to some of the best minds in AI, AGI is often just a "few years away." In truth it feels like we're finally realizing nobody actually knows.Perhaps nobody has had more of a humbling in AI than Apple, which earlier this year axed an iPhone 16 ad that promised several much-hyped new AI features that, as it turned out, were far from ready. When Tim Cook steps onstage next week, don't be surprised if he and other executives strike a more measured tone when talking about AI, as they pull back the curtain on the latest lineup of devices.I hear the new phone will be a little thinner this time.Hugh Langley is a senior correspondent at Business Insider where he writes about Google, tech, and wealth.
          
          
              
                
                  Business Insider's Discourse stories provide perspectives on the day's most pressing issues, informed by analysis, reporting, and expertise.
                  
              
          
          
          
          
          
          
    
    
    
    

    
      
      
    
    
          
          
          
          
            
                
                
                  Discourse
                      
                
                  AI
                      
                
                  Generative AI
                
                
                  More 
            
          
                
          
                  
                  
                    ChatGPT
                          
                  
                    OpenAI
                          
                  
                    Anthropic
                    
          
    
            
            
            
                  Most popular
              
                  
                  
                    
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                          
  

  
  
  

  
  

    

  
  
    
  
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Normalization of deviance (2015)]]></title>
            <link>https://danluu.com/wat/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151661</guid>
            <description><![CDATA[Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples.]]></description>
            <content:encoded><![CDATA[ Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples. There's the company that is perhaps the nicest place I've ever worked, combining the best parts of Valve and Netflix. The people are amazing and you're given near total freedom to do whatever you want. But as a side effect of the culture, they lose perhaps half of new hires in the first year, some voluntarily and some involuntarily. Totally normal, right? Here are a few more anecdotes that were considered totally normal by people in places I've worked. And often not just normal, but laudable. There's the company that's incredibly secretive about infrastructure. For example, there's the team that was afraid that, if they reported bugs to their hardware vendor, the bugs would get fixed and their competitors would be able to use the fixes. Solution: request the firmware and fix bugs themselves! More recently, I know a group of folks outside the company who tried to reproduce the algorithm in the paper the company published earlier this year. The group found that they couldn't reproduce the result, and that the algorithm in the paper resulted in an unusual level of instability; when asked about this, one of the authors responded “well, we have some tweaks that didn't make it into the paper” and declined to share the tweaks, i.e., the company purposely published an unreproducible result to avoid giving away the details, as is normal. This company enforces secrecy by having a strict policy of firing leakers. This is introduced at orientation with examples of people who got fired for leaking (e.g., the guy who leaked that a concert was going to happen inside a particular office), and by announcing firings for leaks at the company all hands. The result of those policies is that I know multiple people who are afraid to forward emails about things like updated info on health insurance to a spouse for fear of forwarding the wrong email and getting fired; instead, they use another computer to retype the email and pass it along, or take photos of the email on their phone. There's the office where I asked one day about the fact that I almost never saw two particular people in the same room together. I was told that they had a feud going back a decade, and that things had actually improved — for years, they literally couldn't be in the same room because one of the two would get too angry and do something regrettable, but things had now cooled to the point where the two could, occasionally, be found in the same wing of the office or even the same room. These weren't just random people, either. They were the two managers of the only two teams in the office. There's the company whose culture is so odd that, when I sat down to write a post about it, I found that I'd not only written more than for any other single post, but more than all other posts combined (which is well over 100k words now, the length of a moderate book). This is the same company where someone recently explained to me how great it is that, instead of using data to make decisions, we use political connections, and that the idea of making decisions based on data is a myth anyway; no one does that. This is also the company where all four of the things they told me to get me to join were false, and the job ended up being the one thing I specifically said I didn't want to do. When I joined this company, my team didn't use version control for months and it was a real fight to get everyone to use version control. Although I won that fight, I lost the fight to get people to run a build, let alone run tests, before checking in, so the build is broken multiple times per day. When I mentioned that I thought this was a problem for our productivity, I was told that it's fine because it affects everyone equally. Since the only thing that mattered was my stack ranked productivity, so I shouldn't care that it impacts the entire team, the fact that it's normal for everyone means that there's no cause for concern. There's the company that created multiple massive initiatives to recruit more women into engineering roles, where women still get rejected in recruiter screens for not being technical enough after being asked questions like "was your experience with algorithms or just coding?". I thought that my referral with a very strong recommendation would have prevented that, but it did not. There's the company where I worked on a four person effort with a multi-hundred million dollar budget and a billion dollar a year impact, where requests for things that cost hundreds of dollars routinely took months or were denied. You might wonder if I've just worked at places that are unusually screwed up. Sure, the companies are generally considered to be ok places to work and two of them are considered to be among the best places to work, but maybe I've just ended up at places that are overrated. But I have the same experience when I hear stories about how other companies work, even places with stellar engineering reputations, except that it's me that's shocked and my conversation partner who thinks their story is normal. There's the companies that use @flaky, which includes the vast majority of Python-using SF Bay area unicorns. If you don't know what this is, this is a library that lets you add a Python annotation to those annoying flaky tests that sometimes pass and sometimes fail. When I asked multiple co-workers and former co-workers from three different companies what they thought this did, they all guessed that it re-runs the test multiple times and reports a failure if any of the runs fail. Close, but not quite. It's technically possible to use @flaky for that, but in practice it's used to re-run the test multiple times and reports a pass if any of the runs pass. The company that created @flaky is effectively a storage infrastructure company, and the library is widely used at its biggest competitor. There's the company with a reputation for having great engineering practices that had 2 9s of reliability last time I checked, for reasons that are entirely predictable from their engineering practices. This is the second thing in a row that can't be deanonymized because multiple companies fit the description. Here, I'm not talking about companies trying to be the next reddit or twitter where it's, apparently, totally fine to have 1 9. I'm talking about companies that sell platforms that other companies rely on, where an outage will cause dependent companies to pause operations for the duration of the outage. Multiple companies that build infrastructure find practices that lead to 2 9s of reliability. As far as I can tell, what happens at a lot these companies is that they started by concentrating almost totally on product growth. That's completely and totally reasonable, because companies are worth approximately zero when they're founded; they don't bother with things that protect them from losses, like good ops practices or actually having security, because there's nothing to lose (well, except for user data when the inevitable security breach happens, and if you talk to security folks at unicorns you'll know that these happen). The result is a culture where people are hyper-focused on growth and ignore risk. That culture tends to stick even after company has grown to be worth well over a billion dollars, and the companies have something to lose. Anyone who comes into one of these companies from Google, Amazon, or another place with solid ops practices is shocked. Often, they try to fix things, and then leave when they can't make a dent. Google probably has the best ops and security practices of any tech company today. It's easy to say that you should take these things as seriously as Google does, but it's instructive to see how they got there. If you look at the codebase, you'll see that various services have names ending in z, as do a curiously large number of variables. I'm told that's because, once upon a time, someone wanted to add monitoring. It wouldn't really be secure to have google.com/somename expose monitoring data, so they added a z. google.com/somenamez. For security. At the company that is now the best in the world at security. They're now so good at security that multiple people I've talked to (all of whom joined after this happened) vehemently deny that this ever happened, even though the reasons they give don't really make sense (e.g., to avoid name collisions) and I have this from sources who were there at the time this happened. Google didn't go from adding z to the end of names to having the world's best security because someone gave a rousing speech or wrote a convincing essay. They did it after getting embarrassed a few times, which gave people who wanted to do things “right” the leverage to fix fundamental process issues. It's the same story at almost every company I know of that has good practices. Microsoft was a joke in the security world for years, until multiple disastrously bad exploits forced them to get serious about security. This makes it sound simple, but if you talk to people who were there at the time, the change was brutal. Despite a mandate from the top, there was vicious political pushback from people whose position was that the company got to where it was in 2003 without wasting time on practices like security. Why change what's worked? You can see this kind of thing in every industry. A classic example that tech folks often bring up is hand-washing by doctors and nurses. It's well known that germs exist, and that washing hands properly very strongly reduces the odds of transmitting germs and thereby significantly reduces hospital mortality rates. Despite that, trained doctors and nurses still often don't do it. Interventions are required. Signs reminding people to wash their hands save lives. But when people stand at hand-washing stations to require others walking by to wash their hands, even more lives are saved. People can ignore signs, but they can't ignore being forced to wash their hands. This mirrors a number of attempts at tech companies to introduce better practices. If you tell people they should do it, that helps a bit. If you enforce better practices via code review, that helps a lot. The data are clear that humans are really bad at taking the time to do things that are well understood to incontrovertibly reduce the risk of rare but catastrophic events. We will rationalize that taking shortcuts is the right, reasonable thing to do. There's a term for this: the normalization of deviance. It's well studied in a number of other contexts including healthcare, aviation, mechanical engineering, aerospace engineering, and civil engineering, but we don't see it discussed in the context of software. In fact, I've never seen the term used in the context of software. Is it possible to learn from other's mistakes instead of making every mistake ourselves? The state of the industry make this sound unlikely, but let's give it a shot. John Banja has a nice summary paper on the normalization of deviance in healthcare, with lessons we can attempt to apply to software development. One thing to note is that, because Banja is concerned with patient outcomes, there's a close analogy to devops failure modes, but normalization of deviance also occurs in cultural contexts that are less directly analogous. The first section of the paper details a number of disasters, both in healthcare and elsewhere. Here's one typical example:  A catastrophic negligence case that the author participated in as an expert witness involved an anesthesiologist's turning off a ventilator at the request of a surgeon who wanted to take an x-ray of the patient's abdomen (Banja, 2005, pp. 87-101). The ventilator was to be off for only a few seconds, but the anesthesiologist forgot to turn it back on, or thought he turned it back on but had not. The patient was without oxygen for a long enough time to cause her to experience global anoxia, which plunged her into a vegetative state. She never recovered, was disconnected from artificial ventilation 9 days later, and then died 2 days after that. It was later discovered that the anesthesia alarms and monitoring equipment in the operating room had been deliberately programmed to a “suspend indefinite” mode such that the anesthesiologist was not alerted to the ventilator problem. Tragically, the very instrumentality that was in place to prevent such a horror was disabled, possibly because the operating room staff found the constant beeping irritating and annoying.  Turning off or ignoring notifications because there are too many of them and they're too annoying? An erroneous manual operation? This could be straight out of the post-mortem of more than a few companies I can think of, except that the result was a tragic death instead of the loss of millions of dollars. If you read a lot of tech post-mortems, every example in Banja's paper will feel familiar even though the details are different. The section concludes,  What these disasters typically reveal is that the factors accounting for them usually had “long incubation periods, typified by rule violations, discrepant events that accumulated unnoticed, and cultural beliefs about hazards that together prevented interventions that might have staved off harmful outcomes”. Furthermore, it is especially striking how multiple rule violations and lapses can coalesce so as to enable a disaster's occurrence.  Once again, this could be from an article about technical failures. That makes the next section, on why these failures happen, seem worth checking out. The reasons given are: The rules are stupid and inefficient The example in the paper is about delivering medication to newborns. To prevent “drug diversion,” nurses were required to enter their password onto the computer to access the medication drawer, get the medication, and administer the correct amount. In order to ensure that the first nurse wasn't stealing drugs, if any drug remained, another nurse was supposed to observe the process, and then enter their password onto the computer to indicate they witnessed the drug being properly disposed of. That sounds familiar. How many technical postmortems start off with “someone skipped some steps because they're inefficient”, e.g., “the programmer force pushed a bad config or bad code because they were sure nothing could go wrong and skipped staging/testing”? The infamous November 2014 Azure outage happened for just that reason. At around the same time, a dev at one of Azure's competitors overrode the rule that you shouldn't push a config that fails tests because they knew that the config couldn't possibly be bad. When that caused the canary deploy to start failing, they overrode the rule that you can't deploy from canary into staging with a failure because they knew their config couldn't possibly be bad and so the failure must be from something else. That postmortem revealed that the config was technically correct, but exposed a bug in the underlying software; it was pure luck that the latent bug the config revealed wasn't as severe as the Azure bug. Humans are bad at reasoning about how failures cascade, so we implement bright line rules about when it's safe to deploy. But the same thing that makes it hard for us to reason about when it's safe to deploy makes the rules seem stupid and inefficient. Knowledge is imperfect and uneven People don't automatically know what should be normal, and when new people are onboarded, they can just as easily learn deviant processes that have become normalized as reasonable processes. Julia Evans described to me how this happens: new person joins new person: WTF WTF WTF WTF WTF old hands: yeah we know we're concerned about it new person: WTF WTF wTF wtf wtf w... new person gets used to it new person #2 joins new person #2: WTF WTF WTF WTF new person: yeah we know. we're concerned about it. The thing that's really insidious here is that people will really buy into the WTF idea, and they can spread it elsewhere for the duration of their career. Once, after doing some work on an open source project that's regularly broken and being told that it's normal to have a broken build, and that they were doing better than average, I ran the numbers, found that project was basically worst in class, and wrote something about the idea that it's possible to have a build that nearly always passes with relatively low effort. The most common comment I got in response was, "Wow that guy must work with superstar programmers. But let's get real. We all break the build at least a few times a week", as if running tests (or for that matter, even attempting to compile) before checking code in requires superhuman abilities. But once people get convinced that some deviation is normal, they often get really invested in the idea. I'm breaking the rule for the good of my patient The example in the paper is of someone who breaks the rule that you should wear gloves when finding a vein. Their reasoning is that wearing gloves makes it harder to find a vein, which may result in their having to stick a baby with a needle multiple times. It's hard to argue against that. No one wants to cause a baby extra pain! The second worst outage I can think of occurred when someone noticed that a database service was experiencing slowness. They pushed a fix to the service, and in order to prevent the service degradation from spreading, they ignored the rule that you should do a proper, slow, staged deploy. Instead, they pushed the fix to all machines. It's hard to argue against that. No one wants their customers to have degraded service! Unfortunately, the fix exposed a bug that caused a global outage. The rules don't apply to me/You can trust me  most human beings perceive themselves as good and decent people, such that they can understand many of their rule violations as entirely rational and ethically acceptable responses to problematic situations. They understand themselves to be doing nothing wrong, and will be outraged and often fiercely defend themselves when confronted with evidence to the contrary.  As companies grow up, they eventually have to impose security that prevents every employee from being able to access basically everything. And at most companies, when that happens, some people get really upset. “Don't you trust me? If you trust me, how come you're revoking my access to X, Y, and Z?” Facebook famously let all employees access everyone's profile for a long time, and you can even find HN comments indicating that some recruiters would explicitly mention that as a perk of working for Facebook. And I can think of more than one well-regarded unicorn where everyone still has access to basically everything, even after their first or second bad security breach. It's hard to get the political capital to restrict people's access to what they believe they need, or are entitled, to know. A lot of trendy startups have core values like “trust” and “transparency” which make it difficult to argue against universal access. Workers are afraid to speak up There are people I simply don't give feedback to because I can't tell if they'd take it well or not, and once you say something, it's impossible to un-say it. In the paper, the author gives an example of a doctor with poor handwriting who gets mean when people ask him to clarify what he's written. As a result, people guess instead of asking. In most company cultures, people feel weird about giving feedback. Everyone has stories about a project that lingered on for months or years after it should have been terminated because no one was willing to offer explicit feedback. This is a problem even when cultures discourage meanness and encourage feedback: cultures of niceness seem to have as many issues around speaking up as cultures of meanness, if not more. In some places, people are afraid to speak up because they'll get attacked by someone mean. In others, they're afraid because they'll be branded as mean. It's a hard problem. Leadership withholding or diluting findings on problems In the paper, this is characterized by flaws and weaknesses being diluted as information flows up the chain of command. One example is how a supervisor might take sub-optimal actions to avoid looking bad to superiors. I was shocked the first time I saw this happen. I must have been half a year or a year out of school. I saw that we were doing something obviously non-optimal, and brought it up with the senior person in the group. He told me that he didn't disagree, but that if we did it my way and there was a failure, it would be really embarrassing. He acknowledged that my way reduced the chance of failure without making the technical consequences of failure worse, but it was more important that we not be embarrassed. Now that I've been working for a decade, I have a better understanding of how and why people play this game, but I still find it absurd. Solutions Let's say you notice that your company has a problem that I've heard people at most companies complain about: people get promoted for heroism and putting out fires, not for preventing fires; and people get promoted for shipping features, not for doing critical maintenance work and bug fixing. How do you change that? The simplest option is to just do the right thing yourself and ignore what's going on around you. That has some positive impact, but the scope of your impact is necessarily limited. Next, you can convince your team to do the right thing: I've done that a few times for practices I feel are really important and are sticky, so that I won't have to continue to expend effort on convincing people once things get moving. But if the incentives are aligned against you, it will require an ongoing and probably unsustainable effort to keep people doing the right thing. In that case, the problem becomes convincing someone to change the incentives, and then making sure the change works as designed. How to convince people is worth discussing, but long and messy enough that it's beyond the scope of this post. As for making the change work, I've seen many “obvious” mistakes repeated, both in places I've worked and those whose internal politics I know a lot about. Small companies have it easy. When I worked at a 100 person company, the hierarchy was individual contributor (IC) -> team lead (TL) -> CEO. That was it. The CEO had a very light touch, but if he wanted something to happen, it happened. Critically, he had a good idea of what everyone was up to and could basically adjust rewards in real-time. If you did something great for the company, there's a good chance you'd get a raise. Not in nine months when the next performance review cycle came up, but basically immediately. Not all small companies do that effectively, but with the right leadership, they can. That's impossible for large companies. At large company A (LCA), they had the problem we're discussing and a mandate came down to reward people better for doing critical but low-visibility grunt work. There were too many employees for the mandator to directly make all decisions about compensation and promotion, but the mandator could review survey data, spot check decisions, and provide feedback until things were normalized. My subjective perception is that the company never managed to achieve parity between boring maintenance work and shiny new projects, but got close enough that people who wanted to make sure things worked correctly didn't have to significantly damage their careers to do it. At large company B (LCB), ICs agreed that it's problematic to reward creating new features more richly than doing critical grunt work. When I talked to managers, they often agreed, too. But nevertheless, the people who get promoted are disproportionately those who ship shiny new things. I saw management attempt a number of cultural and process changes at LCB. Mostly, those took the form of pronouncements from people with fancy titles. For really important things, they might produce a video, and enforce compliance by making people take a multiple choice quiz after watching the video. The net effect I observed among other ICs was that people talked about how disconnected management was from the day-to-day life of ICs. But, for the same reasons that normalization of deviance occurs, that information seems to have no way to reach upper management. It's sort of funny that this ends up being a problem about incentives. As an industry, we spend a lot of time thinking about how to incentivize consumers into doing what we want. But then we set up incentive systems that are generally agreed upon as incentivizing us to do the wrong things, and we do so via a combination of a game of telephone and cargo cult diffusion. Back when Microsoft was ascendant, we copied their interview process and asked brain-teaser interview questions. Now that Google is ascendant, we copy their interview process and ask algorithms questions. If you look around at trendy companies that are younger than Google, most of them basically copy their ranking/leveling system, with some minor tweaks. The good news is that, unlike many companies people previously copied, Google has put a lot of thought into most of their processes and made data driven decisions. The bad news is that Google is unique in a number of ways, which means that their reasoning often doesn't generalize, and that people often cargo cult practices long after they've become deprecated at Google. This kind of diffusion happens for technical decisions, too. Stripe built a reliable message queue on top of Mongo, so we build reliable message queues on top of Mongo1. It's cargo cults all the way down2. The paper has specific sub-sections on how to prevent normalization of deviance, which I recommend reading in full.  Pay attention to weak signals Resist the urge to be unreasonably optimistic Teach employees how to conduct emotionally uncomfortable conversations System operators need to feel safe in speaking up Realize that oversight and monitoring are never-ending  Let's look at how the first one of these, “pay attention to weak signals”, interacts with a single example, the “WTF WTF WTF” a new person gives off when the join the company. If a VP decides something is screwed up, people usually listen. It's a strong signal. And when people don't listen, the VP knows what levers to pull to make things happen. But when someone new comes in, they don't know what levers they can pull to make things happen or who they should talk to almost by definition. They give out weak signals that are easily ignored. By the time they learn enough about the system to give out strong signals, they've acclimated. “Pay attention to weak signals” sure sounds like good advice, but how do we do it? Strong signals are few and far between, making them easy to pay attention to. Weak signals are abundant. How do we filter out the ones that aren't important? And how do we get an entire team or org to actually do it? These kinds of questions can't be answered in a generic way; this takes real thought. We mostly put this thought elsewhere. Startups spend a lot of time thinking about growth, and while they'll all tell you that they care a lot about engineering culture, revealed preference shows that they don't. With a few exceptions, big companies aren't much different. At LCB, I looked through the competitive analysis slide decks and they're amazing. They look at every last detail on hundreds of products to make sure that everything is as nice for users as possible, from onboarding to interop with competing products. If there's any single screen where things are more complex or confusing than any competitor's, people get upset and try to fix it. It's quite impressive. And then when LCB onboards employees in my org, a third of them are missing at least one of, an alias/account, an office, or a computer, a condition which can persist for weeks or months. The competitive analysis slide decks talk about how important onboarding is because you only get one chance to make a first impression, and then employees are onboarded with the impression that the company couldn't care less about them and that it's normal for quotidian processes to be pervasively broken. LCB can't even to get the basics of employee onboarding right, let alone really complex things like acculturation. This is understandable — external metrics like user growth or attrition are measurable, and targets like how to tell if you're acculturating people so that they don't ignore weak signals are softer and harder to determine, but that doesn't mean they're any less important. People write a lot about how things like using fancier languages or techniques like TDD or agile will make your teams more productive, but having a strong engineering culture is much larger force multiplier.  Thanks to Sophie Smithburg and Marc Brooker for introducing me to the term Normalization of Deviance, and Kelly Eskridge, Leah Hanson, Sophie Rapoport, Sophie Smithburg, Julia Evans, Dmitri Kalintsev, Ralph Corderoy, Jamie Brandon, Egor Neliuba, and Victor Felder for comments/corrections/discussion.      People seem to think I'm joking here. I can understand why, but try Googling mongodb message queue. You'll find statements like “replica sets in MongoDB work extremely well to allow automatic failover and redundancy”. Basically every company I know of that's done this and has anything resembling scale finds this to be non-optimal, to say the least, but you can't actually find blog posts or talks that discuss that. All you see are the posts and talks from when they first tried it and are in the honeymoon period. This is common with many technologies. You'll mostly find glowing recommendations in public even when, in private, people will tell you about all the problems. Today, if you do the search mentioned above, you'll get a ton of posts talking about how amazing it is to build a message queue on top of Mongo, this footnote, and a maybe couple of blog posts by Kyle Kingsbury depending on your exact search terms. If there were an acute failure, you might see a postmortem, but while we'll do postmortems for "the site was down for 30 seconds", we rarely do postmortems for "this takes 10x as much ops effort as the alternative and it's a death by a thousand papercuts", "we architected this thing poorly and now it's very difficult to make changes that ought to be trivial", or "a competitor of ours was able to accomplish the same thing with an order of magnitude less effort". I'll sometimes do informal postmortems by asking everyone involved oblique questions about what happened, but more for my own benefit than anything else, because I'm not sure people really want to hear the whole truth. This is especially sensitive if the effort has generated a round of promotions, which seems to be more common the more screwed up the project. The larger the project, the more visibility and promotions, even if the project could have been done with much less effort. [return] I've spent a lot of time asking about why things are the way they are, both in areas where things are working well, and in areas where things are going badly. Where things are going badly, everyone has ideas. But where things are going well, as in the small company with the light-touch CEO mentioned above, almost no one has any idea why things work. It's magic. If you ask, people will literally tell you that it seems really similar to some other place they've worked, except that things are magically good instead of being terrible for reasons they don't understand. But it's not magic. It's hard work that very few people understand. Something I've seen multiple times is that, when a VP leaves, a company will become a substantially worse place to work, and it will slowly dawn on people that the VP was doing an amazing job at supporting not only their direct reports, but making sure that everyone under them was having a good time. It's hard to see until it changes, but if you don't see anything obviously wrong, either you're not paying attention or someone or many someones have put a lot of work into making sure things run smoothly. [return]   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GigaByte CXL memory expansion card with up to 512GB DRAM]]></title>
            <link>https://www.gigabyte.com/PC-Accessory/AI-TOP-CXL-R5X4</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151598</guid>
            <description><![CDATA[Expand Memory Pool 16-Layers HDI PCB Equipped with an AIO Fan DDR5 RDIMM Support Support PCle 5.0 Lower Total Cost of Ownership (TCO)]]></description>
            <content:encoded><![CDATA[


            
            
    
    
    




    








    
			
                
                            
                                
                                     FEATURES 
                                    
                                         Expand Memory Pool 
                                         16-Layers HDI PCB 
                                         Equipped with an AIO Fan 
                                         DDR5 RDIMM Support 
                                         Support PCle 5.0 
                                         Lower Total Cost of Ownership (TCO) 
                                    
                                
                                
                                    
                                    
                                
                            
                            
                            
                        
                
                                    
                                        
                                            
                                            
                                        
                                        
                                                 GIGABYTE AI TOP 
                                                 Train your own AI on your desk. 
                                            
                                    
                                    
                                        
                                            
                                            
                                        
                                        
                                                 Ultimate scalability and Connectivity​ 
                                                 Brings you the future-proofing capability. 
                                            
                                    
                                
                
                    
                    
                        
                            
                                     GIGABYTE AI TOP 
                                     Train your own AI on your desk 
                                     In the age of local AI, GIGABYTE AI TOP is the all-round solution to win advantages ahead of traditional AI training methods. It features a variety of groundbreaking technologies that can be easily adapted by beginners or experts, for most common open-source LLMs, in anyplace even on your desk. 
                                
                            
                                
                                    Supports 236B LLM Local Training
                                
                                
                                    Intuitive Set-up
                                
                                
                                    Flexbility & Upgradability
                                
                                
                                    Privacy & Security
                                
                                
                                    Suitable for Home Use
                                
                            
                        
                        
                            
                                    
                                        
                                    
                                    
                                         AI TOP Utility 2.0 
                                         Reinventing AI Training 
                                    
                                     The all-new AI TOP Utility 2.0 brings enhanced features and broader model support, empowering users from novices to experts to unlock AI's potential right at their desks. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                
                            
                                    
                                         AI TOP Hardware 
                                         Enhanced Performance for AI Training 
                                    
                                     The AI TOP Hardware features a series of GIGABYTE AI TOP products that are optimized in power efficiency and durability for AI training workloads. It includes upgradeable components and is easy to build at home. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                     * Images for reference only, system configuration will vary by model. 
                                
                        
                    
                
                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultimate scalability 
                                                                 Exclusive Memory Offloading Solution 
                                                                 Expand training capacity by offloading data to system memory and SSDs, enabling efficient training of Big models through optimized memory management. 
                                                            
                                                            
                                                            
                                                                                         Offers diverse memory expandability with support for DDR5 ECC Registered memory modules (RDIMM): 
                                                                                        
                                                                                             - 4 x DDR5 DIMM sockets, supporting up to 512 GB of memory (up to 128 GB per DIMM) 
                                                                                        
                                                                                         Provides users with High flexibility and options suitable for various requirement scenarios. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Future Connectivity 
                                                                 Express to PCIE5 Performance 
                                                                 The new AI TOP CXL PCI Express card provides the most efficient solution to enjoy boosted performance of PCIe Gen5 on the existing platform. PCIe 5.0 doubles the data transfer rate of PCIe 4.0 from 16 GT/s to 32 GT/s per lane. This substantial increase in bandwidth particularly benefits high-performance AIO cards, storage devices, and AI accelerators that demand faster data movement. 
                                                            
                                                            
                                                                                         Benefits of CXL Memory: 
                                                                                        
                                                                                            
                                                                                                 *Overall lower total cost of ownership (TCO)* 
                                                                                                 -Improve computational and memory resource utilization for specific applications, thereby reducing capital expenditure and operating costs. 
                                                                                            
                                                                                            
                                                                                                 *Breaking Memory Bottlenecks:* 
                                                                                                 - Traditional server memory capacity limits the development of complex applications like AI. CXL memory provides a way to break through this barrier, allowing servers to expand memory capacity and meet the demands of increasingly complex AI applications. 
                                                                                            
                                                                                            
                                                                                                 *Improving Memory Utilization Efficiency:* 
                                                                                                 - CXL memory enables efficient use of memory resources through cross-device memory sharing and memory pooling technologies, thereby increasing memory utilization rates. 
                                                                                            
                                                                                            
                                                                                                 *Accelerating Computation Speed:* 
                                                                                                 - CXL memory allows the CPU to access memory on accelerators, achieving cross-device memory sharing and heterogeneous computing, which accelerates computation speed. 
                                                                                            
                                                                                            
                                                                                                 *Supporting Memory Consistency:* 
                                                                                                 - The CXL protocol (such as CXL.io, CXL.cache, CXL.mem) ensures consistency in memory operations across devices, avoiding data inconsistency issues. 
                                                                                            
                                                                                            
                                                                                                 *Flexible Scaling and Sharing:* 
                                                                                                 - CXL memory supports many-to-many flexible connections, allowing for many-to-many connections between accelerators and CPUs, achieving flexible scaling and memory sharing. 
                                                                                            
                                                                                        
                                                                                        
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Superior thermal Design 
                                                                 Comprehensive Thermal 
                                                                 Advanced full-metal thermal design and durable heatsinks to keep your system cool and efficient. 
                                                            
                                                            
                                                                                         AIO Fan Thermal Design 
                                                                                         High-efficiency fan design has achieved a simultaneous increase in both airflow and pressure, it helps to quickly dissipate heat and stabilize output performance. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultra Durable 
                                                                 Born from Ultra Durable™ 
                                                                 Technology embodies our commitment to excellence, providing gamers with a platform that's not only powerful but also built for longevity and reliability. AI TOP series Product are engineered to endure and excel. 
                                                            
                                                            
                                                                                    
                                                                                         Long Lifespan Durable™ Solid Caps 
                                                                                         GIGABYTE motherboards integrate the absolute best quality solid state capacitors that are rated to perform at maximum efficiency for extended periods, even in extreme performance configurations. With ultra-low ESR no matter how high the load, this provides peace of mind for end users who want to push their system hard, yet demand absolute reliability and stability. These exclusive capacitors also come in customized jet, exclusively on GIGABYTE product. 
                                                                                        
                                                                                    
                                                                                    
                                                                                         Humidity Protection with New Glass Fabric PCB 
                                                                                         There is nothing more harmful to the longevity of your PC than moisture, and most parts of the world experience moisture in the air as humidity at some point during the year. GIGABYTE motherboards have been designed to make sure that humidity is never an issue, incorporating a new Glass Fabric PCB technology that repels moisture caused by humid and damp conditions.  Glass Fabric PCB technology uses a new PCB material which reduces the amount of space between the fiber weave, making it much more difficult for moisture to penetrate compared to traditional motherboard PCBs. This offers much better protection from short circuit and system malfunction caused by humid and damp conditions. 
                                                                                        
                                                                                    
                                                                                
                                                        
                                                
                                        
                
                    
                        
                    
                
                
                         * This product requires compatible AI TOP hardware to be supported. A single CXL card cannot properly activate the AI TOP Utility. We recommend consulting with our Sales representative or authorized dealer contact before purchasing this product. 
                    
            
	

		* Product specifications and product appearance may differ from country to country. We recommend that you check with your local dealers for the specifications and appearance of the products available in your country. Colors of products may not be perfectly accurate due to variations caused by photographic variables and monitor settings so it may vary from images shown on this site. Although we endeavor to present the most accurate and comprehensive information at the time of publication, we reserve the right to make changes without prior notice.
	







                



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Claude Code SDK to reduce E2E test time]]></title>
            <link>https://jampauchoa.substack.com/p/best-of-both-worlds-using-claude</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151447</guid>
        </item>
        <item>
            <title><![CDATA[Patterns, Predictions, and Actions – A story about machine learning]]></title>
            <link>https://mlstory.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45150820</guid>
            <description><![CDATA[Image copyright: Princeton University Press]]></description>
            <content:encoded><![CDATA[
 
Image copyright: Princeton University Press
Hardcover
Princeton
University Press
Errata

Full preprint as
PDF
Problem sets (pdf)


Table of contents
Preface Acknowledgments

Introduction (PDF)
Fundamentals of prediction (PDF)
Supervised learning (PDF)
Representations and features (PDF)
Optimization (PDF)
Generalization (PDF)
Deep learning (PDF)
Datasets (PDF)
Causality (PDF)
Causal inference in practice (PDF)
Sequential decision making and dynamic
programming (PDF)
Reinforcement learning (PDF)
Epilogue (PDF)
Mathematical background (PDF)



Contact us
We welcome your feedback, questions, and suggestions. You can reach
us at contact@mlstory.org. If you taught from the book,
we’d love to hear about it.


Citations, license, typesetting
Please cite the print edition of this book as:
@book{hardtrecht2022patterns,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: Foundations of machine learning},
  year = {2022},
  publisher = {Princeton University Press}
}

We maintain an archival version of the book at arXiv:2102.05242. The web
version is more up-to-date than the arXiv version. The print version
contains additional improvements and editing not present in the web
version.
The text available on this website is licensed under the Creative
Commons BY-NC-ND 4.0 license.
This book is typeset using pandoc with the unbuch setup.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oldest recorded transaction]]></title>
            <link>https://avi.im/blag/2025/oldest-txn/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149626</guid>
            <description><![CDATA[The oldest recorded transaction was in 3100 BC]]></description>
            <content:encoded><![CDATA[The other day I posted a tweet with this image which I thought was funny:This is the oldest transaction database from 3100 BC - recording accounts of malt and barley groats. Considering this thing survived 5000 years (holy shit!) with zero downtime and has stronger durability guarantees than most databases today.I call it rock solid durability.This got me thinking, can I insert this date in today’s database? What is the oldest timestamp a database can support?So I checked the top three databases: MySQL, Postgres, and SQLite:MySQL1000 ADPostgres4713 BCSQLite4713 BCToo bad you cannot use MySQL for this. Postgres and SQLite support the Julian calendar and the lowest date is Jan 01, 4713 BC:sales=# INSERT INTO orders VALUES ('4713-01-01 BC'::date);
INSERT 0 1
sales=# SELECT * FROM orders;
   timestamp
---------------
 4713-01-01 BC
(1 row)
sales=# INSERT INTO orders VALUES ('4714-01-01 BC'::date);
ERROR:  date out of range: "4714-01-01 BC"
I wonder how people store dates older than this. Maybe if I’m a British Museum manager, and I want to keep theft inventory details. How do I do it? As an epoch? Store it as text? Use some custom system? How do I get it to support all the custom operations that a typical TIMESTAMP supports?Thanks to aku, happy_shady, Mr. Bhat, and General Bruh for reading an early draft of this post.1. Source of the image: Sumer civilization2. I found this from the talk 1000x: The Power of an Interface for Performance by
Joran Dirk Greef, CEO of TigerBeetle, timestamped @ 38:10.3. The talk has other bangers too, like this or this.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI surveillance should be banned while there is still time]]></title>
            <link>https://gabrielweinberg.com/p/ai-surveillance-should-be-banned</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149281</guid>
            <description><![CDATA[All the same privacy harms with online tracking are also present with AI, but worse.]]></description>
            <content:encoded><![CDATA[Original cartoon by Dominique Lizaambard (left), updated for AI, by AI (right).All the same privacy harms with online tracking are also present with AI, but worse.While chatbot conversations resemble longer search queries, chatbot privacy harms have the potential to be significantly worse because the inference potential is dramatically greater. Longer input invites more personal information to be provided, and people are starting to bare their souls to chatbots. The conversational format can make it feel like you’re talking to a friend, a professional, or even a therapist. While search queries reveal interests and personal problems, AI conversations take their specificity to another level and, in addition, reveal thought processes and communication styles, creating a much more comprehensive profile of your personality. This richer personal information can be more thoroughly exploited for manipulation, both commercially and ideologically, for example, through behavioral chatbot advertising and models designed (or themselves manipulated through SEO or hidden system prompts) to nudge you towards a political position or product. Chatbots have already been found to be more persuasive than humans and have caused people to go into delusional spirals as a result. I suspect we’re just scratching the surface, since they can become significantly more attuned to your particular persuasive triggers through chatbot memory features, where they train and fine-tune based on your past conversations, making the influence much more subtle. Instead of an annoying and obvious ad following you around everywhere, you can have a seemingly convincing argument, tailored to your personal style, with an improperly sourced “fact” that you’re unlikely to fact-check or a subtle product recommendation you’re likely to heed. That is, all the privacy debates surrounding Google search results from the past two decades apply one-for-one to AI chats, but to an even greater degree. That’s why we (at DuckDuckGo) started offering Duck.ai for protected chatbot conversations and optional, anonymous AI-assisted answers in our private search engine. In doing so, we’re demonstrating that privacy-respecting AI services are feasible. But unfortunately, such protected chats are not yet standard practice, and privacy mishaps are mounting quickly. Grok leaked hundreds of thousands of chatbot conversations that users thought were private. Perplexity’s AI agent was shown to be vulnerable to hackers who could slurp up your personal information. Open AI is openly talking about their vision for a “super assistant” that tracks everything you do and say (including offline). And Anthropic is going to start training on your chatbot conversations by default (previously the default was off). I collected these from just the past few weeks!It would therefore be ideal if Congress could act quickly to ensure that protected chats become the rule rather than the exception. And yet, I’m not holding my breath because it’s 2025 and the U.S. still doesn’t have a general online privacy law, let alone privacy enshrined in the Constitution as a fundamental right, as it should be. However, there does appear to be an opening right now for AI-specific federal legislation, despite the misguided attempts to ban state AI legislation.Time is running out because every day that passes further entrenches bad privacy practices. Congress must move before history completely repeats itself and everything that happened with online tracking happens again with AI tracking. AI surveillance should be banned while there is still time. No matter what happens, though, we will still be here, offering protected services, including optional AI services, to consumers who want to reap the productivity benefits of online tools without the privacy harms. Share]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[996]]></title>
            <link>https://lucumr.pocoo.org/2025/9/4/996/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149049</guid>
            <description><![CDATA[There is cost to your lifestyle.]]></description>
            <content:encoded><![CDATA[
        
  

  
  written on September 04, 2025
  

  
“Amazing salary, hackerhouse in SF, crazy equity.
996. Our mission is
OSS.” — Gregor Zunic
“The current vibe is no drinking, no drugs, 9-9-6, […].” — Daksh
Gupta
“The truth is, China’s really doing ‘007’ now—midnight to midnight, seven
days a week […] if you want to build a $10 billion company, you have to work
seven days a week.” — Harry Stebbings

I love work.  I love working late nights, hacking on things.  This week I
didn’t go to sleep before midnight once.  And yet…
I also love my wife and kids. I love long walks, contemplating life over good
coffee, and deep, meaningful conversations.  None of this would be possible if
my life was defined by 12 hour days, six days a week.  More importantly, a
successful company is not a sprint, it’s a marathon.
And this is when this is your own company!  When you devote 72 hours a week to
someone else’s startup, you need to really think about that arrangement a few
times.  I find it highly irresponsible for a founder to promote that model.  As
a founder, you are not an employee, and your risks and leverage are
fundamentally different.
I will always advocate for putting the time
in because it is what brought me happiness.
Intensity, and giving a shit about what I’m doing, will always matter to me.
But you don’t measure that by the energy you put in, or the hours you’re
sitting in the office, but the output you produce.  Burning out on twelve-hour
days, six days a week, has no prize at the end.  It’s unsustainable, it
shouldn’t be the standard and it sure as hell should not be seen as a positive
sign of a company.
I’ve pulled many all-nighters, and I’ve enjoyed them.  I still do.  But they’re
enjoyable in the right context, for the right reasons, and when that is a
completely personal choice, not the basis of company culture.
And that all-nighter?  It comes with a fucked up and unproductive morning the
day after.
When someone promotes a 996 work culture, we should push back.


  
  This entry was tagged
    
      thoughts
  

  
    copy as / view markdown
  
  
  

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We hacked Burger King: How auth bypass led to drive-thru audio surveillance]]></title>
            <link>https://bobdahacker.com/blog/rbi-hacked-drive-thrus/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148944</guid>
        </item>
        <item>
            <title><![CDATA[Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5]]></title>
            <link>https://github.com/b4rtaz/distributed-llama/discussions/255</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148237</guid>
            <description><![CDATA[qwen3_30b.mov Setup [🔀 TP-Link LS1008G Switch] | | | | | | | |_______ 🔸 raspberrypi2 (ROOT) 10.0.0.2 | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1 | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0....]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    




  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  




    

      






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    168

  

  
        
            
          Star
          2.5k

  



        

        


          

  
    


  

  




    

        
  


            
    
      
    
  
        
  
    
    qwen3_30b.mov
    
  

  

  


Setup
[🔀 TP-Link LS1008G Switch]
      | | | |
      | | | |_______ 🔸 raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ 🔹 raspberrypi4 (WORKER 3) 10.0.0.4

Device: 4 x Raspberry Pi 5 8GB
Distributed Llama version: 0.16.0
Model: qwen3_30b_a3b_q40
Benchmark




Evaluation
Prediction




4 x Raspberry Pi 5 8GB
14.33 tok/s
13.04 tok/s



b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt "<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant
" --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
📄 AddBos: 0
📄 BosId: 151643 (<|endoftext|>)
📄 EosId: 151645 (<|im_end|>) 
📄 RegularVocabSize: 151643
📄 SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
💡 Arch: Qwen3 MoE
💡 HiddenAct: Silu
💡 Dim: 2048
💡 HeadDim: 128
💡 QDim: 4096
💡 KvDim: 512
💡 HiddenDim: 6144
💡 VocabSize: 151936
💡 nLayers: 48
💡 nHeads: 32
💡 nKvHeads: 4
💡 OrigSeqLen: 262144
💡 nExperts: 128
💡 nActiveExperts: 8
💡 MoeHiddenDim: 768
💡 SeqLen: 4096
💡 NormEpsilon: 0.000001
💡 RopeType: Falcon
💡 RopeTheta: 10000000
📀 RequiredMemory: 5513 MB
⭕ Socket[0]: connecting to 10.0.0.1:9999 worker
⭕ Socket[0]: connected
⭕ Socket[1]: connecting to 10.0.0.3:9999 worker
⭕ Socket[1]: connected
⭕ Socket[2]: connecting to 10.0.0.4:9999 worker
⭕ Socket[2]: connected
⭕ Network is initialized
🧠 CPU: neon dotprod fp16
💿 Loading weights...
💿 Weights loaded
🚁 Network is in non-blocking mode
<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant

🔷️ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
🔶 Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
🔶 Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
🔶 Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
🔶 Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
🔶 Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
🔶 Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
🔶 Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
🔶 Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
🔶 Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
🔶 Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
🔶 Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  –
🔶 Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
🔶 Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
🔶 Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
🔶 Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

🔶 Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
⭕ Network is closed

    
    


          

        

         







  

  

  

  

  

  

  

  


    




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Software Development Methodology for Disciplined LLM Collaboration]]></title>
            <link>https://github.com/Varietyz/Disciplined-AI-Software-Development</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148180</guid>
            <description><![CDATA[This methodology provides a structured approach for collaborating with AI systems on software development projects. It addresses common issues like code bloat, architectural drift, and context dilu...]]></description>
            <content:encoded><![CDATA[

Disciplined AI Software Development - Collaborative
A structured approach for working with AI on development projects. This methodology addresses common issues like code bloat, architectural drift, and context dilution through systematic constraints.
The Context Problem
AI systems work on Question → Answer patterns. When you ask for broad, multi-faceted implementations, you typically get:

Functions that work but lack structure
Repeated code across components
Architectural inconsistency over sessions
Context dilution causing output drift
More debugging time than planning time

How This Works
The methodology uses four stages with systematic constraints and validation checkpoints. Each stage builds on empirical data rather than assumptions.
Planning saves debugging time. Planning thoroughly upfront typically prevents days of fixing architectural issues later.
The Four Stages
Stage 1: AI Configuration
Set up your AI model's custom instructions using AI-PREFERENCES.md. This establishes behavioral constraints and uncertainty flagging with ⚠️ indicators when the AI lacks certainty.
Stage 2: Collaborative Planning
Share METHODOLOGY.md with the AI to structure your project plan. Work together to:

Define scope and completion criteria
Identify components and dependencies
Structure phases based on logical progression
Generate systematic tasks with measurable checkpoints

Output: A development plan following dependency chains with modular boundaries.
Stage 3: Systematic Implementation
Work phase by phase, section by section. Each request follows: "Can you implement [specific component]?" with focused objectives.
File size stays ≤150 lines. This constraint provides:

Smaller context windows for processing
Focused implementation over multi-function attempts
Easier sharing and debugging

Implementation flow:
Request specific component → AI processes → Validate → Benchmark → Continue

Stage 4: Data-Driven Iteration
The benchmarking suite (built first) provides performance data throughout development. Feed this data back to the AI for optimization decisions based on measurements rather than guesswork.
Why This Approach Works
Decision Processing: AI handles "Can you do A?" more reliably than "Can you do A, B, C, D, E, F, G, H?"
Context Management: Small files and bounded problems prevent the AI from juggling multiple concerns simultaneously.
Empirical Validation: Performance data replaces subjective assessment. Decisions come from measurable outcomes.
Systematic Constraints: Architectural checkpoints, file size limits, and dependency gates force consistent behavior.
Example Projects


Discord Bot Template - Production-ready bot foundation with plugin architecture, security, API management, and comprehensive testing. 46 files, all under 150 lines, with benchmarking suite and automated compliance checking. (View Project Structure)


PhiCode Runtime - Programming language runtime engine with transpilation, caching, security validation, and Rust acceleration. Complex system maintaining architectural discipline across 70+ modules. (View Project Structure)


PhiPipe - CI/CD regression detection system with statistical analysis, GitHub integration, and concurrent processing. Go-based service handling performance baselines and automated regression alerts. (View Project Structure)


You can compare the methodology principles to the codebase structure to see how the approach translates to working code.
Implementation Steps
Setup

Configure AI with AI-PREFERENCES.md as custom instructions
Share METHODOLOGY.md for planning session
Collaborate on project structure and phases
Generate systematic development plan

Execution

Build Phase 0 benchmarking infrastructure first
Work through phases sequentially
Implement one component per interaction
Run benchmarks and share results with AI
Validate architectural compliance continuously

Quality Assurance

Performance regression detection
Architectural principle validation
Code duplication auditing
File size compliance checking
Dependency boundary verification

Project State Extraction
Use the included project extraction tool systematically to generate structured snapshots of your codebase:
python scripts/project_extract.py
Configuration Options:

SEPARATE_FILES = False: Single THE_PROJECT.md file (recommended for small codebases)
SEPARATE_FILES = True: Multiple files per directory (recommended for large codebases and focused folder work)
INCLUDE_PATHS: Directories and files to analyze
EXCLUDE_PATTERNS: Skip cache directories, build artifacts, and generated files

Output:

Complete file contents with syntax highlighting
File line counts with architectural warnings (⚠️ for 140-150 lines, ‼️ for >150 lines on code files)
Tree structure visualization
Ready-to-share

output examples can be found here
Use the tool to share a complete or partial project state with the AI system, track architectural compliance, and create focused development context.
What to Expect
AI Behavior: The methodology reduces architectural drift and context degradation compared to unstructured approaches. AI still needs occasional reminders about principles - this is normal.
Development Flow: Systematic planning tends to reduce debugging cycles. Focused implementation helps minimize feature bloat. Performance data supports optimization decisions.
Code Quality: Architectural consistency across components, measurable performance characteristics, maintainable structure as projects scale.

Claude Q&A Documentation
Coverage includes:

Methodology understanding and workflow patterns
Project initialization and Phase 0 requirements
Tool usage and technology stack compatibility
Quality enforcement and violation handling
User experience across different skill levels


Frequently Asked Questions
Origin & Development

What problem led you to create this methodology?

I kept having to restate my preferences and architectural requirements to AI systems. It didn't matter which language or project I was working on - the AI would consistently produce either bloated monolithic code or underdeveloped implementations with issues throughout.
This led me to examine the meta-principles driving code quality and software architecture. I questioned whether pattern matching in AI models might be more effective when focused on underlying software principles rather than surface-level syntax. Since pattern matching is logic-driven and machines fundamentally operate on simple question-answer pairs, I realized that functions with multiple simultaneous questions were overwhelming the system.
The breakthrough came from understanding that everything ultimately transpiles to binary - a series of "can you do this? → yes/no" decisions. This insight shaped my approach: instead of issuing commands, ask focused questions in proper context. Rather than mentally managing complex setups alone, collaborate with AI to devise systematic plans.



How did you discover these specific constraints work?

Through extensive trial and error. AI systems will always tend to drift even under constraints, but they're significantly more accurate with structured boundaries than without them. You occasionally need to remind the AI of its role to prevent deviation - like managing a well-intentioned toddler that knows the rules but sometimes pushes boundaries trying to satisfy you.
These tools are far from perfect, but they're effective instruments for software development when properly constrained.



What failures or frustrations shaped this approach?

Maintenance hell was the primary driver. I grew tired of responses filled with excessive praise: "You have found the solution!", "You have redefined the laws of physics with your paradigm-shifting script!" This verbose fluff wastes time, tokens, and patience without contributing to productive development.
Instead of venting frustration on social media about AI being "just a dumb tool," I decided to find methods that actually work. My approach may not help everyone, but I hope it benefits those who share similar AI development frustrations.


Personal Practice

How consistently do you follow your own methodology?

Since creating the documentation, I haven't deviated. Whenever I see the model producing more lines than my methodology restricts, I immediately interrupt generation with a flag: "‼️ ARCHITECTURAL VIOLATION, ADHERE TO PRINCIPLES ‼️" I then provide the method instructions again, depending on how context is stored and which model I'm using.



What happens when you deviate from it?

I become genuinely uncomfortable. Once I see things starting to degrade or become tangled, I compulsively need to organize and optimize. Deviation simply isn't an option anymore.



Which principles do you find hardest to maintain?

Not cursing at the AI when it drifts during complex algorithms! But seriously, it's a machine - it's not perfect, and neither are we.


AI Development Journey

When did you start using AI for programming?

In August 2024, I created a RuneLite theme pack, but one of the plugin overlays didn't match my custom layout. I opened a GitHub issue (creating my first GitHub account to do so) requesting a customization option. The response was: "It's not a priority - if you want it, build it yourself."
I used ChatGPT to guide me through forking RuneLite and creating a plugin. This experience sparked intense interest in underlying software principles rather than just syntax.



How has your approach evolved over time?

I view development like a book: syntax is the cover, logic is the content itself. Rather than learning syntax structures, I focused on core meta-principles - how software interacts, how logic flows, different algorithm types. I quickly realized everything reduces to the same foundation: question and answer sequences.
Large code structures are essentially chaotic meetings - one coordinator fielding questions and answers from multiple sources, trying to provide correct responses without mix-ups or misinterpretation. If this applies to human communication, it must apply to software principles.



What were your biggest mistakes with AI collaboration?

Expecting it to intuitively understand my requirements, provide perfect fixes, be completely honest, and act like a true expert. This was all elaborate roleplay that produced poor code. While fine for single-purpose scripts, it failed completely for scalable codebases.
I learned not to feed requirements and hope for the best. Instead, I needed to collaborate actively - create plans, ask for feedback on content clarity, and identify uncertainties. This gradual process taught me the AI's actual capabilities and most effective collaboration methods.


Methodology Specifics

Why 150 lines exactly?

Multiple benefits: easy readability, clear understanding, modularity enforcement, architectural clarity, simple maintenance, component testing, optimal AI context retention, reusability, and KISS principle adherence.



How did you determine Phase 0 requirements?

From meta-principles of software: if it displays, it must run; if it runs, it can be measured; if it can be measured, it can be optimized; if it can be optimized, it can be reliable; if it can be reliable, it can be trusted.
Regardless of project type, anything requiring architecture needs these foundations. You must ensure changes don't negatively impact the entire system. A single line modification in a nested function might work perfectly but cause 300ms boot time regression for all users.
By testing during development, you catch inefficiencies early. Integration from the start means simply hooking up new components and running tests via command line - minimal time investment with actual value returned. I prefer validation and consistency throughout development rather than programming blind.


Practical Implementation

How do you handle projects that don't fit the methodology?

I adapt them to fit, or if truly impossible, I adjust the method itself. This is one methodology - I can generate countless variations as needed. Having spent 6700+ hours in AI interactions across multiple domains (not just software), I've developed strong system comprehension that enables creating adjusted methodologies on demand.



What's the learning curve for new users?

I cannot accurately answer this question. I've learned that I'm neurologically different - what I perceive as easy or obvious isn't always the case for others. This question is better addressed by someone who has actually used this methodology to determine its learning curve.



When shouldn't someone use this approach?

If you're not serious about projects, despise AI, dislike planning, don't care about modularization, or are just writing simple scripts. However, for anything requiring reliability, I believe this is currently the most effective method.
You still need programming fundamentals to use this methodology effectively - it's significantly more structured than ad-hoc approaches.



Workflow Visualization

  
      ---
config:
  layout: elk
  theme: neo-dark
---
flowchart TD
    A["Project Idea"] --> B["🤖 Stage 1: AI Configuration<br>AI-PREFERENCES.md Custom Instructions"]
    B --> C["Stage 2: Collaborative Planning<br>Share METHODOLOGY.md"]
    C --> D["Define Scope & Completion Criteria"]
    D --> E["Identify Components & Dependencies"]
    E --> F["Structure Phases Based on Logic"]
    F --> G["Document Edge Cases - No Implementation"]
    G --> H["Generate Development Plan with Checkpoints"]
    H --> I["🔧 Stage 3: Phase 0 Infrastructure<br>MANDATORY BEFORE ANY CODE"]
    I --> J["Benchmarking Suite + Regression Detection"]
    J --> K["GitHub Workflows + Quality Gates"]
    K --> L["Test Suite Infrastructure + Stress Tests"]
    L --> M["Documentation Generation System"]
    M --> N["Centralized Configuration + Constants"]
    N --> O["📁 project_extract.py Setup<br>Single/Multiple File Config"]
    O --> P["Initial Project State Extraction"]
    P --> Q["Share Context with AI"]
    Q --> R["Start Development Session<br>Pre-Session Compliance Audit"]
    R --> S{"Next Phase Available?"}
    S -- No --> Z["Project Complete"]
    S -- Yes --> T["Select Single Component<br>Target ≤150 Lines"]
    T --> U{"Multi-Language Required?"}
    U -- Yes --> V["Document Performance Justification<br>Measurable Benefits Required"]
    V --> W["Request AI Implementation"]
    U -- No --> W
    W --> X{"AI Uncertainty Flag?"}
    X -- ⚠️ Yes --> Y["Request Clarification<br>Provide Additional Context"]
    Y --> W
    X -- Clear --> AA["Stage 3: Systematic Implementation"]
    AA --> BB{"Automated Size Check<br>validate-phase Script"}
    BB -- >150 Lines --> CC["AUTOMATED: Split Required<br>Maintain SoC Boundaries"]
    CC --> W
    BB -- ≤150 Lines --> DD["Incremental Compliance Check<br>DRY/KISS/SoC Validation"]
    DD --> EE{"Architectural Principles Pass?"}
    EE -- No --> FF["Flag Specific Violations<br>Reference Methodology"]
    FF --> W
    EE -- Yes --> GG["📊 Stage 4: Data-Driven Iteration<br>Run Benchmark Suite + Save Baselines"]
    GG --> HH["Compare Against Historical Timeline<br>Regression Analysis"]
    HH --> II{"Performance Gate Pass?"}
    II -- Regression Detected --> JJ["Share Performance Data<br>Request Optimization"]
    JJ --> W
    II -- Pass --> KK["Integration Test<br>Verify System Boundaries"]
    KK --> LL{"Cross-Platform Validation?"}
    LL -- Fail --> MM["Address Deployment Constraints<br>Real-World Considerations"]
    MM --> W
    LL -- Pass --> NN{"More Components in Phase?"}
    NN -- Yes --> T
    NN -- No --> OO["🚦 Phase Quality Gate<br>Full Architecture Audit"]
    OO --> PP["Production Simulation<br>Resource Cleanup + Load Test"]
    PP --> QQ{"All Quality Gates Pass?"}
    QQ -- No --> RR["Document Failed Checkpoints<br>Block Phase Progression"]
    RR --> T
    QQ -- Yes --> SS["End Development Session<br>Technical Debt Assessment"]
    SS --> TT["📁 Extract Updated Project State<br>Generate Fresh Context"]
    TT --> UU["Phase Results Documentation<br>Metrics + Outcomes + Timeline"]
    UU --> VV["Update Development Plan<br>Mark Phase Complete"]
    VV --> S
    WW["validate-phase<br>AUTOMATED: File Size + Structure"] -.-> BB
    XX["dry-audit<br>AUTOMATED: Cross-Module Duplication"] -.-> DD
    YY["CI/CD Workflows<br>AUTOMATED: Merge Gates"] -.-> GG
    ZZ["Performance Timeline<br>AUTOMATED: Historical Data"] -.-> HH
    AAA["Dependency Validator<br>AUTOMATED: Import Boundaries"] -.-> KK
    BBB["Architecture Auditor<br>AUTOMATED: SoC Compliance"] -.-> OO
    WW -. BUILD FAILURE .-> CC
    YY -. MERGE BLOCKED .-> JJ
    BBB -. AUDIT FAILURE .-> RR
    style Y fill:#7d5f00
    style CC fill:#770000
    style FF fill:#7d5f00
    style JJ fill:#7d5f00
    style MM fill:#770000
    style RR fill:#770000

    
  
    
      Loading

  


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why language models hallucinate]]></title>
            <link>https://openai.com/index/why-language-models-hallucinate/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45147385</guid>
        </item>
        <item>
            <title><![CDATA[Rug pulls, forks, and open-source feudalism]]></title>
            <link>https://lwn.net/SubscriberLink/1036465/e80ebbc4cee39bfb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45146967</guid>
            <description><![CDATA[Like almost all human endeavors, open-source software development involves a range of power dyn [...]]]></description>
            <content:encoded><![CDATA[



Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the discount offer on the right.  Thank you
for visiting LWN.net!


Special discount offer
           
           Subscribe to LWN now at the
           "professional hacker" level for at least six months,
           and you will
           receive a special discount of 25%.
           







Like almost all human endeavors, open-source software development involves
a range of power dynamics.  Companies, developers, and users are all
concerned with the power to influence the direction of the software — and,
often, to profit from it.  At the 2025 Open
Source Summit Europe, Dawn Foster talked about how those dynamics can
play out, with an eye toward a couple of tactics — rug pulls and forks — that
are available to try to shift power in one direction or another.
Power dynamics

Since the beginning of history, Foster began, those in power have tended to
use it against those who were weaker.  In the days of feudalism, control of
the land led to exploitation at several levels.  In the open-source world,
the large cloud providers often seem to have the most power, which they use
against smaller companies.  Contributors and maintainers often have less
power than even the smaller companies, and users have less power yet.  



We have built a world where it is often easiest to just use whatever a
cloud provider offers, even with open-source software.  Those providers may
not contribute back to the projects they turn into services, though,
upsetting the smaller companies that are, likely as not, doing the bulk of
the work to provide the software in question in the first place.  Those
companies can have a power of their own, however: the power to relicense
the software.  Pulling the rug out from under users of the software in this
way can change the balance of power with regard to cloud providers, but it
leaves contributors and users in a worse position than before.  But
there is a power at this level too: the power to fork the software,
flipping the power balance yet again.

Companies that control a software project have the power to carry out this
sort of rug pull, and they are often not shy about exercising it.
Single-company projects, clearly, are at a much higher risk of rug pulls;
the company has all the power in this case, and others have little
recourse.  So one should look at a company's reputation before adopting a
software project, but that is only so helpful.  Companies can change
direction without notice, be acquired, or go out of business, making
previous assessments of their reputation irrelevant.

The problem often comes down to the simple fact that companies have to
answer to their investors, and that often leads to pressure to relicense
the software they have created in order to increase revenue.  This is
especially true in cases where cloud providers are competing for the same
customers as the company that owns the project.  The result can be a switch
to a more restrictive license aimed at making it harder for other companies
to profit from the project.

A rug pull of this nature can lead to a fork of the project — a rebellious,
collective action aimed at regaining some power over the code.  But a fork
is not a simple matter; it is a lot of work, and will fail without people
and resources behind it.  The natural source for that is a large company;
cloud providers, too, can try to shift power via a fork, and they have the
ability to back their fork up with the resources it needs to succeed.



A relicensing event does not always lead to a popular fork; that did not
happen with MongoDB or Sentry, for example.  Foster said she had not looked
into why that was the case.  Sometimes rug pulls take other forms, such as
when Perforce, after acquiring Puppet in 2022, moved it development and
releases behind closed doors, with a reduced frequency of releases back to
the public repository.  That action kicked off the OpenVox fork.
Looking at the numbers

Foster has spent some time analyzing rug pulls, forks, and what happens
thereafter; a lot of the results are available
for download as Jupyter notebooks.  For each rug-pull event, she looked
at the contributor makeup of the project before and after the ensuing fork
in an attempt to see what effects are felt by the projects involved.

In 2021, Elastic relicensed Elasticsearch
under the non-free Server Side Public License (SSPL).  Amazon Web Services
then forked the project as OpenSearch.  Before the fork, most of
the Elasticsearch contributors were Elastic employees; that,
unsurprisingly, did not change afterward.  OpenSearch started with no
strong contributor base, so had to build its community from scratch.  As a
result, the project has been dominated by Amazon contributors ever since;
the balance has shifted slowly over time, but there was not a big uptick in
outside contributors even after OpenSearch became a Linux Foundation
project in 2024.  While starting a project under a neutral foundation can
help attract contributors, she said, moving a project under a foundation's
umbrella later on does not seem to provide the same benefit.

Terraform was
developed mostly by Hashicorp, which relicensed
the software under the non-free Business Source License in 2023.  One
month later, the OpenTofu fork was
started under the Linux Foundation.  While the contributor base for
Terraform, which was almost entirely Hashicorp employees, changed little
after the fork, OpenTofu quickly acquired a number of contributors from
several companies, none of whom had been Terraform contributors before.  In
this case, users drove the fork and placed it under a neutral foundation,
resulting in a more active developer community.

In 2024, Redis was relicensed under the
SSPL; the Valkey fork was quickly organized, under the Linux Foundation,
by Redis contributors.  The Redis project differed from the others
mentioned here in that, before the fork, it had nearly twice as many
contributors from outside the company as from within; after the fork, the
number of external Redis contributors dropped to zero.  All of the external
contributors fled to Valkey, with the result that Valkey started with a
strong community representing a dozen or so companies.

Looking at how the usage of these projects changes is harder, she
said, but there appears to be a correlation between the usage of a project
and the number of GitHub forks (cloned repository copies) it has.  There is
typically a spike in these clones after a relicensing event, suggesting
that people are considering creating a hard fork of the project.  In all
cases, the forks that emerged appeared to have less usage than the original
by the "GitHub forks" metric; both branches of the fork continue to go
forward.  But, she said, projects that are relicensed do tend to show
reduced usage, especially when competing forks are created under foundations.
What to do

This kind of power game creates problems for both contributors and users,
she said; we contribute our time to these projects, and need them to not be
pulled out from under us.  There is no way to know when a rug pull might
happen, but there are some warning signs to look out for.  At the top of
her list was the use of a contributor license agreement (CLA); these
agreements create a power imbalance, giving the company involved the power
to relicense the software.  Projects with CLAs more commonly are subject to
rug pulls; projects using a developers certificate of origin do not have the
same power imbalance and are less likely to be rug pulled.

One should also look at the governance of a project; while being housed
under a foundation reduces the chance of a rug pull, that can still happen,
especially in cases where the contributors are mostly from a single
company.  She mentioned the Cortex project, housed under
the Cloud Native Computing Foundation, which was controlled by Grafana; that
company eventually forked its own project to create Mimir.  To avoid this kind of
surprise, one should look for projects with neutral governance, with
leaders from multiple organizations.

Projects should also be evaluated on their contributor base; are there
enough contributors to keep things going?  Companies can help, of course,
by having their employees contribute to the projects they depend on,
increasing influence and making those projects more sustainable.  She
mentioned the CHAOSS project, which
generates metrics to help in the judgment of the viability of development
projects.  CHAOSS has put together a set of
"practitioner guides" intended to help contributors and maintainers
make improvements within a project.

With the sustained rise of the big cloud providers, she concluded, the
power dynamics around open-source software are looking increasingly feudal.
Companies can use relicensing to shift power away from those providers, but
they also take power from contributors when the pull the rug in this way.
Those contributors, though, are in a better position than the serfs of old,
since they have the ability to fork a project they care about, shifting
power back in their direction.


Hazel Weakly asked if there are other protections that contributors and
users might develop to address this problem.  Foster answered that at least
one company changed its mind about a planned relicensing action after
seeing the success of the Valkey and OpenTofu forks.  The ability to fork
has the effect of making companies think harder, knowing that there may be
consequences that follow a rug pull.  Beyond that, she reiterated that
projects should be pushed toward neutral governance.

Dirk Hohndel added that the best thing to do is to bring more outside
contributors into a project; the more of them there are, the higher the
risk associated with a rug pull.  Anybody who just sits back within a
project, he said, is just a passenger; it is better to be driving.

Foster's
slides are available for interested readers.

[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GLM 4.5 with Claude Code]]></title>
            <link>https://docs.z.ai/guides/llm/glm-4.5</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45145457</guid>
            <description><![CDATA[GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.
Both models share a similar training pipeline: an initial pretraining phase on 15 trillion tokens of general-domain data, followed by targeted fine-tuning on datasets covering code, reasoning, and agent-specific tasks. The context length has been extended to 128k tokens, and reinforcement learning was applied to further enhance reasoning, coding, and agent performance.
GLM-4.5 and GLM-4.5-Air are optimized for tool invocation, web browsing, software engineering, and front-end development. They can be integrated into code-centric agents such as Claude Code and Roo Code, and also support arbitrary agent applications through tool invocation APIs.
Both models support hybrid reasoning modes, offering two execution modes: Thinking Mode for complex reasoning and tool usage, and Non-Thinking Mode for instant responses. These modes can be toggled via the thinking.typeparameter (with enabled and disabled settings), and dynamic thinking is enabled by default.]]></description>
            <content:encoded><![CDATA[   Overview

GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.
Both models share a similar training pipeline: an initial pretraining phase on 15 trillion tokens of general-domain data, followed by targeted fine-tuning on datasets covering code, reasoning, and agent-specific tasks. The context length has been extended to 128k tokens, and reinforcement learning was applied to further enhance reasoning, coding, and agent performance.
GLM-4.5 and GLM-4.5-Air are optimized for tool invocation, web browsing, software engineering, and front-end development. They can be integrated into code-centric agents such as Claude Code and Roo Code, and also support arbitrary agent applications through tool invocation APIs.
Both models support hybrid reasoning modes, offering two execution modes: Thinking Mode for complex reasoning and tool usage, and Non-Thinking Mode for instant responses. These modes can be toggled via the thinking.typeparameter (with enabled and disabled settings), and dynamic thinking is enabled by default.
   GLM-4.5 Serials

   Capability

   Introducting GLM-4.5
Overview
The first-principle measure of AGI lies in integrating more general intelligence capabilities without compromising existing functions. GLM-4.5 represents our first complete realization of this concept. It combines advanced reasoning, coding, and agent capabilities within a single model, achieving a significant technological breakthrough by natively fusing reasoning, coding, and agent abilities to meet the complex demands of agent-based applications.
To comprehensively evaluate the model’s general intelligence, we selected 12 of the most representative benchmark suites, including MMLU Pro, AIME24, MATH 500, SciCode, GPQA, HLE, LiveCodeBench, SWE-Bench, Terminal-bench, TAU-Bench, BFCL v3, and BrowseComp. Based on the aggregated average scores, GLM-4.5 ranks second globally among all models, first among domestic models, and first among open-source models.

Higher Parameter Efficiency
GLM-4.5 has half the number of parameters of DeepSeek-R1 and one-third that of Kimi-K2, yet it outperforms them on multiple standard benchmark tests. This is attributed to the higher parameter efficiency of GLM architecture. Notably, GLM-4.5-Air, with 106 billion total parameters and 12 billion active parameters, achieves a significant breakthrough—surpassing models such as Gemini 2.5 Flash, Qwen3-235B, and Claude 4 Opus on reasoning benchmarks like Artificial Analysis, ranking among the top three domestic models in performance.
On charts such as SWE-Bench Verified, the GLM-4.5 series lies on the Pareto frontier for performance-to-parameter ratio, demonstrating that at the same scale, the GLM-4.5 series delivers optimal performance.
Low Cost, High Speed
Beyond performance optimization, the GLM-4.5 series also achieves breakthroughs in cost and efficiency, resulting in pricing far lower than mainstream models: API call costs are as low as $0.2 per million input tokens and $1.1 per million output tokens.
At the same time, the high-speed version demonstrates a generation speed exceeding 100 tokens per second in real-world tests, supporting low-latency and high-concurrency deployment scenarios—balancing cost-effectiveness with user interaction experience.
Real-World Evaluation
Real-world performance matters more than leaderboard rankings. To evaluate GLM-4.5’s effectiveness in practical Agent Coding scenarios, we integrated it into Claude Code and benchmarked it against Claude 4 Sonnet, Kimi-K2, and Qwen3-Coder.
The evaluation consisted of 52 programming and development tasks spanning six major domains, executed in isolated container environments with multi-turn interaction tests.
As shown in the results (below), GLM-4.5 demonstrates a strong competitive advantage over other open-source models, particularly in tool invocation reliability and task completion rate. While there remains room for improvement compared to Claude 4 Sonnet, GLM-4.5 delivers a largely comparable experience in most scenarios.
To ensure transparency, we have released all 52 test problems along with full agent trajectories for industry validation and reproducibility.
   Usage
Core Capability: Coding Skills → Intelligent code generation | Real-time code completion | Automated bug fixing
Supports major languages including Python, JavaScript, and Java.
Generates well-structured, scalable, high-quality code based on natural language instructions.
Focuses on real-world development needs, avoiding templated or generic outputs.
Use Case: Complete refactoring-level tasks within 1 hour; generate full product prototypes in 5 minutes.
   Resources

API Documentation: Learn how to call the API.

    Quick Start
Thinking Mode
GLM-4.5 offers a “Deep Thinking Mode” that users can enable or disable by setting the thinking.type parameter. This parameter supports two values: enabled (enabled) and disabled (disabled). By default, dynamic thinking is enabled.
Simple Tasks (No Thinking Required): For straightforward requests that do not require complex reasoning (e.g., fact retrieval or classification), thinking is unnecessary. Examples include:

When was Z.AI founded?
Translate the sentence “I love you” into Chinese.


Moderate Tasks (Default/Some Thinking Required): Many common requests require stepwise processing or deeper understanding. The GLM-4.5 series can flexibly apply thinking capabilities to handle tasks such as:

Why does Jupiter have more moons than Saturn, despite Saturn being larger?
Compare the advantages and disadvantages of flying versus taking the high-speed train from Beijing to Shanghai.



Difficult Tasks (Maximum Thinking Capacity): For truly complex challenges—such as solving advanced math problems, network-related questions, or coding issues—these tasks require the model to fully engage its reasoning and planning abilities, often involving many internal steps before arriving at an answer. Examples include:
Explain in detail how different experts in a Mixture-of-Experts (MoE) model collaborate.
Based on the recent week’s fluctuations of the Shanghai Composite Index and current political information, should I invest in a stock index ETF? Why?

Samples Code
Basic Callcurl -X POST "https://api.z.ai/api/paas/v4/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "glm-4.5",
    "messages": [
      {
        "role": "user",
        "content": "As a marketing expert, please create an attractive slogan for my product."
      },
      {
        "role": "assistant",
        "content": "Sure, to craft a compelling slogan, please tell me more about your product."
      },
      {
        "role": "user",
        "content": "Z.AI Open Platform"
      }
    ],
    "thinking": {
      "type": "enabled"
    },
    "max_tokens": 4096,
    "temperature": 0.6
  }'
Streaming Callcurl -X POST "https://api.z.ai/api/paas/v4/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "glm-4.5",
    "messages": [
      {
        "role": "user",
        "content": "As a marketing expert, please create an attractive slogan for my product."
      },
      {
        "role": "assistant",
        "content": "Sure, to craft a compelling slogan, please tell me more about your product."
      },
      {
        "role": "user",
        "content": "Z.AI Open Platform"
      }
    ],
    "thinking": {
      "type": "enabled"
    },
    "stream": true,
    "max_tokens": 4096,
    "temperature": 0.6
  }'
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Universe Within 12.5 Light Years]]></title>
            <link>http://www.atlasoftheuniverse.com/12lys.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144337</guid>
            <description><![CDATA[This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.]]></description>
            <content:encoded><![CDATA[
About the Map
This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.

Epsilon Eridani is orbited by a large planet which might look like this.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kenvue stock drops on report RFK Jr will link autism to Tylenol during pregnancy]]></title>
            <link>https://www.cnbc.com/2025/09/05/rfk-tylenol-autism-kenvue-stock-for-url.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144123</guid>
            <description><![CDATA[HHS will release the report that could draw that link between the pain medication and autism this month, according to the Wall Street Journal.]]></description>
            <content:encoded><![CDATA[Kenvue Inc. Tylenol brand pain reliever for sale at a pharmacy in New York, US, on Wednesday, March 27, 2024. Bloomberg | Bloomberg | Getty ImagesShares of Kenvue fell more than 10% on Friday after a report that Health and Human Services Secretary Robert F. Kennedy Jr. will likely link autism to the use of the company's pain medication Tylenol in pregnant women. HHS will release the report that could draw that link this month, the Wall Street Journal reported on Friday.That report will also suggest a medicine derived from folate – a water-soluble vitamin – can be used to treat symptoms of the developmental disorder in some people, according to the Journal.In a statement, an HHS spokesperson said "We are using gold-standard science to get to the bottom of America's unprecedented rise in autism rates." "Until we release the final report, any claims about its contents are nothing more than speculation," they added. Tylenol could be the latest widely used and accepted treatment that Kennedy has undermined at the helm of HHS, which oversees federal health agencies that regulate drugs and other therapies. Kennedy has also taken steps to change vaccine policy in the U.S., and has amplified false claims about safe and effective shots that use mRNA technology.Kennedy has made the disorder a key focus of HHS, pledging in April that the agency will "know what has caused the autism epidemic" by September and eliminate exposures. He also said that month that the agency has launched a "massive testing and research effort" involving hundreds of scientists worldwide that will determine the cause.In a statement, Kenvue said it has "continuously evaluated the science and [continues] to believe there is no causal link" between the use of acetaminophen, the generic name for Tylenol, during pregnancy and autism.The company added that the Food and Drug Administration and leading medical organizations "agree on the safety" of the drug, its use during pregnancy and the information provided on the Tylenol label.The FDA website says the agency has not found "clear evidence" that appropriate use of acetaminophen during pregnancy causes "adverse pregnancy, birth, neurobehavioral, or developmental outcomes." But the FDA said it advises pregnant women to speak with their health-care providers before using over-the-counter drugs.The American College of Obstetricians and Gynecologists maintains that acetaminophen is safe during pregnancy when taken as directed and after consulting a health-care provider. Some previous studies have suggested the drug poses risks to fetal development, and some parents have brought lawsuits claiming that they gave birth to children with autism after using it.But a federal judge in Manhattan ruled in 2023 that some of those lawsuits lacked scientific evidence and later ended the litigation in 2024. Some research has also found no association between acetaminophen use and autism.In a note on Friday, BNP Paribas analyst Navann Ty said the firm believes the "hurdle to proving causation [between the drug and autism] is high, particularly given that the litigation previously concluded in Kenvue's favor."-- CNBC's Angelica Peebles contributed to this report.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anthropic agrees to pay $1.5B to settle lawsuit with book authors]]></title>
            <link>https://www.nytimes.com/2025/09/05/technology/anthropic-settlement-copyright-ai.html?unlocked_article_code=1.jk8.bTTt.Zir9wmtPaTp2&amp;smid=url-share</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45142885</guid>
        </item>
        <item>
            <title><![CDATA[Purposeful animations]]></title>
            <link>https://emilkowal.ski/ui/you-dont-need-animations</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139088</guid>
            <description><![CDATA[Why you are animating more often than you should.]]></description>
            <content:encoded><![CDATA[When done right, animations make an interface feel predictable, faster, and more enjoyable to use. They help you and your product stand out.
But they can also do the opposite. They can make an interface feel unpredictable, slow, and annoying. They can even make your users lose trust in your product.
So how do you know when and how to animate to improve the experience?
Step one is making sure your animations have a purpose.
Purposeful animations
Before you start animating, ask yourself: what’s the purpose of this animation? As an example, what’s the purpose of this marketing animation we built at Linear?

This animation explains how Product Intelligence (Linear’s feature) works. We could have used a static asset, but the animated version helps the user understand what this feature does, straight in the initial viewport of the page.
Another purposeful animation is this subtle scale down effect when pressing a button. It’s a small thing, but it helps the interface feel more alive and responsive.

Sonner’s enter animation, on the other hand, has two purposes:

- Having a toast suddenly appear would feel off, so we animate it in.
- Because it comes from and leaves in the same direction, it creates spatial consistency, making the swipe-down-to-dismiss gesture feel more intuitive.


But sometimes the purpose of an animation might just be to bring delight.
Morphing of the feedback component below helps make the experience more unique and memorable. This works as long as the user will rarely interact with it. It’ll then become a pleasant surprise, rather than a daily annoyance.
Press on the button to see it morph.
Used multiple times a day, this component would quickly become irritating. The initial delight would fade and the animation would slow users down.
How often users will see an animation is a key factor in deciding whether to animate or not. Let’s dive deeper into it next.
Frequency of use
I use Raycast hundreds of times a day. If it animated every time I opened it, it would be very annoying. But there’s no animation at all. That’s the optimal experience.
To see it for yourself, try to toggle the open state of the menu below by using the buttons belowpressing J and then K. Which one feels better if used hundreds of times a day?
Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommand
When I open Raycast, I have a clear goal in mind. I don’t expect to be “delighted”, I don’t need to be. I just want to do my work with no unnecessary friction.
Think about what the user wants to achieve and how often they will see an animation. A hover effect is nice, but if used multiple times a day, it would likely benefit the most from having no animation at all.
Imagine you interact with this list often during the day.
Imagine you interact with this list often during the day.The same goes for keyboard-initiated actions. These actions may be repeated hundreds of times a day, an animation would make them feel slow, delayed, and disconnected from the user’s actions. You should never animate them.
Since we can’t really use a keyboard on touch devices, you can press the buttons below to see how it feels with and without animation.
To see it for yourself, focus on the input below and use arrow keys to navigate through the list. Notice how the highlight feels delayed compared to the keys you press. Now press  (shift) and see how this interaction feels without animation.Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommandPress shift to toggle the animation
But even if your animation won’t be used too often and it fulfills a clear purpose, you still have to think about its speed…
Perception of speed
Unless you are working on marketing sites, your animations have to be fast. They improve the perceived performance of your app, stay connected to user’s actions, and make the interface feel as if it’s truly listening to the user.
To give you an example, a faster-spinning spinner makes the app seem to load faster, even though the load time is the same. This improves perceived performance.
Which one works harder to load the data?
A 180ms dropdown animation feels more responsive than a 400ms one:
Click on the buttons to compare the speed.
As a rule of thumb, UI animations should generally stay under 300ms.
Another example of the importance of speed: tooltips should have a slight delay before appearing to prevent accidental activation. Once a tooltip is open however, hovering over other tooltips should open them with no delay and no animation.
This feels faster without defeating the purpose of the initial delay.
Radix UI and Base UI skip the delay once a tooltip is shown.
Radix UI and Base UI skip the delay once a tooltip is shown.Building great interfaces
The goal is not to animate for animation’s sake, it’s to build great user interfaces. The ones that users will happily use, even on a daily basis. Sometimes this requires animations, but sometimes the best animation is no animation.
Knowing when to animate is just one of many things you need to know in order to craft great animations. If you’d like to dive deeper into the theory and practice of it, I’ve created a course that covers everything you need to know:
Check out "Animations on the Web"]]></content:encoded>
        </item>
    </channel>
</rss>