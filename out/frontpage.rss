<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 14 Sep 2025 06:40:39 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Refurb Weekend: Silicon Graphics IndigoÂ² Impact 10000]]></title>
            <link>http://oldvcr.blogspot.com/2025/09/refurb-weekend-silicon-graphics-indigo.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45237717</guid>
        </item>
        <item>
            <title><![CDATA[High Altitude Living â€“ 8,000 ft and above (2021)]]></title>
            <link>https://studioq.com/blog/2021/5/30/high-altitude-living-8000-ft-and-above-2450-meters</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45237184</guid>
            <description><![CDATA[Living at high altitude reduces risk of dying from heart disease: Low oxygen may spur genes to create blood vessels. Summary: Researchers have found that people living at higher altitudes have a lower chance of dying from heart disease and live longer.  Jeanne and I live at 8,400 feet (2.560 meters)]]></description>
            <content:encoded><![CDATA[
  Living at high altitude reduces risk of dying from heart disease: Low oxygen may spur genes to create blood vessels. Summary: Researchers have found that people living at higher altitudes have a lower chance of dying from heart disease and live longer.Jeanne and I live at 8,400 feet (2.560 meters). We were talking about visitors coming (for workshops, friends, etc.) and if you are coming from a low elevation what you need to be aware of. There is a thing called Acute Mountain Sickness (AMS). It's real and can be very disabling for some people.The best defense is to start at 5,000 feet and stay there for 3 days, drinking water like crazy and resting. Then, come up to the higher elevation and give yourself a day or two to adjust. That usually works.Beyond the AMS, there is the sun! The sun up here will fry you fast! The UV is very very high up here and we have 300 days of sun every year. It's difficult NOT to be out in it. Water, clothing that covers your arms, legs, etc., and a hat. This is the best way to protect yourself from burning up. You will dehydrate and get very sick.There's a lot to think about when we are considering having people visit.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why youâ€™d issue a branded stablecoin]]></title>
            <link>https://text-incubation.com/Why+you%27d+issue+a+branded+stablecoin+like+McDonaldsCoin</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45237035</guid>
        </item>
        <item>
            <title><![CDATA[Visual programming is stuck on the form]]></title>
            <link>https://interjectedfuture.com/visual-programming-is-stuck-on-the-form/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236774</guid>
            <description><![CDATA[Underlying great creations that you loveâ€”be it music, art, or technologyâ€”its form (what it looks like) is driven by an underpinning internal logic (how it works). I noticed this pattern while watching a talk on cellular automaton and realized it's "form follows function" paraphrased from a slightly different angle. Inventing a form is a hard task, so you must approach it obliquelyâ€”by first illuminating the underlying function.

This made me realize something crucial about visual programming: itâ€™]]></description>
            <content:encoded><![CDATA[
            Underlying great creations that you loveâ€”be it music, art, or technologyâ€”its form (what it looks like) is driven by an underpinning internal logic (how it works). I noticed this pattern while watching a talk on cellular automaton and realized it's "form follows function" paraphrased from a slightly different angle. Inventing a form is a hard task, so you must approach it obliquelyâ€”by first illuminating the underlying function.This made me realize something crucial about visual programming: itâ€™s stuck on form, rather than letting form follow function. Visual programming has long been trapped in the node-and-wires paradigm because its designers are overly fixated on form, neglecting the underlying function that should drive it. So as a whole, the field is stuck in a local minima. How can we break out of it and how can we find a function for the field that underpins the form?A clue from CellPondI was watching a talk and was struck not just by the presentation but also by a specific quote from Lu Wilson in a talk about CellPondâ€“a visual programming language that expanded my expectations for cellular automata. And that's given that I'd already seen my share of the Game of Life by John Conway and read lots of A New Kind of Science by Stephen Wolfram. But even though Lu Wilson spent the last 10 minutes showing you the fantastic visuals, none of that was the point. The actual tasty result is that there is a virtual machine with only four operations underlying the CellPond system. And these four operations correspond with memory operations we're familiar with in CPUs: read, write, allocate, and deallocate. To me, that connection was utterly surprising. The grid of patterns (form) was informed and driven by the underlying virtual machine (function)."I think if you were to learn from CellPond, you'd take away not just the UIâ€”but you can take the UI too if you want. I was very surprised by this because, in all my reading of past solutions to these problems, they were all about the high-level user interface; they were about the UI. I thought I'd have to build layers upon layers of UI, but really, as soon as the low-level stuff was sorted out, the UI just figured itself out."- Lu Wilson (ðŸ¤ Â· ðŸ¦‹)I wondered: how did Lu Wilson come up with the underlying function? It seemed magical. This puzzling revelation made me realize it wasnâ€™t just about the UIâ€”there was a deeper principle at play.Form follows functionIn the subsequent months, I kept turning it over in my head. The key lay with the opening quote. When you figure out the low-level stuff, the UI all falls into place.It wasn't until a drive while I was listening to Paul Graham's A Taste for Makers that I made the connection. The CellPond talk was a demonstration of the oft-repeated adage of "form follows function." Here's the relevant excerpt:In art, the highest place has traditionally been given to paintings of people. There is something to this tradition, and not just because pictures of faces get to press buttons in our brains that other pictures don't.  We are so good at looking at faces that we force anyone who draws them to work hard to satisfy us.  If you draw a tree and you change the angle of a branch five degrees, no one will know.  When you change the angle of someone's eye five degrees, people notice.When Bauhaus designers adopted Sullivan's "form follows function," what they meant was, form should follow function.  And if function is hard enough, form is forced to follow it, because there is no effort to spare for error.  Wild animals are beautiful because they have hard lives."- Paul Graham  A Taste for MakersHonestly, I had never thought much about "form follows function." It seems obvious enough when you hear it for the first time. Sure, given an interface, why else would it express anything other than its purpose? It would seem counterproductive otherwise. It wasn't until I was forced to invent a form did I really understood what it meant. The adage "form follows function" is for those tasked to invent the form, not for when you're given it. In my own words, it's this:If a design is any good, how something looks, feels, and works is a naked expression of its function, its algebra, its rationalityâ€“its underlying nature. To design a form, you should not just come up with it out of thin air. You have to attack the problem obliquely and work out its function first. Once the functionâ€“the underlying nature, internal consistency, and algebraâ€“is worked out, the form will fall out as a consequence of it.Three faces of functionWhat I mean by "underlying nature" isn't that it exists independently of human creation; rather, every design is embedded in an environment that shapes its intrinsic properties. The function of anything useful is always in the context of its environment. When we understand the context of a well-designed thing, we understand why it looks the way it does. An animal form reflects its adaptation to the ecological niche in its environment. By "rationality", I mean some kind of internal consistency. The function of something well-designed will have a certain repeated symmetry. Given a choice of design, it'll consistently use the same thing in as many scenarios as possible. Good game design enables a single item to serve multiple functions. The gravity gun in Half-Life 2 enables players to pick up and launch objects. It's used for turning environmental items into weapons, solving physics-based puzzles, and for navigating hard-to-reach areas. In Minecraft, the water bucket can extinguish fires, create waterfalls for safe descent, irrigate farmland, and serve as a barrier against certain enemies.By "algebra", I mean a set of rules about how a design's components compose. Most games have a physics engine that computes how objects in a game interact with each other in space. It's a "movement calculator." Legend of Zelda: Breath of the Wild additionally has a chemistry engine that it uses to compute how different materials interact with each other. It's a "state calculator." In summary, function represents the intangible structure governing the relationships, interactions, and contextual fit of a designâ€™s underlying components. A form can't exist outside of its function, and its function is shaped by its environment. We can observe and interact with the form directly, but not its function. We can exist in the environment, but the function is invisible to us without a lot of work to infer it.A form not informed by function feels disjointed, inconsistent, and frustrating. Without an underlying function to underpin the form, the shape of form is simply at the inconsistent whims of the designer. Functions keep designers honest about the purpose of form: in service of function. Of course you can explore and play with form independent of function, but that's the jurisdiction of art, not design. To invent a form, start with the function"Form follows function" is advice for people making something, especially those whose work has a very visible interface facing the end user. To invent a form, start with the function. But it's easy to make errors of two kinds, even if you already know this in your head.The first kind of error is to pursue form without considering function. Instead, you must ignore the form, at least initially, and focus on figuring out the function first. This is largely due to the intangible nature of function. It's an easy mistake to focus on form, even far into your creative career. This mistake is understandable. Whenever people interact with anything, their initial contact is the interfaceâ€”the bridge between user and design. For anyone new to something, it's natural to start by engaging with that interface, because it's what they're most familiar with. So when they turn around to make something in that domain, they start with the interface, the form. You can see this readily: new creatives in a field start by copying the masters before finding their own voice.It's also understandable because function is largely more abstract and more intangible than form. It's harder to get a grip on something amorphous, and you may have to start off with something concrete. It can be part of the process to draw up concrete examples first. In fact, when confronted with an unfamiliar domain, this can be quite productive in getting a handle on it. But it can be easy to forget and take a step back and ask: "what is the common underlying logic or abstraction to all these examples?" When you are able to take a step back, you're using the concrete examples as a stepping stone to figuring out the underlying function.The error of the second kind is pursuing function without considering the user. As a warning for those that lean too far on the other side of the precipice, this doesn't mean you can ignore the end user when figuring out the function. If we could represent the utility of the underlying function as a vector, it would still need to point in the direction of the user. The underlying function must support and give context to the visible form built on top. Both are built so the direction and magnitude of their utility vector can support the user in the direction of their goals. Too many back-end engineers misinterpret 'form follows function' as a license to design arbitrary database tables and APIs, assuming that the front end will compensate. That's how we get terrible interfaces where the end user needs to be aware of the data model to use it effectively, like Git.When it comes to visual programming, I think it's stuck in the error of the first kind, with its focus on form. Visual programming is not just node-and-wiresNode-and-wire diagrams have become a lazy default. Most visual language designers never ask whether those boxes and arrows genuinely help programmers. Itâ€™s a classic case of letting form precede function.When one looks through the Visual Programming Codex, it's obvious an overwhelming majority are based on the node-and-wires model. Not just that, but there are mostly only two variations:The nodes represent data, and the wires represent functionsThe nodes represent functions, and the wires represent data shunted between functions.Did many of them settle on it because it's the best visual representation to help aid the process of programming? Or did they use it because they're mimicking an existing form?I think node-and-wires is popular because visual programming designers make the fundamental assumption that the underlying nature and logic of programming is just traditional textual programming. If that's your assumption, then you'd naturally think all you have to do is find visual representations for existing textual language constructs. Hence node-and-wires is the form you get when you take pure functions as the underlying logic underpinning the form. On first glance, node-and-wires seem like a good fit. The wires going into a node are like the input parameters of a pure function, and the wires going out are like the output value. But what about differentiating between the definition of a function versus calling it? Often in node-and-wires visual languages, there's no separation. The definition is the application. What about passing around functions or thunks? Much of the power in pure functional programming lies in the power of higher-order functions, and I haven't seen very good node-and-wires representation of that. After decades of trying, most pure functional programming is still largely expressed in text. To me, that's damning evidence against the practice of using nodes-and-wires to model functions. Text is still the better form for expressing the underlying logic of functional programming. 



Imperative programming with node-and-wires fares no better. A loop in LabVIEW gives no more advantage or clarity over writing it in text. Seeing the totality of a sequence of steps in parallel in a circuit-like diagram doesn't solve the fundamental problem with imperative programs; it doesn't help the developer understand combinatorial state explosions or state changes over time.I think where node-and-wires have provided the biggest advantage is in specific domains in which a) there's massive value to examine intermediate data and values between transformations and b) there's a well-known visual representation of that intermediate data and value. This has been demonstrated in visual languages like Unreal Engine's Blueprint for game programming shaders and Max/MSP for sound synthesis in music. But these have been limited to these narrow domains. Visual programming has not found a foothold in general purpose programming domains.Modeling problemsWhat then, if not node-and-wires? The aim here is to uncover an alternative underlying logicâ€”one that can more effectively drive the form in visual programming. How would you go about finding another underlying function in "form follows function" if not the current programming paradigms we know? I think this is the wrong question. Although correct in direction and spirit, I think a better question is: how should we model problems that can leverage the computational power of our visual cortex? We write programs primarily to model and solve real-world problems. We go through the exercise of encoding the problem model in programming languages, because we can automate the generation of solutions. And the reason why we keep banging on the visual programming door is because we understand intuitively that our visual cortex is an under-leveraged power tool.The human visual cortex is a powerful pattern recognition apparatus. It can quickly compare lengths, distinguish foreground from background, recognize spatial patterns, and other amazing feats of perception, all at a glance. We leverage it in data visualizations to make sense of large quantities of data, but we haven't been able to leverage it to make sense of computational systems. â“"Imagine what a visual programming language would look like if it was able to leverage the power of the human visual cortex" For the record, I don't think this is it.If we had a visual programming language that could leverage the human visual cortex, then at any zoom-level of abstraction, at a glance we could understand the overall structure of the program as it relates to the domain at that level of abstraction. And if we were looking at a running program, then we could get an idea of the overall state and process. Yes, we have bespoke visualizations of running programs in the form of metrics and dashboards. But we don't have a universal visual language to represent the structure or state of a program that applies to different programs. What about text? Aren't textual glyphs a kind of visual language? Not in the way I mean. For text to be considered a visual programming language, it'd have to leverage the human visual cortex at different zoom-levels of the program. Certainly, with syntax highlighting we leverage the visual cortex and use color to distinguish between different syntactical elements. This counts. But we only get this at the level of a function. It doesn't apply when we zoom out to the overall structure of the code base. And there's certainly no zoom-out level in which we get visual understanding at the level of the problem domain. The closest thing I can think of that might fit the bill is APL and its ilk. By condensing operators into single characters, sequences form idioms. Just as we recognize whole words rather than individual letters, idioms allow us to comprehend entire operations without parsing each symbol. So as you zoom out of the code, you can see the meaning of the code by identifying common idioms. Strangely, it seems many APL environments don't feature syntax highlighting.So if visual programming is to be useful, I think the angle of attack is to find a way to model problems, and this might not be the same way that we model problems in textual languagesâ€“even if the underpinning implementation is all lambdas and Turing machines. So how do we model problems?Entities and relationshipsI'll say up front, I don't know what modeling problems should look like. Nonetheless, it seems there are two main aspects for any system we're interested in: visually representing the entities in a problem domain visually representing the entity relationships.[2]Regardless of the paradigm, imperative, object-oriented, functional, or logical, there are both "entities" (structs, objects, compound values, terms) and "how they relate" (imperative processes, messages, functions, rules and predicates). If I had to take a stab at it, I'd start here.Of the two, representing the different entities in a problem domain seems more amenable to visual programming because they're nouns. Most of the things we see around us are nouns. Hence, we can imagine that inert data representing entities would have a canonical visual representation. But even then, entities often have far more attributes than we might want to visualize at a time to understand its purpose and behavior. How do we choose what attribute is important to show? And what should be the visual form for the attribute in these entities?The two questions are related, but to drive the point home, I'll focus on the second one. If we have some struct with two attributes in some generic language, how would we visually represent them?struct Foo {
  bar: float,
  baz: float
}We might think a universally useful representation of a collection of these instances is two histograms: one for bar and one for baz. For any given instance, its corresponding value could be highlighted on the histogram.Is this useful? Answer depends on our task at hand. There's no one-size-fits-all visualization of entities. What if I told you bar is an x-coordinate and baz is the y-coordinate? Now, perhaps a visualization that's more fitting is a scatterplot where each instance is represented as an x. We put the relationship between bar and baz in a spatial relationship to see if our visual cortex could recognize a pattern.In the histogram visualization, I wouldn't be able to use my visual cortex to discern the relationships between bar and baz traces out a flower. However, in the spatial canvas visualization, I could easily see the flower trace because by pitting bar and baz in a spatial relationship, I'm creating a mapping that makes an affordance for my visual cortex.This only worked because there was a spatial relationship between bar and baz, especially if I know they represent x and y coordinates. We couldn't just look at the data and easily discern what visualization to use. The label and the intention of the user also give meaning to what visualization is best suited for an entity. Hence, I think there's no one-size-fits-all visualization for entities. There's no single mapping of attributes to visualizations that makes sense, unless the user's intention and goals remain fixed. Besides entities, every program encodes relationships between its entities. How do we visually represent their relationships in a way that's illuminating at a glance without devolving into an illegible spaghetti mess? Relationships can be harder to model, because they're typically invisible to us, as they're often inferred.Like the example with representing entities visually, representing relationships visually is likely to depend on both the goals of the user as well as the meaning of the entities at hand. I suspect a good visual representation of the relationship between two tables in a query is going to be different than a good visual representation of the relationship between two pieces of middleware in a web stack. However, I do think we can do better than a line.The go-to representation of a relationship is often the line or an arrow, where it connects two things on the canvas together. The trouble with lines is that they doesn't scale with the visual cortex. After a couple dozen lines, we lose track of any sense of the overall relationships between entities. But I don't think this can be the only way. The visual cortex also relates visual elements if they have the same color or if they're spatially clustered together. As the previous example on a plot of bar and baz showed, relationships could be spatial, by which we can plot them spatially to reveal relationships, without directly drawing lines and arrows everywhere.As before, it's hard to draw any generally productive conclusions on how to best visually represent relationships between entities without knowing the goal of the user as well as the meaning behind the entity and relationships we're trying to represent. The only point I'm trying to drive home is that we have more tools at our disposal besides lines and arrows, because the visual cortex is perceptive and discerning about colors, groupings, and motion. We typically use these visual elements haphazardly, if at all, rather than as a deliberate attempt to leverage it for understanding. And that's just in graphic design and data visualization. It's completely overlooked in program structure, debugging, and domain problem modeling.At this point, those that hear entities and relationships might be drawn to ask, isn't this just object-oriented programming? It is true that object-oriented thinking trains you to identify entities in the problem domain and model their relationships through method calls and messaging. However, object-oriented programs suffer from private state whose effects are observable from the outside littered everywhere, making it hard to reason about program behavior. What I'm saying is orthogonal to and doesn't invalidate what we've learned about structuring programs in the past 3 decades. To sum up, I'm saying the unit of representation for visually representing programs may not be the function and its input and output parameters, as node-and-wire visual programmers are likely to do. It might be something else, which can leverage the power of the visual cortex.Computation is figuring out the next stateModeling problems as entities and their relationships is only half the equation. By only modeling entities and their relationships, we've only described a static world. We can do that already without computers; it's commonly done on whiteboards in tech companies around the world. Every time we go up to the whiteboard with a coworker to talk through a problem, we're trying to leverage the power of our visual cortex to help us reason through it. But unlike our textual programs, whiteboards aren't computational. If whiteboards were computational, they might show how the state of the problem changes over time, or how it changes in response to different external inputs or effects. Thus, the question is, how do we visually represent how the system state should evolve over time or in response to external inputs? [1]Cellular automaton systems typically express computation through rulesets. Rulesets are typically expressed as a pure functional transformation between the current state and the next state. Taking rule 110 in 1D cellular automaton as an example, the state of the next cell depends on the three cells above it. Given the three cell pattern above, this is what the cell in the next line should be. You can see this like a Î²-reduction, substituting symbols with other symbols until we can substitute no further, with the resulting value as our answer.The famous rule 110 in 1D cellular automaton. This rule is Turing Complete!As the CellPond talk at the top of the page points out, rulesets for more complicated behaviors, like trains on tracks have a combinatorial explosion of rules. One of CellPond's innovations was to have rulesets that represent (or generates?) groups of rulesets, so that visually expressing the rulesets remains tractable for humans.But pure functions are just mappings. Any pure function can be replaced by an equivalent infinite table of key-value pairs. Rulesets are just explicit mappings of inputs to outputs. Hence, if rulesets are to be tractable, we must be able to express not just how a single current state maps to the next state, but how entire groups of states map to a next state.We have familiar mechanisms in textual programming to express a selection of groups of input states in a succinct way. We have boolean logic in if expressions. We have maps and filters. We have select and where clauses in SQL queries. But we have no universal and composable ways of expressing this selection of previous states and mapping them to next states. Additionally, we don't have universally recognized ways of expressing this mapping from groups of inputs to outputs for state types other than a grid of cells. A different way forwardCertainly, it could be possible that multi-dimensional aspects of a codebase would be quite hard to represent in its entirety visually. But I don't think it's a stretch to say that we lean pretty hard on the symbolic reasoning parts of our brain for programming and the visual reasoning parts of our brain are underleveraged.Visual programming hasn't been very successful because it doesn't help developers with any of the actual problems they have when building complex systems. I think this is a result of ignoring the adage "form follows function" and trying to grow a form out of traditional programming paradigms that fail to provide good affordancesâ€“the utility vector is pointing the wrong wayâ€“for those actual problems in complex systems. To make headway, I think we should focus on discovering underlying logic and function of how to model problems visually on a canvasâ€“not just the entities, but also their relationships. In addition to modeling problems, we also have to discover how to model transformations and transitions of state, so our models are also computational. We have the hardware: our visual cortex is a powerhouse for pattern recognition and spatial reasoning. We just donâ€™t have the right computational grammar to feed it. If we want a visual programming breakthrough, we have to leave the legacy of text-based paradigms behind and unearth a new kind of functionâ€”one that only makes sense visually. Once we do, the right â€˜formâ€™ will follow so obviously, weâ€™ll wonder why we waited so long.[1] One way is with visual rule sets. This almost feels like declarative or logic programming. But as the Cell Pond talk at the top of the essay pointed out, unless you have a representation of rule sets that can be expanded, you suffer combinatorial explosion.[2] Depending on who you are, this can sound either like object-oriented programming or category theory. 
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How the restoration of ancient Babylon is drawing tourists back to Iraq]]></title>
            <link>https://www.theartnewspaper.com/2025/09/12/how-the-restoration-of-ancient-babylon-is-helping-to-draw-tourists-back-to-iraq</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236473</guid>
            <description><![CDATA[Work on the Temple of Ninmakh and walls at the Ishtar Gate is nearing completion at the Mesopotamian metropolis, a victim of centuries of damage and neglect]]></description>
            <content:encoded><![CDATA[Mentioned in the sacred texts of all three Abrahamic faiths, the ancient Mesopotamian city of Babylon, in modern-day Iraq, is today undergoing a revival. Two World Monuments Fund (WMF) projects are nearing completion and much-needed cultural tourism is returning.One project mitigates groundwater damage to the north retaining wall of the Ishtar Gate. The second is a restoration of the Temple of Ninmakh, dedicated to the Sumerian mother goddess. The team hopes there will be an official reopening for the temple this autumn, after which it will be available for gatherings such as weddings and concerts, as well as for the Babylon Festival, a celebration of international cultures that takes place every spring.Largely funded by the US embassy in Baghdad, the restoration of the temple and the north retaining wall are part of the Future of Babylon Project, initiated 15 years ago, which aims to document, waterproof and stabilise structures throughout the 2,500-acre site. (The US embassy cancelled funding for a planned walkway spanning the site of the Ishtar Gate in July due to budget cuts.)Visitor boomThe completion of these two projects coincides with a boom in tourism. Even in the midday heat, when tour guides refuse to emerge from their office, visitors from Romania, Russia and Iran enthusiastically explore attractions including the largely intact Lion of Babylon, the processional way and the museum next to a reconstructed Ishtar Gate.The return of heritage tourism is one of Iraqâ€™s few recent success stories. Even as sectarian tensions simmer and the electrical grid has yet to be restored 22 years after it was destroyed in the US invasion, Babylon is being reborn.â€œWeâ€™ve had record numbers of visitors this year,â€ Raad Hamid Abdullah, Babylonâ€™s antiquities and heritage inspector, tells The Art Newspaper. In 2024 Babylon hosted 43,530 Iraqi tourists and 5,370 foreign tourists, an increase from 36,957 Iraqi visitors and 4,109 foreigners in 2023, he says.â€œNow even locals from the adjoining city of Babil are coming,â€ Abdullah says. â€œIt has once more become a popular place for family gatherings and wedding parties,â€ he says, adding proudly, â€œBabylon is a symbol of Iraq.â€Babylon, the survivorAround 80km south of Baghdad, comprising both the ruins of the ancient city as well as surrounding villages and agricultural areas, Babylon is a survivor. From its peak as the Neo-Babylonian capital under King Nebuchadnezzar II through to the Iraq War, when American and Polish troops ran roughshod over its ruins and a decade later, Islamic State (Isis) threatened its very existence, the ancient city has witnessed empires come and go.Babylon has survived decades of looting and ongoing environmental challenges. Construction, too, has taken a toll over the years. In 1927 the British ran a railway line through the site, and in the 1980s Saddam Hussein built a highway through part of it, along with a palace for himself, complete with helipad. There are still three non-functioning oil pipelines, two built in the 1970s and 1980s and a more recent third oneâ€”work on it was blocked after Iraqâ€™s General Authority for Antiquities and Heritage filed a lawsuit in 2012. Babylon was only recognised as a Unesco World Heritage Site in 2019.Now the Egyptian architect Ahmed Abdelgawad, an expert in mud brick buildings, is working with the WMF to train locals in the traditional art that befits the Temple of Ninmakh, named after the mother goddess associated with creation, birth and healing who breathed life into humankind via small clay figures in their likeness.Years of war-related damage and neglect combined with poorly executed mid-century â€œreconstructionâ€ methods resulted in serious structural problems at the temple. Corrosion caused by the intrusion of increasingly salty groundwater is the product of prolonged droughts and soil erosion in climate-vulnerable Iraq.Traditional mud-brick techniquesThe archway at the entrance of Ninmakhâ€™s inner sanctumâ€”on the verge of collapse in 2022â€”was successfully restored at the end of May. â€œWe had to totally dismantle the old arch,â€ Abdelgawad says. â€œIt was full of cracks and worn by weather. So we took it apart and rebuilt it with mud bricks.â€The traditional art of making special low-salt mud brick begins with sourcing soil with low salt levels, which is then mixed with sand, grit and straw.â€œThis is the first arch in Iraq restored totally from mud bricks,â€ says Osama Hisham, the Future of Babylon project manager.A similar but saltier mix of mud brick and bitumen was used to repair the wooden roof of the temple, which was being eroded by termites.Hisham says the temple now comprises poplar timber from the forests of Mosul in northern Iraq, mud from Babylon and reeds from the marshes in the south. A place that has symbolised the heart of Iraq has now been restored with materials from across the nation.Groundwater zappingMeanwhile, the north retaining walls at the Ishtar Gate, reconstructed in the past century with cement that damaged the remains of the historical monument, were demolished and replaced with new retaining walls providing better water management. These new wallsâ€”essentially boxes filled with stones, based on an ancient Egyptian construction technique, Hisham saysâ€”absorb sunlight from the southern side and effectively vaporise groundwater coming from the northern side.The Babylonians, he says, dealt with groundwater intrusion by creating an elevation byâ€œcutting the arch of the gate and burying it, then using it as a foundation for a new gateâ€. As a result of this technique, the Ishtar Gate built by Nebuchadnezzar II, where the WMF is currently finishing work on the north retaining wall, is seven metres below the ancient city, with only two metres remaining above.DisintegrationA subsequent spectacular blue-glazed gate Nebuchadnezzar II built on top of that gate gradually disintegrated in the aftermath of the fall of the Babylonian Empire in the sixth century BC. A replica installed in the 1950s now greets visitors to Babylon.Many Iraqis would like to see the reconstruction of the Ishtar Gate returned from the Pergamon Museum in Berlin. The gate is made of brick fragments from excavations carried out by the Deutsche Orient-Gesellschaft (German Oriental Society) from 1899 to 1917.But Hisham says that even the Ishtar Gate in Berlin is only 20% original. The gate in Babylon, he points out, is 80% original.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[If my kids excel, will they move away?]]></title>
            <link>https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236411</guid>
            <description><![CDATA[If my kids excel, will they move away?]]></description>
            <content:encoded><![CDATA[If my kids excel, will they move away?Jeffrey P. BighamI grew up on a farm outside of a rural town about an hour southeast of Columbus, Ohio. Like many small towns in America, my town knows â€œbrain drainâ€ â€“ all of my friends from high school who went to college (~30% of my class) now live elsewhere, although most are pretty close by (e.g., several live in the suburbs of Columbus and Cincinnati).Sometimes my hometown feels a million miles away, but it only takes two hours and fifty minutes for me to drive there from Pittsburgh, which is where I live now.In Pittsburgh, Iâ€™m a professor at Carnegie Mellon University in the top computer science school in the world. Iâ€™ve also worked in various large technology companies, who have offices in Pittsburgh to connect with and employ Carnegie Mellon faculty and students.I may not live in my small town anymore, but the fact that the best place in the world to study and do research in computer science is in Pittsburgh means Iâ€™m really not that far away. My four kids see their grandparents often, theyâ€™re known in my parentsâ€™ church and have spent a lot of time on my dadâ€™s farm. The photo above is of me on the farm, wearing some of my dadâ€™s clothes, trying to help out when my dad fell ill a few years ago.Most of my story weâ€™ve been able to take for granted in the United States for the past few decades. If you grow up in the United States, and youâ€™re among the best in the world in your field, you could count on the center of excellence for your field also being in the United States, oftentimes pretty close by, like Pittsburgh being close to my hometown.As a professor, Iâ€™m able to recruit the very best students in the world to work on my research. Sometimes that means recruiting Americans and sometimes that means recruiting from elsewhere.  Students come to Pittsburgh from around the world (Iâ€™ve advised PhD students and postdocs from about 10 different countries). Five or six years after they start our intensive graduation program, successful students receive their PhDs and thatâ€™s when I tend to meet their parents for the first time. Oftentimes, this is the first trip theyâ€™ve made to the United States, and they may have only seen their kids a few times during their degree. It hits home because usually these students choose to stay in the United States â€“ after successfully completing their degree with me, they are in high demand not only in our universities but also in technology companies.


These days the students I talk to are less confident about coming to the United States to study and less confident about staying here after theyâ€™re done. They have seen a student grabbed off the street apparently because she wrote an essay expressing concern about the on-going humanitarian crisis in Gaza. They have seen graduate students jailed for what used to be minor immigration offenses. They have seen even greater uncertainty in applying or reapplying for the visas they need to study. And, they have seen their status as students arbitrarily used as leverage in attacking premier universities like Harvard[1]. Most of these incidents have or probably will be resolved, but the message and fear it causes are real and long-lasting.I am worried that policies that have the intention (or effect) of introducing chaos and cruelty to superstar students will make it less likely for the best of the best to come to America, and this in turn will mean centers of excellence will move elsewhere. While incumbents have an advantage, it doesnâ€™t take much to influence group behavior and movements can be self-reinforcing. The best people in a field like to be where other amazing people are, so they can learn and build off of each other. If the centers of excellence move elsewhere, Iâ€™m worried my kids will end up feeling compelled to move away (should they become superstars, as is my hope for them).The brain drain from our small rural communities is real, but many of us have found ways to stay close by and keep those ties. Thereâ€™s a bunch of reasons to treat international students better than we have over the past months, but these concerns are not thousands of miles away as they seem to some â€“ to me, it's incredibly close to home, and not only because I see the effect on students I work with closely.If we cause centers of excellence to move away from Pittsburgh, and away from the United States entirely, thatâ€™s the difference between my grandkids living near or very far, and whether theyâ€™re likely to grow up visiting me and my dadâ€™s farm often or hardly at all.[1]Â Iâ€™ve owned exactly two Harvard t-shirts in my lifetime â€“ the first when I was an undergrad at Princeton said, â€˜Harvard Sucksâ€™, and the second is a normal Harvard t-shirt that I bought this past May.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EFF to court: The Supreme Court must rein in secondary copyright liability]]></title>
            <link>https://www.eff.org/deeplinks/2025/09/eff-court-supreme-court-must-rein-expansive-secondary-copyright-liability</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236314</guid>
            <description><![CDATA[If the Supreme Court doesnâ€™t reverse a lower courtâ€™s ruling, internet service providers (ISPs) could be forced to terminate peopleâ€™s internet access based on nothing more than mere accusations of copyright infringement. This would threaten innocent users who rely on broadband for essential aspects...]]></description>
            <content:encoded><![CDATA[
            
  
  
  If the Supreme Court doesnâ€™t reverse a lower courtâ€™s ruling, internet service providers (ISPs) could be forced to terminate peopleâ€™s internet access based on nothing more than mere accusations of copyright infringement. This would threaten innocent users who rely on broadband for essential aspects of daily life. EFFâ€”along with the American Library Association, the Association of Research Libraries, and Re:Createâ€”filed anÂ amicus brief urging the Court to reverse the decision.
The Stakes: Turning ISPs into Copyright Police
Among other things, the Supreme Court approving the appeals courtâ€™s findings will radically change the amount of risk your ISP takes on if a customer infringes on copyright, forcing the ISP to terminate access to the internet for those users accused of copyright infringementâ€”and everyone else who uses that internet connection.
This issue turns on what courts call â€œsecondary liability,â€ which is the legal idea that someone can be held responsible not for what they did directly, but for what someone else did using their product or service.
The case began when music companies sued Cox Communications, arguing that the ISP should be held liable for copyright infringement committed by some of its subscribers. The Court of Appeals for the Fourth Circuit agreed, adopting a â€œmaterial contributionâ€ standard for contributory copyright liability (a rule for when service providers can be held liable for the actions of users). The lower court said that providing a service that could be used for infringement is enough to create liability when a customer infringes.
In the Patent Act, where Congress has explicitly defined secondary liability, thereâ€™s a different test: contributory infringement exists only where a product is incapable of substantial non-infringing use. Internet access, of course, is overwhelmingly used for lawful purposes, making it the very definition of a â€œstaple article of commerceâ€ that canâ€™t be liable under the patent framework. Yet under the Fourth Circuitâ€™s rule, ISPs could face billion-dollar damages if they fail to terminate users on the basis of even flimsy or automated infringement claims.
Our Argument: Apply Clear Rules from the Patent Act, Not Confusing Judge-Made Tests
Our brief urges the Court to do what it has done in the past: look to patent law to define the limits of secondary liability in copyright. That means contributory infringement must require more than a â€œmaterial contributionâ€ by the service providerâ€”it should apply only when a product or service is especially designed for infringement and lacks substantial non-infringing uses.
The Human Cost: Losing Internet Access Hurts Everyone 
The Fourth Circuitâ€™s rule threatens devastating consequences for the public. Terminating an ISP account doesnâ€™t just affect a person accused of unauthorized file sharingâ€”it cuts off entire households, schools, libraries, or businesses that share an internet connection.

Public libraries, which provide internet access to millions of Americans who lack it at home, could lose essential service.
Universities, hospitals, and local governments could see internet access for whole communities disrupted.
Householdsâ€”especially in low-income and communities of color, which disproportionately share broadband connections with other peopleâ€”would face collective punishment for the alleged actions of a single user.

With more than a third of Americans having only one or no broadband provider, many users would have no way to reconnect once cut off. And given how essential internet access is for education, employment, healthcare, and civic participation, the consequences of termination are severe and disproportionate.
Whatâ€™s Next
The Supreme Court has an opportunity to correct course. Weâ€™re asking the Court to reject the Fourth Circuitâ€™s unfounded â€œmaterial contributionâ€ test, reaffirm that patent law provides the right framework for secondary liability, and make clear that the Constitution requires copyright to serve the public good. The Court should ensure that copyright enforcement doesnâ€™t jeopardize the internet access on which participation in modern life depends.
Weâ€™ll be watching closely as the Court considers this case. In the meantime, you can read our amicus brief here.



          
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Two Slice, a font that's only 2px tall]]></title>
            <link>https://joefatula.com/twoslice.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236263</guid>
            <description><![CDATA[A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.]]></description>
            <content:encoded><![CDATA[
		
		A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.
		You can probably read this, even if you wish you couldn't.It tends to be easier to read at smaller sizes.
		Try it out below, or download it (under CC BY-SA license, so you can use it commercially but you have to give credit).
		
	
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pass: Unix Password Manager]]></title>
            <link>https://www.passwordstore.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236079</guid>
            <description><![CDATA[Pass is the standard unix password manager, a lightweight password manager that uses GPG and Git for Linux, BSD, and Mac OS X.]]></description>
            <content:encoded><![CDATA[

Introducing pass

Password management should be simple and follow Unix philosophy. With pass, each password lives inside of a gpg encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.

pass makes managing these individual password files extremely easy. All passwords live in ~/.password-store, and pass provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using git.

You can edit the password store using ordinary unix shell commands alongside the pass command. There are no funky file formats or new paradigms to learn. There is bash completion so that you can simply hit tab to fill in names and commands, as well as completion for zsh and fish available in the completion folder. The very active community has produced many impressive clients and GUIs for other platforms as well as extensions for pass itself.

The pass command is extensively documented in its man page.



Using the password store

We can list all the existing passwords in the store:

zx2c4@laptop ~ $ pass
Password Store
â”œâ”€â”€ Business
â”‚   â”œâ”€â”€ some-silly-business-site.com
â”‚   â””â”€â”€ another-business-site.net
â”œâ”€â”€ Email
â”‚   â”œâ”€â”€ donenfeld.com
â”‚   â””â”€â”€ zx2c4.com
â””â”€â”€ France
    â”œâ”€â”€ bank
    â”œâ”€â”€ freebox
    â””â”€â”€ mobilephone


And we can show passwords too:

zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3


Or copy them to the clipboard:

zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.


There will be a nice password input dialog using the standard gpg-agent (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.

We can add existing passwords to the store with insert:

zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do


This also handles multiline passwords or other data with --multiline or -m, and passwords can be edited in your default text editor using pass edit pass-name.

The utility can generate new passwords using /dev/urandom internally:

zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&Q=IN2nFBx


It's possible to generate passwords with no symbols using --no-symbols or -n, and we can copy it to the clipboard instead of displaying it at the console using --clip or -c.

And of course, passwords can be removed:

zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file â€˜/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ€™? y
removed â€˜/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpgâ€™


If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using pass git push and pass git pull, which call git-push or git-pull on the store.

You can read more examples and more features in the man page.

Setting it up

To begin, there is a single command to initialize the password store:

zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory â€˜/home/zx2c4/.password-storeâ€™
Password store initialized for ZX2C4 Password Storage Key.


Here, ZX2C4 Password Storage Key is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using -p.

We can additionally initialize the password store as a git repository:

zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store


If a git repository is initialized, pass creates a git commit each time the password store is manipulated.

There is a more detailed initialization example in the man page.

Download

The latest version is 1.7.4.

Ubuntu / Debian

$ sudo apt-get install pass

Fedora / RHEL

$ sudo yum install pass

openSUSE

$ sudo zypper in password-store

Gentoo

# emerge -av pass

Arch

$ pacman -S pass

Macintosh

The password store is available through the Homebrew package manager:

$ brew install pass

FreeBSD

# pkg install password-store

Tarball


Version 1.7.4
Latest Git

The tarball contains a generic makefile, for which a simple sudo make install should do the trick.

Git Repository

You may browse the git repository or clone the repo:


$ git clone https://git.zx2c4.com/password-store

All releases are tagged, and the tags are signed with 0xA5DE03AE.

Data Organization

Usernames, Passwords, PINs, Websites, Metadata, et cetera

The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.

One approach is to use the multi-line functionality of pass (--multiline or -m in insert), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, Amazon/bookreader might look like this:

Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719

This is the preferred organzational scheme used by the author. The --clip / -c options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.

Another approach is to use folders, and store each piece of data inside a file in that folder. For example Amazon/bookreader/password would hold bookreader's password inside the Amazon/bookreader directory, and Amazon/bookreader/secretquestion1 would hold a secret question, and Amazon/bookreader/sensitivecode would hold something else related to bookreader's account. And yet another approach might be to store the password in Amazon/bookreader and the additional data in Amazon/bookreader.meta. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.

The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.

Extensions for pass
In order to faciliate the large variety of uses users come up with, pass supports extensions. Extensions installed to /usr/lib/password-store/extensions (or some distro-specific variety of such) are always enabled. Extensions installed to ~/.password-store/.extensions/COMMAND.bash are enabled if the PASSWORD_STORE_ENABLE_EXTENSIONS environment variable is true Read the man page for more details.

The community has produced many such extensions:

	pass-tomb: manage your password store in a Tomb
	pass-update: an easy flow for updating passwords
	pass-import: a generic importer tool from other password managers
	pass-extension-tail: a way of printing only the tail of a file
	pass-extension-wclip: a plugin to use wclip on Windows
	pass-otp: support for one-time-password (OTP) tokens


Compatible Clients
The community has assembled an impressive list of clients and GUIs for various platforms:


	passmenu: an extremely useful and awesome dmenu script
	qtpass: cross-platform GUI client
	Android-Password-Store: Android app
	passforios: iOS app
	pass-ios: (older) iOS app
	passff: Firefox plugin
	browserpass: Chrome plugin
	Pass4Win: Windows client
	pext_module_pass: module for Pext
	gopass: Go GUI app
	upass: interactive console UI
	alfred-pass: Alfred integration
	pass-alfred: Alfred integration
	simple-pass-alfred: Alfred integration
	pass.applescript: OS X integration
	pass-git-helper: git credential integration
	password-store.el: an emacs package
	XMonad.Prompt.Pass: prompt for Xmonad


Migrating to pass
To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:


	1password2pass.rb: imports 1Password txt or 1pif data
	keepassx2pass.py: imports KeepassX XML data
	keepass2csv2pass.py: imports Keepass2 CSV data
	keepass2pass.py: imports Keepass2 XML data
	fpm2pass.pl: imports Figaro's Password Manager XML data
	lastpass2pass.rb: imports Lastpass CSV data
	kedpm2pass.py: imports Ked Password Manager data
	revelation2pass.py: imports Revelation Password Manager data
	gorilla2pass.rb: imports Password Gorilla data
	pwsafe2pass.sh: imports PWSafe data
	kwallet2pass.py: imports KWallet data
	roboform2pass.rb: imports Roboform data
	password-exporter2pass.py: imports password-exporter data
	pwsafe2pass.py: imports pwsafe data
	firefox_decrypt: full blown Firefox password interface, which supports exporting to pass


Credit & License

pass was written by Jason A. Donenfeld of zx2c4.com and is licensed under the GPLv2+.

Contributing

This is a very active project with a healthy dose of contributors. The best way to contribute to the password store is to join the mailing list and send git formatted patches. You may also join the discussion in #pass on Libera.Chat.


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Will AI be the basis of many future industrial fortunes, or a net loser?]]></title>
            <link>https://joincolossus.com/article/ai-will-not-make-you-rich/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235676</guid>
            <description><![CDATA[The disruption is real. It's also predictable.]]></description>
            <content:encoded><![CDATA[
                        
Fortunes are made by entrepreneurs and investors when revolutionary technologies enable waves of innovative, investable companies. Think of the railroad, the Bessemer process, electric power, the internal combustion engine, or the microprocessorâ€”each of which, like a stray spark in a fireworks factory, set off decades of follow-on innovations, permeated every part of society, and catapulted a new set of inventors and investors into power, influence, and wealth.



Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queenâ€™s race, and inventors and investors were left no better off for non-stop running.



Anyone who invests in the new new thing must answer two questions: First, how much value will this innovation create? And second, who will capture it? Information and communication technology (ICT) was a revolution whose value was captured by startups and led to thousands of newly rich founders, employees, and investors. In contrast, shipping containerization was a revolution whose value was spread so thin that in the end, it made only a single founder temporarily rich and only a single investor a little bit richer.



Is generative AI more like the former or the latter? Will it be the basis of many future industrial fortunes, or a net loser for the investment community as a whole, with a few zero-sum winners here and there?



There are ways to make money investing in the fruits of AI, but they will depend on assuming the latterâ€”that it is once again a less propitious time for inventors and investors, that AI model builders and application companies will eventually compete each other into an oligopoly, and that the gains from AI will accrue not to its builders but to customers. A lot of the money pouring into AI is therefore being invested in the wrong places, and aside from a couple of lucky early investors, those who make money will be the ones with the foresight to get out early.



    




The microprocessor was revolutionary, but the people who invented it at Intel in 1971 did not see it that wayâ€”they just wanted to avoid designing desktop calculator chipsets from scratch every time. But outsiders realized they could use the microprocessor to build their own personal computers, and enthusiasts did. Thousands of tinkerers found configurations and uses that Intel never dreamed of. This distributed and permissionless invention kicked off a â€œgreat surge of development,â€ as the economist Carlota Perez calls it, triggered by technology but driven by economic and societal forces.[1]



There was no real demand for personal computers in the early 1970s; they were expensive toys. But the experimenters laid the technical groundwork and built a community. Then, around 1975, a step-change in the cost of microprocessors made the personal computer market viable. The Intel 8080 had an initial list price of $360 ($2,300 in todayâ€™s dollars). MITS could barely turn a profit on its Altair at a bulk price of $75 each ($490 today). But when MOS Technologies started selling its 6502 for $25 ($150 today), Steve Wozniak could afford to build a prototype Apple. The 6502 and the similarly priced Zilog Z80 forced Intelâ€™s prices down. The nascent PC community started spawning entrepreneurs and a score of companies appeared, each with a slightly different product.



You couldnâ€™t have known in the mid-1970s that the PC (and PC-like products, such as ATMs, POS terminals, smartphones, etc.) would revolutionize everything. While Steve Jobs was telling investors that every household would someday have a personal computer (a wild underestimate, as it turned out), others questioned the need for personal computers at all. As late as 1979, Appleâ€™s ads didnâ€™t tell you what a personal computer could doâ€”it asked what you did with it.[2] The established computer manufacturers (IBM, HP, DEC) had no interest in a product their customers werenâ€™t asking for. Nobody â€œneededâ€ a computer, and so PCs werenâ€™t boughtâ€”they were sold. Flashy startups like Apple and Sinclair used hype to get noticed, while companies with footholds in consumer electronics like Atari, Commodore, and Tandy/RadioShack used strong retail connections to put their PCs in front of potential customers.Â 



        
            
            

            
                    

    



The market grew slowly at first, accelerating only as experiments led to practical applications like the spreadsheet, introduced in 1979. As use grew, observation of use caused a reduction in uncertainty, leading to more adoption in a self-reinforcing cycle. This kind of gathering momentum takes time in every technological wave: It took almost 30 years for electricity to reach half of American households, for example, and it took about the same amount of time for personal computers.[3] When a technological revolution changes everything, it takes a huge amount of innovation, investment, storytelling, time, and plain old work. It also sucks up all the money and talent available. Like Kuhnâ€™s paradigms in science, any technology not part of the waveâ€™s techno-economic paradigm will seem like a sideshow.[4]



        
            
            

            
                            Source: [3]
                    

    



The nascent growth of PCs attracted investorsâ€”venture capitalistsâ€”who started making risky bets on new companies. This development incentivized more inventors, entrepreneurs, and researchers, which in turn drew in more speculative capital.



Companies like IBM, the computing behemoth before the PC, saw poor relative performance. They didnâ€™t believe the PC could survive long enough to become capable in their market and didnâ€™t care about new, small markets that wanted a cheaper solution.



Retroactively, we give the PC pioneers the powers of prophets rather than visionaries. But at the time, nobody outside of a small group of early adopters paid any attention. Establishment media like The New York Times didnâ€™t take the PC seriously until after IBMâ€™s was introduced in August 1981. In the entire year of 1976, when Apple Computer was founded, the NYT mentioned PCs only four times.[5] Apparently, only the crazy ones, the misfits, the rebels, and the troublemakers were paying attention.



        
            
            

            
                            Source: [5]
                    

    



Itâ€™s the element of surprise that should strike us most forcefully when we compare the early days of the computer revolution to today. No one took note of personal computers in the 1970s. In 2025, AI is all we seem to talk about.



    




Big companies hate surprises. Thatâ€™s why uncertainty makes a perfect moat for startups. Apple would never have survived IBM entering the market in 1979, and only lived to compete another day after raising $100 million in its 1980 IPO. It was the only remaining competitor after the IBM-induced winnowing.[6]



        
            
            

            
                            Source: [6]
                    

    



As the tech took hold and started to show promise, innovations in software, memory, and peripherals like floppy disk drives and modems joined it. They reinforced one another, with each advance putting pressure on the technologies adjacent to it. When any part of the system held back the other parts, investors rushed to fund that sector. As increases in PC memory allowed more complicated software, for example, there became a need for more external storage, which caused VC Dave Marquardt to invest in disk drive manufacturer Seagate in 1980. Seagate gave Marquardt a 40x return when it went public in 1981. Other investors noticed, and some $270 million was plowed into the industry in the following three years.[7]



Money also poured into the underlying infrastructureâ€”fiber optic networks, chip making, etc.â€”so that capacity was never a bottleneck. Companies which used the new technological system to outperform incumbents began to take market share, and even staid competitors realized they needed to adopt the new thing or die. The hype became a froth which became an investment bubble: the dot-com frenzy of the late 1990s. The ICT wave was therefore similar to previous onesâ€”like the investment mania of the 1830s and the Roaring â€˜20s, which followed the infrastructure buildout of canals and railways, respectivelyâ€”in which the human response to each stage predictably generated the next.



When the dot-com bubble popped, society found it disapproved of the excesses in the sector and governments found they had the popular support to reassert authority over the tech companies and their investors. This put a brake on the madness. Instead of the reckless innovation of the bubble, companies started to expand into proven markets, and financiers moved from speculating to investing. Entrepreneurs began to focus on finding applications rather than on innovating the underlying technologies. Technological improvements continued, but change became more evolutionary than revolutionary.



As change slowed, companies gained the confidence to invest for the longer term. They began to combine various parts of the system in new ways to create value for a wider group of users. The massive overbuilding of fiber optic telecom networks and other infrastructure during the frenzy left plenty of cheap capacity, keeping the costs of expansion down. It was a great time to be a businessperson and investor.



        
            
            

            
                    

    



In contrast, society did not need a bubble to pop to start excoriating AI. Given that the backlash to tech has been going on for a decade, this seems normal to us. But the AI backlash differs from the general high regard, earlier in the cycle, enjoyed by the likes of Bill Gates, Steve Jobs, Jeff Bezos, and others who built big tech businesses. The world hates change, and only gave tech a pass in the â€˜80s and â€˜90s because it all still seemed reversible: it could be made to go away if it turned out badly. This gave the early computer innovators some leeway to experiment. Now that everyone knows computers are here to stay, AI is not allowed the same wait-and-see attitude. It is seen as part of the ICT revolution.



    




Perez, the economist, breaks each technological wave into four predictable phases: irruption, frenzy, synergy, and maturity. Each has a characteristic investment profile.



        
            
            

            
                    

    



The middle two, frenzy and synergy, are the easy ones for investors. Frenzy is when everyone piles in and investors are rewarded for taking big risks on unproven ideas, culminating in the bubble, when paper profits disappear. When rationality returns, the synergy phase begins, as companies make their products usable and productive for a wide array of users. Synergy pays those who are patient, picky, and can bring more than just money to the table.



Irruption and maturity are more difficult to invest in.



Investing in the 1970s was harder than it might look in hindsight. To invest from 1971 through 1975, you had to be either a true believer or a conglomerator with a knuckle-headed diversification strategy. Intel was a great investment, though it looked at first like a previous-wave electronics company. MOS Technologies was founded in 1969 to compete with Texas Instruments but sold a majority of itself to Allen-Bradley to stay afloat. Zilog was funded in 1975 by Exxon (Exxon!). Apple was a great investment, but it had none of the hallmarks of what VCs look for, as the PC was still a solution in search of a problem.



It was later irruption, in the early 1980s, when great opportunities proliferated: PC makers (Compaq, Dell), software and operating systems (Microsoft, Electronic Arts, Adobe), peripherals (Seagate), workstations (Sun), and computer stores (Businessland), among others. If you invested in the winners, you did well. But there was still more money than ideas, which meant that it was no golden age for investing. By 1983, there were more than 70 companies competing in the disk drive sector alone, and valuations collapsed. There were plenty of people whose fortunes were established in the 1970s and 1980s, and many VCs made their names in that era. But the biggest advantage to being an irruption-stage investor was building institutional knowledge to invest early and well in the frenzy and synergy phases.



Investing in the maturity phase is even more difficult. In irruption, itâ€™s hard to see what will happen; in maturity, nothing much happens at all. The uncertainty about what will work and how customers and society will react is almost gone. Things are predictable, and everyone acts predictably.



The lack of dynamism allows the successful synergy companies to remain entrenched (see: the Nifty 50 and FAANG), but growth becomes harder. They start to enter each otherâ€™s markets, conglomerate, raise prices, and cut costs. The era of products priced to entice new customers ends, and quality suffers. The big companies continue to embrace the idea of revolutionary innovation, but feel the need to control how their advances are used. R&D spending is redirected from product and process innovation toward increasingly fruitless attempts to find ways to extend the current paradigm. Companies frame this as a drive to win, but itâ€™s really a fear of losing.



Innovation can happen during maturity, sometimes spectacularly. But because these innovations only find support if they fit into the current waveâ€™s paradigm, they are easily captured in the dominant companiesâ€™ gravity wells. This means making money as an entrepreneur or investor in them is almost impossible. Generative AI is clearly being captured by the dominant ICT companies, which raises the question of whether this time will be different for inventors and investorsâ€”a different question from whether AI itself is a revolutionary technology.



    




Shipping containerization was a late-wave innovation that changed the world, kicked off our modern era of globalization, resulted in profound changes to society and the economy, and contributed to rapid growth in well-being. But there were, perhaps, only one or two people who made real money investing in it.



The year 1956 was late in the previous wave. But that year, the company soon to be known as SeaLand revolutionized freight shipping with the launch of the first containership, the Ideal-X. SeaLandâ€™s founder, Malcom McLean, had an epiphany that the job to be done by truckers, railroads, and shipping lines was to move goods from shipper to destination, not to drive trucks, fill boxcars, or lade boats. SeaLand allowed freight to transfer seamlessly from one mode to another, saving time, making shipping more predictable, and cutting costsâ€”both the costs of loading, unloading, and reloading, and the cost of a ship sitting idly in port as it was loaded and unloaded.[8]



The benefits of containerization, if it could be made to happen, were obvious. Everybody could see the efficiencies, and customers donâ€™t care how something gets to where they can buy it, as long as it does. But longshoremen would lose work, politicians would lose the votes of those who lost work, port authorities would lose the support of the politicians, federal regulators would be blamed for adverse consequences, railroads might lose freight to shipping lines, shipping lines might lose freight to new shipping lines, and it would all cost a mint. Most thought McLean would never be able to make it work.



McLean squeezed through the cracks of the opposition he faced. He bought and retrofitted war surplus ships, lowering costs. He went after the coastal shipping trade, a dying business in the age of the new interstates, to avoid competition. He set up shop in Newark, NJ, rather than the shipping hub of Hellâ€™s Kitchen, to get buy-in from the port authority and avoid Manhattan congestion. And he made a deal with the New York longshoremenâ€™s union, which was only possible because he was a small player whom they figured was not a threat.



        
            
            

            
                            Source: [10]
                    

    



But competitors and regulators moved too quickly for McLean to seize the few barriers to entry that might have been available to him: domination of the ports, exclusive agreements with shippers or other forms of transportation, standardization on proprietary technology, etc.[9] When it started to look like it might work, around 1965, the obvious advantages of containerization meant that every large shipping line entered the business, and competition took off. Even though containerized freight was less than 1% of total trade by 1968, the number of containerships was already ramping fast.[10] Capacity outstripped demand for years.Â 



The increase in competition led to a rate war, which led to squeezed profits, which in turn led to consolidation and cartels. Meanwhile, the cost of building ever-larger container ships and the port facilities to deal with them meant the business became hugely capital intensive. McLean saw the writing on the wall and sold SeaLand to R.J. Reynolds in January 1969. He was, perhaps, the only entrepreneur to get out unscathed.



It took a long time for the end-to-end vision to be realized. But around 1980, a dramatic drop began in the cost of sea freight.[11] This contributed to a boom in international trade[12] and allowed manufacturers to move away from higher-wage to lower-wage countries, making containerization irreversible.



        
            
            

            
                            Source: [11]
                    

    



Some people did make money, of course; someone always does. McLean did, as did shipping magnate Daniel Ludwig, who had invested $8.5 million in SeaLandâ€™s predecessor, McLean Industries, at $8.50 per share in 1965 and sold in 1969 for $50 per share.[13] Shipbuilders made money, too: between 1967 and 1972, some $10 billion ($80 billion in 2025 dollars) was spent building containerships. The contractors that built the new container ports also made money. And, later, shipping lines that consolidated and dominated the business, like Maersk and Evergreen, became very large. But, â€œfor R.J. Reynolds, and for other companies that had chased fast growth by buying into container shipping in the late 1960s, their investments brought little but disappointment.â€[14] Aside from McLean and Ludwig, it is hard to find anyone who became rich from containerization itself, because competition and capex costs made it hard to grow fast or achieve high margins.



        
            
            

            
                            Source: [12]
                    

    



The business ended up being dominated primarily by the previous incumbents, and the margins went to the companies shipping goods, not the ones they shipped through. Companies like IKEA benefited from cheap shipping, going from a provincial Scandinavian company in 1972 to the worldâ€™s largest furniture retailer by 2008; container shipping was a perfect fit for IKEAâ€™s flat-pack furniture. Others, like Walmart, used the predictability enabled by containerization to lower inventory and its associated costs.



With hindsight, itâ€™s easy to see how you could have invested in containerization: not in the container shipping industry itself, but in the industries that benefited from containerization. But even here, the success of companies like Walmart, Costco, and Target was coupled with the failure of others. The fallout from containerization set Sears and Woolworth on downward spirals, put the final nail in the coffin of Montgomery Ward and A&P, and drove Macyâ€™s into bankruptcy before it was rescued and downsized by Federated. Meanwhile, in North Carolina, â€œthe furniture capital of the world,â€ furniture makers tried to compete with IKEA by importing cheap pieces from China. They ended up being replaced by their suppliers.[15]



If there had been more time to build moats, there might have been a few dominant containerization companies, and the people behind them would be at the top of the Forbes 400, while their investors would be legendary. But moats take time to build and, unlike the personal computer, the adoption of containerization wasnâ€™t a surpriseâ€”every business with interests at stake had a strategic plan immediately.



The economist Joseph Schumpeter said â€œperfect competition is and always has been temporarily suspended whenever anything new is being introduced.â€[16] But containerization shows this isnâ€™t true at the end of tech waves. And because there is no economic profit during perfect competition, there is no money to be made by innovators during maturity. Like containerization, the introduction of AI did not lead to a period of protected profits for its innovators. It led to an immediate competitive free-for-all.



    




Letâ€™s grant that generative AI is revolutionary (but also that, as is becoming increasingly clear, this particular tech is now already in an evolutionary stage). It will create a lot of value for the economy, and investors hope to capture some of it. When, who, and how depends on whether AI is the end of the ICT wave, or the beginning of a new one.Â 



If AI had started a new wave, there would have been an extended period of uncertainty and experimentation. There would have been a population of early adopters experimenting with their own models. When thousands or millions of tinkerers use the tech to solve problems in entirely new ways, its uses proliferate. But because they are using models owned by the big AI companies, their ability to fully experiment is limited to whatâ€™s allowed by the incumbents, who have no desire to permit an extended challenge to the status quo.



This doesnâ€™t mean AI canâ€™t start the next technological revolution. It might, if experimentation becomes cheap, distributed and permissionlessâ€”like Wozniak cobbling together computers in his garage, Ford building his first internal combustion engine in his kitchen, or Trevithick building his high-pressure steam engine as soon as James Wattâ€™s patents expired. When any would-be innovator can build and train an LLM on their laptop and put it to use in any way their imagination dictates, it might be the seed of the next big set of changesâ€”something revolutionary rather than evolutionary. But until and unless that happens, there can be no irruption.



AI is instead the epitome of the ICT wave. The computing visionaries of the 1960s set out to build a machine that could think, which their successors eventually did, by extending gains in algorithms, chips, data, and data center infrastructure. Like containerization, AI is an extension of something that came before, and therefore no one is surprised by what it can and will do. In the 1970s, it took time for people to wrap their heads around the desirability of powerful and ubiquitous computing. But in 2025, machines that think better than previous machines are easy for people to understand.



Consider the extent to which the progress of AI rhymes with the business evolution of containerization:



        
            
            

            
                    

    



In the â€œAI rhymesâ€ column, the first four items are already underway. How you should invest depends on whether you believe Nos. 5â€“7 are next.



    




Economists are predicting that AI will increase global GDP somewhere between 1%[17] to more than 7%[18] over the next decade, which is $1â€“7 trillion of new value created. The big question is where that money will stick as it flows through the value chain.



Most AI market overviews have a score or more categories, breaking each of them into customer and industry served. But these will change dramatically over the next few years. You could, instead, just follow the money to simplify the taxonomy of companies:



        
            
            

            
                    

    



What the history of containerization suggests is that, if you arenâ€™t already an investor in a model company, you shouldnâ€™t bother. Sam Altman and a few other early movers may make a fortune, as McLean and Ludwig did. But the huge costs of building and running a model, coupled with intense competition, means there will, in the end, be only a few companies, each funded and owned by the largest tech companies. If youâ€™re already an investor, congratulations: There will be consolidation, so you might get an exit.



Domain-specific modelsâ€”like Cursor or Harveyâ€”will be part of the consolidation. These are probably the most valuable models. But fine-tuning is relatively cheap, and there are big economies of scope. On the other hand, just as Google had to buy Invite Media in 2010 to figure out how to sell to ad agencies, domain-specific model companies that have earned the trust of their customers will be prime acquisition targets. And although it seems possible that models which generate things other than languageâ€”like Midjourney or Runwayâ€”might use their somewhat different architecture to carve out a separate technological path, the LLM companies have easily entered this space as well. Whether this applies to companies like Osmo remains to be seen.



While itâ€™s too late to invest in the model companies, the profusion of those using the models to solve specific problems is ongoing: Perplexity, InflectionAI, Writer, Abridge, and a hundred others. But if any of these become very valuable, the model companies will take their earnings, either through discriminatory pricing or vertical integration. Success, in other words, will mean defeatâ€”always a bad thesis. At some point, model companies and app companies will converge: There will simply be AI companies, and only a few of them. There will be some winners, as always, but investments in the app layer as a whole will lose money.Â 



The same caveat applies, however: If an app company can build a customer base or an amazing team, it might be acquired. But these companies arenâ€™t really technology companies at all; they are building a market on spec and have to be priced as such. A further caveat is that there will be investors who make a killing arbitraging FOMO-panicked acquirors willing to massively overpay. But this is not really â€œinvesting.â€



There might be an investment opportunity in companies that manage the interface between the AI giants and their customers, or protect company data from the model companiesâ€”like Hugging Face or Gleanâ€”because these businesses are by nature independent of the models. But no analogue in the post-containerization shipping market became very large. Even the successful intermediation companies in the AI space will likely end up mid-sized because the model companies will not allow them to gain strategic leverageâ€”another consequence of the absence of surprise.



    




When an industry is going to be big but there is uncertainty about how it will play out, it often makes sense to swim upstream to the industryâ€™s suppliers. In the case of AI, this means the chip providers, data companies, and cloud/data center companies: SambaNova, Scale AI, and Lambda, as well as those that have been around for a long time, like Nvidia and Bloomberg.



The case for data is mixed. General dataâ€”i.e., things most people know, including everything anyone knew more than, say, 10 years ago, and most of what was learned after thatâ€”is a commodity. There may be room for a few companies to do the grunt work of collating and tagging it, but since the collating and tagging might best be done by AI itself, there will not be a lot of pricing leverage. Domain-specific models will need specialist data, and other models will try to answer questions about the current moment. Specific, timely, and hard to reproduce data will be valuable. This is not a new market, of courseâ€”Bloomberg and others have done well by it. A more concentrated customer base will lower prices for this data, while wider use will raise revenues. On balance, this will probably be a plus for the industry, though not a huge one. There will be new companies built, but only a couple worth investing in.



The high capex of AI companies will primarily be spent with the infrastructure companies. These companies are already valued with this expectation, so there wonâ€™t be an upside surprise. But consider that shipbuilding benefited from containerization from 1965 until demand collapsed after about 1973.[19] If AI companies consolidate or otherwise act in concert, even a slight downturn that forces them to conserve cash could turn into a serious, sudden, and long-lasting decline in infrastructure spending. This would leave companies like Nvidia and its emerging competitorsâ€”who must all make long-term commitments to suppliers and for capacity expansionâ€”unable to lower costs to match the new, smaller market size. Companies priced for an s-curve are overpriced if thereâ€™s a peak and decline.



        
            
            

            
                            Source: [19]
                    

    



All of which means that investors shouldnâ€™t swim upstream, but fish downstream: companies whose products rely on achieving high-quality results from somewhat ambiguous information will see increased productivity and higher profits. These sectors include professional services, healthcare, education, financial services, and creative services, which together account for between a third and a half of global GDP and have not seen much increased productivity from automation. AI can help lower costs, but as with containerization, how individual businesses incorporate lower costs into their strategiesâ€”and what they decide to do with the savingsâ€”will determine success. To put it bluntly, using cost savings to increase profits rather than grow revenue is a loserâ€™s game.



The companies that will benefit most rapidly are those whose strategies are already conditional on lowering costs. IKEAâ€™s longtime strategy was to sell quality furniture for low prices and make it up on volume. After containerization made it possible for them to go worldwide, IKEA became the worldâ€™s largest retailer and Ingvar Kamprad (the IK of IKEA) became a billionaire. Similarly, Walmart, whose strategy was high volume and low prices in underserved markets, benefited from both cost savings and just-in-time supply chains, allowing increased product variety and lower inventory costs.



Todayâ€™s knowledge-work companies that already prioritize the same values are the least risky way to bet on AI, but new companies will form or re-form with a high-volume, low-cost strategy, just as Costco did in the early 1980s. New companies will compete with the incumbents, but with a clean slate and hindsight. Regardless, there are few barriers to entry, so each of these firms will face stiff competition and operate in fragmented markets. Experienced management and flawless execution will be key.



Being an entrepreneur will be a fabulous proposition in these sectors. Being an investor will be harder. Companies will not need much private capitalâ€”IKEA never needed to raise risk capital, and Costco raised only one round in 1983 before going public in 1985â€”because implementing cost-savings technology is not capital intensive. As with containerization, there will be a long lag between technology trigger and the best investments. The opportunities will be later.



Stock pickers will also make money, but they need to be choosy. At the high end of projections, an additional 7% in GDP growth over ten years within one third of the economy gives a tailwind of only about 2% per year to these companiesâ€”even less if productivity growth from older ICT products abates. The primary value shift will be to companies that are embracing the strategic implications of AI from companies that are not, the way Walmart benefited from Sears, which took advantage of cheaper goods prices but did not reinvent itself.



Consumers, however, will be the biggest beneficiaries. Previous waves of mechanization benefited labor productivity in manufacturing, driving prices down and saving consumers money. But increased labor productivity in manufacturing also led to higher manufacturing wages. Wages in services businesses had to rise to compete, even though these businesses did not benefit from productivity gains. This caused the price of services to rise.[20] The share of household spending on food and clothing went from 55% in 1918 to 16% in 2023,[21] but the cost of knowledge-intensive services like healthcare and education have grown well above inflation.Â 



Something similar will happen with AI: Knowledge-intensive services will get cheaper, allowing consumers to buy more of them, while services that require person-to-person interaction will get more expensive, taking up a greater percentage of household spending. This points to obvious opportunities in both. But the big news is that most of the new value created by AI will be captured by consumers, who should see a wider variety of knowledge-intensive goods at reasonable prices, and wider and more affordable access to services like medical care, education, and advice.



    




There is nothing better than the beginning of a new wave, when the opportunities to envision, invent, and build world-changing companies leads to money, fame, and glory. But there is nothing more dangerous for investors and entrepreneurs than wishful thinking. The lessons learned from investing in tech over the last 50 years are not the right ones to apply now. The way to invest in AI is to think through the implications of knowledge workers becoming more efficient, to imagine what markets this efficiency unlocks, and to invest in those. For decades, the way to make money was to bet on what the new thing was. Now, you have to bet on the opportunities it opens up.



    




Jerry Neumann is a retired venture investor, writing and teaching about innovation.



    

                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Myocardial infarction may be an infectious disease]]></title>
            <link>https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235648</guid>
            <description><![CDATA[A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional und...]]></description>
            <content:encoded><![CDATA[A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional understanding of the pathogenesis of myocardial infarction and opens new avenues for treatment, diagnostics, and even vaccine development.According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patientâ€™s immune system and antibiotics because they cannot penetrate the biofilm matrix.A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.Professor Pekka Karhunen, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.â€œBacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material â€“ DNA â€“ from several oral bacteria inside atherosclerotic plaques,â€ Karhunen explains.The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The bodyâ€™s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation.Â The research article Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques was published in the Journal of the American Heart Association on 6 August 2025. Read the article onlineFurther informationProfessor Pekka KarhunenFaculty of Medicine and Health TechnologyTampere Universitypekka.j.karhunen [at] tuni.fi (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)Tel. +358 400 511361]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AMDâ€™s RDNA4 GPU architecture]]></title>
            <link>https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235293</guid>
            <description><![CDATA[RDNA4 is AMDâ€™s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs.]]></description>
            <content:encoded><![CDATA[RDNA4 is AMDâ€™s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMDâ€™s new GPU architecture caters to ML workloads as well.From AMDâ€™s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPUâ€™s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.The Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4â€™s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.265, H.265, and AV1, especially in low latency encoding.Low latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly use H.264, an older codec thatâ€™s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.Testing with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4â€™s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr_latency). Netflixâ€™s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.RDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But itâ€™s worth remembering that the latency-constrained VBR mode uses a VBV buffer of up to three frames, while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.The display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. Itâ€™s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a â€œRadeon Image Sharpeningâ€ filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPUâ€™s programmable shaders means that the sharpening filter wonâ€™t impact performance and can be carried out with better power efficiency. And, AMD doesnâ€™t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though Iâ€™m not sure why anyone would want that.Power consumption is another important optimization area for display engines. Traditionally thatâ€™s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMDâ€™s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didnâ€™t go into more detail, but itâ€™s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4â€™s memory subsystem enter a low power state while still meeting refresh deadlines.Power and GDDR6 data rates for various refresh rate combinations. AMDâ€™s monitoring software (and others) read out extremely low memory clocks when the memory bus is able to idle, so those readings arenâ€™t listed.I have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents donâ€™t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, itâ€™s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents donâ€™t change.RDNA4â€™s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMDâ€™s presentation went over a couple other details worth discussing.AMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4â€™s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.While not discussed in AMDâ€™s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.GPUs use barriers to synchronize threads and enforce memory ordering. For example, a s_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.RDNA4 splits the barrier into separate â€œsignalâ€ and â€œwaitâ€ actions. Instead of s_barrier, RDNA4 has s_barrier_signal and s_barrier_wait. A thread can â€œsignalâ€ the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s_barrier_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.The largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and itâ€™s not surprising that itâ€™s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMarkâ€™s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.RDNA2 still does a good job of keeping data in L2 in absolute terms. But itâ€™s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. Thatâ€™s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMDâ€™s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidiaâ€™s strategy of putting way more eggs in the L2 basket. Blackwellâ€™s L2 cache serves the functions of both AMDâ€™s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workloadâ€™s requested local storage (shared memory) capacity.A mid-level L1 cache was a familiar fixture on prior RDNA generations. Itâ€™s conspicuously missing from RDNA4, as well as AMDâ€™s presentation. One possibility is that L1 cache hitrate wasnâ€™t high enough to justify the complexity of an extra cache level. Perhaps AMD felt its area and transistor budget was better allocated towards increasing L2 capacity. To support this theory, L1 hitrate on RDNA1 was often below 50%. At the same time, the RDNA series always enjoyed a high bandwidth and low latency L2. Putting more pressure on L2 in exchange for reducing L2 misses may have been an enticing tradeoff. Another possibility is that AMD ran into validation issues with the L1 cache and decided to skip it for this generation. Thereâ€™s no way to verify either possibility of course, but I think the former reasons make more sense.Beyond tweaking the cache hierarchy, RDNA4 brings improvements to transparent compression. AMD emphasized that theyâ€™re using compression throughout the SoC, including at points like the display engine and media engine. Compressed data can be stored in caches, and decompressed before being written back to memory. Compression cuts down on data transfer, which reduces bandwidth requirements and improves power efficiency.Transparent compression is not a new feature. It has a long history of being one tool in the GPU toolbox for reducing memory bandwidth usage, and it would be difficult to find any modern GPU without compression features of some sort. Even compression in other blocks like the display engine have precedent. Intelâ€™s display engines for example use Framebuffer Compression (FBC), which can write a compressed copy of frame data and keep fetching the compressed copy to reduce data transfer power usage as long as the data doesnâ€™t change. Prior RDNA generations had compression features too, and AMDâ€™sdocumentation summarizes some compression targets. While AMD didnâ€™t talk about compression efficiency, I tried to take similar frame captures using RGP on both RDNA1 and RDNA4 to see if thereâ€™s a large difference in memory access per frame. It didnâ€™t quite work out the way I expected, but Iâ€™ll put them here anyway and discuss why evaluating compression efficacy is challenging.The first challenge is that both architectures satisfy most memory requests from L0 or L1. AMD slides on RDNA1 suggest the L0 and L1 only hold decompressed data, at least for delta color compression. Compression does apply to L2. For RDNA4, AMDâ€™s slides indicate it applies to the Infinity Cache too. However, focusing on data transfer to and from the L2 wouldnâ€™t work due the large cache hierarchy differences between those RDNA generations.DCC, or delta color compression, is not the only form of compression. But this slide shows one example of compression/decompression happening in front of L2Another issue is, itâ€™s easy to imagine a compression scheme that doesnâ€™t change the number of cache requests involved. For example, data might be compressed to only take up part of a cacheline. A request only causes a subset of the cacheline to be read out, which a decompressor module expands to the full 128B. Older RDNA1 slides are ambiguous about this, indicating that DCC operates on 256B granularity (two cachelines) without providing further details.In any case, compression may be a contributing factor in RDNA4 being able to achieve better performance while using a smaller Infinity Cache than prior generations, despite only having a 256-bit GDDR6 DRAM setup.AMD went over RAS, or reliability, availability, and serviceability features in RDNA4. Modern chips use parity and ECC to detect errors and correct them, and evidently RDNA4 does the same. Unrecoverable errors are handled with driver intervention, by â€œre-initializing the relevant portion of the SoC, thus preventing the platform from shutting downâ€. Thereâ€™s two ways to interpret that statement. One is that the GPU can be re-initialized to recover from hardware errors, obviously affecting any software relying on GPU acceleration. Another is that some parts of the GPU can be re-initialized while the GPU continues handling work. I think the former is more likely, though I can imagine the latter being possible in limited forms too. For example, an unrecoverable error reading from GDDR6 can hypothetically be fixed if that data is backed by a duplicate in system memory. The driver could transfer known-good data from the host to replace the corrupted copy. But errors with modified data would be difficult to recover from, because there might not be an up-to-date copy elsewhere in the system.On the security front, microprocessors get private buses to â€œcritical blocksâ€ and protected register access mechanisms. Security here targets HDCP and other DRM features, which I donâ€™t find particularly amusing. But terminology shown on the slide is interesting, because MP0 and MP1 are also covered in AMDâ€™s CPU-side documentation. On the CPU side, MP0 (microprocessor 0) handles some Secure Encrypted Virtualization (SEV) features. Itâ€™s sometimes called the Platform Security Processor (PSP) too. MP1 on CPUs is called the System Management Unit (SMU), which covers power control functions. Curiously AMDâ€™s slide labels MP1 and the SMU separately on RDNA4. MP0/MP1 could have completely different functions on GPUs of course. But the common terminology raises the possibility that thereâ€™s a lot of shared work between CPU and GPU SoC design. RAS is also a very traditional CPU feature, though GPUs have picked up RAS features over time as GPU compute picked up steam.One of the most obvious examples of shared effort between the CPU and GPU sides is Infinity Fabric making its way to graphics designs. This started years ago with Vega, though back then using Infinity Fabric was more of an implementation detail. But years later, Infinity Fabric components provided an elegant way to implement a large last level cache, or multi-socket coherent systems with gigantic iGPUs (like MI300A).Slide from Hot Chips 29, covering Infinity Fabric used in AMDâ€™s older Vega GPUThe Infinity Fabric memory-side subsystem on RDNA4 consists of 16 CS (Coherent Station) blocks, each paired with a Unified Memory Controller (UMC). Coherent Stations receive requests coming off the graphics L2 and other clients. They ensure coherent memory access by either getting data from a UMC, or by sending a probe if another block has a more up-to-date copy of the requested cacheline. The CS is a logical place to implement a memory side cache, and each CS instance has 4 MB of cache in RDNA4.To save power, Infinity Fabric supports DVFS (dynamic voltage and frequency scaling) to save power, and clocks between 1.5 and 2.5 GHz. Infinity Fabric bandwidth is 1024 bits per clock, which suggests the Infinity Cache can provide 2.5 TB/s of theoretical bandwidth. That roughly lines up with results from Nemesâ€™s Vulkan-based GPU cache and memory bandwidth microbenchmark.AMD also went over their ability to disable various SoC components to harvest dies and create different SKUs. Shader Engines, WGPs, and memory controller channels can be disabled. AMD and other manufacturers have used similar harvesting capabilities in the past. Iâ€™m not sure whatâ€™s new here. Likely, AMD wants to re-emphasize their harvesting options.Finally, AMD mentioned that they chose a monolithic design for RDNA4 because it made sense for a graphics engine of its size. They looked at performance goals, package assembly and turnaround time, and cost. After evaluating those factors, they decided a monolithic design was the right option. Itâ€™s not a surprise. After all, AMD used monolithic designs for lower end RDNA3 products with smaller graphics engines, and only used chiplets for the largest SKUs. Rather, itâ€™s a reminder that thereâ€™s no one size fits all solution. Whether a monolithic or chiplet-based design makes more sense depends heavily on design goals.RDNA4 brings a lot of exciting improvements to the table, while breaking away from any attempt to tackle the top end performance segment. Rather than going for maximum performance, RDNA4 looks optimized to improve efficiency over prior generations. The RX 9070 offers similar performance to the RX 7900XT in rasterization workloads despite having a lower power budget, less memory bandwidth, and a smaller last level cache. Techspot also shows the RX 9070 leading with raytracing workloads, which aligns with AMD's goal of enhancing raytracing performance.Slide from RDNA4â€™s Launch Presentation not Hot Chips 2025AMD achieves this efficiency using compression, better raytracing structures, and a larger L2 cache. As a result, RDNA4 can pack its performance into a relatively small 356.5 mmÂ² die and use a modest 256-bit GDDR6 memory setup. Display and media engine improvements are welcome too. Multi-monitor idle power feels like a neglected area for discrete GPUs, even though I know many people use multiple monitors for productivity. Lowering idle power in those setups is much appreciated. On the media engine side, AMDâ€™s video encoding capabilities have often lagged behind the competition. RDNA4â€™s progress at least prevents AMD from falling as far behind as they have before.If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Safe C++ proposal is not being continued]]></title>
            <link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45234460</guid>
            <description><![CDATA[One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, â€¦]]></description>
            <content:encoded><![CDATA[One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:Code in the safe context exhibits the same strong safety guarantees as code written in Rust.The rest remains â€œunsafeâ€ in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++â€™s design. Also, because C++ already has a huge base of â€œunsafe codeâ€, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++â€™s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesnâ€™t break code that doesnâ€™t use it.The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadnâ€™t read any updates on it for some time. So I searched and found some answers on Reddit.The response from Sean Baxter, one of the original authors of the Safe C++ proposal:The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.And again:The Rust safety model is unpopular with the committee. Further work on my end wonâ€™t change that. Profiles won the argument. All effort should go into getting Profileâ€™s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you donâ€™t enable it, things work as before. So itâ€™s backwards-compatible.Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they donâ€™t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They wonâ€™t give us silver-bullet guarantees, but they are a realistic path forward.[1] Core safety profiles for C++26[2] C++ Profiles: The Framework[3] What are profiles?[4] Note to the C++ standards committee members]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The case against social media is stronger than you think]]></title>
            <link>https://arachnemag.substack.com/p/the-case-against-social-media-is</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45234323</guid>
        </item>
        <item>
            <title><![CDATA[RIP pthread_cancel]]></title>
            <link>https://eissing.org/icing/posts/rip_pthread_cancel/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233713</guid>
            <description><![CDATA[I posted about adding pthread_cancel use in curl about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with #18540 we are ripping it out again. What happened?
short recap pthreads define â€œCancelation pointsâ€, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that may be cancelation points, among those getaddrinfo().
getaddrinfo() is exactly what we are interested in for libcurl. It blocks until it has resolved a name. That may hang for a long time and libcurl is unable to do anything else. Meh. So, we start a pthread and let that call getaddrinfo(). libcurl can do other things while that thread runs.]]></description>
            <content:encoded><![CDATA[I posted about adding pthread_cancel use in curl
about three weeks ago, we released this in curl 8.16.0 and it blew
up right in our faces. Now, with
#18540 we are ripping it
out again. What happened?
short recap
pthreads
define â€œCancelation pointsâ€, a list of POSIX functions where
a pthread may be cancelled. In addition, there is also a list of functions
that may be cancelation points, among those getaddrinfo().
getaddrinfo() is exactly what we are interested in for libcurl. It blocks
until it has resolved a name. That may hang for a long time and libcurl
is unable to do anything else. Meh. So, we start a pthread and let that
call getaddrinfo(). libcurl can do other things while that thread runs.
But eventually, we have to get rid of the pthread again. Which means we
either have to pthread_join() it - which means a blocking wait. Or we
call pthread_detach() - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.
So, we added pthread_cancel() to interrupt a running getaddrinfo()
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.
cancel yes, leakage also yes!
After releasing curl 8.16.0 we got an issue reported in
#18532 that cancelled
pthreads leaked memory.

Digging into the glibc source
shows that there is this thing called
/etc/gai.conf
which defines how getaddrinfo() should sort returned answers.
The implementation in glibc first resolves the name to addresses. For these,
it needs to allocate memory. Then it needs to sort them if there is more
than one address. And in order
to do that it needs to read /etc/gai.conf. And in order to do that
it calls fopen() on the file. And that may be a pthread â€œCancelation Pointâ€
(and if not, it surely calls open() which is a required cancelation point).
So, the pthread may get cancelled when reading /etc/gai.conf and leak all
the allocated responses. And if it gets cancelled there, it will try to
read /etc/gai.conf again the next time it has more than one address
resolved.
At this point, I decided that we need to give up on the whole pthread_cancel()
strategy. The reading of /etc/gai.conf is one point where a cancelled
getaddrinfo() may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).
RIP
Leaking memory potentially on something libcurl does over and over again is
not acceptable. Weâ€™d rather pay the price of having to eventually wait on
a long running getaddrinfo().
Applications using libcurl can avoid this by using c-ares which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.
DNS continues to be tricky to use well.

  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Geedge and MESA leak: Analyzing the great firewallâ€™s largest document leak]]></title>
            <link>https://gfw.report/blog/geedge_and_mesa_leak/en/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233415</guid>
            <description><![CDATA[The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFW's research, development, and operations.]]></description>
            <content:encoded><![CDATA[

						
						
						
						
						
						

						
						1. Introduction
The Great Firewall of China (GFW) experienced the largest leak of internal documents in its history on Thursday September 11, 2025. Over 500 GB of source code, work logs, and internal communication records were leaked, revealing details of the GFWâ€™s research, development, and operations.
The leak originated from a core technical force behind the GFW: Geedge Networks (whose chief scientist is Fang Binxing) and the MESA Lab at the Institute of Information Engineering, Chinese Academy of Sciences. The documents show that the company not only provides services to governments in places like Xinjiang, Jiangsu, and Fujian, but also exports censorship and surveillance technology to countries such as Myanmar, Pakistan, Ethiopia, Kazakhstan, and other unidentified country under the â€œBelt and Roadâ€ framework.
The significance and far-reaching implications of this leak are substantial. Due to the massive volume of data, GFW Report will continue to analyze and provide updates on the current page and on the Net4People.
2. Download Link
Enlace Hacktivista has provided the access to the leak:

BitTorrent: https://enlacehacktivista.org/geedge.torrent
Direct HTTPS download: https://files.enlacehacktivista.org/geedge/

The leaked files total about 600 GB. Among them, the file mirror/repo.tar alone, as an archive of the RPM packaging server, takes up 500 GB.
For detailed instructions on how to use the specific files, David Fifield has already provided a more thorough explanation on Net4People.
     7206346  mirror/filelist.txt
497103482880  mirror/repo.tar
 14811058515  geedge_docs.tar.zst
  2724387262  geedge_jira.tar.zst
 35024722703  mesalab_docs.tar.zst
 63792097732  mesalab_git.tar.zst
       71382  A HAMSON-EN.docx
       16982  A Hamson.docx
      161765  BRI.docx
       14052  CPEC.docx
     2068705  CTF-AWD.docx
       19288  Schedule.docx
       26536  TSG Solution Review Description-20230208.docx
      704281  TSG-é—®é¢˜.docx
       35040  chat.docx
       27242  ty-Schedule.docx
      111244  å¾…å­¦ä¹ æ•´ç†-23å¹´MOTC-SWGåˆåŒè‰æœ¬V.1-2020230320.docx
       52049  æ‰“å°.docx
      418620  æ›¿ç¥¨è¯æ˜Ž.docx
      260551  é¢†å¯¼ä¿®æ”¹ç‰ˆ-å¾…çœ‹Reponse to Customer's Suggestions-2022110-V001--1647350669.docx
3. Safety Considerations
Due to the highly sensitive nature of these leaked materials, we strongly advise anyone who chooses to download and analyze them to take proper operational security precautions. It may be possible that these files may contain potentially risky content and accessing them in an insecure environment could expose you to surveillance or malware.
Please consider analyzing these files only in an isolated (virtual) machine without internet access.
4. Background
Great Firewall of China (GFW) is an umbrella term for a series of Internet censorship systems. Behind it, teams for research and development, operations, hardware, and management each play their roles and coordinate with one another. In addition to fixed government agencies (such as the CNCERT), different entities provide technical support depending on individual contracts and tenders. This leak originates from an important branch of the GFWâ€™s R&D capacity: Geedge Networks and MESA Lab. The MESA lab is affiliated with the Institute of Information Engineering, Chinese Academy of Sciences (IIE, CAS).
The origins trace back to Fang Binxing, the â€œFather of the Great Firewallâ€, coming to Beijing. At the end of 2008, he established the National Engineering Laboratory for Information Content Security (NELIST), initially based at the Institute of Computing Technology, Chinese Academy of Sciences. Beginning in 2012, the supporting institution changed to the Institute of Information Engineering, Chinese Academy of Sciences. In January 2012, some NELIST personnel formed a team at IIE, and in June 2012 the team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis). Below is an excerpt from MESAâ€™s self-introduction:
MESA Timeline

   January 2012: Liu Qingyun, Sun Yong, Zheng Chao, Yang Rong, Qin Peng, Liu Yang, and Li Jia formed a team at IIE;
   June 2012: The team was officially named the Processing Architecture Team, English name MESA (Massive Effective Stream Analysis);
   2012: Liu Qingyun was selected for IIEâ€™s inaugural â€œRising Starâ€ talent program;
   2012: Yang Wei and Zhou Zhou joined the team;
   2012: The team successfully completed the cybersecurity assurance task for the 18th National Congress;
   January 2013: MESAâ€™s first PhD trainee, Liu Tingwen, graduated successfully;
   2013: Li Shu, Liu Junpeng, and Liu Xueli joined the team;
   December 2013: The MESA team received IIEâ€™s 2013 Major Scientific and Technological Progress Award;
   2014: Zhou Zhou was selected for IIEâ€™s â€œRising Starâ€ talent program;
   2014: The MESA component SAPP platform began large-scale engineering deployment;
   2014: Zhang Peng, Yu Lingjing, and Jia Mengdie joined the team;
   2015: Zheng Chao was selected for IIEâ€™s â€œRising Starâ€ talent program, and Zhang Peng was selected for IIEâ€™s â€œOutstanding Talent Introductionâ€ program;
   August 2015: MESA moved from the Agriculture Bureau to the Huayan Beili office area;
   July 2015: PhD student Sha Hongzhou trained by MESA graduated successfully, and Liu Xiaomei received Outstanding Graduate honors;
   2016: Dou Fenghu, Zhu Yujia, Wang Fengmei, Li Zhao, Lu Qiuwen, Du Meijie, Shen Yan, and Fang Xupeng joined MESA in succession, and the team expanded rapidly;
   2016: The team undertook multiple major engineering projects, with annual contracted revenue exceeding 35 million;
   December 2016: The MESA team participated in winning the National Science and Technology Progress Award (Second Prize);
   2018: Sun Yong and Zhou Zhou received the 2017 National State Secrecy Science and Technology Award (Second Prize);
By 2018, Fang Binxing had also established himself in Hainan, and Geedge (Hainan) Information Technology Co., Ltd. (Geedge Networks Ltd.) was founded in the same year. Fang served as chief scientist, and the â€œcore R&D personnel came from universities and research institutes such as the Chinese Academy of Sciences, Harbin Institute of Technology, and Beijing University of Posts and Telecommunications.â€ Much of this talent came from MESAâ€”for example, Zheng Chao served as CTO. Attentive readers will notice that many mentors and students from the MESA timeline appear in the leaked Geedge company git commits.
5. Analysis of Nonâ€“Source Code Files
The nonâ€“source-code portion of the leaked files has already been analyzed in detail by multiple professional teams. Below are David Fifieldâ€™s notes on related media reports and technical write-ups. Please note that the source-code portion of the leak has not yet been analyzed:

David Fifieldâ€™s notes on the related media reports
David Fifieldâ€™s notes on the technical write-ups

6. Analysis of Source Code Files
The source-code portion of the leaked files has not yet been carefully analyzed. This leak is significant and far-reaching. Given the large volume of material, GFW Report will continue to update our analysis and findings on the current page as well as on Net4People.

This report was first published on GFW Report. We also actively updated our analysis and findings on Net4People.
We encourage you to share questions, comments, analysis, or additional evidence on this topic, either publicly or privately. Our private contact information can be found in the footer of the GFW Report website.


						
						
						










						
					]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Recreating the US/* time zone situation]]></title>
            <link>https://rachelbythebay.com/w/2025/09/12/tz/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233237</guid>
        </item>
        <item>
            <title><![CDATA[486Tang â€“ 486 on a credit-card-sized FPGA board]]></title>
            <link>https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232565</guid>
            <description><![CDATA[Yesterday I released 486Tang v0.1 on GitHub. Itâ€™s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. Iâ€™ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Hereâ€™s a short writeâ€‘up of the project.]]></description>
            <content:encoded><![CDATA[Yesterday I released 486Tang v0.1 on GitHub. Itâ€™s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. Iâ€™ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Hereâ€™s a short writeâ€‘up of the project.Thanks to everyone coming from Hacker News! If 486Tang caught your eye, I share progress updates and related projects over on X.486Tang ArchitectureEvery FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:Compared to ao486 on MiSTer, there are a few major differences:Switching to SDRAM for main memory. The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didnâ€™t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; timeâ€‘multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16â€‘bit wide while ao486 expects 32â€‘bit accesses, which would normally mean one 32â€‘bit word every two cycles. I mitigated this by running the SDRAM logic at 2Ã— the system clock so a 32â€‘bit word can be read or written every CPU cycle (â€œdoubleâ€‘pumpingâ€ the memory).SDâ€‘backed IDE. On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPSâ€‘FPGA link; the HPS then accesses a VHD image. Tang doesnâ€™t have a comparable highâ€‘speed MCUâ€‘toâ€‘FPGA interfaceâ€”only a feeble UARTâ€”so I moved disk storage into the SD card and let the FPGA access it directly.Bootâ€‘loading module. A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didnâ€™t rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.System bring-up with the help of a whole-system simulatorAfter restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complexâ€”CPU and peripheralsâ€”more so than the game consoles Iâ€™ve worked on. The ao486 CPU alone is >25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolateâ€”not viable for a hobby project.My solution was Verilator for subsystem and wholeâ€‘system simulation. The codebase is relatively mature, so I skipped perâ€‘module unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutesâ€”an order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardwareâ€”the CPU forwards them over UARTâ€”so BIOS issues show up immediately without waiting for a GAO build.Subsystemâ€‘scoped tracing. For Sound Blaster, IDE, etc., I added --sound, --ide flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.Bochs BIOS assembly listings are invaluable. I initially used a manual disassemblyâ€”old console habitsâ€”without symbols, which was painful. Rebuilding Bochs and using the official listings solved that.A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to toolchain behavior differences. In one case a variable meant to be static behaved like an automatic variable and didnâ€™t retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.Hereâ€™s a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.Performance optimizationsWith simulation help, the core ran on Tang Consoleâ€”just not fast. The Gowin GW5A isnâ€™t a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registersâ€”both risks bugs. A solid test suite is essential; I used test386.asm to validate changes.Here are a few concrete wins:Reset tree and fan-out reduction. Gowinâ€™s tools didnâ€™t replicate resets aggressively enough (even with â€œPlace â†’ Replicate Resourcesâ€). One reset net had >5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other highâ€‘fan-out nets helped a lot.Instruction fetch optimization. A long combinational chain sat in the decode/fetch interface. In decoder_regs.v, the number of bytes the fetcher may accept was computed using the last decoded instructionâ€™s length:reg [3:0] decoder_count;
assign acceptable_1     = 4'd12 - decoder_count + consume_count;
always @(posedge clk) begin
  ...
  decoder_count <= after_consume_count + accepted;
end
Here, 12 is the buffer size, decoder_count is the current occupancy, and consume_count is the length of the outgoing instruction. Reasonableâ€”but computing consume_count (opcode, ModR/M, etc.) was on the Fmaxâ€‘limiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and â€œeffective addressâ€ calculation.The fix was to drop the dependency on consume_count:assign acceptable_1    = 4'd12 - decoder_count;
This may cause the fetcher to â€œunderâ€‘fetchâ€ for one cycle because the outgoing instructionâ€™s space isnâ€™t reclaimed immediately. But decoder_count updates next cycle, reclaiming the space. With a 12â€‘byte buffer, the CPI impact was negligible and Fmax improved measurably on this board.TLB optimization. The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32â€‘entry fullyâ€‘associative TLB with a purely combinational read pathâ€”zero extra cycles, but a long path on every memory access (code and data).DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4â€‘way setâ€‘associative. Thatâ€™s simpler and already slightly faster than fullyâ€‘associative for these workloads. Thereâ€™s room to optimize further since the long combinational path rarely helps.A rough v0.1 endâ€‘toâ€‘end result: about +35% per Landmark 6 benchmarks, reaching roughly 486SXâ€‘20 territory.ReflectionsHere are a few reflections after the port:Clock speed scaling. I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective leverâ€”more so than extra caches or deeper pipelines at this stage. Up to ~200â€“300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes overâ€”the story of the 2000s.x86 vs. ARM. Working with ao486 deepened my respect for x86â€™s complexity. John Crawfordâ€™s 1990 paper â€œThe i486 CPU: Executing Instructions in One Clock Cycleâ€ is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last yearâ€™s ARM7â€‘based GBATang felt refreshingly simple: fixedâ€‘length 32â€‘bit instructions, saner addressing, and competitive performance. You canâ€™t have your cake and eat it.So there you have itâ€”thatâ€™s 486Tang in v0.1. Thanks for reading, and see you next time.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Four-year wedding crasher mystery solved]]></title>
            <link>https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232562</guid>
            <description><![CDATA[Bride finally tracks down awkward-looking stranger she and husband noticed only when looking through photos]]></description>
            <content:encoded><![CDATA[A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on Facebook likewise yielded no clues.Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider â€“ and a sheepish Andrew Hillhouse finally stepped forward.In his explanatory post on Facebook, Hillhouse admitted that he had been â€œcutting it fine, as Iâ€™m known to doâ€ when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel â€“ â€œI remember thinking to myself: â€˜Cool, this is obviously the right placeâ€™â€ â€“ unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.Michelle and John enjoy their wedding, unaware of the crasher. Photograph: Courtesy Michelle Wylie/SWNSHe was initially unperturbed to find himself surrounded by strangers as the ceremony began â€“ at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: â€œOMG thatâ€™s not Michaela â€¦ I was at the wrong wedding!â€Hillhouse said: â€œYou canâ€™t exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.â€At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.skip past newsletter promotionafter newsletter promotionHis post continued: â€œRushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.â€For Michelle Wylie, this amiable resolution brings to a close years of speculation.Hillhouse said the wedding photographer insisted he join other guests for a group shot. Photograph: Courtesy Michelle Wylie/SWNSShe told BBC Scotland: â€œIt would come into my head and Iâ€™d be like: â€˜Someone must know who this guy is.â€™ I said a few times to my husband: â€˜Are you sure you donâ€™t know this guy, is he maybe from your work?â€™ We wondered if he was a mad stalker.â€She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.â€œI could not stop laughing,â€ said Wylie. â€œWe canâ€™t believe weâ€™ve found out who he is after almost four years.â€]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: CLAVIER-36 â€“ A programming environment for generative music]]></title>
            <link>https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232299</guid>
        </item>
    </channel>
</rss>