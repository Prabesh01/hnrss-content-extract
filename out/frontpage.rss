<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 12 Sep 2025 16:09:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[A Beginner's Guide to Extending Emacs]]></title>
            <link>https://blog.tjll.net/a-beginners-guide-to-extending-emacs/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45223239</guid>
            <description><![CDATA[How to build a custom completion backend for emacs from scratch.]]></description>
            <content:encoded><![CDATA[
      

  
    
    «
    
    A Beginner's Guide to Extending Emacs
    
    »
    
  
  
    
       4 February, 2025
      3,983 words
      17 minute read time
    
  



        
	  

This post isn’t about the virtues of some editors versus others: that's already been written by somebody else (and it’s really good) – if you want to know why I use emacs, I suggest reading that instead.



This post will help you understand why "extensibility" and "introspectability" are such prominent emacs features even without an emacs lisp background.
Bridging the gap from spacemacs or doom emacs to a bespoke configuration wasn't easy for me because I didn’t know how to learn emacs, so I'm going to stumble through one of my own use cases to demonstrate how this process goes if you're peeking in from outside the emacs ecosystem, horrified curious about how this all works.



Let's talk about reStructuredText.



Table of Contents


reStructuredText

A Parentheses Prelude


Extensible MACroS

Completions Abound


Gathering Completions

Regexes


Gathering Completions: Continued
Completing the Completion Backend
Dressing Up the Bones

Mode Hooks
Other Files
Fancy Completion


Summary




reStructuredText


At my day job I write our user documentation using Sphinx.
It expects my stilted prose in .rst format, which is kind of like Markdown if you squint.



I do an awful lot of cross-referencing between references (or refs) to link concepts across the documentation.
You define a reference like this:


ReSTFont used for directives and roles.
Font used for all other defining constructs.
.. _code_example:

.. code::
   echo "HELP I'M TRAPPED IN A CODE EXAMPLE"




…and then link to it later like this:


ReSTFont used for field names and interpreted text.
Font used for directives and roles.
This :ref:`doesn't look like anything to me <code_example>`.




…or like this (if the ref is associated with a title of some sort):


ReSTFont used for field names and interpreted text.
Font used for directives and roles.
Don't say :ref:`code_example`.




My problem is that I have an assload of references across the all of the documentation and my brain cannot recall them on the spot.
What I really need is the ability to call up the list of references to easily discover and select from that list – this is basically auto-completion but for documentation headers (or titles).



I am ready to write some shitty elisp with the help of aliens.



A Parentheses Prelude

Before we dig into emacs' guts, here are some principles that I learned after my first elisp experiments that might help somebody digging into this ecosystem for the first time:


1. Emacs Wants You to Extend It


I haven't written plugins for other editors extensively, but I can tell you this: emacs doesn't just make deep customization available, but it actively encourages you to make an absolute customization messes masterpieces.
Core editor functions aren't just documented, but often include tidbits about "you probably want to see this other variable" or "here's how you should use this".



Not only that, but emacs happily hands you functions shaped like nuclear warheads like advice-add (that let you override any function) that can absolutely obliterate your editor if you hold it the wrong way.
Of course, this also grants you unlimited power.



Remember that emacs is designed to be torn apart and rearranged.




2. Geriatric Software


The first public release of GNU emacs happened in 1985.
Literally 40 years of development sits inside of emacs and its developers are still adding non-trivial features (native language server support landed in version 29 in 2023).



The ecosystem is vast and the language has evolved for a long time.
There's nearly always something useful if you need a particular piece of functionality, so even moreso than with other ecosystems: remember to do your homework first.




3. Lisp for for the un-Lisped


The syntax is polarizing, I know.
Gurus will wince when I get this wrong, but:



Writing lisp is like writing any other code, just with the parentheses wrapping everything instead of just arguments. print("Macrodata Refinement") becomes (print "Macrodata Refinement")
Sometimes you don't get functions, you get macros that behave special ways.
For example, let sets variables for an inner block of code.
Like this: (let (name "Mark S.") (print name))
Lispers say "this is actually data and not calling code" by doing this with single quotes: '("list" "of" "strings")



I'm out of my depth in lisp, but if you're a novice, those notes might help.






Extensible MACroS


With that prelude out of the way, let's begin.



Inside of emacs you can call up a list of potential completions by using the keyboard shortcut M-. (that’s "hit the meta key along with period", where "meta" is the Alt key for me).
This applies in a wide variety of scenarios, like when completing class names or variables.
If we want to ask emacs to hand us a list of potential references, then the system we want to hook into is this completions system.



(This is the only time I'll assume we know where to go without crawling through documentation. You could discover it yourself looking for "completion" or similar string in emacs docs).



To start our hero’s journey, we figure out what the hell M-. actually does.
We can ask emacs this by calling the function describe-key, which is bound to C-h k.
Hitting Ctrl-h, then k, then M-. drops us into a help buffer that looks like this:


M-. runs the command completion-at-point (found in
evil-insert-state-map), which is an interactive native-compiled Lisp
function in ‘minibuffer.el’.

It is bound to M-..

(completion-at-point)

Perform completion on the text around point.
The completion method is determined by ‘completion-at-point-functions’.

  Probably introduced at or before Emacs version 23.2.



We have the next breadcrumb to follow, which is the variable completion-at-point-functions.
Running completion-at-point by hitting M-. consults that variable to hand us completion candidates, so we describe-variable it with C-h v and then choose completion-at-point-functions from the list of variables:


completion-at-point-functions is a variable defined in ‘minibuffer.el’.

Its value is (cape-dict cape-file tags-completion-at-point-function)

Special hook to find the completion table for the entity at point.
Each function on this hook is called in turn without any argument and
should return either nil, meaning it is not applicable at point,
or a function of no arguments to perform completion (discouraged),
or a list of the form (START END COLLECTION . PROPS)



…and it goes on from there.
You can see some existing completion functions in there: I use a package called cape to offer helpful suggestions like file paths if I start typing in something like ./filename.



The description for this variable instructs us about how to add our own functions (scary!)
You’ll note that emacs calls this a "hook", which is most often just a term used to describe a variable that is a list of functions that get called at a specific time (hooks show up everywhere).



I elided the full description for completion-at-point-functions – which is lengthy! – but if you parse it all out, you learn the following:



Your completion at point function should return either nil (the elisp "null") – which means your completion function doesn’t apply right now – or another function (which emacs discourages), or a list, which is what we’ll do because it sounds like the most-correct thing to do.
The list we return is (START END COLLECTION . PROPS):

START and END should be positions in the buffer between which emacs will replace the completed symbol with our candidate.
That is, if your cursor is calling a method on a Python object like file.ope| (where the bar is your cursor), emacs will replace just ope when you select open from a list of completions and not the entire file.ope string.
COLLECTION is the juicy bit. The documentation calls it a completion "table", and there’s probably hidden meaning there, but you can just return a list of candidates and move on with your day, which is what I'll do.




Okay, so we need to write something to find the bounds of a string to replace and a function that returns that list.



Completions Abound


I fooled around with some regular expressions for a while until I did the right thing and examined how other completion backends do it.
If you have the package installed, the aforementioned cape-file function gives us a hint: hit M-x, then choose find-function, select cape-file, and poke around. You’ll find the use of a function called bounds-of-thing-at-point.
Describing it with C-h f bounds-of-thing-at-point gives us:


Determine the start and end buffer locations for the THING at point.
THING should be a symbol specifying a type of syntactic entity.
Possibilities include ‘symbol’, ‘list’, ‘sexp’, ‘defun’, ‘number’,
‘filename’, ‘url’, ‘email’, ‘uuid’, ‘word’, ‘sentence’, ‘whitespace’,
‘line’, and ‘page’.



And that is useful for our START and END needs.
You can take it for a test drive at any time with M-: (bounds-of-thing-at-point 'word) to see where emacs thinks the word at your cursor starts and ends.
This is a common theme when developing elisp: try out functions all the time within the editor since they’re near at hand.



The argument to bounds-of-thing-at-point is a symbol for a literal thing that is predefined by the function define-thing-chars.
We pass define-thing-chars a name for our "thing" and a regex, and we can call bounds-of-thing-at-point with it from that point on.
The function documentation in thingatpt.el that emacs refers you to explains more if you’re interested.



define-thing-chars expects a string with characters to put into a regex character class (like [...]) - just any valid character.
This is a pretty standard character class and we can start with something super simple.
I can’t be bothered to look up whatever the reStructedText spec is for references, but let’s start with "word characters, dashes, and underscores".
That expressed as a "thing" looks like this:


ELispFont used to highlight strings.
Font used to highlight keywords.
(define-thing-chars rst-ref "[:alpha:]_-")




Now we have a thing called rst-ref we can use with bounds-of-thing-at-point.
In typical emacs fashion, we can run elisp ad-hoc in our editor just to tinker, so let’s do that now.



Remember: we’re trying to write a function to give us the start and end of whatever piece of text we intend for a completion to replace.
Let’s try it out: in any sort of buffer, put a piece of fake .rst text with a reference, like this:


ReSTFont used for field names and interpreted text.
Font used for directives and roles.
This is a :ref:`other-reference`.




Place your point somewhere within "other-reference" and try out your thing:



M-: (bounds-of-thing-at-point 'rst-ref)



You’ll see something like (number . number) in the echo area (the little minibuffer at the bottom of the emacs window frame).
Congratulations!
We’ve got the first part of the problem solved.





Gathering Completions


Recall the structure of what our "completion backend" needs to return to emacs:


ELisp
(START END COLLECTION . PROPS)




We can construct START and END with bounds-of-thing-at-point, now we just need COLLECTION, which is a list of potential candidates.



Conceptually the task isn’t hard: we should find all instances of strings of the form:


ReSTFont used for all other defining constructs.
.. _my-reference:




in our document and capture my-reference.
Where do we start?



Once again you can rely on discovery mechanisms like searching for functions that sound related (by browsing describe-function) or look at existing code.
Personally, I found this:


(re-search-forward REGEXP &optional BOUND NOERROR COUNT)

Search forward from point for regular expression REGEXP.



The documentation refers you to some other related functions, like this one:


(match-beginning SUBEXP)

Return position of start of text matched by last search.
SUBEXP, a number, specifies which parenthesized expression in the last
regexp.



So we can (re-search-forward) for something then invoke (match-beginning 1), for example, if we used a regex capture group to grab the reference’s label.
Cool: we can start there.



As you get deeper into elisp you’ll find that regular expressions are everywhere, and this case is no different.
We need a solid regex to search through a reStructuredText buffer (and honor any quirks in emacs’ regular expression engine), so we’ll use this opportunity to kick the tires on interactively developing regular expressions in emacs.



Regexes


Geriatric millennial software engineers like myself grew up on https://regexr.com/ when it was still a Flash application.
Unless you’re a masochist that lives and breathes regular expressions, it’s kind of hard to develop a good regex without live feedback, which sites like https://regexr.com/ provide.



Little did I know that emacs comes with its own live regular expression builder and it's goooood.



Within any emacs buffer, run M-x re-builder to open the regex builder window split alongside the current buffer.
If I then enter the string "re-\\(builder\\)" into that buffer, that string a) gets highlighted in my original buffer and b) the capture group gets highlighted in its own unique group color.



You can do this all day long to fine-tune a regular expression, but there’s yet another trick when writing regular expressions, which is to use the rx macro.



My previous example regular expression "re-\\(builder\\)" works, but the quirks when writing emacs regular expressions pile up quickly: escaping characters is one example but there are more, too.



Instead, the rx macro will let you define a regular expression in lisp-y form and evaluate it into a typical string-based regular expression you can use normally, so it works any place emacs expects a string-based regular expression.
For example, if you evaluate this with M-::


ELispFont used to highlight strings.
Font used to highlight keywords.
(rx "re-" (group "builder"))




This is what emacs returns:


ELispFont for backslashes in Lisp regexp grouping constructs.
Font used to highlight strings.
"re-\\(builder\\)"




Identical!
The rx documentation explains all the constructs available to you.



Jumping back to re-builder, with the re-builder window active, invoke M-x reb-change-syntax and choose rx.
Now you can interactively build regular expressions with the rx macro!
In the re-builder window, you’ve got to enter a weird syntax to get it to take rx constructs (I’m… not sure why this is), but you end up with the same outcome:


ELispFont used to highlight strings.
'(: "re-" (group "builder"))




Watch the regex get highlighted live just as it was in the string-based regex mode.



To bring this full circle, hop into a buffer with an example .rst document like this one:


ReSTFont used for all other defining constructs.
Font used for the adornment of a section header.
Default font for section title text at level 1.
A Heading
=========

.. _my-reference:

Link to me!




Using our newfound re-builder knowledge, let’s build a regex interactively to make short work of it:



Invoke M-x re-builder
Change the engine to something easier with M-x reb-change-syntax and choose rx
Start trying out solutions



I’ll refer here to the rx constructs documentation which lists out all the possibilities that you can plug into the rx macro.
Here’s a recorded example of what developing it looks like from start to finish, ending up with a functional rx construct:





Live-highlighting regex development.
Nice.
If you add more groups, more colors show up.
In this example the rx constructs I’m using are:



Any strings end up as literal matches
Special symbols bol and eol for "beginning of line" and "end of line", respectively
Symbols like + behave like their regex counterparts ("at least one")
Some symbols like not are nice little shortcuts (in this case, to negate the next form)



Because rx is a macro, we don’t ever actually need to compile its regular expressions to use elsewhere - we can always just use rx when we need a regex.





Gathering Completions: Continued


Okay, we've cut our teeth on emacs regular expressions.
Let's use 'em.
(Not our teeth. Regexes.)



To start, let's save our reStructuredText regular expression to find a ref so we can easily grab it later.
I'll save the one I came up with to the name tmp/re (this name is arbitrary, I drop temporary variables into tmp/<name> out of habit)


ELispFont used to highlight built-in function names.
Font used to highlight strings.
Font used to highlight keywords.
(setq tmp/re (rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol))




Now we can reference it easily.
I mentioned before that re-search-forward accepts a regex, so let's hop into a reStructuredText rev up the regex.



Here's my sample text that I'll work with:


ReSTFont used for directives and roles.
Font used for all other defining constructs.
Font used for the adornment of a section header.
Default font for section title text at level 1.
A Title
=======

Beware the Jabberwock, my son.

.. _my-reference:

You are like a little baby. Watch this.

.. _code-sample:

.. code:: python

   print("emacs needs telemetry")

The end?




The re-search-forward documentation indicates that it starts at the point's current position, so head to the start of the buffer, hit M-: to enter the elisp Eval prompt, and try:


ELispFont used to highlight built-in function names.
(re-search-forward tmp/re)




This is anticlimactic because you'll just see the point move to the end of one of the references.
BUT.
This means that the search succeeded.
So… what now?



More reading in the re-search-forward documentation will educate you about emacs global match data.
In non-functional-programming style, functions like match-beginning and match-end serve to interrogate a global state that functions like re-search-forward will modify.
In concise terms, our regular expression defines one match group and we can grab it with (match-string-no-properties 1) to get the first group match (match-string will return a string with "properties", which is a bunch of data like font styling that we don't want).



Within our example buffer, executing this after the regex search should return our match:


ELispFont used to highlight function names.
(match-string-no-properties 1)




I see "my-reference" from this command.
Now we're cooking like it's 1985, baby.
You can enter the minibuffer again with M-:, press ↑ to find the re-search-forward command again, and repeat this process again to watch the point move to the next match, after which you can see the matched string with match-string-no-properties.



Note that running this a few times will eventually error out after no matches exist past your point.
We'll address this.



If you're a human (or Claude) at this point, you can see the path ahead – we need to write some elisp that will:



Move the point to the beginning of the buffer (important, remember that re-search-forward relies upon the current position of your point)
Iteratively execute an re-search-forward command to aggregate reference targets
Conclude when there aren't any more matches



I'll start with the code and then explain which demons the parentheses are summoning afterward:


ELispFont used to highlight function names.
Font used to highlight strings.
Font used to highlight special form names.
Font used to highlight built-in function names.
Font used to highlight keywords.


;; This function will save the current position of the cursor and then
;; return it to this position once the code that it wraps has finished
;; executing, which lets us hop around the buffer without driving the
;; programmer insane. Important for any functions that move the point
;; around.
(save-excursion
  ;; progn is a simple function that just executes each lisp form
  ;; step-by-step.
  (progn
    ;; Step one: go to the beginning of the buffer.
    (goto-char (point-min))
    ;; Step two: loop
    ;;
    ;; cl-loop is a macro with a long and venerable heritage stemming
    ;; from the common lisp family of macros, which it mimics the
    ;; behavior of. You could spend hours honing your ability to wield
    ;; the common lisp `loop` macro, but we'll just explain the parts
    ;; we're using:
    ;;
    ;; `while` runs the loop until its argument evalutates to a falsy
    ;; value. We can overload our use of `re-search-forward` here: we
    ;; can use it to step our loop forward each time and also rely
    ;; upon it returning `nil` once it stops matching substrings in
    ;; the buffer and we should finish up.
    (cl-loop while (re-search-forward
                    (rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)
                    ;; The aforementioned `while` termination case
                    ;; relies upon this `t` parameter, which says
                    ;; "don't error out with no matches, just return
                    ;; nil". Once no more matches are found, the loop
                    ;; exits.
                    nil t)
             ;; The `collect` keyword instructs `cl-loop` how to form
             ;; its return value. We can helpfully summarize the regex
             ;; match item by pulling out the global match data.
             collect (match-string-no-properties 1))))




The code is less intimidating without comments:


ELispFont used to highlight function names.
Font used to highlight strings.
Font used to highlight special form names.
Font used to highlight built-in function names.
Font used to highlight keywords.
(save-excursion
  (progn
    (goto-char (point-min))
    (cl-loop while (re-search-forward
                    (rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)
                    nil t)
             collect (match-string-no-properties 1))))




Without belaboring the point, you can – like I did – discover most of these functions by skimming existing elisp code and using it as a launch pad.
Many of these functions are bog standard and show up all over the place in emacs packages (save-excursion, progn, goto-char…)



Here's the result when I run this code against our example .rst file:


ELispFont used to highlight strings.
("my-reference" "code-sample")




Looks good!




Completing the Completion Backend


We're now armed with the ability to:



Identify the bounds of the string we want to replace, and
Collect a list of targets for completion candidates



We are so close.
Recall the description of the variable we need to modify:


completion-at-point-functions is a variable defined in ‘minibuffer.el’.

Its value is (cape-dict cape-file tags-completion-at-point-function)

Special hook to find the completion table for the entity at point.
Each function on this hook is called in turn without any argument and
should return either nil, meaning it is not applicable at point,
or a function of no arguments to perform completion (discouraged),
or a list of the form (START END COLLECTION . PROPS)



To return the list that completion-at-point-functions expects, we already have the ability to identify the bounds of a thing and sweep up a list of candidates in our buffer.
Note the comment about returning nil: we probably don't always want to run our backend, so we should short-circuit our function to eagerly return nil to avoid tying up emacs with a regex loop we don't need.



With all that said, consider the following:


ELispFont used to highlight special form names.
Font to highlight quoted Lisp symbols.
Font used to highlight built-in function names.
Font used to highlight function names.
Font used to highlight documentation embedded in program code.
It is typically used for special documentation comments or strings.
Font used to highlight function names.
Font used to highlight strings.
Font used to highlight keywords.


;; Our reStructuredText reference "thing"
(define-thing-chars rst-ref "[:alpha:]_-")

(defun my/rst-internal-reference-capf ()
  "Completion backend for buffer reStructuredText references"
  ;; Only applies when we're within a reference - outside of a
  ;; reference, we bail out with nil.
  (when (looking-back (rx ":ref:`" (* (not "`"))) (point-at-bol))
    ;; Get potential bounds for the string to replace
    (let* ((bounds (or (bounds-of-thing-at-point 'rst-ref)
                       ;; Fallback to the current position
                       (cons (point) (point))))
           (start (car bounds))
           (end (cdr bounds))
           ;; Collect all reference candidates
           (candidates
            ;; Our previously-noted reference collector
            (save-excursion
              (progn
                (goto-char (point-min))
                (cl-loop while (re-search-forward
                                (rx bol ".." (+ blank) "_" (group (+ (not ":"))) ":" eol)
                                nil t)
                         collect (match-string-no-properties 1))))))
      ;; Return value suitable for `completion-at-point-functions`
      (list start end candidates))))




We're following some naming conventions by calling this a "capf" (a "completion-at-point function) and prefixing with my/ (a habit to namespace your own functions)
Our short-circuit takes the form of using looking-back to ask, "are we inside of a reStructuredText reference"?
Note the use of rx here again to clean up our lisp.
We use our rst-ref thing to easily snag the start and end of the string to replace – note our fallback to just the immediate point if we can't find the bounds of our thing.



We wrap it all up with list.
Personally, even as somebody relatively new to writing Lisps, I find the code pleasant to read and self-evident.
We did a lot in 17 lines of code!



Inside of our test .rst buffer, we can test drive this function.
First, invoke M-x eval-defun with your cursor somewhere in the function to evaluate it, which makes my/rst-internal-reference-capf available.
Then run:


ELispFont to highlight quoted Lisp symbols.
Font used to highlight variable names.
Font used to highlight function names.
(add-hook 'completion-at-point-functions 'my/rst-internal-reference-capf)




Huzzah!
Our function is now live in emacs' completion framework.
You can trigger the completion by calling completion-at-point at a relevant spot in a buffer.
Many batteries-included emacs distributions like spacemacs or doom emacs slap nice-looking porcelain on top of the completion framework; here's an example that uses the corfu package:





Congratulations, you've extended emacs for the first time!




Dressing Up the Bones

Okay, this is a pretty basic setup.
You could improve it in many ways, but here are a few ideas about potential directions:


Mode Hooks


Manually adding your custom completion function to the completion-at-point-functions hook is tedious, but there's a way to automate it.
Recall that in emacs parlance, a "hook" is usually a variable that holds a list of functions that get called at a specific time.



If you use rst-mode, then opening an .rst file will drop you into rst-mode and implicitly call the rst-mode-hook functions.
That means that this line is sufficient to integrate our completion function:


ELispFont to highlight quoted Lisp symbols.
Font to highlight Lisp quotes.
Font used to highlight keywords.
Font used to highlight variable names.
Font used to highlight function names.
(add-hook 'rst-mode-hook (lambda () 
    (add-hook 'completion-at-point-functions  #'my/rst-internal-reference-capf 0 t)))




This says: "when I open an .rst file, run this lambda that modifies completion-at-point-functions only for this buffer by adding my internal reference completion function".
It's a little nested which makes it less obvious with the two add-hook calls.




Other Files


Okay, our example works for references in the same buffer but this is sort of pointless for uses across files.



You can solve this too, although my post is already too long so we won't solve this step-by-step.
However, here's how I solved it:



Turn my capf into a minor mode that manages the completion variables
Doesn't search the buffer every time but instead does so once and then rebuilds it with a hook in after-change-functions, saving it to a hash cache
Walk all .rst files in the current project and run the reference collection function for each, storing the results into a hash cache for all files that don't have live buffers
When it comes time to call the completion function, combine the hash for completions for files without buffers along with each .rst buffer's cached list of references



It sounds complicated, but it works!
Functions like with-temp-buffer make this pretty easy by aggregating reference targets for files using the exact same function we do for live buffers.


ELispFont used to highlight built-in function names.
Font used to highlight keywords.
(with-temp-buffer
  (insert-file-contents file)
  (my/rst-internal-references))





Fancy Completion


Emacs' long history includes company-mode, which is a third-party completion framework that integrates with the completion-at-point set of functions.
Some company-mode features include additional metadata about completion candidates, and I found two that were useful: company-kind and company-doc-buffer.



company-kind is a simple key that just tells the completion caller what the completion cadidate is.
In our case we can add some eye candy by indicating it's 'text.
company-doc-buffer lets us add additional context to a completion candidate.
I leveraged this to include a couple of lines following the reference line to help me figure out what exactly the link refers to.
It's easier to show what this looks like rather than tell:





Notes:



I'm using GUI emacs here for the nicer completion popup with corfu which displays a transparent, floating frame
My completion candidate "context" is a real excerpt from the text around the reference, complete with styling, etc.
The small icon to the left of each candidate comes from the company-kind attribute.
The ~ syntax is part of orderless



Completion candidate context is an extra frill but very helpful.





Summary

My experience extending a core emacs function was an instructive and interesting exercise.
I don't know what the future of emacs looks like in an increasingly LLM-crazed world, but I hope that future includes an open and powerful way to extend and customize the tools we use to write software.
















        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: DWS OS, a Plan 9 Inspired Web "OS"]]></title>
            <link>https://dws.rip</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45223053</guid>
        </item>
        <item>
            <title><![CDATA[Doom-ada: Doom Emacs Ada language module with syntax, LSP and Alire support]]></title>
            <link>https://github.com/tomekw/doom-ada</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222993</guid>
            <description><![CDATA[Doom Emacs Ada language module with syntax highlighting, LSP and Alire support - tomekw/doom-ada]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    



  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  





    






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    0

  

  
        
            
          Star
          5

  



        

        


          

  
    


  

  




          


  
  
  Folders and filesNameNameLast commit messageLast commit dateLatest commitHistory2 CommitsLICENSELICENSEREADME.mdREADME.mdconfig.elconfig.eldoctor.eldoctor.elpackages.elpackages.elREADMEMIT licenseDoom Emacs Ada Module
This is a Doom Emacs :lang ada module providing:

Tree-sitter highlighting via ada-ts-mode
LSP support with ada_language_server
Autocomplete (company-capf)
Alire integration (alr build, alr run, alr clean)

Installation
Clone into your Doom modules folder:
git clone https://github.com/tomekw/doom-ada ~/.doom.d/modules/lang/ada
Enable in ~/.doom.d/init.el:
:lang
ada
Sync Doom:
doom sync
Restart Emacs.
Usage

SPC m b → build with alr build
SPC m r → run with alr run
SPC m c → clean with alr clean

Errors are parsed into the compilation buffer, and eglot provides inline diagnostics and completions.
Requirements

Alire
Ada Language Server





      




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oq: Terminal OpenAPI Spec Viewer]]></title>
            <link>https://github.com/plutov/oq</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222799</guid>
            <description><![CDATA[Terminal OpenAPI Spec viewer. Contribute to plutov/oq development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[oq - a terminal-based OpenAPI Spec (OAS) viewer

Usage
oq openapi.yaml
# or
cat openapi.yaml | oq
# or
curl https://api.example.com/openapi.json | oq
Keyboard Shortcuts

↑/↓ or k/j - Navigate up/down through items
Tab - Switch between Endpoints and Components views
Enter or Space - Toggle fold/unfold for endpoint and component details
q or Ctrl+C - Quit the application

OpenAPI Support
oq supports both modern major OpenAPI specification versions:

OpenAPI 3.0.x
OpenAPI 3.1.x

Both JSON and YAML formats are supported.
Installation
From Source
git clone git@github.com:plutov/oq.git
cd oq
go build -o oq .
License
MIT License - see LICENSE file for details.
Contributing
Contributions are welcome! Please feel free to submit issues and pull requests.
When contributing:

Ensure tests pass: go test -v
Test with both OpenAPI 3.0 and 3.1 examples

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Active phishing campaign targeting crates.io users]]></title>
            <link>https://blog.rust-lang.org/2025/09/12/crates-io-phishing-campaign/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222777</guid>
            <description><![CDATA[Empowering everyone to build reliable and efficient software.]]></description>
            <content:encoded><![CDATA[
      We received multiple reports of a phishing campaign targeting crates.io users
(from the rustfoundation.dev domain name), mentioning a compromise of our
infrastructure and asking users to authenticate to limit damage to their crates.
These emails are malicious and come from a domain name not controlled by  the
Rust Foundation (nor the Rust Project), seemingly with the purpose of stealing
your GitHub credentials. We have no evidence of a compromise of the crates.io
infrastructure.
We are taking steps to get the domain name taken down and to monitor for
suspicious activity on crates.io. Do not follow any links in these emails if you
receive them, and mark them as phishing with your email provider.
If you have any further questions please reach out to security@rust-lang.org
and help@crates.io.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Crates.io Phishing Attempt]]></title>
            <link>https://fasterthanli.me/articles/crates-io-phishing-attempt</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222772</guid>
            <description><![CDATA[Earlier this week, an npm supply chain attack.
It’s turn for crates.io, the main public repository for Rust crates (packages).
The phishing e-mail looks like this:
And it leads to a GitHub login pa...]]></description>
            <content:encoded><![CDATA[
  
    

    
    

  
    
    

    
    

    

    

    
    

    
    
     
     

    
        




     

    

    
    



    

    

    

    Earlier this week, an npm supply chain attack.

It’s turn for crates.io, the main public repository for Rust
crates (packages).

The phishing e-mail looks like this:

Andrew Gallant on BlueSky

And it leads to a GitHub login page that looks like this:

Barre on GitHub

Several maintainers received it — the issue is being discussed on GitHub.

The crates.io team has acknowledged
the attack and said they’d see if they can do something about it.

No compromised packages have been identified as of yet (Sep 12, 14:10 UTC).

Important links:

Rust Security Response WG blog post.
GitHub discussion about the attack



    

    

    

    
    
    

    
        
            (JavaScript is required to see this. Or maybe my stuff broke)
        
     

    



    
        
        
            
                Here's another article just for you:
            
            

    





        
            
                

                
                    Cracking Electron apps open
                
            
         
          I use the draw.io desktop app to
make diagrams for my website. I run it on an actual desktop, like Windows or
macOS, but the asset pipeline that converts .drawio files, to .pdf, to
.svg, and then to .svg again (but smaller) runs on Linux.

So I have a Rust program somewhere that opens headless chromium, and loads just
the HTML/JS/CSS part of draw.io I need to render my diagrams, and then use
Chromium’s “print to PDF” functionality to save a PDF.
















    
        
    
  
  

    
    
    
        
    
    

  
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Many Hard LeetCode Problems Are Easy Constraint Problems]]></title>
            <link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222695</guid>
            <description><![CDATA[Use the right tool for the job.]]></description>
            <content:encoded><![CDATA[
            
            
                
                
                September 10, 2025
                
                
            
            

            

            
            
                Use the right tool for the job.
            
            

            

            
            
            In my first interview out of college I was asked the change counter problem:

Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).

I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were [10, 9, 1], then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (10+9+9+9). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.
But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like MiniZinc and call it a day. 
int: total;
array[int] of int: values = [10, 9, 1];
array[index_set(values)] of var 0..: coins;

constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total;
solve minimize sum(coins);

You can try this online here. It'll give you a prompt to put in total and then give you successively-better solutions:
coins = [0, 0, 37];
----------
coins = [0, 1, 28];
----------
coins = [0, 2, 19];
----------
coins = [0, 3, 10];
----------
coins = [0, 4, 1];
----------
coins = [1, 3, 0];
----------


Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.1 Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.
More examples

This was a question in a different interview (which I thankfully passed):

Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.

It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
var int: buy;
var int: sell;
var int: profit = prices[sell] - prices[buy];

constraint sell > buy;
constraint profit > 0;
solve maximize profit;

Reminder, link to trying it online here. While working at that job, one interview question we tested out was:

Given a list, determine if three numbers in that list can be added or subtracted to give 0? 

This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver; 
include "globals.mzn";
array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array[index_set(numbers)] of var {0, -1, 1}: choices;

constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0;
constraint count(choices, -1) + count(choices, 1) = 3;
solve satisfy;

Okay, one last one, a problem I saw last year at Chipy AlgoSIG. Basically they pick some leetcode problems and we all do them. I failed to solve this one:

Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.


The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:
array[int] of int: numbers = [2,1,5,6,2,3];

var 1..length(numbers): x; 
var 1..length(numbers): dx;
var 1..: y;

constraint x + dx <= length(numbers);
constraint forall (i in x..(x+dx)) (y <= numbers[i]);

var int: area = (dx+1)*y;
solve maximize area;

output ["(\(x)->\(x+dx))*\(y) = \(area)"]

There's even a way to automatically visualize the solution (using vis_geost_2d), but I didn't feel like figuring it out in time for the newsletter.
Is this better?
Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the capability/tractability tradeoff. But even so, they'll do way better than a bad bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.
The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(n²) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to

Maximize the profit by buying and selling up to max_sales stocks, but you can only buy or sell one stock at a given time and you can only hold up to max_hold stocks at a time?

That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:
include "globals.mzn";
int: max_sales = 3;
int: max_hold = 2;
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array [1..max_sales] of var int: buy;
array [1..max_sales] of var int: sell;
array [index_set(prices)] of var 0..max_hold: stocks_held;
var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]);

constraint forall (s in 1..max_sales) (sell[s] > buy[s]);
constraint profit > 0;

constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] <= i) - count(s in 1..max_sales) (sell[s] <= i)));
constraint alldifferent(buy ++ sell);
solve maximize profit;

output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"];


Most constraint solving examples online are puzzles, like Sudoku or "SEND + MORE = MONEY". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.




Because my dad will email me if I don't explain this: "leetcode" is slang for "tricky algorithmic interview questions that have little-to-no relevance in the actual job you're interviewing for." It's from leetcode.com. ↩



            
            

            
            
            If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.
My new book, Logic for Programmers, is now in early access! Get it here.
            
            

            





        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[3D Modeling with Paper]]></title>
            <link>https://www.arvinpoddar.com/blog/3d-modeling-with-paper</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222369</guid>
            <description><![CDATA[Exploring the process of designing and assembling 3D models from paper.]]></description>
            <content:encoded><![CDATA[August 31, 2025

Over the past several years, I've enjoyed the hobby of paper
modeling (or papercraft), the art
of creating 3D models from cut and glued parts from paper sheets. This hobby is
a superset of origami, in that it allows for cutting and gluing, as well as for
multiple sheets of paper for a single model. The alleviation of these
constraints means that papercraft allows for more complex models that are easier
to assemble.
Over many years, I've built models designed by others as well as designed my
own. In this post, I want to share everything I've learned along the way,
covering the entire process from design to assembly.
I love this hobby for three reasons:

It is extremely accessible. There is no fancy hardware or software
involved. As we'll see, the core tools are paper, scissors, and glue;
everything else is an addon to make the experience better. All software tools
can be free. Accidentally messed up during assembly and need a replacement
part? Just print out another page. The entire creation of a model can be done
in the ballpark of a few cents.
It is equally technical and creative. As we'll see, many of the problems
faced in papercraft require an engineering-like approach and a willingness to
experiment and iterate on designs. While it may appear outwardly like a craft
project, the end-to-end process involves constraints and optimizing within
them.
There's no limits on what you can make. What you decide to build is
limited by your patience and imagination. Theoretically, nearly any object can
be represented as a paper model.

Let's dive in. My most recent model is a papercraft plane inspired by the SR-71
Blackbird, a
reconnaissance plane that to this day holds many records for being one of the
fastest aircrafts ever. It's now one of most iconic planes ever designed and an
engineering masterpiece. The program was ultimately retired in 1999.
The model we'll be desiging and assembling in this post
An actual SR-71 Blackbird, By USAF / Judson Brohmer - Armstrong Photo Gallery, Public Domain, https://commons.wikimedia.org/w/index.php?curid=30816
We're going to walk through the full model design and assembly process, while
referencing specific examples I encountered during creating this SR-71.
Constraints#
Let's set some constraints for how we're allowed to model our creation. These
are self-imposed limitations that fit my preferred-style for model design:

All parts in the assembled model must be made of paper.
Each part must be a single, solid color. The parts must not use any printed
textures or designs.
The model must be represented as a simple
polyhedron. There may be
no curvatures, holes, two-dimensional surfaces, or surface-to-surface contact.
If the figure we're trying to capture has any of these features, we must find
a way to approximate it using only flat faces. The object must be manifold (an
edge is only shared by 2 faces).

Why constraints?#
It may feel weird to impose constraints on an art. However, I find that these
constraints encourage a better designed model that can be assembled easily and
predictably, including by others.
Using features like curvatures, printing with textures, etc. are shortcuts. For
example, printing textures helps fill in details that aren't captured inherently
by the model; curvatures and 2d surfaces are flimsy and introduce variances in
how a model can be assembled. Simply polyhedral designs with single color parts
ensure that the 3D form itself captures the object being depicted, and can be
assembled in a structurally sound, predictable way.
Goals#
In addition to constraints, we also have some goals that we're optimzing for.
These goals will be considered in each step of our design process.

Ease of assembly: By far the most important goal, our model should be
easy to put together. Given the nature of paper and glue, a model that is
difficult to assemble will almost certainly look bad. A model can have a
well-designed topology, but still be difficult to assemble based on the parts
design we put together.
Aesthetic appeal: This is an art, after all. The model we design should
be aesthetically pleasing and resemble the object of interest.
Minimal consumption of resources: We should aim to minimize waste and use
our materials efficiently.

As in engineering, we have to consider trade-offs between these goals, and
optimize for these goals within our constraints.
Steps#
The process of designing a paper model is iterative. Each iteration consists of
the following steps:

Mesh modeling - using software to create a 3D polyhedron mesh of our desired form
Mesh unfolding - unfolding the mesh into a 2D layout of parts
Assembly - putting the parts together to create the final model

The remainder of this article will be walking through each step in detail. The
discussion of each step will be centered around the goals and constraints
declared from above.
Mesh Modeling#
Related goals: Ease of assembly, aesthetic appeal
In this phase, we design the mesh for our model. We aim to capture the essence
of an object in a way that can feasibly be built with paper. Depending on how
you approach this, this can easily be the most complicated step.
What do I mean by "feasibly built with paper"? Our mesh is a collection of
polygons that represent a 3D object. The closeness of that representation is
largely determined by how many polygons we use. We could use many really small
polygons to closely match the subtle curves of our plane, but this would be hard
to assemble in reality. Alternatively, we could simplifiy our representation
down to a triangular pyramid. This would be trivially easy to assemble, but it
wouldn't look a lot like our plane.
We can now see that our goals of ease of assembly and aesthetic appeal are at
odds. Imagine that we have a continuum, where on the left we have a triangular
pyramid (the simplest possible polyhedron) and on the right we have a mesh of
the SR-71 with an arbitrarily high number (millions) of polygons.
Our mesh can exist anywhere between the simplest polyhedron and a mesh with near perfect resolution. The example on the right is by USSIowa on Thingiverse (Creative Commons - Attribution): https://www.thingiverse.com/thing:5508640
Generally, an "easy" to assemble model will have somewhere around a few hundred
polygons. Thus, our ideal model exists somewhere on the far left of this
spectrum.
The challenge here is what I call "allocation of resolution" - we have a finite
number of polygons to distribute across the features of our object. Certain
features will naturally require more polygons to be accurately captured than
others. For example, curved features require more polygons than flat features -
in this model, the cylindrical engines will require more detail, than say, the
flat wings.
In addition to the number of polygons and their concentrations, the arrangement
of the polygons themselves matters - this is the topology of the mesh. Most
discourse on 3D mesh topology is related to shading and animation. For our
purposes, we're considered with ease of assembly. Certain topologies are easier
to assemble and more structurally sound. Generally, here's some positive
topological qualities for papercraft:

Symmetries: a good mesh design is symetrical when possible. Symmetrical shapes
are intuitive and easier to reason about when assembling.
No narrow shapes: really narrow shapes are hard to cut out, hard to fold, and
hard to glue. Avoid them at all costs.
Use quads: quad faces have an aesthetic appeal to them.

If all of this is sounding hard, we've got some options, in increasing order of
difficulty:
Easy: Use an existing mesh#
The easiest way past this step is to find an existing mesh. There's a whole
genre of 3D modeling called "low-poly" that you can find with a quick search on
Thingiverse or
Printables. These are
usually designed for video games or 3D printing, but can be taken up for
papercraft.
Medium: Converting an existing mesh#
Sometimes, you can find a high-resolution mesh of your desired object, but not a
low-poly one. In this case, there are tools available to reduce the polygon
count while preserving the overall shape. This is called "mesh simplification"
or "mesh decimation."
This Instructable
goes over the process of doing this with Meshlab,
but there's many other software alternatives out there.
The pitfall of this approach is that automatic mesh decimation typically results
in some nasty topologies, and there's not a lot you can do to control the
output. To get around this, we could add an additional refinement step where
we take the raw decimated mesh output and "clean it up" using a mesh editor
software.
As an example, let's try this with a SR-71 mesh on
Thingiverse. The original mesh has
more than 1.2 million faces, and we're going to try decimating down to ~1,000.
Here's what we get from Meshlab:
Result of mesh decimation in Meshlab
In this case, the output is not usable - it's wildly asymetric and is full of
self-intersections. Refining this topology would take just as long (if not
longer) as creating a model from scratch.
Hard: Creating your own mesh#
The most difficult option is to create your own mesh from scratch. This option
gives you full control over the design, and is what I chose for the SR-71 model.
My software of choice for this is Blender. Blender
has a steep learning curve, but the type of mesh design we're doing for this
project doesn't begin to scratch the surface of its full capabilities. I highly
recommend this low-poly tutorial
if you've never used Blender before and need somewhere to start. Two things I
found very handy were the mirror
modifier
to enforce symmetry, and the 3D Print
Toolbox
to auto-cleanup the mesh and check for manifoldness.
This process is very tedious. My advice here is: simplify your mesh to the point
where you feel uncomfortable. Recall that we're largely optimizing for ease of
assembly. When modeling, it's very tempting to capture finer details, but fine
details have costs (small parts, hard to glue regions, etc.) that are not worth
it during the assembly phase. Scrutinize every feature, and zoom out once in a
while. When you zoom out, your omissions won't feel as weird.
After many days, here's the initial mesh I created. It contains 732 triangles.
Note the symmetry along the y-axis.
Initial mesh created with Blender
Mesh Unfolding#
Related goals: Ease of assembly, minimal consumption of resources
Once we have a mesh, we have to convert it into a 2D template of parts that can
be printed and assembled. This process is called unfolding. Each of the
faces of our mesh are grouped into parts, and the arrangement of our parts
is a layout, or template.
To do this, we're going to turn to software again. The most popular unfolding
tool (and my favorite) is Pepakura
Designer. Pepakura is not free (at the
time of this writing, it's a one time $70 purchase) and it only runs on
Windows. There's also Unfolder for Mac, which is
$30. If you can't use either of these, Blender can save the day again with its
free Paper Model
plugin.
I believe that the unfolding step is one that does not get as much attention as
it deserves. There is a noticable difference between a good template and a bad
one. A good template has parts that make intuitive sense, with logical groupings
and clear flow. The faces themselves are grouped into parts that are easy to cut
out and handle. All of this equates to a better building experience, which means
a better looking model.
Part of unfolding is also deciding the scale of your model. You can make your
model as big or small as you want, but again, ease of assembly should be top of
mind when deciding. A model that's too small will end up with parts that are
hard to cut out and fold. Bigger models are easier to assemble, but you're
limited to the point where the faces of your model must fit on a page.
I ended up making this model 25 inches long. With the original SR-71 being about
107 feet long, this puts our model at around a 1:50 ratio.
Creating many parts#
Let's start off with the creation of parts. In most unfolding software, the
software will auto-unfold for you, and from there you can regroup faces into
whatever parts you want. Here's Pepakura's auto unfold:
Pepakura default unfolding produces complex parts
The parts it generated are pretty complicated, so we have some work to do.
If you have a mesh with nn faces, you can have anywhere from 1 (all the faces
in a single part) to nn total parts (each part is a single face). We want our
model to be easy to assemble, and neither of these extremes are easy.
Rather than trying to fix the number of parts and going from there, I recommend
creating parts that are logical. Identify features that can be captured in a
single part, and go from there. For example, in the SR-71, each engine intake
spike makes sense as a single part. So does the nose cone.
If your mesh has an axis of symmetry, then your parts have symmetrical pairings
as well. The same feature on either side of the axis should be represented with
a mirrored part. In the SR-71, the entire plane is symmetrical on the vertical
axis, so all parts across this axis are mirrored. This is good because once
someone builds one side, they can more easily reason about the other side.
I ended up dividing this model into 42 parts. These parts were carefully divided
in such a way that I felt would make them easier to assemble. If you look at any
part in particular, chances are it'll have a symmetric counterpart.
Finalized parts for the layout
They're arranged pretty haphazardly right now, but we'll cleanup this up in the
next step.
Arranging the parts#
Again, most software will automatically arrange the parts for you as part of
unfolding. Here's the 14 page arrangement Pepakura decided for the parts I created:
Auto arrangement of parts by Pepakura
I highlighted all the parts on the first two pages so you can see where the are
on the finished model. Notice that they're scattered throughout different
sections. That's why I typically don't like auto-arrangement - they're designed
to minimize paper usage, but they often result in a less intuitive assembly
process. You can't look at any particular page and loosely know where its parts
will go.
A good part layout reads like a story. Parts are arranged in a logical
order, with related parts grouped together. I like to arrange mine left to
right, top to bottom on a page. Here's my layout, with the first two pages
highlighted.
Manual arrangement of parts, which now has logical groupings
All the parts that are near each other in the layout are also near each other in
the final assembly. In this case, I even was able to reduce the page count down
to 12 from the starting 14.
Flap structure#
Flaps, or tabs, are the appendages on each part that allow for gluing
parts together. Each flap has a singular counterpart edge that it's glued to -
this is known as an edge/flap pair. Most software will auto-assign a shared
number between an edge and its flap to make identify pairs easy during the
assembly process.
Two edge/flap pairs, with arrows pointing to their matching number IDs
For an edge/flap pair, most unfolding software will allow us to swap the flap
across parts. Doing this strategically is critical for creating an easy to
assemble model, and also has implications for the structural integrity of the
final build.
For example, consider the two example parts shown above. These two parts that
meet at two shared edges, so these parts have two edge/flap pairs between them.
We could arrange the flaps so that one part has both of them:
Arranging both flaps on the same side
We could also interlace the flaps, so each part has one flap on each side.
Arranging flaps as interlaced, with one flap on each part
Interlacing flaps between parts can create a more stable structure, since
there's only one way for the parts to meet. If two flaps are on the same side,
they can over-extend when glued to the edge. That being said, same-side flaps
can be easier to work with, especially when reaching the closing stages of a
model.
In general, I like to using interlaced flaps wherever possible to create an
overall stronger model, and use same-side flaps selectively.
Once we have an arrangement we like, we can export our layout as a PDF.
Assembly#
With our layout PDF ready, we can now print it and move on to assembly. We'll
finally get to see our design come to life.
Materials and Tools#
For our materials, we'll need:

65lb (176 g/m2g/m^2) cardstock: This is the ideal paper weight for creating
sturdy models, while still being thin/flexible enough to pass through a normal
printer and be easy to fold.
Adhesive. My recommended adhesive is tacky glue: it's strong, dries clear, but
is forgiving enough to allow for repositioning during assembly. Specifically,
I use Aleene's Original Tacky
Glue.
I've also had past success with a glue stick.

We'll also need some tools, which I've listed these in order of importance. The
ones with asterisks are essential. Everything else is a nice-to-have.

Printer*: You'll need access to a printer to print the template on the
cardstock. Laser jet printers are great because the prints don't smudge.
Cutting tools*: You'll need a pair of scissors or a craft knife to cut
out the parts. Use sharp tools for clean cuts - it makes a difference.
Ruler*: Cutting/scoring perfectly straight lines is a must. Steel rulers
are great for their consistent edge, and they don't catch against your tools.
That being said, I used a clear plastic ruler for this model. Being able to
see through the ruler helps with alignment.
Scoring tool*: This will help you prepare a part for folding. You can use
a bone folder or scoring wheel. I use an embossing tool I found at a dollar
store, but before that, I used a ballpoint pen than ran out of ink. Anything
with a precise (but not too sharp) tip will do.
Toothpicks: I use toothpicks to spread blobs of glue into thin layers and
get into tight spaces.
Assembly surface: A cutting mat or piece of cardboard will protect your
work surface and give you a stable surface to cut/score your parts.
Tweezers: Tweezers are helpful for handling small parts and getting into
tight spaces, especially while holding parts together as glue dries.

If you want to get fancy, you can also purchase an automatic cutting machine,
like a Cricut or
Silhouette. These machines can precisely
cut/score your parts from cardstock. Getting the template into their software
takes some extra effort, but it results in the best quality parts. I did not use
a machine for this project.
To match the real SR-71, I printed my template on black cardstock. Darker
cardstocks are harder to work with because of the low contrast between the ink
and the paper itself. If you're new to the hobby, I would recommend starting
with a lighter color.
Assembly phases#
The assembly of a model has 4 steps:

Cutting: Cutting the parts out from the paper with your cutting tool of
choice. Scissors are quicker, but the combination of ruler and craft knife
results in cleaner cuts.
Scoring: Running a scoring tool over fold lines to get cleaner folds. This
may be tempting to skip, but I cannot emphasize the importance of this step
enough. Scoring is especially important when dealing with thicker paper.
Folding: Folding the parts in prep for gluing. There's only two types of
folds: mountain folds and valley folds.
Gluing: Gluing the parts together.

How you decide to batch these steps is up to you. For example, you could cut all
the parts out at once, then score all of them, etc. This approach is effective
because you can develop a rhythm by doing each phase only once, so you're not
constantly switching between tools; the downside is that you only get to start
assembly after a pretty lengthy process. Alternatively, you can do it per part:
cut one part out, score it, fold it, and secure it to the assembly. Here, the
pros and cons are flipped: you get to see the model come together quicker, but
there's a lot of context switching between phases. I've tried both of these
approaches, and find that the latter results in a non-negligible increase in the
assembly time of the model.
To strike a balance, the approach I took for this model was performing the
phases at the granularity of sections (engines, wings, fuselage, etc.) of the
model. This approach has the added final step of assembling all the standalone
sections together into the final model.
Here's some pictures I took during the assembly process. In total, assembly took
6-8 hours.
An part cut from the template — this part is for one of the elevons.
The same part from above, but now scored. Note the visible impressions on the fold lines.
All the parts to make one of the engine/wings, cut and scored.
Beginning to assemble the engine (see completed inlet spike).
Both engines and partial wings, fully assembled.
The assembled nose cone and cockpit.
Bottom view of assembling the engines to the main fuselage.
Top view of assembling the engines to the main fuselage.
Tips#
Use little glue: When gluing parts together, apply as little glue as
possible. Using too much glue will result in spillover when the flaps/edges are
put together, and this spillover is hard to wipe away from a porous surface like
paper. Too much glue can even result in subtle paper warping. In the recommended
tools, I suggested a toothpick. I apply a small bead of glue to a flap and use
the toothpick to spread it into a thin film. This prevents any spillage and
keeps the model clean.
Start in complex areas: As you progress further in gluing parts together,
the degrees of freedom of your model will reduce. This is why I recommend
starting with more complicated areas of your model where you'll need those
degrees of freedom. In this model, this meant starting with precise features,
like the engine inlet spikes or the vertical stabilizers.
Finish in hidden areas: This goes hand in hand with the tip above. As you
reach to the end of your model, gluing the final parts together can be very
hard, which means the final edges may come out a bit sloppy. Why does this
happen? Any minor imperfections we made throughout the assembly process result
in stresses in our model that will be felt at the end. Gluing the last part may
be challenging because it'll feel misaligned, and it has the added challenge of
attempting to close a 3D object from the outside. That's why I always recommend
choosing an assembly order that results in the last parts being glued in an area
that is out of sight. For the SR-71, that happens to be the underside of the
fuselage.
Final Model#
Here's the final model, displayed on a stand (also made from paper):
Top view of finished paper model
Side view of finished paper model, on its stand
Iteration#
No matter how much you scrutinize the modeling and layout phases, you will
inevitably find areas for improvement as you assemble. In the case of the SR-71,
I spotted a few minor assymetries in part tabs, and more importantly, an
opportunity to reduce face count by simplifying the topology of the bottom of
the plane and the nose cone.
I took my mesh back into Blender, and was able to get the triangle count down to
636, which is almost a full 100 faces fewer than the original mesh.
Second mesh iteration in Blender.
Below, you can see the old mesh (left) next to the new mesh (right). It's hard
to tell the difference, yet the new one has almost 15% fewer faces.
First mesh iteration (left) vs. latest mesh iteration (right).
A faster way to iterate is to render the model rather than physically building
it. This allows you to quickly identify and fix visual issues without going
through the hours of assembly. Here's some renders (in Blender) of the final
iteration:
Blender isometric-view rendering of final mesh.
Blender front-view rendering of final mesh.
Conclusion#
In total, the full cycle of designing the mesh, creating the parts layout,
assembly, and subsequent refinement iterations occurred over the course of a few
months. The process is long, but the results are well worth it.
If you're interested in making this model yourself, you can download the PDFs for
the first iteration of the model below. I've included a template for the stand as well.

SR71 Template
SR-71 Stand Template

Hope you enjoy!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Over 100 ships have sailed with fake insurance from the Norwegian Ro Marine]]></title>
            <link>https://www.nrk.no/vestland/xl/over-100-ships-have-sailed-without-legitimate-insurance-from-the-norwegian-company-ro-marine-1.17565216</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221996</guid>
            <description><![CDATA[­]]></description>
            <content:encoded><![CDATA[
All large ships must have insurance, and Ro Marine has provided this in a big way.
The Norwegian company did not have permission to sell insurance but did it anyway — to clients worldwide.
In March, we reported how select ships used false papers to deceive inspectors in NATO countries.
Now, NRK and Dossier Center can reveal how extensive and global the fraud was: over 100 ships have sailed with illegitimate insurance documents from Ro Marine.
“It's very serious and unusual that such a serious fraud happens with the help of a Norwegian company. At worst, it could undermine the trust in the Norwegian maritime industry,” says Thomas Angell Bergh from the Norwegian Maritime Authority.

Most of the fake insurance papers were for ships transporting goods out of Russia, mainly oil.
NRK has contacted dozens of Ro Marine’s clients. Only a few were willing to speak with us.
One of the customers says he was scammed by Ro Marine because he believed the insurance policies he purchased were valid.
Another says that "everyone" knows Ro Marine is fake, but that the ships need Western insurance documents. Such documents can make it easier to sail freely, without Western countries interfering in the transport.
This signature appears time and again on documents issued by Ro Marine.






The search for the owner of the signature led us to a Russian website, where you can download many different signatures. The one used by Ro Marine belongs to a "doctor," says the website.
Norwegian-Russian scam
Behind the global hoax is this man.






His name is Andrey Mochalin, a Russian citizen and resident of St. Petersburg. Mochalin has experience working for a reputable Norwegian insurance company.
In March, he, two Norwegians and a Bulgarian were charged with forging documents and operating an insurance business without a permit. Mochalin is also being investigated for violating international sanctions.
Through an attorney, the Norwegians say they do not understand the charges (see their full response in the fact box later in the article). The Bulgarian tells NRK he is innocent.
For several months, NRK and Dossier Center have tried unsuccessfully to get in touch with Mochalin. At the same time, we investigated Ro Marine's operations. What we found surprised experts.
“Among the worst of the worst”
Sanctions expert David Tannenbaum is shocked by the the scale of it. He knows how far Russia is willing to go to protect its oil exports, which are crucial for funding Putin’s illegal war in Ukraine.
Sanctions against Russia can make it difficult for tankers carrying Russian oil to obtain insurance that is approved in the West. But they are finding ways around it.
This is where Ro Marine enters the picture.
“Seems like Ro Marine is popular with sanctions evaders. You don't have this roster by accident,” says David Tannenbaum from Deep Blue Intelligence. The American company specializes in detecting sanctions evasion.






For Tannenbaum, it appears that Ro Marine primarily serves the shadow fleet or ships engaged in illegal activities or sanctions evasion.
“Is Ro Marine the worst of the worst? I think they're definitely in contention,” he both asks and answers.
Our documentation shows, for example, that Gatik, known as one of the largest players in the Russian shadow fleet, appears to have placed almost all of their ships with Ro Marine.
In addition, six ships linked to the Russian gas giant Novatek have had fake insurance from the Norwegian company. All six have sailed along the Norwegian coast towards the gas facility Arctic LNG2 in Russia, which is sanctioned by the USA. The ships are sanctioned by the EU.
Ships linked to the sanctioned Iranian oil industry and Iranian military have also been customers of Ro Marine.

Dangerous cargo from Russia
Among the cargo ships that have purchased invalid insurance from Ro Marine, we found "Agattu". Here, the vessel is sailing between Denmark and Sweden with explosives in the cargo, bound for Algeria.

    Drone footage: Ole Jakobsen
  
Three tonnes of missile weapons were transported from St. Petersburg in Russia, according to Russian port records.
The ship joins the ranks of Ro Marine customers who have contributed to Russia's export revenues by transporting goods from Russian ports. This does not apply to all customers, but the vast majority, according to research by NRK and Dossier Center.
Provoked a NATO country
Not long ago, it was difficult to imagine that ordinary shipping in Europe could lead to military confrontation. Today, the situation is different. European countries may intervene in oil shipments that violate Western sanctions, which are intended to oppose Putin's bloody war in Ukraine. Russia has its countermeasures.
An illustrative example happened to a Ro Marine customer in mid-May: The oil tanker "Blint".
The Estonian navy suspected the ship was sailing without a flag—a clear violation of international regulations. The navy radioed the vessel, but according to Estonian authorities, the captain refused to cooperate.
Suddenly, a Russian fighter jet came whizzing over them, violating the NATO country’s airspace. Instead of stopping, the sanctioned tanker sailed on to the Russian oil port of Primorsk.
This video of the incident is filmed from inside the ship.

    
  
Like many other tankers transporting Russian oil, "Blint" has had fake insurance from Ro Marine, NRK can document.
In this way, Ro Marine has acted in line with the interests of the Russian authorities.
Russia's president, October 2023:
“Thanks to the actions of companies and authorities, the tanker fleet has grown, new mechanisms for payment, insurance and reinsurance of our cargo have been created.”

Researcher Åse Gilje Østensen at the Norwegian Naval Academy tells that “sometimes, entities act in the Russian interest on their own initiative. Sometimes, Putin or other central figures around him have signaled that certain initiatives are welcome. In such cases, actors will often seek to please the regime.”

“Russia is an authoritarian regime that can force civilian actors to assist the regime. Other times, state bodies may be more directly involved. What is the case regarding Ro Marine is difficult to know.”  The Russian Embassy in Norway does not answer NRK's questions about Ro Marine's operations because the company is Norwegian and refers us to the Norwegian authorities. They also do not respond to whether Ro Marine has acted in accordance with the interests of Russian authorities, or any other statements and findings in this case.
The embassy does however choose to point out that the sanctions against the "shadow fleet" are contrary to international law.

Approved by the largest flag state
For several years, the Norwegian company operated without permission and with fake documents without any authorities noticing — neither in Norway nor abroad.
The earliest objectionable activity was in 2021, according to NRK’s investigation.
At that time, Ro Marine applied to be recognized as an insurance company by the world's largest flag state, Panama, despite missing the necessary approval from Norwegian authorities.




NRK has found that most ships among Ro Marine's clients are registered in Panama.




Ro Marine sent the flag state a forged reference, which was originally given to a completely different company, sources tell NRK. With this reference, Ro Marine was recognized by the flag state of Panama in December 2021.

In the time that followed, several flag states were fooled by Ro Marine, including by forged documents that looked like they were from the Norwegian Financial Supervisory Authority.
The illegal activity continued unencumbered until NRK alerted the flag states. 
Russian owner worked many years for a Norwegian company
The Russian owner of Ro Marine, Andrey Mochalin, has gone underground. Mochalin has not responded to any of the numerous inquiries from NRK and Dossier Center.
For over ten years, he worked for a legitimate Norwegian insurance company. Most of the time, he worked from St. Petersburg.
Occasionally, he visited his employer's office in Oslo. Here he is pictured with his former colleagues.

At this time, two of his Norwegian managers also owned another company that offered insurance. This company later became Ro Marine.
A few weeks after Russia's war against Ukraine began in 2022, Ro Marine passed from Norwegian to Russian ownership.
This is when Mochalin bought Ro Marine from the company of the two Norwegians for almost two million NOK.
These two Norwegians, along with Andrey Mochalin and a Bulgarian citizen, are charged with forging documents and conducting illegal insurance business.
Out of respect for the ongoing investigation, the Norwegians did not want to be interviewed by NRK, according to their lawyer.




Ro Marine claimed its address was here at the Norwegian Shipowners' Association building in Oslo, but according to the association, that was not correct.




Money trail in Russia
Alongside the Norwegian company Ro Marine, Andrey Mochalin runs a company in St. Petersburg with direct links to Ro Marine.
NRK and Dossier Center have obtained access to bank documents for his Russian company.
The money transfers are many, and some stand out.
Last year, there were 36 payments totaling approximately five million NOK that we can connect to Ro Marine. 
The bank transfers were marked with the name of the ship and the policy number.

The number corresponds to insurance documents issued by Ro Marine.

The company’s account also shows salary payments to Mochalin.
Those charged






Here’s what we can share about the Russian Andrey Mochalin and the others charged in the case (in parentheses is the time period they had official roles in Ro Marine):
A. Mochalin (2022-2025): 

Sole owner of Ro Marine during the period when the company's illegal insurance activities were most widespread, according to our documentation.
Majority owner of the Russian company that received payments worth millions of NOK last year marked for Ro Marine.

Norwegian 1 (2016-2023): 

Registered as co-owner in the Russian company that last year received payments worth millions of NOK marked for Ro Marine.
Owner and board chairman in 2021 when someone sent a forged reference on behalf of Ro Marine to Panama. At the time the company lacked a permit to sell insurance.
In 2024, a year after he left the board, contributed to ensuring Ro Marine’s continued operations by securing a new board member: the charged Bulgarian.

Norwegian 2 (2017-2023): 

Owner and managing director in 2021 when someone sent a forged reference on behalf of Ro Marine to Panama. At the time the company lacked a permit to sell insurance.

Bulgarian (2024-2025): 

Car mechanic without any experience in marine insurance.
Tenant of Norwegian 1 for years.
Board member in Ro Marine for over half a year, up to March 2025.

After the Norwegians left the board in 2023, the Norwegian company had a problem. With a Russian as the only board member, the company was in violation of the Companies Act of Norway, which requires at least one board member to be from an EEA country. Since Bulgaria is a member of the EEA, the Bulgarian could solve the issue.
According to the Bulgarian, his Norwegian landlord arranged the board position to help him financially.
However, he never received the money he was promised and left the board because he realized something was wrong, the Bulgarian says. He does not understand the police's suspicion towards him.
“I had no idea what they were doing. I have nothing to hide,” says the Bulgarian to NRK. He has been contacted by the police and says he is fully cooperating with them.


The consequences of uncovering the scam
After NRK's revelation in March, several flag states have issued stop orders to ships that used Ro Marine.
Panama alone has banned 16 ships from sailing because the ships have not shown new, real insurance within the deadline they were given.
The UK has sanctioned Ro Marine. Ro Marines’s website has been taken down. In July, the Oslo District Court forcibly dissolved the company for breach of accounting obligations, because Ro Marine had not submitted annual accounts for 2023.
However, the Russian company in St. Petersburg, which received payments worth millions of NOK marked for Ro Marine, is still active.
One month after NRK's revelation in March, another ship in the Russian shadow fleet presented a fake insurance certificate from Ro Marine. Inspectors at the oil port of Primorsk were presented with a document "signed in Oslo."
The expiration date of the fake insurance?
April next year.
NRK has contacted the companies that operate the mentioned ships "Agattu" and "Blint", with no response. We have not been able to contact Gatik. Novatek has not responded to our questions. 




Publisert

12.09.2025, kl. 07.00



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Justice Department Announces Actions to Combat North Korean Remote IT Workers]]></title>
            <link>https://www.justice.gov/opa/pr/justice-department-announces-coordinated-nationwide-actions-combat-north-korean-remote</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221938</guid>
            <description><![CDATA[The Justice Department announced today coordinated actions against the Democratic People’s Republic of North Korea (DPRK) government’s schemes to fund its regime through remote information technology (IT) work for U.S. companies.]]></description>
            <content:encoded><![CDATA[Note: This press release has been updated to reflect new information regarding the guilty plea of one defendant in the District of Massachusetts.The Justice Department announced today coordinated actions against the Democratic People’s Republic of North Korea (DPRK) government’s schemes to fund its regime through remote information technology (IT) work for U.S. companies. These actions include two indictments, an information and related plea agreement, an arrest, searches of 29 known or suspected “laptop farms” across 16 states, and the seizure of 29 financial accounts used to launder illicit funds and 21 fraudulent websites.According to court documents, the schemes involve North Korean individuals fraudulently obtaining employment with U.S. companies as remote IT workers, using stolen and fake identities. The North Korean actors were assisted by individuals in the United States, China, United Arab Emirates, and Taiwan, and successfully obtained employment with more than 100 U.S. companies.As alleged in court documents, certain U.S.-based individuals enabled one of the schemes by creating front companies and fraudulent websites to promote the bona fides of the remote IT workers, and hosted laptop farms where the remote North Korean IT workers could remote access into U.S. victim company-provided laptop computers. Once employed, the North Korean IT workers received regular salary payments, and they gained access to, and in some cases stole, sensitive employer information such as export controlled U.S. military technology and virtual currency. In another scheme, North Korean IT workers used false or fraudulently obtained identities to gain employment with an Atlanta, Georgia-based blockchain research and development company and stole virtual currency worth approximately over $900,000.“These schemes target and steal from U.S. companies and are designed to evade sanctions and fund the North Korean regime’s illicit programs, including its weapons programs,” said Assistant Attorney General John A. Eisenberg of the Department’s National Security Division. “The Justice Department, along with our law enforcement, private sector, and international partners, will persistently pursue and dismantle these cyber-enabled revenue generation networks.”“North Korean IT workers defraud American companies and steal the identities of private citizens, all in support of the North Korean regime,” said Assistant Director Brett Leatherman of FBI’s Cyber Division. “That is why the FBI and our partners continue to work together to disrupt infrastructure, seize revenue, indict overseas IT workers, and arrest their enablers in the United States. Let the actions announced today serve as a warning: if you host laptop farms for the benefit of North Korean actors, law enforcement will be waiting for you.”“North Korea remains intent on funding its weapons programs by defrauding U.S. companies and exploiting American victims of identity theft, but the FBI is equally intent on disrupting this massive campaign and bringing its perpetrators to justice,” said Assistant Director Roman Rozhavsky of the FBI Counterintelligence Division. “North Korean IT workers posing as U.S. citizens fraudulently obtained employment with American businesses so they could funnel hundreds of millions of dollars to North Korea’s authoritarian regime. The FBI will do everything in our power to defend the homeland and protect Americans from being victimized by the North Korean government, and we ask all U.S. companies that employ remote workers to remain vigilant to this sophisticated threat.”Zhenxing Wang, et al. Indictment, Seizure Warrants, and Arrest – District of MassachusettsToday, the United States Attorney’s Office for the District of Massachusetts and the National Security Division announced the arrest of U.S. national Zhenxing “Danny” Wang of New Jersey pursuant to a five-count indictment. The indictment describes a multi-year fraud scheme by Wang and his co-conspirators to obtain remote IT work with U.S. companies that generated more than $5 million in revenue. The indictment also charges Chinese nationals Jing Bin Huang (靖斌 黄), Baoyu Zhou (周宝玉), Tong Yuze (佟雨泽), Yongzhe Xu (徐勇哲 andيونجزهي أكسو), Ziyou Yuan (زيو) and Zhenbang Zhou (周震邦), and Taiwanese nationals Mengting Liu (劉 孟婷) and Enchia Liu (刘恩) for their roles in the scheme. A second U.S. national, Kejia “Tony” Wang of New Jersey, has agreed to plead guilty for his role in the scheme and was charged separately in an information unsealed today.“The threat posed by DPRK operatives is both real and immediate. Thousands of North Korean cyber operatives have been trained and deployed by the regime to blend into the global digital workforce and systematically target U.S. companies,” said U.S. Attorney Leah B. Foley for the District of Massachusetts. “We will continue to work relentlessly to protect U.S. businesses and ensure they are not inadvertently fueling the DPRK’s unlawful and dangerous ambitions.”According to the indictment, from approximately 2021 until October 2024, the defendants and other co-conspirators compromised the identities of more than 80 U.S. persons to obtain remote jobs at more than 100 U.S. companies, including many Fortune 500 companies, and caused U.S. victim companies to incur legal fees, computer network remediation costs, and other damages and losses of at least $3 million. Overseas IT workers were assisted by Kejia Wang, Zhenxing Wang, and at least four other identified U.S. facilitators. Kejia Wang, for example, communicated with overseas co-conspirators and IT workers, and traveled to Shenyang and Dandong, China, including in 2023, to meet with them about the scheme. To deceive U.S. companies into believing the IT workers were located in the United States, Kejia Wang, Zhenxing Wang, and the other U.S. facilitators received and/or hosted laptops belonging to U.S. companies at their residences, and enabled overseas IT workers to access the laptops remotely by, among other things, connecting the laptops to hardware devices designed to allow for remote access (referred to as keyboard-video-mouse or “KVM” switches).Kejia Wang and Zhenxing Wang also created shell companies with corresponding websites and financial accounts, including Hopana Tech LLC, Tony WKJ LLC, and Independent Lab LLC, to make it appear as though the overseas IT workers were affiliated with legitimate U.S. businesses. Kejia Wang and Zhenxing Wang established these and other financial accounts to receive money from victimized U.S. companies, much of which was subsequently transferred to overseas co‑conspirators. In exchange for their services, Kejia Wang, Zhenxing Wang, and the four other U.S. facilitators received a total of at least $696,000 from the IT workers.IT workers employed under this scheme also gained access to sensitive employer data and source code, including International Traffic in Arms Regulations (ITAR) data from a California-based defense contractor that develops artificial intelligence-powered equipment and technologies. Specifically, between on or about Jan. 19, 2024, and on or about April 2, 2024, an overseas co-conspirator remotely accessed without authorization the company’s laptop and computer files  containing technical data and other information. The stolen data included information marked as being controlled under the ITAR.Simultaneously with today’s announcement, the FBI and Defense Criminal Investigative Service (DCIS) seized 17 web domains used in furtherance of the charged scheme and further seized 29 financial accounts, holding tens of thousands of dollars in funds, used to launder revenue for the North Korean regime through the remote IT work scheme.Previously, in October 2024, as part of this investigation, federal law enforcement executed searches at eight locations across three states that resulted in the recovery of more than 70 laptops and remote access devices, such as KVMs. Simultaneously with that action, the FBI seized four web domains associated with Kejia Wang’s and Zhenxing Wang’s shell companies used to facilitate North Korean IT work.The FBI Las Vegas Field Office, DCIS San Diego Resident Agency, and Homeland Security Investigations San Diego Field Office are investigating the case.Assistant U.S. Attorney Jason Casey for the District of Massachusetts and Trial Attorney Gregory J. Nicosia, Jr. of the National Security Division’s National Security Cyber Section are prosecuting the case, with significant assistance from Legal Assistants Daniel Boucher and Margaret Coppes. Valuable assistance was also provided by Mark A. Murphy of the National Security Division’s Counterintelligence and Export Control Section and the U.S. Attorneys’ Offices for the District of New Jersey, Eastern District of New York, and Southern District of California.Kim Kwang Jin et al. Indictment – Northern District of GeorgiaToday, the Northern District of Georgia unsealed a five-count wire fraud and money laundering indictment charging four North Korean nationals, Kim Kwang Jin (김관진), Kang Tae Bok (강태복), Jong Pong Ju (정봉주) and Chang Nam Il (창남일), with a scheme to steal virtual currency from two companies, valued at over $900,000 at the time of the thefts, and to launder proceeds of those thefts. The defendants remain at large and wanted by the FBI.“The defendants used fake and stolen personal identities to conceal their North Korean nationality, pose as remote IT workers, and exploit their victims’ trust to steal hundreds of thousands of dollars,” said U.S. Attorney Theodore S. Hertzberg for the Northern District of Georgia. “This indictment highlights the unique threat North Korea poses to companies that hire remote IT workers and underscores our resolve to prosecute any actor, in the United States or abroad, who steals from Georgia businesses.”According to the indictment, the defendants traveled to the United Arab Emirates on North Korean travel documents and worked as a co-located team. In approximately December 2020 and May 2021, respectively, Kim Kwang Jin (using victim P.S.’s stolen identity) and Jong Pong Ju (using the alias “Bryan Cho”) were hired by a blockchain research and development company headquartered in Atlanta, Georgia, and a virtual token company based in Serbia. Both defendants concealed their North Korean identities from their employers by providing false identification documents containing a mix of stolen and fraudulent identity information. Neither company would have hired Kim Kwang Jin and Jong Pong Ju had they known that they were North Korean citizens. Later, on a recommendation from Jong Pong Ju, the Serbian company hired “Peter Xiao,” who in fact was Chang Nam Il.After gaining their employers’ trust, Kim Kwang Jin and Jong Pong Ju were assigned projects that provided them access to their employers’ virtual currency assets. In February 2022, Jong Pong Ju used that access to steal virtual currency worth approximately $175,000 at the time of the theft, sending it to a virtual currency address he controlled. In March 2022, Kim Kwang Jin stole virtual currency worth approximately $740,000 at the time of theft by modifying the source code of two of his employer’s smart contracts, then sending it to a virtual currency address he controlled.To launder the funds after the thefts, Kim Kwang Jin and Jong Pong Ju “mixed” the stolen funds using the virtual currency mixer Tornado Cash and then transferred the funds to virtual currency exchange accounts controlled by defendants Kang Tae Bok and Chang Nam Il but held in the name of aliases. These accounts were opened using fraudulent Malaysian identification documents.The FBI Atlanta Field Office is investigating the case.Assistant U.S. Attorneys Samir Kaushal and Alex Sistla for the Northern District of Georgia and Trial Attorney Jacques Singer-Emery of the National Security Division’s National Security Cyber Section are prosecuting the case.21 Searches of Known or Suspected U.S.-based Laptop Farms – Multi-DistrictBetween June 10 and June 17, 2025, the FBI executed searches of 21 premises across 14 states hosting known and suspected laptop farms. These actions, coordinated by the FBI Denver Field Office, related to investigations of North Korean remote IT worker schemes being conducted by the U.S. Attorneys’ Offices of the District of Colorado, Eastern District of Missouri, and Northern District of Texas. In total, the FBI seized approximately 137 laptops.Valuable assistance was provided by the U.S. Attorney’s Offices for the District of Connecticut, the Eastern District of Michigan, the Eastern District of Wisconsin, the Middle District of Florida, the Northern District of Georgia, the Northern District of Illinois, the Northern District of Indiana, the District of Oregon, the Southern District of Florida, the Southern District of Ohio, the Western District of New York, and the Western District of Pennsylvania.***The Department’s actions to combat these schemes are the latest in a series of law enforcement actions under a joint National Security Division and FBI Cyber and Counterintelligence Divisions effort, the DPRK RevGen: Domestic Enabler Initiative. This effort prioritizes targeting and disrupting the DPRK’s illicit revenue generation schemes and its U.S.-based enablers. The Department previously announced other actions pursuant to the initiative, including in January 2025 and prior, as well as the filing of a civil forfeiture complaint in early June 2025 for over $7.74 million tied to an illegal employment scheme.As the FBI has described in Public Service Announcements published in May 2024 and January 2025, North Korean remote IT workers posing as legitimate remote IT workers have committed data extortion and exfiltrated the proprietary and sensitive data from U.S. companies. DPRK IT worker schemes typically involve the use of stolen identities, alias emails, social media, online cross-border payment platforms, and online job site accounts, as well as false websites, proxy computers, and witting and unwitting third parties located in the U.S. and elsewhere.Other public advisories about the threats, red flag indicators, and potential mitigation measures for these schemes include a May 2022 advisory released by the FBI, Department of the Treasury, and Department of State; a July 2023 advisory from the Office of the Director of National Intelligence; and guidance issued in October 2023 by the United States and the Republic of Korea (South Korea). As described the May 2022 advisory, North Korean IT workers have been known individually to earn up to $300,000 annually, generating hundreds of millions of dollars collectively each year, on behalf of designated entities, such as the North Korean Ministry of Defense and others directly involved in the DPRK’s weapons programs.The U.S. Department of State has offered potential rewards for up to $5 million in support of international efforts to disrupt the DPRK’s illicit financial activities, including for cybercrimes, money laundering, and sanctions evasion.The details in the above-described court documents are merely allegations. All defendants are presumed innocent until proven guilty beyond a reasonable doubt in a court of law.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UK launches Project Octopus, thousands of interceptor drones to Ukraine]]></title>
            <link>https://www.shephardmedia.com/news/air-warfare/dsei-2025-uk-launches-project-octopus-to-deliver-thousands-of-interceptor-drones-to-ukraine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221772</guid>
            <description><![CDATA[The programme will work to build and deploy the drones to Ukraine to support its fight against Russia, coming a day after Poland shot down 19 Russian drones in its airspace.]]></description>
            <content:encoded><![CDATA[UK defence secretary John Healey has outlined new plans to send thousands of interceptor missiles to Ukraine every month, with the Ukrainian-developed UAV to be shared with the UK to help in the fight against Russia. 
Speaking at DSEI, Healey outlined ‘Project Octopus’, a new partnership between the UK and Ukraine. Under the project, Ukraine would share technology developed for a new interceptor drone that had proved highly effective against Iranian-made, Russian-deployed Shahed one-way attack drones and cost less than 10% of the Russian systems destroyed.
According to Healey, the UK would in turn “rapidly develop” this Ukrainian interceptor drone – with the IP and technology shared with the UK – to mass produce it. Thousands of small interceptor drones are planned to be sent to Ukraine every month.What impact will the massive drone attack on Russian territory have on the future of the war in Ukraine?UK bets big on drone investment and boosts its drone deliveries to Ukraine tenfold
“It demonstrates that wartime necessity really is the mother of constant invention,” he said. “It [Project Octopus] means we have access to the best and developing battlefield technology for our own forces”. 
The agreement followed investment from Ukraine’s largest drone manufacturer, UKRSPECSYSTEMS, which announced that it would invest £200 million (US$271.2 million) into two new UK facilities – the first major investment by a Ukrainian defence company in the UK, according to Healy. 
At DSEI, other UK-Ukraine drone partnerships reared their heads, as the UK sought to boost drone output for its armed forces. This includes a joint venture between the UK firm Prevail Partners and Ukrainian manufacturer Skyeton for its Raybird UAV. The drone is, as Shephard reported, to be submitted for Project Corvus as a potential bid to replace the Watchkeeper drone. 
“We know that whenever equipment is in the hands of the war fighter, whoever can get that new technology into their hands fastest has the edge. We’ve proved we can do it with Ukraine through the excellent work of Task Forces Kindred. We now must do it for ourselves in Britain,” he emphasised.Shahed-136
        
            
                Author
                Lucy Powell
            
            
            
            Lucy Powell is Shephard’s Air Reporter. She has spent the last two years reporting …

            
                Read full bio
            
        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Chat Control faces blocking minority in the EU]]></title>
            <link>https://twitter.com/TutaPrivacy/status/1966384776883142661</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221580</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Treasury Is Expanding the Patriot Act to Attack Bitcoin Self Custody]]></title>
            <link>https://www.tftc.io/treasury-iexpanding-patriot-act/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221274</guid>
            <description><![CDATA[We shouldn't have to cater to the lowest common denominator.]]></description>
            <content:encoded><![CDATA[
                        Marty's Bentvia The RageWe warned a couple of months ago when the Trump administration's "Crypto Brief" was released that there was some language in the brief that advised the government to expand the Patriot Act to account for digital assets. Well, it looks like FinCen and the Treasury have been working on guidelines and a rough outline is shared above courtesy of The Rage, and they are absolutely horrid.It seems that FinCen and the Treasury are preparing to outlaw the use of CoinJoin, atomic swaps, single address use, and transaction broadcast timing delays. All of which are common best use practices that I would recommend any bitcoiner leveraging self-custody practice. This is an all out attack on financial privacy within bitcoin. If enacted, any user who leverages these tools will be flagged as a suspicious, any attempts to send a UTXO that has touched any of these tools will be rejected by regulated services, and could potentially be sent to prison.This is an absurd affront to common sensibilities and freedom in the digital age. The fact that they want to prevent people from using single addresses for individual UTXOs is patently absurd. Not only is it a massive infringement on privacy, but it makes bitcoin usage less economically efficient and degrades the security of every bitcoiner. Loading up a single address with too many UTXOs degrades the entropy of a public-private key pair and makes it easier to brute force a user's private key.Instead of expanding the Patriot Act, it should be abolished. Instead of trying to eliminate financial privacy for the 99.9% of law abiding citizens in this country, the government should be actively trying to foster an environment in which it can be improved. The proposed solutions will do nothing but put good Americans in harm's way and degrade the security of their savings.We shouldn't have to live in a world where standards cater to the lowest common denominator, in this case criminals, and make things worse off for the overwhelming majority of the population. It's crazy that this even has to be said. The onus is on law enforcement to be so good at their jobs that they are able to prevent crimes from happening before they occur and effectively bring criminals to heel after they commit crimes. It shouldn't be on a neutral protocol and the industry being built on top of it that, when used effectively, provides people with a stable monetary system that respects user privacy and equips them with the tools to receive and spend in a way that provides them with peace of mind.Why should everyone have to suffer because of a few bad apples? Isn't that letting the terrorist win? Bitcoin Is Becoming Less Volatile as It Integrates Into Traditional Finance InfrastructureMel Mattison revealed a fascinating shift in Bitcoin's market dynamics that challenges conventional crypto wisdom. He pointed out that Bitcoin futures now exhibit lower volatility than platinum futures - a remarkable transformation for an asset once synonymous with wild price swings. The proliferation of ETFs, options, futures, and other traditional financial instruments has fundamentally altered Bitcoin's behavior, creating what Mel calls "volatility suppression." This institutionalization comes with trade-offs: while reducing dramatic downswings, it also caps explosive upside potential."Bitcoin is becoming a TradFi security instrument and it's getting TradFi vol." - Mel MattisonMel argued that the relationship between volatility and returns means investors must recalibrate expectations. Where 100% annual gains once seemed routine, he now considers 50% returns "massive" for this new era of Bitcoin. This maturation reflects Bitcoin's evolution from speculative experiment to financial infrastructure - less exciting perhaps, but ultimately more sustainable for long-term adoption.Check out the full podcast here for more on China's gold strategy, Fed independence battles, and housing market manipulation plans.Headlines of the DayNew Bill for Strategic Bitcoin Reserve - via XSEC to Host Crypto Roundtable October 17 - via XResearch Proposes Bitcoin for Mars Trade Standard - via XSecure Your Bitcoin The Hard WayTom Honzik has helped 1,000+ people secure more than 5,000 BTC. Now, TFTC and Unchained are teaming up for a live online session on bitcoin custody.What you’ll learn:Biggest mistakes that cause lost coinsTradeoffs of exchanges, ETFs, singlesig, and multisigHow to get optimal security without blindly trust custodians or DIY riskStick around for the AMA to ask Tom Honzik and Marty Bent anything—from privacy considerations to the tradeoffs of different multisig quorums.Register NowObscura – The World’s Best VPN Built by BitcoinersCreated by Carl Dong (former Bitcoin Core contributor), unlike other VPNs, it can’t log your activity by design, delivering verifiable privacy you can trust.USE CODE TFTC25Outsmarts internet censorship: works even on the most restrictive Wi-Fi networks where other VPNs fail.Pay with bitcoin over Lightning: better privacy and low fees.No email required: accounts are generated like bitcoin wallets.No trade-offs: browse freely with fast, reliable speeds.Exclusive Deal for TFTC Listeners:Sign up at obscura.net and use code TFTC25 for 25% off your first 12 months.Now available on macOS, iOS, and WireGuard, with more platforms coming soon — so your privacy travels with you wherever you go.Ten31, the largest bitcoin-focused investor, has deployed $200M across 30+ companies through three funds. I am a Managing Partner at Ten31 and am very proud of the work we are doing. Learn more at ten31.vc/invest.Final thought...Rest in peace, Charlie Kirk. Pray for humanity and for peace. Download our free browser extension, Opportunity Cost: https://www.opportunitycost.app/ start thinking in SATS today. Get this newsletter sent to your inbox daily: https://www.tftc.io/bitcoin-brief/Subscribe to our YouTube channels and follow us on Nostr and X:

  
  
  
  

 


                        
        
          
                        
                    
        
        Spread the signal, earn Bitcoin.
        Get your unique referral link when you subscribe.
        Learn more →
      
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lumina-DiMOO: An open-source discrete multimodal diffusion model]]></title>
            <link>https://synbol.github.io/Lumina-DiMOO/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45221103</guid>
            <description><![CDATA[Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding]]></description>
            <content:encoded><![CDATA[
        
            Abstract
            
                We introduce Lumina-DiMOO, an open-source foundational model for seamless multimodal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities.
                This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-diffusion paradigms and adeptly support a broad spectrum of multimodal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. 
                Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multimodal models. 
                To foster further advancements in multimodal and dicrete diffusion model research, we release our code and checkpoints.
            
        

        
            Overview of Lumina-DiMOO’s Multifunctionality and Superior Performance.
        

        
            Demo Examples
            Text-to-Image Generation
            
                    A striking photograph of a glass of orange juice on a wooden kitchen table, capturing a playful moment. The orange juice splashes out of the glass and forms the word "Smile" in a whimsical, swirling script just above the glass. The background is softly blurred, revealing a cozy, homely kitchen with warm lighting and a sense of comfort.
                
            
                
                
                    Two ramekins filled with a creamy dessert, possibly a fruit custard or pudding, are presented. The dessert is garnished with diced mangoes, blueberries, and a drizzle of honey. The ramekins rest on a wooden board, surrounded by fresh mint leaves and a small bowl of honey. A bowl of fresh strawberries and a bunch of mint leaves are visible. The setting is completed with a white cloth and a wooden spoon. 
                
                
                    A beautifully aged antique book is positioned carefully for a studio close-up, revealing a rich, dark brown leather cover. The words "Knowledge is Power" are prominently featured in the center with thick, flowing brushstrokes, gleaming in opulent gold paint. Tiny flecks of the gold leaf can be seen scattered around the ornately scripted letters, showcasing the craftsmanship that went into its creation.
                
                
                    Clean white brick wall, vibrant colorful spray-paint graffiti covering entire surface: top giant bubble letters "Lumina DiMOO", below stacked "MULTIMODAL DISCRETE", "DIFFUSION MODEL" in rainbow palette, fresh wet paint drips, daylight urban photography.
                
                
                      A collection of vibrant red roses is artfully arranged on a rustic wooden surface. The roses, in full bloom, display their intricate petal layers and deep red hue, while the wooden background, with its visible grain patterns and knots, adds a textured contrast. The roses are placed in a cluster, with some overlapping others, creating a sense of depth and dimension. 
                
            
            
                
                
                    Close-up portrait of a young woman with light skin and long brown hair, looking directly at the camera. Her face is illuminated by dramatic, slatted sunlight casting shadows across her features, creating a pattern of light and shadow. Her eyes are a striking green, and her lips are slightly parted, with a natural pink hue. The background is a soft, dark gradient, enhancing the focus on her face. The lighting is warm and golden. 
                
                
                    Illustrate a poignant moment from a slice-of-life anime, with two high school friends sharing a heartfelt conversation under a cherry blossom tree, petals gently falling around them.
                
                
                    Enchanted forest scene with large wooden letters "B", "E", and "A" adorned with greenery and colorful flowers, positioned in the center and right of the frame. The letters are embellished with red and white flowers, small white ornaments, and trailing vines. The forest floor is covered with brown pine needles and scattered leaves, with small glowing lanterns placed around the letters, casting warm light. 
                
            
            
                
                
                    Design a poster for a fictitious film noir festival, featuring a shadowy detective silhouette against a backdrop of a rain-slicked 1940s city street, the title "Shadows and Fog: A Noir Retrospective" in stylish, vintage typography, all presented in a classic black and white sketch style.
                
                
                    A stunning photograph of a Scandinavian landscape, showcasing snow-capped mountains, dense evergreen forests, and a tranquil lake reflecting the clear blue sky. The scene is bathed in soft sunlight, creating a warm and inviting atmosphere. In the distance, a small, picturesque village can be seen nestled among the trees, with smoke gently rising from a few chimneys.
                
                
                    A captivating photograph of an exquisite wooden dragon sculpture, skillfully carved with intricate details and realistic scales. The dragon is poised on a tree branch, its grand wings spread wide, revealing a mesmerizing woodland landscape below. The sky is painted with a symphony of soft blues and yellows, as the sun casts its final rays beyond the horizon. The dragon's glass eyes lend it a lifelike presence.
                
                
                    A small hamster with a fluffy, light brown coat sits centrally on a red and orange striped sofa. The hamster's eyes are wide and alert, facing directly at the camera. The sofa's fabric features vertical stripes with a dark green and white border, creating a vibrant contrast with the hamster's soft fur. In the background, a dark green knitted blanket partially covers the sofa, adding texture to the scene. 
                
            
            
          
            Image Editing
            
                
                
                    Add a large bowl filled with tomato-based stew garnished with basil on a wooden table in the central area, occupying most of the middle and lower part of the image.
                
                
                    Add a tan and black bike frame with large, thick tires in the lower-left to upper-right center, spanning most of the image and occupying about three-quarters of the total area.
                
            

            
                  
                  
                      Remove all baked goods located in the upper central to upper right section, occupying a large horizontal area across the top of the image.
                  
                  
                      Stylize the image according to book Illustration with clear outlines and narrative focus.
                  
              

              
                  
                  
                      Change the plain wall into brick wall.
                  
                  
                      Replace bird positioned in the upper central-right area of the image with a colorful butterfly.
                  
              

          Style Transfer
          
                  
                  
                  
                  
                  
                  
                  
                  
                  
          
          
          Subject-Driven Generation
          
                  
                  
                      A vibrant, intricately designed beaded hair accessory. At a bustling outdoor market at midday, it glimmers in the bright sunlight, resting delicately on a wooden table covered with colorful woven fabrics, while a gentle breeze rustles the nearby array of potted plants
                  
                  
                      A durable and elegant hardcover writing tool. Set against a sleek minimalist interior with its dark cover contrasting against white marble countertops, under the brilliant, diffused glow of overhead pendant lights that create an atmosphere of quiet elegance and focus.
                  
                  
                      A milkshake with whipped cream topping. During an evening music festival, it glows softly under twinkling fairy lights, with a blurred stage in the background showcasing musicians in action.
                  
          
          Controllable Generation
          
                  
                  
                      A charming porcelain teacup with floral patterns is perched elegantly on a wrought iron café table on a bustling city street, under a large umbrella casting gentle shade, while pedestrians stroll by in the background.
                  
                  
                      On a chic café table, a sleek modern tabletop lamp adds a modern touch to the morning bustle as sunlight filters through a nearby window, blending with the aroma of fresh coffee.
                  
                  
                      Captured in a bustling urban street at twilight, A creamy, rich-flavored dark beverage, is placed on an outdoor café table, as city lights begin to twinkle and passersby create a lively atmosphere.
                  
                  
                      In an urban park during a light drizzle, beneath dense tree cover that filters the overcast light,  a man wearing a waterproof windbreaker is walking.
                  
          

          Inpainting and Extrapolation
          
                  
                  
                      Porsche showroom. Make there be a Porsche logo on the back wall behind the car.
                  
                  
                      A contemporary basement bar area, featuring sharp, bright colors, high-quality lighting, and a mood of modern relaxation. Include depth of field and a strong sense of atmosphere. 
                  
          
          
                  
                  
                      A breathtaking mountain range dramatically rising above a still alpine lake at dawn. The snow-capped peaks are bathed in the warm glow of the rising sun, displaying hues of vibrant orange, pink, and gold.
                  
                  
                  
                      A serene, snow-capped mountain range reflected in a crystal-clear turquoise lake. The towering peaks are dusted with fresh snow, their slopes covered in vibrant green pine trees reaching towards the sky. 
                  
          
    
    
          Image Understanding
          
            
                
                    
                    
                    
                    
                    
                    
                
                
                
                
                
                    这是第一张图片对应的问题。请在这里输入您想要展示的问题内容。
                
            
            
            
                        Answer：
                        这是第一张图片问题对应的答案。您可以在这里提供详细的解答内容。答案可以很长，这个区域会自动支持滚动显示。现在右侧区域会占据更多空间来显示完整的答案内容。
                    
        
    
        
          
        

        

        
            Experimental Results

            GenEval Benchmark
            
                
                    
                        
                            Methods
                            #Params
                            Single Object
                            Two Object
                            Counting
                            Colors
                            Position
                            Attibute
                            Overall ↑
                                
                    
                    
                        
                            Gen. Only
                        
                        
                            SDXL
                            2.6B
                            0.98
                            0.74
                            0.39
                            0.85
                            0.15
                            0.23
                            0.55
                          
                        
                            Emu3-Gen
                            8B
                            0.98
                            0.71
                            0.34
                            0.81
                            0.17
                            0.21
                            0.54
                        
                        
                            SD3-Medium
                            2B
                            0.99
                            0.94
                            0.72
                            0.89
                            0.33
                            0.60
                            0.74
                        
                        
                            DALL-E 3
                            -
                            0.96
                            0.87
                            0.47
                            0.83
                            0.43
                            0.45
                            0.67
                         
                        
                            FLUX.1 [Dev]
                            12B
                            0.98
                            0.81
                            0.74
                            0.79
                            0.22
                            0.45
                            0.66
                             
                        
                            OmniGen
                            3.8B
                            0.98
                            0.84
                            0.66
                            0.74
                            0.40
                            0.43
                            0.68
                         
                        
                            Lumina-mGPT 2.0
                            7B
                            0.99
                            0.87
                            0.44
                            0.85
                            0.44
                            0.54
                            0.69
                        
                        
                            Unified
                        
                        
                            Show-o
                            1.3B
                            0.95
                            0.52
                            0.49
                            0.82
                            0.11
                            0.28
                            0.53
                            
                        
                            TokenFlow-XL
                            14B
                            0.95
                            0.60
                            0.41
                            0.81
                            0.16
                            0.24
                            0.55
                        
                        
                            Janus-Pro
                            7B
                            0.99
                            0.89
                            0.59
                            0.90
                            0.79
                            0.66
                            0.80
                         
                            
                        
                            GPT-4o
                            -
                            0.99
                            0.92
                            0.85
                            0.92
                            0.75
                            0.61
                            0.84
                        
                        
                            BAGAL
                            14B
                            0.99
                            0.94
                            0.81
                            0.88
                            0.64
                            0.63
                            0.82
                        
                        
                            MMaDA
                            8B
                            0.99
                            0.76
                            0.61
                            0.84
                            0.20
                            0.37
                            0.63
                        
                        
                            Lumina-DiMOO
                            8B
                            1.0
                            0.94
                            0.85
                            0.89
                            0.85
                            0.76
                            0.88
                        
                    
                
            

            DPG Benchmark
            
                
                    
                        
                            Methods
                            #Params
                            Global
                            Entity
                            Attribute
                            Relation
                            Other
                            Overall ↑
                                
                    
                    
                        
                            Gen. Only
                             
                              
                            SDXL
                            2.6B
                            83.27
                            82.43
                            80.91
                            86.76
                            80.41
                            74.65
                            
                              
                            Emu3-Gen
                            8B
                            85.21
                            86.68
                            86.84
                            90.22
                            83.15
                            80.60
                        
                        
                            SD3-Medium
                            2B
                            87.90
                            91.01
                            88.83
                            80.70
                            88.68
                            84.08
                        
                        
                            DALL-E 3
                            -
                            90.97
                            89.61
                            88.39
                            90.58
                            89.83
                            83.50
                          
                        
                            FLUX.1 [Dev]
                            12B
                            74.35
                            90.00
                            88.96
                            90.87
                            88.33
                            83.84
                        
                              
                            OmniGen
                            3.8B
                            87.90
                            88.97
                            88.47
                            87.95
                            83.56
                            81.16
                        
                        
                            Lumina-mGPT 2.0
                            7B
                            -
                            88.94
                            88.08
                            91.70
                            -
                            84.30
                         
                        
                            Unified
                        
                        
                            Show-o
                            1.3B
                            -
                            -
                            -
                            -
                            -
                            67.48
                            
                        
                            TokenFlow-XL
                            14B
                            78.72
                            79.22
                            81.29
                            85.22
                            71.20
                            73.38
                        
                        
                            Janus-Pro
                            7B
                            86.90
                            88.90
                            89.40
                            89.32
                            89.48
                            84.19
                          
                          
                      
                            GPT-4o
                            -
                            88.89
                            88.94
                            89.84
                            92.63
                            90.96
                            85.15
                        
                        
                            BAGAL
                            14B
                            88.94
                            90.37
                            91.29
                            90.82
                            88.67
                            85.07
                        
                        
                            MMaDA
                            8B
                            77.81
                            78.48
                            81.74
                            84.79
                            63.2
                            69.97
                        
                        
                            Lumina-DiMOO
                            8B
                            81.46
                            92.08
                            88.98
                            94.31
                            82.0
                            86.04
                        
                    
                
            

            Image Understanding Benchmark
            
                
                    
                        
                        
                            Methods
                            #Params
                            POPE
                            MME-P
                            MMB
                            SEED
                            MMMU
                                
                    
                    
                        
                            Under. Only
                        
                                
                            LLaVA
                            7B
                            76.3
                            809.6
                            38.7
                            33.5
                            -
                         
                              
                            LLaVA-v1.5
                            7B
                            85.9
                            1510.7
                            64.3
                            58.6
                            35.4
                         
                        
                            InstructBLIP
                            7B
                            -
                            -
                            36.0
                            53.4
                            - 
                         
                        
                            Qwen-VL-Chat
                            7B
                            -
                            1487.5
                            60.6
                            58.2
                            -
                        
                        
                            Emu3-Chat
                            8B
                            85.2
                            1244
                            58.5
                            68.2 
                            31.6
                        
                        
                            Unified
                        
                        
                            Show-o
                            1.3B
                            80.0
                            1097.2
                            -
                            -
                            26.7 
                         
                        
                            TokenFlow-XL
                            13B
                            86.8
                            1545.9
                            68.9
                            68.7
                            38.7 
                         
                        
                            Janus-Pro
                            7B
                            87.4
                            1567.1
                            79.2
                            72.1
                            41.0
                        
                        
                            BAGAL
                            14B
                            -
                            1687
                            85.0
                            -
                            55.3
                        
                        
                            MMaDA
                            8B
                            86.1
                            1410.7
                            68.5
                            64.2
                            30.2
                        
                        
                            Lumina-DiMOO
                            8B
                            87.4
                            1534.2
                            84.5
                            83.1
                            58.6           
                        
                    
                
            
        

      
        Acknowledgements
        This work was also supported and implemented by MindSpeed MM, an open-source training framework for large-scale multimodal models designed for distributed training, developed and maintained by Huawei's Computing Product Line.
        Specifically Optimized for Huawei‘s Ascend AI chips, MindSpeed MM offers comprehensive support for distributed training and is tailored for a wide range of multimodal tasks.
       
      
      
          Citation
          
              @article{Lumina-DiMOO,
                 title={Lumina-DiMOO：An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding},
                 author={Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Yuandong Pu, Le Zhuo, Shenglong Ye, Ming Hu, Junjun He, Bo Zhang, Dengyang Jiang, Gen Luo, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Xiaohong Liu, Bin Fu, Yu Qiao, and Yihao Liu},
                 year={2025}
              }
          
      

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Astrophysics Source Code Library]]></title>
            <link>http://ascl.net/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45220843</guid>
            <description><![CDATA[The Astrophysics Source Code Library (ASCL) is a free online registry and repository for source codes of interest to astronomers and astrophysicists, including solar system astronomers, and lists codes that have been used in research that has appeared in, or been submitted to, peer-reviewed publications. The ASCL is indexed by the SAO/NASA Astrophysics Data System (ADS) and Web of Science and is citable by using the unique ascl ID assigned to each code. The ascl ID can be used to link to the code entry by prefacing the number with ascl.net (i.e., ascl.net/1201.001).]]></description>
            <content:encoded><![CDATA[The Astrophysics Source Code Library (ASCL) is a free online registry and repository for source codes of interest to astronomers and astrophysicists, including solar system astronomers, and lists codes that have been used in research that has appeared in, or been submitted to, peer-reviewed publications. The ASCL is indexed by the SAO/NASA Astrophysics Data System (ADS) and Web of Science and is citable by using the unique ascl ID assigned to each code. The ascl ID can be used to link to the code entry by prefacing the number with ascl.net (i.e., ascl.net/1201.001).


Most Recently Added Codes

2025 Sep 01

			
			
								[submitted]
							

			
								adstex: Automated bibliography builder for astronomy literature			
		
			adstex automatically identifies all citation keys in a TeX source file and builds the corresponding bibliography file (.bib file) by fetching the reference information from NASA's Astrophysics Data System (ADS). adstex recognizes all variants of the cite commands in TeX, and works with various styles of citation keys, including arXiv IDs, DOIs, and ADS bibcodes. When a citation key is in the format of first-author name and year, adstex will query NASA's ADS and return a list of possible reference matches for the user to select the intended one. When a reference entry has updated information on NASA's ADS, adstex can detect such changes and fetch the new information and update the user's bibliography file. adstex supports any reference entry that is available on NASA's ADS, and allows the authors to write papers without manually searching for the bibliography entries.
				2025 Aug 31
	
	
		
		

			
			
								[ascl:2508.022]
							

			
								IAR_Model: Autoregressive model to irregularly spaced data			
		
			IAR_Model fits unequally spaced time series from the Irregular Autoregressive (IAR). Available as Python and R functions, IAR_Model can generate observations for each process, compute the negative of the log likelihood of these process, fit each model to irregularly sampled data, and test the significance of the estimate.
				
	
		
		

			
			
								[ascl:2508.021]
							

			
								fm4ar: Inferring atmospheric properties of exoplanets using flow matching posterior estimation			
		
			fm4ar (flow matching for atmospheric retrievals) infers atmospheric properties of exoplanets from observed spectra. It uses flow matching posterior estimation (FMPE) for its machine learning (ML) approach to atmospheric retrieval; this approach provides many of the advantages of neural posterior estimation (NPE) while also providing greater architectural flexibility and scalability. The package uses importance sampling (IS) to verify and correct ML results, and to compute an estimate of the Bayesian evidence. fm4ar's ML models are conditioned on the assumed noise level of a spectrum (i.e., error bars), thus making them adaptable to different noise models.
				
	
		
		

			
			
								[ascl:2508.020]
							

			
								AGNI: Model for extreme atmospheres on rocky exoplanets			
		
			AGNI simulates the atmospheric temperature-, height-, and compositional-structures of atmospheres overlying magma oceans while ensuring that radiative-convective equilibrium is maintained throughout the atmosphere. The code also supports real gas equations of state, self-gravitation, and various spectral surface compositions. Accounting for these energy transport processes permits AGNI to calculate atmospheric structure, which also yields realistic cooling rates for young rocky planets with magma oceans.
				
	
		
		

			
			
								[ascl:2508.019]
							

			
								FiCUS: FItting the stellar Continuum of Uv Spectra			Saldana-Lopez, A.; Schaerer, D.; Chisholm, J.; Calabrò, A.; Pentericci, L.; Cullen, F.; Saxena, A.; Amorín, R.; Carnall, A. C.; Fontanot, F.; Fynbo, J. P. U.; Guaita, L.; Hathi, N. P.; Hibon, P.; Ji, Z.; McLeod, D. J.; Pompei, E.; Zamorani, G.
		
			FiCUS (FItting the stellar Continuum of Uv Spectra) fit the stellar continuum of extragalactic ultraviolet (UV) spectra. The code takes observed-frame wavelength, flux density (with errors) and user-defined mask arrays as inputs, and returns an estimation of the galaxy stellar age, metallicity and dust extinction, as well as other secondary Spectral Energy Distribution (SED) parameters. FiCUS has two scripts; the first reads the INPUT file provided by the user and performs the fit according to selected options. It then gives the best-fit parameters and creates the OUTPUT files and figures. The second script includes pre-defined routines for spectral analysis, loading INPUT files and handling with data and models, as well as functions for the fitting routine, SED-parameters calculations and plotting, and imports functions into the first script.
				
	
		
		

			
			
								[ascl:2508.018]
							

			
								pyStarburst99: Python port of Starburst99			Hawcroft, Calum; Leitherer, Claus; Aranguré, Oskar; Chisholm, John; Ekström, Sylvia; Martinet, Sébastien; Martins, Lucimara P.; Meynet, Georges; Morisset, Christophe; Sander, Andreas A.C.; Wofford, Aida
		
			pyStarburst99 is a Python version of the Starburst99 (ascl:1104.003) population synthesis code for star-forming galaxies. This Python version includes new evolutionary tracks and synthetic spectral energy distributions. pyStarburst99 provides wider coverage in metallicity, mass, and resolution, and includes evolutionary and spectral models of stars up to 300–500 M⊙.
				
	
		
		

			
			
								[ascl:2508.017]
							

			
								SIGWAY: Compute second-order, scalar induced gravitational wave signals			El Gammal, Jonas; Ghaleb, Aya; Franciolini, Gabriele; Papanikolaou, Theodoros; Peloso, Marco; Perna, Gabriele; Pieroni, Mauro; Ricciardone, Angelo; Rosati, Robert; Tasinato, Gianmassimo; Braglia, Matteo; Fumagalli, Jacopo; Kume, Jun'ya; Morgante, Enrico; Nardini, Germano; Racco, Davide; Renaux-Petel, Sébastien; Veermäe, Hardi; Werth, Denis; Zavala, Ivonne; LISA Cosmology Working Group
		
			The SIGWAY data analysis pipeline computes second-order, scalar induced gravitational wave signals emitted by curvature perturbations in the early universe. The package solves the Mukhanov-Sasaki equation for single field ultra-slow roll inflationary models and computes the primordial scalar power spectrum Pζ. SIGWAY also computes the second order gravitational wave power spectrum ΩGW from P ζ for reentry during radiation domination or a phase of early matter domination.
				
	
		
		

			
			
								[ascl:2508.016]
							

			
								sMV: Serial MultiView phase plane estimation			
		
			sMV (serial MultiView) scripts provide a semi-automatic and easy-to-use workflow for serial MultiView phase plane estimation. The phase plane is iteratively rotated based on the time series of calibrator residual phases; because time-domain information is included in the iterations, phase ambiguities are accurately and automatically identified. sMV enables efficient, high-accuracy differential astrometry and artifact-reduced imaging for astrophysical studies.
				
	2025 Aug 30
	
	
		
		

			
			
								[ascl:2508.015]
							

			
								DeepSSM: Cosmological emulator for the GW spectrum from the modified sound-shell model			
		
			Built on Flax (ascl:2504.026), DeepSSM emulates gravitational wave (GW) spectra produced by sound waves during cosmological first-order phase transitions in the radiation-dominated era. It uses neural networks trained on an enhanced version of the Sound Shell Model (SSM). The code provides instantaneous predictions of GW spectra given the phase transition parameters, while achieving agreement with the enhanced SSM model. DeepSSM is particularly suitable for direct Bayesian inference on phase transition parameters without relying on empirical templates, such as broken power-law models.
				
	
		
		

			
			
								[ascl:2508.014]
							

			
								HipFT: High-performance Flux Transport			
		
			The flux transport model HipFT implements advection, diffusion, and data assimilation on the solar surface on a logically rectangular nonuniform spherical grid. It is parallelized for use with multi-core CPUs and GPUs using a combination of Fortran's standard parallel do concurrent (DC), OpenMP Target data directives, and MPI. Serving as the computational core of the Open-source Flux Transport (OFT) software suite (ascl:2508.013), HipFT incorporates various differential rotation, meridional flow, super granular convective flow, and data assimilation models. HipRT also computes multiple realizations in a single run spanning multiple choices of parameters.
				
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Becoming the person who does the thing]]></title>
            <link>https://www.fredrivett.com/2025/09/10/becoming-the-person-who-does-the-thing/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45220656</guid>
            <description><![CDATA[Bad news: your internal identity dictates everything you do. Good news: you can change it]]></description>
            <content:encoded><![CDATA[
It can be disorienting when our beliefs shift. The world we helped create no longer exists, and our role in it transforms too.
It can be unsettling, naturally. But that's kind of the point.
Looking back at times when I held certain beliefs—about how the world works, and what my role in this story is—it can feel less like a mod was installed and more like an entirely new operating system was swapped in.
Up until my late twenties, I could count the number of times I had been to the gym on one hand.
And worse, as a nerd, I was quietly proud of it. Why waste hours a week on something that hardly mattered? I have better things to do. I pitied the jocks who slaved away in the gym chasing vanity. What for? I don't need that. Who cares?
Like all childish thinking, it contained some truth. Physical fitness is less important than spiritual, emotional, and mental fitness; but it’s still important.
Even Paul, one of history’s most influential figures, with a worldview shaped by the utter centrality of spiritual health, said as much.
Your beliefs—and therefore approach to what a healthy life looks like—are foundational. It might sound obvious, but what you believe a "life well lived" looks like has a pretty transformative impact on both what life you end up building and how well lived it looks.
So if we’re a product of our beliefs and our most formative preconceptions are imposed by others, then where’s the hope?
We build, layer upon layer, and the layers laid first—now deeply buried within—are the ones we had the least say in.
We didn't pick them, but they shape everything we are and do. Bad luck, I guess.
Thankfully, we are dynamic beings. Old beliefs can peel off, and new ones take their place. Later layers can somehow seep deeper. Some recent beliefs can even become cornerstones.
For me, something shifted in my late twenties. Growing up I guess you could call it. I don’t remember the exact straw that broke the camel’s back, but a desire for change grew. I started working out.
It began slowly, but I began. Knee press-ups to start, later adding assisted pull-ups.
If anyone was watching, it would have looked stupid. A grown man barely able to push himself off the floor. But I showed up and put in my reps, day by day, week by week, in the privacy of my bedroom.
As the weeks and months passed, my strength grew.
Eventually, I graduated to full press-ups and pull-ups, no mods required.
Every small win reinforced the last and led me further away from who I used to be.
Fast forward almost to a decade and I feel a lot more friction not going to the gym than I do going. Cognitive dissonance is wonderful when it’s on your side, and it pops up whenever my healthy-Fred self-identity and actions diverge.
I'm far from a gym junkie—it hasn't become my life—but I go every weekday, 20 minutes a day. I arrive, do my workout, and leave, while most people are just getting started.
Our self-identity dictates everything, but it is not set in stone.
Changing our beliefs isn’t easy. Both those about the world around us and the world within. We can't simply will our way there and snap our fingers. We must journey. As with all great things, it's a process.
But it is possible. A well-trodden path is ahead for those who wish to walk it.
So how do you? How do you become the person who does the thing?
Earlier this week I spent a couple hours crafting my digital /shelf, a place where I can put the things that have impacted me the most up for all to see, so others can take them for themselves should they wish.
On it are these two quotes that have been living rent-free in my head from the beginning. Together, they create a twin-cog flywheel that cannot be stopped:

“People like us do things like this”
Seth Godin


“Every action you take is a vote for the type of person you wish to become”
James Clear

Your actions follow your self-beliefs.
If you identity as a failure, incapable of achievement, unfit, unlovable, destined to play a bit-part role in your own story, then by heck no matter how much willpower you put in to push that boulder up the hill, it will return to its place.
But there's a way through: every action you take is a vote for who you wish to become. Every day you wake up, look your old identity in the eye and say "thanks for your service, but you're not needed around here anymore," step forward and lean in, is a day your new identity is built.
It takes time. You have to actually want it. You have to choose to adopt a new mindset. Rome wasn't built in a day. But it comes, a little like how Hazel Grace Lancaster describes falling in love in The Fault In Our Stars: "slowly, and then all at once."
The path is there, should you choose.
Identify where your identity needs to shift. Then take a step. Cast today’s vote. Find your way through.
Do that day by day, then soon enough, your inner world will shift and recalibrate around the new reality you're co-creating.
Then one day you'll see it.
People like us really do things like this.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Examples from The LaTeX Companion book (3rd edition)]]></title>
            <link>https://ctan.org/pkg/tlc3-examples</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45220121</guid>
            <description><![CDATA[The PDFs (as used with spotcolor and trimming) and sources for all
      examples from the third edition (Parts I+II), together with necessary
      supporting files. The edition is published by Addison-Wesley, 2023,
      ISBN-13: 978-0-13-816648-9, ISBN-10: 0-13-816648-X (bundle of Part I & II).]]></description>
            <content:encoded><![CDATA[
   
    tlc3-examples – All examples from “The LaTeX Companion”, third edition
    
    
      The PDFs (as used with spotcolor and trimming) and sources for all
      examples from the third edition (Parts I+II), together with necessary
      supporting files. The edition is published by Addison-Wesley, 2023,
      ISBN-13: 978-0-13-816648-9, ISBN-10: 0-13-816648-X (bundle of Part I & II).
    
    
      Sources/info/examples/tlc3
      DocumentationREADME.md
      
      
      Bug trackerhttps://github.com/FrankMittelbach/tlc3-examples/issues
      
      Repositoryhttps://github.com/FrankMittelbach/tlc3-examples
      
      
      LicensesThe LaTeX Project Public License 1.3c
      
      MaintainerFrank Mittelbach
      
      Contained inTeX Live as tlc3-examples
      TopicsBook examples
      
    
    
      Download the contents of this package in one zip archive
        (82.4M).
    
    
    
 
      
      

    
    

    
     Package Links
     
    
    
    
    
  
   
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a generative online drum machine with ClojureScript]]></title>
            <link>https://dopeloop.ai/beat-maker/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45220069</guid>
        </item>
        <item>
            <title><![CDATA[Qwen3-Next]]></title>
            <link>https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&amp;from=research.latest-advancements-list</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45219228</guid>
        </item>
        <item>
            <title><![CDATA[Debian 13, Postgres, and the US time zones]]></title>
            <link>https://rachelbythebay.com/w/2025/09/11/debtz/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45218111</guid>
        </item>
    </channel>
</rss>