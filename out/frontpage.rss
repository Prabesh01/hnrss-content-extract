<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 28 Aug 2025 22:07:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Fuck up my site – Turn any website into beautiful chaos]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid>
            <description><![CDATA[Transform any website into pure chaos. Add burning cursors, Comic Sans everything, fake cursors, and more chaos features to any site. Some people just want to watch the web burn.]]></description>
            <content:encoded><![CDATA[fuckupmysiteSome people just want to watch the web burnTry:😈Chaos Settings3 of 6 agents of chaos enabledHeads up: Not every site plays nice with the chaos. Got feedback or discovered something broken? Let me know on Twitter]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[There Goes the American Muscle Car]]></title>
            <link>https://thedispatch.com/article/dodge-challenger-muscle-cars/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057002</guid>
            <description><![CDATA[Have electric vehicles, regulations, and changing consumer preferences killed a classic?]]></description>
            <content:encoded><![CDATA[
        
                        
                Published August 28, 2025            
                    
        
                PONTIAC, Michigan—Traveling a fair amount for work outside of major metros requires relying on rental cars. And relying on rental cars quickly teaches you that while you might reserve a standard sedan every time, there is no reason to expect the rental car agency to actually give you the keys to a sedan when you show up at the counter.

Eventually, you realize your fate is entirely in the hands of the rental car gods, and the rental car gods are capricious gods.    


Sometimes, the rental car gods frown upon you. This happened to me at the 2011 Iowa straw poll, when I ended up driving a Chevy Aveo across the entire state. If you don’t know what a Chevy Aveo is, consider yourself lucky. If you’ve had the misfortune to ride in one, you know that a go-kart is a smoother, roomier, and safer ride. 

Sometimes, the rental car gods laugh at (or perhaps with) you, which is what happened to me in 2017 when I went to Alabama to cover the Senate campaign of Roy Moore, a notorious creep and alleged pervert. I was told at the rental car counter they couldn’t honor my reservation for a sedan, and the only vehicle available was a boxy full-size van. Pulling up to the first campaign event in the kind of van you’d half-expect to come with the words FREE CANDY scrawled in paint on its side, I was worried I’d be mistaken for a fan of Moore.

But sometimes the rental car gods smile upon you. This happened to me at the 2020 Iowa caucuses. 

“Would a Dodge Challenger be okay?” the agent asked me in the Des Moines airport. 

Why, yes, it would be okay.

Much more than okay.

I had, once before, ended up with a Dodge Charger, the modern sedan version of the two-door Challenger, while covering a special election in upstate New York and was instantly impressed. 

I’m not a car guy, so I lack the expertise to explain to you on a technical level why the Dodge Challenger is the greatest car on the road today—the same way I can’t explain to you why Caravaggio’s The Calling of St. Matthew is the best painting ever painted or why Bernini’s David is the best sculpture ever sculpted. Each of these statements just seems to me to be self-evidently and objectively true. But I can tell you there is nothing quite like going from 0 to 60 in a Challenger. Over the course of four days and 469 miles in January 2020, I stuck to Iowa’s blessedly snow-free backroads whenever possible. Every stop sign was no longer an annoyance slowing me down from getting from Point A to Point B, but another opportunity to experience the thrill of putting the pedal to the metal in a Challenger.

While speeds can vary depending on the car’s trim, the most souped-up Challenger can go from 0 to 60 in 1.66 seconds—faster than the fastest Ferrari. My rental took more than 4 seconds to hit that mark, but there’s more than just its speed that makes the Challenger great. There’s the rumble and roar of the V8 HEMI engine that you can feel in your chest. There’s the beautiful sleek retro design straight out of 1970—the heyday of the classic American muscle car. You don’t have to take it from me—just listen to Jay Leno, the famous comedian and avid car collector, who wrote last year: “To me, the Challenger is really the last great American road car.” 

I’ve driven more than 100 different cars in my life, including some nice ones, but the Challenger is really the only car I’ve ever loved. So when my wife’s work-from-home status was starting to end in 2021 and we owned only one car (a 2011 Hyundai Tucson), I told her I was interested in buying a used Challenger that I found online for $16,000 with 50,000 miles on it.

“You know your friends will laugh at you,” she said. 

“Yes, I’ve considered this,” I replied, fully aware that someone typically dressed in Gap Outlet polos or Brooks Brothers dress shirts doesn’t exactly look like a muscle-car guy. “But there are much more expensive and destructive ways to have a midlife crisis.”

Ultimately, I gave into the practical reality that my wife, whose daily commute could push two hours roundtrip, needed a new car and that I, with a commute that is often one flight of stairs, did not. She got a Mazda, and I figured my midlife crisis could be postponed until our Hyundai gave up the ghost.

But then, tragedy struck: Dodge announced that 2023 would be the last year it would produce the Challenger and the Charger. Then Chevy killed off the Camaro. The last muscle car left in production was the Ford Mustang. But even with the market all to itself, Mustang sales dropped 31 percent in the first quarter of 2025.

Why does the American muscle car seem to be dying off? What, if anything, is America losing as it fades away? I set out to answer these questions after The Dispatch’s editors encouraged us writers to expand our horizons beyond the typical political fare (see Mike Warren on Tiki culture, for example). I decided that if I couldn’t own a muscle car, at least I could write about it. And that’s how I ended up in Pontiac, Michigan, earlier this month inhaling the smoke of burning rubber and talking to muscle-car enthusiasts.



That distinct smell of burning rubber is the first thing you notice when approaching the venue at Roadkill Nights. Woodward Avenue has been closed down for a day of legal drag-racing, and hot rods are doing burnouts for improved traction just before they make their trial runs. 

Those who drove in but aren’t competing have their cars lined up on the street outside. I’m greeted by an array of Challengers—purple, neon green, orange, metallic blue. But the whole scene looks more like a county fair for car enthusiasts than a shot out of Fast & Furious. There are families and food trucks, but instead of the Tilt-a-Whirl and Loop-o-Plane, event-goers wait in long lines for hired drivers to take them on drift rides—in which the driver takes such sharp turns that the car’s rear tires screech and slide sideways. For little kids, some barely old enough to walk, there are Power Wheels-style muscle cars nearby to drive. A full-size monster truck does donuts in a parking lot for entertainment, but the main show for the thousands of attendees is the drag racing.

The first person I talk to on the bleachers overlooking the races that day is Mike Sherrow of Suffolk, Virginia, who has attended the event 10 years in a row. Like many of the enthusiasts I spoke to, Sherrow’s love of muscle cars was inherited from his father. “Lego started it for me, and then building my dad’s ‘65 Chevy truck,” Sherrow told me. “It was like the family hot rod.” 

Asked what he loves about muscle cars now, Sherrow points to the creativity in modifying the vehicles with one’s own hands and tools: “Building your own stuff, and taking somebody’s car and modifying it to make it what you want. It’s just a part of car culture. And it’s going away. This [event] is keeping it alive.”

Sherrow attributes the decline of muscle-car culture to a variety of factors. “Being sued by the EPA for emissions control laws, outlawing even street-legal racing, closing down racetracks because people are building houses too close. It’s a dying art,” he said. “The greats that did it, they’re passing on, and their knowledge is going away with it.”

To understand the decline of the American muscle car, it helps to first remember that it has died—and been resurrected—before.


    
        
                                        1A 965 Pontiac GTO-Grand Prix (Photo by Pat Brollier/The Enthusiast Network via Getty Images/Getty Images)
                    
    


The original muscle car era began, in most tellings, in 1964 with the production of the Pontiac GTO. Competitors—including the Ford Mustang, Plymouth Barracuda, Dodge Charger and Challenger, Chevrolet Camaro and Chevelle—soon appeared on the scene. While sports cars can be foreign or domestic, muscle cars by definition must be American-made, with a powerful engine in a lightweight body. Though there’s some debate about what separates muscle cars from domestic sports cars, muscle cars also prioritize straight-line speed over handling and agility and have that, well, know-it-when-you-see-it muscular look (which is why many don’t consider the sporty Corvette to be a muscle car).

It is no coincidence that the muscle-car era began when the oldest baby boomers turned 18. In 1963, Tom Wolfe chronicled the custom-car craze gripping teenagers in America in an 11,000-word article for Esquire magazine titled, “There Goes (VAROOM! VAROOM!) That Kandy Kolored (THPHHHHHH!) Tangerine-Flake Streamline Baby (RAHGHHHH!) Around the Bend (BRUMMMMMMMMMMMMMMMM…).” 


“Thousands of kids are getting hold of cars and either hopping them up for speed or customizing them to some extent, usually a little of both,” Wolfe wrote. “Even the kids who aren’t full-time car nuts themselves will be influenced by which car is considered ‘boss.’” 

This phenomenon was a direct result of America’s post-war economic boom. “The war created money,” Wolfe wrote in the introduction to his 1965 collection of essays, titled after his Esquire essay. “It made massive infusions of money into every level of society. Suddenly classes of people whose styles of life had been practically invisible had the money to build monuments to their own styles.” 

Where others saw the gaudy or even garish tastes on display in custom cars as unworthy of a second thought, Wolfe saw baroque art objects worthy of study. He presciently predicted in 1963 that car manufacturers in Detroit “may well be on their way to routinizing the charisma” of the custom car. Indeed, within the first two years of production, Ford had sold 1 million Mustangs—before Ford race cars made history in 1966 by beating Ferrari at the 24 Hours of Le Mans.

But by the time Brue Springsteen was singing about muscle cars in “Born to Run” (the album of the same name was released 50 years ago this week)—“Beyond the Palace, hemi-powered drones scream down the boulevard/ Girls comb their hair in rearview mirrors and the boys try to look so hard”—the muscle car era had largely come to a close. Production stopped on the Challenger and Charger, respectively, in 1974 and 1978. The first-generation Mustang stopped being produced in 1973 (but different iterations were available, leaving it the only muscle car in continuous production since its birth). 

The end of the golden era of the muscle car is simultaneously attributed to environmental regulations like the Clean Air Act of 1970 and that decade’s oil crisis that made gas-guzzling muscle cars too expensive for most consumers. The baby boomers were, of course, growing up and starting to have babies of their own and seeking more practical vehicles for their families.

Now, history appears to be rhyming. Since the end of the golden era, the muscle car has experienced revivals, the most recent of which began in the aughts, perhaps not coincidentally after the The Fast and the Furious franchise launched in 2001, in which Vin Diesel’s character drives a 1970 Dodge Charger. The movie and its sequels became the highest-grossing franchise of all time for Universal Pictures, and in 2006 Dodge reintroduced the Charger and the Challenger in 2008, selling more than 2 million of the vehicles combined before Dodge recently stopped production.

If you talk to car enthusiasts, the proximate cause of the recent untimely demise of the Challenger, Charger, and Camaro was increasingly onerous fleet-wide emissions standards, and there’s a lot of truth to that. Those regulations meant that it wasn’t enough for Dodge to simply slap gas-guzzler taxes on the vehicles and purchase offsets from Tesla through a cap-and-trade system to maintain production in the face of billions of dollars of potential fines. It would have made little economic sense to discontinue the vehicles without regulatory pressure: Dodge was selling nearly 80,000 Chargers and more than 50,000 Challengers a year when it announced in 2021 it would discontinue the models and would instead offer a new electric replacement—the Dodge Charger Daytona EV. This model sold just 4,299 units in the first half of 2025—its first and potentially only year on the market. Few muscle car enthusiasts were interested in an electric vehicle that replaced the rumble and roar of the V8 engine with speakers and chambers designed to mimic the sound of a gas-powered engine. “Pretty lame,” declared a review in Edmunds. Following the loosening of emissions standards this year, Dodge announced it would be bringing back a gas-powered Charger (but not the Challenger) in 2026 with a V6 engine, not the classic V8. 


    
        
                                        A Dodge Challenger and a Dodge Charger on display during the 116th New York International Auto Show at the Javits Convention Center in Manhattan on March 24, 2016. (Photo by Cem Ozdel/Anadolu Agency/Getty Images)
                    
    


Jose Pretel, a real-estate photographer and official brand ambassador for Dodge from South Florida who posts photos of his Dodge Charger Hellcat on Instagram (handle @onebadhellcat), acknowledged the challenges the brand has faced but offered an optimistic take at Roadkill Nights in Michigan. “I think some people in the community are skeptical of the changes, right? We went full electric. Now we’re back to a twin turbo, smaller motor than before. They’re kind of wondering: ‘What are they doing?’” Pretel told me. “I actually think this motor is going to last longer, be able to take more abuse … [and] be just as popular in a few years.” 

That, of course, remains to be seen. The environmental regulations that killed off the Challenger and Charger in 2023 are not the entire story behind the decline of the modern muscle car. High interest rates and prices—driven in part by tariffs—plus a declining percentage of young people having driver’s licenses and changing consumer preferences all play a role. The reintroduced gas-powered Charger—even if it brings back the V8 engine—could face the same headwinds in 2026 that the Mustang experienced this year that led to a big drop in sales.

To some, the death of the muscle car would simply be a good thing: They’re loud, gaudy, and (marginally) harm the environment by burning more fossil fuel per mile driven than other vehicles. But to others, they are objects of beauty, outlets for creativity and innovation, and sources of fun, as well as camaraderie.

In speaking to attendees at Roadkill Nights, it quickly became clear that a love of cars isn’t merely about the vehicles themselves; it’s also a way to build and maintain friendships. Shawn Splaney from Flint, Michigan, who attended the event with two friends, told me he owns a ‘68 Mercury Cougar that’s “a basket case” not yet in driving condition and the ‘91 Mustang he typically drives is a labor of love. “I’d rather have the older stuff,” Splaney said. “My Mustang is manual steering, manual valve body, where you have to shift the transmission—you have to drive the car. That’s what I like.”

“It’s my Number One hobby,” he continued. “We have garage time. It’s just a time to hang out with the guys, or girls, and it’s relaxing. It’s fun. Even though you’re struggling with trying to overcome problems at times … you’re just talking, you’re venting, it’s therapeutic.” 

As a non-car-guy interested in becoming one, I’m curious to what extent this hobby, like some other male-dominated hobbies, is largely an excuse for drinking beer. “We’re not heavy drinkers,” Splaney told me, but “sometimes there’s a victory: ‘Hey, let’s go to the bar and kind of show off what we did.’ Sometimes there’s that agony of defeat where you’re like, ‘Hey, maybe I do need a beer because the car’s not fixed yet.’”

Many but not all in attendance are doubtful that car culture can be built up around electric vehicles. Lincoln Brousseau of Grand Blanc, Michigan—a friend of Splaney’s for 15 years since they first met via an online car forum—thinks maybe it’ll catch on with the younger crowd once the proper recharging infrastructure is in place. But the third friend in the group, Brooke Rennert, a 21-year-old from Rochester Hills putting herself through welding school by working as the only woman at her oil-change job, isn’t having any of it. “I don’t like electric cars. I like the sound of a heavy engine. I like the power,” she said. “An electric vehicle has power, but in a different way. It’s not like a big V8, big-block sound.”

For car enthusiasts who like tinkering or hopping up their engines, there’s little interest in electric vehicles. “There’s nothing there to change. You’ve got batteries and a motor,” said Jim Schmittinger from Slinger, Wisconsin, who works on EVs in his job for a Honda dealer. 



    
                    “Building your own stuff, and taking somebody’s car and modifying it to make it what you want. It’s just a part of car culture. And it’s going away.”            
            
                            Mike Sherrow
                                
    


Schmittinger estimates he’s poured $100,000 into the engine of the Buick GNX he’s racing that day, and he said money is one big problem with car culture: “People will spend money they don’t have to compete with somebody they don’t know.” But the worst trend, he said, is that social media has changed the culture for the worse: “It’s not about the cars and getting together with people anymore. It’s about how many likes I get on YouTube or Facebook.” 

Asked for practical advice about how a non-car-guy like myself might become one, Schmittinger simply said: “Start saving your money.” Others tell me the first step is simply getting a car that you love. “Find a car that you actually are enthusiastic about that you want to have and keep,” Splaney said. “Start learning here and there the small stuff, whether it’s just doing oil changes, and just keep pushing that up. We all started somewhere.” 

Rennert, the young woman putting herself through welding school, had some advice for other ladies looking to become car gals: “Make sure you’re in a group of people who respect you. It’s kind of hard sometimes, but you can do it just the same as any guy can do. Meet people. Don’t be afraid not to know anything. Don’t be afraid to ask questions. Do your research. Find a vehicle that really interests you, and go from there.”

As I’m getting ready to leave Pontiac, I’m left wondering, if I ever get around to buying a Challenger, just how expensive and just how old it might be. But, as Schmittinger listened to a friend of his bemoan the demise of the Challenger, he piped up with a prediction: “It’ll be back.”            
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Spiped – secure pipe for SSH, SMTP, etc.]]></title>
            <link>https://www.tarsnap.com/spiped.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056768</guid>
            <description><![CDATA[The spiped secure pipe daemon]]></description>
            <content:encoded><![CDATA[
      
	To set up an encrypted and authenticated pipe for sending email between
	two systems (in the author's case, from many systems around the
	internet to his central SMTP server, which then relays email to the
	rest of the world), one might run
      
      dd if=/dev/urandom bs=32 count=1 of=keyfile
spiped -d -s '[0.0.0.0]:8025' -t '[127.0.0.1]:25' -k keyfile
      
	on a server and after copying keyfile to the local system, run
      
      spiped -e -s '[127.0.0.1]:25' -t $SERVERNAME:8025 -k keyfile
      
	at which point mail delivered via localhost:25 on the
	local system will be securely transmitted to port 25 on the server
	(which is configured to relay mail which arrives from 127.0.0.1 but
	not from other addresses).
      
      
	You can also use spiped to protect SSH servers from attackers: Since
	data is authenticated before being forwarded to the target, this can
	allow you to SSH to a host while protecting you in the event that
	someone finds an exploitable bug in the SSH daemon — this serves the
	same purpose as port knocking or a firewall which restricts source IP
	addresses which can connect to SSH. On the SSH server, run
      
      dd if=/dev/urandom bs=32 count=1 of=/etc/ssh/spiped.key
spiped -d -s '[0.0.0.0]:8022' -t '[127.0.0.1]:22' -k /etc/ssh/spiped.key
      
	then copy the server's /etc/ssh/spiped.key to
	~/.ssh/spiped_HOSTNAME_key on your local system and add
	the lines
      
      Host HOSTNAME
    ProxyCommand spipe -t %h:8022 -k ~/.ssh/spiped_%h_key
      
	to the ~/.ssh/config file. This will cause ssh
	HOSTNAME to automatically connect using the spipe client via
	the spiped daemon; you can then firewall off all incoming traffic on
	port tcp/22.
      
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RFC 8594: The Sunset HTTP Header Field (2019)]]></title>
            <link>https://datatracker.ietf.org/doc/html/rfc8594</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056142</guid>
            <description><![CDATA[This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future. It also defines a sunset link relation type that allows linking to resources providing information about an upcoming resource or service sunset.]]></description>
            <content:encoded><![CDATA[Internet Engineering Task Force (IETF)                          E. Wilde
Request for Comments: 8594                                      May 2019
Category: Informational
ISSN: 2070-1721


                      The Sunset HTTP Header Field

Abstract

   This specification defines the Sunset HTTP response header field,
   which indicates that a URI is likely to become unresponsive at a
   specified point in the future.  It also defines a sunset link
   relation type that allows linking to resources providing information
   about an upcoming resource or service sunset.

Status of This Memo

   This document is not an Internet Standards Track specification; it is
   published for informational purposes.

   This document is a product of the Internet Engineering Task Force
   (IETF).  It represents the consensus of the IETF community.  It has
   received public review and has been approved for publication by the
   Internet Engineering Steering Group (IESG).  Not all documents
   approved by the IESG are candidates for any level of Internet
   Standard; see Section 2 of RFC 7841.

   Information about the current status of this document, any errata,
   and how to provide feedback on it may be obtained at
   https://www.rfc-editor.org/info/rfc8594.

Copyright Notice

   Copyright (c) 2019 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (https://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.





Wilde                         Informational                     [Page 1]

RFC 8594                      Sunset Header                     May 2019


Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Temporary Resources . . . . . . . . . . . . . . . . . . .   3
     1.2.  Migration . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.3.  Retention . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.4.  Deprecation . . . . . . . . . . . . . . . . . . . . . . .   3
   2.  Terminology . . . . . . . . . . . . . . . . . . . . . . . . .   4
   3.  The Sunset HTTP Response Header Field . . . . . . . . . . . .   4
   4.  Sunset and Caching  . . . . . . . . . . . . . . . . . . . . .   5
   5.  Sunset Scope  . . . . . . . . . . . . . . . . . . . . . . . .   6
   6.  The Sunset Link Relation Type . . . . . . . . . . . . . . . .   6
   7.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .   7
     7.1.  The Sunset Response Header Field  . . . . . . . . . . . .   7
     7.2.  The Sunset Link Relation Type . . . . . . . . . . . . . .   8
   8.  Security Considerations . . . . . . . . . . . . . . . . . . .   8
   9.  Example . . . . . . . . . . . . . . . . . . . . . . . . . . .   9
   10. References  . . . . . . . . . . . . . . . . . . . . . . . . .  10
     10.1.  Normative References . . . . . . . . . . . . . . . . . .  10
     10.2.  Informative References . . . . . . . . . . . . . . . . .  10
   Acknowledgements  . . . . . . . . . . . . . . . . . . . . . . . .  10
   Author's Address  . . . . . . . . . . . . . . . . . . . . . . . .  11

1.  Introduction

   As a general rule, URIs should be stable and persistent so that
   applications can use them as stable and persistent identifiers for
   resources.  However, there are many scenarios where, for a variety of
   reasons, URIs have a limited lifetime.  In some of these scenarios,
   this limited lifetime is known in advance.  In this case, it can be
   useful for clients if resources make this information about their
   limited lifetime known.  This specification defines the Sunset HTTP
   response header field, which indicates that a URI is likely to become
   unresponsive at a specified point in the future.

   This specification also defines a sunset link relation type that
   allows information to be provided about 1) the sunset policy of a
   resource or a service, and/or 2) upcoming sunsets, and/or 3) possible
   mitigation scenarios for resource/service users.  This specification
   does not place any constraints on the nature of the linked resource,
   which can be targeted to humans, machines, or both.

   Possible scenarios for known lifetimes of resources include, but are
   not limited to, the following scenarios.







Wilde                         Informational                     [Page 2]

RFC 8594                      Sunset Header                     May 2019


1.1.  Temporary Resources

   Some resources may have a limited lifetime by definition.  For
   example, a pending shopping order represented by a resource may
   already list all order details, but it may only exist for a limited
   time unless it is confirmed and only then becomes an acknowledged
   shopping order.  In such a case, the service managing the pending
   shopping order can make this limited lifetime explicit, allowing
   clients to understand that the pending order, unless confirmed, will
   disappear at some point in time.

1.2.  Migration

   If resources are changing identity because a service migrates them,
   then this may be known in advance.  While it may not yet be
   appropriate to use HTTP redirect status codes (3xx), it may be
   interesting for clients to learn about the service's plan to take
   down the original resource.

1.3.  Retention

   There are many cases where regulation or legislation require that
   resources are kept available for a certain amount of time.  However,
   in many cases there is also a requirement for those resources to be
   permanently deleted after some period of time.  Since the deletion of
   the resource in this scenario is governed by well-defined rules, it
   could be made explicit for clients interacting with the resource.

1.4.  Deprecation

   For Web APIs one standard scenario is that an API or specific subsets
   of an API may get deprecated.  Deprecation often happens in two
   stages: the first stage being that the API is not the preferred or
   recommended version anymore and the second stage being that the API
   or a specific version of the API gets decommissioned.

   For the first stage (the API is not the preferred or recommended
   version anymore), the Sunset header field is not appropriate: at this
   stage, the API remains operational and can still be used.  Other
   mechanisms can be used for signaling that first stage that might help
   with more visible deprecation management, but the Sunset header field
   does not aim to represent that information.

   For the second stage (the API or a specific version of the API gets
   decommissioned), the Sunset header field is appropriate: that is when
   the API or a version does become unresponsive.  From the Sunset
   header field's point of view, it does not matter that the API may not




Wilde                         Informational                     [Page 3]

RFC 8594                      Sunset Header                     May 2019


   have been the preferred or recommended version anymore.  The only
   thing that matters is that it will become unresponsive and that this
   time can be advertised using the Sunset header field.

   In this scenario, the announced sunset date typically affects all of
   the deprecated API or parts of it (i.e., just deprecated sets of
   resources), and not just a single resource.  In this case, it makes
   sense for the API to define rules about how an announced sunset on a
   specific resource (such as the API's home/start resource) implies the
   sunsetting of the whole API or parts of it (i.e., sets of resources),
   and not just the resource returning the sunset header field.
   Section 5 discusses how the scope of the Sunset header field may
   change because of how a resource is using it.

2.  Terminology

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

3.  The Sunset HTTP Response Header Field

   The Sunset HTTP response header field allows a server to communicate
   the fact that a resource is expected to become unresponsive at a
   specific point in time.  It provides information for clients that
   they can use to control their usage of the resource.

   The Sunset header field contains a single timestamp that advertises
   the point in time when the resource is expected to become
   unresponsive.  The Sunset value is an HTTP-date timestamp, as defined
   in Section 7.1.1.1 of [RFC7231], and SHOULD be a timestamp in the
   future.

   It is safest to consider timestamps in the past mean the present
   time, meaning that the resource is expected to become unavailable at
   any time.

   Sunset = HTTP-date

   For example:

   Sunset: Sat, 31 Dec 2018 23:59:59 GMT







Wilde                         Informational                     [Page 4]

RFC 8594                      Sunset Header                     May 2019


   Clients SHOULD treat Sunset timestamps as hints: it is not guaranteed
   that the resource will, in fact, be available until that time and
   will not be available after that time.  However, since this
   information is provided by the resource itself, it does have some
   credibility.

   After the Sunset time has arrived, it is likely that interactions
   with the resource will result in client-side errors (HTTP 4xx status
   codes), redirect responses (HTTP 3xx status codes), or the client
   might not be able to interact with the resource at all.  The Sunset
   header field does not expose any information about which of those
   behaviors can be expected.

   Clients not interpreting an existing Sunset header field can operate
   as usual and simply may experience the resource becoming unavailable
   without recognizing any notification about it beforehand.

4.  Sunset and Caching

   It should be noted that the Sunset HTTP response header field serves
   a different purpose than HTTP caching [RFC7234].  HTTP caching is
   concerned with making resource representations (i.e., represented
   resource state) reusable so that they can be used more efficiently.
   This is achieved by using header fields that allow clients and
   intermediaries to better understand when a resource representation
   can be reused or when resource state (and, thus, the representation)
   may have changed.

   The Sunset header field is not concerned with resource state at all.
   It only signals that a resource is expected to become unavailable at
   a specific point in time.  There are no assumptions about if, when,
   or how often a resource may change state in the meantime.

   For these reasons, the Sunset header field and HTTP caching should be
   seen as complementary and not as overlapping in scope and
   functionality.

   This also means that applications acting as intermediaries, such as
   search engines or archives that make resources discoverable, should
   treat Sunset information differently from caching information.  These
   applications may use Sunset information for signaling to users that a
   resource may become unavailable.  But they still have to account for
   the fact that resource state can change in the meantime and that
   Sunset information is a hint and, thus, future resource availability
   may differ from the advertised timestamp.






Wilde                         Informational                     [Page 5]

RFC 8594                      Sunset Header                     May 2019


5.  Sunset Scope

   The Sunset header field applies to the resource that returns it,
   meaning that it announces the upcoming sunset of that specific
   resource.  However, as discussed in Section 1.4, there may be
   scenarios where the scope of the announced Sunset information is
   larger than just the single resource where it appears.

   Resources are free to define such an increased scope, and usually
   this scope will be documented by the resource so that consumers of
   the resource know about the increased scope and can behave
   accordingly.  However, it is important to take into account that such
   increased scoping is invisible for consumers who are unaware of the
   increased scoping rules.  This means that these consumers will not be
   aware of the increased scope, and they will not interpret Sunset
   information different from its standard meaning (i.e., it applies to
   the resource only).

   Using such an increased scope still may make sense, as Sunset
   information is only a hint anyway; thus, it is optional information
   that cannot be depended on, and clients should always be implemented
   in ways that allow them to function without Sunset information.
   Increased scope information may help clients to glean additional
   hints from resources (e.g., concluding that an API is being
   deprecated because its home/start resource announces a Sunset) and,
   thus, might allow them to implement behavior that allows them to make
   educated guesses about resources becoming unavailable.

6.  The Sunset Link Relation Type

   The Sunset HTTP header field indicates the upcoming retirement of a
   resource or a service.  In addition, a resource may want to make
   information available that provides additional information about how
   retirement will be handled for resources or services.  This
   information can be broadly described by the following three topics:

   Sunset policy:  The policy for which resources and in which way
         sunsets may occur may be published as part of service's
         description.  Sunsets may only/mostly affect a subset of a
         service's resources, and they may be exposed according to a
         certain policy (e.g., one week in advance).

   Upcoming sunset:  There may be additional information about an
         upcoming sunset, which can be published as a resource that can
         be consumed by those looking for this additional information.






Wilde                         Informational                     [Page 6]

RFC 8594                      Sunset Header                     May 2019


   Sunset mitigation:  There may be information about possible
         mitigation/migration strategies, such as possible ways how
         resource users can switch to alternative resources/services.

   Any information regarding the above issues (and possibly additional
   ones) can be made available through a URI that then can be linked to
   using the sunset link relation type.  This specification places no
   constraints on the scope or the type of the linked resource.  The
   scope can be for a resource or for a service.  The type is determined
   by the media type of the linked resource and can be targeted to
   humans, machines, or both.

   If the linked resource does provide machine-readable information,
   consumers should be careful before acting on this information.  Such
   information may, for example, instruct consumers to use a migration
   rule so that sunset resources can be accessed at new URIs.  However,
   this kind of information amounts to a possibly large-scale identity
   migration of resources, so it is crucial that the migration
   information is authentic and accurate.

7.  IANA Considerations

7.1.  The Sunset Response Header Field

   The Sunset response header field has been added to the "Permanent
   Message Header Field Names" registry (see [RFC3864]), taking into
   account the guidelines given by HTTP/1.1 [RFC7231].

      Header Field Name: Sunset

      Protocol: http

      Status: informational

      Author/Change controller: IETF

      Reference: RFC 8594














Wilde                         Informational                     [Page 7]

RFC 8594                      Sunset Header                     May 2019


7.2.  The Sunset Link Relation Type

   The sunset link relation type has been added to the permanent "Link
   Relation Types" registry according to Section 4.2 of [RFC8288]:

      Relation Name: sunset

      Description: Identifies a resource that provides information about
      the context's retirement policy.

      Reference: RFC 8594

8.  Security Considerations

   Generally speaking, information about upcoming sunsets can leak
   information that otherwise might not be available.  For example, a
   resource representing a registration can leak information about the
   expiration date when it exposes sunset information.  For this reason,
   any use of sunset information where the sunset represents an
   expiration or allows the calculation of another date (such as
   calculating a creation date because it is known that resources expire
   after one year) should be treated in the same way as if this
   information would be made available directly in the resource's
   representation.

   The Sunset header field SHOULD be treated as a resource hint, meaning
   that the resource is indicating (and not guaranteeing with certainty)
   its potential retirement.  The definitive test whether or not the
   resource in fact is available will be to attempt to interact with it.
   Applications should never treat an advertised Sunset date as a
   definitive prediction of what is going to happen at the specified
   point in time: the Sunset indication may have been inserted by an
   intermediary or the advertised date may get changed or withdrawn by
   the resource owner.

   The main purpose of the Sunset header field is to signal intent so
   that applications using resources may get a warning ahead of time and
   can react accordingly.  What an appropriate reaction is (such as
   switching to a different resource or service), what it will be based
   on (such as machine-readable formats that allow the switching to be
   done automatically), and when it will happen (such as ahead of the
   advertised date or only when the resource in fact becomes
   unavailable) is outside the scope of this specification.

   In cases where a sunset policy is linked by using the sunset link
   relation type, clients SHOULD be careful about taking any actions
   based on this information.  It SHOULD be verified that the
   information is authentic and accurate.  Furthermore, it SHOULD be



Wilde                         Informational                     [Page 8]

RFC 8594                      Sunset Header                     May 2019


   tested that this information is only applied to resources that are
   within the scope of the policy, making sure that sunset policies
   cannot "hijack" resources by for example providing migration
   information for them.

9.  Example

   If a resource has been created in an archive that, for management or
   compliance reasons, stores resources for ten years and permanently
   deletes them afterwards, the Sunset header field can be used to
   expose this information.  If such a resource has been created on
   November 11, 2016, then the following header field can be included in
   responses:

   Sunset: Wed, 11 Nov 2026 11:11:11 GMT

   This allows clients that are aware of the Sunset header field to
   understand that the resource likely will become unavailable at the
   specified point in time.  Clients can decide to ignore this
   information, adjust their own behavior accordingly, or alert
   applications or users about this timestamp.

   Even though the Sunset header field is made available by the resource
   itself, there is no guarantee that the resource indeed will become
   unavailable, and if so, how the response will look like for requests
   made after that timestamp.  In case of the archive used as an example
   here, the resource indeed may be permanently deleted, and requests
   for the URI after the Sunset timestamp may receive a "410 Gone" HTTP
   response.  (This is assuming that the archive keeps track of the URIs
   that it had previously assigned; if not, the response may be a more
   generic "404 Not Found".)

   Before the Sunset header field even appears for the first time (it
   may not appear from the very beginning), it is possible that the
   resource (or possibly just the "home" resource of the service
   context) communicates its sunset policy by using the sunset link
   relation type.  If communicated as an HTTP header field, it might
   look as follows:

   Link: <http://example.net/sunset>;rel="sunset";type="text/html"

   In this case, the linked resource provides sunset policy information
   about the service context.  It may be documentation aimed at
   developers, for example, informing them that the lifetime of a
   certain class of resources is ten years after creation and that
   Sunset header fields will be served as soon as the sunset date is





Wilde                         Informational                     [Page 9]

RFC 8594                      Sunset Header                     May 2019


   less than some given period of time.  It may also inform developers
   whether the service will respond with 410 or 404 after the sunset
   time, as discussed above.

10.  References

10.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://www.rfc-editor.org/info/rfc2119>.

   [RFC3864]  Klyne, G., Nottingham, M., and J. Mogul, "Registration
              Procedures for Message Header Fields", BCP 90, RFC 3864,
              DOI 10.17487/RFC3864, September 2004,
              <https://www.rfc-editor.org/info/rfc3864>.

   [RFC7231]  Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer
              Protocol (HTTP/1.1): Semantics and Content", RFC 7231,
              DOI 10.17487/RFC7231, June 2014,
              <https://www.rfc-editor.org/info/rfc7231>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://www.rfc-editor.org/info/rfc8174>.

   [RFC8288]  Nottingham, M., "Web Linking", RFC 8288,
              DOI 10.17487/RFC8288, October 2017,
              <https://www.rfc-editor.org/info/rfc8288>.

10.2.  Informative References

   [RFC7234]  Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke,
              Ed., "Hypertext Transfer Protocol (HTTP/1.1): Caching",
              RFC 7234, DOI 10.17487/RFC7234, June 2014,
              <https://www.rfc-editor.org/info/rfc7234>.

Acknowledgements

   Thanks for comments and suggestions provided by Ben Campbell, Alissa
   Cooper, Benjamin Kaduk, Mirja Kuhlewind, Adam Roach, Phil Sturgeon,
   and Asbjorn Ulsberg.








Wilde                         Informational                    [Page 10]

RFC 8594                      Sunset Header                     May 2019


Author's Address

   Erik Wilde

   Email: erik.wilde@dret.net
   URI:   http://dret.net/netdret/













































Wilde                         Informational                    [Page 11]
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[I’m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

                ❄                ❄                ❄                ❄

I’ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren’t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn’t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that’s going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

                ❄                ❄                ❄                ❄

I’m often asked, “what is the future of programming?” Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it’s too late? My answer to all these questions is “I haven’t the foggiest”. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

                ❄                ❄               ❇                ❄

I’m also asked: “is AI a bubble”? To which my answer is “OF COURSE IT’S A BUBBLE”. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don’t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan… but it did not kill Amazon.

                ❄                ❄                ❄                ❄

I retired from public speaking a couple of years ago. But while I don’t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I’m looking forward to catching up with many of them at GOTO Copenhagen. I’ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

                ✢                ❄                ❄                ❄

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren’t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it’s just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn’t ask an LLM to calculate an answer than we can calculate deterministically (yes, I’ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

                ❄                ❄                ❄                ❄

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can’t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

                ❄                ❄                ❄                ❄

I’ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say “all tests green”, yet when I run them, there are failures. If that was a junior engineer’s behavior, how long would it be before H.R. was involved?

                ❄                ❄                ❄                ❄

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (“exfiltration”). That “untrusted content” can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attacker’s web page, and it could trick the agent to go to your bank account in another tab and “buy you a present” by transferring your balance to the kind attacker. Willison’s view is that “the entire concept of an agentic browser extension is fatally flawed and cannot be built safely”.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Bot Auth]]></title>
            <link>https://developers.cloudflare.com/bots/reference/bot-verification/web-bot-auth/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055452</guid>
            <description><![CDATA[Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.]]></description>
            <content:encoded><![CDATA[          Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.
It relies on two active IETF drafts: a directory draft ↗ allowing the crawler to share their public keys, and a protocol draft ↗ defining how these keys should be used to attach crawler's identity to HTTP requests.
This documentation goes over specific integration within Cloudflare.

You need to generate a signing key which will be used to authenticate your bot's requests.



Generate a unique Ed25519 ↗ private key to sign your requests. This example uses the OpenSSL ↗ genpkey command:
openssl genpkey -algorithm ed25519 -out private-key.pem


Extract your public key.
openssl pkey -in private-key.pem -pubout -out public-key.pem


Convert the public key to JSON Web Key (JWK) using a tool of your choice. This example uses jwker ↗ command line application.
go install github.com/jphastings/jwker/cmd/jwker@latestjwker public-key.pem public-key.jwk


By following these steps, you have generated a private key and a public key, then converted the public key to a JWK.


You need to host a key directory which creates a way for your bot to authenticate its requests to Cloudflare.
This directory should follow the definition from the active IETF draft draft-meunier-http-message-signatures-directory-01 ↗.


Host a key directory at /.well-known/http-message-signatures-directory (note that this is a requirement). This key directory should serve a JSON Web Key Set (JWKS) including the public key derived from your signing key.


Serve the web page over HTTPS (not HTTP).


Calculate the base64 URL-encoded JWK thumbprint ↗ associated with your Ed25519 public key.


Sign your HTTP response using the HTTP message signature specification by attaching one signature per key in your key directory. This ensures no one else can mirror your directory and attempt to register on your behalf. Your response must include the following headers:

Content-Type: This header must have the value application/http-message-signatures-directory+json.
Signature: Construct a Signature header ↗ over your chosen components.
Signature-Input: Construct a Signature-Input header ↗ over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to http-message-signatures-directory.keyidJWK thumbprint of the corresponding key in your directory.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message.


The following example shows the annotated request and response with required headers against https://example.com.
GET /.well-known/http-message-signatures-directory HTTP/1.1Host: example.comAccept: application/http-message-signatures-directory+jsonHTTP/1.1 200 OKContent-Type: application/http-message-signatures-directory+jsonSignature: sig1=:TD5arhV1ved6xtx63cUIFCMONT248cpDeVUAljLgkdozbjMNpJGr/WAx4PzHj+WeG0xMHQF1BOdFLDsfjdjvBA==:Signature-Input: sig1=("@authority");alg="ed25519";keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U";nonce="ZO3/XMEZjrvSnLtAP9M7jK0WGQf3J+pbmQRUpKDhF9/jsNCWqUh2sq+TH4WTX3/GpNoSZUa8eNWMKqxWp2/c2g==";tag="http-message-signatures-directory";created=1750105829;expires=1750105839Cache-Control: max-age=86400{  "keys": [{    "kty": "OKP",    "crv": "Ed25519",    "x": "JrQLj5P_89iXES9-vFgrIy29clF9CC_oPPsw3c5D0bs", // Base64 URL-encoded public key, with no padding  }]}



You can use the Cloudflare-developed http-signature-directory CLI tool ↗ to assist you in validating your directory.

You need to register your bot and its key directory to add your bot to the list of verified bots.

Log in to the Cloudflare dashboard ↗, and select your account and domain.
Go to Manage Account > Configurations.
Go to the Verified Bots tab.
For Verification Method: select Request Signature.
For Validation Instructions: enter the URL of your key directory. You can additionally supply User Agents values (and their match patterns) that will be sent by your bot.
Select Submit.

Cloudflare accepts all valid Ed25519 keys found in your key directory. In the event a key already exists in Cloudflare's registered database, Cloudflare will work with you to supply a new key, or rotate your existing key.


After your bot has been successfully verified, your bot is ready to sign its requests. The signature protocol is defined in draft-meunier-web-bot-auth-architecture-02 ↗

Choose a set of components to sign.
A component is either an HTTP header, or any derived components ↗ in the HTTP Message Signatures specification. Cloudflare recommends the following:

Choose at least the @authority derived component, which represents the domain you are sending requests to. For example, a request to https://example.com will be interpreted to have an @authority of example.com.
Use components that only contain ASCII values. HTTP Message Signature specification disallows non-ASCII characters, which will result in failure to validate your bot's requests.




Calculate the base64 URL-encoded JWK thumbprint ↗ from the public key you registered with Cloudflare.

Construct the three required headers for Web Bot Auth.

Construct a Signature-Input header ↗ over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to web-bot-auth.keyidThis should be equal to the thumbprint computed in step 2.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message. A short expires reduces the likelihood of replay attacks, and Cloudflare recommends choosing suitable short-lived intervals.

Construct a Signature header ↗ over your chosen components.

Construct a Signature-Agent header ↗ that points to your key directory. Note that Cloudflare will fail to verify a message if:

The message includes a Signature-Agent header that is not an https://.
The message includes a valid URI but does not enclose it in double quotes. This is due to Signature-Agent being a structured field.
The message has a valid Signature-Agent header, but does not include it in the component list in Signature-Input.


Attach these three headers to your bot's requests.
An example request may look like this:
Signature-Agent: "https://signature-agent.test"Signature-Input: sig2=("@authority" "signature-agent") ;created=1735689600 ;keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U" ;alg="ed25519" ;expires=1735693200 ;nonce="e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==" ;tag="web-bot-auth"Signature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:


You may wish to refer to the following resources.

Bots FAQs.
Cloudflare blog: Message Signatures are now part of our Verified Bots Program ↗.
Cloudflare blog: Forget IPs: using cryptography to verify bot and agent traffic ↗.
Cloudflare's web-bot-auth library in Rust ↗.
Cloudflare's web-bot-auth npm package in Typescript ↗.
        Resources     API     New to Cloudflare?     Directory     Sponsorships     Open Source     Support     Help Center     System Status     Compliance     GDPR     Company     cloudflare.com     Our team     Careers     Tools     Cloudflare Radar     Speed Test     Is BGP Safe Yet?     RPKI Toolkit     Certificate Transparency     Community     X     Discord     YouTube     GitHub      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison’s LLM CLI, you’ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren’t just chatbots or
      autocomplete tools - they’re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that’s exactly what we did, and in
      this article I’ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You’ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they’re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we’re
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (I’ve
      shortened ours for the article). Now something magical happened. I could
      say “X isn’t working” and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes “fixed” failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al’s default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned “Bedrock is timing out”
      or “the model responses are getting truncated,” the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the iceberg—the AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentation—recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask “Why is
      this API call failing intermittently?” and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation—especially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capable—it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say “The authentication tests are failing, please fix the issue”
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestions—it was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit—like having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development—from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourself—even a simple version—gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VLT observations of interstellar comet 3I/ATLAS II]]></title>
            <link>https://arxiv.org/abs/2508.18382</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055335</guid>
            <description><![CDATA[We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Å). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3σ$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.]]></description>
            <content:encoded><![CDATA[
    
    
    Authors:Rohan Rahatgaonkar, Juan Pablo Carvajal, Thomas H. Puzia, Baltasar Luco, Emmanuel Jehin, Damien Hutsemékers, Cyrielle Opitom, Jean Manfroid, Michaël Marsset, Bin Yang, Laura Buchanan, Wesley C. Fraser, John Forbes, Michele Bannister, Dennis Bodewits, Bryce T. Bolin, Matthew Belyakov, Matthew M. Knight, Colin Snodgrass, Erica Bufanda, Rosemary Dorsey, Léa Ferellec, Fiorangela La Forgia, Manuela Lippi, Brian Murphy, Prasanta K. Nayak, Mathieu Vander Donckt            
    View PDF
    HTML (experimental)
            Abstract:We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Å). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3\sigma$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.
    

    
    
              
          Comments:
          12 pages, 3 figures, 2 tables, submitted to ApJL
        

          Subjects:
          
            Solar and Stellar Astrophysics (astro-ph.SR); Earth and Planetary Astrophysics (astro-ph.EP)
        
          Cite as:
          arXiv:2508.18382 [astro-ph.SR]
        
        
           
          (or 
              arXiv:2508.18382v1 [astro-ph.SR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.18382
              
                                arXiv-issued DOI via DataCite
            
          
        
    
  
      Submission history From: Thomas H. Puzia [view email]          [v1]
        Mon, 25 Aug 2025 18:15:44 UTC (2,178 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates aren’t exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesn’t exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know what’s wrong with people?
                They’re too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence — however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say “it depends” all the time.
                Much more fun getting to say
                “let’s ship it and iterate” as Staff or
                “that won’t scale” as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? 🤨
}

              GPS coordinates aren’t exact.
                They’re noisy. They’re approximate. They’re probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              you’re probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesn’t capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach that’s both
              mathematically rigorous and surprisingly practical.
            
            As you’d expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2σ confidence 🤓
}

            When you compare two Uncertain values,
              you don’t get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/m³
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // m²
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed —
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesn’t exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              let’s take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or “fruit machine” for our UK readers 🇬🇧):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "◻️", "◻️", "◻️",  // blanks
            "🍒", "🍋", "🍊", "🍇", "💎"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("💎", "💎", "💎"): return 100  // Jackpot!
        case ("🍒", "🍒", "🍒"): return 10
        case ("🍇", "🍇", "🍇"): return 5
        case ("🍊", "🍊", "🍊"): return 3
        case ("🍋", "🍋", "🍋"): return 2
        case ("🍒", _, _), // Any cherry
             (_, "🍒", _),
             (_, _, "🍒"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically —
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: ≈ $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23°C
let tempSpread = temperature.standardDeviation() // about 1°C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)°C and \(upper)°C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp ≤ 25°C) < 20%
    print("Unlikely to be 25°C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users don’t notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is ±1000 meters,
            that’s a bad look 🤡
          So where do we go from here?
            Let’s channel our Senior+ memes from before for guidance.
          That Staff engineer saying “let’s ship it and iterate”
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineer’s warning of “that won’t scale”.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isn’t to eliminate uncertainty —
            it’s to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: The government of my country blocked VPN access. What should I use?]]></title>
            <link>https://news.ycombinator.com/item?id=45054260</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054260</guid>
            <description><![CDATA[Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).]]></description>
            <content:encoded><![CDATA[
Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).- First things first, you have to get your hands on actual VPN software and configs. Many providers who are aware of VPN censorship and cater to these locales distribute their VPNs through hard-to-block channels and in obfuscated packages. S3 is a popular option but by no means the only one, and some VPN providers partner with local orgs who can figure out the safest and most efficient ways to distribute a VPN package in countries at risk of censorship or undergoing censorship.- Once you've got the software, you should try to use it with an obfuscation layer.Obfs4proxy is a popular tool here, and relies on a pre-shared key to make traffic look like nothing special. IIRC it also hides the VPN handshake. This isn't a perfectly secure model, but it's good enough to defeat most DPI setups.Another option is Shapeshifter, from Operator (https://github.com/OperatorFoundation). Or, in general, anything that uses pluggable transports. While it's a niche technology, it's quite useful in your case.In both cases, the VPN provider must provide support for these protocols.- The toughest step long term is not getting caught using a VPN. By its nature, long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a state actor). I don't know the situation on the ground in Indonesia, so I won't speculate about what the best way to avoid this would be, long-term.I will endorse Mullvad as a trustworthy and technically competent VPN provider in this niche (n.b., I do not work for them, nor have I worked for them; they were a competitor to my employer and we always respected their approach to the space).
This is no 'nothing special' with Obfs4proxy. DPI sees it as random byte stream, thus your government can decide to block unknown protocols. Instead, you should trick DPI into thinking it sees HTTPS. Unless your government decides to block HTTPS.
> First things first, you have to get your hands on actual VPN software and configs.It would be nice if one of the big shortwave operators could datacast these packages to the world as a public service.
The problem is the countries, which censor Internet and block VPNs, also jam shortwave radio signals.
There are some techniques like fragmented TLS and reordered packets that work in some cases. Also using vanilla HTTPS transport is a good start for many places. URnetwork is an open source, decentralized option that does all of these out of the box. You can get it on the major stores or F-Droid.
Thank you very much for a detailed answer. Might I rudely ask -- as you're knowledgeable in this space, what do you think of Mullvad's DAITA, which specifically aims to defeat traffic analysis by moving to a more pulsed constant bandwidth model?
DAITA was introduced after my time in the industry, but this isn't a new idea (though as far as I know, it's the first time this kind of thing's been commercialized).It's clever. It tries to defeat attacks against one of the tougher parts of VPN connections to reliably obfuscate, and the effort's commendable, but I'll stop short of saying it's a good solution for one big reason: with VPNs and censorship circumvention, the data often speaks for itself.A VPN provider working in this space will often have aggregate (and obviously anonymized, if they're working in good faith) stats about success rates and failure classes encountered from clients connecting to their nodes. Where I worked, we didn't publish this information. I'm not sure where Mullvad stands on this right now.In any case -- some VPN providers deploying new technology like this will partner with the research community (because there's a small, but passionate formal research community in this space!) and publish papers, studies, and other digests of their findings. Keep an eye out for this sort of stuff. UMD's Breakerspace in the US in particular had some extremely clever people working on this stuff when I was involved in the industry.
Obfs4proxy and Shapeshifter are an absolute PITA to install.Get your own VPS server (VPS in EU/US with 2GB of ram, 40GB of disk space and TBs/month of traffic go for $10 a year, it's that cheap). Never get anything in the UK and even USA is weird. I'd stick with EU.Install your software (wireguard + obsfuscation or even tailscale with your own DERP server)Another simpler alternative is just `ssh -D port` and use it as a SOCKS server. It's usually not blocked but very obvious.
Because you are leaking information left and right with TCP / DNS and all these basic protocols that powering the internet today. When these were designed people were happy that it worked at all and nobody really tought that it should be state actor proof. Except maybe DJB. https://www.curvecp.org/
The "inspection" part of DPI isn't limited to encrypted payloads. It's straightforward enough to look at application-level protocol headers and identify e.g. a Wireguard or OpenVPN or SSH connection, even if you can't decrypt the payload. That could be used as sufficient grounds to either block the traffic or punish the user.
DPI refers to a broad class of products which attempt to find signals and categorize traffic according to a ruleset, either to block it or throttle the speeds, etc.While access to plaintext is useful, it's not required for other rules which are eg looking at the timing and frequency of packets.
Patterns of data transmission (network behavioral analysis, I just made that term up), analyzing IP and ports, inspecting SSL handshakes for destination site. In short, metadata.
This makes me wonder: are there "cloud drive virtual sneakernet" systems that will communicate e.g. by a client uploading URL request(s) as documents via OneDrive/SharePoint/Google Drive/Baidu etc., a server reacting to this via webhook and uploading (say) a PDF version of the rendered site, then allowing the client to download that PDF? You effectively use the CDN of that service as a (very slow) proxy.Of course, https://xkcd.com/538/ applies in full force, and I don't have any background in the space to make this a recommendation!
It doesn't apply imo as OP is probably not a high value target of the govt, he just wants to bypass his govt restrictions and I doubt the situation is so bad that the govt will send people physically to deal with people circumventing the block.Your solution could technically work over any kind of open connection / data transfer protocol that isn't blocked by the provider but it would be an absolute pain to browse the web that way and there are probably better solutions out there.
I’m curious about what makes it difficult to block a vpn provider long term. You said getting the software is difficult, but can a country not block known vpn ingress points?
A country can and absolutely will block known VPN ingress points. There are two tricks that we can use to circumvent this:- Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc). Bonus points if you can leverage something like ECH (ex-ESNI) to make it harder to identify a single bucket or subdomain.- Keep spawning new domains and subdomains to distribute your binaries.There are complications with both approaches. Some countries block ECH outright. Some have no problem shutting the internet down wholesale for a little bit. The domain-hopping approach presents challenges w/r/t establishing trust (though not insurmountable ones, much of the time).These are thing that have to be judged and balanced on a case-by-case basis, and having partners on the ground in these places really helps reduce risk to users trying to connect from these places, but then you have to be very careful talking to then since they could themselves get in trouble for trying to organize a VPN distribution network with you. It's layers on layers, and at some point it helps to just have someone on the team with a background in working with people in vulnerable sectors and someone else from a global affairs and policy background to try and keep things as safe as they can be for people living under these regimes.
I've heard of domain fronting, where you host something on a subdomain of a large provider like Azure or Amazon. Is this what you're talking about when you say> - Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc).How can one bounce VPN traffic through S3? Or are you just talking about hosting client software, ingress IP address lists, etc?
That's generally for distribution, but yeah, it's a form of domain fronting.There are some more niche techniques that are _really_ cool but haven't gained widespread adoption, too, like refractive routing. The logistics of getting that working are particularly challenging since you need a willing partner who'll undermine some of their trustworthiness with some actors to support (what is, normally, to them) your project.
Sorry I’m referring to WireGuard/ovpn server IPs, not the binaries/configs used to setup a client. Unless you’re talking about fronting for both, but I imagine it is not economical to run a commercial -scale privacy vpn via a cloud provider.
I wonder if it can be embedded in a video stream, like a video of a lava lamp that you always have open, but the lsb of ever byte is meaningful.
That's an interesting idea, and probably something you might be able to achieve with a tool like h26forge.It's also probably more useful to just have a connection be fully dedicated to a VPN, and have the traffic volume over time mimic what you'd see in a video, rather than embedding it in a video -- thanks to letsencrypt, much of the web's served over TLS these days (asterisks for countries like KZ and TM which force the use of a state-sponsored CA), so going to great lengths to embed your VPN in a video isn't really practical.
I lived in China for a while and there were several waves of VPN blocks. Also very few VPN services even try to actively support VPN-blocking nations anymore. Any commercial offering will be blocked eventually.What I settled on for decent reliability and speeds was a free-tier EC2 hosted in an international region. I then setup a SOCKS5 server and connected my devices to it. You mentioned Cloudflare so whatever their VM service is might also work.It's very low profile as it's just your traffic and the state can't easily differentiate your host from the millions of others in that cloud region.LPT for surviving the unfree internet: GitHub won't be blocked and you'll find all the resources and downloads you need for this method and others posted by Chinese engineers.Edit: If you're worried about being too identifiable because of your static IP, well it's just a computer, you can use a VPN on there too if you want to!
GitHub was briefly blocked a couple of years ago in Indonesia. SSH was also blocked briefly by one of the largest mobile providers.
The VM instance is good for setting up a VPN tunnel, but it's not good in terms of bandwidth if it's hosted in. Because of DPI capacity, China has a very limited amount of "real internet" bandwidth. A more capable setup is to have one VM on each side of the firewall on an hosting service with peering between inside and outside - Aliyun (Alibaba Cloud) is an example. The "inside" VM could be just "socat UDP4-RECVFROM:<port>,fork UDP4-SENDTO:<remote>:<port>" or something done using netfilter.Like others commented in this thread, having an obfuscator is a good idea to ensure the traffic is not dropped by DPI.When the inevitable ban comes and your VPN stops working, rotate the IP of the external VPN and update the firewall/socat config to reflect it. Usually, the internal VM's IP doesn't need to be updated.
When I worked in China (not for long periods but frequently enough that the Great Firewall became an irritant) I hosted an OpenVPN server on port 443 and/or port 22 of a server I owned. That worked sufficiently well most of the time.
This doesn't work anymore; the GFW no longer detects VPN connections by port but instead by performing deep packet inspection to characterize the type of traffic going over every connection. Using this technique in combination with some advanced ML systems, they're able to detect any encrypted VPN connection and cut it off; it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore, and it's usually not even possible to _tunnel_ a VPN connection through some other protocol because the GFW now detects that too.Stepping back and looking at it from a purely technical perspective, it's actually insanely impressive.Here's a USENIX paper from a few years ago on how it is done: https://gfw.report/publications/usenixsecurity23/en/
This is what IPsec TFS is for [https://datatracker.ietf.org/doc/rfc9347/]> the focus in this document is to enhance IP Traffic Flow Security (IP-TFS) by adding Traffic Flow Confidentiality (TFC) to encrypted IP-encapsulated traffic.  TFC is provided by obscuring the size and frequency of IP traffic using a fixed-size, constant-send-rate IPsec tunnel(If they block a constant rate stream, that'll hit a whole ton of audio/video streaming setups)
Assuming they don't MITM SSH, you should still be able to use something like wireguard over an SSH tunnel.  At least I would think.. it's all SSH traffic as far as any DPI listener is concerned, you'd of course need to ensure the connection signature through another vector though.
> it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore.Really? Because the paper you linked says they don't block any TLS connections so you can just run a VPN over TLS:> TLS connections start with a TLS Client Hello message, and the first three bytes of this message cause the GFW to exempt the connection from blocking.
> it's basically not possible to run any kind of output VPN connection (even to private servers) from inside of China anymore.What if you run your own HTTPS server that look semi-legitimate and just encapsulate it in that traffic?Can they still detect it?What about a VPS in HK? Is this even doable?
Which is ridiculous because OpenVPN is trivial to identify, even when over TCP since it's different from "regular" HTTPS/SSL traffic.Why they chose this I have no idea.You can even port share.443 -> Web server for HTTPS traffic
443 -> OpenVPN for OpenVPN trafficStill trivial to identify and not uncommon for even public WiFi to do so.Since I changed to tailscale+headscale with my own derp server all these issues have disappeared (for now).
You've come to a wrong place to ask. Most people here (judging by recommendations of own VPN instances, Tor, Tailscale/other Wireguard-based VPNs, and Mullvad) don't have any experience with censorship circumvention.Just look for any VPNs that are advertised specifically for China, Russia, or Iran. These are the cutting edge tech, they may not be so privacy-friendly as Mullvad, but they will certainly work.
> Just look for any VPNs that are advertised specifically for China, Russia, or Iran.If I was working for a secret service for these countries, I would set up many "VPNs that are advertised specifically for x" as honeypots to gather data about any dissidents.
It doesn't matter, he should look into the open source protocols that these services use. He doesn't have to use them.VLESS / v2ray works in Russia, as far as I know.
Mr. Kafka, suspicion is healthy. However, abstraction provides no way forward when faced with practicalities instead of theory. Creates a Kafka-esque situation - anything suitable is by definition unsuitable. Better to focus on practical technical advice.
I don't see parent abstracting. They are simply pointing out a very real risk, which you don't provide any counter points to. Instead you seem to dismiss their point based on a strawman
IMO, the safest route for an individual with tech competency is to setup a small instance server in the cloud outside your country and use ssh port forwarding and a proxy to get at information you want.For an example of a proxy service https://www.digitalocean.com/community/tutorials/how-to-set-...That will give you a hard to snoop proxy service that should completely circumvent a government blockaid (they likely aren't going to be watching or blocking ssh traffic).
Hmm. People who recommend widely used approaches, and well-known, well-established providers, "don't have any experience with cenorship circumvention".So the solution is no-name providers using random ad-hoc hackery, chosen according to a criterion more or less custom designed to lead you into watering hole attacks.Right.
@reisse is 100% right. Most people outside of heavily censored regions have no clue what technology is actually used in those countries. The well-known, well-established providers don't actually work in censored regions because:1) The problem is very difficult and requires a lot of engineering resources
2) It's very hard to make money in these countries for many reasons, including sanctions or the government restricting payments (Alipay, WeChatPay, etc)The immediate response would be: "If the problem is so difficult, how can it be solved if not be well-known, well-established providers?"The answer is simple: the crowdsourcing power of open source combined with billions of people with a huge incentive to get around government blocking.
None of the things I listed are "widely used approaches, and well-known, well-established providers" in the parts of the world where it does matter.Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.
> Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.A lot of people use Telegram and think it's private, too.What about the part about choosing your VPN provider in the way most likely to get you an untrustworthy one who's after you personally?
VPNs that are advertised are for-profit products, which means:1. They are in most cases run by national spy agencies.2. They will at least appear to work, i.e., they will provide you with access to websites that are blocked by the country you are in.  Depending on which country's spies run the system, they may actually work in the sense of hiding your traffic from that country's spies, or they may mark you as a specific target and save all your traffic for later analysis.My inclination is to prefer free (open-source) software that isn't controlled by a company which can use that control against its users.
Well, you have to host your free open-source VPN software somewhere. And then, (N. B.: technical and usability stuff aside, I'm talking only about privacy bits here) everything boils down to two equally nightmarish options.First, you use well-known cloud or dedicated hoster. All your traffic is now tied to the single IP address of that hoster. It may be linked to you by visiting two different sites from the same IP address. Furthermore, this hoster is legally required to do anything with your VPN machine on demand of corresponding state actors (this is not a speculative scenario; i. e. Linode literally silently MitMed one of their customers on German request). Going ever further, residential and company IPs have quite different rules when it comes to law enforcement. Seeding Linux ISOs from your residential IP will be overlooked almost everywhere (sorry, Germany again), but seeding Linux ISOs from AWS can easily be a criminal offense.Second, you use some shady abuse-proof hosting company, which keeps no logs (or at least says that) and accepts payments in XMR. Now you're logging in to your bank account from an IP address that is used to seedbox pirate content or something even more illegal, and you still don't know if anyone meddles with your VPN instance looking for crypto wallet keys in your traffic.VPN services have a lot of "good" customers for a small amount of IP addresses, so even if they have some "bad" actors, their IPs as a whole remain "good enough". And, as the number of customers is big, each IP cannot be reliably tied to a specific customer without access logs.
Tor is a third option, at least as one layer, and seeding Linux ISOs is not, to my knowledge, a criminal offense in any jurisdiction, not even in China.  I don't know where you got that idea.
From gemini.. (edited for brevity)Kape Technologies Owns: ExpressVPN, CyberGhost, Private Internet Access, Zenmate> is there any suspicion that Kape Technologies is influenced or has ties to the Mossad?Yes, there is significant suspicion and public discussion about Kape Technologies having ties to former Israeli intelligence personnel. While a direct operational link to Mossad has not been proven, the concerns stem from the company's history, its key figures, and their backgrounds....Kape Technologies is owned by Israeli billionaire Teddy Sagi. While Sagi himself does not have a documented intelligence background, his business history, which includes a conviction for insider trading in the 1990s, has been a point of concern for some privacy advocates. The consolidation of several major VPN providers under his ownership has raised questions about the potential for centralized data access.----Sure there isn't direct proof but there wasn't any proof the CIA was driving drug trade while it was happening. Proof materializes when the dust settles on such matters.
It is absolutely self-evident that VPNs are considered high-value targets and that all spy agencies invest a chunk of resources to go after high-value targets.
I would invite you to read again the two claims made, and consider whether your statement actually addresses the veracity of either.To be a little trite: we all agree that chickens like grain, but it does not follow that a majority of grain producers are secretly controlled by a cabal of poultry.
Furthermore, you can always run another VPN on top of that if you don’t trust the outer one with the actual plaintext traffic.
Mullvad worked okay in China in June for me. I imagine it will be better in Indonesia with their less sophisticated blocking.
This makes no sense.On the one hand they do DPI with ML.On the other hand a major player is open!Something is not right here...
You can always do v2ray -> Mullvad in a docker container routed with gluetun for censorship avoidance and privacy
OP: look into VLESS (and similar).  And read up on ntc.party (through Google translate).  There are certain VPN providers that offer the protocol.
nah, vless is the protocol, reality is a newer obfuscation method that works over vlessedit: op, protonvpn has a free tier that works in russia, so likely works everywhere, or if you're comfortable with buying a vps, sshing into it and running some commands, look up x-ray, and use on of their gui panels
Wrong threat model. Solutions like mullvad/proton focus on privacy not breaking the blockade. They have well known entry points and therefore easily blocked. You can play cat and mouse game switching servers faster than censorship agency blocks them (e.g. Telegram vs Roskomnadzor circa 2018 [1]) but that gets expensive and not really focus of these companies.What you need is open protocols and hundreds of thousands of small servers only known to their owners and their family/friends1: https://archive.is/sxiha
I have a little, maybe enough to be dangerous. SSH won’t be sufficient to avoid all traffic analysis. Everyone can see how much traffic and the pattern of that traffic, which can leak info about the sort of things you’re doing.If you’re worried about ending up on a list, using things that look like VPNs while the VPNs are locked down is likely to do so.Also… your neighbors in Myanmar didn’t do a lockdown during the genocide and things got pretty fucking dire as a result. People have taken different lessons from this. I’m not sure what the right answer is, and which is the greater evil. Deplatforming and arresting people for inciting riots and hate speech is probably the best you can do to maintain life and liberty for the most people.
>Also… your neighbors in Myanmar didn’t do a lockdown during the genocide and things got pretty fucking dire as a resultThe genocide in Myanmar was incited _by_ the government there; giving it more power to censor it's citizens' communications would have done absolutely nothing to help the people being genocided. Genocides don't just suddenly happen; the vast majority of genocide over the past century (including Indonesian genocides against ethnically Chinese Indonesians) had the support of the state.
Mullvad worked OK in China for me recently. Sometimes I'd have to try a few different endpoints before it worked. Something built specifically to work in those places would probably be better, but it wasn't too much trouble. Not necessarily a recommendation, just sharing one data point.
I remember always needing obfuscation enabled in Mullvad, but it would work in the end (as you said, after trying a few endpoints).
^ this comment is right on. The cutting edge of VPN circumvention is the one marketed to people in China. Last I poked at this there were a lot of options.
Australia and UK might soon go down this path.Something quite depressing is if we (HN crowd) find workarounds, most regular folks won't have the budget/expertise to do so, so citizen journalism will have been successfully muted by government / big media.
I would have laughed in your face if you wrote this comment merely 6 months ago. Now I'm just depressed. (UK)
Don't worry. You'll call us conspiracy theories once you get used to the new goalposts and we warn you about the next thing.How about instead of being depressed you start being vocal and defiant?
When ES leaked his info to the Guardian people, they could still (2013) use the Guardian's US base to publish, protected by the US' stronger freedom of speech laws. Now, in 2025, if the same were to happen again, I'm not sure that would work quite the same way, with Trump aggressively taking American citizens' rights away.Maybe The Guardian should open a branch in Sealand...
It was David Graeber that said we should be wary of places like The Guardian. They are a wolf in sheeps clothing. Used a lot of the more liberal momentum of the early 2010s combined with promoting some of the more left leaning writters to gain a fair bit of clout. But underneath, they will conform to the power structures if it comes down to survival. Alas, they nay not be a Sealand edition although that would be neat.
In oz personally and yes, I warned folks of this a few years back, especially in the 12 months or so. Every time I was met with a fair bit of push back.They would argue back on technical merits, I was talking political, a politics doesn't give a damn about the tech. We have slowly been going down this path for a while now.“The laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia,” - PM Malcolm Turnbull in 2017.
Don't worry, you shouldn't underestimate the capability of society.I grew up in a pretty deprived area of the UK, and we all knew "a guy" who could get you access to free cable, or shim your electric line to bypass the meter, or get you pirated CD's and VHS' and whatever.There will always be "that guy down the pub" selling raspberry pi's with some deranged outdated firmware that runs a proxy for everything in the house or whatever. To be honest with you, I might end up being that guy for a bunch of people once I'm laid off from tech like the rest. :)
Normally I would agree with you, but the ability to pull this kind of thing off hinges on there being enough shadows that the Eye doesn't look at for prolonged periods of time. And the overall trajectory of technological advance lately is such that those shadows are rapidly shrinking. First it was the street cameras (and UK is already one of the most enthusiastic adopters in the world). And now comes AI which can automatically sift through all the mined data, performing sentiment analysis etc. I feel that the time will come pretty soon when "a guy" will need to be so adept at concealing the tracks in order to avoid detection that most people wouldn't have access to one.
I wouldn’t worry about it.They can barely handle wolf-whistlers let alone pedophile rape gangs consisting of the lowest IQ dregs of our society.I know it’s only painfully stupid people who think the law is stupid, but dodgy Dave down the way tends to fly under the radar. Otherwise there wouldn’t be so many of them.
The eye doesn't care as long as you're not politically efficient in opposing their narratives or power.Authoritarianism in the UK doesn't correlate with crime.  The economy does.The point of these things is not really to help citizens. "there's no money for that" like there's no money for healthcare or education (although there is for bombings in foreign countries). The point is protecting power from any threat that could mount against it.
I think both sides of this are fair. Power is interested in stability of itself, to keeps its back to the wall so that nobody can sneak up on it. But also political power has teamed up with corporate power/determination to create a far more nasty beast.Seeing companies like Palantir (and many lesser known ones) buddy up to everyone that wants it, its a clear statement on how they want to monitor and control the populace.Long term I don't think it can be done, but the pain mid term can be vast.
I suppose that for this case, an underground black market of VPN providers might emerge - average individuals setting up VPN software on a cloud service provider, and then selling monthly access to people. Aside from the obvious danger of getting ripped off (someone might put you on a slow shared VPN with many other people, or shut down the server at any time), there is also the possibility of someone monitoring all your Internet activity.
I'd default assume black market VPNs will monitor internet activity since it's both easy and profitable
That absolutely sounds like a world I should be worried about, where our only choices are dodgy ones
Don't worry, you shouldn't underestimate the capability of society.You should be worried. Don't underestimate the capabilities of the government bureaucrats. That "guys down the pub" will quickly disappear once they start getting jail time for their activities.
I think you really overestimate the capability of the UK to enforce laws. Yes, they can write them and yes they can fine large corporations, that's basically it.They cannot enforce laws against such "petty" crimes, the reason society mostly functions in the UK is because most people don't try to break the law.Pretty sure the local punters would kick the cops out if they came for one of their own, especially if he got them their porn back.
It's not just about UK abilities to enforce laws, but also about other factors. The described activities are extremely unattractive as criminal: small market, small margin, the need for planning, preparation and qualification.There is no need for special efforts to enforce the law. Put a few people in jail - and everyone else will quickly find safer and more legal ways to spend their time. No one will do something like that unless they are confident of their impunity.
Yes, it's also dystopian to pin one's future on such hopes. People need to stick it to the government and demand their freedoms. Far too many things are being forced on us in the West that go against fundamental values that have been established for centuries.Somehow, things that could be unifying protests where the working class of every political stripe are able to overlook their differences and push back against government never seem to happen. It is always polarized so that it's only ever one side at a time, and the other side is against them. How does that work?
Reflex. People's opinion on a subject changes if you tell them which political group supports it, sometimes even if they get asked twice in a row. Tribal identity determines ideology more than the other way around for a lot of people.So as soon as Labour comes out for something, Cons are inclined to be against it and so on. The only way to have neutral protests is if no one visibly backs them and they don't become associated with a side, but then how do they get support and organization?
> People need to stick it to the government and demand their freedoms.It will only work if they admit that they supported this and all forms of totalitarianism during COVID.  You can't fall for that and then be surprised when the world keeps going down that obvious path.
In matters of public health, you cannot trust the public to do the right thing.The problem with covid is that we weren't totalitarian enough. Regulations you could drive a coach & horses through and no way to enforce is a sop.The first lock down needed to be a proper 'papers, please' affair. When we get a properly lethal pandemic, we're fucked. Hopefully Laurence Fox and Piers Corbyn will catch it quickly and expire in a painful and televised way, it's the only hope of people complying with actual quarantine measures.
90% of “citizen journalism” is nothing of the sort. Just like “citizen science” researching vaccines.
> 90% of “citizen journalism” is (trash)You're right. But compared to what?I guess 99% of mainstream "journalism" is irrelevant and/or inaccurate, hence citizen journalism is a 10x improvement in accuracy and relevancy! Not 10% better, 900% better! This makes a huge difference to our society as a whole and in our daily lives!But this misses the most important point which is that the user should have the right to choose for themselves what they say and read. Making citizen journalism unduly burdensome deprives everyone of that choice.
Preach comrade!Those citizen journalists with their primary sources, disgusting.Thats nothing but propaganda.Remember it doesnt matter what the video shows, it only matters who showed it to you.
> Remember it doesnt matter what the video shows, it only matters who showed it to you.Both matter.
>Remember it doesnt matter what the video shows, it only matters who showed it to youIn an age of mass media (where there's a video for anything) or now one step further synthetic media knowing who makes something is much more important than the content, given that what's being shown can be created on demand. Propaganda in the modern world is taking something that actually happened, and then framing it as an authentic piece of information found "on the street", twisting its context."what's in the video" is now largely pointless, and anyone who isn't gullible will obviously always focus on where the promoter of any material wants to direct the audiences attention to, or what they want to deflect from.
Citizen journalism avoids the main weakness of a centralised system: it's incredible suspectible to capture. A prime example of this is the mass opposition around the world to Israel's genocide in Gaza.  Israel committed such genocides prior to the event of social media, such as the Nakba, but it was rarely reported on, due to media ownership being concentrated in the hands of a few pro-Zionist individuals.
I am just waiting for red states in the US to try this too since their current laws requiring ID verification for porn sites aren’t effective.
> red statesWell you'd be surprised to find out that this stupid policy (and many more) have been brought forward by Labour (Left).
At this point, anyone who has been watching politics for a few decades understands that the left/right dichotomy is primarily one designed to keep the majority of people within a certain set of bounds. We see it revealed when politicians and ideologies that should be in opposition to one another still cooperate on the same strategies, like this one.The goal right now is to make online anonymity impossible. Adult content is the wedge issue being used to make defending it unpalatable for any elected official, but nobody actually has it as a goal to prevent teenagers from looking at porn - if they did, they would be using more direct and efficient strategies.  No, it's very clear that anonymous online commentary is hurting politicians and they are striking back against it.
It has been my impression that in UK, both parties are strongly authoritarian, with the sole difference being what kinds of speech and expression, precisely, they want to police.
Yep, here in Australia the social media age restriction was pushed through by both sides. Two sides of the same coin.
Both the major Australian parties (Liberal and Labor) seem as spineless as each other.They're being pushed by media conglomerates News Corp and Nine Entertainment [0] to crush competition (social media apps). With the soon-to-be-introduced 'internet licence' (euphemism: 'age verification'), and it's working. If they ban VPN's, it will make social media apps even more burdensome to access and use.[0] News Corp and Nine Entertainment together own 90% of Australian print media, and are hugely influential in radio, digital and paid and free-to-air TV. They have a lot to gain by removing access to social media apps, where many (especially young) people get their information now days.
How long until they produce an  generative AI version of Burt Newton to do new episodes of 20 to 1 based on some social media slop?Yep, not a great time line here.
- Tor. Pros: Reasonably user friendly and easy to get online, strong anonymity, free. Cons: a common target for censorship, not very fast, exit nodes are basically universally distrusted by websites.- Tailscale with Mullvad exit nodes. Pros: little setup but not more than installing and configuring a program, faster than Got, very versatile. Cons: deep packet inspection can probably identify your traffic is using Mullvad, costs some money.- Your own VPSs with Wireguard/Tailscale. Pros: max control, you control how fast you want it, you can share with people you care about (and are willing to support). Cons: the admin effort isn't huge but requires some skill, cost is flexible but probably 20-30$ per month minimum in hosting.
> - Tailscale with Mullvad exit nodesTailscale is completely unnecessary here, unless OP can't connect to Mullvad.net in the first place to sign up. But if the Indonesian government blocks Mullvad nodes, they'll be out of luck either way.> - Your own VPSs with Wireguard/TailscaleKeep in mind that from the POV of any websites you visit, you will be easily identifiable due to your static IP.My suggestion would be to rent a VPS outside Indonesia, set up Mullvad or Tor on the VPS and route all traffic through that VPS (and thereby through Mullvad/Tor). The fastest way to set up the latter across devices is probably to use the VPS as Tailscale exit node.
Tailscale + Mullvad does have a privacy advantage over either one by itself: the party that could potentially spy on the VPN traffic (Mullvad) doesn’t know whose traffic it is beyond that it’s a Tailscale customer. Any government who wanted to trace specific traffic back to OP would need to get the cooperation of both Mullvad and Tailscale, which is a lot less likely than even the quite unlikely event of getting Mullvad to cooperate.
I mean multiple VPSs for redundancy. Contabo is maybe the cheapest I've seen and it's like 3$ mtl for the smallest?
And using another VPN like NordVPN or ProtonVPN is probably in the same category as Mullvad, but worth being cautious. If it's free, you are the product. If you pay, you're still sending your traffic to a publicly (usually) known server of a VPN. That metadata alone in some jurisdictions can still put you in danger.Stay safe
This is good overview, I just wanted to add that a VPS IP is not a residential IP. You will encounter roadblocks when you try to access services if you appear to be coming from a VPS. Not that I had a better solution, just to clarify what you can expect.
Tor also has anti-censorship mechanisms (snowflakes, ...). Depending on how aggressive the blocking is, Tor might be the most effective solution.
Wireguard is not censorship-resistant, and most VPN-averse countries block cross-border Wireguard. Why reply a practical question in an area in which you have no experience?
Yes. Fixed packet headers, predictable packet sizes. I don't know what "a common port" means in relation to wg.
Yeah. Tailscale uses 41641, and you can generally use whatever. I don't think there's any consensus, or majority.
Because Indonesia is new to the game and might still be catching up. They’re probably playing whackamole with the most common public VPN providers and might not be doing deep packet inspection yet. I worked with someone getting traffic out of Hong Kong a year ago and there was a lot trial and error figuring out what was blocked and what was not. Wireguard was one that worked.
They recommend Tailscale in particular. Tailscale control plane and DERPs (which are functionally required on mobile) will be among the first to go.Outline (shadowsocks-based) and amnezia (obfuscated wg and xray) both offer few-click install on your own VPS, which is easier than setting up headscale or static wg infrastructure, and will last you longer.Also, you did not answer my "why" question. I'm not sure what question you were answering.
IMO most people should have a VPS even if you don't need it for tunneling. Living without having a place to just leave services/files is very hard and often "free" services will hold your data hostage to manipulate your behavior which is annoying on a good day.
Yeah they can be cheap, but I would definitely recommend having at least 3 for redundancy. If one get shut down or it's IP blacklisted you still hopefully have a backup line to create a replacement.
No, unless you pay month to month. If you wait till BF you can find some really good deals on sites like lowendspirit
> cost is flexible but probably 20-30$ per month minimum in hosting.$4/month VPS from DigitalOcean is more than enough to handle a few users as per my experience. I have a Wireguard setup like this for more than a year. Didn't notice any issues.
I'm currently traveling in Uzbekistan and am surprised that wireguard as a protocol is just blocked. I use wireguard with my own server, because usually governments just block well known VPN providers and a small individual server is fine.It's the first time I've encountered where the entire protocol is just blocked. Worth checking what is blocked and how before deciding which VPN provider to use.
I should have mentioned that our use case isn't avoiding government firewalls, it's transiting through broken network environments.
WireGuard by itself has a pretty noticeable network pattern and I don't think they make obfuscating it a goal.There are some solutions that mimic the traffic and, say, route it through 443/TCP.
Wow, kinda crazy to think about a government blocking a protocol that just simply lets two computers talk securely over a tunnel.
Well, think about it - almost every other interaction you can have with an individual in another country is mediated by government. Physical interaction? You need to get through a border and customs. Phone call? Going through their exchanges, could be blocked, easy to spy on with wiretaps. Letter mail? Many cases historically of all letters being opened before being forwarded along.We lived through the golden age of the Internet where anyone was allowed to open a raw socket connection to anyone else, anywhere. That age is fading, now, and time may come where even sending an email to someone in Russia or China will be fraught with difficulty. Certainly encryption will be blocked.We're going to need steganographic tech that uses AI-hallucinated content as a carrier, or something.
> surprised that wireguard as a protocol is just blocked.Honestly this is the route I'm sure the UK will decide upon in the not too distant future.The job of us hackers is going to become even more important...
Cloak + wireguard should work fine on the server side. The problem is that I didn't find any clients for Android and I doubt there are clients for iOs that can (a) open a cloak tunnel and then (b) allow wireguard to connect to localhost...
A year ago I was traveling through Uzbekistan while also partly working remotely. IKEv2 VPN was blocked but thankfully I was able to switch to SSL VPN which worked fine. I didn't expect that, everything else (people, culture) in the country seemed quite open.
XRay protocol based VPN worked for me in Uzbekistan when I were travelling there.Wireguard is indeed blocked.
how can they detect it is wireguard, I thought the traffic is encrypted?how does it differ from regular TLS 1.3 traffic?
It's UDP, not TCP (like TLS) and has a distinguishable handshake. Wireguard is not designed as a censorship prevention tool, it's purely a networking solution.The tunnel itself is encrypted, but the tunnel creation and existence is not obfuscated.
There are many instances of Mastodon, and due to its federated nature, you can use any of them to access it, and even host your own.
Sure, but if you have an account on a different server, you can still see things posted on mastodon.social if you have followed someone there.
It would be easy to block on protocol level. Countries that block VPNs usually progress to that level pretty fast once they discover that simple IP blocks don't work.
I doubt that is the case once you do statistical analysis of it.Advanced VPN tunneling protocols, for example, have to take a lot of special measures to conceal their nature from China's and Russia's deep packet inspecting firewalls.
The most effective solution is to use X-ray/V2ray with VLESS, or VMESS, or Trojan as a protocol.Another obfuscated solution is AmneziaIf you are not ready to set up your own VPN server and need any kind of connection right now, try Psiphon, but it's a proprietary centralized service and it's not the best solution.
Very possible, though many of our users are saying that in network environments where WireGuard is blocked they were able to use Obscura.
Hey, I went to take a look at Obscura and I like the ideas but I can't find the source code.You are making some bold claims but without the source I can't verify those claims.Any plans to open-source it?
Tunneling via SSH (ssh -D) is super easy to detect. The government doesn't need any sophisticated analysis to tell SSH connections for tunneling from SSH connections where a human is typing into a terminal.Countries like China have blocked SSH-based tunneling for years.It can also block sessions based on packet sizes: a typical web browsing session involves a short HTTP request and a long HTTP response, during which the receiving end sends TCP ACKs; but if the traffic traffic mimics the above except these "ACKs" are a few dozen bytes larger than a real ACK, it knows you are tunneling over a different protocol. This is how it detects the vast majority of VPNs.
One alternative would be to set up a VPS, run VNC on it, run your browser on that to access the various web sites, and connect over an SSH tunnel to the VNC instance. Then it actually is an interactive ssh session.
Anything more then small text bandwidth use is also detected . Not about interactivity instead this case
15 years ago, I was using EC2 at work, and realized it was surprisingly easy to SSH into it in a way where all my traffic went through EC2. I could watch local Netflix when traveling. It was a de facto VPN.Details are not at the top of my mind these years later, but you can probably rig something up yourself that looks like regular web dev shit and not a known commercial VPN. I think there was a preference in Firefox or something.
The issue these days is that all of the EC2 IP ranges are well known, and are usually not very high-reputation IPs, so a lot of services will block them, or at least aggressively require CAPTCHAs to prevent botting.Source: used to work for a shady SEO company that searched Google 6,000,000 times a day on a huge farm of IPs from every provider we could find
I watched a season of Doctor Who that way back when the BBC were being precious about it. But Digital Ocean, so $5.
Nations severing peoples connections to the world is awful. I'm so sorry for the chaos in general, and the state doing awful things both.Go on https://lowendbox.com and get a cheap cheap cheap VPS. Use ssh SOCKS proxy in your browser to send web traffic through it.Very unfancy, a 30+ year old solution, but uses such primitive internet basics that it will almost certainly never fail. Builtin to everything but Windows (which afaik doesn't have an ssh client built-in).Tailscale is also super fantastic.
>  uses such primitive internet basics that it will almost certainly never fail.It already fails in China and Russia. Simply tunneling HTTP through SSH is too easy to detect with DPI.> Windows (which afaik doesn't have an ssh client built-in)It has had both SSH client and SSH server built-in since Win10.
WireGuard should still work. Tons of different providers. I trust Mullvad but ProtonVPN has a free tier. If they start blocking WireGuard, check out v2ray and xray-core. If those get blocked... that means somehow they're restricting all HTTPS traffic going out of the country
Chinese have developed a significant amount of sophisticated tools countering internet censorship. V2ray as far as I recall is the state-of-the-art.To use them, one need to first rent a (virtual) server somewhere from a foreign cloud provider as long as the payment does not pose a problem. The first step sometimes proves difficult for people in China, but hopefully Indonesia is not at that stage yet. What follows is relatively easy as there are many tutorials for the deployment like: https://guide.v2fly.org/en_US/
In this scenario, Chinese have very rich experience.
you need to use the advance proxy tool like clash ,v2ray, shadowsocks etc.
shadowsocks was the winner of the state of the art I had to do at work. It address the "long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a stat)" comment.
In case known VPN providers are blocked you can pick a small VPS from a hoster like Hetzner and setup your own VPN.
I'd recommend using Outline - it's a one click setup that lets you provision your own VPN on a cloud provider (or your own hardware).Since you get to pick where the hardware is located and it is just you (or you and a small group of friends & family) using the VPN, blocking is more difficult.If you don't want the hassle of using your own hardware you can rent a Digital Ocean droplet for <$5 per month.https://getoutline.org/
I’ve set this up for friends in fairly heavily censored countries before, it has been working well so far, but as others have said, this is a cat and mouse game
What is going on if you don’t mind my asking? Our local news does not mention anything. Nor does ddging help? Any sources?
> the housing allowance for a month for a parliamentarian is now ten times the minimum wage for a month.I'm almost positive that everyone in the US Congress is making at least ten times the minimum wage in this country. The "housing allowance" being referred to is separate from their normal salary in Indonesia, but still, interesting to imagine how much more seriously people there would take that disparity than in many other countries.This caught my attention more:> Indonesia passed a law in March allowing for the military to assume more civilian posts, while this month the government announced 100 new military battalions that will be trained in agriculture and animal husbandry. In July the government said the military would also start manufacturing pharmaceuticals.They're replacing civilian industry with military, apparently not out of any emergency requirement but just to benefit the military with jobs (and the government with control over those sectors) at the expense of civilian jobs.
As someone based in China, it's a bit surprising that techniques used by Chinese people get very few mentions here, while I do think they are quite effective against access blocking, especially after coevolving with GFW for the past decade. While I do hope blocking in Indonesia won't get to GFW level, I will leave this here in case it helps.I found this article [0] summarizing the history of censorship and anti-censorship measures in China, and I think it might be of help to you if the national censorship ever gets worse. As is shown in the article, access blocking in China can be categorized into several kinds: (sorted by severity)1. DNS poisoning by intercepting DNS traffic. This can be easily mitigated by using a DOT/DOH DNS resolver.2. Keyword-based HTTP traffic resetting. You are safe as long as you use HTTPS.3. IP blocking/unencrypted SNI header checking. This will require the use of a VPN/proxy.4. VPN blocking by recognizing traffic signatures. (VPNs with identifiable signatures include OpenVPN and WireGuard (and Tor and SSH forwards if you count those as VPNs), or basically any VPN that was designed without obfuscation in mind.) This really levels up the blocking: if the government don't block VPN access, then maybe any VPN provider will do; but if they do, you will have a harder time finding providers and configuring things.5. Many other ways to detect and block obfuscated proxy traffic. It is the worse (that I'm aware of), but it will also cost the government a lot to pull off, so you probably don't need to worry about this. But if you do, maybe check out V2Ray, XRay, Trojan, Hysteria, NaiveProxy and many other obfuscated proxies.But anyways, bypassing techniques always coevolve with the blocking measures. And many suggestions here by non-Indonesian (including mine!) might not be of help. My personal suggestion is to find a local tech community and see what techniques they are using, which could suit you better.[0] https://danglingpointer.fun/posts/GFWHistory
Thanks for the link!Is there any good DoT/DoH DNS resolver that works well in China? I know I can build one myself, but forwarding all DNS requests to my home server in NA slows down all connections...
Personally, I like Amnezia VPN, it has some ways to work around blocks: https://amnezia.org/en
You can very easily self-host it, their installer automatically works on major cloud platforms.Though if Indonesia has blocked VPNs only now, possibly they only block major providers and don't try to detect the VPN protocol itself, which would make self-hosting any VPN possible.
there is a major protest currently happening due to the legislative body representative just giving themselves a monthly domicile stipend of ~$3300 on top of their salaries (yes, multiple), while the average people earned ~$330 monthly. the information about the protest are not broadcasted on local TVs, so the only spread of information is through social media. i guess since a lot of people went around it using VPN, the gov decided to block it too.
“Some demonstrators on Monday were seen on television footage carrying a flag from the Japanese manga series One Piece, which has become a symbol of protest against government policies in the country.”
The official word is to counter gambling. Lately the government is not really popular after some decisions that could be interpreted as authoritative, and as citizens have spoken out about it online, causing more voices to join and protests erupting..So well, my guess is they're trying to control it.
AmneziaWG is a decent option for censorship resistance, and it can be installed as a container on your own server.
Censorship circumvention tools specialize in this, and are extensively used in China, Iran, and Russia. I work on Lantern, and we're not seeing any significant interruptions to connections in Indonesia at the moment.
https://lantern.io/downloadHope it helps!
What I'm worried most are that most people are not even aware of what is DNS and how to change it.I can't imagine those who are caught in the chaos with only their phone and unable to access information that could help them to be safe.
Generally speaking, the general population that wants to use blocked services will develop enough technical know-how to circumvent it. The biggest risk is that there are bad actors giving malicious advice and to such learners, looking to defraud or otherwise exploit them.
I like mullvad. You can buy a prepaid card off amazon. I figured out how to setup wireguard on various unixes Mac/linux/openbsd
Try looking into tor bridges.You could also buy a VPS and use SSH tunneling to access a tor daemon running on a VPS. Host some sort of web service on the VPS so it looks inconspicuous
Your first option until you get settled is to use an SSH reverse proxy:    ssh -D 9999 user@my.server

Then configure your browser to use local port 9999 for your SOCKS5 proxy.This gets you a temporarily usable system and if you can tunnel this way successfully installing some WireGuard or OpenVPN stuff will likely work.EDIT: Thanks it's -D not -R
Usually when countries block websites they don't block major cloud providers, like AWS and Google Cloud. Because most websites are hosted on them. So you can get a cheap VPS from AWS or GCP (always free VM is available) and host OpenVPN on it.
It is not a real URI... lolThe point was to include something clowns can't filter without incurring collateral costs, and wrapping the ssh protocol in standard web traffic. =3
Use the Tor browser window in Brave. It's nowhere near as anonymous as the Tor browser, but the built in ad blocking makes browsing via Tor usable. And that's what you and your compatriots are interested in.Prepare to fill in Cloudflare captchas all day, but that's what it takes to have a bit of privacy nowadays.
I live in Pakistan and two years back we had this exact same problem, (election interference) and frankly, you just try to scrape through solutions, but without an answerable government, there is little you can do.We tried things like Proton VPN and Windscribe VPN, as well as enabling MT proxy on Telegram, but soon govts find it easier to just mass ban internet access.Use Netblocks.org to analyse the level of internet blockage and try to react accordingly.
I was wondering something like this but in a different capacity.What with certain countries (they know who they are) and their hatred for encryption, it got me wondering how people would communicate securely if - for example - Signal/WhatsApp/etc. pulled out and the country wound up disconnecting the submarine cables to "keep $MORAL_PANIC_OF_THE_DAY safe."How would people communicate securely and privately in a domestic situation like that?
In person or not at all.At that point you've essentially lost.You either hope another country sees value in spreading you some democracy, or you rise up and hope others join you.Or not and you accept the protection the state is graciously providing to you.
Try a ssh socks5 proxy to a cheap vps.It worked well for me in UAE when other solutions didn’t
In this case the blockage will probably just be up for a few days, until the protests calmed down.Other than that: tor
Set up a VM on AWS/azure/gcp/... in the desired cell, install a VPN server and done. Once you have automation in place it takes ~2 minutes to start, you can run it on demand so you can pay per minute.
Aren't there local (online or print) newspapers to get news from, as an alternative to Discord? Hope I'm not asking a dumb question
In countries where it comes to government blocking/censoring internet traffic, traditional media is cleared of all dissent and fully controlled long before. Last stages of that are happening in my country, Serbia, currently.
Right, that makes sense. Did some looking up and nonfree press seems to be indeed the case for Indonesia: https://rsf.org/en/country/indonesiaIt's a mixed bag apparently, free press is technically legal since 1998 but selective prosecution and harassment of those actually uncovering issues (mainly becomes clear in the last section, "Safety")Tried looking up Serbia next on that website but got a cloudflare block. I'm a robot now...
It's not a dumb question at all. Level on hn really got down lately if you're getting downvoted.Think about it Aachen. If the government has enough power to censor internet traffic, that what was the first thing it censored? Which media is traditionally known for being censored or just speaking propaganda? That's the classical newspapers. It's not uncommon in authoritarian countries for editors to need state to sign off on the day's paper. And if not that, articles are signed and publishers are known. They will auto-censor to avoid problems. Just like creators on YouTube don't comment on this one country's treatment of civilians to avoid problems.
All the various proxy solutions offered are good (although the simplest ones - like squid - haven't been mentioned yet). You can also use a remote desktop or even just ssh -Y me@remote-server "firefox"
Launch an EC2 instance in the US region (Ubuntu, open ports 22 and 1194), then connect via SSH and run the OpenVPN install script. Generate the .ovpn profile with the script and download it to your local machine. Finally, import the file into the OpenVPN client and connect to route traffic through the US server.
I'd recommend Obscura because it uses Wireguard over QUIC and it pretty good at avoiding these blocks. It's also open source.
Android doesn't come with system wide socks proxy support, and i couldn't find an open source app for it either. Is anyone aware of one?Nonetheless this is a surprisingly simple and bullet proof solution: SSH, that's not vpn boss, i need it for work.
Outline is an open source shadowsocks client, and you provision your own server to act as the proxy.  You can use it against any Shadowsocks server you want, and the protocol makes it look like regular https traffic.https://github.com/Jigsaw-Code/outline-appsAndroid & iOS & Linux & Mac & Windowstheir server installer will help set up a proxy for users that aren't familiar with shadowsocks, too
Depending on the circumstances, maybe ditch the landline local ISP for a satellite connection with a foreign ISP?
Make your own VPN using a VPS and something like openvpn.Not every website will allow it, but it should get you access to more than you have now.
A proxy service like shadow socks works. There are thousands of providers for $X/month for a decent amount of traffic
Tor should be pretty good even for environments where they crack down on VPNs, although it can be a bit slow, at least it works.
Yeah, sucks, but really should find better places for people to gather regardless, if you're in that sort of environment.
How is this practical advice in a thread where someone mentions that the clampdown happened without notice?The "shoulda done..." advice isn't useful in the slightest, and I'd argue is malicious with how often it's done simply to satiate a poster's ego.
OP, you can rent a VPS from a reputable and cheap provider within the NA region - OVH, Vultr, Linode etc. are decent. Also check out lowendtalk.comThen, setup Tailscale on the server. You can VPN into it and essentially browse the internet as someone from NA.
From some of the comments here I get why you are downvoted. But tbh I would also have gone that route. So are we just inexperienced? I read here indeed that wireguard is very easily blocked. It was at the company I worked for but then I just set port 23 (who uses ftp anyways??). And it worked. But why is this still bad then?Obviously I have 0 real experience with this.
Well, I mean, Tailscale is pretty easy overall. When client apps get blocked, you can literally hook up your router into Tailscale if needed, or you can run a headless version of Tailscale on your home server or the very machine you are on.It should also be possible to use a tunnel to get around the blocking of WireGuard, for example.You can then use it as an exit node if needed. It should work in theory, I have never tried this though. I just speak as a very frequent user of Tailscale with a bunch of nodes that are geographically located in different cities around me.
Sure, I know and use it too. But I saw you being downvoted so I responded to that. I think, reading the rest of the thread, your response (as mine would be) does not work as signals 0 experience with actually oppressing regimes. Not?
Full disclosure, I run a commercial VPN service (Windscribe).There are 2 paths you can take here:1. Roll your own VPN server on a VPS at a less common cloud provider and use it. If you're tech savvy and know what you're doing, you can get this going in <1hr. Be mindful of the downsides of being the sole user of your custom VPN server you pay for: cloud providers log all TCP flows and traffic correlation is trivial. You do something "bad", your gov subpoenas the provider who hands over your personal info. If you used fake info, your TCP flows are still there, which means your ISP's IP is logged, and deanonymizing you after that is a piece of cake (no court order needed in many countries).2. Get a paid commercial VPN service that values your privacy, has a diverse network of endpoints and protocols. Do not use any random free VPN apps from the Play/App stores, as they're either Chinese honeypots (https://www.bitdefender.com/en-us/blog/hotforsecurity/china-...) or total scams (https://www.tomsguide.com/computing/vpns/this-shady-vpn-has-...).Do not go with a VPN service that is "mainstream" (advertised by a Youtuber) or one that has an affiliate program. Doing/having both of these things essentially requires a provider to resort so dishonest billing practices where your subscription renews at 2-5x of the original price. This is because VPNs that advertise or run affiliate programs don't make a profit on the initial purchase for that amazing deal thats 27 months with 4 months free or whatever the random numbers are, they pay all of this to an affiliate, sometimes more. Since commercial VPNs are not charities, they need ROI and that comes only when someone rebills. Since many people cancel their subscriptions immediately after purchase (to avoid the thing that follows) the rebill price is usually significantly more than the initial "amazing deal". This is why both Nord and Express have multiple class action lawsuits for dishonest billing practices - they have to do it, to get their bag (back). It's a race to the bottom of who can offer the most $ to affiliates, and shaft their customers as the inevitable result.Billing quirks aside, a VPN you choose should offer multiple VPN protocols, and obfuscation techniques. There is no 1 magic protocol that just works everywhere, as every country does censorship differently, using different tools.- Some do basic DNS filtering, in which case you don't need a VPN at all, just use an encrypted DNS protocol like DOH, from any provider (Cloudflare, Google, Control D[I also run this company], NextDNS, Adguard DNS)- Then there is SNI filtering, where changing your DNS provider won't have any effect and you will have to use a VPN or a secure proxy (HTTPS forward proxy, or something fancier like shadowsocks or v2ray).- Finally there is full protocol aware DPI that can be implemented with various degrees of aggressiveness that will perform all kinds of unholy traffic inspection on all TCP and UDP flows, for some or all IP subnets.For this last type, having a variety of protocols and endpoints you can connect to is what's gonna define your chance of success to bypass restrictions. Beyond variety of protocols, some VPN providers (like Windscribe, and Mullvad) will mess with packets in order to bypass DPI engines, which works with variable degree of success and is very region/ISP specific. You can learn about some of these concepts in this very handy project: https://github.com/ValdikSS/GoodbyeDPI (we borrow some concepts from here, and have a few of our own).Soooo... what are good VPNs that don't do shady stuff, keeps your privacy in mind, have a reasonably sized server footprint and have features that go beyond basic traffic proxying? There is IVPN, Mullvad, and maybe even Windscribe. All are audited, have open source clients and in case of Windscribe, also court proven to keep no logs (ask me about that 1 time I got criminally charged in Greece for actions of a Windscribe user).If you have any questions, I'd be happy to answer them.
I can relate to this because my country has an election soon and I'm sure we wont have internet for 3 - 5 days then.
You could rent a cheapo instance at a cloud provider and tunnel https over ssh.That’s basically undetectable. Long lived ssh connection? Totally normal. Lots of throughput? Also normal. Bursts throughput? Same.Not sure how to do this on mobile.Tailscale might be an option too (they have a free account for individuals and an exit node out of country nearly bypasses your problem) It uses wireguard which might not be blocked and which comes with some plausible deniability. It’s a secure network overlay not a VPN. It just connects my machines, honest officer.
Just please be safe and necessarily paranoidOne way they tend to "solve" workarounds is making examples of people
Use an Actual Private Network? Radio links that you control. Peer with someone who owns a Starlink terminal. Rent instances in GCP's Jakarta datacenter.
Blocking Twitter is a good start, now Facebook, Instagram, Whatsup and TikTok.This is a good start but more should be blocked. Then force ISP to block ads.Not just for Indonesia but all countries. But we still have a lot more to do to fix the web.
The issue with that is where do they draw the line. Next thing you know each country becomes North Korea.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Dedalus Labs (YC S25) – Vercel for Agents]]></title>
            <link>https://news.ycombinator.com/item?id=45054040</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054040</guid>
            <description><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.]]></description>
            <content:encoded><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.Here’s a demo: https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&t=11Last October, I was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and let’s just say it was super annoying to build… I was thinking to myself the entire time “Why can’t I just pass in `tools=code_execution` to the model and just have it…work?Even with MCP, you’re stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.The code you write looks something like this:  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=["author/server-1”, “author/server-2”],
    model=["openai/gpt-4.1”, “anthropic/claude-sonnet-4-20250514”],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too

Our docs start at https://docs.dedaluslabs.ai. Here’s a simple Hello World example: https://docs.dedaluslabs.ai/examples/01-hello-world. For basic tool execution, see https://docs.dedaluslabs.ai/examples/02-basic-tools. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: https://docs.dedaluslabs.ai/examples/use-case/weather-foreca....There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (we joke that the “S” in MCP stands for “security”). MCP servers right now are expected to act as both the authentication server and the resource server. That is too much to ask of server writers. People just want to expose a resource endpoint and be done.Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve them. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.We aren’t quite yet at the stateful code execution sandbox that I wanted last October, but we’re getting there! Shipping secure and stateful MCP servers is high on our priority list, and we’ll be launching our auth solution next month. We’re also working on an MCP marketplace, so people can monetize their tools, while we handle billing and rev-share.We’re big on open sourcing things and have these SDKs so far (MIT licensed):https://github.com/dedalus-labs/dedalus-sdk-pythonhttps://github.com/dedalus-labs/dedalus-sdk-typescripthttps://github.com/dedalus-labs/dedalus-sdk-gohttps://github.com/dedalus-labs/dedalus-openapiWe would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.Thanks HN!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimising for maintainability – Gleam in production at Strand]]></title>
            <link>https://gleam.run/case-studies/strand/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053462</guid>
            <description><![CDATA[A case study of Gleam in production at Strand]]></description>
            <content:encoded><![CDATA[Strand is a marketing agency based in London,
UK. The company specialises in copywriting and content creation for many of the
world’s largest enterprise technology companies, running marketing programmes
that produce hundreds of white papers, case studies, blog posts and articles
every year.
Challenge
For many years, Strand has relied on a custom-built project management system
to support the operational aspects of its business—creating projects, tracking
activities and managing documents. However, managing the financial aspects of
project management had always been a more manual process, using spreadsheets to
ensure that billable work was assigned to the correct purchase orders and
invoices.
“Just before the pandemic, we decided to build a new financial management
system,” recalls Ed Kelly, Director of Technology at Strand. “It turned out to
be a very timely decision. When we had to pivot to remote working, the fact
that everyone could track their billable work in a centralised system helped us
keep the business on track.”
The new system quickly became an integral part of Strand’s daily workflow, and
users began requesting new features. As the application gradually grew larger
and more complex, the company’s small development team wanted to ensure that
the system would remain reliable, maintainable and scalable.
“Almost by accident, what we launched as a prototype became a business-critical
application,” says Ed Kelly. “Our development resources are limited, so our top
priority was to make sure the system would just run forever without needing
constant maintenance. At the same time, we also wanted to keep the codebase
simple and approachable, so it’s easy for developers to dive back into when
they need to make a change. The challenge for us was to build and maintain this
business-critical system cost-effectively with our lean development team.”
Solution
As a small business, Strand is not afraid to innovate. “We do have systems that
are written in mainstream programming languages like Python and JavaScript, but
our strategy is to pick the best tool for the job, not just the most popular,”
explains Ed Kelly. “Gleam was a good fit for our requirements.”
The features of Gleam that appealed to Strand were its robustness and
maintainability, its combination of modern language features with access to a
broad ecosystem of battle-tested, production-grade libraries, and its strong
focus on developer experience.
Safety and reliability
“Gleam is a safe language,” explains Ed Kelly. “Broadly speaking, if you write
a program in pure Gleam, it’s guaranteed not to crash. And in cases where you
need to interface with code written in other, less-safe languages, there is a
second layer of protection provided by Gleam’s runtime platform, the BEAM.”
The BEAM was developed by Ericsson in the 1980s as a fault-tolerant platform
for managing large telephone switches that need to handle thousands of calls
simultaneously and can never be taken offline for maintenance. The central idea
is that the platform is able to divide applications into thousands or even
millions of lightweight processes. Each process runs independently, and
processes can communicate by sending messages to each other. If an individual
process crashes, it can be restarted automatically without affecting any of the
other processes.
“The application that we’ve built is composed of several services that interact
with the outside world,” explains Ed Kelly. “For example, we have a service
that periodically downloads currency exchange rates from the UK government’s
website, and another that syncs data with our project management system. The
BEAM ensures that if there’s some unforeseen problem with any of these external
services, it won’t crash our application.”
Modernity and pragmatism
Gleam is designed to be a simple language that provides powerful features while
remaining resolutely practical. “It gives us access to features from more
academic programming languages, but it makes them approachable,” says Ed Kelly.
“The language is small—an experienced developer can learn it in an
afternoon—and there is a strong focus on only having one way to do things. That
means you can onboard new developers into a Gleam codebase quickly.”
Because Gleam code runs on the BEAM, developers also have easy access to
thousands of high-quality software libraries. “The Gleam library ecosystem is
growing rapidly year-on-year,” says Ed Kelly. “And when we need to, we can also
reach for 40 years’ worth of battle-tested libraries written in other BEAM
languages such as Erlang and Elixir. The language prioritises pragmatism over
purity, which helps us get things done.”
Developer experience
In Strand’s experience, Gleam’s developer tools are second to none. “When you
download Gleam, you get all the tooling in a single package,” says Ed Kelly.
“It integrates with your code editor to provide features like formatting,
suggestions and autocomplete. The error messages are really friendly and
helpful—when you make a mistake, Gleam will often tell you what you should have
written. And it’s really fast—the days of going for a coffee break while you
wait for your code to build are over.”
He adds: “We’re heading into a new age of AI-assisted coding, and right now,
it’s difficult to predict how that will play out. But if I had to place a bet,
I would say that in the long run, AIs are more likely to generate high-quality
code in a language like Gleam. Gleam makes it quick and easy for AIs to check
their code, get instant feedback, and iterate. That should be an advantage
compared to languages that are slow to build, have cryptic error messages, and
can’t catch mistakes at build-time.”
Incremental adoption
For Strand, introducing Gleam into its codebase was a low-risk, incremental
process. “We started with just one service—our integration with the UK
government’s currency exchange rate API,” says Ed Kelly. “We were so pleased
with how it turned out that we then rewrote some of our other services in
Gleam. And recently, we’ve decided to give Gleam an even more important role by
replacing the whole part of the backend that talks to our database. We’re very
confident that this will give us a safer and more maintainable codebase
overall.”
Results
As one of the first companies in the world to run Gleam in production, Strand
took a risk. Two years later, the development team is delighted with the
decision. “Since we started, the language has really matured and reached a
stable state,” says Ed Kelly. “The community has grown massively and there’s a
real buzz around the language. It’s even starting to be recognised by
mainstream industry analysts like Thoughtworks in their Technology
Radar. I
think today, Gleam is a safe and solid choice for companies to use in
production.”
Since go-live, the Gleam code within Strand’s application has been rock-solid.
“We’ve had zero Gleam-related crashes, and even when there have been issues
with other parts of the system, the BEAM has kept everything running,” says Ed
Kelly. “We’ve been able to fix problems without our users even noticing that
anything was wrong.”
The simplicity of the language and the sophistication of the development tools
also help to keep the codebase maintainable. “Even when we haven’t looked at
the codebase for a few weeks, it’s easy to get back into it,” says Ed Kelly.
“The language and tooling gently push you to use a consistent, idiomatic style,
and to write clearly and simply without trying to be too clever. So, we don’t
have to spend time puzzling out what our past selves were trying to do with the
code that we wrote six months ago.”
He concludes: “Adopting a new language is always a gamble, but Gleam has paid
off. The belt-and-braces approach to safety and fault-tolerance has given us a
system that just works, reliably, day in and day out, without constant
babysitting and maintenance. For a team like ours, with many other priorities
and projects we need to work on, the confidence that Gleam gives us is worth
its weight in gold.”
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Will AI Replace Human Thinking? The Case for Writing and Coding Manually]]></title>
            <link>https://www.ssp.sh/brain/will-ai-replace-humans/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052784</guid>
            <description><![CDATA[Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. It’s OK to use it, and everyone does it, but still learn your craft, and try to think.]]></description>
            <content:encoded><![CDATA[
Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. It’s OK to use it, and everyone does it, but still learn your craft, and try to think.
Similar to what DHH said:

It’s also more fun to be competent in something than constantly waiting for an AI to complete.
The probability that AI will make us unhappy is very high IMO. Use it, yes, but not for every task. For discovering, creating a historical overview, or creating diagrams (Canva, Figma), but a big no to the writing (or coding). Someone needs to add knowledge or new insights; AI cannot train itself. So articles, books, and words will be written, and writers will be more in demand as everyone relies on AI, which at some point just plateaus.
It will be a long-term loss; people stop thinking and learning. Time will tell. My two cents, if you are a senior in something, you know better. 

Bsky
# Guidance on When to Use It
I 

heard from ThePrimeagen: It depends on how far you fix into the future. Short-term autocomplete is fine, but architectural decisions are big no, no’s.

This is about the bottom where we have time and the left where we have amount of errors. This means that the longer we use AI for fixing something in the future, like an architecture, the more errors it will produce.
If we use it for quick autocomplete or creating a well-defined algorithm function, it’s less prone to errors. In that first phase, you gain 20% productivity; in the later phases, you lose more.

This is like in real life, the longer I wait with making m decision, the more information I have, the better the decison will be. And is exactly what Shape Up preaches with maximum deciding for 6 weeks (a cycle),  don’t have roadmaps and backlogs for longer than that in the future. Similar is it with using AI, as all of it is predicted probability.
Another great illustration by 

Forrest Brazeal:

Also to keep in mind what’s most imporant to your usecase like illustrated by Thomas Ptacek in 

My AI Skeptic Friends Are All Nuts · The Fly Blog:

# Soulless
Nobody wants to read some soulless text, and what if it’s even good? Where do you get more from? I think this is a big trap that only over time people will realize. Sure, they help, and everyone needs to use them for “certain” tasks, but not the writing itself.
In the end, LLMs and AI require guidance; they’re just probabilities. See also Writing Manually.
# Distraction
I think we will be more distracted than ever. We can’t even have 2 seconds to think before Grammarly, Copilot, or Cursor suggests something. So instead of doing the thinking, we just cruise on. We are losing the driver’s seat.
It brings me back to the article I wrote recently about «

Finding Flow». More on Don’t use AI for everything, you stop thinking-learning AI Use and Writing is Hard.
# Don’t Get Me Wrong
Don’t get me wrong, I use it every day, too. But more deliberately. I turned off my Grammarly and my Copilot (a long time ago), so I have the space to think and to learn. If you do it once or twice, that’s OK, but if you do it everywhere, you also lose the ability to learn new skills or the fun of it.
Interesting about the LCI (LLM Collaborative Intelligence), and sure, there will be a lot of benefits, but I am not sure if these insights are anything that comes close to a human insight that has felt, sensed, or experienced something through hardship. So yes, but I do not have many expectations, nor do I want it to create new insights. This is the fun part of my job :)
# Exercising a Skill
It’s never always or never; it’s in between. The problem with learning is if you use it often, I’d argue that you, in fact, don’t learn much. You just copy and paste in writing or just tab tab tab in coding. The learning is gone. And do that often enough; our brain is not used to learning or, more critically, thinking anymore. Same as remembering, how good can we remember mobile phone numbers? not really, but I could very well during the early phone times because I trained it every day.
It’s all a matter of exercise, and I learned for myself—it doesn’t have to be true for everyone—that I didn’t learn or think anymore. And frankly, it was also not fun anymore. That’s to be said in the stuff I know well.
In other areas, like creating an image (like the one I created for this article 😆) or updating my website’s front page with HTML/CSS, which would have taken me much longer as I don’t practice, it helped a lot. But I’d argue the fact that I didn’t learn anything new except prompting Claude Code :). It’s all tradeoffs, as always, right? :)
# Other Opinions
# Paul Graham on Writing
Paul Graham says on 

Writes and Write-Nots (internal):

The result will be a world divided into writes and write-nots. There will still be some people who can write.
Yes, it’s bad. The reason is something I mentioned earlier: writing is thinking.
In fact there’s a kind of thinking that can only be done by writing.
If you’re thinking without writing, you only think you’re thinking.
So a world divided into writes and write-nots is more dangerous than it sounds. It will be a world of thinks and think-nots.

# Nathan Baugh
Nathan Baugh shares on 

About AI and ghostwriting:

1st Order Effect:

The world will be overrun with slop content and stories.
We already see this. Just look at AI written comments on this platform.

2nd Order Effect:

People will stop learning the foundational skills – storytelling, writing, rhetoric – required to communicate their experiences and ideas effectively.
They will over rely on AI. It starts as a tool, becomes a crutch, and ends as a hindrance.

3rd Order Effect:

People who invest in those same skills see massive returns.
Writing sharpens your ideas. Story gives leverage to those ideas.

# Ted Gioia
The good news, and why AI won’t replace writers on 2024-08-31 by Ted Gioia. Some of the reasons why he thinks AI Writing won’t be as good:



Source on Twitter/X. Full article 

Google Thinks Beethoven Looks Like Mr. Bean - by Ted Gioia.
# Mitchell Hashimoto

2.5 years into the AI craze, and I continue to firmly believe that if your company wasn’t already interesting/succeeding without AI, then doing “whatever plus AI” isn’t going to save you. For the few that seem this way (eg Cursor), I think their moat is a lot weaker than it seems. You have to play the game and the game is AI, but I don’t think it’s a defensible foundational capability. Might play out as an excellent land and grab strategy to buy you time to fill out the meat though. Mitchell Hashimoto on 

Twitter
# Andrew Ng
Andrew Ng on 

Twitter/X:

Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, “It is far more likely that the programming occupation will become extinct […] than that it will become all-powerful. More and more, computers will program themselves.”​ Statements discouraging people from learning to code are harmful!
In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today’s arguments not to learn to code continue to echo his comment.
As coding becomes easier, more people should code, not fewer!
Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step.
I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals — individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.
den>
One question I’m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is a great way to do that.
When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on — using the language of art — to get the result he wanted. I didn’t know this language, and my paltry attempts at prompting could not deliver as effective a result.
Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools are continuing to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.
# Harry Dry

Big ideas are less about creativity and more about conviction. [..] So, what happened? ‘Sauce and other shit’ got incredibly cheap! [..] There is no AI prompt for conviction. Harry Dry
^64403f
More on Is AI solving this?.
# Jason Fried
As Jason Fried said, initially, it’s magical. After a while, you see it so clearly and it’s just average:



Cover letters? Yes!

The hardest thing is not making something.
The hardest thing is maintaining something.
It’s become so easy to just make stuff and vomit out ideas, and I mean this in the best possible way… 

Jason Fried on LinkedIn
This another valid insights, it’s hard to maintain code that is not made by you, it’s losing it’s fun. Therefore this will be a big part of a winning business, to have sustainable, and energy to want to maintain a product. And not “just making it”.
Also who takes responsibility for the generated (vibed) code?
# David Perell
David Perell has similar views as me on being soulless:

When you outsource your writing to AI, you end up with words that lack soul or personality. Gone go your quirks and your idiosyncrasies, which are the very things that make your writing irreplaceable. 

LinkedIn
# Ezra Klein
Ezra Klein has 

great insights that I very align with in terms of writing. He says that there are no shortcuts for research. When you grapple with a text or book for seven hours, it will change you. This will influence your writing, too. There’s no summary that gives you this kind of in-depth connection.
Also, you can’t prompt your way into it, as there’s no prompt that knows that you don’t know yet, or AI doesn’t know what you wanted to have read or what connections you would have made. On the contrary, you actually lose time reading something, and over time, we think we read lots of stuff, but we actually only read summaries. Full episode on 

The Case Against Writing with AI.
# Will It Replace X
# Writers


Are Cover letters still a thing? Yes. This reminded me of good writing is key for every job these days.  Writing was always an asset, but even more these days; although people think they don’t need it, as AI is doing that. But that’s a very dangerous bet I wouldn’t take.
I wrote more on Writing Manually.
# Data Engineers?
Probably not.
Nice 

comparison by Mehdi Ouazza:


Did the music record replace musicians 100 years ago? Nope, it changed them and the industry.
Did cloud computing take all IT jobs? Nope, it also changed the industry and our jobs.
Same here; it will change our industry and job, but we won’t disappear.

More on Will AI replace Data Engineers.
# Image Generation
Initial generation, yes. But final touch, no. Whenever I try to create images with AI, I am always initially impressed, but that quickly fades over time.
Yesterday, I updated my second brain image, but I changed it again today. I created some more with AI; prompted prompted prompted. In the end, I made one manually based on my copy. I think it’s more powerful. What do you think?
# ChatGPT


# My Own


Some AI-generated images I like too, but they were always missing something, and yeah, they looked so AI-generated. I started to feel the same as I did for AI writing () and AI data engineering (Will AI replace Data Engineers); now, with AI image generation, doing it yourself is more fulfilling, and you end up happier.
More on AI Generated Images.
# How to detect AI Writing
How to detect AI Writing
If we know how AI is writing, should we stop using em dashes or thing AI does?
I don’t think so. I love the em dash. I even have a keyboard shortcut for the em dash. And sometimes when I write a negation, I’m thinking «could that look like it’s written by AI».
But at the end, convition is a good word. I can focus what an AI thinks while I write, I must write. So having something to say, and trying my best to communicate that, is the best I can do.  ^ebca60
# History Logs
# From 2024-10-12
What AI Writing can’t do, because it can only think one word at a time.
E.g., in the below example, as a writer you know you need to start all sentences the same, but the AI model can’t do that.

Writing from Abundance is the art of collecting ideas so you can think better and avoid writer’s block.
Writing from Conversation is the art of using dialogue to identify your best ideas and double down on them. 
Writing in Public is the art of broadcasting your ideas to the Internet so you become a beacon for people, opportunities, and serendipity.

More on Copywriting.
# AI Slop - Companies not doing great
AI Slop is generating more content, no matter the quality. It’s a the never ending Quality vs Quantity discussion, but now ever more important.
Here are some companies backpedaling after going full AI-first:

Klarna 

backpedaling AI customer service.:



“After years of depicting Klarna as an AI-first company, the fintech’s CEO reversed himself, telling Bloomberg the company was once again recruiting humans after the AI approach led to “lower quality.” An IBM survey reveals this is a common occurrence for AI use in business, where just 1 in 4 projects delivers the return it promised and even fewer are scaled up.”



Duolingo getting 

worse with AI
Next up, Shopify after the 

announcement to go full AI?

# Learning With AI
Learning with AI
# Future

Nice insights, why LLMs with token pretictors are not so good for understanding the worlds. It kinda works (but not so so good) for writing, but to understand physics, and world models, this is much harder he says: 

Metas AI Boss Says He DONE With LLMS…

# Further Reads



The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers
Smart Note Taking


My AI Skeptic Friends Are All Nuts · The Fly Blog
Companies that used AI to generate a quick solutions and now spending humans to fix it, expensively



Companies That Tried to Save Money With AI Are Now Spending a Fortune Hiring People to Fix Its Mistakes


Companies That Replaced Humans With AI Are Realizing Their Mistake




AWS CEO says AI replacing junior staff is ‘dumbest idea’


Origin: Artificial General Intelligence
References: ChatGPT, My AI Logs of Will AI replace humans, My AI Prompts
Created 2024-08-31

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI adoption linked to 13% decline in jobs for young U.S. workers: study]]></title>
            <link>https://www.cnbc.com/2025/08/28/generative-ai-reshapes-us-job-market-stanford-study-shows-entry-level-young-workers.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052423</guid>
            <description><![CDATA[A Standford study has found evidence that the widespread adoption of generative AI is impacting the job prospects of early career workers.]]></description>
            <content:encoded><![CDATA[A Standford study has found evidence that the widespread adoption of generative AI is impacting the job prospects of early career workers.Vertigo3d | E+ | Getty ImagesThere is growing evidence that the widespread adoption of generative AI is impacting the job prospects of America's workers, according to a paper released on Tuesday by three Stanford University researchers.The study analyzed payroll records from millions of American workers, generated by ADP, the largest payroll software firm in the U.S.The report found "early, large-scale evidence consistent with the hypothesis that the AI revolution is beginning to have a significant and disproportionate impact on entry-level workers in the American labor market."Most notably, the findings revealed that workers between the ages of 22 and 25 in jobs most exposed to AI — such as customer service, accounting and software development — have seen a 13% decline in employment since 2022.By contrast, employment for more experienced workers in the same fields, and for workers of all ages in less-exposed occupations such as nursing aides, has stayed steady or grown. Jobs for young health aides, for example, rose faster than their older counterparts.Front-line production and operations supervisors' roles also showed an increase in employment for young workers, though this growth was smaller than that for workers over the age of 35.The potential impact of AI on the job market has been a concern across industries and age groups, but the Stanford study appears to show that the results will be far from uniform. The study sought to rule out factors that could skew the data, including education level, remote work, outsourced jobs, and broader economic shifts, which could impact hiring decisions.According to the Stanford study, their findings may explain why national employment growth for young workers has been stagnant, while overall employment has largely remained resilient since the global pandemic, despite recent signs of softening.Young workers were said to be especially vulnerable because AI can replace "codified knowledge," or "book-learning" that comes from formal education. On the other hand, AI may be less capable of replacing knowledge that comes from years of experience. The researchers also noted that not all uses of AI are associated with declines in employment. In occupations where AI complements work and is used to help with efficiency, there have been muted changes in employment rates.The study — which hasn't been peer-reviewed — appears to show mounting evidence that AI will replace jobs, a topic that has been hotly debated. Earlier this month, a Goldman Sachs economist said changes to the American labor market brought on by the arrival of generative AI were already showing up in employment data, particularly in the technology sector and among younger employees. He also noted that most companies were yet to deploy artificial intelligence for day-to-day use, meaning that the job market impact had yet to be fully realized.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: SwiftAI – open-source library to easily build LLM features on iOS/macOS]]></title>
            <link>https://github.com/mi12labs/SwiftAI</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052200</guid>
            <description><![CDATA[Build beautiful and reliable LLM apps on iOS and MacOS - mi12labs/SwiftAI]]></description>
            <content:encoded><![CDATA[SwiftAI
A modern, type-safe Swift library for building AI-powered apps. SwiftAI provides a unified API that works seamlessly across different AI models - from Apple's on-device models to cloud-based services like OpenAI.




✨ Features

🤖 Model Agnostic: Unified API across Apple's on-device models, OpenAI, Anthropic, and custom backends
🎯 Structured Output: Strongly-typed structured outputs with compile-time validation
🔧 Agent Tool Loop: First-class support for tool use
💬 Conversations: Stateful chat sessions with automatic context management
🏗️ Extensible: Plugin architecture for custom models and tools
⚡ Swift-Native: Built with async/await and modern Swift concurrency

🚀 Quick Start
import SwiftAI

let llm = SystemLLM()
let response = try await llm.reply(to: "What is the capital of France?")
print(response.content) // "Paris"
📦 Installation
Swift Package Manager
Xcode:

Go to File → Add Package Dependencies
Enter: https://github.com/mi12labs/SwiftAI
Click Add Package

Package.swift:
dependencies: [
    .package(url: "https://github.com/mi12labs/SwiftAI", from: "main")
]
📚 Getting Started
🚀 Step 1: Your First AI Query
Start with the simplest possible example - just ask a question and get an answer:
import SwiftAI

// Initialize Apple's on-device language model.
let llm = SystemLLM()

// Ask a question and get a response.
let response = try await llm.reply(to: "What is the capital of France?")
print(response.content) // "Paris"
What just happened?

SystemLLM() creates Apple's on-device AI model
reply(to:) sends your question and returns a String by default
try await handles the asynchronous AI processing
The response is wrapped in a response object - use .content to get the actual text

📊 Step 2: Structured Responses
Instead of getting plain text, let's get structured data that your app can use directly:
// Define the structure you want back
@Generable
struct CityInfo {
  let name: String
  let country: String
  let population: Int
}

let response = try await llm.reply(
  to: "Tell me about Tokyo",
  returning: CityInfo.self // Tell the LLM what to output
)

let cityInfo = response.content
print(cityInfo.name)       // "Tokyo"
print(cityInfo.country)    // "Japan"
print(cityInfo.population) // 13960000
What's new here?

@Generable tells SwiftAI this struct can be generated by AI
returning: CityInfo.self specifies you want structured data, not a string
SwiftAI automatically converts the AI's response into your struct
No JSON parsing required!

💡 Key Concept: Type-Safe AI
SwiftAI ensures the AI returns data in exactly the format your code expects. If the AI can't generate valid data, you'll get an error instead of broken data.
🛠️ Step 3: Tool Use
Let your AI call functions in your app to get real-time information:
// Create a tool the AI can use
struct WeatherTool: Tool {
  let description = "Get current weather for a city"

  @Generable
  struct Arguments {
    let city: String
  }

  func call(arguments: Arguments) async throws -> String {
    // Your weather API logic here
    return "It's 72°F and sunny in \(arguments.city)"
  }
}

// Use the tool with your AI
let weatherTool = WeatherTool()
let response = try await llm.reply(
    to: "What's the weather like in San Francisco?",
    tools: [weatherTool]
)
print(response.content) // "Based on current data, it's 72°F and sunny in San Francisco"
What's new here?

Tool protocol lets you create functions the AI can call
Arguments struct defines what parameters your tool needs (also @Generable)
The AI automatically decides when to call your tool
You get back a natural language response that incorporates the tool's data

💡 Key Concept: AI Function Calling
The AI reads your tool's description and automatically decides whether to call it. You don't manually trigger tools - the AI does it when needed.
🔄 Step 4: Model Switching
Different AI models have different strengths. SwiftAI makes switching seamless:
// Choose your model based on availability
let llm: any LLM = {
  let systemLLM = SystemLLM()
  return systemLLM.isAvailable ? systemLLM : OpenaiLLM(apiKey: "your-api-key")
}()

// Same code works with any model
let response = try await llm.reply(to: "Write a haiku about Berlin.")
print(response.content)
What's new here?

SystemLLM runs on-device (private, fast, free)
OpenaiLLM uses the cloud (more capable, requires API key)
isAvailable checks if the on-device model is ready
Same reply() method works with any LLM

💡 Key Concept: Model Agnostic API
Your code doesn't change when you switch models. This lets you optimize for different scenarios (privacy, capabilities, cost) without rewriting your app.
💬 Step 5: Conversations
For multi-turn conversations, use Chat to maintain context across messages:
// Create a chat with tools
let chat = try Chat(with: llm, tools: [weatherTool])

// Have a conversation
let greeting = try await chat.send("Hello! I'm planning a trip.")
let advice = try await chat.send("What should I pack for Seattle?")
// The AI remembers context from previous messages
What's new here?

Chat maintains conversation history automatically
send() is like reply() but remembers previous messages
Tools work in conversations too
The AI remembers context from earlier in the conversation

💡 Key Concept: Stateful vs Stateless

reply() is stateless - each call is independent
Chat is stateful - builds on previous conversation

🎯 Step 6: Advanced Constraints
Add validation rules and descriptions to guide AI generation:
@Generable
struct UserProfile {
  @Guide(description: "A valid username starting with a letter", .pattern("^[a-zA-Z][a-zA-Z0-9_]{2,}$"))
  let username: String

  @Guide(description: "User age in years", .minimum(13), .maximum(120))
  let age: Int

  @Guide(description: "One to three favorite colors", .minimumCount(1), .maximumCount(3))
  let favoriteColors: [String]
}
What's new here?

@Guide adds constraints and descriptions to fields which help LLM generate good content
.pattern() tells the LLM to follow a regex
.minimum() and .maximum() constrain numbers
.minimumCount() and .maximumCount() control array sizes

💡 Key Concept: Validated Generation
Constraints ensure the AI follows your business rules.
🎯 Quick Reference



What You Want
What To Use
Example




Simple text response
reply(to:)
reply(to: "Hello")


Structured data
reply(to:returning:)
reply(to: "...", returning: MyStruct.self)


Function calling
reply(to:tools:)
reply(to: "...", tools: [myTool])


Conversation
Chat
chat.send("Hello")


Model switching
any LLM
SystemLLM() or OpenaiLLM()



🔧 Supported Models



Model
Type
Privacy
Capabilities
Cost




SystemLLM
On-device
🔒 Private
Good
🆓 Free


OpenaiLLM
Cloud API
⚠️ Shared
Excellent
💰 Paid


CustomLLM
Your choice
Your choice
Your choice
Your choice



📖 Examples

 TODO: Add example projects.

⚡ Feature Parity Status vs FoundationModels SDK



Feature
Status




Streaming responses
❌ #issue


Model prewarming
❌ #issue


Structured outputs for enums
❌ #issue



🤝 Contributing
We welcome contributions! Please read our Contributing Guidelines.
Development Setup
git clone https://github.com/your-org/SwiftAI.git
cd SwiftAI
swift build
swift test
📄 License
SwiftAI is released under the MIT License. See LICENSE for details.
⚠️ Alpha ⚠️
SwiftAI is alpha 🚧 – rough edges and breaking changes are expected.

Built with ❤️ for the Swift community
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The sisters “paradox” – counter-intuitive probability]]></title>
            <link>https://blog.engora.com/2025/08/the-sisters-paradox-counter-intuitive.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051798</guid>
        </item>
        <item>
            <title><![CDATA[GPU Prefix Sums: A nearly complete collection]]></title>
            <link>https://github.com/b0nes164/GPUPrefixSums</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051542</guid>
            <description><![CDATA[A nearly complete collection of prefix sum algorithms implemented in CUDA, D3D12, Unity and WGPU. Theoretically portable to all wave/warp/subgroup sizes.  - GitHub - b0nes164/GPUPrefixSums: A nearl...]]></description>
            <content:encoded><![CDATA[GPU Prefix Sums

GPUPrefixSums aims to bring state-of-the-art GPU prefix sum techniques from CUDA and make them available in portable compute shaders. In addition to this, it contributes "Decoupled Fallback," a novel fallback technique for Chained Scan with Decoupled Lookback that should allow devices without forward thread progress guarantees to perform the scan without crashing. The D3D12 implementation includes an extensive survey of GPU prefix sums, ranging from the warp to the device level; all included algorithms utilize wave/warp/subgroup (referred to as "wave" hereon) level parallelism but are completely agnostic of wave size. As a measure of the quality of the code, GPUPrefixSums has also been implemented in CUDA and benchmarked against Nvidia's CUB library. Although GPUPrefixSums aims to be portable to any wave size supported by HLSL, [4, 128], due to hardware limitations, it has only been tested on wave sizes 4, 16, 32, and 64. You have been warned!
If you are interested in prefix sums for their use in radix sorting, check out GPUPrefixSum's sibling repository GPUSorting!
Decoupled Fallback
In Decoupled Fallback, a threadblock will spin for a set amount of cycles while waiting for the reduction of a preceding partition tile. If the maximum spin count is exceeded, the threadblock is free to perform a fallback operation. Multiple thread blocks are allowed to perform fallbacks on the same deadlocking tile, but through use of atomic compare and swap, only one thread block ends up broadcasting its reduction in device memory. Although this means potentially performing redundant calculations, the upside is that fallback performance is no longer limited by the latency of signal propagation between thread blocks.
As of writing this 9/22/2024, Decoupled Fallback shows promising results on Apple M GPU's. However the version included here are out of date, with the most up-to-date development occuring in Vello.
Survey

A prefix sum, also called a scan, is a running total of a sequence of numbers at the n-th element. If the prefix sum is inclusive the n-th element is included in that total, if it is exclusive, the n-th element is not included. The prefix sum is one of the most important algorithmic primitives in parallel computing, underpinning everything from sorting, to compression, to graph traversal.
Basic Scans


Kogge-Stone





Sklansky





Brent-Kung





Reduce Scan





Raking Reduce-Scan



Warp-Synchronized Scans


Warp-Sized-Radix Brent-Kung





Warp-Sized-Radix Brent-Kung with Fused Upsweep-Downsweep





Warp-Sized-Radix Sklansky





Warp-Sized-Radix Serial





Warp-Sized-Radix Raking Reduce-Scan



Block-Level Scan Pattern


First Partition





Second Partition and Onwards



Device Level Scan Pattern (Reduce-Then-Scan)


Reduce





Scan Along the Intermediate Reductions





Scan and Pass in Intermediate Values



Getting Started
GPUPrefixSumsD3D12
Headless implementation in D3D12, includes:

Reduce then Scan
Chained Scan with Decoupled Lookback
Chained Scan with Decoupled Lookback Decoupled Fallback

Requirements:

Visual Studio 2019 or greater
Windows SDK 10.0.20348.0 or greater

The repository folder contains a Visual Studio 2019 project and solution file. Upon building the solution, NuGet will download and link the following external dependencies:

DirectX 12 Agility SDK
DirectX Shader Compiler
Microsoft Windows Implementation Library

See the repository wiki for information on running tests.
GPUPrefixSumsCUDA
GPUPrefixSumsCUDA includes:

Reduce then Scan
Chained Scan with Decoupled Lookback

The purpose of this implementation is to benchmark the algorithms and demystify their implementation in the CUDA environment. It is not intended for production or use; instead, a proper implementation can be found in the CUB library.
Requirements:

Visual Studio 2019 or greater
Windows SDK 10.0.20348.0 or greater
CUDA Toolkit 12.3.2
Nvidia Graphics Card with Compute Capability 7.x or greater.

The repository folder contains a Visual Studio 2019 project and solution file; there are no external dependencies besides the CUDA toolkit. The use of sync primitives necessitates Compute Capability 7.x or greater. See the repository wiki for information on running tests.
GPUPrefixSumsUnity
Released as a Unity package includes:

Reduce then Scan
Chained Scan with Decoupled Lookback

Requirements:

Unity 2021.3.35f1 or greater

Within the Unity package manager, add a package from git URL and enter:
https://github.com/b0nes164/GPUPrefixSums.git?path=/GPUPrefixSumsUnity
See the repository wiki for information on running tests.
GPUPrefixSumsWGPU
WARNING: TESTING ONLY CURRENTLY, NOT FULLY PORTABLE
Barebones implementation--no vectorization, no wave intrinsics--to be used as a testbed.
Requirements:

wgpu 22.0
pollster 0.3
bytemuck 1.16.3

Interesting Reading and Bibliography
Duane Merrill and Michael Garland. “Single-pass Parallel Prefix Scan with De-coupled Lookback”. In: 2016.
url: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
Grimshaw, Andrew S. and Duane Merrill. “Parallel Scan for Stream Architectures.” (2012).
url: https://libraopen.lib.virginia.edu/downloads/6t053g00z
Matt Pettineo. GPU Memory Pools in D3D12. Jul. 2022.
url: https://therealmjp.github.io/posts/gpu-memory-pool/
Ralph Levien. Prefix sum on portable compute shaders. Nov. 2021.
url: https://raphlinus.github.io/gpu/2021/11/17/prefix-sum-portable.html
Tyler Sorensen, Hugues Evrard, and Alastair F. Donaldson. “GPU Schedulers: How Fair Is Fair Enoughl”. In: 29th International Conference on Concurrency Theory (CONCUR 2018). Ed. by Sven Schewe and Lijun Zhang. Vol. 118. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, 23:1–23:17. isbn: 978-3-95977-087-3. doi: 10.4230/LIPIcs.CONCUR.2018.23.
url: http://drops.dagstuhl.de/opus/volltexte/2018/9561.
Vasily Volkov. “Understanding Latency Hiding on GPUs”. PhD thesis. EECS Department, University of California, Berkeley, Aug. 2016.
url: http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.html
Zhe Jia et al. Dissecting the NVidia Turing T4 GPU via Microbenchmarking. 2019. arXiv: 1903.07486.
url: https://arxiv.org/abs/1903.07486
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[That boolean should probably be something else]]></title>
            <link>https://ntietz.com/blog/that-boolean-should-probably-be-something-else/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051361</guid>
            <description><![CDATA[One of the first types we learn about is the boolean.
It's pretty natural to use, because boolean logic underpins much of modern computing.
And yet, it's one of the types we should probably be using a lot less of.
In almost every single instance when you use a boolean, it should be something else.]]></description>
            <content:encoded><![CDATA[One of the first types we learn about is the boolean.
It's pretty natural to use, because boolean logic underpins much of modern computing.
And yet, it's one of the types we should probably be using a lot less of.
In almost every single instance when you use a boolean, it should be something else.
The trick is figuring out what "something else" is.
Doing this is worth the effort.
It tells you a lot about your system, and it will improve your design (even if you end up using a boolean).
There are a few possible types that come up often, hiding as booleans.
Let's take a look at each of these, as well as the case where using a boolean does make sense.
This isn't exhaustive—[1]there are surely other types that can make sense, too.
Datetimes
A lot of boolean data is representing a temporal event having happened.
For example, websites often have you confirm your email.
This may be stored as a boolean column, is_confirmed, in the database.
It makes a lot of sense.
But, you're throwing away data: when the confirmation happened.
You can instead store when the user confirmed their email in a nullable column.
You can still get the same information by checking whether the column is null.
But you also get richer data for other purposes.
Maybe you find out down the road that there was a bug in your confirmation process.
You can use these timestamps to check which users would be affected by that, based on when their confirmation was stored.
This is the one I've seen discussed the most of all these.
We run into it with almost every database we design, after all.
You can detect it by asking if an action has to occur for the boolean to change values, and if values can only change one time.
If you have both of these, then it really looks like it is a datetime being transformed into a boolean.
Store the datetime!
Enums
Much of the remaining boolean data indicates either what type something is, or its status.
Is a user an admin or not?
Check the is_admin column!
Did that job fail?
Check the failed column!
Is the user allowed to take this action?
Return a boolean for that, yes or no!
These usually make more sense as an enum.
Consider the admin case: this is really a user role, and you should have an enum for it.
If it's a boolean, you're going to eventually need more columns, and you'll keep adding on other statuses.
Oh, we had users and admins, but now we also need guest users and we need super-admins.
With an enum, you can add those easily.
enum UserRole {
  User,
  Admin,
  Guest,
  SuperAdmin,
}

And then you can usually use your tooling to make sure that all the new cases are covered in your code.
With a boolean, you have to add more booleans, and then you have to make sure you find all the places where the old booleans were used and make sure they handle these new cases, too.
Enums help you avoid these bugs.
Job status is one that's pretty clearly an enum as well.
If you use booleans, you'll have is_failed, is_started, is_queued, and on and on.
Or you could just have one single field, status, which is an enum with the various statuses.
(Note, though, that you probably do want timestamp fields for each of these events—but you're still best having the status stored explicitly as well.)
This begins to resemble a state machine once you store the status, and it means that you can make much cleaner code and analyze things along state transition lines.
And it's not just for storing in a database, either.
If you're checking a user's permissions, you often return a boolean for that.
fn check_permissions(user: User) -> bool {
  false // no one is allowed to do anything i guess
}

In this case, true means the user can do it and false means they can't.
Usually. I think.
But you can really start to have doubts here, and with any boolean, because the application logic meaning of the value cannot be inferred from the type.
Instead, this can be represented as an enum, even when there are just two choices.
enum PermissionCheck {
  Allowed,
  NotPermitted(reason: String),
}

As a bonus, though, if you use an enum?
You can end up with richer information, like returning a reason for a permission check failing.
And you are safe for future expansions of the enum, just like with roles.
You can detect when something should be an enum a proliferation of booleans which are mutually exclusive or depend on one another.
You'll see multiple columns which are all changed at the same time.
Or you'll see a boolean which is returned and used for a long time.
It's important to use enums here to keep your program maintainable and understandable.
Conditionals
But when should we use a boolean?
I've mainly run into one case where it makes sense: when you're (temporarily) storing the result of a conditional expression for evaluation.
This is in some ways an optimization, either for the computer (reuse a variable[2]) or for the programmer (make it more comprehensible by giving a name to a big conditional) by storing an intermediate value.
Here's a contrived example where using a boolean as an intermediate value.
fn calculate_user_data(user: User, records: RecordStore) {
  // this would be some nice long conditional,
  // but I don't have one. So variables it is!
  let user_can_do_this: bool = (a && b) && (c || !d);

  if user_can_do_this && records.ready() {
    // do the thing
  } else if user_can_do_this && records.in_progress() {
    // do another thing
  } else {
    // and something else!
  }
}

But even here in this contrived example, some enums would make more sense.
I'd keep the boolean, probably, simply to give a name to what we're calculating.
But the rest of it should be a match on an enum!
* * *
Sure, not every boolean should go away.
There's probably no single rule in software design that is always true.
But, we should be paying a lot more attention to booleans.
They're sneaky.
They feel like they make sense for our data, but they make sense for our logic.
The data is usually something different underneath.
By storing a boolean as our data, we're coupling that data tightly to our application logic.
Instead, we should remain critical and ask what data the boolean depends on, and should we maybe store that instead?
It comes easier with practice.
Really, all good design does.
A little thinking up front saves you a lot of time in the long run.


I know that using an em-dash is treated as a sign of using LLMs.
LLMs are never used for my writing.
I just really like em-dashes and have a dedicated key for them on one of my keyboard layers. ↩


This one is probably best left to the compiler. ↩



  If you're looking to grow more effective as a software engineer, please consider my coaching services.
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Group Borrowing: Zero-cost memory safety with fewer restrictions]]></title>
            <link>https://verdagon.dev/blog/group-borrowing</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051345</guid>
            <description><![CDATA[Aug 28, 2025
        

                 —]]></description>
            <content:encoded><![CDATA[
  

        
          
  

            
    

              
    

              Aug 28, 2025
        

                 — 
          

                
        

              
      

            
    


If you've read my blog before, you know that memory safety is a huge unsolved problem, and that there's a vast unexplored space between the various memory safety models. The discerning eye can infer that we're starting to see the lines blur between these seemingly unrelated memory safety approaches.





This is ridiculously exciting to me, because today's popular memory-safe languages are very limited: they're fast, or they're flexible, but not both. Finding new blends is an incredibly challenging and worthy endeavor... one which has claimed the sanity of many explorers.





A few of us have been approaching the problem by starting with reference counting or garbage collection (or generational references!) and allowing functions to "borrow" those objects with much less--or zero--overhead. 0


In my biased 1 opinion, these approaches have some strong benefits. But they're not a panacea, and the world needs more approaches here.





And luckily, my good friend Nick Smith (from the Mojo community 2) has been exploring exactly that for the past few years.


I think he's found a way to add mutable aliasing directly into a borrow checker without building on a foundation of reference counting, garbage collection, or generational references. In other words, an approach for zero-overhead mutable aliasing, which is a big deal. 3





After reading his original explanation here, I knew that it should definitely be more widely known. He's graciously allowed me to take a shot at explaining it, so here we are!





I'll try to explain the approach as simply as possible, but if you have any questions, Nick can be found in the Mojo server (username nick.sm), or feel free to ask me in the r/vale subreddit or the Vale discord's #languages channel. And if you find this interesting, consider sponsoring Nick!





Also, this article is gloriously long and has a lot of context, so I'll let you know when to skip ahead.




      
  


      
      Group Borrowing: Zero-Cost Memory Safety with Fewer Restrictions
    


 Foundation: Builds on C++'s "Single Ownership"


 Recap of Rust Borrowing



 Context and Comments on Borrow Checking Limitations


 An example



 Nick's Borrowing System



 Basic Mutable Aliasing


 When to Invalidate References to Contents



 A More Complex Example



 Child groups



 print(hp_ref)


 print(ring_ref)


 print(len(rings_list_ref))


 print(s.durability)


 Child Groups, Summarized



 Where do groups come from?


 Isolation


 Functions' Group Annotations



 A More Complex Example


 Paths


 Syntax



 The approach, summarized


 Does the approach really not have unique references?



 Comparison to Borrow Checking


 Where we go from here


 Conclusion



      
      
    

      
    


1

My language Vale uses one of these approaches, so I'm of course biased to see its benefits more strongly than others!





2

Disclaimer: I work on the Mojo team at Modular! But I'll keep this post more about Nick's discovery in general, rather than how it would do in any specific language.





3

My long-time readers will recognize my cognitive dissonance here because I think such a thing is a myth.

Nick's approach makes me question that, though. At the very least, we're much closer to achieving the myth, if he hasn't solved it completely already.





        
    


    
    
  


 Foundation: Builds on C++'s "Single Ownership"

TL;DR: Nick's approach is based on single ownership, like C++ and Rust. Every value is "owned" by a containing object, array, stack frame, or global.


If you know how C++ and Rust work already, skip ahead!





If you don't know, or just like reading, I'll explain what single ownership is.





For example if we have this C++ program:


    
      
      #include <vector>
struct Engine { int fuel; };
struct Spaceship { unique_ptr<Engine> engine; };
void foo(vector<Spaceship>* ships) { ... }
void main() {
    vector<Spaceship> ships;
    ...
    foo(&ships);
}
    
  

...we can say this:



main's stack frame "owns" vector<Spaceship> ships;


The vector<Spaceship> ships; owns each Spaceship.


Each Spaceship owns its Engine (via the unique_ptr).


Each Engine owns its int fuel;


foo does not own the vector, it just has a raw pointer.


main's stack frame owns foo's stack frame.


main is the only thing without an owner.



If you've coded in C++ or Rust, you're probably familiar with this mindset.





If you've coded in C, you might think like this too, even though C doesn't explicitly track single ownership. If you trace an object's journey all the way from its malloc() call to its free() call, all of the variables/fields that the pointer passes through are dealing with the "owning pointer", so to speak. It's almost like how detectives track the "chain of custody" for evidence. In other words, who is responsible for it at any given moment.





Heck, even Java and C# programmers sometimes think in terms of single ownership. If you're supposed to call an object's "dispose"/"cleanup"/"destroy"/"unregister"/etc. method at some point, you can trace that object's journey all the way from new to that (conceptually destructive) method call, and those are the variables/fields that are handling its "owning reference", so to speak.





Single ownership, as explained so far, is the foundation for a lot of languages:



If you add regular (unrestricted) pointers and references, you get C++.


If you add generational references and region borrowing, you get Vale.


If you add "aliasable-xor-mutable" references, you get Rust.


If you add "group borrowing", you get Nick's system.



Nick's system is the main topic of this article, but for some context, and to know why Nick's system stands out, let's take a quick detour to recap how Rust's borrow checking works.




      
    
      
  


 Recap of Rust Borrowing

To truly appreciate Nick's approach, it's helpful to know the limitations of Rust's borrow checker.


TL;DR: Rust's borrow checker has the "aliasing xor mutable" rule which makes it conservative. This means it rejects a lot of valid programs and useful patterns 4 and it causes accidental complexity for some use cases. 5 If Nick's approach can solve even some of these pain points, that's a pretty exciting step forward.


If you're already familiar with Rust's limitations, skip ahead to Nick's approach!





If not, here's a very simplified explanation of Rust's borrow checking, and I'll overview the limitations in the next section.


I'll assume some knowledge of modern C++, but if you're primarily a C programmer, check out this post instead.





There are two kinds of references: readwrite, and readonly. These are often called "mutable" and "immutable" (or more accurately "unique" and "shared") but for now, think of them as readwrite and readonly.





There are a few ways to get a readwrite reference:



If you own an object, you can get a single temporary "readwrite" reference for a certain scope (function body, loop body, then/else body, etc). During this scope, you can't access the original object directly, you must use that reference.


If you have a readwrite reference to an object (a struct, collection, etc.), you can get a readwrite reference to one of its fields (or elements) for a certain scope. During this scope, the original reference can't be used.






Using these is pretty restrictive. Because of that first rule:



You can never have two usable readwrite references to the same object in the same scope.


You can never have two usable readwrite references to an object and its field at the same time. In other words, while you have a readwrite reference to a Spaceship's Reactor, you can't read or write that Spaceship.






Now, let's introduce "readonly" references. They operate by different rules:



If you have a readwrite reference, you can get any number of temporary "readonly" references for a certain scope. During this scope, the readwrite reference is inaccessible.


In a scope, if you have an readonly reference to an object (a struct, collection, etc.), you can get a readonly reference to one of its fields (or elements) for that scope.






Rust adds some quality-of-life improvements to make this a little easier. For example, you can get a bunch of immutable references directly from an owned object. It's actually not that bad if you're writing a program that inherently agrees with the rules, like compilers, games using ECS, stateless web servers, or generally anything that transforms input data to output data.




      
  

    

4

Like observers, intrusive data structures, back-references and graphs (like doubly-linked lists), delegates, etc.





5

Like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.





        

    
    
  


 Context and Comments on Borrow Checking Limitations

One can't improve on a paradigm unless they know its limitations. So let's talk about borrow checking's limitations!





Because of those "inaccessible" rules, we can never have a readwrite reference and a readonly reference to an object at the same time. This restriction is known as "aliasability xor mutability".


In theory this doesn't sound like a problem, but in practice it means you can't implement a lot of useful patterns like observers, intrusive data structures, back-references, graphs (like doubly-linked lists), delegates, etc. and it causes accidental complexity for use cases like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.


And besides, borrow checking is generally worth it, because it means we get memory safety without run-time overhead.





Well, mostly.


Like I explain in this post, it's not really free; even if you avoid Rc/RefCell/etc., borrow checking can often incur hidden costs, like extra bounds checking or potentially expensive cloning and hashing.


The borrow checker has long been known to reject programs that are actually safe, causing you to add and change code to satisfy its constraints. When this happens, one might just shrug and say "the borrow checker is conservative," but in reality, the borrow checker is imposing accidental complexity.





And besides, we know that mutable aliasing doesn't conflict with zero-cost memory safety, as we learned from the Arrrlang thought experiment. The only question is... can we get the best of both worlds?





Perhaps! It's not certain, even with Nick's approach. But with enough innovation in this space, I think we can get there.





      
    
      
  


 An example

(Or skip ahead to Nick's approach if you understood the above!)





Here's an example (source):


    
      
      struct Entity {
    hp: u64,
    energy: u64,
}
impl Entity { ... }
fn attack(a: &mut Entity, d: &mut Entity) { ... }
fn main() {
    let mut entities = vec![
        Entity { hp: 10, energy: 10 },
        Entity { hp: 12, energy: 7 }
    ];
    attack(&mut entities[0], &mut entities[1]);
}
    
  

Rust rejects this, giving this output:


    
      
      error[E0499]: cannot borrow `entities` as mutable more than once at a time
  --> src/main.rs:35:35
   |
35 |     attack(&mut entities[0], &mut entities[1]);
   |     ------      --------          ^^^^^^^^ second mutable borrow occurs here
   |     |           |
   |     |           first mutable borrow occurs here
   |     first borrow later used by call
   |
   = help: use `.split_at_mut(position)` to obtain two mutable non-overlapping sub-slices
    
  

Alas, .split_at_mut isn't always great in practice (reasons vary) 6 and besides, we sometimes want to have two &mut referring to the same object.





The more universal workaround is to use IDs and a central collection, like this (source, uses slotmap):


    
      
      fn attack(
    entities: &mut SlotMap<DefaultKey, Entity>,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -> Result<(), String> {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}
    
  

This is using the slotmap crate (similar to generational_arena), though you often see this pattern with HashMap instead (or one could also use raw indices into a Vec, though that risks use-after-release problems).





If you want it to be more efficient, you might be tempted to get two mutable references up-front:


    
      
      fn attack(
    entities: &mut SlotMap<DefaultKey, Entity>,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -> Result<(), String> {
    let a = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);
    a.use_energy(a_energy_cost);
    d.use_energy(d_energy_cost);
    d.damage(damage);
    Ok(())
}
    
  




But alas, rustc complains:


    
      
      error[E0499]: cannot borrow `*entities` as mutable more than once at a time
  --> src/main.rs:34:13
   |
31 |     let a = entities
   |             -------- first mutable borrow occurs here
...
34 |     let d = entities
   |             ^^^^^^^^ second mutable borrow occurs here
...
37 |     let a_energy_cost = a.calculate_attack_cost(d);
   |                         - first borrow later used here
    
  

...because we're mutably borrowing entities twice: once in a's get_mut call, and once in d's get_mut call, and their usages overlap.





Or, said differently, it's worried that a and d might be pointing to the same Entity, thus violating aliasability-xor-mutability.





But why is a compiler telling me that an Entity can't attack itself? That's odd, because in this game, that's totally allowed. Even pokémon can hurt themselves in their confusion.





One might say, "because that's a memory safety risk!" But that's not necessarily true. From what I can tell, that code would be just fine, and not risk memory safety. And in fact, Nick's system handles it just fine.





So let's take a look at Nick's system!





      
  

6

For example, if you need N references instead of just 2, or they don't need to be / shouldn't be distinct, or you want to still hold a reference while also holding those N references, etc.





    
    
      
  


 Nick's Borrowing System

As I explain Nick's system, please keep in mind:



I'm taking some terminology liberties: the proposal calls them "regions", but here I'm describing them as "groups", mainly because I know that "regions" tends to get misinterpreted as "arenas". These are not arenas. 7


This proposal is from about a year ago, and Nick's been working on an even better iteration since then that's not ready yet. Subscribe to the RSS feed because I'll be posting about Nick's next proposal when it comes!


Nick's proposal is for Mojo, so the code examples are modified Mojo syntax.






Our goal is to write something like the Rust attack function from the last section:


    
      
      fn attack(
    entities: &mut SlotMap<DefaultKey, Entity>,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -> Result<(), String> {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}
    
  




But we're going to write it with memory-safe mutable aliasing, so it's simpler and shorter!





A sneak peek of what it would look like:


    
      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  a_power = a.calculate_attack_power()
  a_energy_cost = a.calculate_attack_cost(d)
  d_armor = d.calculate_defense()
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(a_power - d_armor)
    
  




I'll explain Nick's system in four steps:



Basic mutable aliasing


References to an object vs its contents


Child groups, which blur that distinction a bit


Group annotations, which help inter-function reasoning a bit






We'll start simple, and build up gradually.





      
  

7

I know this from experience. I regret naming Vale's regions "regions"!





    
    
      
  


 Basic Mutable Aliasing

Let's start by completely forgetting the difference between readonly and readwrite references. Let's say that all references are readwrite.





Now, take this simple Mojo program that has two readwrite aliases to the same list:


    
      
      fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref_a = my_list
    ref list_ref_b = my_list
    list_ref_a.append(5)
    list_ref_b.append(6)
    
  

Here's the equivalent Rust code:


    
      
      fn example() {
    let mut my_list: Vec<i64> = vec![1, 2, 3, 4];
    let list_ref_a = &mut my_list;
    let list_ref_b = &mut my_list;
    list_ref_a.push(5);
    list_ref_b.push(6);
}
    
  

The Rust compiler rejects it because we're violating aliasability-xor-mutability, specifically in that we have two active readwrite references:


    
      
      error[E0499]: cannot borrow `my_list` as mutable more than once at a time
 --> src/lib.rs:4:22
  |
3 |   let list_ref_a = &mut my_list;
  |                   ------------ first mutable borrow occurs here
4 |   let list_ref_b = &mut my_list;
  |                   ^^^^^^^^^^^^ second mutable borrow occurs here
5 |
6 |   list_ref_a.push(5);
  |   ---------- first borrow later used here
    
  




But... we humans can easily conclude this is safe. After the evaluation of list_ref_a.push(5), my_list is still there, and it's still in a valid state. So there is no risk of memory errors when evaluating the second call to push.


In any language, when we hand a function a non-owning reference to an object, that function can't destroy the object, 8 nor change its type. The same is true here.


Therefore, the caller should be able to have (and keep using) other references to that object, and it's totally fine.





Nick's approach handles this by thinking about "a reference to an object" as different from "a reference to its contents". We can have multiple references to an object, but references into an object's contents will require some special logic.


I'll explain that more in the next section.




      
  

8

If a language supports temporarily destroying a live object's field, like Mojo, Nick's model supports that as well. It tracks that "some object in this group is partially destroyed" and temporarily disables other potential references to that object while that's true.





    
    
  


 When to Invalidate References to Contents

So how do we handle a caller's references to the contents of the object? What kind of special logic does that require?





In the below example, the compiler should reject print(element_ref) because append might have modified the List.


    
      
      fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref = my_list
    ref el_ref = my_list[0]
    list_ref.append(5)
    print(el_ref)
    
  

It would be amazing if a memory safety approach knew that the previous example was fine and this one isn't.





In other words, the approach should know that when we hand append a reference to List, it shouldn't invalidate the other reference list_ref, but it should invalidate any references to its contents (like el_ref).





I like how Nick put it in his proposal:


A dynamic container is a container that stores a dynamically-changing number of items, and/or items whose type changes dynamically. The two archetypal dynamic containers are resizable arrays (i.e. Mojo's List type), and tagged unions (i.e. Mojo's Variant type).


Pointers to the items of a dynamic container need to be treated carefully, because if the container is mutated, those items may no longer reside at their original locations. There are several reasons why the items might have gone missing: the items were deleted. (e.g. a List was cleared.), the items were moved somewhere else. (e.g. a List's buffer was reallocated.), or the item has changed type. (e.g. a Variant was reassigned to a different payload type.)


In all cases, the right action to take is to invalidate the pointer





If I had to boil it down to one sentence, I'd say: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.





Following this general rule, a lot of programs are revealed to be safe.





And this isn't that crazy; if you've used C++ a lot, this likely agrees with your intuition.





Note that we'll relax this rule later, and replace it with a more accurate one. But for now, it's a useful stepping stone.




      
    
      
  


 A More Complex Example

Above, I gave a sneak peek at an attack function.


Let's look at it again:


    
      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)
    
  




For now:



Ignore the [mut r: group Entity], we'll get to that later.


Know that none of these methods delete any Entitys. 9


Know that damage modifies d. Nothing else modifies anything.



(I'll explain both of those points more later.)





Note how this function isn't holding any references to Entitys' contents... only to whole Entitys.





All these methods don't delete any Entitys, so this attack function is completely memory safe. In fact, even though the use_energy and damage methods modify Entitys, every line in attack is still memory-safe. 10





Let's look at this alternate example now to see it catching an actual memory safety risk.


Entity looks like this now:


    
      
      struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    ...
    
  

attack now holds a reference to an Entity's contents, like so:


    
      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  ...
    
  




The compiler views the program like this:





The compiler knows that:



There is a r group (in blue).


There is a r.rings.items[0] group (in green).


The r.rings.items[0] group (green) is a child of group r (blue).






Now let's see what happens when we modify d with a call to damage and then try to use that ring_ref:




      
        ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)

  d.damage(damage)
  print(ring_ref) # Invalid, should show error
    


The compiler shows an error, because one of the functions (like damage) might have deleted that first ring, so the compiler should invalidate any references to the contents of all Entitys in the group.





We're really just following the rule from before: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.




      
  

    

9

More precisely, these methods are only able to access the entities in group r by going through the variables a and d. In other words, there are no "back channels" for gaining access to the entities. This is important for memory safety and also for optimizations' correctness.





10

I'd like to remind everyone that this is all theoretical. Let me know if you have any improvements or comments on the approach!





        

    
    
      
  


 Child groups

That's a useful rule, and it can get us pretty far. But let's make it even more specific, so we can prove more programs memory-safe.





For example, look at this snippet:


    
      
        ref hp_ref = d.hp # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)

  print(hp_ref) # Valid!
    
  




The previous (invalid) program had a ring_ref referring to an element in a ring array.


This new (correct) program has an hp_ref that's pointing to a mere integer instead.


This is actually safe, and the compiler should correctly accept this. After all, since none of these methods can delete an Entity, then they can't delete its contained hp integer.





Good news, Nick's approach takes that into account!





But wait, how? Wouldn't that violate our rule? We might have used a reference (damage may have used d) to mutate an object (the Entity that d is pointing to). So why didn't we invalidate all references to the Entity's contents, like that hp_ref?





So, at long last, let's relax our rule, and replace it with something more precise.





Old rule:  When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its contents.





Better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to anything in its contents that might have been destroyed.





Or, to have more precise terms:


Even better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its "child groups".





So what's a "child group", and how is it different from the "contents" from the old rule?





If Entity was defined like this:


    
      
      struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    var armor: Box[IArmor]            # An owning pointer to heap (C++ "unique_ptr")
    var hand: Variant[Shield, Sword]  # A tagged union (Rust "enum")

struct Ring:
    var power: int

struct Shield:
    var durability: int

struct Sword:
    var sharpness: int

struct SteelArmor:
    var hardness: int
    
  




Then these things would be part of an Entity's group:



hp: Int


rings: ArrayList[Ring]


armor: Box[IArmor] 11


hand: Variant[Shield, Sword] 12






However, these would be in Entity's child groups:



The Rings inside that rings list.


The IArmor object that armor points to.


The Shield or Sword inside the hand variant.






For example, if we had this code:


    
      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp
  ref rings_list_ref = d.rings
  ref ring_ref = d.rings[rand() % len(d.rings)]
  ref armor_ref = d.armor[]  # Dereferences armor pointer

  match ref d.hand:
    case Shield as ref s:
      ...
    
  




Then these are the groups the compiler knows about:








Some observations:



d, hp_ref, and rings_list_ref all point to the r group (in blue).


ring_ref points to the r.rings.items[*] group (in green). That group represents all the rings, because the compiler doesn't know the index rand() % len(d.rings). This is different than the r.rings[0] from before.


armor_ref points to the r.armor[] group (in red).


s points to the r.hand.Shield group (in yellow). 13






As a user, you can use this rule-of-thumb: any element of a Variant or a collection (List, String, Dict, etc) or Box will be in a child group. 





If you want to go deeper, the real rule might be something like: "a Variant's element or anything owned by a pointer will be in a child group." After all, String/List/Dict/Box own things with a pointer under the hood.





That all sounds abstract, so I'll state it in more familiar terms: if an object (even indirectly) owns something that could be independently destroyed, it must be in a child group.





Now, let's see what happens to the groups when we add a damage call in. Remember: Entity.damage mutates the entity, so it has the potential to destroy the rings, armor, shields and/or swords that the entity is holding:




      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp                              # Group r
  ref rings_list_ref = d.rings                   # Group r
  ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ref armor_ref = d.armor[]                      # Group r.armor[]

  match ref d.hand:
    case Shield as ref s:                        # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
                    # Group r.armor[] is invalidated
                    # Group r.hand.Shield is invalidated

      print(hp_ref)               # Okay
      print(len(rings_list_ref))  # Okay
      print(ring_ref.power)       # Error, used invalidated group
      print(s.durability)         # Error, used invalidated group
      print(armor_ref)            # Error, used invalidated group
    


Let's look at it piece-by-piece.





      
  

    

11

An owning pointer to heap, unique_ptr in C++ speak.





12

A tagged union, "enum" in Rust speak.





13

This doesn't have an "(owns)" arrow because in Mojo (which Nick's proposal was for), a Variant is a tagged union, which holds its data inside itself, rather than pointing to its data on the heap.





        

    
    

 print(hp_ref)

The hp: Int isn't in a Variant or a collection, so it's pointing into the r group (not a child group), so the compiler can let us use our reference after the damage method.


Or using our more familiar terms: the integer can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.




    
  


 print(ring_ref)

Now consider ring_ref which points to an item in d.rings.


    
      
        ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
      ...
      print(ring_ref.power)  # Error, used invalidated group
    
  




That ring is in a collection (the d.rings ArrayList), so it's in a child group r.rings.items[*], so the compiler shouldn't let us use our reference after the damage method.





Or using our more familiar terms: the Ring could be independently destroyed (such as via a remove or append call on the ArrayList), so it's in a child group, so the compiler shouldn't let us use our reference after the damage method.





So, as you can see, hp is in the Entity's group, but a Ring is in a child group.





      
    
  


 print(len(rings_list_ref))

Let's do a harder example. Consider the rings_list_ref that points to the whole d.rings list, rather than an individual Ring.




      
        ref rings_list_ref = d.rings  # Group r
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
      ...
      print(len(rings_list_ref))  # Okay
    


That rings_list_ref is actually pointing at group r, not a child group, because the rings ArrayList isn't in a collection (it is the collection). It's in group r (not a child group), which wasn't invalidated, so the compiler can let us use our reference after the damage method.





Or using our more familiar terms: the List itself can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.





That means rings_list_ref is still valid, and we can use it in that print call!




      
    
  


 print(s.durability)

Consider s, which points into the hand variant's Shield value.




      
        match ref d.hand:
    case Shield as ref s:  # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.hand.Shield is invalidated
      ...
      print(s.durability)  # Error, used invalidated group
    


damage could have replaced that Shield with a Sword, thus destroying the Shield.





Because of that risk, the compiler invalidates all of group r's child groups, and catches that print(s.durability) is invalid.




      
    
  


 Child Groups, Summarized




To summarize all the above:



A Variant's element or anything owned by a pointer will be in a child group.



In other words, a group's child group is the objects that that group owns which can be independently destroyed


For example, if an ArrayList is in a group, then its contents array is in its child group.



When someone modifies a parent group, we invalidate all references into any of its child groups.






If any of this doesn't make sense, please help us out by coming to the Vale discord and asking questions! I want to make this explanation as clear as possible, so more people understand it.




      
    
      
  


 Where do groups come from?

So we know what a child group is, but how does one make a group? Where do they come from?





Local variables! Each local variable has its own group. 14





Let's look at main:




      
      fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(entities[0], entities[1])
    


The local variable entities introduces a group, containing only itself. As we've just discussed, this group contains several child groups (that are not created by local variables). When we invoke attack, we're passing the child group that represents the elements of the entities list.





Additionally, groups can be combined to form other groups. This would also work:




      
      fn main():
    entity_a = Entity(10, 10)
    entity_b = Entity(12, 7)
    attack(entity_a, entity_b)
    


This time, when we invoke attack, we're passing a group that represents the "union" of the two local variables.





So, to summarize where groups come from:



Each local variable forms its own group.


Multiple groups can be combined into one.


Variants and collections inside a group can form child groups.





      
  

14

What about heap allocations? For example, if we had a var x = Box[Entity](10, 10). In this case, the local variable x has a group. The Entity it's pointing to is a child group.





    
    
  


 Isolation




There's a restriction I haven't yet mentioned: all items in a group must be mutually isolated, in other words, they can't indirectly own each other, and one can't have references into the other. In other words, in the above example, an Entity cannot contain a reference to another Entity.





With this restriction, we know that e.g. d.damage(42) can't possibly delete some other Entity, for example a. More generally, we know that if a function takes in a bunch of references into a group, it can't use those to delete any items in the group.





I won't go too deeply into this, but if you want an example of why this is needed, try mentally implementing an AVL tree with the system. AVL tree nodes have ownership of other nodes, so any function that has the ability to modify a node suddenly has the ability to destroy a node, and if nodes can be destroyed, we can't know if references to them are still valid. That would be bad. So instead, we have the mutual-isolation rule.




      
    
  


 Functions' Group Annotations




Here's a smaller version of one of the above snippets.




      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref contents_ref = a.armor_pieces[0] # Ref to contents

  d.damage(3)

  print(contents_ref) # Invalid
    


At long last, we can talk about the [mut r: group Entity]! These are group annotations. They help the compiler know that two references might be referring to the same thing. Note that the call site doesn't explicitly have to supply a group for r, the compiler will infer it.





The use of the group r in the signature of attack  informs the compiler that even though d.damage(3) is modifying d, this may change the value of a, and therefore we need to invalidate any references that exist to child groups of a.





Stated more accurately, d.damage(3) is modifying group r, so it invalidates all references that point into r's child groups (like contents_ref).





These group annotations also help at the call site, like in this example:




      
      fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])
    


Specifically, this invocation of attack is valid, because attack has been declared in such a way that the arguments are allowed to alias. This information is explicit in the function signature (in attack), so it is visible to both the programmer and the compiler.




      
    
      
  


 A More Complex Example




Let's see a more complex example, and introduce a new concept called a path which helps the compiler reason about memory safety when calling functions.





Here's our main function again:


    
      
      fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])
    
  




And here's something similar to our attack from before, but with a new call to a new power_up_ring function:


    
      
      fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref armor_ref = a.armor # Ref to a's armor

  # Modifies a.rings' contents
  power_up_ring(a, a.rings[0])

  # Valid, compiler knows we only modified a.rings' contents
  armor_ref.hardness += 2
    
  




As the comments say, power_up_ring is modifying one of a's rings, and it doesn't invalidate our armor_ref.





To see how that's possible, let's see power_up_ring (note I'm taking some liberties with the syntax, a much shorter version is in a section below):


    
      
      # Wielder Entity's energy will power up the ring.
# Changes the ring, but does not change the wielder Entity.
fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4
    
  




Let's unpack that fn line:



e: group Entity means: e is a group of Entitys. Note how there's no mut here.


mut rr: group Ring means: rr is a group of Rings. This one is mut.


 = e.rings* means: rr points to Rings in e's Entitys' field .rings. We call this a path.






Nick's original proposal doesn't design for this particular capability, where we can take an immutable parent group and a mutable child group, but we tossed around the idea offline and we think it'll work. Let us know if you see anything to improve!





With this, the caller (attack) has enough information to know exactly what was modified. 15





Specifically, attack knows that Entitys' .rings elements may have changed. Therefore, after the call to power_up_ring, attack should invalidate any references pointing into Entitys' .rings elements, but not invalidate anything else. Therefore, it should not invalidate that armor_ref.





Inside the function, we see a a_ring.power += entity.energy / 4. Note how it's:



Reading an Entity


Modifying a Ring inside the Entity.






The latter is also why we have mut in mut rr: group Ring; the compiler requires a function put mut on any group it might be modifying.





This is also something that distinguishes this approach from Rust's. Partial borrows can do some of that, but generally you can't have a &Entity while also having an &mut Item pointing to one of the Entity's items.




      
  

15

Well not exactly. Technically, only the .power field is being modified, but power_up_ring is saying that anything inside Ring might have changed.





    
    
  


 Paths

I want to really emphasize something from the last section:


mut rr: group Ring = e.rings*





This is the key that makes this entire approach work across function calls. Whenever there's a callsite, like attack's call to power_up_ring(a, a.rings[0]), it can assemble a full picture of whether that call is valid, and how it affects the code around it.





When compiling attack, the compiler thinks this:



The caller (attack) calling power_up_ring(a, a.rings[0]).


The callee's first argument's group e.


The callee's second argument's group is e.rings*, which corresponds to a.rings[0] in the caller.


The callee's second argument's group is mut, so the callee will modify things in it.


Therefore, the callee will modify the caller's a.rings's elements' contents.






This path is how the caller knows what the callee might have modified. That's the vital information that helps it know exactly what other references it might need to invalidate.




      
    
  


 Syntax

If you thought that syntax was verbose:


    
      
      fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4
    
  

...that's my fault. I wanted to show what's really going on under the hood.





Nick actually has some better syntax in mind:


    
      
      fn power_up_ring(
   entity: Entity,
   mut ref [entity.rings*] a_ring: Ring
):
    a_ring.power += entity.energy / 4
    
  

Way simpler!




      
    
  


 The approach, summarized

With that, you now know all the pieces to Nick's approach. Summarizing:





References to object vs its contents: there's a distinction between an object and its contents. We can have as many references to an object as we'd like. Mutations to the contents will invalidate references that point into the contents, but don't have to invalidate any references to the object itself.





Child groups let us think a little more precisely about what mutations will invalidate what references to what contents.





Group annotations on the function give the compiler enough information at the callsite to know which references in the caller to invalidate.




      
    
      
  


 Does the approach really not have unique references?

When I was learning about the approach, I was kind of surprised that it had no unique references. They seemed inevitable. 16 In his proposal, Nick even mentions this example:


    
      
      fn foo[mut r: group String](names: List[ref[r] String]):
    p1 = names[0]
    p2 = names[1]
    p1[] = p2[]     # Error: cannot copy p2[]; it might be uninitialized.
    
  




The final line of the function first destroys p1's pointee (implicitly, just before assigning it a new value), and then copies data from p2's pointee. (By the way, postfix [] is Mojo-speak for dereference, so p1[] is like C's *p1)





The challenge here, as he explains, is that p1 and p2 might be pointing to the same object. If so, one or both of these objects might end up with uninitialized data.





His solution mentions using escape hatches in this case, like this:


    
      
      fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if __address_of(x) == __address_of(y):
        return
        
    # Now that we know the pointers don't alias, we can use unsafe
    # operations to swap the targets. The exact code isn't important.
    unsafe_x = UnsafePointer.address_of(x)
    unsafe_y = UnsafePointer.address_of(y)
    
    # ...use unsafe_x and unsafe_y here to swap the contents...
    
  




...but this can theoretically be built into the language, like this:


    
      
      fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if not distinct(x, y):
        return
    
    # ...use x and y...
    
  




At first, I saw this and thought, "Aha! distinct hints to the compiler that these are unique references!"





But... maybe not. Instead of thinking of these as unique references, you could think of this as "splitting" group r into two temporary distinct groups.




      
  

16

 Throughout the entire proposal, I was expecting the next section to talk about how we inevitably add unique references back in. And as I was thinking ahead, I kept on adding unique references in, in my tentative understanding of his model. This is the problem with being accustomed to conventional borrow checking... it makes it harder to think of any other approach.


Luckily, Nick consistently tried to understand what operations can cause pointers to dangle, and impose as few restrictions as possible while ensuring that dangling pointers are always invalidated. With that in mind, the AxM constraint never arose. It's the same mindset I used to come up with Vale's generational references + regions blend. It must be like art: design constraints lead to inspiration!






    
    
      
  


 Comparison to Borrow Checking

Group Borrowing could be much better than borrow checking.



It should be much more permissive than borrow checking, and prove a lot more programs correct.


It should lead to better error messages. Whereas rustc gives errors about abstract borrowing violations, this model should be able to give errors that point out the actual real risks: "you can't use this pointer down here, because it might not be valid anymore because of this modification up here".



This is particularly important to a language like Mojo, which is designed for Python programmers. The ramp from dynamic typing to static typing to single ownership to borrowing is steep, and this could help.



Allowing safe mutable aliasing could lead to less people reaching for workarounds like unsafe pointers. In other words, by making safe references more expressive,  we can make systems languages more memory-safe in practice.






Though, it might also result in programs that are architecturally similar to borrow checking.



The mutual isolation restriction might influence our programs' data to look like trees, similar to how Rust's borrow checker does. 17 However, the approach has much more relaxed rules around how we access those trees from the outside (via references in local variables and arguments), which is a nice improvement.


A line of code that deletes something will invalidate any references to any of its child groups. A similar constraint appears in borrow checking, though it's more relaxed here.






It might be faster than borrow checking in some cases.



For example, the attack example doesn't need to do repeated hash lookups like in Rust.


More generally, his approach means we can have more references, and less hashing, cloning, and bounds checking.






But it might be slower in some cases. Not having unique references means it could be challenging for the compiler to compile references to faster noalias 18 pointers. Nick showed me this article to highlight the possible speed differences, and we discussed a few promising options. Perhaps a compiler could:



Emit noalias for an argument when the argument is the only argument into a certain group.


Have a special kind of group where all references are guaranteed distinct.


Notice when the code checks (via if, assert, etc.) that all pointers into a group are distinct, and emit noalias then.






And this model might have downsides:



Since there's no such thing as a unique reference, it could lead to awkward situations, where we have to convince the compiler that two references don't alias.



Then again, Rust kind of has this too, in the form of .split_at_mut. It might be easier here?



This model hasn't been implemented yet and therefore hasn't been proven to work, of course.






So, will this be revolutionary? Perhaps! Or maybe it'll be just a surface-level improvement on borrow checking in practice. Or, it could be the key that unlocks borrowing and makes it more palatable to the mainstream.




      
  

    

17

 It might not actually influence our systems into trees. I suspect that in a multi-layered architecture, upper layers can have many aliasing mutable references to objects in layers below, while still allowing people to modify those lower layers' contents.


If that turns out to be true (Nick and I are still exploring it) then he's found a way to make borrowing work for some DAG-shaped program architectures, rather than just strictly tree-shaped architectures.


On top of that, if we compose this approach with linear types, I think we can get at least halfway towards compile-time-memory-safe back-references, which would unlock a lot of things like doubly-linked lists, back-references, and delegates. TBD whether that works, but that would be pretty exciting.


I'm being a bit vague, but drop by the Vale discord server and I can explain my crazy thoughts a bit more.






18

noalias is an annotation given to LLVM to tell it that no other pointer will be observing the pointed-at data while the pointer is in scope. It helps the compiler skip some loads and stores.





        

    
    
      
  


 Where we go from here




Where does the idea go from here? Not sure!





This idea is still new, and could evolve in a lot of different directions.



Maybe we'll discover some ways we can decompose it into multiple orthogonal mechanisms, like how implementation inheritance (Java's extends) is really just implements+delegation+composition. 19


Maybe we'll discover that this pairs perfectly with another mechanism, like reference counting (or generational references? Who knows!).


Maybe we'll find a different way to communicate across inter-function boundaries, so that child group invalidation can be more precisely expressed and controlled.


Maybe someone will find a way to make these groups (mutably) alias each other! 20






In the grimoire, I hinted about a hypothetical blend of reference counting and borrowing that we don't yet know how to make. I mention that one possible path to it will be to combine various memory safety techniques together. This could be one of them. 





So regardless of how well this model does on its own, it could be an amazing starting point for hybrid memory safety models. I wouldn't be surprised if one of you reads this, reads the grimoire, and discovers a clever way to blend this with existing mechanisms and techniques. Let me know if you do, and I can write an article like this for you too!





      
  

    

19

By this I mean, you can accomplish anything with extends, if you turn the base class into an interface and a struct (like Dart does), and your "subclass" would instead implements the interface, contain the struct in a field, and forward any calls from that interface into that struct.





20

This would have to be opt-in of course. Non-aliasability is a good default, because it allows the compiler to perform optimizations (e.g. keep values in registers for longer) that can actually have a dramatic impact on performance.





        

    
    
      
  


 Conclusion




Once you understand it, the concept is pretty simple in hindsight.





Of course, it pains me to say that "it's simple", because it makes it seem like it was easy to discover. I know from personal experience just how hard it is to come up with something like this... it takes a lot of thinking, trial and error, and bumping into dead ends. 21





And we must remember that Nick's model is a draft, and is still being iterated upon. As with any new model, there will be holes, and there will likely be fixes. Vale's region borrowing design fell apart and was fixed a few times yet is still standing, and Nick's model feels even cleaner than regions, so I have hope.





If there's one big thing to take away from this post, it's that we aren't done yet. There is more to find out there!





That's all! I hope you enjoyed this post. If you have any questions for Nick, he hangs out in the Mojo server (username nick.sm), or feel free to ask questions in the r/vale subreddit or Vale discord server.





And most importantly, if you enjoy this kind of exploration, sponsor Nick!





Cheers,


- Evan Ovadia




      
  

21

 Designing region borrowing for generational references took me years. And before that, I was almost broken by 32 iterations of a (now abandoned) Vale feature called "hybrid-generational memory". Near the end there, I was so burned out on the highs and lows of breaking and repairing and improving that feature, that I almost gave up on language design entirely.


Nick told me he's gone through a similarly grueling experience trying to nail down a design for his "groups". I'm glad he stuck with it!






    
    
    
  

    ]]></content:encoded>
        </item>
    </channel>
</rss>