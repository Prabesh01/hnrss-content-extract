<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 06 Sep 2025 21:03:33 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Europe enters the exascale supercomputing league with Jupiter]]></title>
            <link>https://ec.europa.eu/commission/presscorner/detail/en/ip_25_2029</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152369</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Greppers – fast CLI cheat sheet with instant copy and shareable search]]></title>
            <link>https://www.greppers.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152086</guid>
            <description><![CDATA[Clean, fast CLI commands with real‑world examples and instant copy.]]></description>
            <content:encoded><![CDATA[
    
      Stop Googling the same command twice.    A tiny, blazing‑fast directory of CLI commands with copy‑ready examples. Offline friendly. No BS.      
      Try:   
    

    

    
        Help grow the community!
        Know a useful command or recipe that's missing? Help other developers by contributing.
        Suggest a Command
      

    
      
    

    
      Built for speed and memory.
      
        Instant search: Runs entirely in your browser.
        Copy‑to‑clipboard: One click, no ceremony.
        Opinionated examples: Real‑world flags and patterns.
        Keyboard first: / focuses search, ↑↓ navigate, ⏎ copies.
      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How the "Kim" dump exposed North Korea's credential theft playbook]]></title>
            <link>https://dti.domaintools.com/inside-the-kimsuky-leak-how-the-kim-dump-exposed-north-koreas-credential-theft-playbook/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152066</guid>
            <description><![CDATA[A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor's operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.]]></description>
            <content:encoded><![CDATA[
                                
Contents:Part I: Technical AnalysisPart II: Goals AnalysisPart III: Threat Intelligence Report



Executive Summary



A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor’s operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.



Screen shot of the adversary’s desktop VM



This report is broken down into three parts: 




Technical Analysis of the dump materials



Motivation and Goals of the APT actor (group)



A CTI report compartment for analysts




While this leak only gives a partial idea of what the Kimusky/PRC activities have been, the material provides insight into the expansion of activities, nature of the actor(s), and goals they have in their penetration of the South Korean governmental systems that would benefit not only DPRK, but also PRC.



Phrack article



Without a doubt, there will be more coming out from this dump in the future, particularly if the burned assets have not been taken offline and access is still available, or if others have cloned those assets for further analysis. We may revisit this in the future if additional novel information comes to light.



Part I: Technical Analysis



The Leak at a Glance



The leaked dataset attributed to the “Kim” operator offers a uniquely operational perspective into North Korean-aligned cyber operations. Among the contents were terminal history files revealing active malware development efforts using NASM (Netwide Assembler), a choice consistent with low-level shellcode engineering typically reserved for custom loaders and injection tools. These logs were not static forensic artifacts but active command-line histories showing iterative compilation and cleanup processes, suggesting a hands-on attacker directly involved in tool assembly.



File list of dump



In parallel, the operator ran OCR (Optical Character Recognition) commands against sensitive Korean PDF documents related to public key infrastructure (PKI) standards and VPN deployments. These actions likely aimed to extract structured language or configurations for use in spoofing, credential forgery, or internal tool emulation.



Privileged Access Management (PAM) logs also surfaced in the dump, detailing a timeline of password changes and administrative account use. Many were tagged with the Korean string 변경완료 (“change complete”), and the logs included repeated references to elevated accounts such as oracle, svradmin, and app_adm01, indicating sustained access to critical systems.



The phishing infrastructure was extensive. Domain telemetry pointed to a network of malicious sites designed to mimic legitimate Korean government portals. Sites like nid-security[.]com were crafted to fool users into handing over credentials via advanced AiTM (Adversary-in-the-Middle) techniques.



nid-security[.]com phishing domain (anon reg 2024)



Finally, network artifacts within the dump showed targeted reconnaissance of Taiwanese government and academic institutions. Specific IP addresses and .tw domain access, along with attempts to crawl .git repositories, reveal a deliberate focus on high-value administrative and developer targets.



Perhaps most concerning was the inclusion of a Linux rootkit using syscall hooking (khook) and stealth persistence via directories like /usr/lib64/tracker-fs. This highlights a capability for deep system compromise and covert command-and-control operations, far beyond phishing and data theft.



Artifacts recovered from the dump include:




Terminal history files demonstrating malware compilation using NASM



OCR commands parsing Korean PDF documents related to PKI and VPN infrastructure



PAM logs reflecting password changes and credential lifecycle events



Phishing infrastructure mimicking Korean government sites



IP addresses indicating reconnaissance of Taiwanese government and research institutions



Linux rootkit code using syscall hooking and covert channel deployment




Credential Theft Focus



The dump strongly emphasizes credential harvesting as a central operational goal. Key files such as 136백운규001_env.key (The presence of 136백운규001_env.key is a smoking gun indicator of stolen South Korean Government PKI material, as its structure (numeric ID + Korean name + .key) aligns uniquely with SK GPKI issuance practices and provides clear evidence of compromised, identity-tied state cryptographic keys.) This was discovered alongside plaintext passwords, that indicate clear evidence of active compromise of South Korea’s GPKI (Government Public Key Infrastructure). Possession of such certificates would allow for highly effective identity spoofing across government systems.











PAM logs further confirmed this focus, showing a pattern of administrative account rotation and password resets, all timestamped and labeled with success indicators (변경완료: Change Complete). The accounts affected were not low-privilege; instead, usernames like oracle, svradmin, and app_adm01, often used by IT staff and infrastructure services, suggested access to core backend environments.



These findings point to a strategy centered on capturing and maintaining access to privileged credentials and digital certificates, effectively allowing the attacker to act as an insider within trusted systems.




Leaked .key files (e.g., 136백운규001_env.key) with plaintext passwords confirm access to GPKI systems



PAM logs show administrative password rotations tagged with 변경완료 (change complete)



Admin-level accounts such as oracle, svradmin, and app_adm01 repeatedly appear in compromised logs




Phishing Infrastructure



The operator’s phishing infrastructure was both expansive and regionally tailored. Domains such as nid-security[.]com and webcloud-notice[.]com mimicked Korean identity and document delivery services, likely designed to intercept user logins or deploy malicious payloads. More sophisticated spoofing was seen in sites that emulated official government agencies like dcc.mil[.]kr, spo.go[.]kr, and mofa.go[.]kr.



Whoisof domains created by dysoni91@tutamail[.]com



Historical Whois of webcloud-notice[.]com



Burner email usage added another layer of operational tradecraft. The address jeder97271[@]wuzak[.]com is likely linked to phishing kits that operated through TLS proxies, capturing credentials in real time as victims interacted with spoofed login forms.



These tactics align with previously known Kimsuky behaviors but also demonstrate an evolution in technical implementation, particularly the use of AiTM interception rather than relying solely on credential-harvesting documents.



Domain connections map




Domains include: nid-security[.]com, html-load[.]com, webcloud-notice[.]com, koala-app[.]com, and wuzak[.]com



Mimicked portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Burner email evidence: jeder97271[@]wuzak[.]com



Phishing kits leveraged TLS proxies for AiTM credential capture




Malware Development Activity



Kim’s malware development environment showcased a highly manual, tailored approach. Shellcode was compiled using NASM, specifically with flags like -f win32, revealing a focus on targeting Windows environments. Commands such as make and rm were used to automate and sanitize builds, while hashed API call resolution (VirtualAlloc, HttpSendRequestA, etc.) was implemented to evade antivirus heuristics.



The dump also revealed reliance on GitHub repositories known for offensive tooling. TitanLdr, minbeacon, Blacklotus, and CobaltStrike-Auto-Keystore were all cloned or referenced in command logs. This hybrid use of public frameworks for private malware assembly is consistent with modern APT workflows.



A notable technical indicator was the use of the proxyres library to extract Windows proxy settings, particularly via functions like proxy_config_win_get_auto_config_url. This suggests an interest in hijacking or bypassing network-level security controls within enterprise environments.




Manual shellcode compilation via nasm -f win32 source/asm/x86/start.asm



Use of make, rm, and hash obfuscation of Win32 API calls (e.g., VirtualAlloc, HttpSendRequestA)



GitHub tools in use: TitanLdr, minbeacon, Blacklotus, CobaltStrike-Auto-Keystore



Proxy configuration probing through proxyres library (proxy_config_win_get_auto_config_url)




Rootkit Toolkit and Implant Structure



The Kim dump offers deep insight into a stealthy and modular Linux rootkit attributed to the operator’s post-compromise persistence tactics. The core implant, identified as vmmisc.ko (alternatively VMmisc.ko in some shells), was designed for kernel-mode deployment across multiple x86_64 Linux distributions and utilizes classic syscall hooking and covert channeling to maintain long-term undetected access.







Google Translation of Koh doc: Rootkit Endpoint Reuse Authentication Tool



“This tool uses kernel-level rootkit hiding technology, providing a high degree of stealth and penetration connection capability. It can hide while running on common Linux systems, and at the kernel layer supports connection forwarding, allowing reuse of external ports to connect to controlled hosts. Its communication behavior is hidden within normal traffic.



The tool uses binary merging technology: at compile time, the application layer program is encrypted and fused into a .ko driver file. When installed, only the .ko file exists. When the .ko driver starts, it will automatically decompress and release the hidden application-layer program.



Tools like chkrootkit, rkhunter, and management utilities (such as ps, netstat, etc.) are bypassed through technical evasion and hiding, making them unable to detect hidden networks, ports, processes, or file information.



To ensure software stability, all functions have also passed stress testing.



Supported systems: Linux Kernel 2.6.x / 3.x / 4.x, both x32 and x64 systems”.



Implant Features and Behavior



This rootkit exhibits several advanced features:




Syscall Hooking: Hooks critical kernel functions (e.g., getdents, read, write) to hide files, directories, and processes by name or PID.



SOCKS5 Proxy: Integrated remote networking capability using dynamic port forwarding and chained routing.



PTY Backdoor Shell: Spawns pseudoterminals that operate as interactive reverse shells with password protection.



Encrypted Sessions: Session commands must match a pre-set passphrase (e.g., testtest) to activate rootkit control mode.




Once installed (typically using insmod vmmisc.ko), the rootkit listens silently and allows manipulation via an associated client binary found in the dump. The client supports an extensive set of interactive commands, including:



+p              # list hidden processes



+f              # list hidden files



callrk          # load client ↔ kernel handshake



exitrk          # gracefully unload implant



shell           # spawn reverse shell



socks5          # initiate proxy channel



upload / download # file transfer interface



These capabilities align closely with known DPRK malware behaviors, particularly from the Kimsuky and Lazarus groups, who have historically leveraged rootkits for lateral movement, stealth, persistence, and exfiltration staging.



Observed Deployment



Terminal history (.bash_history) shows the implant was staged and tested from the following paths:



.cache/vmware/drag_and_drop/VMmisc.ko

/usr/lib64/tracker-fs/vmmisc.ko

Execution logs show the use of commands such as:

insmod /usr/lib64/tracker-fs/vmmisc.ko

./client 192.168.0[.]39 testtest



These paths were not random—they mimic legitimate system service locations to avoid detection by file integrity monitoring (FIM) tools.



Deployment map



This structure highlights the modular, command-activated nature of the implant and its ability to serve multiple post-exploitation roles while maintaining stealth through kernel-layer masking.



Strategic Implications



The presence of such an advanced toolkit in the “Kim” dump strongly suggests the actor had persistent access to Linux server environments, likely via credential compromise. The use of kernel-mode implants also indicates long-term intent and trust-based privilege escalation. The implant’s pathing, language patterns, and tactics (e.g., use of /tracker-fs/, use of test passwords) match TTPs previously observed in operations attributed to Kimsuky, enhancing confidence in North Korean origin.



OCR-Based Recon



A defining component of Kim’s tradecraft was the use of OCR to analyze Korean-language security documentation. The attacker issued commands such as ocrmypdf -l kor+eng “file.pdf” to parse documents like 별지2)행정전자서명_기술요건_141125.pdf (“Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf”) and SecuwaySSL U_카달로그.pdf (“SecuwaySSL U_Catalog.pdf”). These files contain technical language around digital signatures, SSL implementations, and identity verification standards used in South Korea’s PKI infrastructure.



This OCR-based collection approach indicates more than passive intelligence gathering – it reflects a deliberate effort to model and potentially clone government-grade authentication systems. The use of bilingual OCR (Korean + English) further confirms the operator’s intention to extract usable configuration data across documentation types.



OCR run on Korean PDFs




OCR commands used to extract Korean PKI policy language from PDFs such as (별지2)행정전자서명_기술요건_141125.pdf and SecuwaySSL U_카달로그.pdf

별지2)행정전자서명_기술요건_141125.pdf → (Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf



SecuwaySSL U_카달로그.pdf → SecuwaySSL U_Catalog.pdf





Command examples: ocrmypdf -l kor+eng “file.pdf”




SSH and Log-Based Evidence



The forensic evidence contained within the logs, specifically SSH authentication records and PAM outputs, provides clear technical confirmation of the operator’s tactics and target focus.



Several IP addresses stood out as sources of brute-force login attempts. These include 23.95.213[.]210 (a known VPS provider used in past credential-stuffing campaigns), 218.92.0[.]210 (allocated to a Chinese ISP), and 122.114.233[.]77 (Henan Mobile, China). These IPs were recorded during multiple failed login events, strongly suggesting automated password attacks against exposed SSH services. Their geographic distribution and known history in malicious infrastructure usage point to an external staging environment, possibly used for pivoting into Korean and Taiwanese systems.



Beyond brute force, the logs also contain evidence of authentication infrastructure reconnaissance. Multiple PAM and OCSP (Online Certificate Status Protocol) errors referenced South Korea’s national PKI authority, including domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. These errors appear during scripted or automated access attempts, indicating a potential strategy of credential replay or certificate misuse against GPKI endpoints, an approach that aligns with Kim’s broader PKI-targeting operations.



Perhaps the most revealing detail was the presence of successful superuser logins labeled with the Korean term 최고 관리자 (“Super Administrator”). This suggests the actor was not just harvesting credentials but successfully leveraging them for privileged access, possibly through cracked accounts, reused credentials, or insider-sourced passwords. The presence of such accounts in conjunction with password rotation entries marked as 변경완료 (“change complete”) further implies active control over PAM-protected systems during the operational window captured in the dump.



Together, these logs demonstrate a methodical campaign combining external brute-force access, PKI service probing, and administrative credential takeover, a sequence tailored for persistent infiltration and lateral movement within sensitive government and enterprise networks.



Brute force mapping




Brute-force IPs: 23.95.213[.]210, 218.92.0[.]210, 122.114.233[.]77




IP AddressOriginRole / Threat Context218.92.0[.]210China Telecom (Jiangsu)Part of Chinanet backbone, likely proxy or scanning node23.95.213[.]210Colocrossing (US)Frequently used in brute-force and anonymized hosting for malware ops122.114.233[.]77Presumed PRC local ISPPossibly mobile/ISP-based proxy used to obfuscate lateral movement




PAM/OCSP errors targeting gva.gpki.go[.]kr, ivs.gpki.go[.]kr



Superuser login events under 최고 관리자 (Super Administrator)




Part II: Goals Analysis



Targeting South Korea: Identity, Infrastructure, and Credential Theft



The “Kim” operator’s campaign against South Korea was deliberate and strategic, aiming to infiltrate the nation’s digital trust infrastructure at multiple levels. A central focus was the Government Public Key Infrastructure (GPKI), where the attacker exfiltrated certificate files, including .key and .crt formats, some with plaintext passwords, and attempted repeated authentication against domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. OCR tools were used to parse Korean technical documents detailing PKI and VPN architectures, demonstrating a sophisticated effort to understand and potentially subvert national identity frameworks. These efforts were not limited to reconnaissance; administrative password changes were logged, and phishing kits targeted military and diplomatic webmail, including clones of mofa.go[.]kr and credential harvesting through adversary-in-the-middle (AiTM) proxy setups.



Attempts at user account authentication



Servlet requests for KR domains



Beyond authentication systems, Kim targeted privileged accounts (oracle, unwadm, svradmin) and rotated credentials to maintain persistent administrative access, as evidenced by PAM and SSH logs showing elevated user activity under the title 최고 관리자 (“Super Administrator”). The actor also showed interest in bypassing VPN controls, parsing SecuwaySSL configurations for exploitation potential, and deployed custom Linux rootkits using syscall hooking to establish covert persistence on compromised machines. Taken together, the dump reveals a threat actor deeply invested in credential dominance, policy reconnaissance, and system-level infiltration, placing South Korea’s public sector identity systems, administrative infrastructure, and secure communications at the core of its long-term espionage objectives.



Taiwan Reconnaissance



Among the most notable aspects of the “Kim” leak is the operator’s deliberate focus on Taiwanese infrastructure. The attacker accessed a number of domains with clear affiliations to the island’s public and private sectors, including tw.systexcloud[.]com (linked to enterprise cloud solutions), mlogin.mdfapps[.]com (a mobile authentication or enterprise login portal), and the .git/ directory of caa.org[.]tw, which belongs to the Chinese Institute of Aeronautics, a government-adjacent research entity.



This last domain is especially telling. Accessing .git/ paths directly implies an attempt to enumerate internal source code repositories, a tactic often used to discover hardcoded secrets, API keys, deployment scripts, or developer credentials inadvertently exposed via misconfigured web servers. This behavior points to  more technical depth than simple phishing; it indicates supply chain reconnaissance and long-term infiltration planning.



Taiwanese target map



The associated IP addresses further reinforce this conclusion. All three, 163.29.3[.]119, 118.163.30[.]45, and 59.125.159[.]81, are registered to academic, government, or research backbone providers in Taiwan. These are not random scans; they reflect targeted probing of strategic digital assets.



Summary of Whois & Ownership Insights




118.163.30[.]45

Appears as part of the IP range used for the domain dtc-tpe.com[.]tw, linked to Taiwan’s HINET provider (118.163.30[.]46 )Site Indices page of HINET provider.





163.29.3[.]119

Falls within the 163.29.3[.]0/24 subnet identified with Taiwanese government or institutional use, notably in Taipei. This corresponds to B‑class subnets assigned to public/government entities IP地址 (繁體中文).





59.125.159[.]81

Belongs to the broader 59.125.159[.]0–59.125.159[.]254 block, commonly used by Taiwanese ISP operators such as Chunghwa Telecom in Taipei






Taken together, this Taiwan-focused activity reveals an expanded operational mandate. Whether the attacker is purely DPRK-aligned or operating within a DPRK–PRC fusion cell, the intent is clear: compromise administrative and developer infrastructure in Taiwan, likely in preparation for broader credential theft, espionage, or disruption campaigns.




Targeted domains: tw.systexcloud[.]com, caa.org[.]tw/.git/, mlogin.mdfapps[.]com



IPs linked to Taiwanese academic/government assets: 163.29.3[.]119, 118.163.30[.]45, 59.125.159[.]81



Git crawling suggests interest in developer secrets or exposed tokens




Hybrid Attribution Model



The “Kim” operator embodies the growing complexity of modern nation-state attribution, where cyber activities often blur traditional boundaries and merge capabilities across geopolitical spheres. This case reveals strong indicators of both North Korean origin and Chinese operational entanglement, presenting a textbook example of a hybrid APT model.







On one hand, the technical and linguistic evidence strongly supports a DPRK-native operator. Terminal environments, OCR parsing routines, and system artifacts consistently leverage Korean language and character sets. The operator’s activities reflect a deep understanding of Korean PKI systems, with targeted extraction of GPKI .key files and automation to parse sensitive Korean government PDF documentation. These are hallmarks of Kimsuky/APT43 operations, known for credential-focused espionage against South Korean institutions and diplomatic targets. The intent to infiltrate identity infrastructure is consistent with North Korea’s historical targeting priorities. Notably, the system time zone on Kim’s host machine was set to UTC+9 (Pyongyang Standard Time), reinforcing the theory that the actor maintains direct ties to the DPRK’s internal environment, even if operating remotely.



However, this actor’s digital footprint extends well into Chinese infrastructure. Browser and download logs reveal frequent interaction with platforms like gitee[.]com, baidu[.]com, and zhihu[.]com, highly popular within the PRC but unusual for DPRK operators who typically minimize exposure to foreign services. Moreover, session logs include simplified Chinese content and PRC browsing behaviors, suggesting that the actor may be physically operating within China or through Chinese-language systems. This aligns with longstanding intelligence on North Korean cyber operators stationed in Chinese border cities such as Shenyang and Dandong, where DPRK nationals often conduct cyber operations with tacit approval or logistical consent from Chinese authorities. These locations provide higher-speed internet, relaxed oversight, and convenient geopolitical proximity.



Browser History viewing Taiwanese and Chinese sites



The targeting of Taiwanese infrastructure further complicates attribution. Kimsuky has not historically prioritized Taiwan, yet in this case, the actor demonstrated direct reconnaissance of Taiwanese government and developer networks. While this overlaps with Chinese APT priorities, recent evidence from the “Kim” dump, including analysis of phishing kits and credential theft workflows, suggests this activity was likely performed by a DPRK actor exploring broader regional interests, possibly in alignment with Chinese strategic goals. Researchers have noted that Kimsuky operators have recently asked questions in phishing lures related to potential Chinese-Taiwanese conflicts, implying interest beyond the Korean peninsula.



Some tooling overlaps with PRC-linked APTs, particularly GitHub-based stagers and proxy-resolving modules, but these are not uncommon in the open-source malware ecosystem and may reflect opportunistic reuse rather than deliberate mimicry.



IMINT Analysis: Visual Tradecraft and Cultural Camouflage



A review of image artifacts linked to the “Kim” actor reveals a deliberate and calculated use of Chinese social and technological visual content as part of their operational persona. These images, extracted from browser history and uploads attributed to the actor, demonstrate both strategic alignment with DPRK priorities and active cultural camouflage within the PRC digital ecosystem.



Uploads of images by Kim found in browser history



Images downloaded from aixfan[.]com



The visual set includes promotional graphics for Honor smartphones, SoC chipset evolution charts, Weibo posts featuring vehicle registration certificates, meme-based sarcasm, and lifestyle imagery typical of Chinese internet users. Notably, the content is exclusively rendered in simplified Chinese, reinforcing prior assessments that the operator either resides within mainland China or maintains a working digital identity embedded in Chinese platforms. Devices and services referenced, such as Xiaomi phones, Zhihu, Weibo, and Baidu, suggest intimate familiarity with PRC user environments.



Operationally, this behavior achieves two goals. First, it enables the actor to blend in seamlessly with native PRC user activity, which complicates attribution and helps bypass platform moderation or behavioral anomaly detection. Second, the content itself may serve as bait or credibility scaffolding (e.g. A framework to give the illusion of trust to allow for easier compromise ) in phishing and social engineering campaigns, especially those targeting developers or technical users on Chinese-language platforms.



Some images, such as the detailed chipset timelines and VPN or device certification posts, suggest a continued interest in supply chain reconnaissance and endpoint profiling—both tradecraft hallmarks of Kimsuky and similar APT units. Simultaneously, meme humor, sarcastic overlays, and visual metaphors (e.g., the “Kaiju’s tail is showing” idiom) indicate the actor’s fluency in PRC netizen culture and possible mockery of operational security breaches—whether their own or others’.



Taken together, this IMINT corpus supports the broader attribution model: a DPRK-origin operator embedded, physically or virtually, within the PRC, leveraging local infrastructure and social platforms to facilitate long-term campaigns against South Korea, Taiwan, and other regional targets while maintaining cultural and technical deniability.



Attribution Scenarios:




Option A: DPRK Operator Embedded in PRC

Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.



Use of PRC infrastructure (e.g., Baidu, Gitee) and simplified Chinese content implies the operator is physically located in China or benefits from access to Chinese internet infrastructure.





Option B: PRC Operator Emulating DPRK

Taiwan-focused reconnaissance aligns with PRC cyber priorities.



Use of open-source tooling and phishing methods shared with PRC APTs could indicate tactical emulation.






The preponderance of evidence supports the hypothesis that “Kim” is a North Korean cyber operator embedded in China or collaborating with PRC infrastructure providers. This operational model allows the DPRK to amplify its reach, mask attribution, and adopt regional targeting strategies beyond South Korea, particularly toward Taiwan. As this hybrid model matures, it reflects the strategic adaptation of DPRK-aligned threat actors who exploit the permissive digital environment of Chinese networks to evade detection and expand their operational playbook.



Targeting Profiles



The “Kim” leak provides one of the clearest windows to date into the role-specific targeting preferences of the operator, revealing a deliberate focus on system administrators, credential issuers, and backend developers, particularly in South Korea and Taiwan.



In South Korea, the operator’s interest centers around PKI administrators and infrastructure engineers. The recovered OCR commands were used to extract technical details from PDF documents outlining Korea’s digital signature protocols, such as identity verification, certificate validation, and encrypted communications, components that form the backbone of Korea’s secure authentication systems. The goal appears to be not only credential theft but full understanding and potential replication of government-trusted PKI procedures. This level of targeting suggests a strategic intent to penetrate deeply trusted systems, potentially for use in later spoofing or identity masquerading operations.



PKI attack targets



In Taiwan, the operator shifted focus to developer infrastructure and cloud access portals. Specific domains accessed, like caa.org[.]tw/.git/, indicate attempts to enumerate internal repositories, most likely to discover hardcoded secrets, authentication tokens, or deployment keys. This is a classic supply chain targeting method, aiming to access downstream systems via compromised developer credentials or misconfigured services.



Additional activity pointed to interaction with cloud service login panels such as tw.systexcloud[.]com and mlogin.mdfapps[.]com. These suggest an attempt to breach centralized authentication systems or identity providers, granting the actor broader access into enterprise or government networks with a single credential set.



Taken together, these targeting profiles reflect a clear emphasis on identity providers, backend engineers, and those with access to system-level secrets. This reinforces the broader theme of the dump: persistent, credential-first intrusion strategies, augmented by reconnaissance of authentication standards, key management policies, and endpoint development infrastructure.



South Korean:




PKI admins, infrastructure engineers



OCR focus on Korean identity standards




Taiwanese:




Developer endpoints and internal .git/ repos



Access to cloud panels and login gateways




Final Assessment



The “Kim” leak represents one of the most comprehensive and technically intimate disclosures ever associated with Kimsuky (APT43) or its adjacent operators. It not only reaffirms known tactics, credential theft, phishing, and PKI compromise, but exposes the inner workings of the operator’s environment, tradecraft, and operational intent in ways rarely observed outside of active forensic investigations.



At the core of the leak is a technically competent actor, well-versed in low-level shellcode development, Linux-based persistence mechanisms, and certificate infrastructure abuse. Their use of NASM, API hashing, and rootkit deployment points to custom malware authorship. Furthermore, the presence of parsed government-issued Korean PDFs, combined with OCR automation, shows not just opportunistic data collection but a concerted effort to model, mimic, or break state-level identity systems, particularly South Korea’s GPKI.



The operator’s cultural and linguistic fluency in Korean, and their targeting of administrative and privileged systems across South Korean institutions, support a high-confidence attribution to a DPRK-native threat actor. However, the extensive use of Chinese platforms like gitee[.]com, Baidu, and Zhihu, and Chinese infrastructure for both malware hosting and browsing activity reveals a geographical pivot or collaboration: a hybrid APT footprint rooted in DPRK tradecraft but operating from or with Chinese support.



Most notably, this leak uncovers a geographical expansion of operational interest; the actor is no longer solely focused on the Korean peninsula. The targeting of Taiwanese developer portals, government research IPs, and .git/ repositories shows a broadened agenda that likely maps to both espionage and supply chain infiltration priorities. This places Taiwan, like South Korea, at the forefront of North Korean cyber interest, whether for intelligence gathering, credential hijacking, or as staging points for more complex campaigns.



The threat uncovered here is not merely malware or phishing; it is an infrastructure-centric, credential-first APT campaign that blends highly manual operations (e.g., hand-compiled shellcode, direct OCR of sensitive PDFs) with modern deception tactics such as AiTM phishing and TLS proxy abuse.



Organizations in Taiwan and South Korea, particularly those managing identity, certificate, and cloud access infrastructure, should consider themselves under persistent, credential-focused surveillance. Defensive strategies must prioritize detection of behavioral anomalies (e.g., use of OCR tools, GPKI access attempts), outbound communications with spoofed Korean domains, and the appearance of low-level toolchains like NASM or proxyres-based scanning utilities within developer or admin environments.



In short: the “Kim” actor embodies the evolution of nation-state cyber threats—a fusion of old-school persistence, credential abuse, and modern multi-jurisdictional staging. The threat is long-term, embedded, and adaptive.



Part III: Threat Intelligence Report



TLP WHITE:



Targeting Summary



The analysis of the “Kim” operator dump reveals a highly focused credential-theft and infrastructure-access campaign targeting high-value assets in both South Korea and Taiwan. Victims were selected based on their proximity to trusted authentication systems, administrative control panels, and development environments.



CategoryDetailsRegionsSouth Korea, TaiwanTargetsGovernment, Telecom, Enterprise ITAccountssvradmin, oracle, app_adm01, unwadm, shkim88, jaejung91Domainstw.systexcloud[.]com, nid-security[.]com, spo.go[.]kr, caa.org[.]tw/.git/



Indicators of Compromise (IOCs)



Domains




Phishing: nid-security[.]com, html-load[.]com, wuzak[.]com, koala-app[.]com, webcloud-notice[.]com



Spoofed portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Pastebin raw links: Used for payload staging and malware delivery




IP Addresses




External Targets (Taiwan):

163.29.3[.]119     National Center for High-performance Computing



118.163.30[.]45   Taiwanese government subnet



59.125.159[.]81   Chunghwa Telecom





Brute Forcing / Infrastructure Origins:

23.95.213[.]210   VPS provider with malicious history



218.92.0[.]210     China Unicom



122.114.233[.]77  Henan Mobile, PRC






Internal Host IPs (Operator Environment)




192.168.130[.]117



192.168.150[.]117



192.168.0[.]39




Operator Environment: Internal Host IP Narrative



The presence of internal IP addresses such as 192.168.130[.]117, 192.168.150[.]117, and 192.168.0[.]39 within the dump offers valuable insight into the attacker’s local infrastructure, an often-overlooked element in threat intelligence analysis. These addresses fall within private, non-routable RFC1918 address space, commonly assigned by consumer off-the-shelf (COTS) routers and small office/home office (SOHO) network gear.



The use of the 192.168.0[.]0/16 subnet, particularly 192.168.0.x and 192.168.150.x, strongly suggests that the actor was operating from a residential or low-profile environment, not a formal nation-state facility or hardened infrastructure. This supports existing assessments that North Korean operators, particularly those affiliated with Kimsuky, often work remotely from locations in third countries such as China or Southeast Asia, where they can maintain inconspicuous, low-cost setups while accessing global infrastructure.



Moreover, the distinction between multiple internal subnets (130.x, 150.x, and 0.x) may indicate segmentation of test environments or multiple virtual machines running within a single NATed network. This aligns with the forensic evidence of iterative development and testing workflows seen in the .bash_history files, where malware stagers, rootkits, and API obfuscation utilities were compiled, cleaned, and rerun repeatedly.



Together, these IPs reveal an operator likely working from a clandestine, residential base of operations, with modest hardware and commercial-grade routers. This operational setup is consistent with known DPRK remote IT workers and cyber operators who avoid attribution by blending into civilian infrastructure. It also suggests the attacker may be physically located outside of North Korea, possibly embedded in a friendly or complicit environment, strengthening the case for China-based activity by DPRK nationals.



MITRE ATT&CK Mapping



PhaseTechnique(s)Initial AccessT1566.002 ,  Adversary-in-the-Middle (AiTM) PhishingExecutionT1059.005 ,  Native API ShellcodeT1059.003 ,  Bash/Shell ScriptsCredential AccessT1555 ,  Credential Store DumpingT1557.003 ,  Session HijackingPersistenceT1176 ,  Rootkit (via khook syscall manipulation)Defense EvasionT1562.001 ,  Disable Security ToolsT1552 ,  Unsecured Credential FilesDiscoveryT1592 ,  Technical Information DiscoveryT1590 ,  Network InformationExfiltrationT1041 ,  Exfiltration over C2 ChannelT1567.002 ,  Exfil via Cloud Services



Tooling and Capabilities



The actor’s toolkit spans multiple disciplines, blending malware development, system reconnaissance, phishing, and proxy evasion:




NASM-based shellcode loaders: Compiled manually for Windows execution.



Win32 API hashing: Obfuscated imports via hashstring.py to evade detection.



GitHub/Gitee abuse: Tooling hosted or cloned from public developer platforms.



OCR exploitation: Used ocrmypdf to parse Korean PDF specs related to digital certificates and VPN appliances.



Rootkit deployment: Hidden persistence paths including /usr/lib64/tracker-fs and /proc/acpi/pcicard.



Proxy config extraction: Investigated PAC URLs using proxyres-based recon.




Attribution Confidence Assessment



Attribution CandidateConfidence LevelDPRK-aligned (Kimsuky)High, Native Korean targeting, GPKI focus, OCR behaviorChina-blended infrastructureModerate, PRC hosting, Gitee usage, Taiwan focusSolely PRC ActorLow-to-Moderate, Tooling overlap but weak linguistic match



Assessment: The actor appears to be a DPRK-based APT operator working from within or in partnership with Chinese infrastructure, representing a hybrid attribution model.



Defensive Recommendations



AreaRecommendationPKI SecurityMonitor usage of .key, .sig, .crt artifacts; enforce HSM or 2FA for key usePhishing DefenseBlock domains identified in IoCs; validate TLS fingerprints and referrer headersEndpoint HardeningDetect use of nasm, make, and OCR tools; monitor /usr/lib*/tracker-* pathsNetwork TelemetryAlert on .git/ directory access from external IPs; monitor outbound to Pastebin/GitHubTaiwan FocusEstablish watchlists for .tw domains targeted by PRC-originating IPsAdmin AccountsReview usage logs for svradmin, oracle, app_adm01, and ensure rotation policies



APPENDIX A



Overlap or Confusion with Chinese Threat Actors



There is notable evidence of operational blur between Kimsuky and Chinese APTs in the context of Taiwan. The 2025 “Kim” data breach revealed an attacker targeting Taiwan whose tools and phishing kits matched Kimsuky’s, yet whose personal indicators (language, browsing habits) suggested a Chinese national. Researchers concluded this actor was likely a Chinese hacker either mimicking Kimsuky tactics or collaborating with them.. In fact, the leaked files on DDoS Secrets hint that Kimsuky has “openly cooperated with other Chinese APTs and shared their tools and techniques”. This overlap can cause attribution confusion – a Taiwan-focused operation might initially be blamed on China but could involve Kimsuky elements, or vice versa. So far, consensus is that North Korean and Chinese cyber operations remain separate, but cases like “Kim” show how a DPRK-aligned actor can operate against Taiwan using TTPs common to Chinese groups, muddying the waters of attribution.



File List from dump:















Master Evidence Inventory:



File NameLanguageContent SummaryCategoryRelevance.bash_historyMixed (EN/KR)Operator shell history commandsSystem/LogShows rootkit compilation, file ops, network testsuser-bash_historyMixed (EN/KR)User-level shell commandsSystem/LogDevelopment and test activityroot-bash_historyMixed (EN/KR)Root-level shell commandsSystem/LogPrivilege-level activity, implant deploymentauth.log.2EN/KRAuthentication logs (PAM/SSH)System/LogCredential changes marked 변경완료, brute force IPs20190315.logENSystem log fileSystem/LogAuth and system access eventschrome-timeline.txtENBrowser activity timelineBrowserVisited domains extractionchromehistory.txtENBrowser history exportBrowserURLs visitedhistory.sqliteENEmpty DB fileBrowserNo useful dataMedia HistoryENEmpty SQLite DBBrowserNo playback activityHistoryENEmpty Brave/Chromium DBBrowserNo visited URLsWeb DataENAutofill/search DBBrowserSearch engines used (Google, DuckDuckGo, Qwant, Startpage, Ecosia)Visited LinksBinaryLevelDB/binary structureBrowserCould not extract URLsCookiesENSQLite DB with cookiesBrowserGoogle cookies foundrequest_log.txt.20250220ENCaptured phishing sessionPhishingSpoofed spo.go.kr, base64 credential logging技术说明书 – 22.docxZHChinese rootkit stealth manualRootkitKernel hiding, binary embedding1.ko 图文编译 .docZHChinese compilation guideRootkitRootkit build process1. build ko .txtZHBuild notesRootkitImplant compilation instructions0. 使用.txtZHUsage notesRootkitImplant usage and commandsre 正向工具修改建议 1.0.txtZHModification notesRootkitReverse tool modification suggestions1111.txtZHRootkit/tool snippetRootkitPart of implant notesclientBinaryRootkit client binaryRootkitController for implant communicationSSA_AO_AD_WT_002_웹보안 프로토콜설계서_Ver1.0_.docKRGPKI protocol design docPKIKorean web PKI standards행자부 웹보안API 인수인계.docKRGPKI API deployment manualPKIDeployment and cert API internalsHIRA-IR-T02_의약품처방조제_ComLibrary_통신전문.docKRMedical ComLibrary XML specHealthcarePrescription system communication(별지2)행정전자서명_기술요건_141125.pdfKRPKI requirements PDFPKIOCR targetSecuwaySSL U_카달로그.pdfKRVPN catalogPKI/VPNOCR targetphrack-apt-down-the-north-korea-files.pdfENPhrack articleReferenceBackground on Kimsuky dumpMuddled Libra Threat Assessment.pdfENThreat intel reportReferenceComparative threat actor studyLeaked North Korean Linux Stealth Rootkit Analysis.pdfENRootkit analysisReferenceDetailed implant studyInside the Kimsuky Leak.docx (various)ENThreat report draftsReportWorking versionsaccount (2).txtENDB export (DBsafer, TrustedOrange)InfraAccounts and DB changesresult.txtKRCert-related parsed dataInfraIncluded GPKI .key/.sigenglish_wikipedia.txtENWikipedia dumpReferenceUnrelated baselinebookmarks-2021-01-04.jsonlz4ENFirefox bookmarks (compressed)BrowserNeeds decompressionScreenshot translationsZHChinese text (rootkit marketing blurb)RootkitKernel hiding tool description




                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Historical Housing Prices Project]]></title>
            <link>https://www.philadelphiafed.org/surveys-and-data/regional-economic-analysis/historical-housing-prices</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152063</guid>
            <description><![CDATA[The Historical Housing Prices (HHP) Project at the Philadelphia Fed provides new data on the price of housing for sale and for rent over the 20th century using the real estate sections of historical newspapers. The data cover 30 major cities, including Philadelphia.]]></description>
            <content:encoded><![CDATA[
        
            

    
    
    
    
    
        
                
                    
                    About Us
                    
                
                
                    
                    Our People
                    
                
                
                    
                    Education 
                    
                
                
                    
                    Banking
                    
                
                
                    
                    Careers
                    
                
                
                    
                    Calendar of Events
                    
                
        
        
                
                    
                    The Economy
                    
                    
                            
                                    
                                        
                                        Monetary Policy
                                        
                                    
                                    
                                        
                                        Banking & Financial Markets
                                        
                                    
                                    
                                        
                                        Macroeconomics
                                        
                                    
                                    
                                        
                                        Regional Economics
                                        
                                    
                            
                
                
                    
                    Consumer Finance
                    
                    
                            
                                    
                                        
                                        Payment Systems
                                        
                                    
                                    
                                        
                                        Consumer Credit
                                        
                                    
                                    
                                        
                                        Education Finance
                                        
                                    
                                    
                                        
                                        Mortgage Markets
                                        
                                    
                            
                
                
                    
                    Community Development
                    
                    
                            
                
                
                    
                    SURVEYS & DATA
                    
                    
                            
                                    
                                        
                                        REAL-TIME DATA RESEARCH
                                        
                                    
                                    
                                        
                                        Regional Economic Analysis
                                        
                                    
                                    
                                        
                                        CONSUMER FINANCE DATA
                                        
                                    
                                    
                                        
                                        COMMUNITY DEVELOPMENT DATA
                                        
                                    
                            
                
            
                
            
        
    






        

        

            

            
        
        Housing is central to the economy of cities across the United States. The Historical Housing Prices (HHP) Project at the Philadelphia Fed provides new data on the price of housing for sale and for rent over the 20th century using the real estate sections of historical newspapers. The data cover 30 major cities, including Philadelphia.
The data set has been expanded and new series on the rental return, capital gains, and total return to owning housing are available for download.
This data page is part of our Center for the REstoration of Economic Data (CREED).
    
    Data Explorer

    

        
    
    
            
                
                    Time Series
                
            
            
                
                    Percent Change
                
            
    
    


        
        

        
        

    
        
            Slide the circles to compare the total change in housing price indices between the two years you select.
            
            
            The total data set spans 1890–2006, but data for some cities start after 1890.
        

        
            Slide the circles to see changes in housing prices over time, compared with the earliest year you select.
            
            
            The total data set spans 1890–2006, but data for some cities start after 1890.
        

        
    
    


    





    Data Downloads

    
        
            Type
            Title
            Description
        

            
                

                
                    City-Level Indices
                

                This file contains the HHP national housing price and returns series for both housing for sale and for rent.
            
            
                

                
                    National Indices
                

                This file contains the HHP national housing price and returns series for both housing for sale and for rent. 
            
    



    

        Related Content

    

    
            
                    Consumer Finance Data
                
                        Data

                    
                

                As part of the Philadelphia Fed’s efforts to understand how housing affects the economy in our District, our researchers are studying the links between past housing discrimination and present-day outcomes.
            
            
                    Consumer Finance
                
                        About

                    
                

                Welcome to CREED (Center for the REstoration of Economic Data). CREED advances research in many fields, including topics in regional economics and consumer finance, by converting information in books, images, and other formats into ready-to-use, publicly available digital data.
            
    



        About the Research
        
                Housing impacts individual economic well-being, influences wealth-building opportunities, and is vital to the overall economy. Despite the critical importance of housing to the U.S. economy, existing long-run housing price series are limited, particularly covering the years before 1970.
The Historical Housing Prices (HHP) Project, now housed at the Philadelphia Fed, began with support from the National Science Foundation (SES-1918554), the Lincoln Institute of Land Policy, and Trinity College in Dublin, with principal investigators Allison Shertzer (now at the Philadelphia Fed), Ronan C. Lyons (Trinity College Dublin), and Rowena Gray (University of California, Merced). (The Philadelphia Fed did not receive funding from external funders in connection with this project.)
As part of our efforts to ensure a strong overall economy, the Philadelphia Fed identifies housing-related issues and informs solutions.  This project aims to bring new data on the price of housing over the long run to inform research and policymaking.

        
    

        Suggested Citation:
            Federal Reserve Bank of Philadelphia.
            Historical Housing Prices Project.
            Accessed Sep 06, 2025,
            https://www.philadelphiafed.org/surveys-and-data/regional-economic-analysis/historical-housing-prices.
        


            

            

        
        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI hype is crashing into reality. Stay calm]]></title>
            <link>https://www.businessinsider.com/ai-hype-crashing-into-reality-iphone-openai-2025-9</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152001</guid>
            <description><![CDATA[Between Nvidia's recent earnings and OpenAI's underwhelming GPT-5, it's clear AI is entering its "meh" era. That's good news for everyone.]]></description>
            <content:encoded><![CDATA[
  
    
  
    
              
              
                  
                    
                    
                  
              
                
                  
                    Getty Images; Tyler Le/BI
                    
              
          

  
    
    
    
    
          
          
          
          
              
              
                  
                    
                    
                  
              
                
                  
                    Getty Images; Tyler Le/BI
                    
              
          
    
    
    
          
            
            
            
            
            
            
            
              
                  2025-09-04T08:17:01Z
                
              
            
            
                
      
          
      
          
          
          
          
          
          
          
          
          
              A market correction. A wake-up call. A great digestion. Call it what you want: AI is going through it.Two things appear to be happening in tandem. Businesses are starting to finally grasp what AI can — and importantly, can't — do to boost their bottom lines. And the sky-high expectations that have been partly inflated and overhyped by AI firms over the past few years are finally coming down to Earth.In short, it's increasingly looking like both the AI doomers and boomers were both wrong. AI's trajectory is starting to look less like a time machine or space elevator and more akin computers, smartphones, televisions: The technology will get better, it will almost certainly change our lives in the fullness of time, but it will more likely do so incrementally — to the point that if AGI (artificial general intelligence) or superintelligence do in fact one day arrive, it might not seem like much of a leap at all.There's perhaps no better example of this happening than OpenAI's latest and long-anticipated model, GPT-5, which was touted with a bang and landed with a shrug. Ahead of launch, OpenAI's Sam Altman said he'd felt "useless" compared to the model's intelligence, even drawing parallels with the Manhattan Project. When it arrived, users apparently felt less intimidated. "The degree of overhyping was too significant," one person wrote. "In the absence of massive gains, all you have is hype," wrote another.But it may be a glimpse at our new reality, where the breakneck speed of AI progress is simply steadying, where progress cannot run on hype alone, and where we will neither experience an overnight white-collar job wipeout nor reach an AI abundance society overnight.Welcome to AI's "meh" era. Stay calm. We've been here before. It'll all be fine. Probably.When the internet revolution took hold in the late 1990s, companies were minting millions overnight with little more than a website and a savvy sales pitch. By the year 2000, the economic reality caught up to the hype, leaving trillions of dollars wiped out overnight. Not familiar? Go ask your parents what happened to Pets.com.
          It's easy to see why talk of a bubble has once again reared its head. Even Altman recently (and in an unusually measured moment from AI's biggest hype man) said he believes the AI market might be in a bubble.Progress has been such that you probably won't even notice the improvements from now on.Carl Benedikt Frey"If you go back to the 1990s when the dotcom bubble burst, there weren't the profits necessarily to back the investments up, but there were tangible productivity gains," says Carl Benedikt Frey, an economist at Oxford. If that sounds eerily familiar, a check on AI now could prevent history from repeating itself.A recent study published by the Massachusetts Institute of Technology further stirred the pot last month, claiming that just 5% of companies it studied have managed to convert the technology into actual revenue — a revelation scary enough to cause a tech stock sell-off, even if the study had a lot of limitations.
            
            
            
          Other evidence suggests AI is starting to have an impact on businesses that are adopting it. A study from Stanford University researchers that analyzed payroll data concluded that AI was killing off entry-level jobs for people aged 22 to 25, and was especially doing so in fields where AI was more likely to replace, rather than augment, labor. Marc Benioff claims AI agents are replacing thousands of Salesforce's support roles, while other companies are boasting about AI automating more of their work.A study of AI's effects on the demand for foreign translators by Frey and fellow Oxford economist Pedro Llanos-Paredes, published earlier this year, concluded that the technology was having a small but provable impact on these jobs."We seem to be seeing reasonable revenue growth for a handful of firms that are pioneering the AI revolution, but we're not seeing it translate into broader economic growth," Frey tells me. "What I find concerning is that we're still not seeing any hint of it in the productivity statistics, and ultimately that's what matters. It doesn't really matter how well AI performs on tests or on some benchmark. What matters is translating that into real economic growth."For the markets, a more modest uptake may be perfectly OK. Evercore ISI strategists predict AI excitement will buoy US stocks a further 20% by the end of 2026. "AI is 'bigger' than the internet," they wrote in a note published this week. "In three years, its effect has touched all parts of society and industry even as adoption only begins to inflect."Last week's Nvidia earnings were a strong indicator of where we're at. The company, which sells the valuable chips on which AI is trained and run, has become something of a bellwether for the entire artificial intelligence boom and counts some of the biggest tech giants as key customers. (Bloomberg estimates that Microsoft spends about 47% of its capital expenditures on Nvidia's chips.) While it beat Wall Street expectations and its own sales records, its stock still dropped, suggesting investors weren't impressed with the figures they saw. Some analysts warn that the companies buying these services from Nvidia aren't yet seeing the returns. One UBS analyst characterized Nvidia's results in a way that may perfectly sum up the new steady-chug-along paradigm of AI: "good enough."All to say, AI seems to have reached its iPhone 4 moment.When Apple's iPhone 4 arrived in 2010, it was nothing short of a smash hit. Onstage in Cupertino, Steve Jobs boasted that Apple had made the thinnest phone in the world with a laundry list of new must-have features: a squared-off design, high-resolution display, a front-facing camera for FaceTime and selfies, and the debut of Apple's custom silicon chip, the A4. It flew off the shelves — despite the antenna fiasco — and further cemented Apple as the king of the smartphone. One could argue that the market has been chasing the iPhone 4's "sandwich glass" design ever since.A lot of this sort of feeling of disappointment is due to unreasonable levels of hype.David KruegerThen things changed: With the exception of a couple of moderate leaps since, the iPhone has been on a more incremental trajectory. Evidence suggests artificial intelligence might be plotting a similar course. Frontier labs have been rolling out a steady flow of updates and mini-leaps, rather than waiting years between generations, and as a result, each new rollout has started feeling evolutionary, incremental."If you're not the leading expert in the field, I think the progress has been such that you probably won't even notice the improvements from now on," says Frey.Last year, the big topic was whether AI labs were seeing diminishing returns when simply trying to throw more data and compute power at the models. That may go some way to explain how GPT-5 landed, but it's not the only factor.Between the launch of GPT-4 in March 2023 and GPT-5 last month, OpenAI rolled out well over a dozen models, each one focusing on specific jobs or incrementally improving another. It's perhaps no wonder, then, that GPT-5 didn't blow our socks off. (In the same conversation in which Altman claimed AI is in a bubble, he also claimed that OpenAI has more advanced models than GPT-5, but can't deploy them because it doesn't have the capacity.)Google's latest frontier model, Gemini 2.5, is also a bridge model, and the launch of GPT-5 may serve as a healthy warning to temper expectations for Gemini 3, which is expected before the year's end.
            
            
            
          "The progress just feels more continuous," says David Krueger, an assistant professor at the University of Montreal who studies AI safety and risks.Krueger still thinks we'll have the occasional "wow" moment, but said he also believes we'll need more breakthroughs on the technical side to reach any level of artificial intelligence that can go toe-to-toe with a human — and he says he doesn't believe we will reach AGI from large language models alone."I think LLMs and more broadly deep learning are probably a big piece of the puzzle. If I had to bet, the biggest one," he says. "But I think we're maybe missing a couple of puzzle pieces."Krueger also lays blame at the feet of certain AI figureheads who have created "unreasonable levels of hype," and who are finally getting a reality check. Altman may be the worst offender, but he's not the only one.In March, Anthropic CEO Dario Amodei predicted that AI would be writing 90% of software developers' code in three to six months. The actual gains appear to be much more modest: During Alphabet's Q1 2025 earnings call, CEO Sundar Pichai said that more than 30% of code written at Google was being generated by AI."I think a lot of this sort of feeling of disappointment is due to unreasonable levels of hype from the companies," said Krueger. As the rubber hits the road and the breakneck speed of AI potentially slows, expectations for the future of AI — and the possible, maybe, one day, arrival of AGI — are finally being put in check.You can see how we got here. In a January interview with Bloomberg, Altman predicted AGI would arrive during Trump's second presidency. Elon Musk once predicted it could be here by the year's end. According to some of the best minds in AI, AGI is often just a "few years away." In truth it feels like we're finally realizing nobody actually knows.Perhaps nobody has had more of a humbling in AI than Apple, which earlier this year axed an iPhone 16 ad that promised several much-hyped new AI features that, as it turned out, were far from ready. When Tim Cook steps onstage next week, don't be surprised if he and other executives strike a more measured tone when talking about AI, as they pull back the curtain on the latest lineup of devices.I hear the new phone will be a little thinner this time.Hugh Langley is a senior correspondent at Business Insider where he writes about Google, tech, and wealth.
          
          
              
                
                  Business Insider's Discourse stories provide perspectives on the day's most pressing issues, informed by analysis, reporting, and expertise.
                  
              
          
          
          
          
          
          
    
    
    
    

    
      
      
    
    
          
          
          
          
            
                
                
                  Discourse
                      
                
                  AI
                      
                
                  Generative AI
                
                
                  More 
            
          
                
          
                  
                  
                    ChatGPT
                          
                  
                    OpenAI
                          
                  
                    Anthropic
                    
          
    
            
            
            
                  Most popular
              
                  
                  
                    
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                      
                    
                    
                    
                     
                        
                                  
                              
                              
                              
                              
                      
                    
                    
                        
                          Business Insider tells the innovative stories you want to know
                        
                    
                    
                    
                          
  

  
  
  

  
  

    

  
  
    
  
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Normalization of deviance (2015)]]></title>
            <link>https://danluu.com/wat/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151661</guid>
            <description><![CDATA[Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples.]]></description>
            <content:encoded><![CDATA[ Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples. There's the company that is perhaps the nicest place I've ever worked, combining the best parts of Valve and Netflix. The people are amazing and you're given near total freedom to do whatever you want. But as a side effect of the culture, they lose perhaps half of new hires in the first year, some voluntarily and some involuntarily. Totally normal, right? Here are a few more anecdotes that were considered totally normal by people in places I've worked. And often not just normal, but laudable. There's the company that's incredibly secretive about infrastructure. For example, there's the team that was afraid that, if they reported bugs to their hardware vendor, the bugs would get fixed and their competitors would be able to use the fixes. Solution: request the firmware and fix bugs themselves! More recently, I know a group of folks outside the company who tried to reproduce the algorithm in the paper the company published earlier this year. The group found that they couldn't reproduce the result, and that the algorithm in the paper resulted in an unusual level of instability; when asked about this, one of the authors responded “well, we have some tweaks that didn't make it into the paper” and declined to share the tweaks, i.e., the company purposely published an unreproducible result to avoid giving away the details, as is normal. This company enforces secrecy by having a strict policy of firing leakers. This is introduced at orientation with examples of people who got fired for leaking (e.g., the guy who leaked that a concert was going to happen inside a particular office), and by announcing firings for leaks at the company all hands. The result of those policies is that I know multiple people who are afraid to forward emails about things like updated info on health insurance to a spouse for fear of forwarding the wrong email and getting fired; instead, they use another computer to retype the email and pass it along, or take photos of the email on their phone. There's the office where I asked one day about the fact that I almost never saw two particular people in the same room together. I was told that they had a feud going back a decade, and that things had actually improved — for years, they literally couldn't be in the same room because one of the two would get too angry and do something regrettable, but things had now cooled to the point where the two could, occasionally, be found in the same wing of the office or even the same room. These weren't just random people, either. They were the two managers of the only two teams in the office. There's the company whose culture is so odd that, when I sat down to write a post about it, I found that I'd not only written more than for any other single post, but more than all other posts combined (which is well over 100k words now, the length of a moderate book). This is the same company where someone recently explained to me how great it is that, instead of using data to make decisions, we use political connections, and that the idea of making decisions based on data is a myth anyway; no one does that. This is also the company where all four of the things they told me to get me to join were false, and the job ended up being the one thing I specifically said I didn't want to do. When I joined this company, my team didn't use version control for months and it was a real fight to get everyone to use version control. Although I won that fight, I lost the fight to get people to run a build, let alone run tests, before checking in, so the build is broken multiple times per day. When I mentioned that I thought this was a problem for our productivity, I was told that it's fine because it affects everyone equally. Since the only thing that mattered was my stack ranked productivity, so I shouldn't care that it impacts the entire team, the fact that it's normal for everyone means that there's no cause for concern. There's the company that created multiple massive initiatives to recruit more women into engineering roles, where women still get rejected in recruiter screens for not being technical enough after being asked questions like "was your experience with algorithms or just coding?". I thought that my referral with a very strong recommendation would have prevented that, but it did not. There's the company where I worked on a four person effort with a multi-hundred million dollar budget and a billion dollar a year impact, where requests for things that cost hundreds of dollars routinely took months or were denied. You might wonder if I've just worked at places that are unusually screwed up. Sure, the companies are generally considered to be ok places to work and two of them are considered to be among the best places to work, but maybe I've just ended up at places that are overrated. But I have the same experience when I hear stories about how other companies work, even places with stellar engineering reputations, except that it's me that's shocked and my conversation partner who thinks their story is normal. There's the companies that use @flaky, which includes the vast majority of Python-using SF Bay area unicorns. If you don't know what this is, this is a library that lets you add a Python annotation to those annoying flaky tests that sometimes pass and sometimes fail. When I asked multiple co-workers and former co-workers from three different companies what they thought this did, they all guessed that it re-runs the test multiple times and reports a failure if any of the runs fail. Close, but not quite. It's technically possible to use @flaky for that, but in practice it's used to re-run the test multiple times and reports a pass if any of the runs pass. The company that created @flaky is effectively a storage infrastructure company, and the library is widely used at its biggest competitor. There's the company with a reputation for having great engineering practices that had 2 9s of reliability last time I checked, for reasons that are entirely predictable from their engineering practices. This is the second thing in a row that can't be deanonymized because multiple companies fit the description. Here, I'm not talking about companies trying to be the next reddit or twitter where it's, apparently, totally fine to have 1 9. I'm talking about companies that sell platforms that other companies rely on, where an outage will cause dependent companies to pause operations for the duration of the outage. Multiple companies that build infrastructure find practices that lead to 2 9s of reliability. As far as I can tell, what happens at a lot these companies is that they started by concentrating almost totally on product growth. That's completely and totally reasonable, because companies are worth approximately zero when they're founded; they don't bother with things that protect them from losses, like good ops practices or actually having security, because there's nothing to lose (well, except for user data when the inevitable security breach happens, and if you talk to security folks at unicorns you'll know that these happen). The result is a culture where people are hyper-focused on growth and ignore risk. That culture tends to stick even after company has grown to be worth well over a billion dollars, and the companies have something to lose. Anyone who comes into one of these companies from Google, Amazon, or another place with solid ops practices is shocked. Often, they try to fix things, and then leave when they can't make a dent. Google probably has the best ops and security practices of any tech company today. It's easy to say that you should take these things as seriously as Google does, but it's instructive to see how they got there. If you look at the codebase, you'll see that various services have names ending in z, as do a curiously large number of variables. I'm told that's because, once upon a time, someone wanted to add monitoring. It wouldn't really be secure to have google.com/somename expose monitoring data, so they added a z. google.com/somenamez. For security. At the company that is now the best in the world at security. They're now so good at security that multiple people I've talked to (all of whom joined after this happened) vehemently deny that this ever happened, even though the reasons they give don't really make sense (e.g., to avoid name collisions) and I have this from sources who were there at the time this happened. Google didn't go from adding z to the end of names to having the world's best security because someone gave a rousing speech or wrote a convincing essay. They did it after getting embarrassed a few times, which gave people who wanted to do things “right” the leverage to fix fundamental process issues. It's the same story at almost every company I know of that has good practices. Microsoft was a joke in the security world for years, until multiple disastrously bad exploits forced them to get serious about security. This makes it sound simple, but if you talk to people who were there at the time, the change was brutal. Despite a mandate from the top, there was vicious political pushback from people whose position was that the company got to where it was in 2003 without wasting time on practices like security. Why change what's worked? You can see this kind of thing in every industry. A classic example that tech folks often bring up is hand-washing by doctors and nurses. It's well known that germs exist, and that washing hands properly very strongly reduces the odds of transmitting germs and thereby significantly reduces hospital mortality rates. Despite that, trained doctors and nurses still often don't do it. Interventions are required. Signs reminding people to wash their hands save lives. But when people stand at hand-washing stations to require others walking by to wash their hands, even more lives are saved. People can ignore signs, but they can't ignore being forced to wash their hands. This mirrors a number of attempts at tech companies to introduce better practices. If you tell people they should do it, that helps a bit. If you enforce better practices via code review, that helps a lot. The data are clear that humans are really bad at taking the time to do things that are well understood to incontrovertibly reduce the risk of rare but catastrophic events. We will rationalize that taking shortcuts is the right, reasonable thing to do. There's a term for this: the normalization of deviance. It's well studied in a number of other contexts including healthcare, aviation, mechanical engineering, aerospace engineering, and civil engineering, but we don't see it discussed in the context of software. In fact, I've never seen the term used in the context of software. Is it possible to learn from other's mistakes instead of making every mistake ourselves? The state of the industry make this sound unlikely, but let's give it a shot. John Banja has a nice summary paper on the normalization of deviance in healthcare, with lessons we can attempt to apply to software development. One thing to note is that, because Banja is concerned with patient outcomes, there's a close analogy to devops failure modes, but normalization of deviance also occurs in cultural contexts that are less directly analogous. The first section of the paper details a number of disasters, both in healthcare and elsewhere. Here's one typical example:  A catastrophic negligence case that the author participated in as an expert witness involved an anesthesiologist's turning off a ventilator at the request of a surgeon who wanted to take an x-ray of the patient's abdomen (Banja, 2005, pp. 87-101). The ventilator was to be off for only a few seconds, but the anesthesiologist forgot to turn it back on, or thought he turned it back on but had not. The patient was without oxygen for a long enough time to cause her to experience global anoxia, which plunged her into a vegetative state. She never recovered, was disconnected from artificial ventilation 9 days later, and then died 2 days after that. It was later discovered that the anesthesia alarms and monitoring equipment in the operating room had been deliberately programmed to a “suspend indefinite” mode such that the anesthesiologist was not alerted to the ventilator problem. Tragically, the very instrumentality that was in place to prevent such a horror was disabled, possibly because the operating room staff found the constant beeping irritating and annoying.  Turning off or ignoring notifications because there are too many of them and they're too annoying? An erroneous manual operation? This could be straight out of the post-mortem of more than a few companies I can think of, except that the result was a tragic death instead of the loss of millions of dollars. If you read a lot of tech post-mortems, every example in Banja's paper will feel familiar even though the details are different. The section concludes,  What these disasters typically reveal is that the factors accounting for them usually had “long incubation periods, typified by rule violations, discrepant events that accumulated unnoticed, and cultural beliefs about hazards that together prevented interventions that might have staved off harmful outcomes”. Furthermore, it is especially striking how multiple rule violations and lapses can coalesce so as to enable a disaster's occurrence.  Once again, this could be from an article about technical failures. That makes the next section, on why these failures happen, seem worth checking out. The reasons given are: The rules are stupid and inefficient The example in the paper is about delivering medication to newborns. To prevent “drug diversion,” nurses were required to enter their password onto the computer to access the medication drawer, get the medication, and administer the correct amount. In order to ensure that the first nurse wasn't stealing drugs, if any drug remained, another nurse was supposed to observe the process, and then enter their password onto the computer to indicate they witnessed the drug being properly disposed of. That sounds familiar. How many technical postmortems start off with “someone skipped some steps because they're inefficient”, e.g., “the programmer force pushed a bad config or bad code because they were sure nothing could go wrong and skipped staging/testing”? The infamous November 2014 Azure outage happened for just that reason. At around the same time, a dev at one of Azure's competitors overrode the rule that you shouldn't push a config that fails tests because they knew that the config couldn't possibly be bad. When that caused the canary deploy to start failing, they overrode the rule that you can't deploy from canary into staging with a failure because they knew their config couldn't possibly be bad and so the failure must be from something else. That postmortem revealed that the config was technically correct, but exposed a bug in the underlying software; it was pure luck that the latent bug the config revealed wasn't as severe as the Azure bug. Humans are bad at reasoning about how failures cascade, so we implement bright line rules about when it's safe to deploy. But the same thing that makes it hard for us to reason about when it's safe to deploy makes the rules seem stupid and inefficient. Knowledge is imperfect and uneven People don't automatically know what should be normal, and when new people are onboarded, they can just as easily learn deviant processes that have become normalized as reasonable processes. Julia Evans described to me how this happens: new person joins new person: WTF WTF WTF WTF WTF old hands: yeah we know we're concerned about it new person: WTF WTF wTF wtf wtf w... new person gets used to it new person #2 joins new person #2: WTF WTF WTF WTF new person: yeah we know. we're concerned about it. The thing that's really insidious here is that people will really buy into the WTF idea, and they can spread it elsewhere for the duration of their career. Once, after doing some work on an open source project that's regularly broken and being told that it's normal to have a broken build, and that they were doing better than average, I ran the numbers, found that project was basically worst in class, and wrote something about the idea that it's possible to have a build that nearly always passes with relatively low effort. The most common comment I got in response was, "Wow that guy must work with superstar programmers. But let's get real. We all break the build at least a few times a week", as if running tests (or for that matter, even attempting to compile) before checking code in requires superhuman abilities. But once people get convinced that some deviation is normal, they often get really invested in the idea. I'm breaking the rule for the good of my patient The example in the paper is of someone who breaks the rule that you should wear gloves when finding a vein. Their reasoning is that wearing gloves makes it harder to find a vein, which may result in their having to stick a baby with a needle multiple times. It's hard to argue against that. No one wants to cause a baby extra pain! The second worst outage I can think of occurred when someone noticed that a database service was experiencing slowness. They pushed a fix to the service, and in order to prevent the service degradation from spreading, they ignored the rule that you should do a proper, slow, staged deploy. Instead, they pushed the fix to all machines. It's hard to argue against that. No one wants their customers to have degraded service! Unfortunately, the fix exposed a bug that caused a global outage. The rules don't apply to me/You can trust me  most human beings perceive themselves as good and decent people, such that they can understand many of their rule violations as entirely rational and ethically acceptable responses to problematic situations. They understand themselves to be doing nothing wrong, and will be outraged and often fiercely defend themselves when confronted with evidence to the contrary.  As companies grow up, they eventually have to impose security that prevents every employee from being able to access basically everything. And at most companies, when that happens, some people get really upset. “Don't you trust me? If you trust me, how come you're revoking my access to X, Y, and Z?” Facebook famously let all employees access everyone's profile for a long time, and you can even find HN comments indicating that some recruiters would explicitly mention that as a perk of working for Facebook. And I can think of more than one well-regarded unicorn where everyone still has access to basically everything, even after their first or second bad security breach. It's hard to get the political capital to restrict people's access to what they believe they need, or are entitled, to know. A lot of trendy startups have core values like “trust” and “transparency” which make it difficult to argue against universal access. Workers are afraid to speak up There are people I simply don't give feedback to because I can't tell if they'd take it well or not, and once you say something, it's impossible to un-say it. In the paper, the author gives an example of a doctor with poor handwriting who gets mean when people ask him to clarify what he's written. As a result, people guess instead of asking. In most company cultures, people feel weird about giving feedback. Everyone has stories about a project that lingered on for months or years after it should have been terminated because no one was willing to offer explicit feedback. This is a problem even when cultures discourage meanness and encourage feedback: cultures of niceness seem to have as many issues around speaking up as cultures of meanness, if not more. In some places, people are afraid to speak up because they'll get attacked by someone mean. In others, they're afraid because they'll be branded as mean. It's a hard problem. Leadership withholding or diluting findings on problems In the paper, this is characterized by flaws and weaknesses being diluted as information flows up the chain of command. One example is how a supervisor might take sub-optimal actions to avoid looking bad to superiors. I was shocked the first time I saw this happen. I must have been half a year or a year out of school. I saw that we were doing something obviously non-optimal, and brought it up with the senior person in the group. He told me that he didn't disagree, but that if we did it my way and there was a failure, it would be really embarrassing. He acknowledged that my way reduced the chance of failure without making the technical consequences of failure worse, but it was more important that we not be embarrassed. Now that I've been working for a decade, I have a better understanding of how and why people play this game, but I still find it absurd. Solutions Let's say you notice that your company has a problem that I've heard people at most companies complain about: people get promoted for heroism and putting out fires, not for preventing fires; and people get promoted for shipping features, not for doing critical maintenance work and bug fixing. How do you change that? The simplest option is to just do the right thing yourself and ignore what's going on around you. That has some positive impact, but the scope of your impact is necessarily limited. Next, you can convince your team to do the right thing: I've done that a few times for practices I feel are really important and are sticky, so that I won't have to continue to expend effort on convincing people once things get moving. But if the incentives are aligned against you, it will require an ongoing and probably unsustainable effort to keep people doing the right thing. In that case, the problem becomes convincing someone to change the incentives, and then making sure the change works as designed. How to convince people is worth discussing, but long and messy enough that it's beyond the scope of this post. As for making the change work, I've seen many “obvious” mistakes repeated, both in places I've worked and those whose internal politics I know a lot about. Small companies have it easy. When I worked at a 100 person company, the hierarchy was individual contributor (IC) -> team lead (TL) -> CEO. That was it. The CEO had a very light touch, but if he wanted something to happen, it happened. Critically, he had a good idea of what everyone was up to and could basically adjust rewards in real-time. If you did something great for the company, there's a good chance you'd get a raise. Not in nine months when the next performance review cycle came up, but basically immediately. Not all small companies do that effectively, but with the right leadership, they can. That's impossible for large companies. At large company A (LCA), they had the problem we're discussing and a mandate came down to reward people better for doing critical but low-visibility grunt work. There were too many employees for the mandator to directly make all decisions about compensation and promotion, but the mandator could review survey data, spot check decisions, and provide feedback until things were normalized. My subjective perception is that the company never managed to achieve parity between boring maintenance work and shiny new projects, but got close enough that people who wanted to make sure things worked correctly didn't have to significantly damage their careers to do it. At large company B (LCB), ICs agreed that it's problematic to reward creating new features more richly than doing critical grunt work. When I talked to managers, they often agreed, too. But nevertheless, the people who get promoted are disproportionately those who ship shiny new things. I saw management attempt a number of cultural and process changes at LCB. Mostly, those took the form of pronouncements from people with fancy titles. For really important things, they might produce a video, and enforce compliance by making people take a multiple choice quiz after watching the video. The net effect I observed among other ICs was that people talked about how disconnected management was from the day-to-day life of ICs. But, for the same reasons that normalization of deviance occurs, that information seems to have no way to reach upper management. It's sort of funny that this ends up being a problem about incentives. As an industry, we spend a lot of time thinking about how to incentivize consumers into doing what we want. But then we set up incentive systems that are generally agreed upon as incentivizing us to do the wrong things, and we do so via a combination of a game of telephone and cargo cult diffusion. Back when Microsoft was ascendant, we copied their interview process and asked brain-teaser interview questions. Now that Google is ascendant, we copy their interview process and ask algorithms questions. If you look around at trendy companies that are younger than Google, most of them basically copy their ranking/leveling system, with some minor tweaks. The good news is that, unlike many companies people previously copied, Google has put a lot of thought into most of their processes and made data driven decisions. The bad news is that Google is unique in a number of ways, which means that their reasoning often doesn't generalize, and that people often cargo cult practices long after they've become deprecated at Google. This kind of diffusion happens for technical decisions, too. Stripe built a reliable message queue on top of Mongo, so we build reliable message queues on top of Mongo1. It's cargo cults all the way down2. The paper has specific sub-sections on how to prevent normalization of deviance, which I recommend reading in full.  Pay attention to weak signals Resist the urge to be unreasonably optimistic Teach employees how to conduct emotionally uncomfortable conversations System operators need to feel safe in speaking up Realize that oversight and monitoring are never-ending  Let's look at how the first one of these, “pay attention to weak signals”, interacts with a single example, the “WTF WTF WTF” a new person gives off when the join the company. If a VP decides something is screwed up, people usually listen. It's a strong signal. And when people don't listen, the VP knows what levers to pull to make things happen. But when someone new comes in, they don't know what levers they can pull to make things happen or who they should talk to almost by definition. They give out weak signals that are easily ignored. By the time they learn enough about the system to give out strong signals, they've acclimated. “Pay attention to weak signals” sure sounds like good advice, but how do we do it? Strong signals are few and far between, making them easy to pay attention to. Weak signals are abundant. How do we filter out the ones that aren't important? And how do we get an entire team or org to actually do it? These kinds of questions can't be answered in a generic way; this takes real thought. We mostly put this thought elsewhere. Startups spend a lot of time thinking about growth, and while they'll all tell you that they care a lot about engineering culture, revealed preference shows that they don't. With a few exceptions, big companies aren't much different. At LCB, I looked through the competitive analysis slide decks and they're amazing. They look at every last detail on hundreds of products to make sure that everything is as nice for users as possible, from onboarding to interop with competing products. If there's any single screen where things are more complex or confusing than any competitor's, people get upset and try to fix it. It's quite impressive. And then when LCB onboards employees in my org, a third of them are missing at least one of, an alias/account, an office, or a computer, a condition which can persist for weeks or months. The competitive analysis slide decks talk about how important onboarding is because you only get one chance to make a first impression, and then employees are onboarded with the impression that the company couldn't care less about them and that it's normal for quotidian processes to be pervasively broken. LCB can't even to get the basics of employee onboarding right, let alone really complex things like acculturation. This is understandable — external metrics like user growth or attrition are measurable, and targets like how to tell if you're acculturating people so that they don't ignore weak signals are softer and harder to determine, but that doesn't mean they're any less important. People write a lot about how things like using fancier languages or techniques like TDD or agile will make your teams more productive, but having a strong engineering culture is much larger force multiplier.  Thanks to Sophie Smithburg and Marc Brooker for introducing me to the term Normalization of Deviance, and Kelly Eskridge, Leah Hanson, Sophie Rapoport, Sophie Smithburg, Julia Evans, Dmitri Kalintsev, Ralph Corderoy, Jamie Brandon, Egor Neliuba, and Victor Felder for comments/corrections/discussion.      People seem to think I'm joking here. I can understand why, but try Googling mongodb message queue. You'll find statements like “replica sets in MongoDB work extremely well to allow automatic failover and redundancy”. Basically every company I know of that's done this and has anything resembling scale finds this to be non-optimal, to say the least, but you can't actually find blog posts or talks that discuss that. All you see are the posts and talks from when they first tried it and are in the honeymoon period. This is common with many technologies. You'll mostly find glowing recommendations in public even when, in private, people will tell you about all the problems. Today, if you do the search mentioned above, you'll get a ton of posts talking about how amazing it is to build a message queue on top of Mongo, this footnote, and a maybe couple of blog posts by Kyle Kingsbury depending on your exact search terms. If there were an acute failure, you might see a postmortem, but while we'll do postmortems for "the site was down for 30 seconds", we rarely do postmortems for "this takes 10x as much ops effort as the alternative and it's a death by a thousand papercuts", "we architected this thing poorly and now it's very difficult to make changes that ought to be trivial", or "a competitor of ours was able to accomplish the same thing with an order of magnitude less effort". I'll sometimes do informal postmortems by asking everyone involved oblique questions about what happened, but more for my own benefit than anything else, because I'm not sure people really want to hear the whole truth. This is especially sensitive if the effort has generated a round of promotions, which seems to be more common the more screwed up the project. The larger the project, the more visibility and promotions, even if the project could have been done with much less effort. [return] I've spent a lot of time asking about why things are the way they are, both in areas where things are working well, and in areas where things are going badly. Where things are going badly, everyone has ideas. But where things are going well, as in the small company with the light-touch CEO mentioned above, almost no one has any idea why things work. It's magic. If you ask, people will literally tell you that it seems really similar to some other place they've worked, except that things are magically good instead of being terrible for reasons they don't understand. But it's not magic. It's hard work that very few people understand. Something I've seen multiple times is that, when a VP leaves, a company will become a substantially worse place to work, and it will slowly dawn on people that the VP was doing an amazing job at supporting not only their direct reports, but making sure that everyone under them was having a good time. It's hard to see until it changes, but if you don't see anything obviously wrong, either you're not paying attention or someone or many someones have put a lot of work into making sure things run smoothly. [return]   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GigaByte CXL memory expansion card with up to 512GB DRAM]]></title>
            <link>https://www.gigabyte.com/PC-Accessory/AI-TOP-CXL-R5X4</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151598</guid>
            <description><![CDATA[Expand Memory Pool 16-Layers HDI PCB Equipped with an AIO Fan DDR5 RDIMM Support Support PCle 5.0 Lower Total Cost of Ownership (TCO)]]></description>
            <content:encoded><![CDATA[


            
            
    
    
    




    








    
			
                
                            
                                
                                     FEATURES 
                                    
                                         Expand Memory Pool 
                                         16-Layers HDI PCB 
                                         Equipped with an AIO Fan 
                                         DDR5 RDIMM Support 
                                         Support PCle 5.0 
                                         Lower Total Cost of Ownership (TCO) 
                                    
                                
                                
                                    
                                    
                                
                            
                            
                            
                        
                
                                    
                                        
                                            
                                            
                                        
                                        
                                                 GIGABYTE AI TOP 
                                                 Train your own AI on your desk. 
                                            
                                    
                                    
                                        
                                            
                                            
                                        
                                        
                                                 Ultimate scalability and Connectivity​ 
                                                 Brings you the future-proofing capability. 
                                            
                                    
                                
                
                    
                    
                        
                            
                                     GIGABYTE AI TOP 
                                     Train your own AI on your desk 
                                     In the age of local AI, GIGABYTE AI TOP is the all-round solution to win advantages ahead of traditional AI training methods. It features a variety of groundbreaking technologies that can be easily adapted by beginners or experts, for most common open-source LLMs, in anyplace even on your desk. 
                                
                            
                                
                                    Supports 236B LLM Local Training
                                
                                
                                    Intuitive Set-up
                                
                                
                                    Flexbility & Upgradability
                                
                                
                                    Privacy & Security
                                
                                
                                    Suitable for Home Use
                                
                            
                        
                        
                            
                                    
                                        
                                    
                                    
                                         AI TOP Utility 2.0 
                                         Reinventing AI Training 
                                    
                                     The all-new AI TOP Utility 2.0 brings enhanced features and broader model support, empowering users from novices to experts to unlock AI's potential right at their desks. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                
                            
                                    
                                         AI TOP Hardware 
                                         Enhanced Performance for AI Training 
                                    
                                     The AI TOP Hardware features a series of GIGABYTE AI TOP products that are optimized in power efficiency and durability for AI training workloads. It includes upgradeable components and is easy to build at home. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                     * Images for reference only, system configuration will vary by model. 
                                
                        
                    
                
                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultimate scalability 
                                                                 Exclusive Memory Offloading Solution 
                                                                 Expand training capacity by offloading data to system memory and SSDs, enabling efficient training of Big models through optimized memory management. 
                                                            
                                                            
                                                            
                                                                                         Offers diverse memory expandability with support for DDR5 ECC Registered memory modules (RDIMM): 
                                                                                        
                                                                                             - 4 x DDR5 DIMM sockets, supporting up to 512 GB of memory (up to 128 GB per DIMM) 
                                                                                        
                                                                                         Provides users with High flexibility and options suitable for various requirement scenarios. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Future Connectivity 
                                                                 Express to PCIE5 Performance 
                                                                 The new AI TOP CXL PCI Express card provides the most efficient solution to enjoy boosted performance of PCIe Gen5 on the existing platform. PCIe 5.0 doubles the data transfer rate of PCIe 4.0 from 16 GT/s to 32 GT/s per lane. This substantial increase in bandwidth particularly benefits high-performance AIO cards, storage devices, and AI accelerators that demand faster data movement. 
                                                            
                                                            
                                                                                         Benefits of CXL Memory: 
                                                                                        
                                                                                            
                                                                                                 *Overall lower total cost of ownership (TCO)* 
                                                                                                 -Improve computational and memory resource utilization for specific applications, thereby reducing capital expenditure and operating costs. 
                                                                                            
                                                                                            
                                                                                                 *Breaking Memory Bottlenecks:* 
                                                                                                 - Traditional server memory capacity limits the development of complex applications like AI. CXL memory provides a way to break through this barrier, allowing servers to expand memory capacity and meet the demands of increasingly complex AI applications. 
                                                                                            
                                                                                            
                                                                                                 *Improving Memory Utilization Efficiency:* 
                                                                                                 - CXL memory enables efficient use of memory resources through cross-device memory sharing and memory pooling technologies, thereby increasing memory utilization rates. 
                                                                                            
                                                                                            
                                                                                                 *Accelerating Computation Speed:* 
                                                                                                 - CXL memory allows the CPU to access memory on accelerators, achieving cross-device memory sharing and heterogeneous computing, which accelerates computation speed. 
                                                                                            
                                                                                            
                                                                                                 *Supporting Memory Consistency:* 
                                                                                                 - The CXL protocol (such as CXL.io, CXL.cache, CXL.mem) ensures consistency in memory operations across devices, avoiding data inconsistency issues. 
                                                                                            
                                                                                            
                                                                                                 *Flexible Scaling and Sharing:* 
                                                                                                 - CXL memory supports many-to-many flexible connections, allowing for many-to-many connections between accelerators and CPUs, achieving flexible scaling and memory sharing. 
                                                                                            
                                                                                        
                                                                                        
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Superior thermal Design 
                                                                 Comprehensive Thermal 
                                                                 Advanced full-metal thermal design and durable heatsinks to keep your system cool and efficient. 
                                                            
                                                            
                                                                                         AIO Fan Thermal Design 
                                                                                         High-efficiency fan design has achieved a simultaneous increase in both airflow and pressure, it helps to quickly dissipate heat and stabilize output performance. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultra Durable 
                                                                 Born from Ultra Durable™ 
                                                                 Technology embodies our commitment to excellence, providing gamers with a platform that's not only powerful but also built for longevity and reliability. AI TOP series Product are engineered to endure and excel. 
                                                            
                                                            
                                                                                    
                                                                                         Long Lifespan Durable™ Solid Caps 
                                                                                         GIGABYTE motherboards integrate the absolute best quality solid state capacitors that are rated to perform at maximum efficiency for extended periods, even in extreme performance configurations. With ultra-low ESR no matter how high the load, this provides peace of mind for end users who want to push their system hard, yet demand absolute reliability and stability. These exclusive capacitors also come in customized jet, exclusively on GIGABYTE product. 
                                                                                        
                                                                                    
                                                                                    
                                                                                         Humidity Protection with New Glass Fabric PCB 
                                                                                         There is nothing more harmful to the longevity of your PC than moisture, and most parts of the world experience moisture in the air as humidity at some point during the year. GIGABYTE motherboards have been designed to make sure that humidity is never an issue, incorporating a new Glass Fabric PCB technology that repels moisture caused by humid and damp conditions.  Glass Fabric PCB technology uses a new PCB material which reduces the amount of space between the fiber weave, making it much more difficult for moisture to penetrate compared to traditional motherboard PCBs. This offers much better protection from short circuit and system malfunction caused by humid and damp conditions. 
                                                                                        
                                                                                    
                                                                                
                                                        
                                                
                                        
                
                    
                        
                    
                
                
                         * This product requires compatible AI TOP hardware to be supported. A single CXL card cannot properly activate the AI TOP Utility. We recommend consulting with our Sales representative or authorized dealer contact before purchasing this product. 
                    
            
	

		* Product specifications and product appearance may differ from country to country. We recommend that you check with your local dealers for the specifications and appearance of the products available in your country. Colors of products may not be perfectly accurate due to variations caused by photographic variables and monitor settings so it may vary from images shown on this site. Although we endeavor to present the most accurate and comprehensive information at the time of publication, we reserve the right to make changes without prior notice.
	







                



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Claude Code SDK to reduce E2E test time]]></title>
            <link>https://jampauchoa.substack.com/p/best-of-both-worlds-using-claude</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151447</guid>
        </item>
        <item>
            <title><![CDATA[Patterns, Predictions, and Actions – A story about machine learning]]></title>
            <link>https://mlstory.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45150820</guid>
            <description><![CDATA[Image copyright: Princeton University Press]]></description>
            <content:encoded><![CDATA[
 
Image copyright: Princeton University Press
Hardcover
Princeton
University Press
Errata

Full preprint as
PDF
Problem sets (pdf)


Table of contents
Preface Acknowledgments

Introduction (PDF)
Fundamentals of prediction (PDF)
Supervised learning (PDF)
Representations and features (PDF)
Optimization (PDF)
Generalization (PDF)
Deep learning (PDF)
Datasets (PDF)
Causality (PDF)
Causal inference in practice (PDF)
Sequential decision making and dynamic
programming (PDF)
Reinforcement learning (PDF)
Epilogue (PDF)
Mathematical background (PDF)



Contact us
We welcome your feedback, questions, and suggestions. You can reach
us at contact@mlstory.org. If you taught from the book,
we’d love to hear about it.


Citations, license, typesetting
Please cite the print edition of this book as:
@book{hardtrecht2022patterns,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: Foundations of machine learning},
  year = {2022},
  publisher = {Princeton University Press}
}

We maintain an archival version of the book at arXiv:2102.05242. The web
version is more up-to-date than the arXiv version. The print version
contains additional improvements and editing not present in the web
version.
The text available on this website is licensed under the Creative
Commons BY-NC-ND 4.0 license.
This book is typeset using pandoc with the unbuch setup.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oldest recorded transaction]]></title>
            <link>https://avi.im/blag/2025/oldest-txn/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149626</guid>
            <description><![CDATA[The oldest recorded transaction was in 3100 BC]]></description>
            <content:encoded><![CDATA[The other day I posted a tweet with this image which I thought was funny:This is the oldest transaction database from 3100 BC - recording accounts of malt and barley groats. Considering this thing survived 5000 years (holy shit!) with zero downtime and has stronger durability guarantees than most databases today.I call it rock solid durability.This got me thinking, can I insert this date in today’s database? What is the oldest timestamp a database can support?So I checked the top three databases: MySQL, Postgres, and SQLite:MySQL1000 ADPostgres4713 BCSQLite4713 BCToo bad you cannot use MySQL for this. Postgres and SQLite support the Julian calendar and the lowest date is Jan 01, 4713 BC:sales=# INSERT INTO orders VALUES ('4713-01-01 BC'::date);
INSERT 0 1
sales=# SELECT * FROM orders;
   timestamp
---------------
 4713-01-01 BC
(1 row)
sales=# INSERT INTO orders VALUES ('4714-01-01 BC'::date);
ERROR:  date out of range: "4714-01-01 BC"
I wonder how people store dates older than this. Maybe if I’m a British Museum manager, and I want to keep theft inventory details. How do I do it? As an epoch? Store it as text? Use some custom system? How do I get it to support all the custom operations that a typical TIMESTAMP supports?Thanks to aku, happy_shady, Mr. Bhat, and General Bruh for reading an early draft of this post.1. Source of the image: Sumer civilization2. I found this from the talk 1000x: The Power of an Interface for Performance by
Joran Dirk Greef, CEO of TigerBeetle, timestamped @ 38:10.3. The talk has other bangers too, like this or this.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI surveillance should be banned while there is still time]]></title>
            <link>https://gabrielweinberg.com/p/ai-surveillance-should-be-banned</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149281</guid>
            <description><![CDATA[All the same privacy harms with online tracking are also present with AI, but worse.]]></description>
            <content:encoded><![CDATA[Original cartoon by Dominique Lizaambard (left), updated for AI, by AI (right).All the same privacy harms with online tracking are also present with AI, but worse.While chatbot conversations resemble longer search queries, chatbot privacy harms have the potential to be significantly worse because the inference potential is dramatically greater. Longer input invites more personal information to be provided, and people are starting to bare their souls to chatbots. The conversational format can make it feel like you’re talking to a friend, a professional, or even a therapist. While search queries reveal interests and personal problems, AI conversations take their specificity to another level and, in addition, reveal thought processes and communication styles, creating a much more comprehensive profile of your personality. This richer personal information can be more thoroughly exploited for manipulation, both commercially and ideologically, for example, through behavioral chatbot advertising and models designed (or themselves manipulated through SEO or hidden system prompts) to nudge you towards a political position or product. Chatbots have already been found to be more persuasive than humans and have caused people to go into delusional spirals as a result. I suspect we’re just scratching the surface, since they can become significantly more attuned to your particular persuasive triggers through chatbot memory features, where they train and fine-tune based on your past conversations, making the influence much more subtle. Instead of an annoying and obvious ad following you around everywhere, you can have a seemingly convincing argument, tailored to your personal style, with an improperly sourced “fact” that you’re unlikely to fact-check or a subtle product recommendation you’re likely to heed. That is, all the privacy debates surrounding Google search results from the past two decades apply one-for-one to AI chats, but to an even greater degree. That’s why we (at DuckDuckGo) started offering Duck.ai for protected chatbot conversations and optional, anonymous AI-assisted answers in our private search engine. In doing so, we’re demonstrating that privacy-respecting AI services are feasible. But unfortunately, such protected chats are not yet standard practice, and privacy mishaps are mounting quickly. Grok leaked hundreds of thousands of chatbot conversations that users thought were private. Perplexity’s AI agent was shown to be vulnerable to hackers who could slurp up your personal information. Open AI is openly talking about their vision for a “super assistant” that tracks everything you do and say (including offline). And Anthropic is going to start training on your chatbot conversations by default (previously the default was off). I collected these from just the past few weeks!It would therefore be ideal if Congress could act quickly to ensure that protected chats become the rule rather than the exception. And yet, I’m not holding my breath because it’s 2025 and the U.S. still doesn’t have a general online privacy law, let alone privacy enshrined in the Constitution as a fundamental right, as it should be. However, there does appear to be an opening right now for AI-specific federal legislation, despite the misguided attempts to ban state AI legislation.Time is running out because every day that passes further entrenches bad privacy practices. Congress must move before history completely repeats itself and everything that happened with online tracking happens again with AI tracking. AI surveillance should be banned while there is still time. No matter what happens, though, we will still be here, offering protected services, including optional AI services, to consumers who want to reap the productivity benefits of online tools without the privacy harms. Share]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We hacked Burger King: How auth bypass led to drive-thru audio surveillance]]></title>
            <link>https://bobdahacker.com/blog/rbi-hacked-drive-thrus/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148944</guid>
        </item>
        <item>
            <title><![CDATA[Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5]]></title>
            <link>https://github.com/b4rtaz/distributed-llama/discussions/255</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148237</guid>
            <description><![CDATA[qwen3_30b.mov Setup [🔀 TP-Link LS1008G Switch] | | | | | | | |_______ 🔸 raspberrypi2 (ROOT) 10.0.0.2 | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1 | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0....]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    




  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  




    

      






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    168

  

  
        
            
          Star
          2.5k

  



        

        


          

  
    


  

  




    

        
  


            
    
      
    
  
        
  
    
    qwen3_30b.mov
    
  

  

  


Setup
[🔀 TP-Link LS1008G Switch]
      | | | |
      | | | |_______ 🔸 raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ 🔹 raspberrypi4 (WORKER 3) 10.0.0.4

Device: 4 x Raspberry Pi 5 8GB
Distributed Llama version: 0.16.0
Model: qwen3_30b_a3b_q40
Benchmark




Evaluation
Prediction




4 x Raspberry Pi 5 8GB
14.33 tok/s
13.04 tok/s



b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt "<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant
" --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
📄 AddBos: 0
📄 BosId: 151643 (<|endoftext|>)
📄 EosId: 151645 (<|im_end|>) 
📄 RegularVocabSize: 151643
📄 SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
💡 Arch: Qwen3 MoE
💡 HiddenAct: Silu
💡 Dim: 2048
💡 HeadDim: 128
💡 QDim: 4096
💡 KvDim: 512
💡 HiddenDim: 6144
💡 VocabSize: 151936
💡 nLayers: 48
💡 nHeads: 32
💡 nKvHeads: 4
💡 OrigSeqLen: 262144
💡 nExperts: 128
💡 nActiveExperts: 8
💡 MoeHiddenDim: 768
💡 SeqLen: 4096
💡 NormEpsilon: 0.000001
💡 RopeType: Falcon
💡 RopeTheta: 10000000
📀 RequiredMemory: 5513 MB
⭕ Socket[0]: connecting to 10.0.0.1:9999 worker
⭕ Socket[0]: connected
⭕ Socket[1]: connecting to 10.0.0.3:9999 worker
⭕ Socket[1]: connected
⭕ Socket[2]: connecting to 10.0.0.4:9999 worker
⭕ Socket[2]: connected
⭕ Network is initialized
🧠 CPU: neon dotprod fp16
💿 Loading weights...
💿 Weights loaded
🚁 Network is in non-blocking mode
<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant

🔷️ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
🔶 Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
🔶 Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
🔶 Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
🔶 Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
🔶 Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
🔶 Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
🔶 Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
🔶 Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
🔶 Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
🔶 Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
🔶 Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  –
🔶 Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
🔶 Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
🔶 Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
🔶 Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

🔶 Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
⭕ Network is closed

    
    


          

        

         







  

  

  

  

  

  

  

  


    




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Software Development Methodology for Disciplined LLM Collaboration]]></title>
            <link>https://github.com/Varietyz/Disciplined-AI-Software-Development</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148180</guid>
            <description><![CDATA[This methodology provides a structured approach for collaborating with AI systems on software development projects. It addresses common issues like code bloat, architectural drift, and context dilu...]]></description>
            <content:encoded><![CDATA[

Disciplined AI Software Development - Collaborative
A structured approach for working with AI on development projects. This methodology addresses common issues like code bloat, architectural drift, and context dilution through systematic constraints.
The Context Problem
AI systems work on Question → Answer patterns. When you ask for broad, multi-faceted implementations, you typically get:

Functions that work but lack structure
Repeated code across components
Architectural inconsistency over sessions
Context dilution causing output drift
More debugging time than planning time

How This Works
The methodology uses four stages with systematic constraints and validation checkpoints. Each stage builds on empirical data rather than assumptions.
Planning saves debugging time. Planning thoroughly upfront typically prevents days of fixing architectural issues later.
The Four Stages
Stage 1: AI Configuration
Set up your AI model's custom instructions using AI-PREFERENCES.md. This establishes behavioral constraints and uncertainty flagging with ⚠️ indicators when the AI lacks certainty.
Stage 2: Collaborative Planning
Share METHODOLOGY.md with the AI to structure your project plan. Work together to:

Define scope and completion criteria
Identify components and dependencies
Structure phases based on logical progression
Generate systematic tasks with measurable checkpoints

Output: A development plan following dependency chains with modular boundaries.
Stage 3: Systematic Implementation
Work phase by phase, section by section. Each request follows: "Can you implement [specific component]?" with focused objectives.
File size stays ≤150 lines. This constraint provides:

Smaller context windows for processing
Focused implementation over multi-function attempts
Easier sharing and debugging

Implementation flow:
Request specific component → AI processes → Validate → Benchmark → Continue

Stage 4: Data-Driven Iteration
The benchmarking suite (built first) provides performance data throughout development. Feed this data back to the AI for optimization decisions based on measurements rather than guesswork.
Why This Approach Works
Decision Processing: AI handles "Can you do A?" more reliably than "Can you do A, B, C, D, E, F, G, H?"
Context Management: Small files and bounded problems prevent the AI from juggling multiple concerns simultaneously.
Empirical Validation: Performance data replaces subjective assessment. Decisions come from measurable outcomes.
Systematic Constraints: Architectural checkpoints, file size limits, and dependency gates force consistent behavior.
Example Projects


Discord Bot Template - Production-ready bot foundation with plugin architecture, security, API management, and comprehensive testing. 46 files, all under 150 lines, with benchmarking suite and automated compliance checking. (View Project Structure)


PhiCode Runtime - Programming language runtime engine with transpilation, caching, security validation, and Rust acceleration. Complex system maintaining architectural discipline across 70+ modules. (View Project Structure)


PhiPipe - CI/CD regression detection system with statistical analysis, GitHub integration, and concurrent processing. Go-based service handling performance baselines and automated regression alerts. (View Project Structure)


You can compare the methodology principles to the codebase structure to see how the approach translates to working code.
Implementation Steps
Setup

Configure AI with AI-PREFERENCES.md as custom instructions
Share METHODOLOGY.md for planning session
Collaborate on project structure and phases
Generate systematic development plan

Execution

Build Phase 0 benchmarking infrastructure first
Work through phases sequentially
Implement one component per interaction
Run benchmarks and share results with AI
Validate architectural compliance continuously

Quality Assurance

Performance regression detection
Architectural principle validation
Code duplication auditing
File size compliance checking
Dependency boundary verification

Project State Extraction
Use the included project extraction tool systematically to generate structured snapshots of your codebase:
python scripts/project_extract.py
Configuration Options:

SEPARATE_FILES = False: Single THE_PROJECT.md file (recommended for small codebases)
SEPARATE_FILES = True: Multiple files per directory (recommended for large codebases and focused folder work)
INCLUDE_PATHS: Directories and files to analyze
EXCLUDE_PATTERNS: Skip cache directories, build artifacts, and generated files

Output:

Complete file contents with syntax highlighting
File line counts with architectural warnings (⚠️ for 140-150 lines, ‼️ for >150 lines on code files)
Tree structure visualization
Ready-to-share

output examples can be found here
Use the tool to share a complete or partial project state with the AI system, track architectural compliance, and create focused development context.
What to Expect
AI Behavior: The methodology reduces architectural drift and context degradation compared to unstructured approaches. AI still needs occasional reminders about principles - this is normal.
Development Flow: Systematic planning tends to reduce debugging cycles. Focused implementation helps minimize feature bloat. Performance data supports optimization decisions.
Code Quality: Architectural consistency across components, measurable performance characteristics, maintainable structure as projects scale.

Claude Q&A Documentation
Coverage includes:

Methodology understanding and workflow patterns
Project initialization and Phase 0 requirements
Tool usage and technology stack compatibility
Quality enforcement and violation handling
User experience across different skill levels


Frequently Asked Questions
Origin & Development

What problem led you to create this methodology?

I kept having to restate my preferences and architectural requirements to AI systems. It didn't matter which language or project I was working on - the AI would consistently produce either bloated monolithic code or underdeveloped implementations with issues throughout.
This led me to examine the meta-principles driving code quality and software architecture. I questioned whether pattern matching in AI models might be more effective when focused on underlying software principles rather than surface-level syntax. Since pattern matching is logic-driven and machines fundamentally operate on simple question-answer pairs, I realized that functions with multiple simultaneous questions were overwhelming the system.
The breakthrough came from understanding that everything ultimately transpiles to binary - a series of "can you do this? → yes/no" decisions. This insight shaped my approach: instead of issuing commands, ask focused questions in proper context. Rather than mentally managing complex setups alone, collaborate with AI to devise systematic plans.



How did you discover these specific constraints work?

Through extensive trial and error. AI systems will always tend to drift even under constraints, but they're significantly more accurate with structured boundaries than without them. You occasionally need to remind the AI of its role to prevent deviation - like managing a well-intentioned toddler that knows the rules but sometimes pushes boundaries trying to satisfy you.
These tools are far from perfect, but they're effective instruments for software development when properly constrained.



What failures or frustrations shaped this approach?

Maintenance hell was the primary driver. I grew tired of responses filled with excessive praise: "You have found the solution!", "You have redefined the laws of physics with your paradigm-shifting script!" This verbose fluff wastes time, tokens, and patience without contributing to productive development.
Instead of venting frustration on social media about AI being "just a dumb tool," I decided to find methods that actually work. My approach may not help everyone, but I hope it benefits those who share similar AI development frustrations.


Personal Practice

How consistently do you follow your own methodology?

Since creating the documentation, I haven't deviated. Whenever I see the model producing more lines than my methodology restricts, I immediately interrupt generation with a flag: "‼️ ARCHITECTURAL VIOLATION, ADHERE TO PRINCIPLES ‼️" I then provide the method instructions again, depending on how context is stored and which model I'm using.



What happens when you deviate from it?

I become genuinely uncomfortable. Once I see things starting to degrade or become tangled, I compulsively need to organize and optimize. Deviation simply isn't an option anymore.



Which principles do you find hardest to maintain?

Not cursing at the AI when it drifts during complex algorithms! But seriously, it's a machine - it's not perfect, and neither are we.


AI Development Journey

When did you start using AI for programming?

In August 2024, I created a RuneLite theme pack, but one of the plugin overlays didn't match my custom layout. I opened a GitHub issue (creating my first GitHub account to do so) requesting a customization option. The response was: "It's not a priority - if you want it, build it yourself."
I used ChatGPT to guide me through forking RuneLite and creating a plugin. This experience sparked intense interest in underlying software principles rather than just syntax.



How has your approach evolved over time?

I view development like a book: syntax is the cover, logic is the content itself. Rather than learning syntax structures, I focused on core meta-principles - how software interacts, how logic flows, different algorithm types. I quickly realized everything reduces to the same foundation: question and answer sequences.
Large code structures are essentially chaotic meetings - one coordinator fielding questions and answers from multiple sources, trying to provide correct responses without mix-ups or misinterpretation. If this applies to human communication, it must apply to software principles.



What were your biggest mistakes with AI collaboration?

Expecting it to intuitively understand my requirements, provide perfect fixes, be completely honest, and act like a true expert. This was all elaborate roleplay that produced poor code. While fine for single-purpose scripts, it failed completely for scalable codebases.
I learned not to feed requirements and hope for the best. Instead, I needed to collaborate actively - create plans, ask for feedback on content clarity, and identify uncertainties. This gradual process taught me the AI's actual capabilities and most effective collaboration methods.


Methodology Specifics

Why 150 lines exactly?

Multiple benefits: easy readability, clear understanding, modularity enforcement, architectural clarity, simple maintenance, component testing, optimal AI context retention, reusability, and KISS principle adherence.



How did you determine Phase 0 requirements?

From meta-principles of software: if it displays, it must run; if it runs, it can be measured; if it can be measured, it can be optimized; if it can be optimized, it can be reliable; if it can be reliable, it can be trusted.
Regardless of project type, anything requiring architecture needs these foundations. You must ensure changes don't negatively impact the entire system. A single line modification in a nested function might work perfectly but cause 300ms boot time regression for all users.
By testing during development, you catch inefficiencies early. Integration from the start means simply hooking up new components and running tests via command line - minimal time investment with actual value returned. I prefer validation and consistency throughout development rather than programming blind.


Practical Implementation

How do you handle projects that don't fit the methodology?

I adapt them to fit, or if truly impossible, I adjust the method itself. This is one methodology - I can generate countless variations as needed. Having spent 6700+ hours in AI interactions across multiple domains (not just software), I've developed strong system comprehension that enables creating adjusted methodologies on demand.



What's the learning curve for new users?

I cannot accurately answer this question. I've learned that I'm neurologically different - what I perceive as easy or obvious isn't always the case for others. This question is better addressed by someone who has actually used this methodology to determine its learning curve.



When shouldn't someone use this approach?

If you're not serious about projects, despise AI, dislike planning, don't care about modularization, or are just writing simple scripts. However, for anything requiring reliability, I believe this is currently the most effective method.
You still need programming fundamentals to use this methodology effectively - it's significantly more structured than ad-hoc approaches.



Workflow Visualization

  
      ---
config:
  layout: elk
  theme: neo-dark
---
flowchart TD
    A["Project Idea"] --> B["🤖 Stage 1: AI Configuration<br>AI-PREFERENCES.md Custom Instructions"]
    B --> C["Stage 2: Collaborative Planning<br>Share METHODOLOGY.md"]
    C --> D["Define Scope & Completion Criteria"]
    D --> E["Identify Components & Dependencies"]
    E --> F["Structure Phases Based on Logic"]
    F --> G["Document Edge Cases - No Implementation"]
    G --> H["Generate Development Plan with Checkpoints"]
    H --> I["🔧 Stage 3: Phase 0 Infrastructure<br>MANDATORY BEFORE ANY CODE"]
    I --> J["Benchmarking Suite + Regression Detection"]
    J --> K["GitHub Workflows + Quality Gates"]
    K --> L["Test Suite Infrastructure + Stress Tests"]
    L --> M["Documentation Generation System"]
    M --> N["Centralized Configuration + Constants"]
    N --> O["📁 project_extract.py Setup<br>Single/Multiple File Config"]
    O --> P["Initial Project State Extraction"]
    P --> Q["Share Context with AI"]
    Q --> R["Start Development Session<br>Pre-Session Compliance Audit"]
    R --> S{"Next Phase Available?"}
    S -- No --> Z["Project Complete"]
    S -- Yes --> T["Select Single Component<br>Target ≤150 Lines"]
    T --> U{"Multi-Language Required?"}
    U -- Yes --> V["Document Performance Justification<br>Measurable Benefits Required"]
    V --> W["Request AI Implementation"]
    U -- No --> W
    W --> X{"AI Uncertainty Flag?"}
    X -- ⚠️ Yes --> Y["Request Clarification<br>Provide Additional Context"]
    Y --> W
    X -- Clear --> AA["Stage 3: Systematic Implementation"]
    AA --> BB{"Automated Size Check<br>validate-phase Script"}
    BB -- >150 Lines --> CC["AUTOMATED: Split Required<br>Maintain SoC Boundaries"]
    CC --> W
    BB -- ≤150 Lines --> DD["Incremental Compliance Check<br>DRY/KISS/SoC Validation"]
    DD --> EE{"Architectural Principles Pass?"}
    EE -- No --> FF["Flag Specific Violations<br>Reference Methodology"]
    FF --> W
    EE -- Yes --> GG["📊 Stage 4: Data-Driven Iteration<br>Run Benchmark Suite + Save Baselines"]
    GG --> HH["Compare Against Historical Timeline<br>Regression Analysis"]
    HH --> II{"Performance Gate Pass?"}
    II -- Regression Detected --> JJ["Share Performance Data<br>Request Optimization"]
    JJ --> W
    II -- Pass --> KK["Integration Test<br>Verify System Boundaries"]
    KK --> LL{"Cross-Platform Validation?"}
    LL -- Fail --> MM["Address Deployment Constraints<br>Real-World Considerations"]
    MM --> W
    LL -- Pass --> NN{"More Components in Phase?"}
    NN -- Yes --> T
    NN -- No --> OO["🚦 Phase Quality Gate<br>Full Architecture Audit"]
    OO --> PP["Production Simulation<br>Resource Cleanup + Load Test"]
    PP --> QQ{"All Quality Gates Pass?"}
    QQ -- No --> RR["Document Failed Checkpoints<br>Block Phase Progression"]
    RR --> T
    QQ -- Yes --> SS["End Development Session<br>Technical Debt Assessment"]
    SS --> TT["📁 Extract Updated Project State<br>Generate Fresh Context"]
    TT --> UU["Phase Results Documentation<br>Metrics + Outcomes + Timeline"]
    UU --> VV["Update Development Plan<br>Mark Phase Complete"]
    VV --> S
    WW["validate-phase<br>AUTOMATED: File Size + Structure"] -.-> BB
    XX["dry-audit<br>AUTOMATED: Cross-Module Duplication"] -.-> DD
    YY["CI/CD Workflows<br>AUTOMATED: Merge Gates"] -.-> GG
    ZZ["Performance Timeline<br>AUTOMATED: Historical Data"] -.-> HH
    AAA["Dependency Validator<br>AUTOMATED: Import Boundaries"] -.-> KK
    BBB["Architecture Auditor<br>AUTOMATED: SoC Compliance"] -.-> OO
    WW -. BUILD FAILURE .-> CC
    YY -. MERGE BLOCKED .-> JJ
    BBB -. AUDIT FAILURE .-> RR
    style Y fill:#7d5f00
    style CC fill:#770000
    style FF fill:#7d5f00
    style JJ fill:#7d5f00
    style MM fill:#770000
    style RR fill:#770000

    
  
    
      Loading

  


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why language models hallucinate]]></title>
            <link>https://openai.com/index/why-language-models-hallucinate/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45147385</guid>
        </item>
        <item>
            <title><![CDATA[Rug pulls, forks, and open-source feudalism]]></title>
            <link>https://lwn.net/SubscriberLink/1036465/e80ebbc4cee39bfb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45146967</guid>
            <description><![CDATA[Like almost all human endeavors, open-source software development involves a range of power dyn [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



Like almost all human endeavors, open-source software development involves
a range of power dynamics.  Companies, developers, and users are all
concerned with the power to influence the direction of the software — and,
often, to profit from it.  At the 2025 Open
Source Summit Europe, Dawn Foster talked about how those dynamics can
play out, with an eye toward a couple of tactics — rug pulls and forks — that
are available to try to shift power in one direction or another.
Power dynamics

Since the beginning of history, Foster began, those in power have tended to
use it against those who were weaker.  In the days of feudalism, control of
the land led to exploitation at several levels.  In the open-source world,
the large cloud providers often seem to have the most power, which they use
against smaller companies.  Contributors and maintainers often have less
power than even the smaller companies, and users have less power yet.  



We have built a world where it is often easiest to just use whatever a
cloud provider offers, even with open-source software.  Those providers may
not contribute back to the projects they turn into services, though,
upsetting the smaller companies that are, likely as not, doing the bulk of
the work to provide the software in question in the first place.  Those
companies can have a power of their own, however: the power to relicense
the software.  Pulling the rug out from under users of the software in this
way can change the balance of power with regard to cloud providers, but it
leaves contributors and users in a worse position than before.  But
there is a power at this level too: the power to fork the software,
flipping the power balance yet again.

Companies that control a software project have the power to carry out this
sort of rug pull, and they are often not shy about exercising it.
Single-company projects, clearly, are at a much higher risk of rug pulls;
the company has all the power in this case, and others have little
recourse.  So one should look at a company's reputation before adopting a
software project, but that is only so helpful.  Companies can change
direction without notice, be acquired, or go out of business, making
previous assessments of their reputation irrelevant.

The problem often comes down to the simple fact that companies have to
answer to their investors, and that often leads to pressure to relicense
the software they have created in order to increase revenue.  This is
especially true in cases where cloud providers are competing for the same
customers as the company that owns the project.  The result can be a switch
to a more restrictive license aimed at making it harder for other companies
to profit from the project.

A rug pull of this nature can lead to a fork of the project — a rebellious,
collective action aimed at regaining some power over the code.  But a fork
is not a simple matter; it is a lot of work, and will fail without people
and resources behind it.  The natural source for that is a large company;
cloud providers, too, can try to shift power via a fork, and they have the
ability to back their fork up with the resources it needs to succeed.



A relicensing event does not always lead to a popular fork; that did not
happen with MongoDB or Sentry, for example.  Foster said she had not looked
into why that was the case.  Sometimes rug pulls take other forms, such as
when Perforce, after acquiring Puppet in 2022, moved it development and
releases behind closed doors, with a reduced frequency of releases back to
the public repository.  That action kicked off the OpenVox fork.
Looking at the numbers

Foster has spent some time analyzing rug pulls, forks, and what happens
thereafter; a lot of the results are available
for download as Jupyter notebooks.  For each rug-pull event, she looked
at the contributor makeup of the project before and after the ensuing fork
in an attempt to see what effects are felt by the projects involved.

In 2021, Elastic relicensed Elasticsearch
under the non-free Server Side Public License (SSPL).  Amazon Web Services
then forked the project as OpenSearch.  Before the fork, most of
the Elasticsearch contributors were Elastic employees; that,
unsurprisingly, did not change afterward.  OpenSearch started with no
strong contributor base, so had to build its community from scratch.  As a
result, the project has been dominated by Amazon contributors ever since;
the balance has shifted slowly over time, but there was not a big uptick in
outside contributors even after OpenSearch became a Linux Foundation
project in 2024.  While starting a project under a neutral foundation can
help attract contributors, she said, moving a project under a foundation's
umbrella later on does not seem to provide the same benefit.

Terraform was
developed mostly by Hashicorp, which relicensed
the software under the non-free Business Source License in 2023.  One
month later, the OpenTofu fork was
started under the Linux Foundation.  While the contributor base for
Terraform, which was almost entirely Hashicorp employees, changed little
after the fork, OpenTofu quickly acquired a number of contributors from
several companies, none of whom had been Terraform contributors before.  In
this case, users drove the fork and placed it under a neutral foundation,
resulting in a more active developer community.

In 2024, Redis was relicensed under the
SSPL; the Valkey fork was quickly organized, under the Linux Foundation,
by Redis contributors.  The Redis project differed from the others
mentioned here in that, before the fork, it had nearly twice as many
contributors from outside the company as from within; after the fork, the
number of external Redis contributors dropped to zero.  All of the external
contributors fled to Valkey, with the result that Valkey started with a
strong community representing a dozen or so companies.

Looking at how the usage of these projects changes is harder, she
said, but there appears to be a correlation between the usage of a project
and the number of GitHub forks (cloned repository copies) it has.  There is
typically a spike in these clones after a relicensing event, suggesting
that people are considering creating a hard fork of the project.  In all
cases, the forks that emerged appeared to have less usage than the original
by the "GitHub forks" metric; both branches of the fork continue to go
forward.  But, she said, projects that are relicensed do tend to show
reduced usage, especially when competing forks are created under foundations.
What to do

This kind of power game creates problems for both contributors and users,
she said; we contribute our time to these projects, and need them to not be
pulled out from under us.  There is no way to know when a rug pull might
happen, but there are some warning signs to look out for.  At the top of
her list was the use of a contributor license agreement (CLA); these
agreements create a power imbalance, giving the company involved the power
to relicense the software.  Projects with CLAs more commonly are subject to
rug pulls; projects using a developers certificate of origin do not have the
same power imbalance and are less likely to be rug pulled.

One should also look at the governance of a project; while being housed
under a foundation reduces the chance of a rug pull, that can still happen,
especially in cases where the contributors are mostly from a single
company.  She mentioned the Cortex project, housed under
the Cloud Native Computing Foundation, which was controlled by Grafana; that
company eventually forked its own project to create Mimir.  To avoid this kind of
surprise, one should look for projects with neutral governance, with
leaders from multiple organizations.

Projects should also be evaluated on their contributor base; are there
enough contributors to keep things going?  Companies can help, of course,
by having their employees contribute to the projects they depend on,
increasing influence and making those projects more sustainable.  She
mentioned the CHAOSS project, which
generates metrics to help in the judgment of the viability of development
projects.  CHAOSS has put together a set of
"practitioner guides" intended to help contributors and maintainers
make improvements within a project.

With the sustained rise of the big cloud providers, she concluded, the
power dynamics around open-source software are looking increasingly feudal.
Companies can use relicensing to shift power away from those providers, but
they also take power from contributors when the pull the rug in this way.
Those contributors, though, are in a better position than the serfs of old,
since they have the ability to fork a project they care about, shifting
power back in their direction.


Hazel Weakly asked if there are other protections that contributors and
users might develop to address this problem.  Foster answered that at least
one company changed its mind about a planned relicensing action after
seeing the success of the Valkey and OpenTofu forks.  The ability to fork
has the effect of making companies think harder, knowing that there may be
consequences that follow a rug pull.  Beyond that, she reiterated that
projects should be pushed toward neutral governance.

Dirk Hohndel added that the best thing to do is to bring more outside
contributors into a project; the more of them there are, the higher the
risk associated with a rug pull.  Anybody who just sits back within a
project, he said, is just a passenger; it is better to be driving.

Foster's
slides are available for interested readers.

[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Universe Within 12.5 Light Years]]></title>
            <link>http://www.atlasoftheuniverse.com/12lys.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144337</guid>
            <description><![CDATA[This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.]]></description>
            <content:encoded><![CDATA[
About the Map
This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.

Epsilon Eridani is orbited by a large planet which might look like this.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kenvue stock drops on report RFK Jr will link autism to Tylenol during pregnancy]]></title>
            <link>https://www.cnbc.com/2025/09/05/rfk-tylenol-autism-kenvue-stock-for-url.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144123</guid>
            <description><![CDATA[HHS will release the report that could draw that link between the pain medication and autism this month, according to the Wall Street Journal.]]></description>
            <content:encoded><![CDATA[Kenvue Inc. Tylenol brand pain reliever for sale at a pharmacy in New York, US, on Wednesday, March 27, 2024. Bloomberg | Bloomberg | Getty ImagesShares of Kenvue fell more than 10% on Friday after a report that Health and Human Services Secretary Robert F. Kennedy Jr. will likely link autism to the use of the company's pain medication Tylenol in pregnant women. HHS will release the report that could draw that link this month, the Wall Street Journal reported on Friday.That report will also suggest a medicine derived from folate – a water-soluble vitamin – can be used to treat symptoms of the developmental disorder in some people, according to the Journal.In a statement, an HHS spokesperson said "We are using gold-standard science to get to the bottom of America's unprecedented rise in autism rates." "Until we release the final report, any claims about its contents are nothing more than speculation," they added. Tylenol could be the latest widely used and accepted treatment that Kennedy has undermined at the helm of HHS, which oversees federal health agencies that regulate drugs and other therapies. Kennedy has also taken steps to change vaccine policy in the U.S., and has amplified false claims about safe and effective shots that use mRNA technology.Kennedy has made the disorder a key focus of HHS, pledging in April that the agency will "know what has caused the autism epidemic" by September and eliminate exposures. He also said that month that the agency has launched a "massive testing and research effort" involving hundreds of scientists worldwide that will determine the cause.In a statement, Kenvue said it has "continuously evaluated the science and [continues] to believe there is no causal link" between the use of acetaminophen, the generic name for Tylenol, during pregnancy and autism.The company added that the Food and Drug Administration and leading medical organizations "agree on the safety" of the drug, its use during pregnancy and the information provided on the Tylenol label.The FDA website says the agency has not found "clear evidence" that appropriate use of acetaminophen during pregnancy causes "adverse pregnancy, birth, neurobehavioral, or developmental outcomes." But the FDA said it advises pregnant women to speak with their health-care providers before using over-the-counter drugs.The American College of Obstetricians and Gynecologists maintains that acetaminophen is safe during pregnancy when taken as directed and after consulting a health-care provider. Some previous studies have suggested the drug poses risks to fetal development, and some parents have brought lawsuits claiming that they gave birth to children with autism after using it.But a federal judge in Manhattan ruled in 2023 that some of those lawsuits lacked scientific evidence and later ended the litigation in 2024. Some research has also found no association between acetaminophen use and autism.In a note on Friday, BNP Paribas analyst Navann Ty said the firm believes the "hurdle to proving causation [between the drug and autism] is high, particularly given that the litigation previously concluded in Kenvue's favor."-- CNBC's Angelica Peebles contributed to this report.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[William James at CERN (1995)]]></title>
            <link>http://bactra.org/wm-james-at-cern/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45143019</guid>
            <description><![CDATA[This is obviously true of action.  Whatever views your views on free
will, it is indubitable that differing options occur to us, that we compare
them, that we prefer some to others, that eventually we elect one and dismiss
the rest.  More interestingly, James describes the role of selection in
perception, and finds it at every level of neural and mental life.
The sense organs, to begin with, are insensitive to almost all that happens
around them.  When they are excited and transmit nervous impulses to the brain,
these are sifted for significant patterns (often found on dubious grounds).
News of these is relayed to other parts of the brain, which look for more
subtle, more detailed, and more broad patterns, until at last we reach our
perceptions, grouped together by another process of selection into things.
Some of these we attend to; the rest we ignore.]]></description>
            <content:encoded><![CDATA[
William James at CERN
Some Examples of Selection in Minds and Computers
Cosma Rohilla Shalizi



In the famous chapter on ``The Stream of Thought'' in his Principles of
Psychology --- the one where he coins the phrase ``stream of
consciousness'' --- William James considers as the last of the five ``important
characters'' of the stream, the fact that ``It is always more interested in
one part of its object [thought] than in another, and welcomes or rejects, or
chooses, all the while it thinks.''
This is obviously true of action.  Whatever views your views on free
will, it is indubitable that differing options occur to us, that we compare
them, that we prefer some to others, that eventually we elect one and dismiss
the rest.  More interestingly, James describes the role of selection in
perception, and finds it at every level of neural and mental life.
The sense organs, to begin with, are insensitive to almost all that happens
around them.  When they are excited and transmit nervous impulses to the brain,
these are sifted for significant patterns (often found on dubious grounds).
News of these is relayed to other parts of the brain, which look for more
subtle, more detailed, and more broad patterns, until at last we reach our
perceptions, grouped together by another process of selection into things.
Some of these we attend to; the rest we ignore.

``The mind is at every stage a theatre of simultaneous
possibilities.  Consciousness consists in the comparison of these with each
other, the selection of some, and the suppression of the rest by the
reinforcing and inhibiting agency of attention.  The highest and most
elaborated mental products are filtered from the data chosen by the faculty
next beneath, out of the mass offered by the faculty below that, which mass in
turn was sifted from a still larger amount of yet simpler material, and so on.
The mind, in short, works on the data it receives very much as a sculptor works
on his block of stone.  In a sense the statue stood there from eternity.  But
there were a thousand different ones beside it, and the sculptor alone is to
thank for having extricated this one from the rest.  Just so the world of each
of us, how so ever different our several views of it may be, all lay embedded
in the primordial chaos of sensations, which gave the mere matter to
the thought of all of us indifferently.  We may, if we like, by our reasonings
unwind things back to that black and jointless continuity of space and moving
clouds of swarming atoms which science calls the only real world.  But all the
while the world we feel and live in will be that which our ancestors
and we, by slowly cumulative strokes of choice, have extricated out of this,
like sculptors, by simply removing portions of the given stuff.  Other
sculptors, other statues from the same stone!  Other minds, other worlds from
the same monotonous and inexpressive chaos!  My world is but one in a million
alike embedded, alike real to those who may abstract them.  How different must
be the worlds in the consciousness of ant, cuttlefish, or crab!''

James wrote in 1890, and the last century of research into brain and mind have
done nothing to diminish our confidence in this (admittedly very general)
picture.  Indeed, we can now point to parts of the brain which select specific
features out of the signals of the sense organs --- cells in the occipital
lobe, for instance, which respond only to straight lines at certain angles, or
motion, or contrasts of light and darkness --- and we are beginning to
understand the more elaborate construction of things out of such elements.
From almost any authority on neurobiology, cognitive science, psychology, the
philosophy of mind, artificial intelligence or computer science, I could have
quoted passages saying substantially the same things as James, though in worse
prose and without quite the same emphasis on selection.

I propose, now, to see whether these ideas of selection shed any light on
the various uses of ``thinking machines,'' that is to say, computers.

2. The Julia Set
The first is computer-generated art.

The reader is almost certainly familiar with fractals, whether of the
abstract or the naturalistic variety, but is perhaps less likely to know that
computer programs have written verse (rhymed, blank and free), short stories,
and even a novel.  Art critics --- and more particularly, theoretical art
critics --- have been understandably interested in these developments.  Some
have dismissed them as, at best, amusements for the boys in the basement
computer lab across campus, a folk art for those whose native language is C.
Others --- such as the late O. B. Hardison Jr., whose views are set forth with
admirable clarity in Disappearing Through the Skylight --- have
been thrown into a kind of ecstasy of obsolescence.  ``Gazing at the
thirty-nine sea-horses of the Mandelbrot set,'' they say in effect, ``we can
see that human art, Art with a capital A, is at an end, not perhaps this
week-end, but soon: it is later than you think.  The day will come when a human
artist could no more rival a computer than than a sprinter out-race a Ferrari.
The coming art will be digital, perfect, timeless, inimitable, perhaps
incomprehensible.  We and all our works shall pass to dust, and only they will
remain, dreaming their silicon dreams of unknown space.''  Such, in essence and
composite, is the rhetoric.  It seems to me to miss the most interesting aspect
of the new computer art, which is its human angle, and with it the most
plausible future.

The connection between selection and art is, to revert to James,
``obvious''.

``The artist notoriously selects his items, rejecting all tones,
colors, shapes, which do not harmonize with each other and with the main
purpose of his work.  That unity, harmony, `convergence of characters,' as M.
Taine calls it, which gives to works of art their superiority over works of
nature, is wholly due to elimination. Any natural subject will do, if
the artist has wit enough to pounce upon some one feature of it as
characteristic, and suppress all merely accidental items which do not harmonize
with this.''

Now the peculiarity of computer art is that computer programs are very bad at
just this sort of pruning.  They will make a pattern --- of colors, of sounds,
of words --- according to a rule, and that is all.  Give, let us say, a fractal
program one rule, and it will draw you the corresponding picture; change the
rule slightly and it draws another, similar picture.  It does not linger over
the interesting, balk at the trite, turn away from the boring or disturbing: it
is a machine without preferences, ``a gaze blank and pitiless as the sun.''

Artists, notoriously, are different.  Even those who use ``found objects''
select the ones they find interesting, relevant or marketable, and eliminate
everything else in the world.  Selection is inescapable --- or at least not yet
escaped.  Computer programs, then, are poor artists because their powers of
choice are absolutely miniscule; they select not from a pool of possibilities
but a handful of drops, often not even that.

Yet the fact remains that computer-made graphics, if not music or
literature, are quite popular, even delightful.  How can this be?

We have all had the experience of writing out a sentence and then crossing
it out, in bits and pieces, putting in new words and phrases until we find ones
which fit to our satisfaction; until, that is, we select one sentence out of a
mob of candidates.  Computer art programs show us the more promising members of
this mob.  We then pick and choose among them, according to our tastes and
purposes.  It is a strange man who would put a view of the Mandelbrot set on a
condolence-card; and a rash one who would say there is none suitable for this
purpose.

In reality, then, the computer is not an independent artist, but a sort of
dumb assistant, an automatic producer of first sketches.  If the initial
attempt is not perfectly satisfying --- and what first sketch is? --- either
improve it by hand, or modify the computer's instructions slightly and have it
try again.  The afore-mentioned novel was written in the first way, most
fractal pictures are arrived at in the second.

It may be asked, Must this be so? Must the computer always be just an
adjunct, a patient but moronic apprentice?  The key, as we have seen, is to
give the computer preferences, and this does not sound impossible.
Let it produce one picture, one tune, one sonnet, and see whether it satisfy
its principles.  If not change it - a more drastic change the further the
sketch falls short of giving satisfaction - and repeat this cycle until all the
criteria are met.  In fact, the computer could consider a number of sketches in
parallel, working on them simultaneously and combining promising features, and
in this way progress much faster than if it had (so to speak) a one-track mind.
(Some will recognize this as an application of the technique of ``genetic algorithms.'')

The difficulty, and it is a large one, lies in spelling out those principles
in ways simple and clear enough for a computer to act on.  It is hard enough to
give an intelligible account of why we like a painting, switch off the radio
when that tune comes on, gaze at one statue for inspiration and use
another for a door-stop.  It is easy to despair of ever being able to
deduce the Ninth Symphony's superiority to one of its crossed-out
early drafts; it may be better not to contemplate even the attempt.  Yet
without such formal criteria, independent computer art will remain, at most,
a curiosity.

I will only close this subject by saying that there has, recently, been some
work done in this field, though I do not know if the investigators have
considered it in quite this light.  In the closing pages of his recent book
Strange Attractors, Professor Julien Sprott of the University of
Wisconsin describes a survey he and colleagues made of taste in fractal
pictures.  People were asked to rank different fractal pictures, and these
preferences were plotted against the fractal dimension of the picture (a number
which measures how rough, convoluted and ``space-filling'' the image is) and
the ``Lyapunov exponent'' of the equations which generated it (another number,
which measures how quickly the equations amplify small differences in their
variables, the fabled ``sensitive dependence on initial conditions.'')  Strange
as it may seem, people consistently prefer pictures whose fractal dimension and
Lyapunov exponents cluster rather tightly about a constant center.  If, then, a
computer could be programmed to consistently produce images in that area, it
would be a small first step towards ``automated taste.''  A fascinating
speculation on where this might lead --- uninfluenced, as far as I know, by
Prof. Sprott --- is found in Ian McDonald's science fiction novel
Scissors Cut Paper Wrap Stone.

3. James at CERN

After this excursion into Art, a reminder of the notion of perception we have
taken from James is perhaps not out of place: The stream of thought is
incurably and necessarily selective.  Out of the ``blooming, buzzing
confusion'' of sensory nerve impulses, variously organized, bundled and
transformed by different parts of the cortex, a miniscule fraction are selected
--- elected? --- for awareness and conscious attention.  Most of the world,
even most of what impinges on the sense-organs, is simply thrown away.  What
remains is not pure sensation, but an elaborate construction or reconstruction,
influenced by memory, expectation, attention and hypothesis, as well as those
quirks and kluges of the nervous systems whose effects can be learned from any
book on illusions.  If all goes well, this represents the world, not in all its
breadth and detail --- how could it? --- but fairly enough for our purposes.
If not, then, as we say nowadays, ``the model is inadequate and must be
revised'', with luck through a restful stay at a sympathetic institution,
through the elimination of the modeler without it.

It is curious, and I believe not previously noticed, that something very
similar is essential to high-energy physics.  (Physics also needs normal
perception, of course.)

At this point alarms ring in the minds of my colleagues, since we are all
too familiar with books on the profound connection between ``the new physics''
and consciousness and various sophomoric distortions of Asian mysticism.  The
authors of this school are seldom discussed, save by graduate students who
laugh at the errors and covet the royalties.  Rest assured, I shall not discuss
the torture of cats, Buddhist puns, interpretive dance, the Tao of the
relativistic Euler-Lagrange equations, the maya-aspects of
renormalizable gauge field theories, or even how to find a cheap Chinese
restaurant in Copenhagen without a Danish interpreter.

My subject is, instead, rather more massive and solid and sweaty: the
detectors attached to particle accelerators.  A word or three of reminder about
these, too, may not be out of place.

Particle physicists are interested in what the smallest discoverable bits of
matter are, and how they behave.  They are especially interested in how they
behave at very high energies, since these let them probe very short distances
and led to unusual (and hence informative) events, like the creation of new
kinds of particles.  The only practical way to give elementary particles lots
of energy is to accelerate them to very high speeds; the electro-magnetic
machines which do this are called, imaginatively enough, ``accelerators''.
Some accelerators send a beam of particles into a fixed target of more normal
matter, say, gold foil.  The really high-energy ones collide two beams of
particles moving in opposite directions.  There are all sorts of 
fascinating technical issues, on which I may well end up writing a
dissertation --- but another time.

More interesting for us than the accelerators are the detectors, the
machines which sense what happens when the particles collide.  The need for
such machines is quite real.  The events happen far too quickly (over 10^-23
to, at the most lackadaisical, 10^-10, seconds) and in too small a region (on
the order of 10^-18 meters) for human perception.

I come at last to the heart of the matter.  Most of the oceans of data from
detectors are uninteresting and worthless.  Recall that physicists want to
learn about unusual, hard-to-achieve or anomalous events; everything else is
noise.  But common, easily occurring events are by definition the majority;
therefore most events are uninteresting.  Sturgeon's Law states that ``ninety
percent of everything is crap.''  For particle physics, this is wildly
optimistic; interesting events can be outnumbered by billions or trillions to
one.  In theory, combing haystacks for needles is what professors have graduate
students for.  In practice, not even an army would suffice.

What does suffice is very high speed electronics, working on
time-scales of under a microsecond.  The lowest level, known as the trigger,
scans the signals from the detector for an interesting pattern, usually
something very simple, like ``two diametrically opposed detectors activated.''
The data is recorded only if the trigger is (for want of a better word)
triggered.  Once it is recorded, the computers set to work on it, attempting a
more and more detailed reconstruction of the event.  At each stage in the
reconstruction there are ``cuts'', i.e. some events are selected for their
interesting characteristics and the rest discarded.  (For instance, we might
want events where all the outgoing particles concentrate into two back-to-back
jets, and so cut those where lots of other detectors got triggered, along with
a diametrically opposed pair.) Great care is lavished on both the design of the
cuts and the reconstruction, for figuring out what to ignore is, practically,
as important as figuring out what happened.  What bubbles up, in the end, are a
handful of reconstructions selected --- elected? --- for conscious, human
attention.

Piling layers of selection atop each other is essential if we are,
reasonably quickly, to direct our resources where we they ought to be most
useful; triage is a dramatic example.  And in fact successive cuts give
experiments remarkable leverage. (See again our  back of the envelope calculation.)
Physicists have been in love with leverage since Archimedes, but there is a
cost, and to illustrate it I shall, with the reader's kind permission, once
again quote William James, this time on attention:

``[I]n those puzzles where certain lines in a picture form by their
combination an object that has no connection with what the picture ostensibly
represents; or indeed in every case where an object is inconspicuous and hard
to discern from the background; we may not be able to see it for a long time;
but, having once seen it, we can attend to it whenever we like, on account of
the mental duplicate of it which our imagination now bears.  In the meaningless
French words `pas de lieu Rhone que nous,' who can recognize
immediately the English `paddle your own canoe?'  But who that has once noticed
the identity can fail to have it arrest his attention again?  When watching for
the distant clock to strike, our mind is so filled with its image that at every
moment we think we hear the longed-for or dreaded sound.  So of an awaited
footstep.  Every stir in the wood is for the the hunter his game; for the
fugitive his pursuers.  Every bonnet in the street is momentarily taken by the
lover to enshroud the head of his idol.  The image in the mind
is the attention; the preperception, as Mr. Lewes calls it,
is half of the perception of the looked-for thing.

``It is for this reason that men have no eyes but for those aspects of
things which they have already been taught to discern.  Any one of us can
notice a phenomenon after it has once been pointed out, which not one in ten
thousand could ever have discovered for himself.  Even in poetry and the arts,
some one has to come and tell us what aspects we may single out, and what
effects we may admire, before our aesthetic nature can `dilate' to its full
extent and never `with the wrong emotion.'  In kindergarten instruction one of
the exercises is to make the children see how many features they can point out
in such an object as a flower or a stuffed bird.  They readily name the
features they know already, such as leaves, tail, bill, feet.  But they may
look for hours without distinguishing nostrils, claws, scales, etc., until
their attention is called to these details; thereafter, however, they see them
every time.  In short, the only things which we commonly see are those
which we preperceive, and the only things which we preperceive are those
which have been labelled for us, and the labels stamped into our
minds.'' In detectors, ``preperception'' takes the form of the
hard-wired trigger and programmed cuts.  An event which might be fantastically
interesting, if only we knew about it, will be sent into oblivion if it does
not match our a priori criteria at every step.  In this sense,
experimenters only find what they are looking for --- if it exists.

The analogy between detectors and our view of perception is rather close.
(Technological determinists, kindly note that James began writing in 1880, but
the first accelerators were built in the 1930s.)  It would be rash to claim
that large particle detectors are conscious.  In them we have perhaps
the foundations and basic plumbing (with special attention to sewage disposal)
of the building of consciousness; perhaps some scaffolding for the higher
floors as well.

4. The Immoral Equivalent of War, War Itself

Handling outrageous amounts of information arriving very fast, most of it
utterly worthless, all of it of uncertain veracity, and picking out from it a
few events of absolutely essential importance, recording them very fully, and
inferring a detailed picture of the outside world: the job of detector
electronics and military commanders during combat.  The major differences are
that military commanders need the data represented to them in real time, and
accelerator physicists don't give (fancy) orders to the machinery while it's
running.  This is because high energy physics happens much faster than battle.


5. Questions

Has DARPA, the Defense Advanced Research Projects Agency, (nee ARPA, the
Advanced Research Projects Agency, fairy-godmother to AI and the net) ever
funded work on processing experimental data from detectors?  Has Edward Teller
read William James? Have the artificial intelligentsia taken a look at CDF or
ALEPH?  How much fancier must detector electronics be, before they become
conscious?  What is it like to be CDF or ALEPH, to have your consciousness
restricted to two colliding proton beams?  Is it at all significant that WWW is
headquartered at CERN?

Books

William James to begin with, of course.  The Principles of
Psychology was first published in 1890, and is currently available in
two paperback volumes from Dover Books in New York, and in a single hardback as
vol. 53 of the Britannica Great Books.  I have quoted from the latter.  His
Shorter Course is, indeed, substantially shorter, and issued by a
number of different publishing houses.  An even more condensed presentation is
found in Jacques Barzun's worshipful A Stroll with William James
(University of Chicago Press, 1981, currently in paperback).

Following James, P. N. Johnson-Laird's The Computer and the
Mind presents more or less the standard view of the ``artificial
intelligentsia and cognitive cognoscenti'' with lucidity and no more detail
than a common reader may be expected to accept.  Cognitive science is a
``top-down'' approach; good sources for the complentary ``bottom-up'' view of
the brain scientists, which is considerably wetter and messier, are William
Calvin and George Ojemann,
Conversations with Neil's Brain (New York: Addison-Wesley, 1994)
and, at a higher level but still very accessible, Shepherd's delightful
Neurobiology (Oxford University Press, 1983) and A. R. Luria's
The Working Brain (New York: Basic Books, 1973).

More idiosyncratic but still broadly mainstream views can be found in Marvin
Minsky, The Society of Mind (artificial intelligence), William
Calvin, The Cerebral Symphony (neurology) and Daniel Dennett,
Consciousness Explained (philosophy).

The literature on fractals and computer art is swiftly becoming as
unsurveyable as that on anything else.  James Gleick's Chaos is
too well-known to need a plug here.  Benoit Mandelbrot's The Fractal
Geometry of Nature is recommended only for the strongly mathematical.
The picture-books of fractals are beyond counting.  In addition to the results
of his work on aesthetics, Prof. Sprott's book Strange Attractors
contains details of procedures for rapidly making fractal pictures.  It is
interesting to compare abstract fractals with the pictures in James O'Brien,
Design by Accident (New York: Dover, 1964).

Manuel De Landa, War in the Age of Intelligent Machines. New
York: Zone Books, 1991 (distributed by the MIT Press).  Interesting military
history and fascinating, horrifying reports on the latest Pentagon uses of
computers and AI, along with very dubious history of philosophy and ideas,
smothered throughout in an appalling mis-use of technical terms from dynamics.
(Even as metaphors, they don't make much sense.)  Alas, I can't find a better
book on the subject.



Things to Do

Expand the military section.

Consider whether any finite cognitive entity (ugh! cogitator?
cognitator? --- double ugh! --- knower!) wouldn't be forced to be
selective, and hierarchically selective at that.  (Selection I think is a
necessary consequence of finitude; but hierarchies and combinations a la James
seem to follow more from needs to accomplish some purposes quickly, i.e. from
functions.  Cf. Dennett in Elbow Room (especially the discussion
of Laplace's Vast and Considerable Intellect) and Simon in
Sciences of the Artificial on the nature of ``artifacts.''



(Sat Mar 18 20:42:22 CST 1995)



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anthropic agrees to pay $1.5B to settle lawsuit with book authors]]></title>
            <link>https://www.nytimes.com/2025/09/05/technology/anthropic-settlement-copyright-ai.html?unlocked_article_code=1.jk8.bTTt.Zir9wmtPaTp2&amp;smid=url-share</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45142885</guid>
        </item>
    </channel>
</rss>