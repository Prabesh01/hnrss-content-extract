<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Tue, 02 Sep 2025 20:30:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[iNaturalist keeps full species classification models private]]></title>
            <link>https://github.com/inaturalist/inatVisionAPI</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107939</guid>
            <description><![CDATA[Contribute to inaturalist/inatVisionAPI development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[computervision
We're doing some computer vision stuff at iNat.
models
iNaturalist makes a subset of its machine learning models publicly available while keeping full species classification models private due to intellectual property considerations and organizational policy. We provide “small” models trained on approximately 500 taxa, including taxonomy files and a geographic model, which are suitable for on-device testing and other applications. Additionally, researchers have independently developed and released open-source models based on iNaturalist data, which can be found in various model distribution venues (for example Hugging Face or Kaggle).
os x dependencies

brew install libmagic

python

python3 -m venv venv
source ./venv/bin/activate
pip3 install -U pip
pip3 install -r requirements.txt

installation
Here's a rough script for OS X assuming you already have homebrew, Python, and virtualenv installed.
# Get dependencies
brew install libmagic

# Get the repo
git clone git@github.com:inaturalist/inatVisionAPI.git
cd inatVisionAPI/

# Set up your python environment
python3 -m venv venv
source venv/bin/activate
pip3 install -U pip
pip3 install -r requirements.txt

# Copy your config file (and edit, of course)
cp config.yml.example config.yml

# Run the app
python app.py

Now you should be able to test at http://localhost:6006 via the browser.
Notes
If the device you're installing on has AVX extensions (check flags in /proc/cpuinfo), try compiling tensorflow for better performance:
https://www.tensorflow.org/install/install_sources
This is a good idea on AWS or bare metal, but won't make a difference on Rackspace due to them using an old hypervisor.
If you're not compiling, install tensorflow from pip: pip install tensorflow
If the device you're installing on has AVX2 or SSE4, install pillow-simd for faster image resizing:
pip install pillow-simd if you only have SSE4, or CC="cc -mavx2" pip install pillow-simd if you have AVX2. I saw a significant increase in performance from pillow to pillow-simd with SSE4, less of an increase for AVX2.
otherwise, install pillow from pip: pip install pillow
tensorflow seems to want to compile against your system copy of numpy on OS X regardless of the virtualenv, so if you see stupid errors like ImportError: numpy.core.multiarray failed to import, try running deactivate to get out the virtualenv, then pip install -U numpy or somesuch to update your system copy of numpy. Then source inatvision-venv/bin/activate to get back in your virtualend and try again.
Some performance data from a 15" MBP, 2.5GHz i7:



task
pip tensorflow
compiled tensorflow
compiled tensorflow + pillow-simd




100x medium.jpg
25 seconds
17 seconds
15 seconds


100x iphone photos
81 seconds
72 seconds
46 seconds



The larger the images coming into the pipeline, the more important optimized resize (like pillow-simd) is.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Amazon must face US nationwide class action over third-party sales]]></title>
            <link>https://www.reuters.com/legal/government/amazon-must-face-us-nationwide-class-action-over-third-party-sales-2025-09-02/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107891</guid>
        </item>
        <item>
            <title><![CDATA[Civics Is Boring. So, Let's Encrypt Something]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3703126</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107505</guid>
            <content:encoded><![CDATA[
	
		
		

	





		The Bike Shed
	

December 2, 2024Volume 22, issue 5 


  
	
			
				
				PDF
			
		



 
   
  IT professionals can either passively suffer political solutions or participate in the process to achieve something better. 
  Poul-Henning Kamp 
  It's a common trope in entertainment for some character to deliver a nonlinear response to something seemingly trivial, only for that to later prove to have been a vitally important clue. So, that room the janitor won't let anybody into? Right, that isn't actually a storage closet, but instead it's the Portal to Hell. Governments have a quirk like that in the sense that you can get away with a lot of crap—in particular, if it looks like it might benefit the economy—But Nobody Messes with Fundamental Human Rights, OK? 
  As I write this, the founder of the encrypted communication service Telegram is under arrest in France. And, depending on where you get your news, he's either a freedom fighter subject to political persecution or a criminal mastermind getting his due. He probably is a bit of both, but he's under arrest now because he messed with the Fundamental Human Rights of people in France. 
  I'll spare you a long civics lesson, but I will provide two important clues to figure out what is going on with politicians and encryption right now. First, when legislators write laws to protect human rights, they decide who has to take responsibility for the problem, and what happens if they fail to lift the burden. So, if you're present when somebody falls off a ladder, the law has made it your problem to try to save that person's life. If you witness a crime, the law has made it your problem to tell the truth about what you saw in court. Similarly, if you publish something that somebody else wrote, the law makes you responsible for ensuring it doesn't endanger national security. 
  Second clue: Judges are superusers. To perform their job, which is to correct wrongs, judges are empowered to write court orders that sanction otherwise illegal violations of human rights. So, a judge who is convinced you're about to kill somebody can unleash the police to follow you everywhere in hopes of preventing that crime. Similarly, a judge who thinks your computer system contains information related to financial crimes can allow the police to hack that system. Likewise, a judge who thinks you're stalking your ex can order you to stay out of a certain part of town. And if there doesn't seem to be any other way to keep you from harming somebody else's human rights, you can be jailed. 
  Then, should you fail to comply with a court order, that's considered contempt of court and can be addressed with punishments far more severe than most people imagine, since court orders are deemed to be crucially important to the maintenance of law and order. What's more, a judge who becomes convinced you are planning a crime or human-rights violation—or have participated in one—can order that the privacy of your communications be violated as part of a search for evidence. 
  The problem for law enforcement in all this is that modern computer-aided encryption is fast, effortless, omnipresent, and unbreakable, thus negating many of these efforts. This is the frustration law-enforcement types are referring to whenever they complain about "criminals going dark." It's also what leads some politicians to say silly things about "banning encryption." 
  It's not as if people didn't communicate in code previously, if only to save on telco expenses. But this used to be slow, bothersome, and error prone, which limited usage and left law enforcement with places to insert the knife—so it was somewhat tolerated. 
  IT libertarians have gone so far as to set up "offshore" services that employ encryption specifically designed to make it impossible for anyone to comply with a court order. So, because the Internet is global, now even petty criminals in Hoople, North Dakota, can effortlessly prevent judges from employing their superuser privileges. 
  This is a direct, in-your-face challenge to any state that considers itself to be a nation built on laws. Predictably, a response delivered with all due force is certain to come. The United Nations' new "cybercrime" treaty, readied for signatures at the time of this writing, is very much focused on how to get court orders to work quickly and efficiently across borders. Bear in mind that international bodies don't fashion treaties like this unless they think an urgent response is vital. 
  Which means we, as IT professionals, now have a choice to make. We can either sit by passively and suffer the consequences of whatever ill-conceived solution the politicians cook up for us, or we can participate in the process in hopes of achieving a less awful solution. 
  In terms of what might be done in that way, here's one straw-man proposal to consider. 
  First, we provide legislators with the essential technical tools. 
  We can make it possible for one side of a TLS protocol negotiation to declare, "I'll deal with court orders related to this communication," in such a way that law enforcement can find out where to send the court order for their wiretap without learning more than they already know. 
  Moreover, parties to a TLS connection should be able to insist that the session key starts with a certain number of zero bits. If the other party thinks that isn't good enough, the TLS handshake fails. 
  Then the legislators can get to work. First, they'll need to make it a crime to force or trick anyone into using stronger encryption than they consent to, no matter how that might be done. (Note that IT liberalists who claim encryption is a human right never realize this should also include the right not to be forced to use encryption against one's will.) 
  Second, they'll need to lay out what it takes for an attestation to handle court orders to be validated—along with the consequences for noncompliance. This will probably be something along the lines of: "The attestation must be signed with Interpol's or XYZ government's certificate." 
  Third, it will need to be legislated that, if the other end attested to handling court orders or if the session key requires fewer than N bits to brute force, you will not be subject to any adverse treatment for using encryption. (N is a political choice since the hardware that law enforcement will need in order to brute-force the N bits will be paid for out of your taxes. Don't argue here; take it up with your politicians.) 
  Then, fourth and finally (drum roll, please!), they'll need to allow courts to jail the accused until: (a) the communication has been decrypted by someone; (b) the maximum penalty for the charged crime has been exceeded; or (c) the court decides to release the accused. 
  Following a bit of implementation work, your browser or mobile phone will then work as follows: 
  You'll configure your jurisdiction—for example, USA, EU, or China—so that the browser will know how to validate attestations from the other end. 
  Whenever you connect to a site that attests, you'll be able to use any kind of encryption with any key size, and since almost all commercial sites, such as your bank, already are legally required to keep records and respond to court orders, they'll have no trouble attesting. 
  Should you contact a site that does not attest—be it Crimes R Us in Elbonia or your Homeowner Association's "50 Rules for Appropriate Lawn Maintenance," your browser will keep you out of jail by refusing to use a session key longer than the N legal bits. 
  If for some reason, however, you think that isn't nearly enough encryption, you'll also be at liberty to go into your browser settings to select whatever session key size you are willing to use—provided, of course, that the other end accepts that as well. 
  The slider should probably be graduated in units of time, days, weeks, months, and years since what you're really setting is the length of time you're willing to rot in jail while refusing to comply with a court order. 
  It goes without saying that you'll suffer no ill consequences even if you set the slider to "eternity," provided you keep a logfile of all your session keys and then hand them over whenever a court order demands it. Just make sure you don't lose that file. 
  Companies can also set up client-side proxies that attest to handling court orders and insist upon proper session key sizes, according to company policy, so their employees won't even have to think about it. 
  Which is to say that this straw-man proposal, in theory, ought to make everybody happy. What's not to like? Law enforcement will have ways to gain access to communications, provided they can convince a judge it's necessary. All important communications will be able to continue using the same strength of encryption they use today. Communications that didn't require encryption in the first place, like that HOA guide to proper lawn maintenance, will be able to employ sufficient encryption to prevent trivial wiretapping, but nothing strong enough to prevent brute-force access should a judge decide that's necessary. And if legislators think that too much or too little encryption is being brute-forced, they can always revise the law to change N. 
  IT libertarians, meanwhile, will have the freedom to encrypt any way they please, and they can even throw away their session keys if they so choose, but they won't be able to force anyone else to do so. If they try, they'll have to stand up in court for it—just like that IT libertarian who's currently in French custody. 
  In reality, I expect that law enforcement will demand more access and that IT libertarians will consider any kind of compromise to be treasonous. So, no, I do not expect my proposed compromise has any chance of adoption whatsoever. 
  But, then, don't tell me 10 or 20 years from now that we didn't have any other options. 
   
  Poul-Henning Kamp has haunted the Unix world for 40 years and written a lot of widely used open-source software, including bits of FreeBSD and the Varnish HTTP Cache. Living in Denmark with his wife, two cats, and three lawn-mower robots, he remains unconvinced that an older/wiser correlation exists. 
  Copyright © 2024 held by owner/author. Publication rights licensed to ACM.  
  

	
	
		
	
	Originally published in Queue vol. 22, no. 5—
 	
	Comment on this article in the ACM Digital Library
	
































More related articles:

	  
	  Jinnan Guo, Peter Pietzuch, Andrew Paverd, Kapil Vaswani - Trustworthy AI using Confidential Federated Learning
	  
	  The principles of security, privacy, accountability, transparency, and fairness are the cornerstones of modern AI regulations. Classic FL was designed with a strong emphasis on security and privacy, at the cost of transparency and accountability. CFL addresses this gap with a careful combination of FL with TEEs and commitments. In addition, CFL brings other desirable security properties, such as code-based access control, model confidentiality, and protection of models during inference. Recent advances in confidential computing such as confidential containers and confidential GPUs mean that existing FL frameworks can be extended seamlessly to support CFL with low overheads.
	  
	  

	  
	  Raluca Ada Popa - Confidential Computing or Cryptographic Computing?
	  
	  Secure computation via MPC/homomorphic encryption versus hardware enclaves presents tradeoffs involving deployment, security, and performance. Regarding performance, it matters a lot which workload you have in mind. For simple workloads such as simple summations, low-degree polynomials, or simple machine-learning tasks, both approaches can be ready to use in practice, but for rich computations such as complex SQL analytics or training large machine-learning models, only the hardware enclave approach is at this moment practical enough for many real-world deployment scenarios.
	  
	  

	  
	  Matthew A. Johnson, Stavros Volos, Ken Gordon, Sean T. Allen, Christoph M. Wintersteiger, Sylvan Clebsch, John Starks, Manuel Costa - Confidential Container Groups
	  
	  The experiments presented here demonstrate that Parma, the architecture that drives confidential containers on Azure container instances, adds less than one percent additional performance overhead beyond that added by the underlying TEE. Importantly, Parma ensures a security invariant over all reachable states of the container group rooted in the attestation report. This allows external third parties to communicate securely with containers, enabling a wide range of containerized workflows that require confidential access to secure data. Companies obtain the advantages of running their most confidential workflows in the cloud without having to compromise on their security requirements.
	  
	  

	  
	  Charles Garcia-Tobin, Mark Knight - Elevating Security with Arm CCA
	  
	  Confidential computing has great potential to improve the security of general-purpose computing platforms by taking supervisory systems out of the TCB, thereby reducing the size of the TCB, the attack surface, and the attack vectors that security architects must consider. Confidential computing requires innovations in platform hardware and software, but these have the potential to enable greater trust in computing, especially on devices that are owned or controlled by third parties. Early consumers of confidential computing will need to make their own decisions about the platforms they choose to trust.
	  
	  










	
	
	
	© ACM, Inc. All Rights Reserved.
	

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Kafka Replication Protocol with KIP-966]]></title>
            <link>https://github.com/Vanlightly/kafka-tlaplus/blob/main/kafka_data_replication/kraft/kip-966/description/0_kafka_replication_protocol.md</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107353</guid>
            <description><![CDATA[TLA+ specifications for Kafka related algorithms. Contribute to Vanlightly/kafka-tlaplus development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[
          
            


                
      

      
          

                
                    
  
      
      
          
            GitHub Copilot
          
        Write better code with AI
      

    


                    
  
      
      
          
            GitHub Spark
              
                New
              
          
        Build and deploy intelligent apps
      

    


                    
  
      
      
          
            GitHub Models
              
                New
              
          
        Manage and compare prompts
      

    


                    
  
      
      
          
            GitHub Advanced Security
          
        Find and fix vulnerabilities
      

    


                    
  
      
      
          
            Actions
          
        Automate any workflow
      

    


                    
                
              
          

                
                    
  
      
      
          
            Codespaces
          
        Instant dev environments
      

    


                    
  
      
      
          
            Issues
          
        Plan and track work
      

    


                    
  
      
      
          
            Code Review
          
        Manage code changes
      

    


                    
  
      
      
          
            Discussions
          
        Collaborate outside of code
      

    


                    
  
      
      
          
            Code Search
          
        Find more, search less
      

    


                
              
          

      



                
      

      



                
      

      
                    Explore
                    
  
      Learning Pathways

    


                    
  
      Events & Webinars

    


                    
  
      Ebooks & Whitepapers

    


                    
  
      Customer Stories

    


                    
  
      Partners

    


                    
  
      Executive Insights

    


                
              



                
      

      
              

                
                    
  
      
      
          
            GitHub Sponsors
          
        Fund open source developers
      

    


                
              
              

                
                    
  
      
      
          
            The ReadME Project
          
        GitHub community articles
      

    


                
              
              
          



                
      

      

                
                    
  
      
      
          
            Enterprise platform
          
        AI-powered developer platform
      

    


                
              



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A gentle introduction to CP/M]]></title>
            <link>https://eerielinux.wordpress.com/2025/08/28/a-gentle-introduction-to-cp-m/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107249</guid>
            <description><![CDATA[[New to Gemini? Have a look at my Gemini FAQ.] This article was bi-posted to Gemini and the Web; Gemini version is here: gemini://gemini.circumlunar.space/users/kraileth/neunix/2025/gentle_introduc…]]></description>
            <content:encoded><![CDATA[
	

	
		[New to Gemini? Have a look at my Gemini FAQ.]
This article was bi-posted to Gemini and the Web; Gemini version is here: gemini://gemini.circumlunar.space/users/kraileth/neunix/2025/gentle_introduction_cpm.gmi
This article is just what the headline promises: an introduction to the CP/M operating system. No previous knowledge of 1970s and early ’80s operating systems is required. However, some familiarity with Linux or a BSD-style operating system is assumed, as the setup process suggested here involves using a package manager and command-line tools. But why explore CP/M in the 2020s? There are (at least) two good reasons: 1) historical education 2) gaining a better understanding of how computers actually work.
Last year I wrote two articles about CP/M after having taken a first look at it:
A journey into the 8-Bit microcomputing past: Exploring the CP/M operating system – part 1
A journey into the 8-Bit microcomputing past: Exploring the CP/M operating system – part 2
These were written with a focus on the first reason; I had (partially) read the manuals and tried out a few commands in an emulator (as well as done a little bit of research). I wrote an outsider’s look at CP/M and covered the various versions that were released and some of their notable features.
This article is different. It’s for readers who want to get started with CP/M themselves. Expect a practical introduction to get familiar enough with the platform to be able to explore a wealth of historic software, often enough ground-breaking and influential.
Getting Ready (Installing an Emulator)
Last time I had tried out a couple of Z80 emulators and found Udo Munk’s z80pack to be the one I liked best. It’s not widely packaged; only FreeBSD includes it, but using the package requires some setup. Other options like YAZE exist, but it’s more work to get the original CP/M working on them whereas z80pack comes with disk images of various CP/M versions.
Of course you can compile the emulator and tools yourself. But to simplify the setup process, I’ve created a port for Ravenports, a universal package system for POSIX operating systems. Via Ravenports, the emulator and tools I use here will soon be available as binary packages for the following operating systems (in the future additional platforms might get added):

DragonFly BSD
FreeBSD
Linux (glibc-based distributions)
MidnightBSD
NetBSD

If you’re on one of those systems, you can download, inspect and run this script if you want to go that route. It will bootstrap a secondary package manager on your system, which you can use to install additional software on your machine. Those programs live in a separate portion of the filesystem (i. e. below /raven), which means that they won’t interfere with the native package manager of your platform.
The package manager is called ‘rvn’. It supports subpackages and so-called variants which is why the package names look a little complicated at first glance. The z80pack port has no variants, hence only the standard (“std”) is available, but the port is split into three subpackages: “docs”, “images” and “primary”. The first contains documentation, the second one provides the CP/M disk images and the last one is the actual emulator. A special subpackage called “set” can always be used to install all available subpackages of a project.
Tilde characters separate the fields of the package name. At the time of this writing, the complete package is ‘z80pack~set~std~1.38’ (base name, subpackage, variant, version). Package names can be shortened as long as they are unambiguous. So to install the complete package, you can run this:
# /raven/sbin/rvn install z80pack~s
This will pull in ‘z80pack~docs~std~1.38’, ‘z80pack~images~std~1.38’ and ‘z80pack~primary~std~1.38’ as well as the required dependencies.
You will have to add /raven/bin to your PATH environment variable to be able to use it. Depending on your shell of choice use setenv or export. Most people will want to do this:
% export PATH=/raven/bin:$PATH
When you installed the package, the installation message hints at a utility script that I wrote for the port. Just execute ‘runcpm’ and it will display a little help text to let you know what it does and what other command names it’s available under.
The help message from the utility script
Running any of these requires the /raven/bin to be in the PATH variable, otherwise they won’t find the actual emulator binary. Of course you can also modify /raven/bin/runcpm accordingly if you prefer that.
Running CP/M
If you’ve installed the emulator and have your PATH configured, you’re ready to go. You just pick a version from the list that ‘runcpm’ told you about and start the emulator. But which one to try? You can of course try out any of them, but I highly recommend to start with version 2.2 for a couple of reasons. Versions 1.x work but are pretty limited in terms of command-line editing and things like that. They are fascinating relics from an age before monitors were common and when output was usually printed on paper. You can explore them later. Version 3 is more complex (by CP/M standards) and might confuse you. CP/M 2.x is basically “classic” CP/M, a solid but simplistic OS that’s straight-forward to get into. That’s also the version used in this article, so if you want to follow along, just go with 2.2 for now.
Now that we’ve chosen a version, let’s start the emulator. So, as your user just issue
% cpm22
and that will turn your terminal into an emulated Z80 system with CP/M 2.2!
Emulated CP/M 2.2 booted up and ready
On modern systems, the boot process is too short to notice and the system is up instantly. CP/M displays ‘A>’ to let you know that it’s ready to take commands from you. The CCP (Console Command Processor) is the core component of CP/M that handles user interaction like prompting you and executing commands. Think of it as your shell.
In their simplest form, commands are just simple words or abbreviations which you type after the prompt symbol and then hit ENTER to execute them. Let’s issue our first command. Try this:
A>cls
What did it do? Right, all the previous messages are gone. And that’s no wonder: CLS is short for “CLear Screen”. The CCP is case insensitive; it doesn’t care if you input “cls”, “CLS” or “cLs” – that’s all the same thing.
You can always give empty command lines, if you want a bit of screen space between some output and the next. So pressing ENTER without first typing a command is perfectly acceptable. But let’s try a different command:
A>bye
Quitting the emulator
As you might have guessed from the name, this command is used to quit the emulator and returns you to your standard *nix terminal. It’s not the most interesting command but definitely one you will want to know about. It was added to stop z80-sim and is not part of the original CP/M.
Filename Basics
With these basics done, we can finally take a look at something more useful. Try this command:
A>dir
Output of the DIR command
DIR is short for “DIRectory” and it lists all the files recorded in the – you guessed it – directory (i. e. all the files present on the drive). CP/M’s filesystem is flat, which means that there’s no folders or subdirectories. All the files on one drive are together in one place. Files are referenced by what CP/M calls a file specification. These consist of up to four pieces of information (on 2.x).
Okay, let’s take a look at the output. We’ll ignore the “A:” for the moment. File naming follows a schema known as 8.3. This means that a filename can be 8 characters long, then a dot follows and after that it can have a type of up to three characters. Note: Terminology evolved over time. In older CP/M versions, these were known as the primary and secondary names. Later, during DOS times, they were referred to as the filename and the extension.
It’s best to think of the whole thing as the actual name of the file, i. e. the up to 8 characters, the dot, and the type. Only the first character is strictly required, though, so “A” is in fact a perfectly valid filename. Internally, CP/M will fill up the remaining characters with blanks, so this file is represented as an A, 7 spaces, a dot and another 3 spaces. Filenames, just like CCP input, are case-insensitive, too.
Other than being mindful of the maximum length, try to stick to letters and numbers for names. Many special characters are allowed, but some are reserved and must not be used. But you cannot just memorize these once: different versions of CP/M reserved a different set of special characters! The full list for CP/M 3 is this: . , ; : = ? * [ ] | ( ) / \ ! & $ + –
Some of these are not reserved in earlier versions, but again, don’t get fancy and you’ll stay out of trouble. Other than that, you need to be aware that the part before the dot is up to you entirely whereas the type is meant to hint at what kind of file this is.
Now that we understand the filename schema, let’s look at the types of files as the list provided by DIR has them. For example there’s “DUMP.COM” (DIR doesn’t display the dot for some reason), “STAT.COM”, and so on. These are both COM files, which is short for “command” files – and it means that these are executable commands (programs). 
The other types that you can see here are UTL and HLP. The former are two “util” files; these are special programs that cannot run on their own but can be loaded by CP/M’s debugger program. The other is short for “help” and was chosen because this file contains the help text for the WM program. DIR uses colons to delimit one column of files from the others.
Now that you know that the COM files are executable commands, DIR basically gave you a list of what programs are available for you to run. But wait a moment! There’s CLS.COM and BYE.COM – but where’s DIR.COM? Good catch. There’s actually two kinds of commands: those like CLS, which exists as COM file on a drive, and the others like DIR. The first kind are called transient commands, the others are built-ins like your Unix shell’s ‘echo’ command. DIR and a few others are part of the CCP and do not exist as separate programs. (Well, in CP/M 3 DIR.COM does exist, even though the command is a built-in, too! That’s because the transient offers additional functionality over the standard command. But like I said before, CP/M 3 is a little different.)
File Specification Basics
So far we have only executed programs which serve a pretty simple purpose and can thus work on their own. Time to take the next step. Let’s execute another program:
A>dump
The result is this error message:
NO INPUT FILE PRESENT ON DISK
DUMP is a tool to get a hex dump of a binary file and with the error message it is telling us that it needs an input file but couldn’t find it (in this case because we didn’t specify one!). So we need to give this program a file to operate on. Let’s have the program display a hex representation of itself. We can do that like this:
A> dump dump.com
This time the result looks much more interesting (see screenshot).
Output of the DUMP command on itself
While admittedly the output is not terribly useful for a user without a programming background, this is still an important achievement. We’ve not just executed another program, we’ve executed it on a specific file. In our little command line, DUMP is the program name like always. However after that (delimited by a space character), we’ve given it the so-called file specification (or filespec for short) to let the program know which file we want a hexadecimal representation of.
Now we will take a look at another command, TYPE. Don’t look for TYPE.COM, it’s another built-in. This is a simple command for displaying the contents of files (i. e. “typing it out” to the console). If we run it without a filespec, this happens:
A>type
?
Unlike the more verbose DUMP, this program is fairly minimal in letting you know that you screwed up. The question mark tells you “nope, doesn’t work that way!” and it’s on you to figure out what the problem is. That is, you have to know that TYPE requires a filespec to be able to type out the file’s contents, of course.
But why is it so minimal? Well, text strings are wasteful. The long error message that DUMP provides could have been used for several program instructions. And remember, that we are in an emulated environment where the machine has 64 kilobytes (!!) of memory, which is not a lot. To make matters worse, CP/M could run on machines with as little as 16k of RAM (versions 1.x at least). Since TYPE is a built-in and the whole CCP has to stay in memory all the time, putting anything in there that’s not strictly required, means stealing from the precious memory which would then no longer be available for other programs. Always keep in mind the extremely constrained environment that people had to make do with back in the day, and you’ll understand most of the design decisions that seem rather weird from today’s perspective.
But since we figured out what we did wrong, let’s try again:
A>type dump.com
This results in the following output:
!9"1W>2!QG}DrYQ|͏}͏#> ex͏#r*
e>	_>
e
 ҉0Ë7e}} :³ʳ7_<2!~ɯ2|\\FILE DUMP VERSION 1.4$
NO INPUT FILE PRESENT ON DISK$
Most of the file is unreadable garbage and some of it even consists of unprintable characters. This is why you normally use TYPE only for plain-text files and view binary files with DUMP. We can see two text strings here, though, one of which is the error message that we’ve already encountered. Given how short the program is, you can see pretty well how wasteful those text strings are!
Some commands take more than one filespec. For example REN (“REName”), another built-in. It allows you to change the name of an existing file to another. One CP/M quirk that you have to be aware of, is that it borrowed the notation from a line of DEC operating systems. It doesn’t copy / rename file 1 TO file 2 as you’re probably used to. Source and destination are inverted, so you copy / rename file 1 FROM file 2.
Let’s see what REN does when you give it only one filespec:
A>ren wm.hlp
FILE EXISTS
The command took the information that it had, tried to rename the file to itself – and couldn’t do that because, of course, that file’s name is already taken. Here’s how to rename the file to RENAME.TST (new name) from the file WM.HLP (old name):
A>ren rename.tst=wm.hlp
The equals character is required as the separator of the two filespecs. This command line doesn’t output any error, which in this case means that it succeeded. Feel free to check it with DIR, before we revert what we just did:
A>ren wm.hlp=rename.tst
More on Filespecs
Some commands like DIR can work both with and without a filespec. We’ve only done the latter so far, so it’s time to give the other option a try:
A>dir dump.com
A: DUMP COM
This will make DIR only display the file that we asked for instead of the whole list. What is this form of DIR good for? Just to check whether a specific file exists? No, there’s a much better use for it. But to understand that, we need to know a bit more about file specifications.
So far we have only used what CP/M calls unambiguous filespecs aka. unambiguous file references aka. unambiguous filenames (ufn), which refer to one particular file only. That means we’ve always used exact names with our commands so far. CP/M supports two kinds of wildcards, though, the question mark and the asterisk. You can use these to construct ambiguous filespecs aka. ambiguous file references aka. ambiguous filenames (afn) which can potentially match several files.
A question mark means any character. For example we could modify the previous command line slightly like this:
A>dir dump.?om
A: DUMP COM
The output is the same, because DUMP.COM is the only file in our directory that matched the afs. But let’s assume there were also files such as DUMP.BOM, DUMP.LOM and so on – in that case the afs would match them, too, and DIR would display all of them. You can use multiple wildcards in a filespec, so for example DUM?.C?M would still match our DUMP.COM file but also other possible files like DUMB.CIM and so on.
The asterisk is even more powerful; it doesn’t match a single character at that position, but translates to anything. For example *.COM means “any filename as long as the file has a type of COM”. You can use this to have DIR list all the available commands only and filter out any other files:
A>dir *.com
This will produce a list where our two UTL files and the HLP file are missing. You can also use something like A*.* to list all the files that begin with the letter A.
Combining these wildcards, you can do some pretty advanced but useful name matching. Think for a moment about what this example matches: ??G*.C* – it matches all files which have a filename of three or more characters where the third one is a G and which has a filetype that starts with C.
By the way, when you run DIR without any filespec, that’s the same as if you run DIR *.* – for the DIR command the universal filespec is the default.
Alright! Now for the last thing that you need to know about filespecs: they can consist of a third component in addition to the filename and type. Run these two commands and compare the output:
A>dir
A>dir a:
Hey, there’s the a: finally. And yes, the output is identical. Try another one, before we talk about what this does:
A>dir a:*.*
It’s exactly the same again! Okay, let’s take one step back and take a look at the prompt that we see all the time: A>. With the greater than character, the CCP tells you that it’s ready to let you input a command. But what’s the A all about? It refers to what CP/M calls the logged-on disk. It let’s you know that disk drive A is the one it will assume commands refer to unless told otherwise.
And that’s what we did with the A: – we requested DIR to list the files on drive A. Since that’s implicitly assumed when we don’t state it, it didn’t make any difference. And the universal filespec (*.*) is the implicit default for DIR, so in our case all of these were identical.
So let’s try out accessing a different disk for the first time, shall we? CP/M 2.2 as it is provided by z80pack consists of two disks, so we have a drive B, too. How about taking a look at which additional programs are on there? We can do that like this:
A>dir b:*.com
Here’s the output:
B: CLS COM : BYE COM : SPEED COM : SURVEY COM
B: R COM : RESET COM : W COM
As you can see, for convenience, there’s CLS.COM and BYE.COM on there, too, but also some additional programs that we haven’t seen, yet.
Drives
Since we’re on the matter, anyways, let’s talk about drives next. A useful command that we haven’t used, yet, is STAT (from STATus). You can use it to find out how much space remains on a certain disk. Let’s check that for both drives:
A>stat a:
Bytes Remaining On A: 11k
A>stat b:
Bytes Remaining On B: 168k
Seems like drive A is somewhat short on space while on B there’s still a lot of room for additional files. If we want to examine the programs on drive B, for example R.COM, we can do this:
A>dump b:r.com
However it’s a bit annoying to always have to use the full filespec including the drive, right? And that’s why the logged-on drive can be changed. We want to do some work mostly on drive B next, so let’s do that. It’s as easy as this:
A>B:
This will change the prompt to B> to let you know that now drive B is the default disk. If you for example run DIR without a filespec now, you’ll get a list of files on that drive until you change back. Let’s try to get a hex dump of one of the other programs on this drive:
B>dump w.com
DUMP?
Huh? What’s this? Well, the CCP let’s you know that it has no idea what you’re talking about. Remember that unlike DIR the DUMP command is a transient. It’s on drive A and it was readily available so far because that drive was the logged-on default. Now we’re on drive B and there is no DUMP.COM there! So to get the hex dump that we were looking for, we can do this:
B>a:dump w.com
That works! But while we don’t have to include B: for the filespec anymore, now we have to include A: to run the command… So we have merely traded one little headache for another. But there’s a solution to this, of course! Let’s take a look at STAT again. It is not only able to tell you about the remaining space on a disk, it can also give you information about a file. Let’s use it to take a look at DUMP.COM on drive A:
B>a:stat a:dump.com
Recs Bytes Ext Acc
3 1k 1 R/W A:DUMP.COM
Bytes Remaining On A: 11k
Okay, looks like the file takes up 3 records in the filesystem which is equivalent to a size of 1k. That’s a fairly small program and we have more than enough space to simply copy it over to drive B. That’s what we will use PIP (from “Peripheral Interchange Program”) for. Remember the syntax of REN? For PIP it’s similar and unlike REN it can actually copy files rather than renaming them and supports doing so across different devices as well. Here’s how we copy over DUMP.COM from drive A:
B>a:pip b:dump.com=a:dump.com
Think about this command line for a second. Do you see which part of a filespec is unnecessary? Exactly, since we have drive B logged-on, we could also have used A:PIP DUMP.COM=A:DUMP.COM instead for the same result. Check with DIR whether the file was copied over if you wish. Now we can simply run the program from the current disk which is much more convenient:
B>dump w.com
Great! Now let’s assume we’re done with exploring programs with DUMP and are eventually running out of space. We need to clean up now and then. Removing files is what the ERA (from “ERAse”) command is good at. To get rid of our additional DUMP.COM on the current drive, we can issue the following commands:
B>dir dump.com
B: DUMP COM
B>era dump.com
B>dir dump.com
NO FILE
That’s 1k of space reclaimed. It may not sound like much, but as we all know, even small files do add up. Oh, and you cannot only run out of space on a drive. You can also run out directory entries! CP/M 2.2 supports up to 64 files on any drive, which is a lot of files, but at the same time not an exceptionally high limit, either.
Control Characters
Let’s change the logged-on drive back to A now:
B>a:
Next we’re going to use STAT again but on its own rather than on a drive or a file:
A>stat
A: R/W, Space: 11k
B: R/W, Space: 168k
This is pretty useful for getting a quick overview. Keep in mind, though, that it will only display information about drives that you have accessed in your current session! If you use STAT the next time after you just started the emulator, it won’t know about drive B, yet. Now let’s do something stupid and try to list files on a non-existing drive:
A>dir c:
Bdos Err On C: Bad Sector
BDOS (or Basic Disk Operating System) is the OS component responsible for disks and filesystems. And it rightfully complains that there’s an error on drive C. You cannot simply acknowledge the error or something; if you press ENTER, the error is simply repeated. The system is in a state from which it cannot recover.
What you have to do in this case is sending the ^C control character (press CTRL-C to produce it). This will make CP/M perform a warm start and you get the CCP prompt back. Never try to change the logged-on drive to a none-existing one, though! In that case a warm start is not enough and you will have to kill the emulator from your host system. Historically warm starts were also required if you physically changed the diskette in a drive.
There’s a couple of other control sequences that are useful to know about. For example if you typed a longer command line and change your mind (or mistyped something right at the beginning), it is useful to press CTRL-U which invalidates the current command line. You can simply press ENTER afterwards and the CCP will ignore what you typed. CP/M 2.x supports a more useful control character, though, CTRL-X, which will simply erase the current command line, allowing you to try again right away.
If you are for example using TYPE to display a longer text file, the contents will rush by on the screen. In case you’re interested not in the end of the file but in some section in the middle, you’re supposed to press CTRL-S to suspend further printing until another key is pressed. This may have worked back in the day (and you can probably still use it if you configure the emulator to run at a slower speed), but it’s not a particularly great mechanic for today.
CTRL-Z means end-of-input. It’s not used on the command line but some programs like the editor ED make use of it.
There’s a few more, but the last one that I want to cover is CTRL-E. Sending this control character causes a carriage return without executing the command line. This is useful if you have to enter a very long command line which won’t fit on a single line. Now this might surprise you since so far all of our command lines have been rather short. But they don’t necessarily have to be! For example, PIP can be used to concatenate multiple files into one. If you’ve got a lot of long filenames, the resulting command line might run over the terminal width.
Other Things to be Aware of
We’ve already covered a bit of ground here and you should have a good idea of basic CP/M usage that you can build upon. But while the known unknowns can be annoying, it’s usually the unknown unknowns that actually bite you. So let’s at least convert some of the latter to the former, shall we?
CP/M 2.x supports 16 so-called user areas, which can be used to organize files, so in a way the previously made statement about the filesystem being flat with all files in one location is not entirely correct. It’s good to know that they exist, but by default only user area 0 is being used and that’s what you may want to stick with.
What is typed after a command name is called a command trail in CP/M lingo. We’ve only used filespecs here, but there’s another thing: parameters. These don’t refer to files but modify the behavior of the command. Unfortunately, they are not standardized! For example, STAT uses dollar notation and PIP expects parameters in square brackets. Here’s a few examples:
A>stat b:speed.c $R/O
This instructs STAT to set the read-only flag for the file instead of displaying file information. STAT also accepts a few special keywords, too, like DSK: which will make it display detailed disk information (see screenshot).
Output of the STAT command displaying disk information
The trouble with PIP is that it can only copy things FROM a different user area and not TO one. This means that when you switch the user area, you need to have PIP available there to copy something else over. But how do you do that without PIP? Well, first you need to load the program into memory. The debugger DDT can do that for us, but then instead of actually debugging it, we’re leaving the application by issuing “G0” at its dash prompt.
A>ddt pip.com
DDT VERS 2.2
NEXT PC
1E00 0100
-g0
PIP is now loaded into RAM, and DDT was nice enough to tell us that the next free memory address is 1E00, which in turn means that PIP occupies 1C memory pages. Converted to decimal, that’s 29 pages. Knowing that, we can change to another user area, e. g. number 8 using the USER build-in:
A>user 8
A>dir
NO FILE
As you can see, it’s empty. Now we can use the build-in command SAVE to write the contents of the 29 memory pages into a file:
A>save 29 pip.com
A>dir
A: PIP COM
Here’s the result: we have PIP! Which means we can use it to copy over another file to this user area, using the G parameter for PIP with user area number 0 from where we want to copy it:
A>pip stat.com=stat.com[g0]

DISK WRITE ERROR: =STAT.COM[G0]
Oops. Or rather, we can’t. Got any guesses why that is?
A>user 0
A>stat stat.com

Recs Bytes Ext Acc
40 5k 1 R/W A:STAT.COM
Bytes Remaining On A: 3k
Of course! The STAT program we wanted to copy over is 5k in size, but the drive has only 3k left…
Having seen these limitations, you probably understand why I suggest not bothering with user areas as you’re getting started with CP/M. I wanted to at least touch on them, though, so you don’t get the feeling that you’re missing out!
Another thing that you should know exists are devices. CP/M reserves some three-letter abbreviations for device names. If you ever wondered, why to this day you cannot create a file called CON in Windows, that’s because it’s CP/M’s reserved device name for the console. PUN is for a paper punch, RDR is a paper tape reader, LST is for a printer and so on. You could use STAT to switch input or output to different devices and PIP supports them, too. This allowed you to read files from the RDR or print a file by copying it to LST.
With this information you should be good to generally find your way around in CP/M. You cannot really do much with it, yet, but we’ll take care of that another time. Tune in again if you like.
What’s next?
I haven’t decided whether I’ll write another CP/M article next (it would make sense, though) or if it will be something else. In autumn, I definitely want to get back to my CBSD series, though!
			

	

	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenAI: Vijaye Raji to Become CTO of Applications with Acquisition of Statsig]]></title>
            <link>https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106981</guid>
        </item>
        <item>
            <title><![CDATA[ICE obtains access to Israeli-made spyware that hack phones and encrypted apps]]></title>
            <link>https://www.theguardian.com/us-news/2025/sep/02/trump-immigration-ice-israeli-spyware</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106903</guid>
            <description><![CDATA[Trump administration contract with Paragon Solutions gives immigration agency access to one of the most powerful stealth cyberweapons]]></description>
            <content:encoded><![CDATA[US immigration agents will have access to one of the world’s most sophisticated hacking tools after a decision by the Trump administration to move ahead with a contract with Paragon Solutions, a company founded in Israel which makes spyware that can be used to hack into any mobile phone – including encrypted applications.The Department of Homeland Security first entered into a contract with Paragon, now owned by a US firm, in late 2024, under the Biden administration. But the $2m contract was put on hold pending a compliance review to make sure it adhered to an executive order that restricts the US government’s use of spyware, Wired reported at the time.That pause has now been lifted, according to public procurement documents, which list US Immigration and Customs Enforcement (Ice) as the contracting agency.It means that one of the most powerful stealth cyberweapons ever created – which was produced outside the US – is now in the hands of an agency that has repeatedly been accused by civil and human rights groups of violating people’s due process rights.The story was first reported by the journalist Jack Poulson on his All-Source Intelligence Substack newsletter.Neither Paragon nor Ice immediately responded to a request for comment.When it is successfully deployed against a target, the hacking software – called Graphite – can hack into any phone. By essentially taking control of the mobile phone, the user – in this case, Ice – can not only track an individual’s whereabouts, read their messages, look at their photographs, but it can also open and read information held on encrypted applications, like WhatsApp or Signal. Spyware like Graphite can also be used as a listening device, through the manipulation of the phone’s recorder.An executive order signed by the Biden administration sought to establish some guardrails around the US government’s use of spyware. It said that the US “shall not make operational use of commercial spyware that poses significant counterintelligence or security risks to the United States government or significant risks of improper use by a foreign government or foreign person”. The Biden administration also took the extraordinary step of placing one of Paragon’s rival spyware makers, NSO Group, on a commerce department blacklist, saying the company had knowingly supplied foreign governments to “maliciously target” the phones of dissidents, human rights activists and journalists.Paragon has sought to differentiate itself from NSO Group. It has said that, unlike NSO – which previously sold its spyware to Saudi Arabia and other regimes – that it only does business with democracies. It has also said it has a no tolerance policy and will cut off government clients who use the spyware to target members of civil society, such as journalists. Paragon refuses to disclose who its clients are and has said it does not have insight into how its clients use the technology against targets.Spyware makers like Paragon and NSO have said their products are intended to be used to prevent crime and terrorist attacks. But both companies’ software has been used in the past to target innocent people, including individuals who have been perceived to be government enemies.John Scott-Railton, a senior research at the Citizen Lab at the University of Toronto, who is one of the world’s leading experts on cases in which spyware like Graphite has been abused by governments, said in a statement that such tools “were designed for dictatorships, not democracies built on liberty and protection of individual rights”.“Invasive, secret hacking power is corrupting. That’s why there’s a growing pile of spyware scandals in democracies, including with Paragon’s Graphite,” he said, referring to a controversy in Italy that erupted late last year.Paragon broke off its ties to Italy after it was revealed that 90 people, including journalists and members of civil society, in two dozen countries, had been targeted with the spyware. The individuals who were targeted by the Italian government included human rights activists who have been critical of Italy’s dealings with Libya. Several journalists were also targeted, though it is still unclear who ordered those hacking attacks.The US government has in the past resisted using spyware technology made outside the US because of concerns that any company that sells technology to multiple government agencies around the world represents a potential security risk.“As long as the same mercenary spyware tech is going to multiple governments, there is a baked-in counterintelligence risk. Since all of them now know what secret surveillance tech the US is using, and would have special insights on how to detect it and track what the US is doing with it,” Scott-Railton said. “Short of Paragon cancelling all foreign contracts, I’m not sure how this goes away.”]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Physically based rendering from first principles]]></title>
            <link>https://imadr.me/pbr/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106846</guid>
            <description><![CDATA[In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.]]></description>
            <content:encoded><![CDATA[
    
    
    In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.
    Chapter 1: What is light?
    We are all familiar with light: it’s the thing that allows us to see the world, distinguish colors and textures, and keeps the universe from being a dark, lifeless void. But precisely defining what light is has proven to be a tricky question.Throughout history, many philosophers (and later, physicists) studied light in an effort to demystify its nature. Some ancient Greeks considered it to be one of the four fundamental elements that composed the universe: beams of fire emanating from our eyes.
    Descartes proposed that light behaved like waves, while Newton thought that it consisted of tiny particles of matter called corpuscles.

    Each of these more or less scientific theories explained some aspects of light's behavior, but none could account for all of them in a single, unified framework. That was until the 1920s when physicists came up with quantum electrodynamics. This theory is, as of now, the most accurate way to describe every interaction of light and matter.

    You can hover the diagram below to see which light's phenomena can be explained using each model:


    
            Quantum Optics
            
                Electromagnetic Optics
                
                    Wave Optics
                    
                        Ray Optics
                    
                
            
        
    
        
            Reflection / Refraction / Transmission
            Diffraction
            Interference
            Polarization
            Dispersion
            Fluorescence
            Phosphorescence
        
    

    

    For the purpose of computer graphics, the ray optics model is accurate enough at simulating light interactions. But for the sake of scientific curiosity, we will explore some aspects of the other models, starting with electromagnetism.

    The Electric force
    One of the fundamental properties of matter is the electric charge, and it comes in two types: positive and negative.Charges determine how particles interact: charges of the same type repel each other, while opposite charges attract.
    The amount of force affecting two charged particles is calculated using Coulomb's law:
    


    Where  is a constant,   and  are the quantities of each charge, and  is the distance between them.

    You can drag around these charges to see how the electric force affects them:
    
    Every charge contributes to the electric field, it represents the force exerted on other charges at each point in space. We can visualize the electric field with a  or a  :
    
    
    Another way to visualize the electric field is by coloring each point in space with a color gradient representing the force experienced by a small charge at that point:
    

    Special relativity and magnetism
    Imagine a moving object carrying a positive electric charge placed under a cable carrying an electrical current.
    From , the object and the negative charges in the wire are moving, and since the positive and negative charges in the cable compensate each other, the object doesn't experience any force.
    In the , it appears to be static alongside the negative charges, while the positive charges are moving to the left, and the object still doesn't get affected by any force.
    Now if we take into account , the moving charges in the wire appear "stretched" due to relativistic effects, causing a change in the distribution of charges. This stretching leads to a repulsive force between the object and the wire, which we interpret as magnetism.
    
    
    Maxwell's equations
    Maxwell's equations describe how electric and magnetic fields are created and interact with each other. We will focus on the third and fourth equations.
    Maxwell's third equation, known as Faraday's law of induction, shows how changing magnetic fields can generate electric currents.An example of this is moving a magnet inside a coil, which induces an electric current in the wire due to the changing magnetic field.
    This is the principle behind electric generators: Mechanical energy (like the flow of steam) is used to move magnets inside coils (a turbine), converting it to electrical energy through electromagnetic induction.
    By moving the magnet left and right, we can see the voltmeter picking up a current and the electric charges in the coil moving back and forth:
    
    
        Show magnetic field
            
        
        Slide the magnet
        
    

    
    Maxwell's fourth and final equation, Ampère's Law, illustrates how electric currents (moving charges) produce magnetic fields around them. This is the basis of how electromagnets function:
    
    
        Voltage: 0 volts
        
    
    
    Together, these laws demonstrate how electric and magnetic fields are interdependent. A changing magnetic field generates an electric field, and a changing electric field generates a magnetic field.
    This continuous cycle enables self-sustaining, self-propagating electromagnetic waves, which can travel through space without requiring a medium.
    Electromagnetic radiation
    Electromagnetic radiation consists of waves created by synchronized oscillations of electric and magnetic fields. These waves travel at the speed of light in a vacuum.
    The amplitude of a wave determines the maximum strength of its electric or magnetic field. It represents the wave's intensity or "brightness". In quantum terms, a higher amplitude corresponds to a greater number of photons.
    The frequency of a wave determines the energy of the individual photons that compose it. Higher frequencies correspond to shorter wavelengths and more energetic photons.
    
    Amplitude
    
    Frequency
    

    When the wavelength falls between approximately 400 nm and 700 nm, the human eye perceives it as visible light.
    While other wavelengths are invisible to the human eye, many are quite familiar in everyday life.
For example, microwaves are used for Wi-Fi and cooking, X-rays are used in medical imaging, and radio waves enable communication.
Some insects, like bees, can see ultraviolet light, which helps them locate flowers by revealing hidden patterns and markings created by specialized pigments, such as flavonoids, that reflect UV wavelengths.
On the other end of the spectrum, gamma rays are highly energetic and can be dangerous, they are generated by radioactive decay, nuclear bombs, and space phenomena like supernovas.
    
    Frequency
    

    Generating Light
    There are many ways for light to be generated, the two most common ones we encounter everyday are incandescence and electroluminescence.

    Incandescence is the process by which a material emits visible light due to high temperature. It is how incandescent lightbulbs and the sun generates light.
    An incandescent lightbulb produces light through the heating of a filament until it starts glowing. The filament is made of tungsten, an element with a high melting point, high durability, and a positive temperature coefficient of resistance, which means its resistance increases with temperature.

    When we increase the current flowing through the filament, it starts heating up (Due to Joule heating), which increases the resistance in turn causing more heat to get dissipated. This feedback loop stabilizes at around 2500°C.

    This heat makes the electrons in the filament wiggle and collide with each other, releasing photons in the process. This radiation can be approximated as Black-body radiation.



Voltage


The Sun also generates light by incandescence, but unlike the lightbulb's filament glowing via Joule heating, the Sun’s energy is produced by nuclear fusion in the core, where hydrogen nuclei fuse to form helium and release photons as gamma rays.These photons travel from the core through the radiative zone, getting absorbed and remitted countless times while shifting to longer wavelengths. After hundreds of thousands of years of bouncing around, the photons make it to the surface of the Sun, called the photosphere, where they get radiated away.
Most (~49%) of the sun's emissions are in infrared, which is responsible for the heat we get on Earth, ~43% is visible light and the ~8% left is ultraviolet.

An interesting fact is that illustrations of the Sun's cross-section typically depict the interior with bright orange or yellow colors. However, if we could actually see a cross-section of the Sun, even the hottest regions like the core would appear dark and opaque, because the radiation generated there isn't in the visible spectrum.





Another way to generate light is by electroluminescence, this is the phenomenon that powers LEDs

The main component of a light-emitting diode is a semiconductor chip. Semiconductors are materials whose electrical conductivity can be modified by mixing them with impurities in a process known as doping.

Depending on the type of impurity (called the dopant) used in the mix, the semiconductor can be turned into either an n-type, which has extra electrons freely moving around, or a p-type, which has a lack of electrons and instead carrying an electron "hole", also moving around and acting as a positive charge.

When you stick a p-type and an n-type semiconductor side by side, they form a p-n junction. When a current flows through the junction, the electrons and the holes recombine and emit photons in the process.


Aside from incandescence and electroluminescence, which are the two most common sources of light we encounter in everyday life, light can come from other places. Some materials glow when exposed to ultraviolet radiation, others absorb that radiation and re-emit it after some time. Some animals like fireflies use special enzymes to produce light. You can read this page to learn more about other sources of luminescence.

Chapter 2: Abstracting Away

In the previous chapter, we examined the nature of light and the various methods by which it can be emitted, we will now focus on how it interacts with matter.

When a photon hits a material, it interacts with the electrons in the atoms and molecules of that material, then two things can happen, it can either be absorbed or scattered.

The electrons occupy atomic orbitals: regions around the nucleus of the atom where an electron is most likely to be found. A higher orbital corresponds to a higher energy level of the electron.


If the photon has the energy needed to excite the electron to a higher energy level, the photon can be absorbed. Eventually the electron returns to a lower level and releases the energy as heat.

If the photon does not get absorbed, its electric field will make the electrons oscillate in return and generate secondary waves that interfere constructively and destructively with the photon waves in complicated ways.

We can simplify these complicated interactions by making a few assumptions about the material:

    The material is homogeneous, as in the material has the same properties everywhere
    The material is a perfectly smooth surface

We can use Maxwell's equations to show that such a perfect flat material splits the incoming light waves into two parts: reflected and refracted.

The angle of reflection is equal to the angle of incidence relative to the normal of the surface, as per the law of reflection:


        
        Angle 
    


The angle of refraction is determined by how much slower (or faster) light travels through the material, that speed is defined by the index of refraction, and the angle is calculated using Snell's law:






        
        
        
        Angle 
        Index of refraction 
        Index of refraction 
    


At a  and refractive indices light is no longer refracted and seems to disappear.

The amount of light that is reflected and refracted is calculated using Fresnel equations.

However, computing the full Fresnel equation in real time can be slow, so in 1994 Christophe Schlick came up with an approximation.
First we compute the reflectance at zero degrees from the normal:


Then we plug  in the approximation function for the reflectance:


The transmitted (or refracted) light simply becomes:



        
        
        Angle 
        Index of refraction 
        Index of refraction 
    

If we try the  where the refracted ray disappeared, we can now see it getting reflected back inside the medium, this is called total internal reflection.

Total internal reflection gives rise to an interesting phenomenon called Snell's window. If you dive underwater and look up, the light above the surface is refracted through a circular window 96 degrees wide, and everything outside is a reflection of the bottom of the water.


Angle



This is what it looks underwater:




The Microfacet Model

Like we saw earlier, we can explain light reflecting and refracting using different models, depending on the size of the surface irregularities we are considering.
For example, wave optics explains light interacting with matter as light waves diffracting on the surface nanogeometry.
If we zoom out a bit and use ray optics, we consider light as straight line rays that reflect and refract on the surface microgeometry. With this model we can use the optical laws we described earlier: law of reflection, Snell's law, Fresnel equations.
Now for rendering, we can zoom out even further and consider one pixel at a time, each pixel contains many microgeometry surfaces that we call a microfacet. We can use a statistical average of the microfacets in a pixel to simulate the appearance of the surface at that pixel, without considering each individual microfacet which would be unfeasible in real time.

    
        Size
        Model
        Phenomenon
    
    
        Nanogeometry
        Wave optics
        Light diffraction
    
    
        Microgeometry
        Ray optics
        Reflection/refraction, change in local normal
    
    
        Macrogeometry
        BRDF
        Statistical average over a pixel, wider cone -> more roughness
    


Here we can see a microgeometry surface, changing the roughness makes it more bumpy and the microfacets normals aren't aligned anymore:

Roughness


At the macrogeometry level, a bigger roughness value means light rays have a wider cone where they can spread out. The function that describes this cone is called bidirectional reflectance distribution function, we will discuss it in the next chapter.

Roughness


In our microfacet model, we distinguish two types of materials by the nature of their interaction with light: metals and non-metals.

Metals

Metals have a sea of free electrons that absorb light very easily when the photons enter a few nanometers deep inside the surface. The light that isn't absorbed is reflected equally across the visible light spectrum, this is why metals have that distinct "silvery" gray color.
Notable exceptions are gold, copper, osmium and caesium.



Changing the roughness of a metal only changes its specular reflection, making it more or less mirror-like. But there is no diffuse reflection at all.

Roughness


Non-metals

Also called dielectrics, these are materials that do not conduct electricity (insulators). They include plastic, wood, glass, water, diamond, air...



When a photon hits a dielectric material, it only gets absorbed if it's energy matches the electron's energy in the material. So light either gets reflected, and the specular reflection depends on the roughness of the surface.
The light can also get refracted inside the dielectric material, it bounces around and interacts with the pigments inside the material until it exits the surface, this is called diffuse reflection.


Roughness


Spectral Power Distribution

If we take the example of a red apple. When we shine a white light (which contains all visible wavelengths) on it, the apple's pigments (anthocyanins) absorb most of the wavelengths like violet, blue and green wavelengths, thus decreasing the intensity of those colors from the light. The remaining wavelengths, mostly red, gets scattered off the apple's surface making us perceive the apple as red.



We can characterize the incoming light by describing the amount of energy it carries at each wavelength using a function called the Spectral Power Distribution or SPD for short.
For example, below is the SPD for D65, a theoretical source of light standardized by The International Commission on Illumination (CIE). It represents the spectrum of average midday light in Western Europe or North America:

We can compare this SPD to AM0, which is the measured solar radiation in outer space before entering Earth's atmosphere. Notice the absence of a dip in the ultraviolet range:


And here is the SPD of a typical tungsten incandescent light:


Spectral Reflectance Curve
The SPD shows us how much of each "color" a light is composed of. Another interesting function we can look at is called the spectral reflectance curve, which shows the fraction of incident light reflected by an object at each wavelength, effectivly representing the color of said object.
Going back to our apple example, since it reflects most of its light in the red wavelength, its spectral reflectance curve might look like this:


The light we see is the combination of the light spectral power distribution with the object spectral reflectance.
If we shine a light on our red apple, depending on the wavelengths of the light, the final color we see changes. A  makes the apple appear red, because it's like multiplying the apple's color by one. We get the same result with a , because the apple reflects mostly in the red spectrum.However if we shine a , besides the leaf, the rest of the apple doesn't reflect any light, thus appearing black.
On the top right you can see the SPD of the flashlight, under it the reflectance curve of the apple, and the resulting reflected light below it:



If we now add a banana and shine a , we can obviously tell the apple and the banana apart, one being red while the other is yellow.But what happens when the light is ? Both objects appear reddish to our eyes, because the banana doesn't have any green light to reflect, making it lose its yellow color. This phenomenon is called metamerism.
You can display the  or the  :





There are different types of metamerism, depending on when it happens during the light transport process. The apple and banana example is called illuminant metamerism, where objects that reflect light differently appear the same under some specific illumination.
Observer metamerism is when objects appear different between observers, a good example of this is colorblindness.

Chapter 3 : The Rendering equation




 is the outgoing light at point  to the direction 
 is the incoming light at point  from the direction 
The BRDF (Bidirectional reflectance distribution function) is a function that tells use how much of the incoming light  is reflected to the outgoing direction  at point , this function characterizes the surface of our material.
The dot product is called the cosine term.




The rendering equation gives us the light reflected towards a direction  at a point  by summing all the incoming lights  at that point coming from direction  in the hemisphere , weighted by the BRDF at that point and the cosine term.

Let's peel off this equation step by step, starting with the easiest part:

Lambert's cosine law

When a beam of light hits a surface, the area it touches is inversly proportional to the cosine of the angle of incidence. When the angle of incidence is , the area is at minimum and the intensity is concentrated, but the more  the angle gets, the larger the area and the intensity gets spread out.



Angle


The BRDF

The BRDF is arguably the most important part of the rendering equation, it characterizes the surface of our material and its appearance. This is where the we can apply the microfacet theory and energy conservation to make our rendering model physically based.

It takes as input the incoming  and outgoing  light direction, and the roughness of the surface . It equals the diffuse and the specular components weighted by their respective coefficients  and .
There are many different BRDFs, the most common in realtime rendering is the Cook-Torrance specular microfacet model combined with Lambertian diffuse model.



The lambertian diffuse component is the diffuse color, called albedo, multiplied by the cosine factor. But since we already have the cosine factor in the rendering equation, the diffuse equation becomes: 



The Cook-Torrance specular component itself has three components: the normal distribution function , the geometric function  and the Fresnel equation .

Normal Distribution Function

The normal distribution function is an approximation of the number of microfacets oriented in such a way that they will reflect light from the incoming direction  to the outgoing direction .

The one we will use is the Trowbridge-Reitz GGX function:



 is the halfway vector between the incoming and outgoing directions, we calculate it like this:




Roughness


Geometric Function

Some incoming rays get occluded by some microfacets before they get a chance to bounce off to the outgoing direction, this is called shadowing. Other rays get occluded by microfacets on their way to the outgoing direction, this is called masking. The geometric function approximates this effect.

Here we can see the shadowed rays in red and the masked rays in blue. The yellow rays succesfully reflected to the outgoing direction:

Angle


We will use the Schlick-GGX geometric function:




Where:



Roughness


Fresnel Equation

Like we discussed in the previous chapter, we will use the Fresnel-Schlick approximation which is fast for realtime rendering and accurate enough:



Base reflectance F0


Combining everything

Now we can combine the diffuse and specular components to get our final PBR render:


Roughness
    Metallic
    Albedo


Here is a grid of spheres with different roughness and metallic values on each axis:


Usually the metallic values is either 0 or 1, but it is useful in PBR rendering to consider intermediate values to smoothly interpolate between metals and non-metals. Take this rusted metal material for example:


To be continued...

Physically based rendering is a very vast topic and there is a lot more to cover.
In the chapter about the physics of light, I omitted the quantum explanation of light's behaviour using probability amplitudes. We didn't talk about the double slit experiment or the wave-particle duality. I may cover this in the future when I learn more about it, for now I'll leave you with this quote from Richard Feynman's QED book:
The theory of quantum electrodynamics describes Nature as absurd from the point of view of common sense. And it agrees fully with experiment. So I hope you accept Nature as She is — absurd.


We didn't talk about polarization and assumed all our light sources are unpolarized, this isn't very important for general rendering but can be useful for research.

We focused on surface rendering, in the future I will cover volume rendering, subsurface scattering, effects like optical dispersion, thin-film interference/iridescence...etc

There are a lot more implementation specific details. Whether we are implementing PBR in raytracing or rasterization, we need to use optimization techniques to make the rendering faster while still being accurate. Examples that come to mind are prefiltred envmaps and importance sampling (or efficient sampling in general).

Further reading
This article is mainly based on this SIGGRAPH talk by Naty Hoffman and Physically Based Rendering: From Theory To Implementation
My main inspiration for writing interactive articles is this fantastic blog by Bartosz Ciechanowski. A lot of interactive demos in this article are similar to the ones in this post.
Other resources include LearnOpenGL, the ScienceClic youtube channel, and 3Blue1Brown of course.
I can't recommend enough the famous book QED: The Strange Theory of Light and Matter by Richard Feynman.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introduction to Ada: a project-based exploration with rosettas]]></title>
            <link>https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106314</guid>
            <description><![CDATA[by Romain Gora – Sep 01, 2025. Discover Ada through a fun, project-based tutorial! Learn the language’s clarity, safety, and modern features while building an SVG rosetta generator. A fresh, visual way to explore Ada 2022.]]></description>
            <content:encoded><![CDATA[ContextThis practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, I’ll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirograph™ toy.In this walkthrough, we’ll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.Let's dive in!A brief note on AdaThis section leans a bit more into background context, with a slightly encyclopedic flavor that's especially useful for readers new to Ada. If you're already familiar with Ada’s history and principles, feel free to joyfully skip ahead to the next section!Ada was created in the late 1970s after a call from the U.S. Department of Defense to unify its fragmented software landscape. The winning proposal became Ada, a language that's been literally battle-tested (!) and built on a deeply thought-out design that continues to evolve today.While Ada is absolutely a general-purpose programming language, it has carved out a strong niche in fields where software correctness and reliability are mission-critical:Embedded and real-time systemsAerospace and defenseRail, automotive, and aviationAny system where failure is not just a bug, but a riskIts strict compile-time checks, safety features, and clear structure make it particularly appealing when you need your software to be dependable from day one and still maintainable ten years later.Ada's design is grounded in a strong and principled philosophy:Readability over conciseness: Ada favors clarity. It avoids symbols and abbreviations in favor of full keywords, making the language more accessible and less error-prone.Strong and explicit typing: It is extremely easy to declare new types in Ada, with precise constraints, which makes it much harder to accidentally misuse data. While some functional languages share this strong typing discipline, Ada stands out by requiring the programmer to be very explicit. It uses little to no type inference.Explicit is better than implicit: Unlike many modern languages that prioritize convenience, Ada leans heavily toward precision. Most types must be explicitly named and matched.Defined semantics and minimal undefined behavior: Ada offers a level of predictability and safety unmatched in many languages. This makes it a strong choice not only for safety-critical systems, but also for codebases where long-term maintenance, verifiability, and correctness are essential.Compiler as a partner: Ada compilers are strict by design, not to frustrate, but to help the programmer write clearer, more correct code. This philosophy encourages the developer to communicate intent clearly, both to the compiler and to future readers.How the program worksSometimes the best way to figure out how something works is to start at the end. Let's do that!In this tutorial, we'll walk through how the program produces its final output — a rosetta SVG file — and use that as a way to explore how Ada's structure, type system, and tooling come together.This is a simple command-line program that generates an SVG file. You run it like this:./bin/rosettaThe idea was to create something visual: learning is more fun when there's an immediate, satisfying result and generating rosettas fits that goal perfectly.Why SVG? Because it's a lightweight and portable vector format that you can view in any modern browser. I wanted to avoid relying on a graphical library, which would have added extra weight and gone beyond the scope of this approach. And while XML isn't the most pleasant format to write by hand, generating it from code is straightforward and gives a surprisingly clean result.Tooling & setupTo build and run the project, I used Alire, the Ada package manager. It plays a similar role in the Ada ecosystem as Cargo does for Rust or npm for JavaScript. It's well-documented, and while we won't dive deep into it here, it's a solid and accessible way to manage Ada projects. I encourage anyone curious to get it from https://alire.ada.dev. Interestingly, "Alire" is also the French expression for "à lire" — which means "for reading." A fitting name for a tool that supports a language so focused on clarity and readability!Once Alire is set up, the natural next step is choosing where to write the code. You have two excellent options for your development environment. For a dedicated experience, you can download the latest release of GNAT Studio from its GitHub repository. If you prefer a more general-purpose editor, you can install the official Ada & SPARK for Visual Studio Code extension from AdaCore.As a new learner, I also kept https://learn.adacore.com close at hand. It’s a particularly clear and comprehensive resource — and I especially appreciated being able to download the ebook version and read through it on my phone.Entry pointwith Rosetta_Renderer;

procedure Main is
begin
   Rosetta_Renderer.Put_SVG_Rosettas;
end Main;There are several interesting things to notice right away:The with clause is not a preprocessor directive like in C or C++. It’s a compiled, checked reference to another package — a reliable and explicit way to express a dependency. This eliminates entire classes of bugs related to fragile #include chains, macro collisions, or dependency order issues.This procedure is not a function: it does not return a value. In Ada, procedures are used to perform actions (like printing or modifying state), and functions are used to compute and query values.The syntax is designed for readability. You’ll find begin and end here instead of {} as in C/C++, reinforcing Ada’s philosophy that clarity matters more than brevity.Put_SVG_Rosettas uses the idiomatic Pascal_Snake_Case naming style. This reflects a common Ada convention and avoids acronyms or compressed identifiers in favor of more descriptive names.The entry point is minimal but meaningful: it simply calls a procedure which generates the output we'll explore in the next sections.Geometry and computation (package Rosetta)In Ada, a package is a modular unit that groups related types, procedures, and functions. Following the convention from GNAT (the Ada compiler, part of the GNU Compiler Collection, fondly known as GCC), each package has a specification file (with the .ads extension — short for Ada Specification) and an implementation file (with the .adb extension — short for Ada Body). This clear and enforced split means you always know where to find interface definitions versus their implementation.The following code is the package specification for Rosetta. It defines the data types for the rosetta shapes and declares the public interface of operations available to manipulate them.with Ada.Strings.Text_Buffers;

package Rosetta is

   --  A mathematical description of a rosetta (specifically, a hypotrochoid).
   --  formed by tracing a point attached to a circle rolling inside another circle.
   type Hypotrochoid is record
      Outer_Radius : Float;     --  Radius of the fixed outer circle.
      Inner_Radius : Float;     --  Radius of the rolling inner circle.
      Pen_Offset   : Float;     --  From the center of the inner circle to the drawing point.
      Steps        : Positive;  --  Number of steps (points) used to approximate the curve.
   end record;

   --  A 2D coordinate in Cartesian space.
   type Coordinate is record
      X_Coord, Y_Coord : Float;
   end record
     with Put_Image => Put_Image_Coordinate;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate 
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class; 
      Value  : Coordinate);

   --  A type for an unconstrained array of 2D points forming a curve.
   --  The actual bounds are set when an array object of this type is declared.
   type Coordinate_Array is array (Natural range <>) of Coordinate;

   --  Computes the coordinates of the rosetta curve defined by Curve (a hypotrochoid).
   --  Returns a centered array of coordinates.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array;

end Rosetta;The Rosetta package is responsible for all the math and curve computation. It defines:Hypotrochoid, type describing the geometry of the rosettaCoordinate, type representing points in 2D spaceCoordinate_Array, type holding a series of such pointsCompute_Points, function which calculates all the points of the curve based on the Hypotrochoid parameters and recenters them around the originThis package is focused solely on computation. It doesn’t concern itself with how the result is rendered.Fun fact for the curious: when the rolling circle rolls outside the fixed circle rather than inside, the resulting curve is called an epitrochoid.In Ada, a record is similar to a struct in C or a class with only data members in other languages. It's a user-defined type composed of named components, making it ideal for modeling structured data.Using a record for Hypotrochoid was particularly appropriate: it allows grouping all geometric parameters (outer radius, inner radius, pen offset, and steps) into a single, cohesive unit. This improves readability and maintainability. The compiler enforces correctness by ensuring all required values are present and of the expected type — reinforcing Ada’s philosophy of clarity and safety.The type Coordinate_Array is an unconstrained array type that holds a range of Coordinate records. In this context, ‘unconstrained’ simply means that we don’t define the array’s size when we declare the type. Instead, the size is defined when we declare an object of that type. This gives us the flexibility to use this type for a variety of shapes.You may also notice the use of Natural range <>. Natural is a predefined subtype of Integer that only allows non-negative values. And yes, I mean subtype: Ada’s powerful type system allows you to take an existing type and create a more specific, constrained version of it.Highlights from the .adb fileHere are a few notable aspects from the implementation (rosetta.adb) that illustrate Ada’s strengths for writing safe, clear, and structured code:Declarative and modular design: Both Generate_Point and Compute_Points are pure functions that operate only on their inputs. Their behavior is fully deterministic and encapsulated.Safe bounds and array handling: The Points array is statically bounded using (0 .. Curve.Steps), and its access is strictly safe. The compiler ensures that any index outside this range would raise an error at runtime. This immediate error is a feature, not a bug. It stops silent memory corruption and security flaws by ensuring the program fails predictably and safely at the source of the problem.Use of constants for robustness: Variables such as Pi, R_Diff, and Ratio are declared as constant, enforcing immutability. This helps ensure clarity of intent and prevents accidental reassignment, a common source of subtle bugs in more permissive languages. Ada encourages this explicit declaration style, promoting safer code.with Ada.Numerics;
with Ada.Numerics.Elementary_Functions;

use Ada.Numerics;
use Ada.Numerics.Elementary_Functions;

package body Rosetta is

   --  Computes a single point on the hypotrochoid curve for a given angle Theta.
   --  Uses the standard parametric equation of a hypotrochoid.
   function Generate_Point (Curve : Hypotrochoid; Theta : Float) return Coordinate is
      R_Diff : constant Float := Curve.Outer_Radius - Curve.Inner_Radius;
      Ratio  : constant Float := R_Diff / Curve.Inner_Radius;
   begin
      return (
              X_Coord => R_Diff * Cos (Theta) + Curve.Pen_Offset * Cos (Ratio * Theta),
              Y_Coord => R_Diff * Sin (Theta) - Curve.Pen_Offset * Sin (Ratio * Theta)
             );
   end Generate_Point;

   --  Computes all the points of the hypotrochoid curve and recenters them.
   --  The result is an array of coordinates centered around the origin.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array is
      Points : Coordinate_Array (0 .. Curve.Steps);
      Max_X  : Float := Float'First;
      Min_X  : Float := Float'Last;
      Max_Y  : Float := Float'First;
      Min_Y  : Float := Float'Last;
      Offset : Coordinate;
   begin
      --  Computes raw points and updates the bounding box extents.
      for J in 0 .. Curve.Steps loop
         declare
            Theta : constant Float := 2.0 * Pi * Float (J) / Float (Curve.Steps) * 50.0;
            P     : constant Coordinate := Generate_Point (Curve, Theta);
         begin
            Points (J) := P;
            Max_X := Float'Max (Max_X, P.X_Coord);
            Min_X := Float'Min (Min_X, P.X_Coord);
            Max_Y := Float'Max (Max_Y, P.Y_Coord);
            Min_Y := Float'Min (Min_Y, P.Y_Coord);
         end;
      end loop;

      --  Computes the center offset based on the bounding box.
      Offset := (
                 X_Coord => (Max_X + Min_X) / 2.0,
                 Y_Coord => (Max_Y + Min_Y) / 2.0
                );

      --  Recenters all points by subtracting the center offset.
      for J in Points'Range loop
         Points (J).X_Coord := @ - Offset.X_Coord;
         Points (J).Y_Coord := @ - Offset.Y_Coord;
      end loop;

      return Points;
   end Compute_Points;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class;
      Value  : Coordinate)
   is   
      X_Text : constant String := Float'Image (Value.X_Coord);
      Y_Text : constant String := Float'Image (Value.Y_Coord);
   begin
      Output.Put (X_Text & "," & Y_Text);
   end Put_Image_Coordinate;

end Rosetta;On style: strict and predictable (and satisfying!)Ada is one of those rare languages that not only compiles your code but asks you to write it properly. With the compiler switch -gnaty, you can enforce a comprehensive set of style rules, many of which are stricter than what you'd see in most languages.This includes things like:No trailing whitespace at the end of linesNo consecutive blank linesProper indentation and alignment of keywords and parametersA space before “(“ when calling a procedure or functionConsistent casingAt first, this can feel surprisingly strict. But once you get used to it, the benefits are clear: it helps enforce a consistent and clean coding style across a codebase. That in turn improves readability, reduces ambiguity, and leads to more maintainable programs.Rather than leaving formatting up to personal taste or optional linter tools, Ada integrates this attention to detail into the compilation process itself. The result is not only more elegant: it's genuinely satisfying. And you can do even more with GNATcheck and GNATformat but it’s outside of the scope of this post.Outputting to SVG (package Rosetta_Renderer)The Rosetta_Renderer package is responsible for producing the SVG output. It defines a single high-level procedure:package Rosetta_Renderer is

   --  Renders a predefined set of rosettas into an SVG output.
   procedure Put_SVG_Rosettas;

end Rosetta_Renderer;This procedure generates an SVG file directly. It takes care of formatting the SVG structure (header, shapes, animations, and footer) and calls into the math logic defined in the Rosetta package to generate point data.This separation of concerns is deliberate and beneficial: the math logic doesn’t need to know anything about SVG, and the renderer doesn’t care how the coordinates were generated.Now let's talk about the body of the package... but not for long. We're keeping it brief because its core is essentially the SVG plumbing required to draw and animate the curves, so we'll skip the fine details. And for those who enjoy seeing how the sausage is made, I've made the fully commented source code available for you right here.The procedure Put_Path handles the creation of the SVG path. Its main job is to take an array of coordinates and write the corresponding command string to the d attribute of a <path> element. In SVG, this attribute defines the geometry of the shape. The code iterates over each coordinate, using M (moveto) for the first point and L (lineto) for all the others to draw the connecting lines.--  Puts coordinates to a single SVG path string ("d" attribute).
   procedure Put_Path (Stream : File_Type; Points : Coordinate_Array) is
   begin
      Put (Stream, "M "); -- Moves the pen without drawing.
      for J in Points'Range loop
         declare 
            Coord_Text : constant String := Coordinate'Image (Points (J));
         begin   
            Put (Stream, Coord_Text);
            if J < Points'Last then
               Put (Stream, " L "); --  Draws a line.
            end if;
         end;
      end loop;
   end Put_Path;AfterwordThis small project was an enjoyable and useful way to get back into Ada. It helped me reconnect with the language’s main strengths and refamiliarize myself with its tools and design. It was a great reminder of how fun, easy to learn, and remarkably modern Ada can be, especially for developers focused on building robust, maintainable, and efficient software.I hope this short walkthrough gives a good idea of that feeling, whether you're already into Ada or just starting to explore it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Microsoft rewarded for security failures with another US Government contract]]></title>
            <link>https://www.theregister.com/2025/09/02/microsoft_rewarded_for_security_failures/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106295</guid>
            <description><![CDATA[: Free Copilot for any agency who actually wants it]]></description>
            <content:encoded><![CDATA[
Microsoft, the latest tech firm to agree to big software discounts for the US government, is digging even deeper into its bargain bin than the competition by offering a year of free Copilot access to government agencies willing to put up with its other problem products. 
The General Services Administration (GSA) announced its new deal with Microsoft on Tuesday, describing it as a "strategic partnership" that could save the federal government as much as $3.1 billion over the next year. The GSA didn't mention specific discount terms, but it said that services, including Microsoft 365, Azure cloud services, Dynamics 365, Entra ID Governance, and Microsoft Sentinel, will be cheaper than ever for feds. 
That, and Microsoft's next-gen Clippy, also known as Copilot, is free to access for any agency with a G5 contract as part of the new deal, too. That free price undercuts Google's previously cheapest-in-show deal to inject Gemini into government agencies for just $0.47 for a year.

    

The GSA made this Microsoft deal as part of its OneGov initiative, which seeks to centralize purchasing of products and services used across the government under a single contract. While the agency intends for OneGov to extend across the federal government, the first phase of the program focuses exclusively on IT contracts. 

        


        

Though it only announced OneGov in April, the GSA has awarded contracts under the plan at a rapid pace, with Oracle the first firm to sign a deal in July. That agreement includes a 75 percent discount on its products to government agencies. 
The agency wrote many of the other OneGov contracts to get AI products into the hands of government agencies. OpenAI and Anthropic both made deals with the GSA in August to provide a year of their services to agencies for $1 each, which Google undercut later last month.

        

Even Box made an AI discount deal with the federal government, though it didn't disclose pricing. Outside of AI offerings, Amazon Web Services inked its own OneGov deal with the GSA to offer discounted cloud services through 2028. 
With the exception of AWS, all the other OneGov deals that have been announced so far have a very short shelf life, with most expirations at the end of 2026. Critics of the OneGov program have raised concerns that OneGov deals have set government agencies up for a new era of vendor lock-in not seen since the early cloud days, where one-year discounts leave agencies dependent on services that could suddenly become considerably more expensive by the end of next year. 
Nicholas Chaillan, former US Air Force and Space Force chief software officer and founder of AI firm Ask Sage, told The Register in a recent conversation that he's protested the OpenAI, Anthropic, and Google deals, accusing the GSA of undermining its own rules on fair and open competition for government-wide contracts. 

        

"Pricing this low is not about serving agencies – it's about forcing dependence on a single vendor, hiding future costs, and squeezing out fair competition," Chaillan told us in an email. "What looks cheap today will leave the government with higher costs, fewer options, and greater risk tomorrow."
Chillain told us that GSA hasn't made the OneGov contracts public so they could be scrutinized for any unfair elements. We've tried obtaining copies but the GSA hasn't acknowledged those requests. As with the other OneGov contracts, what happens to the discounts after the September 2026 end of the offering isn't clear.
The GSA's press release mentioned that discounted pricing will be available for "certain products" for up to 36 months, but the terms of those discounts or the specific products available weren't mentioned. Microsoft's announcement of its new OneGov deal said those extended discounts will save the government as much as $6 billion over three years.
The GSA didn't respond to questions for this story. 
Microsoft gets rewarded for security failures – again
Like other tech giants making OneGov deals, Microsoft will likely have to burn some capital to reap the monetary rewards from government agencies who grow dependent on its cheap or free software in the next year. Unlike those other tech giants making OneGov deals, however, Microsoft is yet again being rewarded by the US government with a pathway to profit after making a massive national security mistake. 
It was mere days ago that we reported on the Pentagon's decision to formally bar Microsoft from using China-based engineers to support sensitive cloud services deployed by the Defense Department, a practice Defense Secretary Pete Hegseth called "mind-blowing" in a statement last week. 
Then there was last year's episodes that allowed Chinese and Russian cyber spies to break into Exchange accounts used by high-level federal officials and steal a whole bunch of emails and other information. That incident, and plenty more before it, led former senior White House cyber policy director AJ Grotto to conclude that Microsoft was an honest-to-goodness national security threat. None of that has mattered much, as the feds seem content to continue paying Microsoft for its services, despite wagging their finger at Redmond for "avoidable errors." 


Pentagon 'doubling down' on Microsoft despite 'massive hack,' senators complain

Microsoft eggheads say AI can never be made secure – after testing Redmond's own products

Microsoft answered Congress' questions on security. Now the White House needs to act

Google takes shots at Microsoft for shoddy security record with enterprise apps

When it comes to government customers, using China-based support staff isn't Microsoft's only sin. The company had a Sharepoint zero-day that it only "partially" addressed with July security updates. Suspected state-backed hackers used that vuln to target an unspecified "major western government," per the company.
That, senior cybersecurity and counterterrorism advisor for the Clinton and Bush II administrations Roger Cressey told us last month, is among the reasons he considers Microsoft to be a continual gift to America's foreign adversaries, as the Sharepoint issue is just "the latest episode of a decades-long process of Microsoft not taking security seriously." 
"The Chinese are so well prepared and positioned on Microsoft products that in the event of hostilities, we know for a fact that Chinese actors will target our critical infrastructure through Microsoft," Cressey told us in an interview last month. 
When asked what it had done to improve its security posture, Microsoft declined to answer any of our questions directly, instead pointing us to its press release about today's GSA deal, specifically its section on security.
Agencies are safe to adopt Microsoft software, the company said, because "these services have already achieved key FedRAMP security and compliance authorizations." FedRAMP is the government's security approval process for cloud software.
"Microsoft 365, Azure and our key AI services are authorized at FedRAMP High," the company statement says. "Microsoft 365 Copilot received provisional authorization from the US Department of Defense, with FedRAMP High expected soon."
That's not exactly reassuring considering Microsoft's products have variously been authorized for government use for years, well before many of its recent security failings that affected federal agencies. ®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[<template>: The Content Template element]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106049</guid>
            <description><![CDATA[The <template> HTML element serves as a mechanism for holding HTML fragments, which can either be used later via JavaScript or generated immediately into shadow DOM.]]></description>
            <content:encoded><![CDATA[
            
            
    Attributes
    This element includes the global attributes.

shadowrootmode

Creates a shadow root for the parent element.
It is a declarative version of the Element.attachShadow() method and accepts the same enumerated values.

open

Exposes the internal shadow root DOM for JavaScript (recommended for most use cases).

closed

Hides the internal shadow root DOM from JavaScript.


Note:
The HTML parser creates a ShadowRoot object in the DOM for the first <template> in a node with this attribute set to an allowed value.
If the attribute is not set, or not set to an allowed value — or if a ShadowRoot has already been declaratively created in the same parent — then an HTMLTemplateElement is constructed.
A HTMLTemplateElement cannot subsequently be changed into a shadow root after parsing, for example, by setting HTMLTemplateElement.shadowRootMode.
Note:
You may find the non-standard shadowroot attribute in older tutorials and examples that used to be supported in Chrome 90-110. This attribute has since been removed and replaced by the standard shadowrootmode attribute.

shadowrootclonable

Sets the value of the clonable property of a ShadowRoot created using this element to true.
If set, a clone of the shadow host (the parent element of this <template>) created with Node.cloneNode() or Document.importNode() will include a shadow root in the copy.

shadowrootdelegatesfocus

Sets the value of the delegatesFocus property of a ShadowRoot created using this element to true.
If this is set and a non-focusable element in the shadow tree is selected, then focus is delegated to the first focusable element in the tree.
The value defaults to false.

shadowrootserializable 
Experimental


Sets the value of the serializable property of a ShadowRoot created using this element to true.
If set, the shadow root may be serialized by calling the Element.getHTML() or ShadowRoot.getHTML() methods with the options.serializableShadowRoots parameter set true.
The value defaults to false.


  
    Usage notes
    This element has no permitted content, because everything nested inside it in the HTML source does not actually become the children of the <template> element. The Node.childNodes property of the <template> element is always empty, and you can only access said nested content via the special content property. However, if you call Node.appendChild() or similar methods on the <template> element, then you would be inserting children into the <template> element itself, which is a violation of its content model and does not actually update the DocumentFragment returned by the content property.
Due to the way the <template> element is parsed, all <html>, <head>, and <body> opening and closing tags inside the template are syntax errors and are ignored by the parser, so <template><head><title>Test</title></head></template> is the same as <template><title>Test</title></template>.
There are two main ways to use the <template> element.
  
    Template document fragment
    By default, the element's content is not rendered.
The corresponding HTMLTemplateElement interface includes a standard content property (without an equivalent content/markup attribute). This content property is read-only and holds a DocumentFragment that contains the DOM subtree represented by the template.
This fragment can be cloned via the cloneNode method and inserted into the DOM.
Be careful when using the content property because the returned DocumentFragment can exhibit unexpected behavior.
For more details, see the Avoiding DocumentFragment pitfalls section below.
  
    Declarative Shadow DOM
    If the <template> element contains the shadowrootmode attribute with a value of either open or closed, the HTML parser will immediately generate a shadow DOM. The element is replaced in the DOM by its content wrapped in a ShadowRoot, which is attached to the parent element.
This is the declarative equivalent of calling Element.attachShadow() to attach a shadow root to an element.
If the element has any other value for shadowrootmode, or does not have the shadowrootmode attribute, the parser generates a HTMLTemplateElement.
Similarly, if there are multiple declarative shadow roots, only the first one is replaced by a ShadowRoot — subsequent instances are parsed as HTMLTemplateElement objects.
  
    Examples
    
  
    Generating table rows
    First we start with the HTML portion of the example.
<table id="producttable">
  <thead>
    <tr>
      <td>UPC_Code</td>
      <td>Product_Name</td>
    </tr>
  </thead>
  <tbody>
    <!-- existing data could optionally be included here -->
  </tbody>
</table>

<template id="productrow">
  <tr>
    <td class="record"></td>
    <td></td>
  </tr>
</template>

First, we have a table into which we will later insert content using JavaScript code. Then comes the template, which describes the structure of an HTML fragment representing a single table row.
Now that the table has been created and the template defined, we use JavaScript to insert rows into the table, with each row being constructed using the template as its basis.
// Test to see if the browser supports the HTML template element by checking
// for the presence of the template element's content attribute.
if ("content" in document.createElement("template")) {
  // Instantiate the table with the existing HTML tbody
  // and the row with the template
  const tbody = document.querySelector("tbody");
  const template = document.querySelector("#productrow");

  // Clone the new row and insert it into the table
  const clone = template.content.cloneNode(true);
  let td = clone.querySelectorAll("td");
  td[0].textContent = "1235646565";
  td[1].textContent = "Stuff";

  tbody.appendChild(clone);

  // Clone the new row and insert it into the table
  const clone2 = template.content.cloneNode(true);
  td = clone2.querySelectorAll("td");
  td[0].textContent = "0384928528";
  td[1].textContent = "Acme Kidney Beans 2";

  tbody.appendChild(clone2);
} else {
  // Find another way to add the rows to the table because
  // the HTML template element is not supported.
}

The result is the original HTML table, with two new rows appended to it via JavaScript:
table {
  background: black;
}
table td {
  background: white;
}


  
    Implementing a declarative shadow DOM
    In this example, a hidden support warning is included at the beginning of the markup. This warning is later set to be displayed via JavaScript if the browser doesn't support the shadowrootmode attribute. Next, there are two <article> elements, each containing nested <style> elements with different behaviors. The first <style> element is global to the whole document. The second one is scoped to the shadow root generated in place of the <template> element because of the presence of the shadowrootmode attribute.
<p hidden>
  ⛔ Your browser doesn't support <code>shadowrootmode</code> attribute yet.
</p>
<article>
  <style>
    p {
      padding: 8px;
      background-color: wheat;
    }
  </style>
  <p>I'm in the DOM.</p>
</article>
<article>
  <template shadowrootmode="open">
    <style>
      p {
        padding: 8px;
        background-color: plum;
      }
    </style>
    <p>I'm in the shadow DOM.</p>
  </template>
</article>

const isShadowRootModeSupported = Object.hasOwn(
  HTMLTemplateElement.prototype,
  "shadowRootMode",
);

document
  .querySelector("p[hidden]")
  .toggleAttribute("hidden", isShadowRootModeSupported);


  
    Declarative Shadow DOM with delegated focus
    This example demonstrates how shadowrootdelegatesfocus is applied to a shadow root that is created declaratively, and the effect this has on focus.
The code first declares a shadow root inside a <div> element, using the <template> element with the shadowrootmode attribute.
This displays both a non-focusable <div> containing text and a focusable <input> element.
It also uses CSS to style elements with :focus to blue, and to set the normal styling of the host element.
<div>
  <template shadowrootmode="open">
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

The second code block is identical except that it sets the shadowrootdelegatesfocus attribute, which delegates focus to the first focusable element in the tree if a non-focusable element in the tree is selected.
<div>
  <template shadowrootmode="open" shadowrootdelegatesfocus>
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

Last of all we use the following CSS to apply a red border to the parent <div> element when it has focus.
div:focus {
  border: 2px solid red;
}

The results are shown below.
When the HTML is first rendered, the elements have no styling, as shown in the first image.
For the shadow root that does not have shadowrootdelegatesfocus set you can click anywhere except the <input> and the focus does not change (if you select the <input> element it will look like the second image).

For the shadow root with shadowrootdelegatesfocus set, clicking on the text (which is non-focusable) selects the <input> element, as this is the first focusable element in the tree.
This also focuses the parent element as shown below.

  
    Avoiding DocumentFragment pitfalls
    When a DocumentFragment value is passed, Node.appendChild and similar methods move only the child nodes of that value into the target node. Therefore, it is usually preferable to attach event handlers to the children of a DocumentFragment, rather than to the DocumentFragment itself.
Consider the following HTML and JavaScript:
  
    HTML
    <div id="container"></div>

<template id="template">
  <div>Click me</div>
</template>

  
    JavaScript
    const container = document.getElementById("container");
const template = document.getElementById("template");

function clickHandler(event) {
  event.target.append(" — Clicked this div");
}

const firstClone = template.content.cloneNode(true);
firstClone.addEventListener("click", clickHandler);
container.appendChild(firstClone);

const secondClone = template.content.cloneNode(true);
secondClone.children[0].addEventListener("click", clickHandler);
container.appendChild(secondClone);

  
    Result
    Since firstClone is a DocumentFragment, only its children are added to container when appendChild is called; the event handlers of firstClone are not copied. In contrast, because an event handler is added to the first child node of secondClone, the event handler is copied when appendChild is called, and clicking on it works as one would expect.

  
    Technical summary
    
  
    
      
        Content categories
      
      
        Metadata content,
        flow content,
        phrasing content,
        script-supporting element
      
    
    
      Permitted content
      Nothing (see Usage notes)
    
    
      Tag omission
      None, both the starting and ending tag are mandatory.
    
    
      Permitted parents
      
        Any element that accepts
        metadata content,
        phrasing content, or
        script-supporting elements. Also allowed as a child of a <colgroup>
        element that does not have a
        span attribute.
      
    
    
      Implicit ARIA role
      
        No corresponding role
      
    
    
      Permitted ARIA roles
      No role permitted
    
    
      DOM interface
      HTMLTemplateElement
    
  

  
    Specifications
    
    
      
        Specification
      
    
    
      
              HTML# the-template-element
            
    
  
  
    Browser compatibility
    
  
    See also
    
part and exportparts HTML attributes
<slot> HTML element
:has-slotted, :host, :host(), and :host-context() CSS pseudo-classes
::part and ::slotted CSS pseudo-elements
ShadowRoot interface
Using templates and slots
CSS scoping module
Declarative Shadow DOM (with html) in Using Shadow DOM
Declarative shadow DOM on web.dev (2023)

   
      
    
          ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We already live in social credit, we just don't call it that]]></title>
            <link>https://www.thenexus.media/your-phone-already-has-social-credit-we-just-lie-about-it/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106011</guid>
            <description><![CDATA[Your credit score is social credit. Your LinkedIn endorsements are social credit. Your Uber passenger rating, Instagram engagement metrics, Amazon reviews, and Airbnb host status are all social credit systems that track you, score you, and reward you based on your behavior.

Social credit, in its original economic definition, means distributing industry profits to consumers to increase purchasing power. But the term has evolved far beyond economics. Today, it describes any kind of metric that tr]]></description>
            <content:encoded><![CDATA[
        Photo by Avery Evans on UnsplashYour credit score is social credit. Your LinkedIn endorsements are social credit. Your Uber passenger rating, Instagram engagement metrics, Amazon reviews, and Airbnb host status are all social credit systems that track you, score you, and reward you based on your behavior.Social credit, in its original economic definition, means distributing industry profits to consumers to increase purchasing power. But the term has evolved far beyond economics. Today, it describes any kind of metric that tracks individual behavior, assigns scores based on that behavior, and uses those scores to determine access to services, opportunities, or social standing.Sounds dystopian, doesn’t it? But guess what? Every time an algorithm evaluates your trustworthiness, reliability, or social value, whether for a loan, a job, a date, or a ride, you're participating in a social credit system. The scoring happens constantly, invisibly, and across dozens of platforms that weave into your daily life.The only difference between your phone and China's social credit system is that China tells you what they're doing. We pretend our algorithmic reputation scores are just “user experience features.” At least Beijing admits they're gamifying human behavior.When Americans think of the "Chinese social credit system," they likely picture Black Mirror episodes and Orwellian nightmares. Citizens are tracked for every jaywalking incident, points are deducted for buying too much alcohol, and facial recognition cameras are monitoring social gatherings; the image is so powerful that Utah's House passed a law banning social credit systems, despite none existing in America.Here's what's actually happening. As of 2024, there's still no nationwide social credit score in China. Most private scoring systems have been shut down, and local government pilots have largely ended. It’s mainly a fragmented collection of regulatory compliance tools, mostly focused on financial behavior and business oversight. While well over 33 million businesses have been scored under corporate social credit systems, individual scoring remains limited to small pilot cities like Rongcheng. Even there, scoring systems have had "very limited impact" since they've never been elevated to provincial or national levels.What actually gets tracked? Primarily court judgment defaults: people who refuse to pay fines or loans despite having the ability. The Supreme People's Court's blacklist is composed of citizens and companies that refuse to comply with court orders, typically to pay fines or repay loans. Some experimental programs in specific cities track broader social behavior, but these remain isolated experiments.The gap between Western perception and Chinese reality is enormous, and it reveals something important: we're worried about a system that barely exists while ignoring the behavioral scoring systems we actually live with.You already live in social credit.Open your phone right now and count the apps that are scoring your behavior. Uber drivers rate you as a passenger. Instagram tracks your engagement patterns. Your bank is analyzing your Venmo transactions and Afterpay usage. LinkedIn measures your professional networking activity. Amazon evaluates your purchasing behavior. Each platform maintains detailed behavioral profiles that determine your access to services, opportunities, and social connections.We just don't call it social credit.Your credit score doesn't just determine loan eligibility; it affects where you can live, which jobs you can get, and how much you pay for car insurance. But traditional credit scoring is expanding rapidly. Some specialized lenders scan social media profiles as part of alternative credit assessments, particularly for borrowers with limited credit histories. Payment apps and financial services increasingly track spending patterns and transaction behaviors to build comprehensive risk profiles. The European Central Bank has asked some institutions to monitor social media chatter for early warnings of bank runs, though this is more about systemic risk than individual account decisions. Background check companies routinely analyze social media presence for character assessment. LinkedIn algorithmically manages your professional visibility based on engagement patterns, posting frequency, and network connections, rankings that recruiters increasingly rely on to filter candidates. Even dating has become a scoring system: apps use engagement rates and response patterns to determine who rises to the top of the queue and who gets buried.What we have aren't unified social credit systems…yet. They're fragmented behavioral scoring networks that don't directly communicate. Your Uber rating doesn't affect your mortgage rate, and your LinkedIn engagement doesn't determine your insurance premiums. But the infrastructure is being built to connect these systems. We're building the technical and cultural foundations that could eventually create comprehensive social credit systems. The question isn't whether we have Chinese-style social credit now (because we don't). The question is whether we're building toward it without acknowledging what we're creating.Where China's limited experiments have been explicit about scoring criteria, Western systems hide their decision-making processes entirely. Even China's fragmented approach offers more visibility into how behavioral data gets used than our black box algorithms do.You may argue there's a fundamental difference between corporate tracking and government surveillance. Corporations compete; you can switch services. Governments have monopoly power and can restrict fundamental freedoms.This misses three key points: First, switching costs for major platforms are enormous. Try leaving Google's ecosystem or abandoning your LinkedIn network. Second, corporate social credit systems increasingly collaborate. Bad Uber ratings can affect other services; poor credit scores impact everything from insurance to employment. Third, Western governments already access this corporate data through legal channels and data purchases.Social credit systems are spreading globally because they solve coordination problems. They reduce fraud, encourage cooperation, and create behavioral incentives at scale. The question isn't whether Western societies will adopt social credit (because we're building toward it). The question is whether we'll be transparent and accountable about it or continue pretending our algorithmic reputation scores are just neutral technology.Current trends suggest both systems are evolving toward more comprehensive behavioral scoring. European digital identity initiatives are linking multiple service scores. US cities are experimenting with behavioral incentive programs. Corporate platforms increasingly share reputation data. Financial services integrate social media analysis into lending decisions.If both countries evolve toward comprehensive behavioral scoring, and current trends suggest they will, which approach better serves individual agencies? One that admits it's scoring you, or one that pretends algorithmic recommendations are just helpful suggestions?When Uber can destroy your transportation access with a hidden algorithm, and when credit scores determine your housing options through opaque calculations, is that really more free than a system where you know at least some of the behaviors that affect your score?So when China's explicit social credit approach inevitably influences Western platforms, when your apps start showing you the behavioral scores they've always been calculating, when the rules become visible instead of hidden, don't panic.Because for the first time, you'll finally understand the game you've been playing all along. And knowing the rules means you can finally choose whether you want to play.
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA['World Models,' an old idea in AI, mount a comeback]]></title>
            <link>https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45105710</guid>
            <description><![CDATA[You’re carrying around in your head a model of how the world works. Will AI systems need to do the same?]]></description>
            <content:encoded><![CDATA[
    The latest ambition of artificial intelligence research — particularly within the labs seeking “artificial general intelligence,” or AGI — is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Bengio (of Mila, the Quebec Artificial Intelligence Institute) all believe world models are essential for building AI systems that are truly smart, scientific and safe. 
The fields of psychology, robotics and machine learning have each been using some version of the concept for decades. You likely have a world model running inside your skull right now — it’s how you know not to step in front of a moving train without needing to run the experiment first. 
So does this mean that AI researchers have finally found a core concept whose meaning everyone can agree upon? As a famous physicist once wrote: Surely you’re joking. A world model may sound straightforward — but as usual, no one can agree on the details. What gets represented in the model, and to what level of fidelity? Is it innate or learned, or some combination of both? And how do you detect that it’s even there at all?

It helps to know where the whole idea started. In 1943, a dozen years before the term “artificial intelligence” was coined, a 29-year-old Scottish psychologist named Kenneth Craik published an influential monograph in which he mused that “if the organism carries a ‘small-scale model’ of external reality … within its head, it is able to try out various alternatives, conclude which is the best of them … and in every way to react in a much fuller, safer, and more competent manner.” Craik’s notion of a mental model or simulation presaged the “cognitive revolution” that transformed psychology in the 1950s and still rules the cognitive sciences today. What’s more, it directly linked cognition with computation: Craik considered the “power to parallel or model external events” to be “the fundamental feature” of both “neural machinery” and “calculating machines.”
The nascent field of artificial intelligence eagerly adopted the world-modeling approach. In the late 1960s, an AI system called SHRDLU wowed observers by using a rudimentary “block world” to answer commonsense questions about tabletop objects, like “Can a pyramid support a block?” But these handcrafted models couldn’t scale up to handle the complexity of more realistic settings. By the late 1980s, the AI and robotics pioneer Rodney Brooks had given up on world models completely, famously asserting that “the world is its own best model” and “explicit representations … simply get in the way.”
It took the rise of machine learning, especially deep learning based on artificial neural networks, to breathe life back into Craik’s brainchild. Instead of relying on brittle hand-coded rules, deep neural networks could build up internal approximations of their training environments through trial and error and then use them to accomplish narrowly specified tasks, such as driving a virtual race car. In the past few years, as the large language models behind chatbots like ChatGPT began to demonstrate emergent capabilities that they weren’t explicitly trained for — like inferring movie titles from strings of emojis, or playing the board game Othello — world models provided a convenient explanation for the mystery. To prominent AI experts such as Geoffrey Hinton, Ilya Sutskever and Chris Olah, it was obvious: Buried somewhere deep within an LLM’s thicket of virtual neurons must lie “a small-scale model of external reality,” just as Craik imagined.

The truth, at least so far as we know, is less impressive. Instead of world models, today’s generative AIs appear to learn “bags of heuristics”: scores of disconnected rules of thumb that can approximate responses to specific scenarios, but don’t cohere into a consistent whole. (Some may actually contradict each other.) It’s a lot like the parable of the blind men and the elephant, where each man only touches one part of the animal at a time and fails to apprehend its full form. One man feels the trunk and assumes the entire elephant is snakelike; another touches a leg and guesses it’s more like a tree; a third grasps the elephant’s tail and says it’s a rope. When researchers attempt to recover evidence of a world model from within an LLM — for example, a coherent computational representation of an Othello game board — they’re looking for the whole elephant. What they find instead is a bit of snake here, a chunk of tree there, and some rope.
Of course, such heuristics are hardly worthless. LLMs can encode untold sackfuls of them within their trillions of parameters — and as the old saw goes, quantity has a quality all its own. That’s what makes it possible to train a language model to generate nearly perfect directions between any two points in Manhattan without learning a coherent world model of the entire street network in the process, as researchers from Harvard University and the Massachusetts Institute of Technology recently discovered. 
So if bits of snake, tree and rope can do the job, why bother with the elephant? In a word, robustness: When the researchers threw their Manhattan-navigating LLM a mild curveball by randomly blocking 1% of the streets, its performance cratered. If the AI had simply encoded a street map whose details were consistent — instead of an immensely complicated, corner-by-corner patchwork of conflicting best guesses — it could have easily rerouted around the obstructions.
        
        
Given the benefits that even simple world models can confer, it’s easy to understand why every large AI lab is desperate to develop them — and why academic researchers are increasingly interested in scrutinizing them, too. Robust and verifiable world models could uncover, if not the El Dorado of AGI, then at least a scientifically plausible tool for extinguishing AI hallucinations, enabling reliable reasoning, and increasing the interpretability of AI systems.
That’s the “what” and “why” of world models. The “how,” though, is still anyone’s guess. Google DeepMind and OpenAI are betting that with enough “multimodal” training data — like video, 3D simulations, and other input beyond mere text — a world model will spontaneously congeal within a neural network’s statistical soup. Meta’s LeCun, meanwhile, thinks that an entirely new (and non-generative) AI architecture will provide the necessary scaffolding. In the quest to build these computational snow globes, no one has a crystal ball — but the prize, for once, may just be worth the hype.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI web crawlers are destroying websites in their never-ending content hunger]]></title>
            <link>https://www.theregister.com/2025/08/29/ai_web_crawlers_are_destroying/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45105230</guid>
            <description><![CDATA[Opinion: But the cure may ruin the web....]]></description>
            <content:encoded><![CDATA[
Opinion With AI's rise, AI web crawlers are strip-mining the web in their perpetual hunt for ever more content to feed into their Large Language Model (LLM) mills. How much traffic do they account for? According to Cloudflare, a major content delivery network (CDN) force, 30% of global web traffic now comes from bots. Leading the way and growing fast? AI bots.
Cloud services company Fastly agrees. It reports that 80% of all AI bot traffic comes from AI data fetcher bots.  So, you ask, "What's the problem? Haven't web crawlers been around since 1993 with the arrival of the World Wide Web Wanderer in 1993?"  Well, yes, they have. Anyone who runs a website, though, knows there's a huge, honking difference between the old-style crawlers and today's AI crawlers. The new ones are site killers. 
Fastly warns that they're causing "performance degradation, service disruption, and increased operational costs." Why? Because they're hammering websites with traffic spikes that can reach up to ten or even twenty times normal levels within minutes. 

    

Moreover, AI crawlers are much more aggressive than standard crawlers. As the InMotionhosting web hosting company notes, they also tend to disregard crawl delays or bandwidth-saving guidelines and extract full page text, and sometimes attempt to follow dynamic links or scripts.

        


        

The result? If you're using a shared server for your website, as many small businesses do, even if your site isn't being shaken down for content, other sites on the same hardware with the same Internet pipe may be getting hit. This means your site's performance drops through the floor even if an AI crawler isn't raiding your website.
Smaller sites, like my own Practical Tech, get slammed to the point where they're simply knocked out of service. Thanks to Cloudflare Distributed Denial of Service (DDoS) protection, my microsite can shrug off DDoS attacks. AI bot attacks – and let's face it, they are attacks – not so much. 

        

Even large websites are feeling the crush. To handle the load, they must increase their processor, memory, and network resources. If they don't? Well, according to most web hosting companies, if a website takes longer than three seconds to load, more than half of visitors will abandon the site. Bounce rates jump up for every second beyond that threshold.
So when AI searchbots, with Meta (52% of AI searchbot traffic), Google (23%), and OpenAI (20%) leading the way, clobber websites with as much as 30 Terabits in a single surge, they're damaging even the largest companies' site performance.
Now, if that were traffic that I could monetize, it would be one thing. It's not. It used to be when search indexing crawler, Googlebot, came calling, I could always hope that some story on my site would land on the magical first page of someone's search results so they'd visit me, they'd read the story, and two or three times out of a hundred visits, they'd click on an ad, and I'd get a few pennies of income. Or, if I had a business site, I might sell a widget or get someone to do business with me.

        

AI searchbots? Not so much. AI crawlers don't direct users back to the original sources. They kick our sites around, return nothing, and we're left trying to decide how we're to make a living in the AI-driven web world. 
Yes, of course, we can try to fend them off with logins, paywalls, CAPTCHA challenges, and sophisticated anti-bot technologies. You know one thing AI is good at? It's getting around those walls. 
As for robots.txt files, the old-school way of blocking crawlers? Many – most? – AI crawlers simply ignore them. 
For example, Perplexity has been accused by Cloudflare of ignoring robots.txt files. Perplexity, in turn, hotly denies this accusation.  Me? All I know is I see regular waves of multiple companies' AI bots raiding my site. 
There are efforts afoot to supplement robots.txt with llms.txt files. This is a proposed standard to provide LLM-friendly content that LLMs can access without compromising the site's performance. Not everyone is thrilled with this approach, though, and it may yet come to nothing. 
In the meantime, to combat excessive crawling, some infrastructure providers, such as Cloudflare, now offer default bot-blocking services to block AI crawlers and provide mechanisms to deter AI companies from accessing their data. Other programs, such as the popular open-source and free Anubis AI crawler blocker, just attempt to slow down their visits to a, if you'll pardon the expression, a crawl. 
In the arms race between all businesses and their websites and AI companies, eventually, they'll reach some kind of neutrality. Unfortunately, the web will be more fragmented than ever. Sites will further restrict or monetize access. Important, accurate information will end up siloed behind walls or removed altogether. 
Remember the open web? I do. I can see our kids on the Internet, where you must pay cash money to access almost anything. I don't think anyone wants a Balkanized Internet, but I fear that's exactly where we're going.                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps]]></title>
            <link>https://news.ycombinator.com/item?id=45104974</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104974</guid>
            <description><![CDATA[Hey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.]]></description>
            <content:encoded><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps30 points by nickpapciak 4 hours ago  | hide | past | favorite | 24 commentsHey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.Demo video: https://www.youtube.com/watch?v=2FitSggI7tg.Right now, we have two main methods to interact with Datafruit:(1) automated infrastructure audits— agents periodically scan your environment to find cost optimization opportunities, detect infrastructure drift, and validate your infra against compliance requirements.(2) chat interface (available as a web UI and through slack) — ask the agent questions for real-time insights, or assign tasks directly, such as investigating spend anomalies, reviewing security posture, or applying changes to IaC resources.Working at FAANG and various high-growth startups, we realized that infra work requires an enormous amount of context, often more than traditional software engineering. The business decisions, codebase, and cloud itself are all extremely important in any task that has been assigned. To maximize the success of the agents, we do a fair amount of context engineering. Not hallucinating is super important!One thing which has worked incredibly well for us is a multi-agent system where we have specialized sub-agents with access to specific tool calls and documentation for their specialty. Agents choose to “handoff” to each other when they feel like another agent would be more specialized for the task. However, all agents share the same context (https://cognition.ai/blog/dont-build-multi-agents). We’re pretty happy with this approach, and believe it could work in other disciplines which require high amounts of specialized expertise.Infrastructure is probably the most mission-critical part of any software organization, and needs extremely heavy guardrails to keep it safe. Language models are not yet at the point where they can be trusted to make changes (we’ve talked to a couple of startups where the Claude Code + AWS CLI combo has taken their infra down). Right now, Datafruit receives read-only access to your infrastructure and can only make changes through pull requests to your IaC repositories. The agent also operates in a sandboxed virtual environment so that it could not write cloud CLI commands if it wanted to!Where LLMs can add significant value is in reducing the constant operational inefficiencies that eat up cloud spend and delay deadlines—the small-but-urgent ops work. Once Datafruit indexes your environment, you can ask it to do things like:  "Grant @User write access to analytics S3 bucket for 24 hours"
    -> Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrow

  "Find where this secret is used so I can rotate it without downtime"
    -> Discovers all instances of your secret, including old cron-jobs you might not know about, so you can safely rotate your keys


  "Why did database costs spike yesterday?"
    -> Identifies expensive queries, shows optimization options, implements fixes


We charge a straightforward subscription model for a managed version, but we also offer a bring-your-own-cloud model. All of Datafruit can be deployed on Kubernetes using Helm charts for enterprise customers where data can’t leave your VPC.
For the time being, we’re installing the product ourselves on customers' clouds. It doesn’t exist in a self-serve form yet. We’ll get there eventually, but in the meantime if you’re interested we’d love for you guys to email us at founders@datafruit.dev.We would love to hear your thoughts! If you work with cloud infra, we are especially interested in learning about what kinds of work you do which you wish could be offloaded onto an agent.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anthropic raises $13B Series F at $183B post-money valuation]]></title>
            <link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104907</guid>
            <description><![CDATA[Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management & Research Company and Lightspeed Venture Partners. The investment reflects Anthropic’s continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.]]></description>
            <content:encoded><![CDATA[Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management & Research Company and Lightspeed Venture Partners. The investment reflects Anthropic’s continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.Significant investors in this round include Altimeter, Baillie Gifford, affiliated funds of BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price Associates, Inc., T. Rowe Price Investment Management, Inc., WCM Investment Management, and XN.“From Fortune 500 companies to AI-native startups, our customers rely on Anthropic’s frontier models and platform products for their most important, mission-critical work,” said Krishna Rao, Chief Financial Officer of Anthropic. “We are seeing exponential growth in demand across our entire customer base. This financing demonstrates investors’ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.”Anthropic has seen rapid growth since the launch of Claude in March 2023. At the beginning of 2025, less than two years after launch, Anthropic’s run-rate revenue had grown to approximately $1 billion. By August 2025, just eight months later, our run-rate revenue reached over $5 billion—making Anthropic one of the fastest-growing technology companies in history.Anthropic’s trajectory has been driven by our leading technical talent, our focus on safety, and our frontier research, including pioneering alignment and interpretability work, all of which underpin the performance and reliability of our models. Every day more businesses, developers, and consumer power users are trusting Claude to help them solve their most challenging problems. Anthropic now serves over 300,000 business customers, and our number of large accounts—customers that each represent over $100,000 in run-rate revenue—has grown nearly 7x in the past year.This growth spans the entire Anthropic platform, with advancements for businesses, developers, and consumers. For businesses, our API and industry-specific products make it easy to add powerful AI to their critical applications without complex integration work. Developers have made Claude Code their tool of choice since its full launch in May 2025. Claude Code has quickly taken off—already generating over $500 million in run-rate revenue with usage growing more than 10x in just three months. For individual users, the Pro and Max plans for Claude deliver enhanced AI capabilities for everyday tasks and specialized projects.“Anthropic is on an exceptional trajectory, combining research excellence, technological leadership, and relentless focus on customers. We’re honored to partner with Dario and the team, and our lead investment in their Series F reflects our belief in their values and their ability to shape the future of responsible AI,” said Divesh Makan, Partner at ICONIQ. “Enterprise leaders tell us what we’re seeing firsthand—Claude is reliable, built on a trustworthy foundation, and guided by leaders truly focused on the long term.”The Series F investment will expand our capacity to meet growing enterprise demand, deepen our safety research, and support international expansion as we continue building reliable, interpretable, and steerable AI systems.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Static sites enable a good time travel experience]]></title>
            <link>https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104303</guid>
            <description><![CDATA[A static site with version control history enables me to travel into any point in the project’s past and serve the site as it was back in the day.]]></description>
            <content:encoded><![CDATA[
        
        
        
          Aug 30th, 2025
          by Juha-Matti Santala
          
        
	
          
        
	

        
          
        

        
          

  Varun wrote about
  gamifying blogging and personal website maintenance
  which reminded me of the time when
  I awarded myself some badges for blogging.



  I mentioned this to Varun who asked if I had any screenshots of what it looked
  like on my website. My initial answer was “no”, then I looked at Wayback
  Machine but there were not pictures of the badges.



  Then, a bit later it hit me. I don’t need any archived screenshots: my website
  is built with Eleventy and it's static so I can check out a git commit from
  the time I had those badges up, fire up Eleventy and see the website — as it
  was in the spring of 2021.


  That’s a beauty of a static site generator combined with my workflow of
  fetching posts from CMS before build time so each commit contains a full
  snapshot of the website.



  Comparing this to a website that uses a database for posts (like WordPress) or
  a flow where posts from CMS are not stored in version control but rather
  fetched at build time only, my solution makes time travel to (almost) any
  given moment in time a two-command operation (git checkout
  with the right commit hash and
  @11ty/eleventy serve to serve a dev
  server). I say almost because back in the day I wasn’t quite as diligent in
  commiting every change as I was deploying manually and not through version
  control automation.



  A year ago, inspired by Alex Chan’s blog post
  Taking regular screenshots of my website
  I set up a GitHub Action that takes a snapshot of my front page once a month
  to keep a record. At the time, I felt bit sad that I hadn’t started it before.
  However, now that I realised how easy it is for me to go back in time thanks
  to Eleventy and git, I’m not so worried anymore. Maybe I should do a collage
  of changes on my design one day by going through my project history.


One more major point for static site generators!



            
          If something above resonated with you, let's start a discussion about it! Email me at juhamattisantala at gmail dot com and share your thoughts. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.

        

        

        

        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Removing Guix from Debian]]></title>
            <link>https://lwn.net/SubscriberLink/1035491/d8100135a8ae4246/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45103857</guid>
            <description><![CDATA[As a rule, if a package is shipped with a Debian release, users can count on it being available [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



As a rule, if a package is shipped with a Debian release, users can
count on it being available, and updated, for the entire
life of the release. If package foo is included in the stable
release—currently Debian 13
("trixie")—a user can
reasonably expect that it will continue to be available with security
backports as long as that release is supported, though it may not be
included in Debian 14 ("forky"). However, it is likely that the
Guix package manager will soon
be removed from the repositories for Debian 13 and
Debian 12 ("bookworm", also called oldstable).

Debian has the Advanced
Package Tool (APT) for package management, of course, but Guix
offers a different approach and can be used in conjunction with other
distribution package managers. Guix is inspired by Nix's
functional package management; it offers transactional upgrades and
rollbacks, package management for unprivileged users, and more. Unlike
Nix, its packages are defined
using the Guile
implementation of the Scheme programming
language. There is also a GNU Guix distribution as well; LWN covered both NixOS and
Guix in February 2024, and looked at Nix alternative Lix in July 2024.

On June 24, the Guix project disclosed
several security vulnerabilities that affected the guix-daemon,
which is a program that is used to build software and
access the store
where successful builds are kept. Two of the vulnerabilities, CVE-2025-46415
and CVE-2025-46416,
would allow a local user to gain the privileges of any build users,
manipulate build output, as well as gain the privileges of the daemon
user. The vulnerabilities also impacted Nix
and Lix
package managers.

The disclosure blog post gave instructions on how to mitigate the
vulnerabilities by updating guix-daemon using the
"guix pull" command, but the project did not make a new
Guix release. The last actual release from the project
was version 1.4.0,
which was announced in December 2022. The Guix project has a
rolling-release model, with sporadic releases, and does not maintain
stable branches with security updates. This may not pose a problem for
users getting Guix directly from the project, but it poses some
obstacles for inclusion in other Linux distributions.

Debian package

Salvatore Bonaccorso filed a bug
against Debian's
guix package
on June 25 to report the vulnerabilities. Vagrant
Cascadian, the maintainer of the package, replied
on July 15, and said that the fixes for the security
vulnerabilities had been "commingled with a lot of other upstream
changes", and it would be "trickier than in the past" to try
to backport the fixes without the other changes in Guix. He said he
had just managed to "get something to compile" with the
security fixes applied, using a backport
repository maintained by Denis 'GNUtoo' Carikli.

Carikli had brought
up the difficulty of backporting Guix fixes on the guix-devel
mailing list on July 8. Various distributions had Guix versions
1.2.0, 1.3.0, and 1.4.0, with Debian shipping 1.2.0 and 1.4.0 and used
as the upstream for other distributions' packages:


But the Debian package maintainer has the almost impossible task to
backport all the security fixes without a community nor help behind
[maintaining it] and as things are going, this will probably lead to
the Debian guix package being removed with cascading effect for the
other distributions.


He said he had applied about 50 patches that involve guix-daemon
between the 1.4.0 release and the last-known security fix. He also
noted that his effort would probably not be suitable for Linux
distributions that ship a Guix package. He wondered what the best way
would be to collaborate on a branch that distributions could pull from
for updates.

Liam Hupfer said
that "we gave up and shipped the last commit on master mentioned in
the recent CVE disclosure" for Nix. He said he would also like to
see Guix figure out backporting patches, but could Cascadian consider
the Nix approach until then?

No, that approach would not make sense, Cascadian said. If
Guix was "truly a rolling release", then it may just not make
sense to maintain distribution packages:


Up till recently, it has always been possible to backport changes with
relative ease, but that was perhaps just lack of development... the
recent unprivileged daemon stuff really made backporting patches
harder. [...]

In the Debian release model, ideally we would avoid bringing in
unrelated patchsets (e.g. the unprivileged daemon code bringing in an
entire network stack?) but that might be too hard to pull off. Not sure
if the security team would accept a patchset that includes more than the
minimum necessary to fix the security issues.


Without a branch that contained backported patches, they would be
inclined "to recommend dropping the Guix package in Debian".

On August 27, they did just that. Cascadian filed a bug
to remove guix from Debian 11 ("bullseye"), bookworm,
and trixie. They also filed a bug to
remove the package from the upcoming forky release. Adam D. Barratt replied
and said that it would not be possible to remove guix from
bullseye, which is now an LTS
release; only updates from the security archive were now
allowed.

There are no dependencies on guix, so the removal of the
package should not affect any other Debian packages in the trixie and
bookworm releases.

What's next?

I emailed Cascadian to find out what the next steps would be for
the package in those releases. They said that the package is likely to
be removed from the upcoming point
releases for trixie and bookworm. Users who have it installed
already will be stuck at the old version, unless they take manual
steps to update. Cascadian said this was not a great outcome "but
better than keeping it available for people to install the vulnerable
version".

The guix package should not have landed in trixie at all,
Cascadian said, "my understanding was that [the bug report] should
have blocked it from being released". However, it seems there was
enough activity on the bug that prevented Debian's autoremoval
mechanisms from triggering and pulling the package from the final
release. "There apparently is not manual review of all blockers in
the current Debian release process". Even if the timing had been
better for the trixie release, though, the same fixes would need to
have been applied to bookworm and older releases.

Cascadian said that it has been a fair amount of work to keep Guix
working on Debian. For example, he has had to maintain various Guile
dependencies, and deal with the fact that Guix uses "fairly
old" GCC versions whereas Debian usually ships the latest GCC
version available for a given release. At some point, they said,
"you have to evaluate whether that work is worth it" when the
upstream provides a binary that people can install.

Guix is better for having been packaged for Debian and run through
Debian's Lintian
tool. Cascadian said that they have probably fixed more typos in Guix
than anyone else, and have found other problems while checking that
the builds of Guix are reproducible. "Any time you run a piece of
software through processes outside of the primary development
workflow, you find surprises worth fixing."

Regular releases

There is an effort in the Guix project to create yearly releases. In
May, Steve George proposed that the project adopt a Guix
Consensus Document for "Regular
and efficient releases", and it was accepted by the project in
July. It calls for a release every year in June, with a one-time
exception for a November 2025 release, and a short development
cycle for the June 2026 release. Even so, that will not provide
stable branches for Debian and other distributions to pull from; it
will just shorten the interval and feature differences between major
releases.

Debian users will, of course, still be able to use Guix by installing
it using binaries provided by the Guix project. That will, at
least potentially, leave some users out in the cold—Debian
currently provides a guix package for x86-64, Arm64, PowerPC,
RISC-V, 32-bit Arm, and 32-bit x86. The Guix project itself does does
not have RISC-V binaries, though it does cover the other
architectures.

It is fairly unusual for a package to be removed from a stable or
oldstable release. For example, the bookworm release has been out for
more than two years, but a search
of the Debian bug database only shows one
package—guix—that has the "RM" tag in the subject
to flag a package for removal. 

According to Debian's popularity
contest (popcon) statistics, there are not quite 230 systems with
Guix installed. Popcon statistics only hint at the actual number of
package installs, but assuming they are approximately accurate, then
removing Guix will not inconvenience a significant number of Debian
users. It will, however, mean that fewer people are poking at Guix
with an intent to making it work on distributions like Debian, while finding
distribution-specific bugs.


               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Little Book of Linear Algebra]]></title>
            <link>https://github.com/the-litte-book-of/linear-algebra</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45103436</guid>
            <description><![CDATA[There is hardly any theory which is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calcul...]]></description>
            <content:encoded><![CDATA[The Little Book of Linear Algebra
A concise, beginner-friendly introduction to the core ideas of linear algebra.
Formats

Download PDF – print-ready version
Download EPUB – e-reader friendly
View LaTeX – Latex source

Chapter 1. Vectors
1.1 Scalars and Vectors
A scalar is a single numerical quantity, most often taken from the real numbers, denoted by $\mathbb{R}$. Scalars are
the fundamental building blocks of arithmetic: they can be added, subtracted, multiplied, and, except in the case of
zero, divided. In linear algebra, scalars play the role of coefficients, scaling factors, and entries of larger
structures such as vectors and matrices. They provide the weights by which more complex objects are measured and
combined. A vector is an ordered collection of scalars, arranged either in a row or a column. When the scalars are real
numbers, the vector is said to belong to real $n$-dimensional space, written
$$
\mathbb{R}^n = { (x_1, x_2, \dots, x_n) \mid x_i \in \mathbb{R} }.
$$
An element of $\mathbb{R}^n$ is called a vector of dimension $n$ or an n-vector. The number $n$ is called the
dimension of the vector space. Thus $\mathbb{R}^2$ is the space of all ordered pairs of real numbers, $\mathbb{R}^3$ the
space of all ordered triples, and so on.
Example 1.1.1.

A 2-dimensional vector: $(3, -1) \in \mathbb{R}^2$.
A 3-dimensional vector: $(2, 0, 5) \in \mathbb{R}^3$.
A 1-dimensional vector: $(7) \in \mathbb{R}^1$, which corresponds to the scalar $7$ itself.

Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:
$$
\mathbf{v} = \begin{bmatrix} 2 \ 0 \ 5 \end{bmatrix} \in \mathbb{R}^3.
$$
The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.
Geometric Interpretation
In $\mathbb{R}^2$, a vector $(x_1, x_2)$ can be visualized as an arrow starting at the origin $(0,0)$ and ending at the
point $(x_1, x_2)$. Its length corresponds to the distance from the origin, and its orientation gives a direction in the
plane. In $\mathbb{R}^3$, the same picture extends into three dimensions: a vector is an arrow from the origin
to $(x_1, x_2, x_3)$. Beyond three dimensions, direct visualization is no longer possible, but the algebraic rules of
vectors remain identical. Even though we cannot draw a vector in $\mathbb{R}^{10}$, it behaves under addition, scaling,
and transformation exactly as a 2- or 3-dimensional vector does. This abstract point of view is what allows linear
algebra to apply to data science, physics, and machine learning, where data often lives in very high-dimensional spaces.
Thus a vector may be regarded in three complementary ways:

As a point in space, described by its coordinates.
As a displacement or arrow, described by a direction and a length.
As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.

Notation

Vectors are written in boldface lowercase letters: $\mathbf{v}, \mathbf{w}, \mathbf{x}$.
The i-th entry of a vector $\mathbf{v}$ is written $v_i$, where indices begin at 1.
The set of all n-dimensional vectors over $\mathbb{R}$ is denoted $\mathbb{R}^n$.
Column vectors will be the default form unless otherwise stated.

Why begin here?
Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear
transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once
vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to
subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with
powerful applications to geometry, computation, and data.
Exercises 1.1

Write three different vectors in $\mathbb{R}^2$ and sketch them as arrows from the origin. Identify their coordinates
explicitly.
Give an example of a vector in $\mathbb{R}^4$. Can you visualize it directly? Explain why high-dimensional
visualization is challenging.
Let $\mathbf{v} = (4, -3, 2)$. Write $\mathbf{v}$ in column form and state $v_1, v_2, v_3$.
In what sense is the set $\mathbb{R}^1$ both a line and a vector space? Illustrate with examples.
Consider the vector $\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$. What is special about this vector when $n$ is
large? What might it represent in applications?

1.2 Vector Addition and Scalar Multiplication
Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two
fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations
satisfy simple but far-reaching rules that underpin the entire subject.
Vector Addition
Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if
$$
\mathbf{u} = (u_1, u_2, \dots, u_n), \quad
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$
then their sum is
$$
\mathbf{u} + \mathbf{v} = (u_1+v_1, u_2+v_2, \dots, u_n+v_n).
$$
Example 1.2.1.
Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -5)$. Then
$$
\mathbf{u} + \mathbf{v} = (2+4, -1+0, 3+(-5)) = (6, -1, -2).
$$
Geometrically, vector addition corresponds to the parallelogram rule. If we draw both vectors as arrows from the
origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram
they form represents the resulting vector.
Scalar Multiplication
Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is
negative, in which case the vector is also reversed. If $c \in \mathbb{R}$ and
$$
\mathbf{v} = (v_1, v_2, \dots, v_n),
$$
then
$$
c \mathbf{v} = (c v_1, c v_2, \dots, c v_n).
$$
Example 1.2.2.
Let $\mathbf{v} = (3, -2)$ and $c = -2$. Then
$$
c\mathbf{v} = -2(3, -2) = (-6, 4).
$$
This corresponds to flipping the vector through the origin and doubling its length.
Linear Combinations
The interaction of addition and scalar multiplication allows us to form linear combinations. A linear combination of
vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k$ is any vector of the form
$$
c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_k \mathbf{v}_k, \quad c_i \in \mathbb{R}.
$$
Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of
vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.
Example 1.2.3.
Let $\mathbf{v}_1 = (1,0)$ and $\mathbf{v}_2 = (0,1)$. Then any vector $(a,b)\in\mathbb{R}^2$ can be expressed as
$$
a\mathbf{v}_1 + b\mathbf{v}_2.
$$
Thus $(1,0)$ and $(0,1)$ form the basic building blocks of the plane.
Notation

Addition: $\mathbf{u} + \mathbf{v}$ means component-wise addition.
Scalar multiplication: $c\mathbf{v}$ scales each entry of $\mathbf{v}$ by $c$.
Linear combination: a sum of the form $c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$.

Why this matters
Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector
spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving
systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound
rules.
Exercises 1.2

Compute $\mathbf{u} + \mathbf{v}$ where $\mathbf{u} = (1,2,3)$ and $\mathbf{v} = (4, -1, 0)$.
Find $3\mathbf{v}$ where $\mathbf{v} = (-2,5)$. Sketch both vectors to illustrate the scaling.
Show that $(5,7)$ can be written as a linear combination of $(1,0)$ and $(0,1)$.
Write $(4,4)$ as a linear combination of $(1,1)$ and $(1,-1)$.
Prove that if $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$,
then $(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$ for
scalars $c,d \in \mathbb{R}$.

1.3 Dot Product, Norms, and Angles
The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure
lengths, compute angles, and determine orthogonality. From this single definition flow the notions of norm and
angle, which give geometry to abstract vector spaces.
The Dot Product
For two vectors in $\mathbb{R}^n$, the dot product (also called the inner product) is defined by
$$
\mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$
Equivalently, in matrix notation:
$$
\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}.
$$
Example 1.3.1.
Let $\mathbf{u} = (2, -1, 3)$ and $\mathbf{v} = (4, 0, -2)$. Then
$$
\mathbf{u} \cdot \mathbf{v} = 2\cdot 4 + (-1)\cdot 0 + 3\cdot (-2) = 8 - 6 = 2.
$$
The dot product outputs a single scalar, not another vector.
Norms (Length of a Vector)
The Euclidean norm of a vector is the square root of its dot product with itself:
$$
|\mathbf{v}| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$
This generalizes the Pythagorean theorem to arbitrary dimensions.
Example 1.3.2.
For $\mathbf{v} = (3, 4)$,
$$
|\mathbf{v}| = \sqrt{3^2 + 4^2} = \sqrt{25} = 5.
$$
This is exactly the length of the vector as an arrow in the plane.
Angles Between Vectors
The dot product also encodes the angle between two vectors. For nonzero vectors $\mathbf{u}, \mathbf{v}$,
$$
\mathbf{u} \cdot \mathbf{v} = |\mathbf{u}| , |\mathbf{v}| \cos \theta,
$$
where $\theta$ is the angle between them. Thus,
$$
\cos \theta = \frac{\mathbf{u} \cdot \mathbf{v}}{|\mathbf{u}||\mathbf{v}|}.
$$
Example 1.3.3.
Let $\mathbf{u} = (1,0)$ and $\mathbf{v} = (0,1)$. Then
$$
\mathbf{u} \cdot \mathbf{v} = 0, \quad |\mathbf{u}| = 1, \quad |\mathbf{v}| = 1.
$$
Hence
$$
\cos \theta = \frac{0}{1\cdot 1} = 0 \quad \Rightarrow \quad \theta = \frac{\pi}{2}.
$$
The vectors are perpendicular.
Orthogonality
Two vectors are said to be orthogonal if their dot product is zero:
$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$
Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.
Notation

Dot product: $\mathbf{u} \cdot \mathbf{v}$.
Norm (length): $|\mathbf{v}|$.
Orthogonality: $\mathbf{u} \perp \mathbf{v}$ if $\mathbf{u} \cdot \mathbf{v} = 0$.

Why this matters
The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of
perpendicularity. This foundation will later support the study of orthogonal projections, Gram–Schmidt
orthogonalization, eigenvectors, and least squares problems.
Exercises 1.3

Compute $\mathbf{u} \cdot \mathbf{v}$ for $\mathbf{u} = (1,2,3)$, $\mathbf{v} = (4,5,6)$.
Find the norm of $\mathbf{v} = (2, -2, 1)$.
Determine whether $\mathbf{u} = (1,1,0)$ and $\mathbf{v} = (1,-1,2)$ are orthogonal.
Let $\mathbf{u} = (3,4)$, $\mathbf{v} = (4,3)$. Compute the angle between them.
Prove that $|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$. This
identity is the algebraic version of the Law of Cosines.

1.4 Orthogonality
Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas
in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant
properties.
Definition
Two vectors $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$ are said to be orthogonal if their dot product is zero:
$$
\mathbf{u} \cdot \mathbf{v} = 0.
$$
This condition ensures that the angle between them is $\pi/2$ radians (90 degrees).
Example 1.4.1.
In $\mathbb{R}^2$, the vectors $(1,2)$ and $(2,-1)$ are orthogonal since
$$
(1,2) \cdot (2,-1) = 1\cdot 2 + 2\cdot (-1) = 0.
$$
Orthogonal Sets
A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in
addition, each vector has norm 1, the set is called orthonormal.
Example 1.4.2.
In $\mathbb{R}^3$, the standard basis vectors
$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$
form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.
Projections
Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one
orthogonal to it. Given a nonzero vector $\mathbf{u}$ and any vector $\mathbf{v}$, the projection of $\mathbf{v}$
onto $\mathbf{u}$ is
$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\mathbf{u} \cdot \mathbf{u}} \mathbf{u}.
$$
The difference
$$
\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})
$$
is orthogonal to $\mathbf{u}$. Thus every vector can be decomposed uniquely into a parallel and perpendicular part with
respect to another vector.
Example 1.4.3.
Let $\mathbf{u} = (1,0)$, $\mathbf{v} = (2,3)$. Then
$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{(1,0)\cdot(2,3)}{(1,0)\cdot(1,0)} (1,0)
= \frac{2}{1}(1,0) = (2,0).
$$
Thus
$$
\mathbf{v} = (2,3) = (2,0) + (0,3),
$$
where $(2,0)$ is parallel to $(1,0)$ and $(0,3)$ is orthogonal to it.
Orthogonal Decomposition
In general, if $\mathbf{u} \neq \mathbf{0}$ and $\mathbf{v} \in \mathbb{R}^n$, then
$$
\mathbf{v} = \text{proj}{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}{\mathbf{u}}(\mathbf{v})\big),
$$
where the first term is parallel to $\mathbf{u}$ and the second term is orthogonal. This decomposition underlies methods
such as least squares approximation and the Gram–Schmidt process.
Notation


$\mathbf{u} \perp \mathbf{v}$: vectors $\mathbf{u}$ and $\mathbf{v}$ are orthogonal.
An orthogonal set: vectors pairwise orthogonal.
An orthonormal set: pairwise orthogonal, each of norm 1.

Why this matters
Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify
computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data
science (QR decomposition, least squares regression, PCA) rely on orthogonality.
Exercises 1.4

Verify that the vectors $(1,2,2)$ and $(2,0,-1)$ are orthogonal.
Find the projection of $(3,4)$ onto $(1,1)$.
Show that any two distinct standard basis vectors in $\mathbb{R}^n$ are orthogonal.
Decompose $(5,2)$ into components parallel and orthogonal to $(2,1)$.
Prove that if $\mathbf{u}, \mathbf{v}$ are orthogonal and nonzero,
then $(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$.

Chapter 2. Matrices
2.1 Definition and Notation
Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear
transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows
and columns.
Formal Definition
An $m \times n$ matrix is an array with $m$ rows and $n$ columns, written
$$
A =
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}.
$$
Each entry $a_{ij}$ is a scalar, located in the i-th row and j-th column. The size (or dimension) of the matrix is
denoted by $m \times n$.

If $m = n$, the matrix is square.
If $m = 1$, the matrix is a row vector.
If $n = 1$, the matrix is a column vector.

Thus, vectors are simply special cases of matrices.
Examples
Example 2.1.1. A $2 \times 3$ matrix:
$$
A = \begin{bmatrix}
1 & -2 & 4 \\
0 & 3 & 5
\end{bmatrix}.
$$
Here, $a_{12} = -2$, $a_{23} = 5$, and the matrix has 2 rows, 3 columns.
Example 2.1.2. A $3 \times 3$ square matrix:
$$
B = \begin{bmatrix}
2 & 0 & 1 \\
-1 & 3 & 4 \\
0 & 5 & -2
\end{bmatrix}.
$$
This will later serve as the representation of a linear transformation on $\mathbb{R}^3$.
Indexing and Notation

Matrices are denoted by uppercase bold letters: $A, B, C$.
Entries are written as $a_{ij}$, with the row index first, column index second.
The set of all real $m \times n$ matrices is denoted $\mathbb{R}^{m \times n}$.

Thus, a matrix is a function $A: {1,\dots,m} \times {1,\dots,n} \to \mathbb{R}$, assigning a scalar to each row-column
position.
Why this matters
Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems
of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a
single compact object can represent both numerical data and functional rules.
Exercises 2.1

Write a $3 \times 2$ matrix of your choice and identify its entries $a_{ij}$.
Is every vector a matrix? Is every matrix a vector? Explain.
Which of the following are square
matrices: $A \in \mathbb{R}^{4\times4}$, $B \in \mathbb{R}^{3\times5}$, $C \in \mathbb{R}^{1\times1}$?
Let $D = \begin{bmatrix} 1 & 0 \ 0 & 1 \end{bmatrix}$. What kind of matrix is this?
Consider the matrix $E = \begin{bmatrix} a & b \ c & d \end{bmatrix}$. Express $e_{11}, e_{12}, e_{21}, e_{22}$
explicitly.

2.2 Matrix Addition and Multiplication
Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through
addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.
Matrix Addition
Two matrices of the same size are added by adding corresponding entries. If
$$
A = [a_{ij}] \in \mathbb{R}^{m \times n}, \quad
B = [b_{ij}] \in \mathbb{R}^{m \times n},
$$
then
$$
A + B = [a_{ij} + b_{ij}] \in \mathbb{R}^{m \times n}.
$$
Example 2.2.1.
Let
$$
A = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}, \quad
B = \begin{bmatrix}
-1 & 0 \\
5 & 2
\end{bmatrix}.
$$
Then
$$
A + B = \begin{bmatrix}
1 + (-1) & 2 + 0 \
3 + 5 & 4 + 2
\end{bmatrix}
\begin{bmatrix}
0 & 2 \
8 & 6
\end{bmatrix}.
$$
Matrix addition is commutative ($A+B = B+A$) and associative ($(A+B)+C = A+(B+C)$). The zero matrix, with all entries 0,
acts as the additive identity.
Scalar Multiplication
For a scalar $c \in \mathbb{R}$ and a matrix $A = [[a_{ij}]$, we define
$$
cA = [c \cdot a_{ij}].
$$
This stretches or shrinks all entries of the matrix uniformly.
Example 2.2.2.
If
$$
A = \begin{bmatrix}
2 & -1 \\
0 & 3
\end{bmatrix}, \quad c = -2,
$$
then
$$
cA = \begin{bmatrix}
-4 & 2 \\
0 & -6
\end{bmatrix}.
$$
Matrix Multiplication
The defining operation of matrices is multiplication. If
$$
A \in \mathbb{R}^{m \times n}, \quad B \in \mathbb{R}^{n \times p},
$$
then their product is the $m \times p$ matrix
$$
AB = C = [c_{ij}], \quad c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}.
$$
Thus, the entry in the $i$-th row and $j$-th column of $AB$ is the dot product of the $i$-th row of $A$ with the $j$-th
column of $B$.
Example 2.2.3.
Let
$$
A = \begin{bmatrix}
1 & 2 \\
0 & 3
\end{bmatrix}, \quad
B = \begin{bmatrix}
4 & -1 \\
2 & 5
\end{bmatrix}.
$$
Then
$$
AB = \begin{bmatrix}
1\cdot4 + 2\cdot2 & 1\cdot(-1) + 2\cdot5 \
0\cdot4 + 3\cdot2 & 0\cdot(-1) + 3\cdot5
\end{bmatrix}
\begin{bmatrix}
8 & 9 \
6 & 15
\end{bmatrix}.
$$
Notice that matrix multiplication is not commutative in general: $AB \neq BA$. Sometimes $BA$ may not even be defined if
dimensions do not align.
Geometric Meaning
Matrix multiplication corresponds to the composition of linear transformations. If $A$ transforms vectors
in $\mathbb{R}^n$ and $B$ transforms vectors in $\mathbb{R}^p$, then $AB$ represents applying $B$ first, then $A$. This
makes matrices the algebraic language of transformations.
Notation

Matrix sum: $A+B$.
Scalar multiple: $cA$.
Product: $AB$, defined only when the number of columns of $A$ equals the number of rows of $B$.

Why this matters
Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of
equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a
vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and
networks.
Exercises 2.2

Compute $A+B$ for

$$
A = \begin{bmatrix} 2 & 3 \ -1 & 0 \end{bmatrix}, \quad
B = \begin{bmatrix} 4 & -2 \ 5 & 7 \end{bmatrix}.
$$

Find $3A$ where

$$
A = \begin{bmatrix} 1 & -4 \ 2 & 6 \end{bmatrix}.
$$

Multiply

$$
A = \begin{bmatrix} 1 & 0 & 2 \ -1 & 3 & 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 2 & 1 \ 0 & -1 \ 3 & 4 \end{bmatrix}.
$$

Verify with an explicit example that $AB \neq BA$.
Prove that matrix multiplication is distributive: $A(B+C) = AB + AC$.

2.3 Transpose and Inverse
Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties.
The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as
the undo operation for matrix multiplication.
The Transpose
The transpose of an $m \times n$ matrix $A = [a_{ij}]$ is the $n \times m$ matrix $A^T = [a_{ji}]$, obtained by swapping
rows and columns.
Formally,
$$
(A^T){ij} = a{ji}.
$$
Example 2.3.1.
If
$$
A = \begin{bmatrix}
1 & 4 & -2 \\
0 & 3 & 5
\end{bmatrix},
$$
then
$$
A^T = \begin{bmatrix}
1 & 0 \\
4 & 3 \\
-2 & 5
\end{bmatrix}.
$$
Properties of the Transpose.

$ (A^T)^T = A$.
$ (A+B)^T = A^T + B^T$.
$ (cA)^T = cA^T$, for scalar $c$.
$ (AB)^T = B^T A^T$.

The last rule is crucial: the order reverses.
The Inverse
A square matrix $A \in \mathbb{R}^{n \times n}$ is said to be invertible (or nonsingular) if there exists another
matrix $A^{-1}$ such that
$$
AA^{-1} = A^{-1}A = I_n,
$$
where $I_n$ is the $n \times n$ identity matrix. In this case, $A^{-1}$ is called the inverse of $A$.
Not every matrix is invertible. A necessary condition is that $\det(A) \neq 0$, a fact that will be developed in Chapter
6.
Example 2.3.2.
Let
$$
A = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}.
$$
Its determinant is $\det(A) = (1)(4) - (2)(3) = -2 \neq 0$. The inverse is
$$
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix}
4 & -2 \
-3 & 1
\end{bmatrix}
\begin{bmatrix}
-2 & 1 \
1.5 & -0.5
\end{bmatrix}.
$$
Verification:
$$
AA^{-1} = \begin{bmatrix}
1 & 2 \
3 & 4
\end{bmatrix}
\begin{bmatrix}
-2 & 1 \
1.5 & -0.5
\end{bmatrix}
\begin{bmatrix}
1 & 0 \
0 & 1
\end{bmatrix}.
$$
Geometric Meaning

The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between
row and column forms.
The inverse, when it exists, corresponds to reversing a linear transformation. For example, if $A$ scales and rotates
vectors, $A^{-1}$ rescales and rotates them back.

Notation

Transpose: $A^T$.
Inverse: $A^{-1}$, defined only for invertible square matrices.
Identity: $I_n$, acts as the multiplicative identity.

Why this matters
The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The
inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these
operations set the stage for determinants, eigenvalues, and orthogonalization.
Exercises 2.3

Compute the transpose of

$$
A = \begin{bmatrix} 2 & -1 & 3 \ 0 & 4 & 5 \end{bmatrix}.
$$

Verify that $(AB)^T = B^T A^T$ for

$$
A = \begin{bmatrix} 1 & 2 \ 0 & 1 \end{bmatrix}, \quad
B = \begin{bmatrix} 3 & 4 \ 5 & 6 \end{bmatrix}.
$$

Determine whether

$$
C = \begin{bmatrix} 2 & 1 \ 4 & 2 \end{bmatrix}
$$
is invertible. If so, find $C^{-1}$.

Find the inverse of

$$
D = \begin{bmatrix} 0 & 1 \ -1 & 0 \end{bmatrix},
$$
and explain its geometric action on vectors in the plane.

Prove that if $A$ is invertible, then so is $A^T$, and $(A^T)^{-1} = (A^{-1})^T$.

2.4 Special Matrices
Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their
properties allows us to simplify computations and understand the structure of linear transformations more clearly.
The Identity Matrix
The identity matrix $I_n$ is the $n \times n$ matrix with ones on the diagonal and zeros elsewhere:
$$
I_n = \begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1
\end{bmatrix}.
$$
It acts as the multiplicative identity:
$$
AI_n = I_nA = A, \quad \text{for all } A \in \mathbb{R}^{n \times n}.
$$
Geometrically, $I_n$ represents the transformation that leaves every vector unchanged.
Diagonal Matrices
A diagonal matrix has all off-diagonal entries zero:
$$
D = \begin{bmatrix}
d_{11} & 0 & \cdots & 0 \\
0 & d_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_{nn}
\end{bmatrix}.
$$
Multiplication by a diagonal matrix scales each coordinate independently:
$$
D\mathbf{x} = (d_{11}x_1, d_{22}x_2, \dots, d_{nn}x_n).
$$
Example 2.4.1.
Let
$$
D = \begin{bmatrix} 2 & 0 & 0 \ 0 & 3 & 0 \ 0 & 0 & -1 \end{bmatrix}, \quad
\mathbf{x} = \begin{bmatrix} 1 \ 4 \ -2 \end{bmatrix}.
$$
Then
$$
D\mathbf{x} = \begin{bmatrix} 2 \ 12 \ 2 \end{bmatrix}.
$$
Permutation Matrices
A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation
matrix reorders its coordinates.
Example 2.4.2.
Let
$$
P = \begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix}.
$$
Then
$$
P\begin{bmatrix} a \ b \ c \end{bmatrix} =
\begin{bmatrix} b \ a \ c \end{bmatrix}.
$$
Thus, $P$ swaps the first two coordinates.
Permutation matrices are always invertible; their inverses are simply their transposes.
Symmetric and Skew-Symmetric Matrices
A matrix is symmetric if
$$
A^T = A,
$$
and skew-symmetric if
$$
A^T = -A.
$$
Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and
cross products in geometry.
Orthogonal Matrices
A square matrix $Q$ is orthogonal if
$$
Q^T Q = QQ^T = I.
$$
Equivalently, the rows (and columns) of $Q$ form an orthonormal set. Orthogonal matrices preserve lengths and angles;
they represent rotations and reflections.
Example 2.4.3.
The rotation matrix in the plane:
$$
R(\theta) = \begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}
$$
is orthogonal, since
$$
R(\theta)^T R(\theta) = I_2.
$$
Why this matters
Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal
matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe
fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving
these simple forms.
Exercises 2.4

Show that the product of two diagonal matrices is diagonal, and compute an example.
Find the permutation matrix that cycles $(a,b,c)$ into $(b,c,a)$.
Prove that every permutation matrix is invertible and its inverse is its transpose.
Verify that

$$
Q = \begin{bmatrix} 0 & 1 \ -1 & 0 \end{bmatrix}
$$
is orthogonal. What geometric transformation does it represent?
5. Determine whether
$$
A = \begin{bmatrix} 2 & 3 \ 3 & 2 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 & 5 \ -5 & 0 \end{bmatrix}
$$
are symmetric, skew-symmetric, or neither.
Chapter 3. Systems of Linear Equations
3.1 Linear Systems and Solutions
One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally
in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language
for expressing and solving them.
Linear Systems
A linear system consists of equations where each unknown appears only to the first power and with no products between
variables. A general system of $m$ equations in $n$ unknowns can be written as:
$$
\begin{aligned}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &= b_1, \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &= b_2, \\
&\vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &= b_m.
\end{aligned}
$$
Here the coefficients $a_{ij}$ and constants $b_i$ are scalars, and the unknowns are $x_1, x_2, \dots, x_n$.
Matrix Form
The system can be expressed compactly as:
$$
A\mathbf{x} = \mathbf{b},
$$
where


$A \in \mathbb{R}^{m \times n}$ is the coefficient matrix $[a_{ij}]$,

$\mathbf{x} \in \mathbb{R}^n$ is the column vector of unknowns,

$\mathbf{b} \in \mathbb{R}^m$ is the column vector of constants.

This formulation turns the problem of solving equations into analyzing the action of a matrix.
Example 3.1.1.
The system
$$
\begin{cases}
x + 2y = 5, \\
3x - y = 4
\end{cases}
$$
can be written as
$$
\begin{bmatrix} 1 & 2 \ 3 & -1 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}
\begin{bmatrix} 5 \ 4 \end{bmatrix}.
$$
Types of Solutions
A linear system may have:


No solution (inconsistent): The equations conflict.
Example:
$
\begin{cases}
x + y = 1 \
x + y = 2
\end{cases}
$
has no solution.


Exactly one solution (unique): The system’s equations intersect at a single point.
Example: The above system with coefficient matrix $
\begin{bmatrix} 1 & 2 \ 3 & -1 \end{bmatrix}
$ has a unique solution.


Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the
same line or plane).


The nature of the solution depends on the rank of $A$ and its relation to the augmented matrix $(A|\mathbf{b})$, which
we will study later.
Geometric Interpretation

In $\mathbb{R}^2$, each linear equation represents a line. Solving a system means finding intersection points of
lines.
In $\mathbb{R}^3$, each equation represents a plane. A system may have no solution (parallel planes), one solution (a
unique intersection point), or infinitely many (a line of intersection).
In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.

Why this matters
Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit
analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify
their solutions is the first step toward systematic solution methods like Gaussian elimination.
Exercises 3.1


Write the following system in matrix form:
$
\begin{cases}
2x + 3y - z = 7, \
x - y + 4z = 1, \
3x + 2y + z = 5
\end{cases}
$


Determine whether the system
$
\begin{cases}
x + y = 1, \
2x + 2y = 2
\end{cases}
$
has no solution, one solution, or infinitely many solutions.


Geometrically interpret the system
$
\begin{cases}
x + y = 3, \
x - y = 1
\end{cases}
$
in the plane.


Solve the system
$
\begin{cases}
2x + y = 1, \
x - y = 4
\end{cases}
$
and check your solution.


In $\mathbb{R}^3$, describe the solution set of
$
\begin{cases}
x + y + z = 0, \
2x + 2y + 2z = 0
\end{cases}
$.
What geometric object does it represent?


3.2 Gaussian Elimination
To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a
simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve
the solution set.
Elementary Row Operations
On an augmented matrix $(A|\mathbf{b})$, we are allowed three operations:

Row swapping: interchange two rows.
Row scaling: multiply a row by a nonzero scalar.
Row replacement: replace one row by itself plus a multiple of another row.

These operations correspond to re-expressing equations in different but equivalent forms.
Row Echelon Form
A matrix is in row echelon form (REF) if:

All nonzero rows are above any zero rows.
Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row
above.
All entries below a leading entry are zero.

Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon
form (RREF).
Algorithm of Gaussian Elimination

Write the augmented matrix for the system.
Use row operations to create zeros below each pivot (the leading entry in a row).
Continue column by column until the matrix is in echelon form.
Solve by back substitution: starting from the last pivot equation and working upward.

If we continue to RREF, the solution can be read off directly.
Example
Example 3.2.1. Solve
$$
\begin{cases}
x + 2y - z = 3, \\
2x + y + z = 7, \\
3x - y + 2z = 4.
\end{cases}
$$
Step 1. Augmented matrix
$$
\left[\begin{array}{ccc|c}
1 & 2 & -1 & 3 \\
2 & 1 & 1 & 7 \\
3 & -1 & 2 & 4
\end{array}\right].
$$
Step 2. Eliminate below the first pivot
Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:
$$
\left[\begin{array}{ccc|c}
1 & 2 & -1 & 3 \\
0 & -3 & 3 & 1 \\
0 & -7 & 5 & -5
\end{array}\right].
$$
Step 3. Pivot in column 2
Divide row 2 by -3:
$$
\left[\begin{array}{ccc|c}
1 & 2 & -1 & 3 \\
0 & 1 & -1 & -\tfrac{1}{3} \\
0 & -7 & 5 & -5
\end{array}\right].
$$
Add 7 times row 2 to row 3:
$$
\left[\begin{array}{ccc|c}
1 & 2 & -1 & 3 \\
0 & 1 & -1 & -\tfrac{1}{3} \\
0 & 0 & -2 & -\tfrac{22}{3}
\end{array}\right].
$$
Step 4. Pivot in column 3
Divide row 3 by -2:
$$
\left[\begin{array}{ccc|c}
1 & 2 & -1 & 3 \\
0 & 1 & -1 & -\tfrac{1}{3} \\
0 & 0 & 1 & \tfrac{11}{3}
\end{array}\right].
$$
Step 5. Back substitution
From the last row:
$
z = \tfrac{11}{3}.
$
Second row:
$
y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}.
$
First row:
$
x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3.
$
So
$
x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0.
$
Solution:
$
(x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big).
$
Why this matters
Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where
solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and
machine learning.
Exercises 3.2


Solve by Gaussian elimination:
$
\begin{cases}
x + y = 2, \
2x - y = 0.
\end{cases}
$


Reduce the following augmented matrix to REF:
$
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 6 \
2 & -1 & 3 & 14 \
1 & 4 & -2 & -2
\end{array}\right].
$


Show that Gaussian elimination always produces either:

a unique solution,
infinitely many solutions, or
a contradiction (no solution).



Use Gaussian elimination to find all solutions of
$
\begin{cases}
x + y + z = 0, \
2x + y + z = 1.
\end{cases}
$


Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.


3.3 Rank and Consistency
Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are
the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the
equations, while consistency determines whether the system has at least one solution.
Rank of a Matrix
The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of
linearly independent rows or columns.
Formally,
$$
\text{rank}(A) = \dim(\text{row space of } A) = \dim(\text{column space of } A).
$$
The rank tells us the effective dimension of the space spanned by the rows (or columns).
Example 3.3.1.
For
$$
A = \begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
3 & 6 & 9
\end{bmatrix},
$$
row reduction gives
$$
\begin{bmatrix}
1 & 2 & 3 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}.
$$
Thus, $\text{rank}(A) = 1$, since all rows are multiples of the first.
Consistency of Linear Systems
Consider the system $A\mathbf{x} = \mathbf{b}$.
The system is consistent (has at least one solution) if and only if
$$
\text{rank}(A) = \text{rank}(A|\mathbf{b}),
$$
where $(A|\mathbf{b})$ is the augmented matrix.
If the ranks differ, the system is inconsistent.

If $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$ (number of unknowns), the system has a unique solution.
If $\text{rank}(A) = \text{rank}(A|\mathbf{b}) &lt; n$, the system has infinitely many solutions.

Example
Example 3.3.2.
Consider
$$
\begin{cases}
x + y + z = 1, \\
2x + 2y + 2z = 2, \\
x + y + z = 3.
\end{cases}
$$
The augmented matrix is
$$
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
2 & 2 & 2 & 2 \\
1 & 1 & 1 & 3
\end{array}\right].
$$
Row reduction gives
$$
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 2
\end{array}\right].
$$
Here, $\text{rank}(A) = 1$, but $\text{rank}(A|\mathbf{b}) = 2$. Since the ranks differ, the system is inconsistent: no
solution exists.
Example with Infinite Solutions
Example 3.3.3.
For
$$
\begin{cases}
x + y = 2, \\
2x + 2y = 4,
\end{cases}
$$
the augmented matrix reduces to
$$
\left[\begin{array}{cc|c}
1 & 1 & 2 \\
0 & 0 & 0
\end{array}\right].
$$
Here, $\text{rank}(A) = \text{rank}(A|\mathbf{b}) = 1 &lt; 2$. Thus, infinitely many solutions exist, forming a line.
Why this matters
Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency
explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and
prepare for the ideas of dimension, basis, and the Rank–Nullity Theorem.
Exercises 3.3

Compute the rank of

$$
A = \begin{bmatrix}
1 & 2 & 1 \\
0 & 1 & -1 \\
2 & 5 & -1
\end{bmatrix}.
$$

Determine whether the system

$$
\begin{cases}
x + y + z = 1, \\
2x + 3y + z = 2, \\
3x + 5y + 2z = 3
\end{cases}
$$
is consistent.


Show that the rank of the identity matrix $I_n$ is $n$.


Give an example of a system in $\mathbb{R}^3$ with infinitely many solutions, and explain why it satisfies the rank
condition.


Prove that for any matrix $A \in \mathbb{R}^{m \times n}$,
$
\text{rank}(A) \leq \min(m,n).
$


3.4 Homogeneous Systems
A homogeneous system is a linear system in which all constant terms are zero:
$$
A\mathbf{x} = \mathbf{0},
$$
where $A \in \mathbb{R}^{m \times n}$, and $\mathbf{0}$ is the zero vector in $\mathbb{R}^m$.
The Trivial Solution
Every homogeneous system has at least one solution:
$$
\mathbf{x} = \mathbf{0}.
$$
This is called the trivial solution. The interesting question is whether nontrivial solutions (nonzero vectors) exist.
Existence of Nontrivial Solutions
Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:
$$
\text{rank}(A) < n.
$$
In this case, there are infinitely many solutions, forming a subspace of $\mathbb{R}^n$. The dimension of this solution
space is
$$
\dim(\text{null}(A)) = n - \text{rank}(A),
$$
where null(A) is the set of all solutions to $A\mathbf{x} = 0$. This set is called the null space or kernel of $A$.
Example
Example 3.4.1.
Consider
$$
\begin{cases}
x + y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$
The augmented matrix is
$$
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 0 \\
2 & 1 & -1 & 0
\end{array}\right].
$$
Row reduction:
$$
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 0 \\
0 & -1 & -3 & 0
\end{array}\right]
\quad\to\quad
\left[\begin{array}{ccc|c}
1 & 1 & 1 & 0 \\
0 & 1 & 3 & 0
\end{array}\right].
$$
So the system is equivalent to:
$$
\begin{cases}
x + y + z = 0, \\
y + 3z = 0.
\end{cases}
$$
From the second equation, $y = -3z$. Substituting into the first:
$
x - 3z + z = 0 \implies x = 2z.
$
Thus solutions are:
$$
(x,y,z) = z(2, -3, 1), \quad z \in \mathbb{R}.
$$
The null space is the line spanned by the vector $(2, -3, 1)$.
Geometric Interpretation
The solution set of a homogeneous system is always a subspace of $\mathbb{R}^n$.

If $\text{rank}(A) = n$, the only solution is the zero vector.
If $\text{rank}(A) = n-1$, the solution set is a line through the origin.
If $\text{rank}(A) = n-2$, the solution set is a plane through the origin.

More generally, the null space has dimension $n - \text{rank}(A)$, known as the nullity.
Why this matters
Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the
concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium
problems, eigenvalue equations, and computer graphics transformations.
Exercises 3.4

Solve the homogeneous system

$$
\begin{cases}
x + 2y - z = 0, \\
2x + 4y - 2z = 0.
\end{cases}
$$
What is the dimension of its solution space?

Find all solutions of

$$
\begin{cases}
x - y + z = 0, \\
2x + y - z = 0.
\end{cases}
$$


Show that the solution set of any homogeneous system is a subspace of $\mathbb{R}^n$.


Suppose $A$ is a $3 \times 3$ matrix with $\text{rank}(A) = 2$. What is the dimension of the null space of $A$?


For


$$
A = \begin{bmatrix} 1 & 2 & -1 \ 0 & 1 & 3 \end{bmatrix},
$$
compute a basis for the null space of $A$.
Chapter 4. Vector Spaces
4.1 Definition of a Vector Space
Up to now we have studied vectors and matrices concretely in $\mathbb{R}^n$. The next step is to move beyond coordinates
and define vector spaces in full generality. A vector space is an abstract setting where the familiar rules of addition
and scalar multiplication hold, regardless of whether the elements are geometric vectors, polynomials, functions, or
other objects.
Formal Definition
A vector space over the real numbers $\mathbb{R}$ is a set $V$ equipped with two operations:

Vector addition: For any $\mathbf{u}, \mathbf{v} \in V$, there is a vector $\mathbf{u} + \mathbf{v} \in V$.
Scalar multiplication: For any scalar $c \in \mathbb{R}$ and any $\mathbf{v} \in V$, there is a
vector $c\mathbf{v} \in V$.

These operations must satisfy the following axioms (for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and all
scalars $a,b \in \mathbb{R}$):

Commutativity of addition: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$.
Associativity of addition: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$.
Additive identity: There exists a zero vector $\mathbf{0} \in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$.
Additive inverses: For each $\mathbf{v} \in V$, there exists $(-\mathbf{v} \in V$ such
that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$.
Compatibility of scalar multiplication: $a(b\mathbf{v}) = (ab)\mathbf{v}$.
Identity element of scalars: $1 \cdot \mathbf{v} = \mathbf{v}$.
Distributivity over vector addition: $a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$.
Distributivity over scalar addition: $(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$.

If a set $V$ with operations satisfies all eight axioms, we call it a vector space.
Examples
Example 4.1.1. Standard Euclidean space
$\mathbb{R}^n$ with ordinary addition and scalar multiplication is a vector space. This is the model case from which the
axioms are abstracted.
Example 4.1.2. Polynomials
The set of all polynomials with real coefficients, denoted $\mathbb{R}[x]$, forms a vector space. Addition and scalar
multiplication are defined term by term.
Example 4.1.3. Functions
The set of all real-valued functions on an interval, e.g. $f: [0,1] \to \mathbb{R}$, forms a vector space, since
functions can be added and scaled pointwise.
Non-Examples
Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a
vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.
Geometric Interpretation
In familiar cases like $\mathbb{R}^2$ or $\mathbb{R}^3$, vector spaces provide the stage for geometry: vectors can be
added, scaled, and combined to form lines, planes, and higher-dimensional structures. In abstract settings like function
spaces, the same algebraic rules let us apply geometric intuition to infinite-dimensional problems.
Why this matters
The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing
with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows
us to use the same techniques everywhere.
Exercises 4.1

Verify that $\mathbb{R}^2$ with standard addition and scalar multiplication satisfies all eight vector space axioms.
Show that the set of integers $\mathbb{Z}$ with ordinary operations is not a vector space over $\mathbb{R}$. Which
axiom fails?
Consider the set of all polynomials of degree at most 3. Show it forms a vector space over $\mathbb{R}$. What is its
dimension?
Give an example of a vector space where the vectors are not geometric objects.
Prove that in any vector space, the zero vector is unique.

4.2 Subspaces
A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside
three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.
Definition
Let $V$ be a vector space. A subset $W \subseteq V$ is called a subspace of $V$ if:


$\mathbf{0} \in W$ (contains the zero vector),
For all $\mathbf{u}, \mathbf{v} \in W$, the sum $\mathbf{u} + \mathbf{v} \in W$ (closed under addition),
For all scalars $c \in \mathbb{R}$ and vectors $\mathbf{v} \in W$, the product $c\mathbf{v} \in W$ (closed under
scalar multiplication).

If these hold, then $W$ is itself a vector space with the inherited operations.
Examples
Example 4.2.1. Line through the origin in $\mathbb{R}^2$
The set
$$
W = { (t, 2t) \mid t \in \mathbb{R} }
$$
is a subspace of $\mathbb{R}^2$. It contains the zero vector, is closed under addition, and is closed under scalar
multiplication.
Example 4.2.2. The x–y plane in $\mathbb{R}^3$
The set
$$
W = { (x, y, 0) \mid x,y \in \mathbb{R} }
$$
is a subspace of $\mathbb{R}^3$. It is the collection of all vectors lying in the plane through the origin parallel to
the x–y plane.
Example 4.2.3. Null space of a matrix
For a matrix $A \in \mathbb{R}^{m \times n}$, the null space
$$
{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} }
$$
is a subspace of $\mathbb{R}^n$. This subspace represents all solutions to the homogeneous system.
Non-Examples
Not every subset is a subspace.

The set ${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$ is not a subspace: it is not closed under scalar multiplication (a
negative scalar breaks the condition).
Any line in $\mathbb{R}^2$ that does not pass through the origin is not a subspace, because it does not
contain $\mathbf{0}$.

Geometric Interpretation
Subspaces are the linear structures inside vector spaces.

In $\mathbb{R}^2$, the subspaces are: the zero vector, any line through the origin, or the entire plane.
In $\mathbb{R}^3$, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or
the entire space.
In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.

Why this matters
Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all
subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each
other.
Exercises 4.2

Prove that the set $W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$ is a subspace.
Show that the line ${ (1+t, 2t) \mid t \in \mathbb{R} }$ is not a subspace of $\mathbb{R}^2$. Which condition fails?
Determine whether the set of all vectors $(x,y,z) \in \mathbb{R}^3$ satisfying $x+y+z=0$ is a subspace.
For the matrix

$$
A = \begin{bmatrix} 1 & 2 & 3 \ 4 & 5 & 6 \end{bmatrix},
$$
describe the null space of $A$ as a subspace of $\mathbb{R}^3$.
5. List all possible subspaces of $\mathbb{R}^2$.
4.3 Span, Basis, Dimension
The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces.
Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can
be chosen.
Span
Given a set of vectors ${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k} \subseteq V$, the span is the collection of
all linear combinations:
$$
\text{span}{\mathbf{v}_1, \dots, \mathbf{v}_k} = { c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k \mid c_i \in \mathbb{R} }.
$$
The span is always a subspace of $V$, namely the smallest subspace containing those vectors.
Example 4.3.1.
In $\mathbb{R}^2$, $ \text{span}{(1,0)} = {(x,0) \mid x \in \mathbb{R}},$ the x-axis.
Similarly, $\text{span}{(1,0),(0,1)} = \mathbb{R}^2.$
Basis
A basis of a vector space $V$ is a set of vectors that:

Span $V$.
Are linearly independent (no vector in the set is a linear combination of the others).

If either condition fails, the set is not a basis.
Example 4.3.2.
In $\mathbb{R}^3$, the standard unit vectors
$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$
form a basis. Every vector $(x,y,z)$ can be uniquely written as
$$
x\mathbf{e}_1 + y\mathbf{e}_2 + z\mathbf{e}_3.
$$
Dimension
The dimension of a vector space $V$, written $\dim(V)$, is the number of vectors in any basis of $V$. This number is
well-defined: all bases of a vector space have the same cardinality.
Examples 4.3.3.


$\dim(\mathbb{R}^2) = 2$, with basis $(1,0), (0,1)$.

$\dim(\mathbb{R}^3) = 3$, with basis $(1,0,0), (0,1,0), (0,0,1)$.
The set of polynomials of degree at most 3 has dimension 4, with basis $(1, x, x^2, x^3)$.

Geometric Interpretation

The span is like the reach of a set of vectors.
A basis is the minimal set of directions needed to reach everything in the space.
The dimension is the count of those independent directions.

Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.
Why this matters
These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such
as the Rank–Nullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are
how we encode data in coordinates, and dimension tells us how much freedom a system truly has.
Exercises 4.3

Show that $(1,0,0)$, $(0,1,0)$, $(1,1,0)$ span the $xy$-plane in $\mathbb{R}^3$. Are they a basis?
Find a basis for the line ${(2t,-3t,t) : t \in \mathbb{R}}$ in $\mathbb{R}^3$.
Determine the dimension of the subspace of $\mathbb{R}^3$ defined by $x+y+z=0$.
Prove that any two different bases of $\mathbb{R}^n$ must contain exactly $n$ vectors.
Give a basis for the set of polynomials of degree $\leq 2$. What is its dimension?

4.4 Coordinates
Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis
vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis.
Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.
Coordinates Relative to a Basis
Let $V$ be a vector space, and let
$$
\mathcal{B} = {\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}
$$
be an ordered basis for $V$. Every vector $\mathbf{u} \in V$ can be written uniquely as
$$
\mathbf{u} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_n \mathbf{v}_n.
$$
The scalars $(c_1, c_2, \dots, c_n)$ are the coordinates of $\mathbf{u}$ relative to $\mathcal{B}$, written
$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} c_1 \ c_2 \ \vdots \ c_n \end{bmatrix}.
$$
Example in $\mathbb{R}^2$

Example 4.4.1.
Let the basis be
$$
\mathcal{B} = { (1,1), (1,-1) }.
$$
To find the coordinates of $\mathbf{u} = (3,1)$ relative to $\mathcal{B}$, solve
$$
(3,1) = c_1(1,1) + c_2(1,-1).
$$
This gives the system
$$
\begin{cases}
c_1 + c_2 = 3, \\
c_1 - c_2 = 1.
\end{cases}
$$
Adding: $2c_1 = 4 \implies c_1 = 2$. Then $c_2 = 1$.
So,
$$
[\mathbf{u}]_{\mathcal{B}} = \begin{bmatrix} 2 \ 1 \end{bmatrix}.
$$
Standard Coordinates
In $\mathbb{R}^n$, the standard basis is
$$
\mathbf{e}_1 = (1,0,\dots,0), \quad \mathbf{e}_2 = (0,1,0,\dots,0), \dots, \mathbf{e}_n = (0,\dots,0,1).
$$
Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate
representations by default.
Change of Basis
If $\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}_n}$ is a basis of $\mathbb{R}^n$, the change of basis matrix is
$$
P = \begin{bmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \end{bmatrix},
$$
with basis vectors as columns. For any vector $\mathbf{u}$,
$$
\mathbf{u} = P[\mathbf{u}]{\mathcal{B}}, \qquad [\mathbf{u}]{\mathcal{B}} = P^{-1}\mathbf{u}.
$$
Thus, switching between bases reduces to matrix multiplication.
Geometric Interpretation
Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different
coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending
on the basis, but its geometric identity is unchanged.
Why this matters
Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations
of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is
essential for moving fluidly between geometry, algebra, and computation.
Exercises 4.4

Express $(4,2)$ in terms of the basis $(1,1), (1,-1)$.
Find the coordinates of $(1,2,3)$ relative to the standard basis of $\mathbb{R}^3$.
If $\mathcal{B} = {(2,0), (0,3)}$, compute $[ (4,6) ]_{\mathcal{B}}$.
Construct the change of basis matrix from the standard basis of $\mathbb{R}^2$ to $\mathcal{B} = {(1,1), (1,-1)}$.
Prove that coordinate representation with respect to a basis is unique.

Chapter 5. Linear Transformations
5.1 Functions that Preserve Linearity
A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve
their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of
linear behavior.
Definition
Let $V$ and $W$ be vector spaces over $\mathbb{R}$. A function
$$
T : V \to W
$$
is called a linear transformation (or linear map) if for all vectors $\mathbf{u}, \mathbf{v} \in V$ and all
scalars $c \in \mathbb{R}$:


Additivity:
$$
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}),
$$


Homogeneity:
$$
T(c\mathbf{u}) = cT(\mathbf{u}).
$$


If both conditions hold, then $T$ automatically respects linear combinations:
$$
T(c_1\mathbf{v}_1 + \cdots + c_k\mathbf{v}_k) = c_1 T(\mathbf{v}_1) + \cdots + c_k T(\mathbf{v}_k).
$$
Examples
Example 5.1.1. Scaling in $\mathbb{R}^2$.
Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be defined by
$$
T(x,y) = (2x, 2y).
$$
This doubles the length of every vector, preserving direction. It is linear.
Example 5.1.2. Rotation.
Let $R_\theta: \mathbb{R}^2 \to \mathbb{R}^2$ be
$$
R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta).
$$
This rotates vectors by angle $\theta$. It satisfies additivity and homogeneity, hence is linear.
Example 5.1.3. Differentiation.
Let $D: \mathbb{R}[x] \to \mathbb{R}[x]$ be differentiation: $D(p(x)) = p'(x)$. Since derivatives respect addition and
scalar multiples, differentiation is a linear transformation.
Non-Example
The map $S:\mathbb{R}^2 \to \mathbb{R}^2$ defined by
$$
S(x,y) = (x^2, y^2)
$$
is not linear, because $S(\mathbf{u} + \mathbf{v}) \neq S(\mathbf{u}) + S(\mathbf{v})$ in general.
Geometric Interpretation
Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those
lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear
transformations bend or curve space, breaking these properties.
Why this matters
Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can
be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding
these transformations, their representations, and their invariants.
Exercises 5.1


Verify that $T(x,y) = (3x-y, 2y)$ is a linear transformation on $\mathbb{R}^2$.


Show that $T(x,y) = (x+1, y)$ is not linear. Which axiom fails?


Prove that if $T$ and $S$ are linear transformations, then so is $T+S$.


Give an example of a linear transformation from $\mathbb{R}^3$ to $\mathbb{R}^2$.


Let $T:\mathbb{R}[x] \to \mathbb{R}[x]$ be integration:
$$
T(p(x)) = \int_0^x p(t),dt.
$$
Prove that $T$ is a linear transformation.


5.2 Matrix Representation of Linear Maps
Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence
is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract
transformations.
From Linear Map to Matrix
Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Choose the standard
basis ${ \mathbf{e}_1, \dots, \mathbf{e}_n }$ of $\mathbb{R}^n$, where $\mathbf{e}_i$ has a 1 in the $i$-th position
and 0 elsewhere.
The action of $T$ on each basis vector determines the entire transformation:
$$
T(\mathbf{e}j) = \begin{bmatrix} a{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}.
$$
Placing these outputs as columns gives the matrix of $T$:
$$
[T] = A = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}.
$$
Then for any vector $\mathbf{x} \in \mathbb{R}^n$:
$$
T(\mathbf{x}) = A\mathbf{x}.
$$
Examples
Example 5.2.1. Scaling in $\mathbb{R}^2$.
Let $T(x,y) = (2x, 3y)$. Then
$$
T(\mathbf{e}_1) = (2,0), \quad T(\mathbf{e}_2) = (0,3).
$$
So the matrix is
$$
[T] = \begin{bmatrix}
2 & 0 \\
0 & 3
\end{bmatrix}.
$$
Example 5.2.2. Rotation in the plane.
The rotation transformation $R_\theta(x,y) = (x\cos\theta - y\sin\theta, ; x\sin\theta + y\cos\theta)$ has matrix
$$
[R_\theta] = \begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}.
$$
Example 5.2.3. Projection onto the x-axis.
The map $P(x,y) = (x,0)$ corresponds to
$$
[P] = \begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}.
$$
Change of Basis
Matrix representations depend on the chosen basis. If $\mathcal{B}$ and $\mathcal{C}$ are bases of $\mathbb{R}^n$
and $\mathbb{R}^m$, then the matrix of $T: \mathbb{R}^n \to \mathbb{R}^m$ with respect to these bases is obtained by
expressing $T(\mathbf{v}_j)$ in terms of $\mathcal{C}$ for each $\mathbf{v}_j \in \mathcal{B}$. Changing bases
corresponds to conjugating the matrix by the appropriate change-of-basis matrices.
Geometric Interpretation
Matrices are not just convenient notation-they are linear maps once a basis is fixed. Every rotation, reflection,
projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations
reduces to studying their matrices.
Why this matters
Matrix representations make linear transformations computable. They connect abstract definitions to explicit
calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications
from graphics to machine learning depend on this translation.
Exercises 5.2

Find the matrix representation of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x+y, x-y)$.
Determine the matrix of the linear transformation $T:\mathbb{R}^3 \to \mathbb{R}^2$, $T(x,y,z) = (x+z, y-2z)$.
What matrix represents reflection across the line $y=x$ in $\mathbb{R}^2$?
Show that the matrix of the identity transformation on $\mathbb{R}^n$ is $I_n$.
For the differentiation map $D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$, where $\mathbb{R}_k[x]$ is the space of
polynomials of degree at most $k$, find the matrix of $D$ relative to the bases ${1,x,x^2}$ and ${1,x}$.

5.3 Kernel and Image
To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are
captured by the kernel and the image, two fundamental subspaces associated with any linear map.
The Kernel
The kernel (or null space) of a linear transformation $T: V \to W$ is the set of all vectors in $V$ that map to the zero
vector in $W$:
$$
\ker(T) = { \mathbf{v} \in V \mid T(\mathbf{v}) = \mathbf{0} }.
$$
The kernel is always a subspace of $V$. It measures the degeneracy of the transformation-directions that collapse to
nothing.
Example 5.3.1.
Let $T:\mathbb{R}^3 \to \mathbb{R}^2$ be defined by
$$
T(x,y,z) = (x+y, y+z).
$$
In matrix form,
$$
[T] = \begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}.
$$
To find the kernel, solve
$$
\begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix}
\begin{bmatrix} x \ y \ z \end{bmatrix}
= \begin{bmatrix} 0 \ 0 \end{bmatrix}.
$$
This gives the equations $x + y = 0$, $y + z = 0$. Hence $x = -y, z = -y$. The kernel is
$$
\ker(T) = { (-t, t, -t) \mid t \in \mathbb{R} },
$$
a line in $\mathbb{R}^3$.
The Image
The image (or range) of a linear transformation $T: V \to W$ is the set of all outputs:
$$
\text{im}(T) = { T(\mathbf{v}) \mid \mathbf{v} \in V } \subseteq W.
$$
Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of $W$.
Example 5.3.2.
For the same transformation as above,
$$
[T] = \begin{bmatrix}
1 & 1 & 0 \\
0 & 1 & 1
\end{bmatrix},
$$
the columns are $(1,0)$, $(1,1)$, and $(0,1)$. Since $(1,1) = (1,0) + (0,1)$, the image is
$$
\text{im}(T) = \text{span}{ (1,0), (0,1) } = \mathbb{R}^2.
$$
Dimension Formula (Rank–Nullity Theorem)
For a linear transformation $T: V \to W$ with $V$ finite-dimensional,
$$
\dim(\ker(T)) + \dim(\text{im}(T)) = \dim(V).
$$
This fundamental result connects the lost directions (kernel) with the achieved directions (image).
Geometric Interpretation

The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).
The image describes the target subspace reached by the transformation.
The rank–nullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.

Why this matters
Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or
infinite solutions, and form the backbone of important results like the Rank–Nullity Theorem, diagonalization, and
spectral theory.
Exercises 5.3

Find the kernel and image of $T:\mathbb{R}^2 \to \mathbb{R}^2$, $T(x,y) = (x-y, x+y)$.
Let $A = \begin{bmatrix} 1 & 2 & 3 \ 0 & 1 & 4 \end{bmatrix}$. Find bases for $\ker(A)$ and $\text{im}(A)$.
For the projection map $P(x,y,z) = (x,y,0)$, describe the kernel and image.
Prove that $\ker(T)$ and $\text{im}(T)$ are always subspaces.
Verify the Rank–Nullity Theorem for the transformation in Example 5.3.1.

5.4 Change of Basis
Linear transformations can look very different depending on the coordinate system we use. The process of rewriting
vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of
diagonalization, orthogonalization, and many computational techniques.
Coordinate Change
Suppose $V$ is an $n$-dimensional vector space, and let $\mathcal{B} = {\mathbf{v}_1, \dots, \mathbf{v}n}$ be a
basis. Every vector $\mathbf{x} \in V$ has a coordinate vector $[\mathbf{x}]{\mathcal{B}} \in \mathbb{R}^n$.
If $P$ is the change-of-basis matrix from $\mathcal{B}$ to the standard basis, then
$$
\mathbf{x} = P [\mathbf{x}]_{\mathcal{B}}.
$$
Equivalently,
$$
[\mathbf{x}]_{\mathcal{B}} = P^{-1} \mathbf{x}.
$$
Here, $P$ has the basis vectors of $\mathcal{B}$ as its columns:
$$
P = \begin{bmatrix}
\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n
\end{bmatrix}.
$$
Transformation of Matrices
Let $T: V \to V$ be a linear transformation. Suppose its matrix in the standard basis is $A$. In the
basis $\mathcal{B}$, the representing matrix becomes
$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$
Thus, changing basis corresponds to a similarity transformation of the matrix.
Example
Example 5.4.1.
Let $T:\mathbb{R}^2 \to \mathbb{R}^2$ be given by
$$
T(x,y) = (3x + y, x + y).
$$
In the standard basis, its matrix is
$$
A = \begin{bmatrix}
3 & 1 \\
1 & 1
\end{bmatrix}.
$$
Now consider the basis $\mathcal{B} = { (1,1), (1,-1) }$. The change-of-basis matrix is
$$
P = \begin{bmatrix}
1 & 1 \\
1 & -1
\end{bmatrix}.
$$
Then
$$
[T]_{\mathcal{B}} = P^{-1} A P.
$$
Computing gives
$$
[T]_{\mathcal{B}} =
\begin{bmatrix}
4 & 0 \\
0 & 0
\end{bmatrix}.
$$
In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.
Geometric Interpretation
Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its
description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a
transformation (often a diagonal basis) is a key theme in linear algebra.
Why this matters
Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to
diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to
choosing a more natural coordinate system-whether in geometry, physics, or machine learning.
Exercises 5.4

Let $A = \begin{bmatrix} 2 & 1 \ 0 & 2 \end{bmatrix}$. Compute its representation in the basis ${(1,0),(1,1)}$.
Find the change-of-basis matrix from the standard basis of $\mathbb{R}^2$ to ${(2,1),(1,1)}$.
Prove that similar matrices (related by $P^{-1}AP$) represent the same linear transformation under different bases.
Diagonalize the matrix $A = \begin{bmatrix} 1 & 0 \ 0 & -1 \end{bmatrix}$ in the basis ${(1,1),(1,-1)}$.
In $\mathbb{R}^3$, let $\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$. Construct the change-of-basis matrix $P$ and
compute $P^{-1}$.

Chapter 6. Determinants
6.1 Motivation and Geometric Meaning
Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula,
but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear
transformations. They bridge algebra and geometry.
Determinants of $2 \times 2$ Matrices
For a $2 \times 2$ matrix
$$
A = \begin{bmatrix} a & b \ c & d \end{bmatrix},
$$
the determinant is defined as
$$
\det(A) = ad - bc.
$$
Geometric meaning: If $A$ represents a linear transformation of the plane, then $|\det(A)|$ is the area scaling factor.
For example, if $\det(A) = 2$, areas of shapes are doubled. If $\det(A) = 0$, the transformation collapses the plane to
a line: all area is lost.
Determinants of $3 \times 3$ Matrices
For
$$
A = \begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix},
$$
the determinant can be computed as
$$
\det(A) = a(ei - fh) - b(di - fg) + c(dh - eg).
$$
Geometric meaning: In $\mathbb{R}^3$, $|\det(A)|$ is the volume scaling factor. If $\det(A) &lt; 0$, orientation is
reversed (a handedness flip), such as turning a right-handed coordinate system into a left-handed one.
General Case
For $A \in \mathbb{R}^{n \times n}$, the determinant is a scalar that measures how the linear transformation given
by $A$ scales n-dimensional volume.

If $\det(A) = 0$: the transformation squashes space into a lower dimension, so $A$ is not invertible.
If $\det(A) &gt; 0$: volume is scaled by $\det(A)$, orientation preserved.
If $\det(A) &lt; 0$: volume is scaled by $|\det(A)|$, orientation reversed.

Visual Examples


Shear in $\mathbb{R}^2$:
$A = \begin{bmatrix} 1 & 1 \ 0 & 1 \end{bmatrix}$.
Then $\det(A) = 1$. The transformation slants the unit square into a parallelogram but preserves area.


Projection in $\mathbb{R}^2$:
$A = \begin{bmatrix} 1 & 0 \ 0 & 0 \end{bmatrix}$.
Then $\det(A) = 0$. The unit square collapses into a line segment: area vanishes.


Rotation in $\mathbb{R}^2$:
$R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \ \sin\theta & \cos\theta \end{bmatrix}$.
Then $\det(R_\theta) = 1$. Rotations preserve area and orientation.


Why this matters
The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how
it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in
analysis, geometry, and applied mathematics.
Exercises 6.1

Compute the determinant of $\begin{bmatrix} 2 & 3 \ 1 & 4 \end{bmatrix}$. What area scaling factor does it
represent?
Find the determinant of the shear matrix $\begin{bmatrix} 1 & 2 \ 0 & 1 \end{bmatrix}$. What happens to the area of
the unit square?
For the $3 \times 3$ matrix
$\begin{bmatrix} 1 & 0 & 0 \ 0 & 2 & 0 \ 0 & 0 & 3 \end{bmatrix}$, compute the determinant. How does it scale
volume in $\mathbb{R}^3$?
Show that any rotation matrix in $\mathbb{R}^2$ has determinant $1$.
Give an example of a $2 \times 2$ matrix with determinant $-1$. What geometric action does it represent?

6.2 Properties of Determinants
Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in
linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants
behave under matrix operations.
Basic Properties
Let $A, B \in \mathbb{R}^{n \times n}$, and let $c \in \mathbb{R}$. Then:


Identity:
$$
\det(I_n) = 1.
$$


Triangular matrices:
If $A$ is upper or lower triangular, then
$$
\det(A) = a_{11} a_{22} \cdots a_{nn}.
$$


Row/column swap:
Interchanging two rows (or columns) multiplies the determinant by $-1$.


Row/column scaling:
Multiplying a row (or column) by a scalar $c$ multiplies the determinant by $c$.


Row/column addition:
Adding a multiple of one row to another does not change the determinant.


Transpose:
$$
\det(A^T) = \det(A).
$$


Multiplicativity:
$$
\det(AB) = \det(A)\det(B).
$$


Invertibility:
$A$ is invertible if and only if $\det(A) \neq 0$.


Example Computations
Example 6.2.1.
For
$$
A = \begin{bmatrix} 2 & 0 & 0 \ 1 & 3 & 0 \ -1 & 4 & 5 \end{bmatrix},
$$
$A$ is lower triangular, so
$$
\det(A) = 2 \cdot 3 \cdot 5 = 30.
$$
Example 6.2.2.
Let
$$
B = \begin{bmatrix} 1 & 2 \ 3 & 4 \end{bmatrix}, \quad
C = \begin{bmatrix} 0 & 1 \ 1 & 0 \end{bmatrix}.
$$
Then
$$
\det(B) = 1\cdot 4 - 2\cdot 3 = -2, \quad \det(C) = -1.
$$
Since $CB$ is obtained by swapping rows of $B$,
$$
\det(CB) = -\det(B) = 2.
$$
This matches the multiplicativity rule: $\det(CB) = \det(C)\det(B) = (-1)(-2) = 2.$
Geometric Insights

Row swaps: flipping orientation of space.
Scaling a row: stretching space in one direction.
Row replacement: sliding hyperplanes without altering volume.
Multiplicativity: performing two transformations multiplies their scaling factors.

These properties make determinants both computationally manageable and geometrically interpretable.
Why this matters
Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why
invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume
computation, eigenvalue theory, and differential equations.
Exercises 6.2


Compute the determinant of
$$
A = \begin{bmatrix} 1 & 2 & 3 \ 0 & 1 & 4 \ 0 & 0 & 2 \end{bmatrix}.
$$


Show that if two rows of a square matrix are identical, then its determinant is zero.


Verify $\det(A^T) = \det(A)$ for
$$
A = \begin{bmatrix} 2 & -1 \ 3 & 4 \end{bmatrix}.
$$


If $A$ is invertible, prove that
$$
\det(A^{-1}) = \frac{1}{\det(A)}.
$$


Suppose $A$ is a $3\times 3$ matrix with $\det(A) = 5$. What is $\det(2A)$?


6.3 Cofactor Expansion
While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic
method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by
breaking them into smaller ones.
Minors and Cofactors
For an $n \times n$ matrix $A = [a_{ij}]$:

The minor $M_{ij}$ is the determinant of the $(n-1) \times (n-1)$ matrix obtained by deleting the $i$-th row and $j$
-th column of $A$.
The cofactor $C_{ij}$ is defined by

$$
C_{ij} = (-1)^{i+j} M_{ij}.
$$
The sign factor $(-1)^{i+j}$ alternates in a checkerboard pattern:
$$
\begin{bmatrix}

& - & + & - & \cdots \


& + & - & + & \cdots \


& - & + & - & \cdots \
\vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}.
$$

Cofactor Expansion Formula
The determinant of $A$ can be computed by expanding along any row or any column:
$$
\det(A) = \sum_{j=1}^n a_{ij} C_{ij} \quad \text{(expansion along row (i))},
$$
$$
\det(A) = \sum_{i=1}^n a_{ij} C_{ij} \quad \text{(expansion along column (j))}.
$$
Example
Example 6.3.1.
Compute
$$
A = \begin{bmatrix}
1 & 2 & 3 \\
0 & 4 & 5 \\
1 & 0 & 6
\end{bmatrix}.
$$
Expand along the first row:
$$
\det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13}.
$$

For $C_{11}$:
$M_{11} = \det \begin{bmatrix} 4 & 5 \ 0 & 6 \end{bmatrix} = 24$, so $C_{11} = (+1)(24) = 24$.
For $C_{12}$:
$M_{12} = \det \begin{bmatrix} 0 & 5 \ 1 & 6 \end{bmatrix} = 0 - 5 = -5$, so $C_{12} = (-1)(-5) = 5$.
For $C_{13}$:
$M_{13} = \det \begin{bmatrix} 0 & 4 \ 1 & 0 \end{bmatrix} = 0 - 4 = -4$, so $C_{13} = (+1)(-4) = -4$.

Thus,
$$
\det(A) = 1(24) + 2(5) + 3(-4) = 24 + 10 - 12 = 22.
$$
Properties of Cofactor Expansion

Expansion along any row or column yields the same result.
The cofactor expansion provides a recursive definition of determinant: a determinant of size $n$ is expressed in
terms of determinants of size $n-1$.
Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:

$$
A^{-1} = \frac{1}{\det(A)} , \text{adj}(A), \quad \text{where adj}(A) = [C_{ji}].
$$
Geometric Interpretation
Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column
at a time. Each cofactor measures how that row/column influences the overall volume scaling.
Why this matters
Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not
the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections
to adjugates, Cramer’s rule, and classical geometry.
Exercises 6.3


Compute the determinant of
$$
\begin{bmatrix}
2 & 0 & 1 \
3 & -1 & 4 \
1 & 2 & 0
\end{bmatrix}
$$
by cofactor expansion along the first column.


Verify that expanding along the second row of Example 6.3.1 gives the same determinant.


Prove that expansion along any row gives the same value.


Show that if a row of a matrix is zero, then its determinant is zero.


Use cofactor expansion to prove that $\det(A) = \det(A^T)$.


6.4 Applications (Volume, Invertibility Test)
Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most
important applications are measuring volumes and testing invertibility of matrices.
Determinants as Volume Scalers
Given vectors $\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n \in \mathbb{R}^n$, arrange them as columns of a matrix:
$$
A = \begin{bmatrix}
| & | & & | \\
\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
| & | & & |
\end{bmatrix}.
$$
Then $|\det(A)|$ equals the volume of the parallelepiped spanned by these vectors.

In $\mathbb{R}^2$, $|\det(A)|$ gives the area of the parallelogram spanned by $\mathbf{v}_1, \mathbf{v}_2$.
In $\mathbb{R}^3$, $|\det(A)|$ gives the volume of the parallelepiped spanned
by $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$.
In higher dimensions, it generalizes to $n$-dimensional volume (hypervolume).

Example 6.4.1.
Let
$$
\mathbf{v}_1 = (1,0,0), \quad \mathbf{v}_2 = (1,1,0), \quad \mathbf{v}_3 = (1,1,1).
$$
Then
$$
A = \begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{bmatrix}, \quad \det(A) = 1.
$$
So the parallelepiped has volume $1$, even though the vectors are not orthogonal.
Invertibility Test
A square matrix $A$ is invertible if and only if $\det(A) \neq 0$.

If $\det(A) = 0$: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.
If $\det(A) \neq 0$: the transformation scales volume by $|\det(A)|$, and is reversible.

Example 6.4.2.
The matrix
$$
B = \begin{bmatrix} 2 & 4 \ 1 & 2 \end{bmatrix}
$$
has determinant $\det(B) = 2 \cdot 2 - 4 \cdot 1 = 0$.
Thus, $B$ is not invertible. Geometrically, the two column vectors are collinear, spanning only a line
in $\mathbb{R}^2$.
Cramer’s Rule
Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible.
For $A\mathbf{x} = \mathbf{b}$ with $A \in \mathbb{R}^{n \times n}$:
$$
x_i = \frac{\det(A_i)}{\det(A)},
$$
where $A_i$ is obtained by replacing the $i$-th column of $A$ with $\mathbf{b}$.
While inefficient computationally, Cramer’s rule highlights the determinant’s role in solutions and uniqueness.
Orientation
The sign of $\det(A)$ indicates whether a transformation preserves or reverses orientation. For example, a reflection in
the plane has determinant $-1$, flipping handedness.
Why this matters
Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights
are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation (
solving systems and checking singularity).
Exercises 6.4


Compute the area of the parallelogram spanned by $(2,1)$ and $(1,3)$.


Find the volume of the parallelepiped spanned by $(1,0,0), (1,1,0), (1,1,1)$.


Determine whether the matrix $\begin{bmatrix} 1 & 2 \ 3 & 6 \end{bmatrix}$ is invertible. Justify using
determinants.


Use Cramer’s rule to solve
$$
\begin{cases}
x + y = 3, \
2x - y = 0.
\end{cases}
$$


Explain geometrically why a determinant of zero implies no inverse exists.


Chapter 7. Inner Product Spaces
7.1 Inner Products and Norms
To extend the geometric ideas of length, distance, and angle beyond $\mathbb{R}^2$ and $\mathbb{R}^3$, we introduce
inner products. Inner products provide a way of measuring similarity between vectors, while norms derived from them
measure length. These concepts are the foundation of geometry inside vector spaces.
Inner Product
An inner product on a real vector space $V$ is a function
$$
\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}
$$
that assigns to each pair of vectors $(\mathbf{u}, \mathbf{v})$ a real number, subject to the following properties:


Symmetry:
$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$


Linearity in the first argument:
$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$


Positive-definiteness:
$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$, and equality holds if and only if $\mathbf{v} = \mathbf{0}$.


The standard inner product on $\mathbb{R}^n$ is the dot product:
$$
\langle \mathbf{u}, \mathbf{v} \rangle = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.
$$
Norms
The norm of a vector is its length, defined in terms of the inner product:
$$
|\mathbf{v}| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
$$
For the dot product in $\mathbb{R}^n$:
$$
|(x_1, x_2, \dots, x_n)| = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.
$$
Angles Between Vectors
The inner product allows us to define the angle $\theta$ between two nonzero vectors $\mathbf{u}, \mathbf{v}$ by
$$
\cos \theta = \frac{\langle \mathbf{u}, \mathbf{v} \rangle}{|\mathbf{u}| , |\mathbf{v}|}.
$$
Thus, two vectors are orthogonal if $\langle \mathbf{u}, \mathbf{v} \rangle = 0$.
Examples
Example 7.1.1.
In $\mathbb{R}^2$, with $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,4)$:
$$
\langle \mathbf{u}, \mathbf{v} \rangle = 1\cdot 3 + 2\cdot 4 = 11.
$$
$$
|\mathbf{u}| = \sqrt{1^2 + 2^2} = \sqrt{5}, \quad |\mathbf{v}| = \sqrt{3^2 + 4^2} = 5.
$$
So,
$$
\cos \theta = \frac{11}{\sqrt{5}\cdot 5}.
$$
Example 7.1.2.
In the function space $C[0,1]$, the inner product
$$
\langle f, g \rangle = \int_0^1 f(x) g(x), dx
$$
defines a length
$$
|f| = \sqrt{\int_0^1 f(x)^2 dx}.
$$
This generalizes geometry to infinite-dimensional spaces.
Geometric Interpretation

Inner product: measures similarity between vectors.
Norm: length of a vector.
Angle: measure of alignment between two directions.

These concepts unify algebraic operations with geometric intuition.
Why this matters
Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality,
projections, Fourier series, least squares approximation, and many applications in physics and machine learning.
Exercises 7.1


Compute $\langle (2,-1,3), (1,4,0) \rangle$. Then find the angle between them.


Show that $|(x,y)| = \sqrt{x^2+y^2}$ satisfies the properties of a norm.


In $\mathbb{R}^3$, verify that $(1,1,0)$ and $(1,-1,0)$ are orthogonal.


In $C[0,1]$, compute $\langle f,g \rangle$ for $f(x)=x$, $g(x)=1$.


Prove the Cauchy–Schwarz inequality:
$$
|\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|.
$$


7.2 Orthogonal Projections
One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to
approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins
geometry, statistics, and numerical analysis.
Projection onto a Line
Let $\mathbf{u} \in \mathbb{R}^n$ be a nonzero vector. The line spanned by $\mathbf{u}$ is
$$
L = { c\mathbf{u} \mid c \in \mathbb{R} }.
$$
Given a vector $\mathbf{v}$, the projection of $\mathbf{v}$ onto $\mathbf{u}$ is the vector in $L$ closest
to $\mathbf{v}$. Geometrically, it is the shadow of $\mathbf{v}$ on the line.
The formula is
$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{\langle \mathbf{v}, \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle} , \mathbf{u}.
$$
The error vector $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.
Example 7.2.1
Let $\mathbf{u} = (1,2)$, $\mathbf{v} = (3,1)$.
$$
\langle \mathbf{v}, \mathbf{u} \rangle = 3\cdot 1 + 1\cdot 2 = 5, \quad
\langle \mathbf{u}, \mathbf{u} \rangle = 1^2 + 2^2 = 5.
$$
So
$$
\text{proj}_{\mathbf{u}}(\mathbf{v}) = \frac{5}{5}(1,2) = (1,2).
$$
The error vector is $(3,1) - (1,2) = (2,-1)$, which is orthogonal to $(1,2)$.
Projection onto a Subspace
Suppose $W \subseteq \mathbb{R}^n$ is a subspace with orthonormal basis ${ \mathbf{w}_1, \dots, \mathbf{w}_k }$. The
projection of a vector $\mathbf{v}$ onto $W$ is
$$
\text{proj}_{W}(\mathbf{v}) = \langle \mathbf{v}, \mathbf{w}_1 \rangle \mathbf{w}_1 + \cdots + \langle \mathbf{v}, \mathbf{w}_k \rangle \mathbf{w}_k.
$$
This is the unique vector in $W$ closest to $\mathbf{v}$. The difference $\mathbf{v} - \text{proj}_{W}(\mathbf{v})$ is
orthogonal to all of $W$.
Least Squares Approximation
Orthogonal projection explains the method of least squares. To solve an overdetermined
system $A\mathbf{x} \approx \mathbf{b}$, we seek the $\mathbf{x}$ that makes $A\mathbf{x}$ the projection
of $\mathbf{b}$ onto the column space of $A$. This gives the normal equations
$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$
Thus, least squares is just projection in disguise.
Geometric Interpretation

Projection finds the closest point in a subspace to a given vector.
It minimizes distance (error) in the sense of Euclidean norm.
Orthogonality ensures the error vector points directly away from the subspace.

Why this matters
Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the
theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we
fit data with a simpler model, projection is at work.
Exercises 7.2

Compute the projection of $(2,3)$ onto the vector $(1,1)$.
Show that $\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$ is orthogonal to $\mathbf{u}$.
Let $W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$. Find the projection of $(1,2,3)$ onto $W$.
Explain why least squares fitting corresponds to projection onto the column space of $A$.
Prove that projection onto a subspace $W$ is unique: there is exactly one closest vector in $W$ to a
given $\mathbf{v}$.

7.3 Gram–Schmidt Process
The Gram–Schmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis.
This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate
comparisons, and projections take clean forms.
The Idea
Given a linearly independent set of vectors ${\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n}$ in an inner product
space, we want to construct an orthonormal set ${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$ that spans the same
subspace.
We proceed step by step:

Start with $\mathbf{v}_1$, normalize it to get $\mathbf{u}_1$.
Subtract from $\mathbf{v}_2$ its projection onto $\mathbf{u}_1$, leaving a vector orthogonal to $\mathbf{u}_1$.
Normalize to get $\mathbf{u}_2$.
For each $\mathbf{v}_k$, subtract projections onto all previously
constructed $\mathbf{u}1, \dots, \mathbf{u}{k-1}$, then normalize.

The Algorithm
For $k = 1, 2, \dots, n$:
$$
\mathbf{w}_k = \mathbf{v}k - \sum{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j,
$$
$$
\mathbf{u}_k = \frac{\mathbf{w}_k}{|\mathbf{w}_k|}.
$$
The result ${\mathbf{u}_1, \dots, \mathbf{u}_n}$ is an orthonormal basis of the span of the original vectors.
Example 7.3.1
Take $\mathbf{v}_1 = (1,1,0), \ \mathbf{v}_2 = (1,0,1), \ \mathbf{v}_3 = (0,1,1)$ in $\mathbb{R}^3$.

Normalize $\mathbf{v}_1$:

$$
\mathbf{u}_1 = \frac{1}{\sqrt{2}}(1,1,0).
$$

Subtract projection of $\mathbf{v}_2$ on $\mathbf{u}_1$:

$$
\mathbf{w}_2 = \mathbf{v}_2 - \langle \mathbf{v}_2,\mathbf{u}_1 \rangle \mathbf{u}_1.
$$
$$
\langle \mathbf{v}_2,\mathbf{u}_1 \rangle = \frac{1}{\sqrt{2}}(1\cdot 1 + 0\cdot 1 + 1\cdot 0) = \tfrac{1}{\sqrt{2}}.
$$
So
$$
\mathbf{w}_2 = (1,0,1) - \tfrac{1}{\sqrt{2}}\cdot \tfrac{1}{\sqrt{2}}(1,1,0)
= (1,0,1) - \tfrac{1}{2}(1,1,0)
= \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$
Normalize:
$$
\mathbf{u}_2 = \frac{1}{\sqrt{\tfrac{1}{4}+\tfrac{1}{4}+1}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)
= \frac{1}{\sqrt{\tfrac{3}{2}}}\left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right).
$$

Subtract projections from $\mathbf{v}_3$:

$$
\mathbf{w}_3 = \mathbf{v}_3 - \langle \mathbf{v}_3,\mathbf{u}_1 \rangle \mathbf{u}_1 - \langle \mathbf{v}_3,\mathbf{u}_2 \rangle \mathbf{u}_2.
$$
After computing, normalize to obtain $\mathbf{u}_3$.
The result is an orthonormal basis of the span of ${\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3}$.
Geometric Interpretation
Gram–Schmidt is like straightening out a set of vectors: you start with the original directions and adjust each new
vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while
preserving the span.
Why this matters
Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier
to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal
polynomials, principal component analysis).
Exercises 7.3

Apply Gram–Schmidt to $(1,0), (1,1)$ in $\mathbb{R}^2$.
Orthogonalize $(1,1,1), (1,0,1)$ in $\mathbb{R}^3$.
Prove that each step of Gram–Schmidt yields a vector orthogonal to all previous ones.
Show that Gram–Schmidt preserves the span of the original vectors.
Explain how Gram–Schmidt leads to the QR decomposition of a matrix.

7.4 Orthonormal Bases
An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit
length. Such bases are the most convenient possible coordinate systems: computations involving inner products,
projections, and norms become exceptionally simple.
Definition
A set of vectors ${\mathbf{u}_1, \mathbf{u}_2, \dots, \mathbf{u}_n}$ in an inner product space $V$ is called an
orthonormal basis if


$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$ whenever $i \neq j$ (orthogonality),

$|\mathbf{u}_i| = 1$ for all $i$ (normalization),
The set spans $V$.

Examples
Example 7.4.1. In $\mathbb{R}^2$, the standard basis
$$
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
$$
is orthonormal under the dot product.
Example 7.4.2. In $\mathbb{R}^3$, the standard basis
$$
\mathbf{e}_1 = (1,0,0), \quad \mathbf{e}_2 = (0,1,0), \quad \mathbf{e}_3 = (0,0,1)
$$
is orthonormal.
Example 7.4.3. Fourier basis on functions:
$$
{1, \cos x, \sin x, \cos 2x, \sin 2x, \dots}
$$
is an orthogonal set in the space of square-integrable functions on $[-\pi,\pi]$ with inner product
$$
\langle f,g \rangle = \int_{-\pi}^{\pi} f(x) g(x), dx.
$$
After normalization, it becomes an orthonormal basis.
Properties


Coordinate simplicity: If ${\mathbf{u}_1,\dots,\mathbf{u}_n}$ is an orthonormal basis of $V$, then any
vector $\mathbf{v}\in V$ has coordinates
$$
[\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}.
$$
That is, coordinates are just inner products.


Parseval’s identity:
For any $\mathbf{v} \in V$,
$$
|\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2.
$$


Projections:
The orthogonal projection onto the span of ${\mathbf{u}_1,\dots,\mathbf{u}_k}$ is
$$
\text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i.
$$


Constructing Orthonormal Bases

Start with any linearly independent set, then apply the Gram–Schmidt process to obtain an orthonormal set spanning the
same subspace.
In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.

Geometric Interpretation
An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed
directly using coordinates without correction factors. They are the ideal rulers of linear algebra.
Why this matters
Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions,
diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis
produces orthonormal directions capturing maximum variance.
Exercises 7.4

Verify that $(1/\sqrt{2})(1,1)$ and $(1/\sqrt{2})(1,-1)$ form an orthonormal basis of $\mathbb{R}^2$.
Express $(3,4)$ in terms of the orthonormal basis ${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$.
Prove Parseval’s identity for $\mathbb{R}^n$ with the dot product.
Find an orthonormal basis for the plane $x+y+z=0$ in $\mathbb{R}^3$.
Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.

Chapter 8. Eigenvalues and eigenvectors
8.1 Definitions and Intuition
The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They
identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or
distortion.
Definition
Let $T: V \to V$ be a linear transformation on a vector space $V$. A nonzero vector $\mathbf{v} \in V$ is called an
eigenvector of $T$ if
$$
T(\mathbf{v}) = \lambda \mathbf{v}
$$
for some scalar $\lambda \in \mathbb{R}$ (or $\mathbb{C}$). The scalar $\lambda$ is the eigenvalue corresponding
to $\mathbf{v}$.
Equivalently, if $A$ is the matrix of $T$, then eigenvalues and eigenvectors satisfy
$$
A\mathbf{v} = \lambda \mathbf{v}.
$$
Basic Examples
Example 8.1.1.
Let
$$
A = \begin{bmatrix} 2 & 0 \ 0 & 3 \end{bmatrix}.
$$
Then
$$
A(1,0)^T = 2(1,0)^T, \quad A(0,1)^T = 3(0,1)^T.
$$
So $(1,0)$ is an eigenvector with eigenvalue $2$, and $(0,1)$ is an eigenvector with eigenvalue $3$.
Example 8.1.2.
Rotation matrix in $\mathbb{R}^2$:
$$
R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \ \sin\theta & \cos\theta \end{bmatrix}.
$$
If $\theta \neq 0, \pi$, $R_\theta$ has no real eigenvalues: every vector is rotated, not scaled. Over $\mathbb{C}$,
however, it has eigenvalues $e^{i\theta}, e^{-i\theta}$.
Algebraic Formulation
Eigenvalues arise from solving the characteristic equation:
$$
\det(A - \lambda I) = 0.
$$
This polynomial in $\lambda$ is the characteristic polynomial. Its roots are the eigenvalues.
Geometric Intuition

Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.
Eigenvalues tell us the scaling factor along those directions.
If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.

Applications in Geometry and Science

Stretching along principal axes of an ellipse (quadratic forms).
Stable directions of dynamical systems.
Principal components in statistics and machine learning.
Quantum mechanics, where observables correspond to operators with eigenvalues.

Why this matters
Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear
transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics,
physics, computer science-relies on eigen-analysis.
Exercises 8.1

Find the eigenvalues and eigenvectors of
$\begin{bmatrix} 4 & 0 \ 0 & -1 \end{bmatrix}$.
Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.
Verify that the rotation matrix $R_\theta$ has no real eigenvalues unless $\theta = 0$ or $\pi$.
Compute the characteristic polynomial of
$\begin{bmatrix} 1 & 2 \ 2 & 1 \end{bmatrix}$.
Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix
$\begin{bmatrix} 1 & 1 \ 0 & 1 \end{bmatrix}$.

8.2 Diagonalization
A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the
process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations
such as powers, exponentials, and solving differential equations far easier.
Definition
A square matrix $A \in \mathbb{R}^{n \times n}$ is diagonalizable if there exists an invertible matrix $P$ such that
$$
P^{-1} A P = D,
$$
where $D$ is a diagonal matrix.
The diagonal entries of $D$ are eigenvalues of $A$, and the columns of $P$ are the corresponding eigenvectors.
When is a Matrix Diagonalizable?

A matrix is diagonalizable if it has $n$ linearly independent eigenvectors.
Equivalently, the sum of the dimensions of its eigenspaces equals $n$.
Symmetric matrices (over $\mathbb{R}$) are always diagonalizable, with an orthonormal basis of eigenvectors.

Example 8.2.1
Let
$$
A = \begin{bmatrix} 4 & 1 \ 0 & 2 \end{bmatrix}.
$$

Characteristic polynomial:

$$
\det(A - \lambda I) = (4-\lambda)(2-\lambda).
$$
So eigenvalues are $\lambda_1 = 4$, $\lambda_2 = 2$.

Eigenvectors:


For $\lambda = 4$, solve $(A-4I)\mathbf{v}=0$:
$\begin{bmatrix} 0 & 1 \ 0 & -2 \end{bmatrix}\mathbf{v} = 0$, giving $\mathbf{v}_1 = (1,0)$.
For $\lambda = 2$: $(A-2I)\mathbf{v}=0$, giving $\mathbf{v}_2 = (1,-2)$.


Construct $P = \begin{bmatrix} 1 & 1 \ 0 & -2 \end{bmatrix}$. Then

$$
P^{-1} A P = \begin{bmatrix} 4 & 0 \ 0 & 2 \end{bmatrix}.
$$
Thus, $A$ is diagonalizable.
Why Diagonalize?


Computing powers:
If $A = P D P^{-1}$, then
$$
A^k = P D^k P^{-1}.
$$
Since $D$ is diagonal, $D^k$ is easy to compute.


Matrix exponentials:
$e^A = P e^D P^{-1}$, useful in solving differential equations.


Understanding geometry:
Diagonalization reveals the directions along which a transformation stretches or compresses space independently.


Non-Diagonalizable Example
Not all matrices can be diagonalized.
$$
A = \begin{bmatrix} 1 & 1 \ 0 & 1 \end{bmatrix}
$$
has only one eigenvalue $\lambda = 1$, with eigenspace dimension 1. Since $n=2$ but we only have 1 independent
eigenvector, $A$ is not diagonalizable.
Geometric Interpretation
Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each
coordinate axis. It transforms complicated motion into independent 1D motions.
Why this matters
Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting
point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.
Exercises 8.2


Diagonalize
$$
A = \begin{bmatrix} 2 & 0 \ 0 & 3 \end{bmatrix}.
$$


Determine whether
$$
A = \begin{bmatrix} 1 & 1 \ 0 & 1 \end{bmatrix}
$$
is diagonalizable. Why or why not?


Find $A^5$ for
$$
A = \begin{bmatrix} 4 & 1 \ 0 & 2 \end{bmatrix}
$$
using diagonalization.


Show that any $n \times n$ matrix with $n$ distinct eigenvalues is diagonalizable.


Explain why real symmetric matrices are always diagonalizable.


8.3 Characteristic Polynomials
The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values
of $\lambda$ for which the matrix $A - \lambda I$ fails to be invertible.
Definition
For an $n \times n$ matrix $A$, the characteristic polynomial is
$$
p_A(\lambda) = \det(A - \lambda I).
$$
The roots of $p_A(\lambda)$ are the eigenvalues of $A$.
Examples
Example 8.3.1.
Let
$$
A = \begin{bmatrix} 2 & 1 \ 1 & 2 \end{bmatrix}.
$$
Then
$$
p_A(\lambda) = \det!\begin{bmatrix} 2-\lambda & 1 \ 1 & 2-\lambda \end{bmatrix}
= (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$
Thus eigenvalues are $\lambda = 1, 3$.
Example 8.3.2.
For
$$
A = \begin{bmatrix} 0 & -1 \ 1 & 0 \end{bmatrix}
$$
(rotation by 90°),
$$
p_A(\lambda) = \det!\begin{bmatrix} -\lambda & -1 \ 1 & -\lambda \end{bmatrix}
= \lambda^2 + 1.
$$
Eigenvalues are $\lambda = \pm i$. No real eigenvalues exist, consistent with pure rotation.
Example 8.3.3.
For a triangular matrix
$$
A = \begin{bmatrix} 2 & 1 & 0 \ 0 & 3 & 5 \ 0 & 0 & 4 \end{bmatrix},
$$
the determinant is simply the product of diagonal entries minus $\lambda$:
$$
p_A(\lambda) = (2-\lambda)(3-\lambda)(4-\lambda).
$$
So eigenvalues are $2, 3, 4$.
Properties


The characteristic polynomial of an $n \times n$ matrix has degree $n$.


The sum of the eigenvalues (counted with multiplicity) equals the trace of $A$:
$$
\text{tr}(A) = \lambda_1 + \cdots + \lambda_n.
$$


The product of the eigenvalues equals the determinant of $A$:
$$
\det(A) = \lambda_1 \cdots \lambda_n.
$$


Similar matrices have the same characteristic polynomial, hence the same eigenvalues.


Geometric Interpretation
The characteristic polynomial captures when $A - \lambda I$ collapses space: its determinant is zero precisely when the
transformation $A - \lambda I$ is singular. Thus, eigenvalues mark the critical scalings where the matrix loses
invertibility.
Why this matters
Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace
and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis
in dynamical systems.
Exercises 8.3


Compute the characteristic polynomial of
$$
A = \begin{bmatrix} 4 & 2 \ 1 & 3 \end{bmatrix}.
$$


Verify that the sum of the eigenvalues of
$\begin{bmatrix} 5 & 0 \ 0 & -2 \end{bmatrix}$
equals its trace, and their product equals its determinant.


Show that for any triangular matrix, the eigenvalues are just the diagonal entries.


Prove that if $A$ and $B$ are similar matrices, then $p_A(\lambda) = p_B(\lambda)$.


Compute the characteristic polynomial of
$\begin{bmatrix} 1 & 1 & 0 \ 0 & 1 & 1 \ 0 & 0 & 1 \end{bmatrix}$.


8.4 Applications (Differential Equations, Markov Chains)
Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across
mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing
Markov chains.
Linear Differential Equations
Consider the system
$$
\frac{d\mathbf{x}}{dt} = A \mathbf{x},
$$
where $A$ is an $n \times n$ matrix and $\mathbf{x}(t)$ is a vector-valued function.
If $\mathbf{v}$ is an eigenvector of $A$ with eigenvalue $\lambda$, then the function
$$
\mathbf{x}(t) = e^{\lambda t}\mathbf{v}
$$
is a solution.


Eigenvalues determine the growth or decay rate:

If $\lambda &lt; 0$, solutions decay (stable).
If $\lambda &gt; 0$, solutions grow (unstable).
If $\lambda$ is complex, oscillations occur.



By combining eigenvector solutions, we can solve general initial conditions.
Example 8.4.1.
Let
$$
A = \begin{bmatrix} 2 & 0 \ 0 & -1 \end{bmatrix}.
$$
Then eigenvalues are $2, -1$ with eigenvectors $(1,0)$, $(0,1)$. Solutions are
$$
\mathbf{x}(t) = c_1 e^{2t}(1,0) + c_2 e^{-t}(0,1).
$$
Thus one component grows exponentially, the other decays.
Markov Chains
A Markov chain is described by a stochastic matrix $P$, where each column sums to 1 and entries are nonnegative.
If $\mathbf{x}_k$ represents the probability distribution after $k$ steps, then
$$
\mathbf{x}_{k+1} = P \mathbf{x}_k.
$$
Iterating gives
$$
\mathbf{x}_k = P^k \mathbf{x}_0.
$$
Understanding long-term behavior reduces to analyzing powers of $P$.

The eigenvalue $\lambda = 1$ always exists. Its eigenvector gives the steady-state distribution.
All other eigenvalues satisfy $|\lambda| \leq 1$. Their influence decays as $k \to \infty$.

Example 8.4.2.
Consider
$$
P = \begin{bmatrix} 0.9 & 0.5 \ 0.1 & 0.5 \end{bmatrix}.
$$
Eigenvalues are $\lambda_1 = 1$, $\lambda_2 = 0.4$. The eigenvector for $\lambda = 1$ is proportional to $(5,1)$.
Normalizing gives the steady state
$$
\pi = \left(\tfrac{5}{6}, \tfrac{1}{6}\right).
$$
Thus, regardless of the starting distribution, the chain converges to $\pi$.
Geometric Interpretation

In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.
In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.

Why this matters
Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and
finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google’s
PageRank to modern machine learning.
Exercises 8.4


Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 & 0 \ 0 & -2 \end{bmatrix}\mathbf{x}$.


Show that if $A$ has a complex eigenvalue $\alpha \pm i\beta$, then solutions
of $\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$ involve oscillations of frequency $\beta$.


Find the steady-state distribution of
$$
P = \begin{bmatrix} 0.7 & 0.2 \ 0.3 & 0.8 \end{bmatrix}.
$$


Prove that for any stochastic matrix $P$, $1$ is always an eigenvalue.


Explain why all eigenvalues of a stochastic matrix satisfy $|\lambda| \leq 1$.


Chapter 9. Quadratic Forms and Spectral Theorems
9.1 Quadratic Forms
A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms
appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy
functions).
Definition
Let $A$ be an $n \times n$ symmetric matrix and $\mathbf{x} \in \mathbb{R}^n$. The quadratic form associated with $A$ is
$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x}.
$$
Expanded,
$$
Q(\mathbf{x}) = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j.
$$
Because $A$ is symmetric ($a_{ij} = a_{ji}$), the cross-terms can be grouped naturally.
Examples
Example 9.1.1.
For
$$
A = \begin{bmatrix} 2 & 1 \ 1 & 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x \ y \end{bmatrix},
$$
$$
Q(x,y) = \begin{bmatrix} x & y \end{bmatrix}
\begin{bmatrix} 2 & 1 \ 1 & 3 \end{bmatrix}
\begin{bmatrix} x \ y \end{bmatrix}
= 2x^2 + 2xy + 3y^2.
$$
Example 9.1.2.
The quadratic form
$$
Q(x,y) = x^2 + y^2
$$
corresponds to the matrix $A = I_2$. It measures squared Euclidean distance from the origin.
Example 9.1.3.
The conic section equation
$$
4x^2 + 2xy + 5y^2 = 1
$$
is described by the quadratic form $\mathbf{x}^T A \mathbf{x} = 1$ with
$$
A = \begin{bmatrix} 4 & 1 \ 1 & 5 \end{bmatrix}.
$$
Diagonalization of Quadratic Forms
By choosing a new basis consisting of eigenvectors of $A$, we can rewrite the quadratic form without cross terms.
If $A = PDP^{-1}$ with $D$ diagonal, then
$$
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = (P^{-1}\mathbf{x})^T D (P^{-1}\mathbf{x}).
$$
Thus quadratic forms can always be expressed as a sum of weighted squares:
$$
Q(\mathbf{y}) = \lambda_1 y_1^2 + \cdots + \lambda_n y_n^2,
$$
where $\lambda_i$ are the eigenvalues of $A$.
Geometric Interpretation
Quadratic forms describe geometric shapes:

In 2D: ellipses, parabolas, hyperbolas.
In 3D: ellipsoids, paraboloids, hyperboloids.
In higher dimensions: generalizations of ellipsoids.

Diagonalization aligns the coordinate axes with the principal axes of the shape.
Why this matters
Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics (
covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms
leads directly to the spectral theorem.
Exercises 9.1

Write the quadratic form $Q(x,y) = 3x^2 + 4xy + y^2$ as $\mathbf{x}^T A \mathbf{x}$ for some symmetric matrix $A$.
For $A = \begin{bmatrix} 1 & 2 \ 2 & 1 \end{bmatrix}$, compute $Q(x,y)$ explicitly.
Diagonalize the quadratic form $Q(x,y) = 2x^2 + 2xy + 3y^2$.
Identify the conic section given by $Q(x,y) = x^2 - y^2$.
Show that if $A$ is symmetric, quadratic forms defined by $A$ and $A^T$ are identical.

9.2 Positive Definite Matrices
Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee
positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis,
and statistics.
Definition
A symmetric matrix $A \in \mathbb{R}^{n \times n}$ is called:


Positive definite if
$$
\mathbf{x}^T A \mathbf{x} > 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n.
$$


Positive semidefinite if
$$
\mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}.
$$


Similarly, negative definite (always < 0) and indefinite (can be both < 0 and > 0) matrices are defined.
Examples
Example 9.2.1.
$$
A = \begin{bmatrix} 2 & 0 \ 0 & 3 \end{bmatrix}
$$
is positive definite, since
$$
Q(x,y) = 2x^2 + 3y^2 > 0
$$
for all $(x,y) \neq (0,0)$.
Example 9.2.2.
$$
A = \begin{bmatrix} 1 & 2 \ 2 & 1 \end{bmatrix}
$$
has quadratic form
$$
Q(x,y) = x^2 + 4xy + y^2.
$$
This matrix is not positive definite, since $Q(1,-1) = -2 &lt; 0$.
Characterizations
For a symmetric matrix $A$:


Eigenvalue test: $A$ is positive definite if and only if all eigenvalues of $A$ are positive.


Principal minors test (Sylvester’s criterion): $A$ is positive definite if and only if all leading principal minors (
determinants of top-left $k \times k$ submatrices) are positive.


Cholesky factorization: $A$ is positive definite if and only if it can be written as
$$
A = R^T R,
$$
where $R$ is an upper triangular matrix with positive diagonal entries.


Geometric Interpretation

Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.
Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).
Indefinite matrices define hyperbolas or saddle-shaped surfaces.

Applications

Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive
definite Hessians.
Statistics: Covariance matrices are positive semidefinite.
Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.

Why this matters
Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are
bounded below, optimization problems have unique solutions, and statistical models are meaningful.
Exercises 9.2


Use Sylvester’s criterion to check whether
$$
A = \begin{bmatrix} 2 & -1 \ -1 & 2 \end{bmatrix}
$$
is positive definite.


Determine whether
$$
A = \begin{bmatrix} 0 & 1 \ 1 & 0 \end{bmatrix}
$$
is positive definite, semidefinite, or indefinite.


Find the eigenvalues of
$$
A = \begin{bmatrix} 4 & 2 \ 2 & 3 \end{bmatrix},
$$
and use them to classify definiteness.


Prove that all diagonal matrices with positive entries are positive definite.


Show that if $A$ is positive definite, then so is $P^T A P$ for any invertible matrix $P$.


9.3 Spectral Theorem
The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always
be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal
directions), and applications (stability, optimization, statistics).
Statement of the Spectral Theorem
If $A \in \mathbb{R}^{n \times n}$ is symmetric ($A^T = A$), then:


All eigenvalues of $A$ are real.


There exists an orthonormal basis of $\mathbb{R}^n$ consisting of eigenvectors of $A$.


Thus, $A$ can be written as
$$
A = Q \Lambda Q^T,
$$
where $Q$ is an orthogonal matrix ($Q^T Q = I$) and $\Lambda$ is diagonal with eigenvalues of $A$ on the diagonal.


Consequences

Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.
Quadratic forms $\mathbf{x}^T A \mathbf{x}$ can be expressed in terms of eigenvalues and eigenvectors, showing
ellipsoids aligned with eigen-directions.
Positive definiteness can be checked by confirming that all eigenvalues are positive.

Example 9.3.1
Let
$$
A = \begin{bmatrix} 2 & 1 \ 1 & 2 \end{bmatrix}.
$$

Characteristic polynomial:

$$
p(\lambda) = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
$$
Eigenvalues: $\lambda_1 = 1, \ \lambda_2 = 3$.

Eigenvectors:


For $\lambda=1$: solve $(A-I)\mathbf{v} = 0$, giving $(1,-1)$.
For $\lambda=3$: solve $(A-3I)\mathbf{v} = 0$, giving $(1,1)$.


Normalize eigenvectors:

$$
\mathbf{u}_1 = \tfrac{1}{\sqrt{2}}(1,-1), \quad \mathbf{u}_2 = \tfrac{1}{\sqrt{2}}(1,1).
$$

Then

$$
Q = \begin{bmatrix} \tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}} [6pt] -\tfrac{1}{\sqrt{2}} & \tfrac{1}{\sqrt{2}} \end{bmatrix}, \quad
\Lambda = \begin{bmatrix} 1 & 0 \ 0 & 3 \end{bmatrix}.
$$
So
$$
A = Q \Lambda Q^T.
$$
Geometric Interpretation
The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry,
this corresponds to stretching space along perpendicular axes.

Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.
Orthogonality ensures directions remain perpendicular after transformation.

Applications

Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.
PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of
maximum variance.
Differential equations & physics: Symmetric operators correspond to measurable quantities with real eigenvalues (
stability, energy).

Why this matters
The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms
of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.
Exercises 9.3


Diagonalize
$$
A = \begin{bmatrix} 4 & 2 \ 2 & 3 \end{bmatrix}
$$
using the spectral theorem.


Prove that all eigenvalues of a real symmetric matrix are real.


Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.


Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.


Apply the spectral theorem to the covariance matrix
$$
\Sigma = \begin{bmatrix} 2 & 1 \ 1 & 2 \end{bmatrix},
$$
and interpret the eigenvectors as principal directions of variance.


9.4 Principal Component Analysis (PCA)
Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its
core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal
components) that capture the maximum variance in data.
The Idea
Given a dataset of vectors $\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_m \in \mathbb{R}^n$:


Center the data by subtracting the mean vector $\bar{\mathbf{x}}$.


Form the covariance matrix
$$
\Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T.
$$


Apply the spectral theorem: $\Sigma = Q \Lambda Q^T$.

Columns of $Q$ are orthonormal eigenvectors (principal directions).
Eigenvalues in $\Lambda$ measure variance explained by each direction.



The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum
variance.
Example 9.4.1
Suppose we have two-dimensional data points roughly aligned along the line $y = x$. The covariance matrix is
approximately
$$
\Sigma = \begin{bmatrix} 2 & 1.9 \ 1.9 & 2 \end{bmatrix}.
$$
Eigenvalues are about $3.9$ and $0.1$. The eigenvector for $\lambda = 3.9$ is approximately $(1,1)/\sqrt{2}$.

First principal component: the line $y = x$.
Most variance lies along this direction.
Second component is nearly orthogonal ($y = -x$), but variance there is tiny.

Thus PCA reduces the data to essentially one dimension.
Applications of PCA

Dimensionality reduction: Represent data with fewer features while retaining most variance.
Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.
Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.
Compression: PCA is used in image and signal compression.

Connection to the Spectral Theorem
The covariance matrix $\Sigma$ is always symmetric and positive semidefinite. Hence by the spectral theorem, it has an
orthonormal basis of eigenvectors and nonnegative real eigenvalues. PCA is nothing more than re-expressing data in this
eigenbasis.
Why this matters
PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a
practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important
algorithms derived from the spectral theorem.
Exercises 9.4

Show that the covariance matrix is symmetric and positive semidefinite.
Compute the covariance matrix of the dataset $(1,2), (2,3), (3,4)$, and find its eigenvalues and eigenvectors.
Explain why the first principal component captures the maximum variance.
In image compression, explain how PCA can reduce storage by keeping only the top $k$ principal components.
Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.

Chapter 10. Linear Algebra in Practice
10.1 Computer Graphics (Rotations, Projections)
Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or
projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections
are all linear transformations, making matrices the natural tool for manipulating geometry.
Rotations in 2D
A counterclockwise rotation by an angle $\theta$ in the plane is represented by
$$
R_\theta =
\begin{bmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{bmatrix}.
$$
For any vector $\mathbf{v} \in \mathbb{R}^2$, the rotated vector is
$$
\mathbf{v}' = R_\theta \mathbf{v}.
$$
This preserves lengths and angles, since $R_\theta$ is orthogonal with determinant $1$.
Rotations in 3D
In three dimensions, rotations are represented by $3 \times 3$ orthogonal matrices with determinant $1$. For example, a
rotation about the $z$-axis is
$$
R_z(\theta) =
\begin{bmatrix}
\cos\theta & -\sin\theta & 0 \\
\sin\theta & \cos\theta & 0 \\
0 & 0 & 1
\end{bmatrix}.
$$
Similar formulas exist for rotations about the $x$- and $y$-axes.
More general 3D rotations can be described by axis–angle representation or quaternions, but the underlying idea is still
linear transformations represented by matrices.
Projections
To display 3D objects on a 2D screen, we use projections:


Orthogonal projection: drops the $z$-coordinate, mapping $(x,y,z) \mapsto (x,y)$.
$$
P = \begin{bmatrix}
1 & 0 & 0 \
0 & 1 & 0
\end{bmatrix}.
$$


Perspective projection: mimics the effect of a camera. A point $(x,y,z)$ projects to
$$
\left(\frac{x}{z}, \frac{y}{z}\right),
$$
capturing how distant objects appear smaller.


These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in
homogeneous coordinates).
Homogeneous Coordinates
To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D
point $(x,y,z)$ is represented as a 4D vector $(x,y,z,1)$. Transformations are then $4 \times 4$ matrices, which can
represent rotations, scalings, and translations in a single framework.
Example: Translation by $(a,b,c)$:
$$
T = \begin{bmatrix}
1 & 0 & 0 & a \\
0 & 1 & 0 & b \\
0 & 0 & 1 & c \\
0 & 0 & 0 & 1
\end{bmatrix}.
$$
Geometric Interpretation

Rotations preserve shape and size, only changing orientation.
Projections reduce dimension: from 3D world space to 2D screen space.
Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a
single matrix multiplication.

Why this matters
Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining
simple matrix operations, complex transformations are applied efficiently to millions of points per second.
Exercises 10.1

Write the rotation matrix for a 90° counterclockwise rotation in $\mathbb{R}^2$. Apply it to $(1,0)$.
Rotate the point $(1,1,0)$ about the $z$-axis by 180°.
Show that the determinant of any 2D or 3D rotation matrix is 1.
Derive the orthogonal projection matrix from $\mathbb{R}^3$ to the $xy$-plane.
Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.

10.2 Data Science (Dimensionality Reduction, Least Squares)
Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality
reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares
method, which underlies regression and model fitting.
Dimensionality Reduction
High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a
lower-dimensional subspace. Dimensionality reduction identifies these subspaces.


PCA (Principal Component Analysis):
As introduced earlier, PCA diagonalizes the covariance matrix of the data.

Eigenvectors (principal components) define orthogonal directions of maximum variance.
Eigenvalues measure how much variance lies along each direction.
Keeping only the top $k$ components reduces data from $n$-dimensional space to $k$-dimensional space while
retaining most variability.



Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors
of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.
Least Squares
Often, we have more equations than unknowns-an overdetermined system:
$$
A\mathbf{x} \approx \mathbf{b}, \quad A \in \mathbb{R}^{m \times n}, \ m > n.
$$
An exact solution may not exist. Instead, we seek $\mathbf{x}$ that minimizes the error
$$
|A\mathbf{x} - \mathbf{b}|^2.
$$
This leads to the normal equations:
$$
A^T A \mathbf{x} = A^T \mathbf{b}.
$$
The solution is the orthogonal projection of $\mathbf{b}$ onto the column space of $A$.
Example 10.2.2
Fit a line $y = mx + c$ to data points $(x_i, y_i)$.
Matrix form:
$$
A = \begin{bmatrix}
x_1 & 1 \\
x_2 & 1 \\
\vdots & \vdots \\
x_m & 1
\end{bmatrix},
\quad
\mathbf{b} = \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_m \end{bmatrix},
\quad
\mathbf{x} = \begin{bmatrix} m \ c \end{bmatrix}.
$$
Solve $A^T A \mathbf{x} = A^T \mathbf{b}$. This yields the best-fit line in the least squares sense.
Geometric Interpretation

Dimensionality reduction: Find the best subspace capturing most variance.
Least squares: Project the target vector onto the subspace spanned by predictors.

Both are projection problems, solved using inner products and orthogonality.
Why this matters
Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting
powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and
projections-core tools of linear algebra.
Exercises 10.2

Explain why PCA reduces noise in datasets by discarding small eigenvalue components.
Compute the least squares solution to fitting a line through $(0,0), (1,1), (2,2)$.
Show that the least squares solution is unique if and only if $A^T A$ is invertible.
Prove that the least squares solution minimizes the squared error by projection arguments.
Apply PCA to the data points $(1,0), (2,1), (3,2)$ and find the first principal component.

10.3 Networks and Markov Chains
Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity
to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already
introduced in Section 8.4, are a central example of networks evolving over time.
Adjacency Matrices
A network (graph) with $n$ nodes can be represented by an adjacency matrix $A \in \mathbb{R}^{n \times n}$:
$$
A_{ij} =
\begin{cases}
1 & \text{if there is an edge from node (i) to node (j)} \\
0 & \text{otherwise.}
\end{cases}
$$
For weighted graphs, entries may be positive weights instead of $0/1$.

The number of walks of length $k$ from node $i$ to node $j$ is given by the entry $(A^k)_{ij}$.
Powers of adjacency matrices thus encode connectivity over time.

Laplacian Matrices
Another important matrix is the graph Laplacian:
$$
L = D - A,
$$
where $D$ is the diagonal degree matrix ($D_{ii} = \text{degree}(i)$).


$L$ is symmetric and positive semidefinite.
The smallest eigenvalue is always $0$, with eigenvector $(1,1,\dots,1)$.
The multiplicity of eigenvalue $0$ equals the number of connected components in the graph.

This connection between eigenvalues and connectivity forms the basis of spectral graph theory.
Markov Chains on Graphs
A Markov chain can be viewed as a random walk on a graph. If $P$ is the transition matrix where $P_{ij}$ is the
probability of moving from node $i$ to node $j$, then
$$
\mathbf{x}_{k+1} = P \mathbf{x}_k
$$
describes the distribution of positions after $k$ steps.

The steady-state distribution is given by the eigenvector of $P$ with eigenvalue $1$.
The speed of convergence depends on the gap between the largest eigenvalue (which is always $1$) and the second
largest eigenvalue.

Example 10.3.1
Consider a simple 3-node cycle graph:
$$
P = \begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix}.
$$
This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of
unity: $1, e^{2\pi i/3}, e^{4\pi i/3}$. The eigenvalue $1$ corresponds to the steady state, which is the uniform
distribution $(1/3,1/3,1/3)$.
Applications

Search engines: Google’s PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank
pages.
Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.
Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.

Why this matters
Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow,
stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these
tools are indispensable.
Exercises 10.3


Write the adjacency matrix of a square graph with 4 nodes. Compute $A^2$ and interpret the entries.


Show that the Laplacian of a connected graph has exactly one zero eigenvalue.


Find the steady-state distribution of the Markov chain with
$$
P = \begin{bmatrix} 0.5 & 0.5 \ 0.4 & 0.6 \end{bmatrix}.
$$


Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.


Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.


10.4 Machine Learning Connections
Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of
large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix
decompositions.
Data as Matrices
A dataset with $m$ examples and $n$ features is represented as a matrix $X \in \mathbb{R}^{m \times n}$:
$$
X =
\begin{bmatrix}

& \mathbf{x}_1^T & - \
& \mathbf{x}_2^T & - \
& \vdots & \
& \mathbf{x}_m^T & -
\end{bmatrix},
$$

where each row $\mathbf{x}_i \in \mathbb{R}^n$ is a feature vector. Linear algebra provides tools to analyze, compress,
and transform this data.
Linear Models
At the heart of machine learning are linear predictors:
$$
\hat{y} = X\mathbf{w},
$$
where $\mathbf{w}$ is the weight vector. Training often involves solving a least squares problem or a regularized
variant such as ridge regression:
$$
\min_{\mathbf{w}} |X\mathbf{w} - \mathbf{y}|^2 + \lambda |\mathbf{w}|^2.
$$
This is solved efficiently using matrix factorizations.
Singular Value Decomposition (SVD)
The SVD of a matrix $X$ is
$$
X = U \Sigma V^T,
$$
where $U, V$ are orthogonal and $\Sigma$ is diagonal with nonnegative entries (singular values).

Singular values measure the importance of directions in feature space.
SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.

Eigenvalues in Machine Learning

PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal
variance.
Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.
Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.

Neural Networks
Even deep learning, though nonlinear, uses linear algebra at its core:

Each layer is a matrix multiplication followed by a nonlinear activation.
Training requires computing gradients, which are expressed in terms of matrix calculus.
Backpropagation is essentially repeated applications of the chain rule with linear algebra.

Why this matters
Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the
algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would
be intractable.
Exercises 10.4


Show that ridge regression leads to the normal equations
$$
(X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}.
$$


Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.


For a covariance matrix $\Sigma$, show why its eigenvalues represent variances along principal components.


Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.


In a neural network with one hidden layer, write the forward pass in matrix form.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Passkeys and Modern Authentication]]></title>
            <link>https://lucumr.pocoo.org/2025/9/2/passkeys/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45103065</guid>
            <description><![CDATA[Some thoughts in support of simple solutions.]]></description>
            <content:encoded><![CDATA[
        
  

  
  written on September 02, 2025
  

  There is an ongoing trend in the industry to move people away from username and
password towards passkeys.  The
intentions here are good, and I would assume that this has a significant net
benefit for the average consumer.  At the same time, the underlying standard
has some peculiarities.  These enable behaviors by large corporations,
employers, and governments that are worth thinking about.
Attestations
One potential source of problems here is the attestation system.  It allows the
authenticator to provide more information about what it is to the website that
you’re authenticating with.  In particular it is what tells a website if you
have a Yubikey plugged in versus something like 1password.  This is the
mechanism by which the Austrian government, for instance, prevents you from
using an Open Source or any other software-based authenticator to sign in to do
your taxes, access medical records or do anything else that is protected by
eID.  Instead you
have to buy a whitelisted hardware
token.
Attestations themselves are not used by software authenticators today, or
anything that syncs.  Both Apple and Google do not expose attestation data in
their own software authenticators (Keychain and Google Authenticator) for
consumer passkeys.  However, they will pass through attestation data from
hardware tokens just fine.  Both of them also, to the best of my knowledge,
expose attestation data for enterprises through Mobile Device Management.
One could make the argument that it is unlikely that attestation data will be
used at scale to create vendor lock-in.  However, I’m not sufficiently
convinced that this won’t create sub-ecosystems where we see exactly that
happening.  If for no other reason, this API exists and it has already been
used to restrict keys for governmental sign-in systems.
Auth Lock-in
One slightly more concerning issue today is that there is effectively no way to
export private keys between authentication password managers.  You need to
enroll all of your ecosystems individually into a password manager.  An attempt
by an open source password manager to reveal private keys to the user was ruled
insecure and should not be
supported.
This taking away agency from the user is not an accident.  You can also see this
with the passkey export specification which comes with a
protocol that,
while enabling exports in principle, encourages a system to system transfer
that does not hand over the user’s credentials to the user. 1
This might be for good intentions, but it also creates problems.  As someone
recently trying to leave the Apple ecosystem step by step, I have noticed how
many services are now bound to an iCloud-based passkey.  Particularly when it
comes to Apple, this fear is not entirely unwarranted.  Sign-in with Apple
using non-shared email addresses makes it very hard to migrate to Android
unless you retain an iCloud subscription.
Obviously, one could pay for an authenticator like 1Password, which at least is
ecosystem independent.  However, not everybody is in a situation where they can
afford to pay for basic services like password managers.
Sneaky Onboarding
One reason why passkeys are adopted so well today is because it happens
automatically for many.  I discovered that non-technical family members now all
have passkeys for some services, and they did not even notice doing that.  A
notable example is Amazon.  After every sign-in, it attempts to enroll you into
a passkey automatically without clear notification.  It just brings up the
fingerprint prompt, and users will instinctively touch it.
If you use different types of devices to authenticate — for instance, a Windows
and an iOS device — you may eventually have both authenticators associated.
This now covers the devices you already use.  However, it can make moving to a
completely different ecosystem later much harder.
We Are Run By Corporations
For many years already, people lose access to their Google account every day
and can never regain it.  Google is well known for terminating accounts without
stating any reasons.  With that comes the loss of access to your data.  In this
case, you also lose your credentials for third-party websites.
There is no legal recourse for this and no mechanism for appeal.  You just have
to hope that you’re a good citizen and not doing anything that would upset
Google’s account flagging systems.
As a sufficiently technical person, you might weigh the risks, but others will
not.  Many years ago, I tried to help another family gain access to their
child’s Facebook account after they passed away.  Even then, it was a
bureaucratic nightmare where there was little support by Facebook to make it
happen.  There is a real risk that access becomes much harder for families.
This is particularly true in situations where someone is incapacitated or dead.
The more we move away from basic authentication systems, the worse this
becomes.  It’s also really inconvenient when you are not on your own devices.
Signing into my accounts on my children’s devices has turned from a
straightforward process to an incredibly frustrating experience.  I find myself
juggling all kinds of different apps and flows.
Complexity and Gatekeepers Everywhere
Every once in a while, I find myself in a situation where I have very little
foundation to build on.  This is mostly just because of a hobby.  I like to see
how things work and build them from scratch.  Increasingly, that has become
harder.  Many username and password authentication schemes have been replaced
with OAuth sign-ins over the years.  Nowadays, some services are moving towards
passkeys, though most places do not enforce these yet.  If you want to build an
operating system from scratch, or even just build a client yourself, you often
find yourself needing to do a lot of yak-shaving.  All this work is necessary
just to get basic things working.
I think this is at least something to be wary of.  It doesn’t mean that bad
things will necessarily happen, but there is potential for loss of individual
agency.
An accelerated version of this has been seen with email.  Accessing your own
personal IMAP account from Google today has been significantly restricted under
security arguments.  Getting OAuth credentials that can access someone’s IMAP
accounts with their approval has become increasingly harder.  It is also very
costly.
Username and password authentication has largely been removed.  Even the
app-specific passwords on Google are now entirely undocumented.  They are no
longer exposed in the settings unless you know the
link 2.
What Does Any Of This Mean?
I don’t know.  I am both a user of passkeys and generally wary of making myself
overly dependent on tech giants and complex solutions.  I’m noticing an
increased reliance and potential loss of access to my own data.  This does
abstractly concern me.  Not to the degree that it changes anything I’m doing,
but still.  As annoying as managing usernames and passwords was, I don’t think
I have ever spent so much time authenticating on a daily basis.  The systems
that we now need to interface with for authentication are vast and
complex.
This might just be the path we’re going.  However, it is also one where we
maybe want to reflect a little bit on whether this is really what we want.
Edit: I reworded the statement about pass key exports to not misrepresent
the original comment on GitHub.



The details can be debated, but the protocol explicitly does not permit
a user to just hold on to a symmetrically encrypted export (or even a
plain text one).  The best option is the HPKE scheme.↩

This OAuth dependency also puts Open Source projects in an interesting
situation.  For instance, the Thunderbird client ships with OAuth
credentials for Google when you download it from Mozilla.  However, if you
self-compile it, you don’t have that access.↩




  
  This entry was tagged
    
      security and 
      thoughts
  

  
    copy as / view markdown
  
  
  

      ]]></content:encoded>
        </item>
    </channel>
</rss>