<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 29 Aug 2025 02:16:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Claude Sonnet Will Ship in Xcode]]></title>
            <link>https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058688</guid>
        </item>
        <item>
            <title><![CDATA[Python: The Documentary]]></title>
            <link>https://lwn.net/Articles/1035537/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058171</guid>
            <description><![CDATA[Attendees at EuroPython had the chance to preview part of Python: The Documentary during a keyn [...]]]></description>
            <content:encoded><![CDATA[
Attendees at EuroPython had the chance to preview part of
Python: The Documentary during a
keynote panel. The full film, created by CultRepo, is now available on YouTube:


This is the story of the world's most beloved programming language:
Python. What began as a side project in Amsterdam during the 1990s
became the software powering artificial intelligence, data science and
some of the world's biggest companies. But Python's future wasn't
certain; at one point it almost disappeared.

This 90-minute documentary features Guido van Rossum, Travis
Oliphant, Barry Warsaw, and many more, and they tell the story of
Python's rise, its community-driven evolution, the conflicts that
almost tore it apart, and the language's impact on... well...
everything.


The video
of the keynote is also available.

            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RSS is awesome]]></title>
            <link>https://evanverma.com/rss-is-awesome</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058024</guid>
            <description><![CDATA[Evan briefly discusses rss]]></description>
            <content:encoded><![CDATA[ 
    
      NetNewsWire
    
  
    
       is my latest most-used iPhone app. It is a simple, free RSS reader. 
    
  
    
      RSS is an old technology that it seems most people have forgotten about. Here's how it works: you enter a link to an RSS "feed", and your app pulls data from this feed every few minutes or so. When there is a new post from your feed, that post is pulled directly to your app. 
    
  
    
      RSS is really simple, so it is still very well supported. Notably, all substack publications automatically have an RSS feed included at 
    
  https://{{substack-domain}}/feed
    
      . 
    
  
    
      Blogs are great but I don't enjoy reading posts in my email, having to remember the websites each one is hosted at, or reading from each publications' different typesetting opinions with varying pop-ups and advertisements. An RSS reader centralizes all content from your blogs into a single place for reading. 
    
  
    
      Since I started using this app I spend much more of my "mindless phone time" reading blog posts, which I think is somewhere between marginally good and good. 
    
   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rupert's Property]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/08/28/a-polyhedron-without-ruperts-property/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057561</guid>
        </item>
        <item>
            <title><![CDATA[The Space Shuttle Columbia disaster and the over-reliance on PowerPoint (2019)]]></title>
            <link>https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057404</guid>
            <description><![CDATA[Weâ€™ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  Weâ€™re so used to it we expect it.  We accept it.  We even consider it â€˜learningâ€™. As an educator I push against â€˜death by PowerPointâ€™ and I'm fascinated with how we]]></description>
            <content:encoded><![CDATA[

      

      

      
        
          
            
              
            
          
        
      


      
      
      

      
        
        

  
  

    

    

      

      
        
          
        
        

        
          
            
          
        

        
          
          
            The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)
          
        
      
        
      

    
  Weâ€™ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  Weâ€™re so used to it we expect it.  We accept it.  We even consider it â€˜learningâ€™. As an educator I push against â€˜death by PowerPointâ€™ and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audienceâ€™s inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Whereas other shuttle crews had focused on work to the Hubble Space Telescope or to the International Space Station this mission was one of pure scientific research.  The launch proceeded as normal.  The crew settled into their mission.  They would spend 16 days in orbit, completing 80 experiments.  One day into their mission it was clear to those back on Earth that something had gone wrong.  As a matter of protocol NASA staff reviewed footage from an external camera mounted to the fuel tank.  At eighty-two seconds into the launch a piece of spray on foam insulation (SOFI) fell from one of the ramps that attached the shuttle to its external fuel tank.  As the crew rose at 28,968 kilometres per hour the piece of foam collided with one of the tiles on the outer edge of the shuttleâ€™s left wing.  


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Frame of NASA launch footage showing the moment the foam struck the shuttleâ€™s left wing (Creative Commons)
          
        
      
        
      

    
  It was impossible to tell from Earth how much damage this foam, falling nine times faster than a fired bullet, would have caused when it collided with the wing.   Foam falling during launch was nothing new.  It had happened on four previous missions and was one of the reasons why the camera was there in the first place.  But the tile the foam had struck was on the edge of the wing designed to protect the shuttle from the heat of Earthâ€™s atmosphere during launch and re-entry.  In space the shuttle was safe but NASA didnâ€™t know how it would respond to re-entry.  There were a number of options.  The astronauts could perform a spacewalk and visually inspect the hull.  NASA could launch another Space Shuttle to pick the crew up.  Or they could risk re-entry.  NASA officials sat down with Boeing Corporation engineers who took them through three reports; a total of 28 slides.    The salient point was whilst there was data showing that the tiles on the shuttle wing could tolerate being hit by the foam this was based on test conditions using foam more than 600 times smaller than that that had struck Columbia.  This is the slide the engineers chose to illustrate this point:

  NASA managers listened to the engineers and their PowerPoint.  The engineers felt they had communicated the potential risks.  NASA felt the engineers didnâ€™t know what would happen but that all data pointed to there not being enough damage to put the lives of the crew in danger.  They rejected the other options and pushed ahead with Columbia re-entering Earthâ€™s atmosphere as normal.  Columbia was scheduled to land at 0916 (EST) on February 1st 2003.  Just before 0900, 61,170 metres above Dallas at 18 times the speed of sound, temperature readings on the shuttleâ€™s left wing were abnormally high and then were lost.  Tyre pressures on the left side were soon lost as was communication with the crew.  At 0912, as Columbia should have been approaching the runway, ground control heard reports from residents near Dallas that the shuttle had been seen disintegrating.  Columbia was lost and with it her crew of seven.  The oldest crew member was 48.  The shuttle programme was on lock down, grounded for two years as the investigation began.  The cause of the accident became clear: a hole in a tile on the left wing caused by the foam let the wing dangerously overheat until the shuttle disintegrated.  The questions to answer included a very simple one: Why, given that the foam strike had occurred at a force massively out of test conditions had NASA proceeded with re-entry?  Edward Tufte, a Professor at Yale University and expert in communication reviewed the slideshow the Boeing engineers had given NASA, in particular the above slide.  His findings were tragically profound.


 Firstly, the slide had a misleadingly reassuring title claiming that test data pointed to the tile being able to withstand the foam strike.  This was not the case but the presence of the title, centred in the largest font makes this seem the salient, summary point of this slide.  This helped Boeingâ€™s message be lost almost immediately.























  Secondly, the slide contains four different bullet points with no explanation of what they mean.  This means that interpretation is left up to the reader.  Is number 1 the main bullet point?  Do the bullet points become less important or more?  Itâ€™s not helped that thereâ€™s a change in font sizes as well.  In all with bullet points and indents six levels of hierarchy were created.  This allowed NASA managers to imply a hierarchy of importance in their head: the writing lower down and in smaller font was ignored.  Actually, this had been where the contradictory (and most important) information was placed.  


Thirdly, there is a huge amount of text, more than 100 words or figures on one screen.   Two words, â€˜SOFIâ€™ and â€˜rampâ€™ both mean the same thing: the foam.  Vague terms are used.  Sufficient is used once, significant or significantly, five times with little or no quantifiable data.  As a result this left a lot open to audience interpretation.  How much is significant?  Is it statistical significance you mean or something else?  
























Finally the single most important fact, that the foam strike had occurred at forces massively out of test conditions, is hidden at the very bottom.  Twelve little words which the audience would have had to wade through more than 100 to get to.  If they even managed to keep reading to that point.  In the middle it does say that it is possible for the foam to damage the tile.  This is in the smallest font, lost. 























  NASAâ€™s subsequent report criticised technical aspects along with human factors.  Their report mentioned an over-reliance on PowerPoint: â€œThe Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA.â€  Edward Tufteâ€™s full report makes for fascinating reading. Since being released in 1987 PowerPoint has grown exponentially to the point where it is now estimated than thirty million PowerPoint presentations are made every day.  Yet, PowerPoint is blamed by academics for killing critical thought.  Amazonâ€™s CEO Jeff Bezos has banned it from meetings.   Typing text on a screen and reading it out loud does not count as teaching.  An audience reading text off the screen does not count as learning.  Imagine if the engineers had put up a slide with just: â€œfoam strike more than 600 times bigger than test data.â€  Maybe NASA would have listened.  Maybe they wouldnâ€™t have attempted re-entry.  Next time youâ€™re asked to give a talk remember Columbia. Donâ€™t just jump to your laptop and write out slides of text.  Think about your message.  Donâ€™t let that message be lost amongst text.  Death by PowerPoint is a real thing.  Sometimes literally.Thanks for reading - Jamie 


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Columbiaâ€™s final crew (from https://www.space.com/19436-columbia-disaster.html)
          
        
      
        
      

    

    

    

  

  

  

  
  
  


        
        
      

      

      

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sometimes CPU cores are odd]]></title>
            <link>https://anubis.techaro.lol/blog/2025/cpu-core-odd/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057346</guid>
            <description><![CDATA[TL;DR: all the assumptions you have about processor design are wrong and if you are unlucky you will never run into problems that users do through sheer chance.]]></description>
            <content:encoded><![CDATA[Protected by Anubis From Techaro. Made with â¤ï¸ in ðŸ‡¨ðŸ‡¦.Mascot design by CELPHASE.This website is hosted by Techaro. If you have any complaints or notes about the service, please contact support@techaro.lol and we will assist you as soon as possible.
-- ImprintThis website is running Anubis version v1.22.0-pre1-8-g21c3e0c.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Expert LSP the official language server implementation for Elixir]]></title>
            <link>https://github.com/elixir-lang/expert</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057322</guid>
            <description><![CDATA[Official Elixir Language Server Protocol implementation - elixir-lang/expert]]></description>
            <content:encoded><![CDATA[Expert
Expert is the official language server implementation for the Elixir programming language.
Installation
You can download Expert from the releases page for your
operating system and architecture. Put the executable somewhere on your $PATH, like ~/.local/bin/expert
For editor specific installation instructions, please refer to the Installation Instructions
Nightly Builds
If you want to try out the latest features, you can download a nightly build.
Using the GH CLI, you can run the following command to download the latest nightly build:
gh release download nightly --pattern 'expert_linux_amd64'
Then point your editor to the downloaded binary.
Building from source
To build Expert from source, you need Zig 1.14.1 installed on your system.
Then you can run the following command:
just release-local
This will build the Expert binary and place it in the apps/expert/burrito_out directory. You can then point your
editor to this binary.
Sponsorship
Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.






Corporate
For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at danj@erlef.org.
Individual
Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.
Other resources

Architecture
Development Guide
Glossary
Installation Instructions

LICENSE
Expert source code is released under Apache License 2.0.
Check LICENSE file for more information.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Speed-coding for the 6502 â€“ a simple example]]></title>
            <link>https://www.colino.net/wordpress/en/archives/2025/08/28/speed-coding-for-the-6502-a-simple-example/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057209</guid>
            <description><![CDATA[A real-life example showing how to scale an image in either 10 seconds, or 0.2 seconds, using 6502 assembly.]]></description>
            <content:encoded><![CDATA[
	
		
		
			

				
				
				
					2025/08/28
						 - 
						About 5 minutes read
					
					
				

				
				
					
						
Usually clocked at 1MHz, with no hardware support for multiplication or division, and limited support for bit shifting, it is often important to take a step back from an algorithm in order to make it do the same thing, hundreds of times faster.



Note: this article doesnâ€™t describe any technological breakthrough or extremely clever new way of doing things. Itâ€™s just a real-life example using a little part of an algorithm I made, and where at first, I stopped at step 2 instead of going all the way to the last step.



In this example, we want to scale down a 256Ã—192 bitmap down to 192Ã—144, which means the scaling factor is 3/4.



Weâ€™re going to have to apply this factor to every one of the 192 lines, and for each line, for every one of the 256 pixels composing the line. This means weâ€™ll have to apply our factor (192*256)+192 times, 49344 times in total. 



We can also note that the numbers we want to scale are 8 bits, but at first, it looks like weâ€™ll need to do 16 bits math, as 256*3 is greater than 256.



The first, naive way is to do scaled_value = value*3/4:



lda value       ; load the value
ldx #3          ; load the multiplicator
jsr mult_a_x    ; jump to the multiplication function
                ; which returns a 16-bit number in A and X
ldy #4          ; load the divisor
jsr div_ax_y    ; jump to the division function
sta scaled_value; we have our result!



Given that multiplication and division is made via software, and each one takes about 200 cycles (approximately), our image scaling will require about 20 million cycles, aka 20 seconds at 1MHz!



So letâ€™s try to avoid multiplication and division. Notice our formula can also be written as scaled_value = (value*2 + value)/4, and as we know, multiplying by 2 is shifting left, and dividing by 4 is shifting right twice: scaled_value = (value<<1 + value) >> 2, which in assembly, translates to:



 lda value      ; load the value
 ldx #$00       ; init the high byte of the multiplication to zero
 stx tmp1_zp    ; store it in a variable, as the 6502 can't bit-
                ; shift the X or Y registers

 asl a          ; shift left, which sets the carry if the result
                ; is > 255.
 rol tmp1_zp    ; bring the carry into the high byte
                ; We have done *2

 adc value      ; Add the value to the temporary result
 bcc :+         ; Did we overflow again?
 inc tmp1_zp    ; if so, we need to increment the high byte.
:
                ; We now have value*3 in A and tmp1_zp. Let's 
                ; divide by four.

lsr tmp1_zp     ; Shift high byte right, leaving low bit in carry
ror a           ; Shift low byte taking carry in high bit
lsr tmp1_zp     ; and a second time.
ror a



We have made the same calculation using â€œonlyâ€ 35 cycles at best/39 at worst. Taking an average of 37 cycles, we can now scale our image in a bit less than two seconds, a ten-fold improvement over the first algorithm. But we can do better!



Our formula can also be rewritten as scaled_value = value*1.5/2, that is, scaled_value = (value>>1 + value)>>1. It looks like it could lead to loss of precision, but it doesnâ€™t. That will simplify greatly the previous algorithm, as we will be able to do 9-bits math (the carry being the 9th bit):



lda value      ; load our value
lsr a          ; divide it by 2 (sets the carry if low bit=1)
clc            ; clear the carry, we don't care about
               ; what the low bit was
adc value      ; add our value to its half (sets the carry if >255)
ror a          ; divide by two, bringing the carry to the high bit.



Weâ€™re now at a nice 12 cycles per operation! And weâ€™re able to scale our image in only 0.6 seconds.



Should we stop there? We also know that â€œthere is no faster computation than the one you already know the result ofâ€, which, is 6502 assembly, translates to â€œprovide lookup tables to spare computing to the computerâ€. It often comes trading size for speed: in this case, weâ€™ll need a table of 256 values containing pre-calculated results of X*3/4.



I didnâ€™t really want to dedicate a full memory page to that little part of a much larger program where memory constraints existâ€¦ But, this program already has a few buffers that are completely unused, and trashable, during this scaling operation. So, instead of providing a dedicated, calculated at build time table, I will just generate it before starting scaling, applying the formula only once for every possible value in the 0-255 range:



 ldy #$00       ; start at 0

:
 tya            ; transfer to A
 ....           ; the previous 12-cycles *1.5/2 algorithm goes here
 sta table,y    ; store the result
 iny
 bne :-         ; and loop 256 times.



Our table is now built at runtime, which has a cost of less than 6000 cycles, and scaling down a pixel is now just a matter of lda table,y: 4 cycles. We just went from needing 10 seconds to scale an image, to 0.2 seconds.
																
					
				
				
				
			
		
		
		
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fuck up my site â€“ Turn any website into beautiful chaos]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid>
            <description><![CDATA[Transform any website into pure chaos. Add burning cursors, Comic Sans everything, fake cursors, and more chaos features to any site. Some people just want to watch the web burn.]]></description>
            <content:encoded><![CDATA[fuckupmysiteSome people just want to watch the web burnTry:ðŸ˜ˆChaos Settings3 of 6 agents of chaos enabledHeads up: Not every site plays nice with the chaos. Got feedback or discovered something broken? Let me know on Twitter]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[There Goes the American Muscle Car]]></title>
            <link>https://thedispatch.com/article/dodge-challenger-muscle-cars/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057002</guid>
            <description><![CDATA[Have electric vehicles, regulations, and changing consumer preferences killed a classic?]]></description>
            <content:encoded><![CDATA[
        




    
    
    
    
        Turn any article into a podcast. Upgrade now to start listening.
        

    PONTIAC, Michiganâ€”Traveling a fair amount for work outside of major metros requires relying on rental cars. And relying on rental cars quickly teaches you that while you might reserve a standard sedan every time, there is no reason to expect the rental car agency to actually give you the keys to a sedan when you show up at the counter.

Eventually, you realize your fate is entirely in the hands of the rental car gods, and the rental car gods are capricious gods.    


Sometimes, the rental car gods frown upon you. This happened to me at the 2011 Iowa straw poll, when I ended up driving a Chevy Aveo across the entire state. If you donâ€™t know what a Chevy Aveo is, consider yourself lucky. If youâ€™ve had the misfortune to ride in one, you know that a go-kart is a smoother, roomier, and safer ride.Â 

Sometimes, the rental car gods laugh at (or perhaps with) you, which is what happened to me in 2017 when I went to Alabama to cover the Senate campaign of Roy Moore, a notorious creep and alleged pervert. I was told at the rental car counter they couldnâ€™t honor my reservation for a sedan, and the only vehicle available was a boxy full-size van. Pulling up to the first campaign event in the kind of van youâ€™d half-expect to come with the words FREE CANDY scrawled in paint on its side, I was worried Iâ€™d be mistaken for a fan of Moore.

But sometimes the rental car gods smile upon you. This happened to me at the 2020 Iowa caucuses.Â 

â€œWould a Dodge Challenger be okay?â€ the agent asked me in the Des Moines airport.Â 

Why, yes, it would be okay.

Much more than okay.

I had, once before, ended up with a Dodge Charger, the modern sedan version of the two-door Challenger, while covering a special election in upstate New York and was instantly impressed.Â 

Iâ€™m not a car guy, so I lack the expertise to explain to you on a technical level why the Dodge Challenger is the greatest car on the road todayâ€”the same way I canâ€™t explain to you why Caravaggioâ€™s The Calling of St. Matthew is the best painting ever painted or why Berniniâ€™s David is the best sculpture ever sculpted. Each of these statements just seems to me to be self-evidently and objectively true. But I can tell you there is nothing quite like going from 0 to 60 in a Challenger. Over the course of four days and 469 miles in January 2020, I stuck to Iowaâ€™s blessedly snow-free backroads whenever possible. Every stop sign was no longer an annoyance slowing me down from getting from Point A to Point B, but another opportunity to experience the thrill of putting the pedal to the metal in a Challenger.

While speeds can vary depending on the carâ€™s trim, the most souped-up Challenger can go from 0 to 60 in 1.66 secondsâ€”faster than the fastest Ferrari. My rental took more than 4 seconds to hit that mark, but thereâ€™s more than just its speed that makes the Challenger great. Thereâ€™s the rumble and roar of the V8 HEMI engine that you can feel in your chest. Thereâ€™s the beautiful sleek retro design straight out of 1970â€”the heyday of the classic American muscle car. You donâ€™t have to take it from meâ€”just listen to Jay Leno, the famous comedian and avid car collector, who wrote last year: â€œTo me, the Challenger is really the last great American road car.â€Â 

Iâ€™ve driven more than 100 different cars in my life, including some nice ones, but the Challenger is really the only car Iâ€™ve ever loved. So when my wifeâ€™s work-from-home status was starting to end in 2021 and we owned only one car (a 2011 Hyundai Tucson), I told her I was interested in buying a used Challenger that I found online for $16,000 with 50,000 miles on it.

â€œYou know your friends will laugh at you,â€ she said.Â 

â€œYes, Iâ€™ve considered this,â€ I replied, fully aware that someone typically dressed in Gap Outlet polos or Brooks Brothers dress shirts doesnâ€™t exactly look like a muscle-car guy. â€œBut there are much more expensive and destructive ways to have a midlife crisis.â€

Ultimately, I gave into the practical reality that my wife, whose daily commute could push two hours roundtrip, needed a new car and that I, with a commute that is often one flight of stairs, did not. She got a Mazda, and I figured my midlife crisis could be postponed until our Hyundai gave up the ghost.

But then, tragedy struck: Dodge announced that 2023 would be the last year it would produce the Challenger and the Charger. Then Chevy killed off the Camaro. The last muscle car left in production was the Ford Mustang. But even with the market all to itself, Mustang sales dropped 31 percent in the first quarter of 2025.

Why does the American muscle car seem to be dying off? What, if anything, is America losing as it fades away? I set out to answer these questions after The Dispatchâ€™s editors encouraged us writers to expand our horizons beyond the typical political fare (see Mike Warren on Tiki culture, for example). I decided that if I couldnâ€™t own a muscle car, at least I could write about it. And thatâ€™s how I ended up in Pontiac, Michigan, earlier this month inhaling the smoke of burning rubber and talking to muscle-car enthusiasts.



That distinct smell of burning rubber is the first thing you notice when approaching the venue at Roadkill Nights. Woodward Avenue has been closed down for a day of legal drag-racing, and hot rods are doing burnouts for improved traction just before they make their trial runs.Â 

Those who drove in but arenâ€™t competing have their cars lined up on the street outside. Iâ€™m greeted by an array of Challengersâ€”purple, neon green, orange, metallic blue. But the whole scene looks more like a county fair for car enthusiasts than a shot out of Fast & Furious. There are families and food trucks, but instead of the Tilt-a-Whirl and Loop-o-Plane, event-goers wait in long lines for hired drivers to take them on drift ridesâ€”in which the driver takes such sharp turns that the carâ€™s rear tires screech and slide sideways. For little kids, some barely old enough to walk, there are Power Wheels-style muscle cars nearby to drive. A full-size monster truck does donuts in a parking lot for entertainment, but the main show for the thousands of attendees is the drag racing.

The first person I talk to on the bleachers overlooking the races that day is Mike Sherrow of Suffolk, Virginia, who has attended the event 10 years in a row. Like many of the enthusiasts I spoke to, Sherrowâ€™s love of muscle cars was inherited from his father. â€œLego started it for me, and then building my dadâ€™s â€˜65 Chevy truck,â€ Sherrow told me. â€œIt was like the family hot rod.â€Â 

Asked what he loves about muscle cars now, Sherrow points to the creativity in modifying the vehicles with oneâ€™s own hands and tools: â€œBuilding your own stuff, and taking somebodyâ€™s car and modifying it to make it what you want. Itâ€™s just a part of car culture. And itâ€™s going away. This [event] is keeping it alive.â€

Sherrow attributes the decline of muscle-car culture to a variety of factors. â€œBeing sued by the EPA for emissions control laws, outlawing even street-legal racing, closing down racetracks because people are building houses too close. Itâ€™s a dying art,â€ he said. â€œThe greats that did it, theyâ€™re passing on, and their knowledge is going away with it.â€

To understand the decline of the American muscle car, it helps to first remember that it has diedâ€”and been resurrectedâ€”before.



                            
                                                                1A 965 Pontiac GTO-Grand Prix (Photo by Pat Brollier/The Enthusiast Network via Getty Images/Getty Images)
                                    
                    

The original muscle car era began, in most tellings, in 1964 with the production of the Pontiac GTO. Competitorsâ€”including the Ford Mustang, Plymouth Barracuda, Dodge Charger and Challenger, Chevrolet Camaro and Chevelleâ€”soon appeared on the scene. While sports cars can be foreign or domestic, muscle cars by definition must be American-made, with a powerful engine in a lightweight body. Though thereâ€™s some debate about what separates muscle cars from domestic sports cars, muscle cars also prioritize straight-line speed over handling and agility and have that, well, know-it-when-you-see-it muscular look (which is why many donâ€™t consider the sporty Corvette to be a muscle car).


It is no coincidence that the muscle-car era began when the oldest baby boomers turned 18. In 1963, Tom Wolfe chronicled the custom-car craze gripping teenagers in America in an 11,000-word article for Esquire magazine titled, â€œThere Goes (VAROOM! VAROOM!) That Kandy Kolored (THPHHHHHH!) Tangerine-Flake Streamline Baby (RAHGHHHH!) Around the Bend (BRUMMMMMMMMMMMMMMMMâ€¦).â€Â 


â€œThousands of kids are getting hold of cars and either hopping them up for speed or customizing them to some extent, usually a little of both,â€ Wolfe wrote. â€œEven the kids who arenâ€™t full-time car nuts themselves will be influenced by which car is considered â€˜boss.â€™â€Â 

This phenomenon was a direct result of Americaâ€™s post-war economic boom. â€œThe war created money,â€ Wolfe wrote in the introduction to his 1965 collection of essays, titled after his Esquire essay. â€œIt made massive infusions of money into every level of society. Suddenly classes of people whose styles of life had been practically invisible had the money to build monuments to their own styles.â€Â 

Where others saw the gaudy or even garish tastes on display in custom cars as unworthy of a second thought, Wolfe saw baroque art objects worthy of study. He presciently predicted in 1963 that car manufacturers in Detroit â€œmay well be on their way to routinizing the charismaâ€ of the custom car. Indeed, within the first two years of production, Ford had sold 1 million Mustangsâ€”before Ford race cars made history in 1966 by beating Ferrari at the 24 Hours of Le Mans.

But by the time Brue Springsteen was singing about muscle cars in â€œBorn to Runâ€ (the album of the same name was released 50 years ago this week)â€”â€œBeyond the Palace, hemi-powered drones scream down the boulevard/ Girls comb their hair in rearview mirrors and the boys try to look so hardâ€â€”the muscle car era had largely come to a close. Production stopped on the Challenger and Charger, respectively, in 1974 and 1978. The first-generation Mustang stopped being produced in 1973 (but different iterations were available, leaving it the only muscle car in continuous production since its birth).Â 

The end of the golden era of the muscle car is simultaneously attributed to environmental regulations like the Clean Air Act of 1970 and that decadeâ€™s oil crisis that made gas-guzzling muscle cars too expensive for most consumers. The baby boomers were, of course, growing up and starting to have babies of their own and seeking more practical vehicles for their families.

Now, history appears to be rhyming. Since the end of the golden era, the muscle car has experienced revivals, the most recent of which began in the aughts, perhaps not coincidentally after the The Fast and the Furious franchise launched in 2001, in which Vin Dieselâ€™s character drives a 1970 Dodge Charger. The movie and its sequels became the highest-grossing franchise of all time for Universal Pictures, and in 2006 Dodge reintroduced the Charger and the Challenger in 2008, selling more than 2 million of the vehicles combined before Dodge recently stopped production.

If you talk to car enthusiasts, the proximate cause of the recent untimely demise of the Challenger, Charger, and Camaro was increasingly onerous fleet-wide emissions standards, and thereâ€™s a lot of truth to that. Those regulations meant that it wasnâ€™t enough for Dodge to simply slap gas-guzzler taxes on the vehicles and purchase offsets from Tesla through a cap-and-trade system to maintain production in the face of billions of dollars of potential fines. It would have made little economic sense to discontinue the vehicles without regulatory pressure: Dodge was selling nearly 80,000 Chargers and more than 50,000 Challengers a year when it announced in 2021 it would discontinue the models and would instead offer a new electric replacementâ€”the Dodge Charger Daytona EV. This model sold just 4,299 units in the first half of 2025â€”its first and potentially only year on the market. Few muscle car enthusiasts were interested in an electric vehicle that replaced the rumble and roar of the V8 engine with speakers and chambers designed to mimic the sound of a gas-powered engine. â€œPretty lame,â€ declared a review in Edmunds. Following the loosening of emissions standards this year, Dodge announced it would be bringing back a gas-powered Charger (but not the Challenger) in 2026 with a V6 engine, not the classic V8.Â 



                            
                                                                A Dodge Challenger and a Dodge Charger on display during the 116th New York International Auto Show at the Javits Convention Center in Manhattan on March 24, 2016. (Photo by Cem Ozdel/Anadolu Agency/Getty Images)
                                    
                    

Jose Pretel, a real-estate photographer and official brand ambassador for Dodge from South Florida who posts photos of his Dodge Charger Hellcat on Instagram (handle @onebadhellcat), acknowledged the challenges the brand has faced but offered an optimistic take at Roadkill Nights in Michigan. â€œI think some people in the community are skeptical of the changes, right? We went full electric. Now weâ€™re back to a twin turbo, smaller motor than before. Theyâ€™re kind of wondering: â€˜What are they doing?â€™â€ Pretel told me. â€œI actually think this motor is going to last longer, be able to take more abuse â€¦ [and] be just as popular in a few years.â€Â 

That, of course, remains to be seen. The environmental regulations that killed off the Challenger and Charger in 2023 are not the entire story behind the decline of the modern muscle car. High interest rates and pricesâ€”driven in part by tariffsâ€”plus a declining percentage of young people having driverâ€™s licenses and changing consumer preferences all play a role. The reintroduced gas-powered Chargerâ€”even if it brings back the V8 engineâ€”could face the same headwinds in 2026 that the Mustang experienced this year that led to a big drop in sales.

To some, the death of the muscle car would simply be a good thing: Theyâ€™re loud, gaudy, and (marginally) harm the environment by burning more fossil fuel per mile driven than other vehicles. But to others, they are objects of beauty, outlets for creativity and innovation, and sources of fun, as well as camaraderie.

In speaking to attendees at Roadkill Nights, it quickly became clear that a love of cars isnâ€™t merely about the vehicles themselves; itâ€™s also a way to build and maintain friendships. Shawn Splaney from Flint, Michigan, who attended the event with two friends, told me he owns a â€˜68 Mercury Cougar thatâ€™s â€œa basket caseâ€ not yet in driving condition and the â€˜91 Mustang he typically drives is a labor of love. â€œIâ€™d rather have the older stuff,â€ Splaney said. â€œMy Mustang is manual steering, manual valve body, where you have to shift the transmissionâ€”you have to drive the car. Thatâ€™s what I like.â€

â€œItâ€™s my Number One hobby,â€ he continued. â€œWe have garage time. Itâ€™s just a time to hang out with the guys, or girls, and itâ€™s relaxing. Itâ€™s fun. Even though youâ€™re struggling with trying to overcome problems at times â€¦ youâ€™re just talking, youâ€™re venting, itâ€™s therapeutic.â€Â 

As a non-car-guy interested in becoming one, Iâ€™m curious to what extent this hobby, like some other male-dominated hobbies, is largely an excuse for drinking beer. â€œWeâ€™re not heavy drinkers,â€ Splaney told me, but â€œsometimes thereâ€™s a victory: â€˜Hey, letâ€™s go to the bar and kind of show off what we did.â€™ Sometimes thereâ€™s that agony of defeat where youâ€™re like, â€˜Hey, maybe I do need a beer because the carâ€™s not fixed yet.â€™â€

Many but not all in attendance are doubtful that car culture can be built up around electric vehicles. Lincoln Brousseau of Grand Blanc, Michiganâ€”a friend of Splaneyâ€™s for 15 years since they first met via an online car forumâ€”thinks maybe itâ€™ll catch on with the younger crowd once the proper recharging infrastructure is in place. But the third friend in the group, Brooke Rennert, a 21-year-old from Rochester Hills putting herself through welding school by working as the only woman at her oil-change job, isnâ€™t having any of it. â€œI donâ€™t like electric cars. I like the sound of a heavy engine. I like the power,â€ she said. â€œAn electric vehicle has power, but in a different way. Itâ€™s not like a big V8, big-block sound.â€

For car enthusiasts who like tinkering or hopping up their engines, thereâ€™s little interest in electric vehicles. â€œThereâ€™s nothing there to change. Youâ€™ve got batteries and a motor,â€ said Jim Schmittinger from Slinger, Wisconsin, who works on EVs in his job for a Honda dealer.Â 




            


        â€œBuilding your own stuff, and taking somebodyâ€™s car and modifying it to make it what you want. Itâ€™s just a part of car culture. And itâ€™s going away.â€
                    
                                    Mike Sherrow
                                            
                

Schmittinger estimates heâ€™s poured $100,000 into the engine of the Buick GNX heâ€™s racing that day, and he said money is one big problem with car culture: â€œPeople will spend money they don't have to compete with somebody they donâ€™t know.â€ But the worst trend, he said, is that social media has changed the culture for the worse: â€œItâ€™s not about the cars and getting together with people anymore. Itâ€™s about how many likes I get on YouTube or Facebook.â€Â 

Asked for practical advice about how a non-car-guy like myself might become one, Schmittinger simply said: â€œStart saving your money.â€ Others tell me the first step is simply getting a car that you love. â€œFind a car that you actually are enthusiastic about that you want to have and keep,â€ Splaney said. â€œStart learning here and there the small stuff, whether itâ€™s just doing oil changes, and just keep pushing that up. We all started somewhere.â€Â 

Rennert, the young woman putting herself through welding school, had some advice for other ladies looking to become car gals: â€œMake sure youâ€™re in a group of people who respect you. Itâ€™s kind of hard sometimes, but you can do it just the same as any guy can do. Meet people. Donâ€™t be afraid not to know anything. Donâ€™t be afraid to ask questions. Do your research. Find a vehicle that really interests you, and go from there.â€

As Iâ€™m getting ready to leave Pontiac, Iâ€™m left wondering, if I ever get around to buying a Challenger, just how expensive and just how old it might be. But, as Schmittinger listened to a friend of his bemoan the demise of the Challenger, he piped up with a prediction: â€œItâ€™ll be back.â€        

    
            
            John McCormack is a senior editor at The Dispatch and is based in Washington, D.C. Prior to joining the company in 2023, he was Washington correspondent at National Review and a senior writer at The Weekly Standard. He is also a visiting fellow at the Ethics and Public Policy Center. When John is not reporting on politics and policy, he is probably enjoying life with his wife and daughter in northern Virginia or having fun visiting family in Wisconsin.        
        
    
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You no longer need JavaScript: an overview of what makes modern CSS so awesome]]></title>
            <link>https://lyra.horse/blog/2025/08/you-dont-need-js/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056878</guid>
            <description><![CDATA[An overview of what makes modern CSS so awesome.]]></description>
            <content:encoded><![CDATA[
    
  
  2025-08-28  Â¦ css


  So much of the web these days is ruined by the bloat that is modern JavaScript frameworks. React apps that take several seconds to load. NextJS sites that throw random hydration errors. The node_modules folder that takes up gigabytes on your hard drive.
Itâ€™s awful. And you donâ€™t need it.




  
    
      Name
      Status
      Type
      Size
      Time
    
  
  
    app200document153.8 kB51 ms
    6920616d20612066-s.p.6f6e7421.woff2200font31.5 kB32 ms
    686579206d652074-s.p.6f6f2121.woff2200font28.5 kB116 ms
    77687920646f6573.css200stylesheet253 kB47 ms
    2074686520646566.js200script648 kB83 ms
    61756c74206e6578.js200script166 kB363 ms
    746a732074616b65.js200script83.3 kB46 ms
    turbopack-20757020302e354d.js200script38.0 kB95 ms
    423f207468617427.js200script414 B34 ms
    73206d6f72652074.js200script32.6 kB49 ms
    68616e206d792065.js200script15.1 kB71 ms
    6e7469726520626c.js200script143 kB48 ms
    6f6721 hey there!200script4.1 kB103 ms
  



The intro paragraph of this post is tongue-in-cheek. Itâ€™s there to get you to read the rest of the post. I suspect the megabytes of tracking scripts intertwined with bad code is far more likely to be the real culprit behind all the terrible sites out there. Web frameworks have their time and place. And despite my personal distaste for them, I know they are used by many teams to build awesome well-optimized apps.
Despite that, I think thereâ€™s some beauty in leaving it all behind. Not just the frameworks, but JavaScript altogether. Not every site needs JavaScript. Perhaps your e-commerce site needs it for its complex carts and data visualization dashboards, but is it really a necessity for most of whatâ€™s out there?
Itâ€™s actually pretty incredible what HTML and CSS alone can achieve.

So, what do you say?
My goal with this article is to share my perspectives on the web, as well as introduce many aspects of modern HTML/CSS you may not be familiar with. Iâ€™m not trying to make you give up JavaScript, Iâ€™m just trying to show you everything thatâ€™s possible, leaving it up to you to pick what works best for whatever youâ€™re working on.
I think thereâ€™s a lot most web developers donâ€™t know about CSS.
And I think JS is often used where better alternatives exist.
So, let me show you whatâ€™s out there.

â€œBut CSS sucksâ€
I believe a lot of the negativity towards CSS stems from not really knowing how to use it. Many developers kind of just skip learning the CSS fundamentals in favor of the more interesting Java- and TypeScript, and then go on to complain about a styling language they donâ€™t understand.
I suspect this is due to many treating CSS as this silly third wheel for adding borders and box-shadows to a webapp. Itâ€™s undervalued and often compared to glorified crayons, rather than what it really is - a powerful domain-specific programming language.
Itâ€™s telling when to this day the only CSS joke in the webdev circles is centering a div.


i am a div

body {  display: flex;  flex-direction: rowcolumnrow-reversecolumn-reverse;  flex-wrap: nowrapwrap;  align-content: centerflex-startflex-endspace-aroundspace-betweenstretch;  justify-content: centerflex-startflex-endspace-aroundspace-betweenspace-evenly;  align-items: centerflex-startflex-endstretchbaseline;}




Yes, the syntax isnâ€™t the prettiest, but is it really that hard?
Besides, your devtools probably1 come with a fun little gadget that lets you fiddle with the flexbox by just clicking around. You donâ€™t even need to remember the syntax.

I donâ€™t think CSS is fundamentally any more difficult than JS, but if you skip the basics on one and only focus on the other, itâ€™s no surprise it feels that way.
â€œBut itâ€™s painful to writeâ€
Another source of disdain for CSS is how awful it has been to write in the past. This is very much true, and is probably why things like Sass and Tailwind2 exist.
But thatâ€™s the thing, it used to be bad.



ðŸ¦Š


Rebane@rebane2001
btw u should write css like

cool-thing {
    display: flex;
    &[shadow] {
        box-shadow: 1px 1px #0007;
    }
    @media (width < 480px) {
        flex-direction: column;
    }
}

and html like

<cool-thing shadow>wow</cool-thing>

because it's allowed & modern & neat!
11:58 AM Â· Apr 8, 2025

â¤ï¸ 1.5K


(yes! the code above is standards compliant3)
In the past few years, CSS has received a ton of awesome quality-of-life additions, making it nice to do stuff that has historically required preprocessors or JavaScript.
Nesting is definitely one of my favorite additions!
In the past, youâ€™ve had to write code that looks like this:
:root {
  --like-color: #24A4F3;
  --like-color-hover: #54B8F5;
  --like-color-active: #0A6BA8;
}

.post {
  display: block;
  background: #EEE;
  color: #111;
}

.post .avatar {
  width: 48px;
  height: 48px;
}

.post > .buttons {
  display: flex;
}

.post > .buttons .label {
  font-size: 24px;
  padding: 8px;
}

.post > .buttons .like {
  cursor: pointer;
  color: var(--like-color);
}

.post > .buttons .like:hover {
  color: var(--like-color-hover);
}

.post > .buttons .like:active {
  color: var(--like-color-active);
}

@media screen (max-width: 800px) {
  .post > .buttons .label {
    font-size: 16px;
    padding: 4px;
  }
}

@media (prefers-color-scheme: dark) {
  .post {
    background: #222;
    color: #FFF;
  }
}


And yeah, thatâ€™s pretty awful to work with. For anything that involves multiple chained selectors, you kind of have to keep a mental map of how every parent selector relates to its children, and the more CSS you add the harder it gets.
But letâ€™s try it with nesting:
:root {
  --like-color: #24A4F3;
  --like-color-hover: hsl(from var(--like-color) h s calc(l + 10));
  --like-color-active: hsl(from var(--like-color) h s calc(l - 20));
}

.post {
  display: block;
  background: #EEE;
  color: #111;
  @media (prefers-color-scheme: dark) {
    background: #222;
    color: #FFF;
  }
  .avatar {
    width: 48px;
    height: 48px;
  }
  & > .buttons {
    display: flex;
    .label {
      font-size: 24px;
      padding: 8px;
      @media (width <= 800px) {
        font-size: 16px;
        padding: 4px;
      }
    }
    .like {
      cursor: pointer;
      color: var(--like-color);
      &:hover { color: var(--like-color-hover); }
      &:active { color: var(--like-color-active); }
    }
  }
}

That is way nicer to read4! All the relevant parts are right next to each other, so itâ€™s a lot easier to understand whatâ€™s going on. Seeing the &:hover and &:active right next to the .like button is especially nice imo.
And since you can sort of see the structure - the parent selectors â€œguardingâ€ the child ones - it also makes it a lot easier to get away with short and simple class names (or even referring to elements themselves).
You may have noticed that Iâ€™m also making use of relative colors in the second example. I think the MDN article has a lot of awesome examples, but the jist of it is that you can take an existing color, modify it in many different ways across multiple color spaces, and mix it with other colors using color-mix().
/* remove blue from a color */
rgb(from #123456 r g 0);
/* make a color transparent */
rgb(from #123456 r g b / 0.5);
/* make a color lighter */
hsl(from #123456 h s calc(l + 10));
/* change the hue in oklch color space */
oklch(from #123456 l c calc(h + 10));
/* mix two colors in oklab color space */
color-mix(in oklab, #8CFFDB, #04593B 25%);

These snippets are really useful for when you want something to be just ever so slightly darker or brighter, such as a button hover effect or a matching border color, and theyâ€™re way nicer to use than doing all those color conversions in JavaScript. If youâ€™re feeling particularly adventurous, you could even go ahead and generate your entire color scheme in just CSS.

100200300400500600700800900
-40Â°-20Â°0Â°+20Â°+40Â°
primarycomplimentarysecondary
successdangerwarninginfo

view-source


(yes! the color picker above is written in just css)
Safari is currently broken when handling of cqw/cqh units, therefore the demo above may not work correctly. If this happens, try using Firefox or Chrome instead.
There are so many cool new CSS features that make writing it just that little bit nicer. Things like letting you use (width <= 768px) instead of (max-width: 768px) in your @media query, the lh unit that matches the line-height, the scrollbar-gutter property that solves the little scrollbar-related layout shifts, or the ability to finally center stuff vertically without flex/grid.


Baseline
And all of this is brought together by the cherry on top that is Baseline. Itâ€™s a guarantee that a specific feature works in every major browser5, and it also lets you know since when - newly available features work in all the latest browsers, and widely available ones work in browsers up to 2.5 years old. Nesting, for example, has been fully supported in all browsers since December 2023, and thus will become widely available in June 2026. You can find the Baseline symbols in various places, such as the MDN docs6.
These are just a few examples of what makes modern CSS so much nicer to write than what we had even just 5 years ago. It almost feels like comparing ES37 to ECMAScript 2025 - and I wouldnâ€™t blame your grudge if the former is what youâ€™re used to.
Why bother?
Okay, so CSS has more quality-of-life stuff than before. Still, why would one choose to use it over something else? Doesnâ€™t JavaScript already let us do everything just fine?
You need to disable JavaScript to run this app.
I think my reasons for using CSS fall into two main categories - because some users donâ€™t want to use JavaScript, and because doing things in CSS can be genuinely better.
My blog, for example, focuses on infosec topics. Many security researchers (myself included) use a hardened browser configuration to protect themselves, which often means disabling JavaScript by default. I think itâ€™s nice that they can fully experience my blog without changing their security settings or running a separate, sandboxed browser.
The same goes for privacy-conscious users, and it makes sense! As an experiment, I opened up a local Estonian news site in a web browser with JavaScript enabled. Can you guess how many js files it fetched? (answer in footnote8) Thatâ€™s crazy! You do not want that running on your computer.
But surely, you are not one of the evil devs who loads a double-digit number of analytics scripts on your site - is there still any reason to reach for CSS?
Well, I think a lot of things are just plain nicer to make in HTML/CSS, both from the developer and end-user perspectives, be it for ease of use, accessibility, or performance.
Hover effects for your buttons? Toast animations? Input validation? All of these things just work in CSS, and you wonâ€™t have to reinvent the wheel, or throw kilobytes of someone elseâ€™s code at it. There will always be some cases where you do need that extra flexibility JavaScript often provides, but if you donâ€™t need that, and doing it in CSS is easier, then why not save yourself the trouble?


  
  
  
  
  
  
  
  
  
  
  
  













And the performance of CSS is so much better! Every JavaScript interaction has to go through an event loop that wastes CPU cycles, eats some battery, and adds that tiny bit of stutter to everything.
Sure, in the grand scale of things it isnâ€™t that bad, APIs like requestAnimationFrame are really good at keeping things smooth. But CSS animations run in the separate compositor thread, and arenâ€™t affected by stutters and blocking in the event loop.
It makes quite a difference on low-end devices, but feels nice even on high-end ones. CSS animations on my 240hz monitor look amazing9 - JS can look pretty good too, but it has that tiny bit of stutter to it that keeps it from being perfect, especially if you plan on running other heavy code at the same time.
It also means you wonâ€™t have to worry as much about optimization, as the browser takes care of a lot more of the rendering side of things, and often runs your stuff on the GPU if possible.
Pro tip! Wanna trigger animations from JS anyways? Use the modern Web Animations API to easily play the smooth CSS animations from JS.
Transitioning

Speaking of which, I think itâ€™s time I start showing you practical examples, and a good place to start showing the styles is well, @starting-style.
In the past it has been pretty annoying to add start animations (such as fade-ins) to elements. Youâ€™ve had to either set up an entire CSS animation with a separate @keyframes block to go with it, or do a transition using JavaScript where you first add an element to the page, then wait a frame, and then add a class to the element.
.toast {
  transition: opacity 1s, translate 1s;
  opacity: 1;
  translate: 0 0;
  @starting-style {
    opacity: 0;
    translate: 0 10px;
  }
}
Success!

But this has all changed thanks to the new @starting-style at-rule!
Pretty much all you have to do is set your properties as usual, add the initial transition states to @starting-style, and add those properties to a transition. Itâ€™s pretty simple and it kind of just works without having to trigger the animation in any way.
Lunalover
Another good example of where CSS shines is theming. Many sites need separate light and dark modes, and modern CSS makes dealing with that pretty easy.
:root {
  color-scheme: light dark;
  --text: light-dark(#000, #FFF);
  --bg: light-dark(#EEE, #242936);
}
hi there!you are awesome!i am!

By setting the color-scheme property to light dark, you are telling the browser to automatically pick the theme according to the user preference, and you can then make use of that by setting color values with the light-dark() function.
Not only does it set your own colors, but also those of the native components, such as the default buttons, form elements, and scrollbars. It kind of just makes stuff work by default, and thatâ€™s nice!
:root {
  color-scheme: light dark;
  &:has(#theme-light:checked) {
    color-scheme: light;
  }
  &:has(#theme-dark:checked) {
    color-scheme: dark;
  }
}

    Auto
    Light
    Dark
  

You can then add some way of overriding the color-scheme property to let the user pick a theme different from their system setting. Here I am using radio buttons to accomplish that.
Pro tip! CSS canâ€™t save the theme preference, but you can still do progressive enhancement. Make the themes work CSS-only, and then add the saving/loading of preference as an optional extra in JavaScript or server-side code.
Lyres and accordions
â€œBut those donâ€™t look like radio buttonsâ€ I hear you cry.
Input elements such as radio buttons and checkboxes are a great foundation to build other stuff on top of - the example above consists of labels for the buttons and invisible radio buttons that can be checked for with the :checked pseudo-class.
<radio-picker aria-label="Radio buttons example" role="radiogroup">
  <label><input type="radio" name="demo" id="veni" checked>veni</label>
  <label><input type="radio" name="demo" id="vidi">vidi</label>
  <label><input type="radio" name="demo" id="vici">vici</label>
</radio-picker>
<style>
  radio-picker {
    display: flex;
    label {
      &:has(input:checked) {
        box-shadow: inset 0px 0px 8px 0px #888;
      }
      &:has(input:focus-visible) {
        outline: 2px solid #000;
      }
      box-shadow: inset 0px 0px 1.2px 0px #000;
      padding: 10px;
      cursor: pointer;
      background: #0002;
      &:hover { background: #0004; }
      &:active { background: #0006; }
    }
    input {
      /* To allow screen reader to still access these. */
      opacity: 0;
      position: absolute;
      pointer-events: none;
    }
  }
</style>


    veni
    vidi
    vici
  

This is how I made the theme selector from the previous example. Iâ€™ve made the radio buttons half-visible in the demo for clarity, but with the opacity: 0 they would not actually be visible.
Thereâ€™s a whole lot going on here, so letâ€™s break it down.
<radio-picker aria-label="Radio buttons example" role="radiogroup">

We start off with the radio-picker element - I just made it up, you can use a div instead if youâ€™d prefer. We give it an aria-label to give the group an accessible name, and the aria role of radiogroup to make it work as a group for the radio buttons.
You could also use the fieldset element instead of doing the aria roles if thatâ€™d fit your use case better.
<label><input type="radio" name="demo" id="veni" checked>veni</label>
<label><input type="radio" name="demo" id="vidi">vidi</label>
<label><input type="radio" name="demo" id="vici">vici</label>

Next, we add the radio buttons with their respective labels - usually youâ€™d have to use the for attribute on labels to define which element theyâ€™re referring to, but since we have the input inside the label we donâ€™t have to do that.
All the type="radio" inputs should also have a name value set to the same thing so that they are grouped together (you still need10 the radiogroup though). And then you can give them values or ids however you want.
label {
  &:has(input:checked) {
    box-shadow: inset 0px 0px 8px 0px #888;
  }
  &:has(input:focus-visible) {
    outline: 2px solid #000;
  }
  box-shadow: inset 0px 0px 1.2px 0px #000;
  padding: 10px;
  cursor: pointer;
  background: #0002;
  &:hover { background: #0004; }
  &:active { background: #0006; }
}

We then style the labels as we wish - the :hover and :active pseudo-classes can be used to make the buttons more fun to click, the :has(input:checked) selector can be used to define the style of the selected button, and the :has(input:focus-visible) selector can be used to add an outline when someone tabs over to the button.
The difference between :focus and :focus-visible is that the former shows up even if you use your mouse, while the latter only shows up when you use keyboard navigation, so itâ€™s often visually more clean to use the latter.
input {
  opacity: 0;
  position: absolute;
  pointer-events: none;
}

And last, we make the radio button input exist while not being visible. This is a bit hacky, but itâ€™s how you can keep this control accessible to keyboard navigation and screen readers.
And thatâ€™s how we get the cool-looking radio buttons!
<radio-tabs>
  <div tabindex=0 id="tab-veni">veni...</div>
  <div tabindex=0 id="tab-vidi">vidi...</div>
  <div tabindex=0 id="tab-vici">vici...</div>
</radio-tabs>
<style>
  body:has(#veni:not(:checked)) #tab-veni,
  body:has(#vidi:not(:checked)) #tab-vidi,
  body:has(#vici:not(:checked)) #tab-vici {
    display: none;
  }
</style>

    veni
    vidi
    vici


  veni/ËˆveÉªni/(intransitive) to come
  vidi/ËˆviËdi/(intransitive) to see
  vici/ËˆviËtÍ¡Êƒi/(intransitive) to conquer


We can now use them in the CSS however we want by just seeing if theyâ€™re :checked. Here I made tabs with separate divs for the content by using a :has selector on a parent element to find out which radio button is currently selected.
The :has selector has to be on a parent element that contains both the radio button and the target element - you can simply use html or body if you want it to work across the entire page. You should never use something like :has(â€¦) by itself as itâ€™ll run the selector for every element of the page, which can cause performance issues (body:has(â€¦) is okay).
<div>
  <details name="deets">
    <summary>What's your name?</summary>
    My name is Lyra Rebane.
  </details>
  <details name="deets">
    ...
  </details>
</div>
<style>
  div {
    border: 1px solid #AAA;
    border-radius: 8px;
    /* based on the MDN example */
    summary {
      font-weight: bold;
      margin: -0.5em -0.5em 0;
      padding: 0.5em;
      cursor: pointer;
    }
    details {
      &:last-child { border: none }
      border-bottom: 1px solid #aaa;
      padding: 0.5em 0.5em 0;
      &[open] {
        padding: 0.5em;
        summary {
          border-bottom: 1px solid #aaa;
          margin-bottom: 0.5em;
        }
      }
    }
  }
</style>

  
    
    What's your name?My name is Lyra Rebane.
    Cool name!I know ^_^
    Where can I learn more?On my website, lyra.horse!
  


Finally, before we move on, I want to give you a quick introduction to the details element. Itâ€™s great for if you want an accordion-style menu, such as for a FAQ section. The details open and close independently of each other, but you can set their name attribute to the same value to have only one open at a time.
Using them is pretty easy, put your content and a summary tag inside a details tag, and put the title inside the summary tag. The example above is a bit more convoluted for the visual flair, but all you really need is the html part of it.
The details elements are pretty stylable! You can add animations depending on the [open] state, and you can also get rid of the arrow by setting list-style: none on the summary.
Also, ctrl+f works with it, which is a big win in my book!
Validation
And lastly, I want to show you the power of input validation in HTML and CSS.
<label for="usrname">Username</label>
<input type="text" id="usrname" pattern="\w{3,16}" required>
<small>3-16 letters, only alphanum and _.</small>
<style>
 input:valid {
   border: 1px solid green;
 }
 input:invalid {
   border: 1px solid red;
 }
</style>

  Username
    
    3-16 letters, only alphanum and _.
  


This is a simple example of how you can validate an input field with a regex pattern. If you set a pattern attribute like above, a form that contains the input cannot be submitted unless the field matches the pattern. If youâ€™re submitting something like an e-mail address, a phone number, or a url, it might make sense to use the respective input types instead of writing your own regex.
Now, where CSS comes in is styling the input to show whether its value is valid. In the example above, Iâ€™m using :valid and :invalid to set a border color, but that comes with the downside of always having your input marked, even if the user hasnâ€™t entered anything yet.
input {
  border: none;
  border-radius: 2px;
  outline: 1px solid #000;
  &:focus { outline-width: 2px; }
  &:user-valid { outline-color: green; }
  &:user-invalid { outline-color: red; }
}

  Username
    
    3-16 letters, only alphanum and _.
  


An easy win here is to instead use :user-valid and :user-invalid - these pseudo-classes only become active once youâ€™ve interacted with input field. I also made this example use an outline instead of a border, which I think looks a lot nicer.
It may sometimes even make sense to use a combination of :valid and :user-invalid.
And of course, you can use the :has selector to style other elements depending on the input too!


  Password
    
    The password must:
- be 8-16 characters
- contain at least â…° roman numeral
- not end with a letter

  


This one's just for fun ^_-! you win! yay!
I do want to mention that for some stuff, such as date pickers () or datalists (), there are built-in elements that do the job, but you may find them limited in one way or the other. If youâ€™re making an input like that with specific requirements, you may still need to dip your feet in a bit of JavaScript.

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


Do not the vw/vh
This section is kind of random but I wanted to include it here because I think a lot of people are messing this one up and I want more people to know how to do this stuff right.
So CSS has vw/vh units that correspond to 1% of the viewport width and height respectively, which makes perfect sense for desktop browsers.





  CBSignal chatAre you feeling encrypted?
  MMaratit smells of onions in here...
  bmblackle moriwhat's the scoop in yer smacker, horseberry?
  RRhynoraterCSS go BRRRRR
  PPatTheHyrulerI just lost the game
  MMalkI can't wait to taste the sorbet!


ðŸ”’lyra.horse/blog/â€¢â€¢â€¢

lyra's epic blog posts tags
You no longer need JavaScript
yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap




Where it becomes a bit more nuanced is on mobile devices. For example, mobile versions of both Firefox and Chrome will hide the URL bar when scrolling down on a page.
This causes the vw/vh units to be a bit ambigous - do they represent the entire available screen, only the area thatâ€™s visible with the URL bar, or something in between?
If itâ€™s the first option, you might end up with buttons or links off-screen11! If itâ€™s the second, you may end up with a background div that doesnâ€™t cover the entire background.




  ppingotuxcss spec so good i transitioned
  mmayahi wife!!
  ZZvitlol. lmao, sogar.
  !!!! HAND !!yap yap yap
  JJonesGlory to KuK
  SSpaxim goop
  eenscribeI need a job


  lvh
  dvh
  svh











ðŸ”’lyra.horse/blog/â€¢â€¢â€¢




  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh

Your values

  
    
      Unit
      Value
    
  
  
    
      vh
      px
    
    
      lvh
      px
    
    
      dvh
      px
    
    
      svh
      px
    
  

Above is a table of values your browser reports - if you're on mobile, try scrolling the blogpost up and down so that the URL bar hides and see how the numbers change.
The values are multiplied by 100 (eg 100vh is used instead of 1vh).




The solution to this is to use the new responsive viewport units: lvh, svh, and dvh.
lvh stands for largest viewport height, and thus is useful for things like backgrounds that youâ€™d want to cover the entire screen with, and wouldnâ€™t care about getting cut off.
svh stands for smallest viewport height, and should be used for things that must always fit on the screen, such as buttons and links.
And dvh stands for dynamic viewport height - this one will update to whatever the current viewport height is. It might seem like the obvious choice, but it should not be used for elements you donâ€™t want resizing or moving around as the user scrolls the page, as it could become quite annoying and possibly even laggy otherwise.
Of course, the respective lvw, svw, and dvw units exist too :).
Keyboard cat
By default, the viewport units do not account for the keyboard overlaying the page.
There are two ways to deal with that: the interactive-widget attribute, and the VirtualKeyboard API.
The former option is widely supported across browsers, works without JS, and goes in the meta viewport tag. It makes it so that opening the keyboard will change all of the viewport units.
<meta name="viewport" content="width=device-width, interactive-widget=resizes-content">

The latter option is currently only supported in Chromium-based browsers, and requires a single line of JavaScript to use:
navigator.virtualKeyboard.overlaysContent = true;

The advantage of the second option is that it allows you to use environment variables in CSS to get the position and size of the keyboard, which is pretty cool.
floating-button {
  margin-bottom: env(keyboard-inset-height, 0px);
}

But considering the fact that it doesnâ€™t work cross-browser, Iâ€™d avoid it.
CSS wishlist
Alright, so this is a little different from the rest of the post, but I wanted to bring up some things that I wish were in CSS. I havenâ€™t fully fleshed out all of them, so some definitely wouldnâ€™t fit the spec as-is, but maybe they can inspire some other stuff at least.
They are just fun ideas, donâ€™t take them too seriously.
Reusable blocks
I wish it was possible to put classes in other classes in CSS, so that you could write something like:
.border {
  border: 2px solid;
  border-radius: 4px;
}

.button {
  @apply border;
}

.card {
  @apply border;
}

This is something that Tailwind already has, and that makes me jealous.

Combined @media selectors
We can currently do nested @media queries, and also multiple selectors at the same time:
div {
  &.foo, &.bar {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
  @media (width < 480px) {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

But we cannot combine the two into a single selector:
div {
  @media (width < 480px), &.foo {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

Which means if you want to do that youâ€™ll inevitably have to repeat code or do some silly variable hacks, neither of which is ideal.
n-th child variable
For many of the CSS crimes I like to commit, I often end up writing code like:
div {
  span:nth-child(1) { --nth: 1; }
  span:nth-child(2) { --nth: 2; }
  span:nth-child(3) { --nth: 3; }
  span:nth-child(4) { --nth: 4; }
  span:nth-child(5) { --nth: 5; }
  ...
  span {
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

And I think it would be a lot nicer if we could instead just do:
div {
  span {
    --nth: nth-child();
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

n-th letter targeting
CSS has the ability to style the ::first-letter of text. Itâ€™d be cool if were was also a ::nth-letter(â€¦) selector, similar to :nth-child. I suspect the reason this isnâ€™t a thing is because the ::first-letter selector is a pseudo-element, which would be a bit tricky to implement with the nth-letter idea.
/* not a real feature */
p::nth-letter(2) {
  color: red;
}

hi there~


Blackle suggested that combining the nth-child() variable with :nth-letter targeting would also be fun for certain effects, such as putting the value in the sin() function to create wavy text.
div {
  /* not a real feature */
  --nth: nth-child(nth-letter);
  will-change: transform;
  translate: 0 calc(sin(var(--nth) * 0.35 - var(--wave) * 3) * 5px);
  color: color-mix(in oklch, #58C8F2, #EDA4B2 calc(sin(var(--nth) * 0.5 - var(--wave)) * 50% + 50%));
}

  
    untucknowqueen
  
  (taphover to play animation)


Unit removal
I wish you could easily remove units from values, for example by dividing them.

div {
  /* Turns into:  (no unit) */
  --screen-width: calc(100vw / 1px);
  color: hsl(var(--screen-width) 100, 50);
}
This would allow you to use the size of the viewport or container as a numeric variable for things other than length. For example, the color picker from earlier uses it to convert the location of the color picker dot to a number to be used in a color value instead.
Uh, but wait? Does that mean this feature already exists?
Yeah, lol! We already have the ability to get unitless values in CSS, but it involves doing hacky stuff such as tan(atan2(var(--vw), 1px)) with a custom @property. Itâ€™d be nice to have this as just a division, for example.
Oh, and good news, this one we might actually be getting soon!
Also if you do something like calc(1px + sqrt(1px * 1px)) your browser will crash12.
A better image function
The image() function exists, but no browsers implement it. Itâ€™s similar to just using url(), but adds some really cool features such as a fallback color, and image fragments to crop a smaller section out of a bigger image (think spritesheets).
We can already do both fallbacks and spritesheets with the various background properties, but itâ€™d be nice to have this pretty syntax. Iâ€™d honestly love this syntax even more for <img> tags than CSS.
style tags in body
I make heavy use of <style> tags in <body> for my projects. On my blog, for example, I write the relevant CSS close to their graphics so that you can start reading the blog before the entire page (or the entire CSS) has finished loading13. And it works great!
But whatâ€™s unfortunate is that despite browsers supporting this, and major sites using this, itâ€™s not officially spec-compliant. I suspect itâ€™s in the spec to avoid the FOUC footgun, but there are so many reasons you would want/need style in body that I donâ€™t think it justifies it.
I think an HTML validator should warn for this, but not error.
The art
I want to end this article by saying that to me, web development is an art, and thus, CSS is too. I often have a hard time relating to people who do webdev solely to earn money or build a startup - web development is very different when youâ€™re on a team and are given tasks from above instead of having free will over what you create for fun.
Itâ€™s probably most apparent with things like AI14, that for me take all the fun and creativity out of my work. But it also applies to build chain tooling such as linters and minifiers - the way I write my code is part of the art, and I donâ€™t want a tool to erase that. I donâ€™t even use an IDE15.
Among the practical reasons for sticking to CSS listed throughout this post, thereâ€™s a secret extra reason I like to do everything in CSS, and thatâ€™s expression and art. Art isnâ€™t always practical, and using CSS isnâ€™t either. But itâ€™s how I like to express myself, and itâ€™s why I do what I do.
I tried to keep this post approachable and practical for all web developers. But there is so much more to CSS that Iâ€™d like to talk about, so expect another post about the stuff that isnâ€™t practical, and is instead just cool as fuck. I think CSS is a programming language, and I made a game to prove it.
But thatâ€™s a topic for another time.
afterword
itâ€™s been almost a year since my last post, but i hope itâ€™s been worth the wait ^_^
as usual, this post is a self-contained html file with no javascript, images, or other external resources - everything on the page is handwritten html/css, weighing in at around 49kB gzipped. it was really fun creating all the little interactive widgets and visuals this time around, i think iâ€™ve improved in css a lot since the last time i posted.
this entire post turned out to be a bit of a fun mess (as did i!), itâ€™s almost like a chaotic gradient of tone throughout, i hope it was still interesting and enjoyable to read though.
i have a few new posts in the works: in addition to the second css one mentioned earlier, i also have one about a new web vulnerability subclass i discovered, and one about a trans topic. iâ€™m not sure when these posts will come out, but weâ€™ll see! make sure to add me to your rss reader if that sounds fun.
iâ€™ll also be giving a talk at bsides tallinn in september! iâ€™m hoping to also do css-related talks at the next ccc and disobey, but weâ€™ll have to see whether i get accepted and have the travel budget for those.
thank you so much for reading <3
you're awesome!! (i can tell because you checked that checkbox from earlier)

Discuss this post on: twitter, mastodon, lobsters





Chromeâ€™s DevTools come with the cool flexbox widget. Firefoxâ€™s however donâ€™t seem to for some reason? I find that weird because Firefox does have really good tools for flexbox and grid development, so this seems like an odd omission.Â â†©ï¸Ž


While I think what I said is true, Tailwind does have more to its existence, the core of which can be found in this post by its creator.Â â†©ï¸Ž


You are allowed to just make up elements as long as their names contain a hyphen. Apart from the 8 existing tags listed at the link, no HTML tags contain a hyphen and none ever will. The spec even has <math-Î±> and <emotion-ðŸ˜> as examples of allowed names. You are allowed to make up attributes on an autonomous custom element, but for other elements (built-in or extended) you should only make up data-* attributes. I make heavy use of this on my blog to make writing HTML and CSS nicer and avoid meaningless div-soup.Â â†©ï¸Ž


Still not nice to read for you? Iâ€™m personally not a fan of BEM, but Iâ€™d definitely recommend reading up on it too if you just donâ€™t vibe with the way Iâ€™m writing my examples. Also, my example intentionally shows off a lot of the syntax at once, but in the real world it might make sense to structure things a little differently.Â â†©ï¸Ž


Baseline browsers are Safari (macOS/iOS), Chrome (desktop/Android), Edge (desktop), and Firefox (desktop/Android).Â â†©ï¸Ž


The MDN docs of course also list detailed browser compatibility, but the Baseline symbols are nice for just getting a quick â€œyeah, we can use it and itâ€™ll work for everyoneâ€ type overview.Â â†©ï¸Ž


ES3 (1999) is the last â€œclassicâ€ version of JavaScript. In 2009 we got the first major revision known as ES5, and a few years later we kicked off the yearly spec updates with ES2015. Also ES4 was abandoned which makes me feel sad :c.Â â†©ï¸Ž


93 files!! Seems like theyâ€™re 1/3 functionality, 1/3 ads, and 1/3 analytics. The site works just fine with JavaScript disabled - only stuff like the comments section and ads wonâ€™t load. Itâ€™s no longer a laggy mess either for some reason.Â â†©ï¸Ž


I think the x3ctf challenges page looks really smooth on my computer - the marquee text animation and clicking on the challenges is buttery. And it also runs pretty well on the low-end hardware I have. Note that some browser performance recording tools can act a bit weird with CSS animations, so make sure your tools are working as expected before using them. Unrelated, but I made some other cool x3ctf web stuff too - check out the archive.Â â†©ï¸Ž


Thereâ€™s a bug in Chrome that requires you to use a fieldset/radiogroup for the radio button index to work correctly in screenreaders. Eg if you have 3 radio buttons with the same name, selecting one of them should read â€œradio button 1 of 3â€, which is what Firefox does, but in Chrome it will instead read it as â€œradio button 4 of 9â€ or whatever if you donâ€™t have a fieldset/radiogroup because it kind of just combines all the radio buttons on the page into a single index.Â â†©ï¸Ž


A certain HR platform I have to use puts its action buttons at the very bottom of a 100vh container, leading to them not being visible/interactable on my phone - not a headache you want to go through when requesting sick days. Itâ€™s a good example of how just using the wrong unit can cause a pretty bad real world accessibility problem.Â â†©ï¸Ž


Well, probably not. This is a bug I found while writing this post that only affects Chrome, and itâ€™ll probably get fixed before it even manages to hit stable. Update: I took so long to get this blog post out that it has been fixed now. During the writing of this blog post I found another bug in Chrome though, which is pretty funny. Update 2: I found yet another Chrome bug while writing this post, this one is kinda weird, you should read it.Â â†©ï¸Ž


This matters for people on slow connections, such as bad mobile data, satellite internet, tor, or iodine. While my blog posts are very small in size, the CSS alone can take up more than the first 14kB of a TCP round trip, so with blocking CSS in the head you might have to wait a few extra seconds (or minutes, in the case of iodine) just to start reading the first paragraph. Now, that 14kB number isnâ€™t completely accurate in the modern world, but testing on my own server (HTTP/2, TLS 1.3), around ~16kB of the compressed html reaches the browser in the first batch of http data.Â â†©ï¸Ž


By this I mean tools such as Copilot, Cursor, chatbots etc. I understand there is a huge difference between full-on vibe coding and just using the tab key, but I do not want to use or interact with any of those tools. Please respect that.Â â†©ï¸Ž


I write all my code (and blogposts) in Sublime Text, which to me is just a glorified version of Notepad. The features over Notepad it gives me are syntax highlighting, multiple cursors, keyboard shortcuts, and a better visual design. It doesnâ€™t do that much, and yet, itâ€™s perfect. Itâ€™s so good I paid for it.Â â†©ï¸Ž





  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RFC 8594: The Sunset HTTP Header Field (2019)]]></title>
            <link>https://datatracker.ietf.org/doc/html/rfc8594</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056142</guid>
            <description><![CDATA[This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future. It also defines a sunset link relation type that allows linking to resources providing information about an upcoming resource or service sunset.]]></description>
            <content:encoded><![CDATA[Internet Engineering Task Force (IETF)                          E. Wilde
Request for Comments: 8594                                      May 2019
Category: Informational
ISSN: 2070-1721


                      The Sunset HTTP Header Field

Abstract

   This specification defines the Sunset HTTP response header field,
   which indicates that a URI is likely to become unresponsive at a
   specified point in the future.  It also defines a sunset link
   relation type that allows linking to resources providing information
   about an upcoming resource or service sunset.

Status of This Memo

   This document is not an Internet Standards Track specification; it is
   published for informational purposes.

   This document is a product of the Internet Engineering Task Force
   (IETF).  It represents the consensus of the IETF community.  It has
   received public review and has been approved for publication by the
   Internet Engineering Steering Group (IESG).  Not all documents
   approved by the IESG are candidates for any level of Internet
   Standard; see SectionÂ 2 of RFC 7841.

   Information about the current status of this document, any errata,
   and how to provide feedback on it may be obtained at
   https://www.rfc-editor.org/info/rfc8594.

Copyright Notice

   Copyright (c) 2019 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (https://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.





Wilde                         Informational                     [Page 1]

RFC 8594                      Sunset Header                     May 2019


Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Temporary Resources . . . . . . . . . . . . . . . . . . .   3
     1.2.  Migration . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.3.  Retention . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.4.  Deprecation . . . . . . . . . . . . . . . . . . . . . . .   3
   2.  Terminology . . . . . . . . . . . . . . . . . . . . . . . . .   4
   3.  The Sunset HTTP Response Header Field . . . . . . . . . . . .   4
   4.  Sunset and Caching  . . . . . . . . . . . . . . . . . . . . .   5
   5.  Sunset Scope  . . . . . . . . . . . . . . . . . . . . . . . .   6
   6.  The Sunset Link Relation Type . . . . . . . . . . . . . . . .   6
   7.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .   7
     7.1.  The Sunset Response Header Field  . . . . . . . . . . . .   7
     7.2.  The Sunset Link Relation Type . . . . . . . . . . . . . .   8
   8.  Security Considerations . . . . . . . . . . . . . . . . . . .   8
   9.  Example . . . . . . . . . . . . . . . . . . . . . . . . . . .   9
   10. References  . . . . . . . . . . . . . . . . . . . . . . . . .  10
     10.1.  Normative References . . . . . . . . . . . . . . . . . .  10
     10.2.  Informative References . . . . . . . . . . . . . . . . .  10
   Acknowledgements  . . . . . . . . . . . . . . . . . . . . . . . .  10
   Author's Address  . . . . . . . . . . . . . . . . . . . . . . . .  11

1.  Introduction

   As a general rule, URIs should be stable and persistent so that
   applications can use them as stable and persistent identifiers for
   resources.  However, there are many scenarios where, for a variety of
   reasons, URIs have a limited lifetime.  In some of these scenarios,
   this limited lifetime is known in advance.  In this case, it can be
   useful for clients if resources make this information about their
   limited lifetime known.  This specification defines the Sunset HTTP
   response header field, which indicates that a URI is likely to become
   unresponsive at a specified point in the future.

   This specification also defines a sunset link relation type that
   allows information to be provided about 1) the sunset policy of a
   resource or a service, and/or 2) upcoming sunsets, and/or 3) possible
   mitigation scenarios for resource/service users.  This specification
   does not place any constraints on the nature of the linked resource,
   which can be targeted to humans, machines, or both.

   Possible scenarios for known lifetimes of resources include, but are
   not limited to, the following scenarios.







Wilde                         Informational                     [Page 2]

RFC 8594                      Sunset Header                     May 2019


1.1.  Temporary Resources

   Some resources may have a limited lifetime by definition.  For
   example, a pending shopping order represented by a resource may
   already list all order details, but it may only exist for a limited
   time unless it is confirmed and only then becomes an acknowledged
   shopping order.  In such a case, the service managing the pending
   shopping order can make this limited lifetime explicit, allowing
   clients to understand that the pending order, unless confirmed, will
   disappear at some point in time.

1.2.  Migration

   If resources are changing identity because a service migrates them,
   then this may be known in advance.  While it may not yet be
   appropriate to use HTTP redirect status codes (3xx), it may be
   interesting for clients to learn about the service's plan to take
   down the original resource.

1.3.  Retention

   There are many cases where regulation or legislation require that
   resources are kept available for a certain amount of time.  However,
   in many cases there is also a requirement for those resources to be
   permanently deleted after some period of time.  Since the deletion of
   the resource in this scenario is governed by well-defined rules, it
   could be made explicit for clients interacting with the resource.

1.4.  Deprecation

   For Web APIs one standard scenario is that an API or specific subsets
   of an API may get deprecated.  Deprecation often happens in two
   stages: the first stage being that the API is not the preferred or
   recommended version anymore and the second stage being that the API
   or a specific version of the API gets decommissioned.

   For the first stage (the API is not the preferred or recommended
   version anymore), the Sunset header field is not appropriate: at this
   stage, the API remains operational and can still be used.  Other
   mechanisms can be used for signaling that first stage that might help
   with more visible deprecation management, but the Sunset header field
   does not aim to represent that information.

   For the second stage (the API or a specific version of the API gets
   decommissioned), the Sunset header field is appropriate: that is when
   the API or a version does become unresponsive.  From the Sunset
   header field's point of view, it does not matter that the API may not




Wilde                         Informational                     [Page 3]

RFC 8594                      Sunset Header                     May 2019


   have been the preferred or recommended version anymore.  The only
   thing that matters is that it will become unresponsive and that this
   time can be advertised using the Sunset header field.

   In this scenario, the announced sunset date typically affects all of
   the deprecated API or parts of it (i.e., just deprecated sets of
   resources), and not just a single resource.  In this case, it makes
   sense for the API to define rules about how an announced sunset on a
   specific resource (such as the API's home/start resource) implies the
   sunsetting of the whole API or parts of it (i.e., sets of resources),
   and not just the resource returning the sunset header field.
   Section 5 discusses how the scope of the Sunset header field may
   change because of how a resource is using it.

2.  Terminology

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

3.  The Sunset HTTP Response Header Field

   The Sunset HTTP response header field allows a server to communicate
   the fact that a resource is expected to become unresponsive at a
   specific point in time.  It provides information for clients that
   they can use to control their usage of the resource.

   The Sunset header field contains a single timestamp that advertises
   the point in time when the resource is expected to become
   unresponsive.  The Sunset value is an HTTP-date timestamp, as defined
   in SectionÂ 7.1.1.1 of [RFC7231], and SHOULD be a timestamp in the
   future.

   It is safest to consider timestamps in the past mean the present
   time, meaning that the resource is expected to become unavailable at
   any time.

   Sunset = HTTP-date

   For example:

   Sunset: Sat, 31 Dec 2018 23:59:59 GMT







Wilde                         Informational                     [Page 4]

RFC 8594                      Sunset Header                     May 2019


   Clients SHOULD treat Sunset timestamps as hints: it is not guaranteed
   that the resource will, in fact, be available until that time and
   will not be available after that time.  However, since this
   information is provided by the resource itself, it does have some
   credibility.

   After the Sunset time has arrived, it is likely that interactions
   with the resource will result in client-side errors (HTTP 4xx status
   codes), redirect responses (HTTP 3xx status codes), or the client
   might not be able to interact with the resource at all.  The Sunset
   header field does not expose any information about which of those
   behaviors can be expected.

   Clients not interpreting an existing Sunset header field can operate
   as usual and simply may experience the resource becoming unavailable
   without recognizing any notification about it beforehand.

4.  Sunset and Caching

   It should be noted that the Sunset HTTP response header field serves
   a different purpose than HTTP caching [RFC7234].  HTTP caching is
   concerned with making resource representations (i.e., represented
   resource state) reusable so that they can be used more efficiently.
   This is achieved by using header fields that allow clients and
   intermediaries to better understand when a resource representation
   can be reused or when resource state (and, thus, the representation)
   may have changed.

   The Sunset header field is not concerned with resource state at all.
   It only signals that a resource is expected to become unavailable at
   a specific point in time.  There are no assumptions about if, when,
   or how often a resource may change state in the meantime.

   For these reasons, the Sunset header field and HTTP caching should be
   seen as complementary and not as overlapping in scope and
   functionality.

   This also means that applications acting as intermediaries, such as
   search engines or archives that make resources discoverable, should
   treat Sunset information differently from caching information.  These
   applications may use Sunset information for signaling to users that a
   resource may become unavailable.  But they still have to account for
   the fact that resource state can change in the meantime and that
   Sunset information is a hint and, thus, future resource availability
   may differ from the advertised timestamp.






Wilde                         Informational                     [Page 5]

RFC 8594                      Sunset Header                     May 2019


5.  Sunset Scope

   The Sunset header field applies to the resource that returns it,
   meaning that it announces the upcoming sunset of that specific
   resource.  However, as discussed in Section 1.4, there may be
   scenarios where the scope of the announced Sunset information is
   larger than just the single resource where it appears.

   Resources are free to define such an increased scope, and usually
   this scope will be documented by the resource so that consumers of
   the resource know about the increased scope and can behave
   accordingly.  However, it is important to take into account that such
   increased scoping is invisible for consumers who are unaware of the
   increased scoping rules.  This means that these consumers will not be
   aware of the increased scope, and they will not interpret Sunset
   information different from its standard meaning (i.e., it applies to
   the resource only).

   Using such an increased scope still may make sense, as Sunset
   information is only a hint anyway; thus, it is optional information
   that cannot be depended on, and clients should always be implemented
   in ways that allow them to function without Sunset information.
   Increased scope information may help clients to glean additional
   hints from resources (e.g., concluding that an API is being
   deprecated because its home/start resource announces a Sunset) and,
   thus, might allow them to implement behavior that allows them to make
   educated guesses about resources becoming unavailable.

6.  The Sunset Link Relation Type

   The Sunset HTTP header field indicates the upcoming retirement of a
   resource or a service.  In addition, a resource may want to make
   information available that provides additional information about how
   retirement will be handled for resources or services.  This
   information can be broadly described by the following three topics:

   Sunset policy:  The policy for which resources and in which way
         sunsets may occur may be published as part of service's
         description.  Sunsets may only/mostly affect a subset of a
         service's resources, and they may be exposed according to a
         certain policy (e.g., one week in advance).

   Upcoming sunset:  There may be additional information about an
         upcoming sunset, which can be published as a resource that can
         be consumed by those looking for this additional information.






Wilde                         Informational                     [Page 6]

RFC 8594                      Sunset Header                     May 2019


   Sunset mitigation:  There may be information about possible
         mitigation/migration strategies, such as possible ways how
         resource users can switch to alternative resources/services.

   Any information regarding the above issues (and possibly additional
   ones) can be made available through a URI that then can be linked to
   using the sunset link relation type.  This specification places no
   constraints on the scope or the type of the linked resource.  The
   scope can be for a resource or for a service.  The type is determined
   by the media type of the linked resource and can be targeted to
   humans, machines, or both.

   If the linked resource does provide machine-readable information,
   consumers should be careful before acting on this information.  Such
   information may, for example, instruct consumers to use a migration
   rule so that sunset resources can be accessed at new URIs.  However,
   this kind of information amounts to a possibly large-scale identity
   migration of resources, so it is crucial that the migration
   information is authentic and accurate.

7.  IANA Considerations

7.1.  The Sunset Response Header Field

   The Sunset response header field has been added to the "Permanent
   Message Header Field Names" registry (see [RFC3864]), taking into
   account the guidelines given by HTTP/1.1 [RFC7231].

      Header Field Name: Sunset

      Protocol: http

      Status: informational

      Author/Change controller: IETF

      Reference: RFC 8594














Wilde                         Informational                     [Page 7]

RFC 8594                      Sunset Header                     May 2019


7.2.  The Sunset Link Relation Type

   The sunset link relation type has been added to the permanent "Link
   Relation Types" registry according to SectionÂ 4.2 of [RFC8288]:

      Relation Name: sunset

      Description: Identifies a resource that provides information about
      the context's retirement policy.

      Reference: RFC 8594

8.  Security Considerations

   Generally speaking, information about upcoming sunsets can leak
   information that otherwise might not be available.  For example, a
   resource representing a registration can leak information about the
   expiration date when it exposes sunset information.  For this reason,
   any use of sunset information where the sunset represents an
   expiration or allows the calculation of another date (such as
   calculating a creation date because it is known that resources expire
   after one year) should be treated in the same way as if this
   information would be made available directly in the resource's
   representation.

   The Sunset header field SHOULD be treated as a resource hint, meaning
   that the resource is indicating (and not guaranteeing with certainty)
   its potential retirement.  The definitive test whether or not the
   resource in fact is available will be to attempt to interact with it.
   Applications should never treat an advertised Sunset date as a
   definitive prediction of what is going to happen at the specified
   point in time: the Sunset indication may have been inserted by an
   intermediary or the advertised date may get changed or withdrawn by
   the resource owner.

   The main purpose of the Sunset header field is to signal intent so
   that applications using resources may get a warning ahead of time and
   can react accordingly.  What an appropriate reaction is (such as
   switching to a different resource or service), what it will be based
   on (such as machine-readable formats that allow the switching to be
   done automatically), and when it will happen (such as ahead of the
   advertised date or only when the resource in fact becomes
   unavailable) is outside the scope of this specification.

   In cases where a sunset policy is linked by using the sunset link
   relation type, clients SHOULD be careful about taking any actions
   based on this information.  It SHOULD be verified that the
   information is authentic and accurate.  Furthermore, it SHOULD be



Wilde                         Informational                     [Page 8]

RFC 8594                      Sunset Header                     May 2019


   tested that this information is only applied to resources that are
   within the scope of the policy, making sure that sunset policies
   cannot "hijack" resources by for example providing migration
   information for them.

9.  Example

   If a resource has been created in an archive that, for management or
   compliance reasons, stores resources for ten years and permanently
   deletes them afterwards, the Sunset header field can be used to
   expose this information.  If such a resource has been created on
   November 11, 2016, then the following header field can be included in
   responses:

   Sunset: Wed, 11 Nov 2026 11:11:11 GMT

   This allows clients that are aware of the Sunset header field to
   understand that the resource likely will become unavailable at the
   specified point in time.  Clients can decide to ignore this
   information, adjust their own behavior accordingly, or alert
   applications or users about this timestamp.

   Even though the Sunset header field is made available by the resource
   itself, there is no guarantee that the resource indeed will become
   unavailable, and if so, how the response will look like for requests
   made after that timestamp.  In case of the archive used as an example
   here, the resource indeed may be permanently deleted, and requests
   for the URI after the Sunset timestamp may receive a "410 Gone" HTTP
   response.  (This is assuming that the archive keeps track of the URIs
   that it had previously assigned; if not, the response may be a more
   generic "404 Not Found".)

   Before the Sunset header field even appears for the first time (it
   may not appear from the very beginning), it is possible that the
   resource (or possibly just the "home" resource of the service
   context) communicates its sunset policy by using the sunset link
   relation type.  If communicated as an HTTP header field, it might
   look as follows:

   Link: <http://example.net/sunset>;rel="sunset";type="text/html"

   In this case, the linked resource provides sunset policy information
   about the service context.  It may be documentation aimed at
   developers, for example, informing them that the lifetime of a
   certain class of resources is ten years after creation and that
   Sunset header fields will be served as soon as the sunset date is





Wilde                         Informational                     [Page 9]

RFC 8594                      Sunset Header                     May 2019


   less than some given period of time.  It may also inform developers
   whether the service will respond with 410 or 404 after the sunset
   time, as discussed above.

10.  References

10.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://www.rfc-editor.org/info/rfc2119>.

   [RFC3864]  Klyne, G., Nottingham, M., and J. Mogul, "Registration
              Procedures for Message Header Fields", BCP 90, RFC 3864,
              DOI 10.17487/RFC3864, September 2004,
              <https://www.rfc-editor.org/info/rfc3864>.

   [RFC7231]  Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer
              Protocol (HTTP/1.1): Semantics and Content", RFC 7231,
              DOI 10.17487/RFC7231, June 2014,
              <https://www.rfc-editor.org/info/rfc7231>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://www.rfc-editor.org/info/rfc8174>.

   [RFC8288]  Nottingham, M., "Web Linking", RFC 8288,
              DOI 10.17487/RFC8288, October 2017,
              <https://www.rfc-editor.org/info/rfc8288>.

10.2.  Informative References

   [RFC7234]  Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke,
              Ed., "Hypertext Transfer Protocol (HTTP/1.1): Caching",
              RFC 7234, DOI 10.17487/RFC7234, June 2014,
              <https://www.rfc-editor.org/info/rfc7234>.

Acknowledgements

   Thanks for comments and suggestions provided by Ben Campbell, Alissa
   Cooper, Benjamin Kaduk, Mirja Kuhlewind, Adam Roach, Phil Sturgeon,
   and Asbjorn Ulsberg.








Wilde                         Informational                    [Page 10]

RFC 8594                      Sunset Header                     May 2019


Author's Address

   Erik Wilde

   Email: erik.wilde@dret.net
   URI:   http://dret.net/netdret/













































Wilde                         Informational                    [Page 11]
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[Iâ€™m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

Iâ€™ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they arenâ€™t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isnâ€™t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data thatâ€™s going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

Iâ€™m often asked, â€œwhat is the future of programming?â€ Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before itâ€™s too late? My answer to all these questions is â€œI havenâ€™t the foggiestâ€. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â‡Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

Iâ€™m also asked: â€œis AI a bubbleâ€? To which my answer is â€œOF COURSE ITâ€™S A BUBBLEâ€. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we donâ€™t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvanâ€¦ but it did not kill Amazon.

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

I retired from public speaking a couple of years ago. But while I donâ€™t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So Iâ€™m looking forward to catching up with many of them at GOTO Copenhagen. Iâ€™ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â âœ¢Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations arenâ€™t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, itâ€™s just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldnâ€™t ask an LLM to calculate an answer than we can calculate deterministically (yes, Iâ€™ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she canâ€™t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

Iâ€™ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say â€œall tests greenâ€, yet when I run them, there are failures. If that was a junior engineerâ€™s behavior, how long would it be before H.R. was involved?

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â â„

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (â€œexfiltrationâ€). That â€œuntrusted contentâ€ can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attackerâ€™s web page, and it could trick the agent to go to your bank account in another tab and â€œbuy you a presentâ€ by transferring your balance to the kind attacker. Willisonâ€™s view is that â€œthe entire concept of an agentic browser extension is fatally flawed and cannot be built safelyâ€.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willisonâ€™s LLM CLI, youâ€™ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These arenâ€™t just chatbots or
      autocomplete tools - theyâ€™re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So thatâ€™s exactly what we did, and in
      this article Iâ€™ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      Youâ€™ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, theyâ€™re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but weâ€™re
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (Iâ€™ve
      shortened ours for the article). Now something magical happened. I could
      say â€œX isnâ€™t workingâ€ and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes â€œfixedâ€ failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Alâ€™s default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned â€œBedrock is timing outâ€
      or â€œthe model responses are getting truncated,â€ the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the icebergâ€”the AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentationâ€”recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask â€œWhy is
      this API call failing intermittently?â€ and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generationâ€”especially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capableâ€”it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say â€œThe authentication tests are failing, please fix the issueâ€
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestionsâ€”it was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commitâ€”like having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        developmentâ€”from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourselfâ€”even a simple versionâ€”gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VLT observations of interstellar comet 3I/ATLAS II]]></title>
            <link>https://arxiv.org/abs/2508.18382</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055335</guid>
            <description><![CDATA[We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Ã…). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3Ïƒ$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.]]></description>
            <content:encoded><![CDATA[
    
    
    Authors:Rohan Rahatgaonkar, Juan Pablo Carvajal, Thomas H. Puzia, Baltasar Luco, Emmanuel Jehin, Damien HutsemÃ©kers, Cyrielle Opitom, Jean Manfroid, MichaÃ«l Marsset, Bin Yang, Laura Buchanan, Wesley C. Fraser, John Forbes, Michele Bannister, Dennis Bodewits, Bryce T. Bolin, Matthew Belyakov, Matthew M. Knight, Colin Snodgrass, Erica Bufanda, Rosemary Dorsey, LÃ©a Ferellec, Fiorangela La Forgia, Manuela Lippi, Brian Murphy, Prasanta K. Nayak, Mathieu Vander Donckt            
    View PDF
    HTML (experimental)
            Abstract:We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Ã…). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3\sigma$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.
    

    
    
              
          Comments:
          12 pages, 3 figures, 2 tables, submitted to ApJL
        

          Subjects:
          
            Solar and Stellar Astrophysics (astro-ph.SR); Earth and Planetary Astrophysics (astro-ph.EP)
        
          Cite as:
          arXiv:2508.18382 [astro-ph.SR]
        
        
          Â 
          (or 
              arXiv:2508.18382v1 [astro-ph.SR] for this version)
          
        
        
          Â 
                        https://doi.org/10.48550/arXiv.2508.18382
              
                                arXiv-issued DOI via DataCite
            
          
        
    
  
      Submission history From: Thomas H. Puzia [view email]          [v1]
        Mon, 25 Aug 2025 18:15:44 UTC (2,178 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates arenâ€™t exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesnâ€™t exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know whatâ€™s wrong with people?
                Theyâ€™re too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence â€” however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say â€œit dependsâ€ all the time.
                Much more fun getting to say
                â€œletâ€™s ship it and iterateâ€ as Staff or
                â€œthat wonâ€™t scaleâ€ as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? ðŸ¤¨
}

              GPS coordinates arenâ€™t exact.
                Theyâ€™re noisy. Theyâ€™re approximate. Theyâ€™re probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              youâ€™re probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesnâ€™t capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach thatâ€™s both
              mathematically rigorous and surprisingly practical.
            
            As youâ€™d expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2Ïƒ confidence ðŸ¤“
}

            When you compare two Uncertain values,
              you donâ€™t get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/mÂ³
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // mÂ²
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed â€”
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesnâ€™t exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              letâ€™s take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or â€œfruit machineâ€ for our UK readers ðŸ‡¬ðŸ‡§):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "â—»ï¸", "â—»ï¸", "â—»ï¸",  // blanks
            "ðŸ’", "ðŸ‹", "ðŸŠ", "ðŸ‡", "ðŸ’Ž"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("ðŸ’Ž", "ðŸ’Ž", "ðŸ’Ž"): return 100  // Jackpot!
        case ("ðŸ’", "ðŸ’", "ðŸ’"): return 10
        case ("ðŸ‡", "ðŸ‡", "ðŸ‡"): return 5
        case ("ðŸŠ", "ðŸŠ", "ðŸŠ"): return 3
        case ("ðŸ‹", "ðŸ‹", "ðŸ‹"): return 2
        case ("ðŸ’", _, _), // Any cherry
             (_, "ðŸ’", _),
             (_, _, "ðŸ’"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically â€”
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: â‰ˆ $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23Â°C
let tempSpread = temperature.standardDeviation() // about 1Â°C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)Â°C and \(upper)Â°C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp â‰¤ 25Â°C) < 20%
    print("Unlikely to be 25Â°C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users donâ€™t notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is Â±1000 meters,
            thatâ€™s a bad look ðŸ¤¡
          So where do we go from here?
            Letâ€™s channel our Senior+ memes from before for guidance.
          That Staff engineer saying â€œletâ€™s ship it and iterateâ€
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineerâ€™s warning of â€œthat wonâ€™t scaleâ€.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isnâ€™t to eliminate uncertainty â€”
            itâ€™s to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: The government of my country blocked VPN access. What should I use?]]></title>
            <link>https://news.ycombinator.com/item?id=45054260</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054260</guid>
            <description><![CDATA[Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).]]></description>
            <content:encoded><![CDATA[
Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).- First things first, you have to get your hands on actual VPN software and configs. Many providers who are aware of VPN censorship and cater to these locales distribute their VPNs through hard-to-block channels and in obfuscated packages. S3 is a popular option but by no means the only one, and some VPN providers partner with local orgs who can figure out the safest and most efficient ways to distribute a VPN package in countries at risk of censorship or undergoing censorship.- Once you've got the software, you should try to use it with an obfuscation layer.Obfs4proxy is a popular tool here, and relies on a pre-shared key to make traffic look like nothing special. IIRC it also hides the VPN handshake. This isn't a perfectly secure model, but it's good enough to defeat most DPI setups.Another option is Shapeshifter, from Operator (https://github.com/OperatorFoundation). Or, in general, anything that uses pluggable transports. While it's a niche technology, it's quite useful in your case.In both cases, the VPN provider must provide support for these protocols.- The toughest step long term is not getting caught using a VPN. By its nature, long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a state actor). I don't know the situation on the ground in Indonesia, so I won't speculate about what the best way to avoid this would be, long-term.I will endorse Mullvad as a trustworthy and technically competent VPN provider in this niche (n.b., I do not work for them, nor have I worked for them; they were a competitor to my employer and we always respected their approach to the space).
> First things first, you have to get your hands on actual VPN software and configs.It would be nice if one of the big shortwave operators could datacast these packages to the world as a public service.
There isn't enough bandwidth in HF to transmit data. Digital HF audio is 20 kHz wide so maybe 50kbps. The entire HF band is only 3-30 MHz.
50 kb/s x 1000 bits/kb x 3600 s/hr x 24 hr/day x 1 byte/8 bits x 1 MB / 1000000 bytes = 540 MB/day. That's enough to download VPN software and a Linux distribution to run it on in a day.If you've already got a Linux system, the Debian openvpn package is under 1 MB and at 50 kb/s would take under 3 minutes to download. I don't know if openvpn in particular is suitable for people who are trying to evade their government, but would whatever features it is missing add substantially more size?
sure there is, you can send files over HF, it may not be FAST, but once you get it into the country, you can just copy the file with a faster method (eg: usb drive), WINLINK supports attachments, so you could absolutely send these files over HF
The problem is the countries, which censor Internet and block VPNs, also jam shortwave radio signals.
It's possible but also difficult to jam radio. That's part of why programs like Radio Free Asia[0,1] exist. Even if you can't broadcast from inside a territory you can broadcast from outside. It can be jammed but it is a tough cat and mouse game and jamming isn't precise. So when you jam there are causalities. Not to mention that jamming can be quite expensive.I'm not saying that makes the problem easy, but I'll say that jamming isn't a very strong defense.Though the bigger issue here is probably bandwith. It's hard to be both long range and data dense. There's probably easier ways to distribute this. Hell, both Koreas are known to transport different things via balloons.[0] https://en.wikipedia.org/wiki/Radio_Free_Asia[1] It is also why projects like Tor and Signal get funding from RFA. Maybe the US doesn't want encrypted services here, but if anything, it's for the same reason they do want encrypted services in other countries.
Iâ€™m not sure thatâ€™s super feasible any longer with the advent of cheap SDRs. Over-the-horizon HF broadcast can be heard with a simple speaker wire antenna inside your house. If anyone is interested in trying to deploy such an idea, Iâ€™d love to participate as an avid ham.
Could I ask for a source on that and how common it is?Seems like it was used way back in the cold war (and even then not blocked/jammed) and I'd guess that current authoritarian regimes would perhaps not bother considering how few could use it.
Source: trust me bro, but you can find HF jamming pretty easily on Internet connected SDRs, especially near "sensitive" countries.
if it became a widespread practice, wouldnt even the countries that yet dont do it probably start doing it?
Streisand is extremely out of date and wouldnâ€™t last long in China, but I donâ€™t know how sophisticated Indonesiaâ€™s firewall is
This is no 'nothing special' with Obfs4proxy. DPI sees it as random byte stream, thus your government can decide to block unknown protocols. Instead, you should trick DPI into thinking it sees HTTPS. Unless your government decides to block HTTPS.
> your government can decide to block unknown protocolsHas any government ever done that? Seems like it would just break everything (because the world is full of devices that use custom protocols!) at great computational expense.
The only VPN technology I see that blends as HTTPS is MASQUE IP Proxying, and the only implementation I know that does this is iCloud Private Relay. It is also trivial to block because blocking 443/udp doesn't really affect accessing the Internet.
Unless your government decides to block HTTPS.In which case you use stenography, but I believe even the Great Firewall of China doesn't block HTTPS completely.
Exactly this. Hell, for OP's use case of accessing things like twitter, a good old fashioned https proxy would be entirely fine, and likely not even illegal.
Thank you very much for a detailed answer. Might I rudely ask -- as you're knowledgeable in this space, what do you think of Mullvad's DAITA, which specifically aims to defeat traffic analysis by moving to a more pulsed constant bandwidth model?
DAITA was introduced after my time in the industry, but this isn't a new idea (though as far as I know, it's the first time this kind of thing's been commercialized).It's clever. It tries to defeat attacks against one of the tougher parts of VPN connections to reliably obfuscate, and the effort's commendable, but I'll stop short of saying it's a good solution for one big reason: with VPNs and censorship circumvention, the data often speaks for itself.A VPN provider working in this space will often have aggregate (and obviously anonymized, if they're working in good faith) stats about success rates and failure classes encountered from clients connecting to their nodes. Where I worked, we didn't publish this information. I'm not sure where Mullvad stands on this right now.In any case -- some VPN providers deploying new technology like this will partner with the research community (because there's a small, but passionate formal research community in this space!) and publish papers, studies, and other digests of their findings. Keep an eye out for this sort of stuff. UMD's Breakerspace in the US in particular had some extremely clever people working on this stuff when I was involved in the industry.
There are some techniques like fragmented TLS and reordered packets that work in some cases. Also using vanilla HTTPS transport is a good start for many places. URnetwork is an open source, decentralized option that does all of these out of the box. You can get it on the major stores or F-Droid.
Obfs4proxy and Shapeshifter are an absolute PITA to install.Get your own VPS server (VPS in EU/US with 2GB of ram, 40GB of disk space and TBs/month of traffic go for $10 a year, it's that cheap). Never get anything in the UK and even USA is weird. I'd stick with EU.Install your software (wireguard + obsfuscation or even tailscale with your own DERP server)Another simpler alternative is just `ssh -D port` and use it as a SOCKS server. It's usually not blocked but very obvious.
Which countries you need to avoid depends on your threat model. For example, there is need to avoid the USA if all you're trying to do is bypass the Chinese firewall. There might even be a legitimate use case for pretending to have a UK IP address.Since OP is in Southeast Asia, a VPS in JP or SG will probably hit a decent balance between latency and censorship avoidance.
I just spent 3 months in China this summer. The GFW has become much more sophisticated than I remember. I found only one method that reliably worked. That was to use Holafly (an international eSIM provider) and use its built-in VPN. China largely doesnâ€™t care if foreigners get around the GFW, I guess.Another method that usually worked was ProtonVPN with protocol set to Wireguard. Not sure why this worked, itâ€™s definitely a lot more detectable than other methods I tried. But as long as I rotated which US server I used every few days, this worked fine.No luck with shadowsocks, ProtonVPN â€œstealthâ€ mode, Outline+Digital Ocean, or even Jump / Remote Desktop. Jump worked the longest at several hours before it became unbearably slow, Iâ€™m still not sure if I was actually throttled or my home computer started misbehaving.I didnâ€™t get around to setting up a pure TLS proxy, or proxying traffic through a domain that serves â€œlegitimateâ€ traffic, so no idea if that still works.
The "inspection" part of DPI isn't limited to encrypted payloads. It's straightforward enough to look at application-level protocol headers and identify e.g. a Wireguard or OpenVPN or SSH connection, even if you can't decrypt the payload. That could be used as sufficient grounds to either block the traffic or punish the user.
DPI refers to a broad class of products which attempt to find signals and categorize traffic according to a ruleset, either to block it or throttle the speeds, etc.While access to plaintext is useful, it's not required for other rules which are eg looking at the timing and frequency of packets.
Because you are leaking information left and right with TCP / DNS and all these basic protocols that powering the internet today. When these were designed people were happy that it worked at all and nobody really tought that it should be state actor proof. Except maybe DJB. https://www.curvecp.org/
There are a couple of ways.The main one is called an Eclipse Attack in cyber circles, and it can be done at any entity operating at the ASN layer so long as they can position themselves to relay your traffic.The adversary can invisibly (to victim PoV) modify traffic if they have a cooperating rootPKI cert (anywhere in the ecosystem) that isn't the originating content provider, so long as they recognize the network signature (connection handshake); solely by terminating encryption early.Without a cert, you can still listen in with traffic analysis, the fetched traffic that's already been encrypted with their key (bit for bit), as known plaintext the math quickly reduces. SNI and a few other artifacts referencing the resources/sites are not part of the encrypted payload.Its more commonly known in a crypto context, but that kind of attack can happen anywhere. It even works against TOR. One of the first instances (afaik) was disclosed by Princeton researches in 2015, under the Raptor paper.
I've studied and worked in computer security for over a decade and have never heard of an "eclipse attack" before. Is this blockchain specific terminology? It seems like an adversarial network partition?
Patterns of data transmission (network behavioral analysis, I just made that term up), analyzing IP and ports, inspecting SSL handshakes for destination site. In short, metadata.
Mullvad is a bad choice for this particular case because they publish all their IPs, which makes them very easy to block. You should look into VPN providers that do not publish their IPs and that have a wide range of IP classes and multiple ASNs, which look like ordinary networks not associated with VPNs. In my experience, NordVPN and ExpressVPN have many of these.
I wonder if it can be embedded in a video stream, like a video of a lava lamp that you always have open, but the lsb of ever byte is meaningful.
That's an interesting idea, and probably something you might be able to achieve with a tool like h26forge.It's also probably more useful to just have a connection be fully dedicated to a VPN, and have the traffic volume over time mimic what you'd see in a video, rather than embedding it in a video -- thanks to letsencrypt, much of the web's served over TLS these days (asterisks for countries like KZ and TM which force the use of a state-sponsored CA), so going to great lengths to embed your VPN in a video isn't really practical.
Iâ€™m curious about what makes it difficult to block a vpn provider long term. You said getting the software is difficult, but can a country not block known vpn ingress points?
A country can and absolutely will block known VPN ingress points. There are two tricks that we can use to circumvent this:- Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc). Bonus points if you can leverage something like ECH (ex-ESNI) to make it harder to identify a single bucket or subdomain.- Keep spawning new domains and subdomains to distribute your binaries.There are complications with both approaches. Some countries block ECH outright. Some have no problem shutting the internet down wholesale for a little bit. The domain-hopping approach presents challenges w/r/t establishing trust (though not insurmountable ones, much of the time).These are thing that have to be judged and balanced on a case-by-case basis, and having partners on the ground in these places really helps reduce risk to users trying to connect from these places, but then you have to be very careful talking to then since they could themselves get in trouble for trying to organize a VPN distribution network with you. It's layers on layers, and at some point it helps to just have someone on the team with a background in working with people in vulnerable sectors and someone else from a global affairs and policy background to try and keep things as safe as they can be for people living under these regimes.
I've heard of domain fronting, where you host something on a subdomain of a large provider like Azure or Amazon. Is this what you're talking about when you say> - Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc).How can one bounce VPN traffic through S3? Or are you just talking about hosting client software, ingress IP address lists, etc?
That's generally for distribution, but yeah, it's a form of domain fronting.There are some more niche techniques that are _really_ cool but haven't gained widespread adoption, too, like refractive routing. The logistics of getting that working are particularly challenging since you need a willing partner who'll undermine some of their trustworthiness with some actors to support (what is, normally, to them) your project.
If I understand correctly, refractive routing basically just gets big trustworthy cloud providers to host the VPNs so that third world governments can't block them without blocking the cloud too. It's an unfortunate solution since tech platforms are international entities that should be neutral. When America asks them to take sides and prevent other countries from implementing their desired policies, America is spending the political capital and trust that tech companies worked hard to earn. It's also really foolish of those countries to just block things outright. They could probably achieve their policy goals simply by slowing down access to VPN endpoints.
ECH (Encrypted Client Hello) brings back a kind of domain fronting, except you don't need to front anything at all. the Client Hello itself is encrypted, so the SNI is hidden.hopefully ECH will catch on. I suspect the corporate backlash over domain fronting was them not wanting to be caught in the crossfire if their domain was used as a front. if e.g. Signal used "giphy.com" as a front, Russia might block giphy to block Signal. but if Signal is hosted on, say, AWS, and ECH was used, Russia would have no option other than blocking the entirety of AWS, since all TLS handshakes to AWS would look the same.though cloud providers (other than CloudFlare, respect!) don't seem to care about censorship or surveillance anymore, and might decline to adopt ECH if some lucrative market complains.
Sorry Iâ€™m referring to WireGuard/ovpn server IPs, not the binaries/configs used to setup a client. Unless youâ€™re talking about fronting for both, but I imagine it is not economical to run a commercial -scale privacy vpn via a cloud provider.
This makes me wonder: are there "cloud drive virtual sneakernet" systems that will communicate e.g. by a client uploading URL request(s) as documents via OneDrive/SharePoint/Google Drive/Baidu etc., a server reacting to this via webhook and uploading (say) a PDF version of the rendered site, then allowing the client to download that PDF? You effectively use the CDN of that service as a (very slow) proxy.Of course, https://xkcd.com/538/ applies in full force, and I don't have any background in the space to make this a recommendation!
It doesn't apply imo as OP is probably not a high value target of the govt, he just wants to bypass his govt restrictions and I doubt the situation is so bad that the govt will send people physically to deal with people circumventing the block.Your solution could technically work over any kind of open connection / data transfer protocol that isn't blocked by the provider but it would be an absolute pain to browse the web that way and there are probably better solutions out there.
I lived in China for a while and there were several waves of VPN blocks. Also very few VPN services even try to actively support VPN-blocking nations anymore. Any commercial offering will be blocked eventually.What I settled on for decent reliability and speeds was a free-tier EC2 hosted in an international region. I then setup a SOCKS5 server and connected my devices to it. You mentioned Cloudflare so whatever their VM service is might also work.It's very low profile as it's just your traffic and the state can't easily differentiate your host from the millions of others in that cloud region.LPT for surviving the unfree internet: GitHub won't be blocked and you'll find all the resources and downloads you need for this method and others posted by Chinese engineers.Edit: If you're worried about being too identifiable because of your static IP, well it's just a computer, you can use a VPN on there too if you want to!
The VM instance is good for setting up a VPN tunnel, but it's not good in terms of bandwidth if it's hosted in. Because of DPI capacity, China has a very limited amount of "real internet" bandwidth. A more capable setup is to have one VM on each side of the firewall on an hosting service with peering between inside and outside - Aliyun (Alibaba Cloud) is an example. The "inside" VM could be just "socat UDP4-RECVFROM:<port>,fork UDP4-SENDTO:<remote>:<port>" or something done using netfilter.Like others commented in this thread, having an obfuscator is a good idea to ensure the traffic is not dropped by DPI.When the inevitable ban comes and your VPN stops working, rotate the IP of the external VPN and update the firewall/socat config to reflect it. Usually, the internal VM's IP doesn't need to be updated.
When I worked in China (not for long periods but frequently enough that the Great Firewall became an irritant) I hosted an OpenVPN server on port 443 and/or port 22 of a server I owned. That worked sufficiently well most of the time.
This doesn't work anymore; the GFW no longer detects VPN connections by port but instead by performing deep packet inspection to characterize the type of traffic going over every connection. Using this technique in combination with some advanced ML systems, they're able to detect any encrypted VPN connection and cut it off; it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore, and it's usually not even possible to _tunnel_ a VPN connection through some other protocol because the GFW now detects that too.Stepping back and looking at it from a purely technical perspective, it's actually insanely impressive.Here's a USENIX paper from a few years ago on how it is done: https://gfw.report/publications/usenixsecurity23/en/
This is what IPsec TFS is for [https://datatracker.ietf.org/doc/rfc9347/]> the focus in this document is to enhance IP Traffic Flow Security (IP-TFS) by adding Traffic Flow Confidentiality (TFC) to encrypted IP-encapsulated traffic.  TFC is provided by obscuring the size and frequency of IP traffic using a fixed-size, constant-send-rate IPsec tunnel(If they block a constant rate stream, that'll hit a whole ton of audio/video streaming setups)
So they'll just block any constant rate stream that isn't containing AV data or a whilelisted streaming service.
So there's a disconnect between what you're saying and what others and myself have experienced in China even recently. You appear to be saying that it's not possible to use a VPN to bypass the GFW, but I apologise if I have misunderstood.The comments have multiple examples of people successfully bypassing the firewall. I personally just used Mullvad with wireguard + obfuscation (possibly also DAITA) and it just worked. No issues whatsoever.
Assuming they don't MITM SSH, you should still be able to use something like wireguard over an SSH tunnel.  At least I would think.. it's all SSH traffic as far as any DPI listener is concerned, you'd of course need to ensure the connection signature through another vector though.
> it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore.Really? Because the paper you linked says they don't block any TLS connections so you can just run a VPN over TLS:> TLS connections start with a TLS Client Hello message, and the first three bytes of this message cause the GFW to exempt the connection from blocking.
Give it a try if you want; it doesn't work. For TLS traffic they track what the connection looks like over time; a TLS connection for normal web traffic versus a VPN connection tunneling through TLS apparently look different enough that they can detect and cut it off.
> it's basically not possible to run any kind of output VPN connection (even to private servers) from inside of China anymore.What if you run your own HTTPS server that look semi-legitimate and just encapsulate it in that traffic?Can they still detect it?What about a VPS in HK? Is this even doable?
v2ray and similar servers do exactly that, and I would assume they're still working as they're actively developed.
Which is ridiculous because OpenVPN is trivial to identify, even when over TCP since it's different from "regular" HTTPS/SSL traffic.Why they chose this I have no idea.You can even port share.443 -> Web server for HTTPS traffic
443 -> OpenVPN for OpenVPN trafficStill trivial to identify and not uncommon for even public WiFi to do so.Since I changed to tailscale+headscale with my own derp server all these issues have disappeared (for now).
GitHub was briefly blocked a couple of years ago in Indonesia. SSH was also blocked briefly by one of the largest mobile providers.
Isn't VPS's public IP blocks are well known and very easy to block? I read that this is not a viable solution in case of China's firewall.
You've come to a wrong place to ask. Most people here (judging by recommendations of own VPN instances, Tor, Tailscale/other Wireguard-based VPNs, and Mullvad) don't have any experience with censorship circumvention.Just look for any VPNs that are advertised specifically for China, Russia, or Iran. These are the cutting edge tech, they may not be so privacy-friendly as Mullvad, but they will certainly work.
I also want to add here because a lot of people either mention Tor as a succesful solution, or mention why Tor is not a solution but state completely wrong reasons. And I have a good soapbox to stand once in a while.Number one reason why Tor is dead is Cloudflare.Let me digress here. In my opinion, Cloudflare does a lot more censoring than all state actors combined, because they singlehandedly decide if the IP you use is "trustworthy" or "not", and if they decided it is not, you're cut off from like half of the Internet, and the only thing you can do is to look for another one. I'd really like if their engineers understood what Orwellian mammoth have they created and resign, but for now they're only bragging without the realization. Or at least if any sane antitrust or comms agency shred their business in pieces.And Cloudflare by default makes browsing with Tor unusable. Either you're stuck with endless captchas, or you're banned outright.Number two reason why Tor is dead is all other antifraud protections combined. Try paying with Stripe through Tor. There is quite a big chance you'll get an "unknown error" of sorts on Stripe side. Try to watch Netflix in Tor - exit nodes are banned.Everyone kept shouting "Tor bad, Tor for criminals", and it became a self-fulfilling prophecy. It's really hard to do just browse web normally in Tor, because all "normal" sites consider it bad. The "wrong" sites, however, who expect Tor visitors...
> Just look for any VPNs that are advertised specifically for China, Russia, or Iran.If I was working for a secret service for these countries, I would set up many "VPNs that are advertised specifically for x" as honeypots to gather data about any dissidents.
It doesn't matter, he should look into the open source protocols that these services use. He doesn't have to use them.VLESS / v2ray works in Russia, as far as I know.
Mr. Kafka, suspicion is healthy. However, abstraction provides no way forward when faced with practicalities instead of theory. Creates a Kafka-esque situation - anything suitable is by definition unsuitable. Better to focus on practical technical advice.
Sir Night: may I ask, what should it mean to me that some businesses are fronts?I hope I do not present the presence of a dullard unfamiliar with this.
I don't see parent abstracting. They are simply pointing out a very real risk, which you don't provide any counter points to. Instead you seem to dismiss their point based on a strawman
IMO, the safest route for an individual with tech competency is to setup a small instance server in the cloud outside your country and use ssh port forwarding and a proxy to get at information you want.For an example of a proxy service https://www.digitalocean.com/community/tutorials/how-to-set-...That will give you a hard to snoop proxy service that should completely circumvent a government blockaid (they likely aren't going to be watching or blocking ssh traffic).
Hmm. People who recommend widely used approaches, and well-known, well-established providers, "don't have any experience with cenorship circumvention".So the solution is no-name providers using random ad-hoc hackery, chosen according to a criterion more or less custom designed to lead you into watering hole attacks.Right.
@reisse is 100% right. Most people outside of heavily censored regions have no clue what technology is actually used in those countries. The well-known, well-established providers don't actually work in censored regions because:1) The problem is very difficult and requires a lot of engineering resources
2) It's very hard to make money in these countries for many reasons, including sanctions or the government restricting payments (Alipay, WeChatPay, etc)The immediate response would be: "If the problem is so difficult, how can it be solved if not be well-known, well-established providers?"The answer is simple: the crowdsourcing power of open source combined with billions of people with a huge incentive to get around government blocking.
> It's very hard to make money in these countries for many reasonsTor and I2P, for example, don't actually make money anywhere. Which is not to say that they work for any of the users in all of these places, or for all of the users in any of these places.> The answer is simple: the crowdsourcing power of open source combined with billions of people with a huge incentive to get around government blocking.The actual answer is that (a) they're using so many different weird approaches that the censors and/or secret police can't easily keep up with the whack-a-mole, and (b) they're relying on folklore and survivorship bias to tell them what "works", without really knowing when or how it might fail, or even whether it's already failing.Oh, and most of them are playing for the limited stakes of being blocked, rather than for the larger stakes of being arrested. Or at least they think they are.Maybe that's "solving" it, maybe not.
It's very sad that every sane and informed comment (like reisse's) has to meet this kind of snarky comment whose only purpose is being snarky on HN.Perhaps you should stop and think about why people living in countries where governments actually censor a lot hardly use these "well-established providers" to circumvent censorship. Tip: it's not because they're stupid.
Actually, my main original purpose was to call (more) attention to the fact that looking for somebody specifically advertising a VPN to your particular country, for a censorship-resistance purpose, has a vastly greater chance of getting you a honey pot than almost any other possible way of looking for a relay. Honey pots are particularly dangerous in one-hop protocols with cleartext exit.The part about the unreliable ad-hockery is also true, albeit less critical. The fact is that you don't know what your adversary is doing now, and you definitely don't know what they're going to to roll out next. You don't have to be stupid to decide to take that risk, but you also don't have to be particularly stupid to not think about that risk in the first place, especially when people are egging you on to take it.The greater purpose underlying both is to keep people from unknowingly getting in over their heads. I have seen lots of people do actually stupid things, up close and personal, especially when given instructions without the appropriate cautions.And "services and providers" doesn't necessarily mean commercial VPNs. In fact those were way down the list of what I had in mind. Your own VPS is a "provider". So is Tor or I2P (not that those won't usually run into problems). So is your personal friend in another country.
> Actually, my main original purpose was to call (more) attention to the fact that looking for somebody specifically advertising a VPN to your particular country, for a censorship-resistance purposePlease re-read my post then. I do not call to look for VPN for your or anyone's particular country, I call to look for VPNs for these specific countries because they have the current bleeding edge blocking tech, and if VPN works there now, it will 100% work in every other country. If you're in China, you don't have to look for Chinese VPNs, some of Russian ones will work there too.
None of the things I listed are "widely used approaches, and well-known, well-established providers" in the parts of the world where it does matter.Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.
> Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.A lot of people use Telegram and think it's private, too.What about the part about choosing your VPN provider in the way most likely to get you an untrustworthy one who's after you personally?
> What about the part about choosing your VPN provider in the way most likely to get you an untrustworthy one who's after you personally?That's the straw man of yours, please read my answer to another your comment.
At DefCon 26 (25?) I attended two presentations that scared me:1. there was a presentation about several admins in a hostile country who had been arrested because someone from Harvard pinged a server they ran as part of IPv4 measurement. The suggestion was to avoid measuring countries with strong censorship laws to prevent accidental imprisonment of innocent IT.2. similar presentation about ToR project struggling to find fresh egress/ingress addresses. Authoritarian countries were making lists of any IP addresses that were known ToR IPs and prosecuting/imprisoning users associated with them as a result of traffic on those addresses.I would be extremely careful trying to bypass authoritarian restrictions unless I was 110% confident what I was doing.
Yeah. If an authoritarian government controls the network infrastructure, there's no way to use that network infra without risk.To actually bypass this, you need your own network. Does anyone know of any sneakernet protocols that would be useful here?
Furthermore, you can always run another VPN on top of that if you donâ€™t trust the outer one with the actual plaintext traffic.
Not on mobile - iOS doesn't support nested VPNs, and neither does stock Android.
VPNs that are advertised are for-profit products, which means:1. They are in most cases run by national spy agencies.2. They will at least appear to work, i.e., they will provide you with access to websites that are blocked by the country you are in.  Depending on which country's spies run the system, they may actually work in the sense of hiding your traffic from that country's spies, or they may mark you as a specific target and save all your traffic for later analysis.My inclination is to prefer free (open-source) software that isn't controlled by a company which can use that control against its users.
Well, you have to host your free open-source VPN software somewhere. And then, (N. B.: technical and usability stuff aside, I'm talking only about privacy bits here) everything boils down to two equally nightmarish options.First, you use well-known cloud or dedicated hoster. All your traffic is now tied to the single IP address of that hoster. It may be linked to you by visiting two different sites from the same IP address. Furthermore, this hoster is legally required to do anything with your VPN machine on demand of corresponding state actors (this is not a speculative scenario; i. e. Linode literally silently MitMed one of their customers on German request). Going ever further, residential and company IPs have quite different rules when it comes to law enforcement. Seeding Linux ISOs from your residential IP will be overlooked almost everywhere (sorry, Germany again), but seeding Linux ISOs from AWS can easily be a criminal offense.Second, you use some shady abuse-proof hosting company, which keeps no logs (or at least says that) and accepts payments in XMR. Now you're logging in to your bank account from an IP address that is used to seedbox pirate content or something even more illegal, and you still don't know if anyone meddles with your VPN instance looking for crypto wallet keys in your traffic.VPN services have a lot of "good" customers for a small amount of IP addresses, so even if they have some "bad" actors, their IPs as a whole remain "good enough". And, as the number of customers is big, each IP cannot be reliably tied to a specific customer without access logs.
Tor is a third option, at least as one layer, and seeding Linux ISOs is not, to my knowledge, a criminal offense in any jurisdiction, not even in China.  I don't know where you got that idea.
From gemini.. (edited for brevity)Kape Technologies Owns: ExpressVPN, CyberGhost, Private Internet Access, Zenmate> is there any suspicion that Kape Technologies is influenced or has ties to the Mossad?Yes, there is significant suspicion and public discussion about Kape Technologies having ties to former Israeli intelligence personnel. While a direct operational link to Mossad has not been proven, the concerns stem from the company's history, its key figures, and their backgrounds....Kape Technologies is owned by Israeli billionaire Teddy Sagi. While Sagi himself does not have a documented intelligence background, his business history, which includes a conviction for insider trading in the 1990s, has been a point of concern for some privacy advocates. The consolidation of several major VPN providers under his ownership has raised questions about the potential for centralized data access.----Sure there isn't direct proof but there wasn't any proof the CIA was driving drug trade while it was happening. Proof materializes when the dust settles on such matters.
It is absolutely self-evident that VPNs are considered high-value targets and that all spy agencies invest a chunk of resources to go after high-value targets.
I would invite you to read again the two claims made, and consider whether your statement actually addresses the veracity of either.To be a little trite: we all agree that chickens like grain, but it does not follow that a majority of grain producers are secretly controlled by a cabal of poultry.
>... but it does not follow that a majority of grain producers are secretly controlled by a cabal of poultry.That's precisely what someone who's in on it would say.
You can always do v2ray -> Mullvad in a docker container routed with gluetun for censorship avoidance and privacy
Mullvad worked okay in China in June for me. I imagine it will be better in Indonesia with their less sophisticated blocking.
This makes no sense.On the one hand they do DPI with ML.On the other hand a major player is open!Something is not right here...
Spell out your argument more. Find some hard evidence. Even â€œmajor playerâ€ needs to be backed up.Do you even know how many users Mullvad has in CN? I donâ€™t. Searching says the whole company apparently has ~500k users. I donâ€™t think thatâ€™s enough to be a significant presence in China.
OP: look into VLESS (and similar).  And read up on ntc.party (through Google translate).  There are certain VPN providers that offer the protocol.
nah, vless is the protocol, reality is a newer obfuscation method that works over vlessedit: op, protonvpn has a free tier that works in russia, so likely works everywhere, or if you're comfortable with buying a vps, sshing into it and running some commands, look up x-ray, and use on of their gui panels
Wrong threat model. Solutions like mullvad/proton focus on privacy not breaking the blockade. They have well known entry points and therefore easily blocked. You can play cat and mouse game switching servers faster than censorship agency blocks them (e.g. Telegram vs Roskomnadzor circa 2018 [1]) but that gets expensive and not really focus of these companies.What you need is open protocols and hundreds of thousands of small servers only known to their owners and their family/friends1: https://archive.is/sxiha
I have a little, maybe enough to be dangerous. SSH wonâ€™t be sufficient to avoid all traffic analysis. Everyone can see how much traffic and the pattern of that traffic, which can leak info about the sort of things youâ€™re doing.If youâ€™re worried about ending up on a list, using things that look like VPNs while the VPNs are locked down is likely to do so.Alsoâ€¦ your neighbors in Myanmar didnâ€™t do a lockdown during the genocide and things got pretty fucking dire as a result. People have taken different lessons from this. Iâ€™m not sure what the right answer is, and which is the greater evil. Deplatforming and arresting people for inciting riots and hate speech is probably the best you can do to maintain life and liberty for the most people.
>Alsoâ€¦ your neighbors in Myanmar didnâ€™t do a lockdown during the genocide and things got pretty fucking dire as a resultThe genocide in Myanmar was incited _by_ the government there; giving it more power to censor it's citizens' communications would have done absolutely nothing to help the people being genocided. Genocides don't just suddenly happen; the vast majority of genocide over the past century (including Indonesian genocides against ethnically Chinese Indonesians) had the support of the state.
This has been simmering for a very long time. The first I heard of it was violence that broke out after the defacement of a Buddhist temple statue. That would have been almost 20 years ago. Buddhists murdering people tends to lead one to ask a lot of questions.At that time I think the government was hands off, let it happen rather than tried to stop it.Regardless of who was behind the violence, the whole region has thought about what to do in such situations and they arenâ€™t the same answers the West would choose.
Mullvad worked OK in China for me recently. Sometimes I'd have to try a few different endpoints before it worked. Something built specifically to work in those places would probably be better, but it wasn't too much trouble. Not necessarily a recommendation, just sharing one data point.
I remember always needing obfuscation enabled in Mullvad, but it would work in the end (as you said, after trying a few endpoints).
^ this comment is right on. The cutting edge of VPN circumvention is the one marketed to people in China. Last I poked at this there were a lot of options.
I live in Indonesia, and I don't find any recent news that mention X (formerly Twittwr) and or Discord being blocked by the government. The only relevant news from a quick Google search I can find is about the government threatened to block X due to pornography content in 2024.
You can even check for yourself if a domain is blocked by visiting https://trustpositif.komdigi.go.id/.Also for your unability to access the VPN, as far as my experience goes, in the past some providers do block access to VPN. But, I am not experiencing that for at least the last 5 years.So, maybe you can try changing your internet provider and see if you can connect to VPN?
How can it be that one person living in Indonesia says everything is blocked and the country is in chaos and another, very calmly, is completely unaware and can't even find any news about it? This is so odd. What is the truth?
Indonesia is a big country with over ten thousand islands and uneven coverage. What is blocked on one ISP might not be enforced on another (e.g. the state-owned ISP might block or use DNS poisoning on several "non-compliant" DNS providers but my current ISP doesn't). Also, in addition to what the sibling commenter (and another commenter regarding Cloudflare outage) said there might be a general overload on the mobile network near the affected areas since there are lots of users and limited bandwidth.
The context that was likely left out due to HN rules is, there are mass protests turned violent in the face of police brutality in several cities. The Indonesian government has a history of blocking/throttling internet access in immediate areas of the unrest to limit coverage.
There was a reported outage of cloudflare in Jakarta, while simultaneously people can't access Twitter and Discord. The worst part is that it coincides with the time when people need the information to find a safe route to go home after the protest.
Australia and UK might soon go down this path.Something quite depressing is if we (HN crowd) find workarounds, most regular folks won't have the budget/expertise to do so, so citizen journalism will have been successfully muted by government / big media.
I would have laughed in your face if you wrote this comment merely 6 months ago. Now I'm just depressed. (UK)
America's Founders saw civil rights as inherent in the Constitution's framework, rooted in natural law. They added the Bill of Rights as an explicit bulwark. That's why we have the 1st Amendment's free speech, and if that falls, the 2nd Amendment ensures we have guns.
Don't worry. You'll call us conspiracy theories once you get used to the new goalposts and we warn you about the next thing.How about instead of being depressed you start being vocal and defiant?
You know what, I think I've become lethargic after all the backwards garbage going on in my country attacking my way of life on all fronts - from rampant crime to government censorship. 
Your comment just gave me a kick up the ass. I'm gonna try and get some local stuff going in opposition to this lunacy.
When ES leaked his info to the Guardian people, they could still (2013) use the Guardian's US base to publish, protected by the US' stronger freedom of speech laws. Now, in 2025, if the same were to happen again, I'm not sure that would work quite the same way, with Trump aggressively taking American citizens' rights away.Maybe The Guardian should open a branch in Sealand...
It was David Graeber that said we should be wary of places like The Guardian. They are a wolf in sheeps clothing. Used a lot of the more liberal momentum of the early 2010s combined with promoting some of the more left leaning writters to gain a fair bit of clout. But underneath, they will conform to the power structures if it comes down to survival. Alas, they nay not be a Sealand edition although that would be neat.
No American citizensâ€™ rights have been taken away or can be taken away by a President.We have whistleblowers and leakers from the administration itself on a literal weekly basis, our own Department of State actively funds Signal and Tor, our media has been heavily criticizing Trump and his allies for years. A couple organizations got hit with lawsuits for publishing misinformation or skirting campaign law, but thatâ€™s about it.They tried to make flag burning illegal - which is illegal in Mexico, most of South America, all of Asia, and most of Europe - and it was shot down almost immediately as even that comes under 1st amendment rights.Please donâ€™t lump us into the same bucket as the UK. We may have a sharply divided electorate but we donâ€™t have a failing state!
In oz personally and yes, I warned folks of this a few years back, especially in the 12 months or so. Every time I was met with a fair bit of push back.They would argue back on technical merits, I was talking political, a politics doesn't give a damn about the tech. We have slowly been going down this path for a while now.â€œThe laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia,â€ - PM Malcolm Turnbull in 2017.
Don't worry, you shouldn't underestimate the capability of society.I grew up in a pretty deprived area of the UK, and we all knew "a guy" who could get you access to free cable, or shim your electric line to bypass the meter, or get you pirated CD's and VHS' and whatever.There will always be "that guy down the pub" selling raspberry pi's with some deranged outdated firmware that runs a proxy for everything in the house or whatever. To be honest with you, I might end up being that guy for a bunch of people once I'm laid off from tech like the rest. :)
Normally I would agree with you, but the ability to pull this kind of thing off hinges on there being enough shadows that the Eye doesn't look at for prolonged periods of time. And the overall trajectory of technological advance lately is such that those shadows are rapidly shrinking. First it was the street cameras (and UK is already one of the most enthusiastic adopters in the world). And now comes AI which can automatically sift through all the mined data, performing sentiment analysis etc. I feel that the time will come pretty soon when "a guy" will need to be so adept at concealing the tracks in order to avoid detection that most people wouldn't have access to one.
I wouldnâ€™t worry about it.They can barely handle wolf-whistlers let alone pedophile rape gangs consisting of the lowest IQ dregs of our society.I know itâ€™s only painfully stupid people who think the law is stupid, but dodgy Dave down the way tends to fly under the radar. Otherwise there wouldnâ€™t be so many of them.
One of the problems with authoritarianism is that even though most dodgy Daves will be fine because the political apparatus doesn't have the time or energy to arrest everyone for everything, they retain the ability to arrest anyone for anything.The moment your dodgy Dave offends your local cadre, even for reasons entirely other than being dodgy, they'll throw the book at him. And because there is now unpredictability around who will be arrested and for what reason, it acts as a chilling effect for everyone who values some degree of stability in their lives. So the arc of dodgy Daves bends toward compliance.
The eye doesn't care as long as you're not politically efficient in opposing their narratives or power.Authoritarianism in the UK doesn't correlate with crime.  The economy does.The point of these things is not really to help citizens. "there's no money for that" like there's no money for healthcare or education (although there is for bombings in foreign countries). The point is protecting power from any threat that could mount against it.
I think both sides of this are fair. Power is interested in stability of itself, to keeps its back to the wall so that nobody can sneak up on it. But also political power has teamed up with corporate power/determination to create a far more nasty beast.Seeing companies like Palantir (and many lesser known ones) buddy up to everyone that wants it, its a clear statement on how they want to monitor and control the populace.Long term I don't think it can be done, but the pain mid term can be vast.
That absolutely sounds like a world I should be worried about, where our only choices are dodgy ones
I suppose that for this case, an underground black market of VPN providers might emerge - average individuals setting up VPN software on a cloud service provider, and then selling monthly access to people. Aside from the obvious danger of getting ripped off (someone might put you on a slow shared VPN with many other people, or shut down the server at any time), there is also the possibility of someone monitoring all your Internet activity.
I'd default assume black market VPNs will monitor internet activity since it's both easy and profitable
Don't worry, you shouldn't underestimate the capability of society.You should be worried. Don't underestimate the capabilities of the government bureaucrats. That "guys down the pub" will quickly disappear once they start getting jail time for their activities.
I think you really overestimate the capability of the UK to enforce laws. Yes, they can write them and yes they can fine large corporations, that's basically it.They cannot enforce laws against such "petty" crimes, the reason society mostly functions in the UK is because most people don't try to break the law.Pretty sure the local punters would kick the cops out if they came for one of their own, especially if he got them their porn back.
> They cannot enforce laws against such "petty" crimesNo, they aren't interested in enforcing laws against petty crimes. The establishment literally don't give a toss if someone breaks into your house and nicks your telly.They are very interested in enforcing the kinds of infringements we're talking about here.
What do you mean? They already arrest thousands of people a year for posting (or even retweeting) things online in the UK.What makes you think, if the Gov was to implement some sophisticated DPI firewall that blocks a million different things, they won't come after the people who circumvent it? They already enforce petty crimes. I could report you for causing me anxiety and you would have a copper show up at your door.
It's not just about UK abilities to enforce laws, but also about other factors. The described activities are extremely unattractive as criminal: small market, small margin, the need for planning, preparation and qualification.There is no need for special efforts to enforce the law. Put a few people in jail - and everyone else will quickly find safer and more legal ways to spend their time. No one will do something like that unless they are confident of their impunity.
Yes, it's also dystopian to pin one's future on such hopes. People need to stick it to the government and demand their freedoms. Far too many things are being forced on us in the West that go against fundamental values that have been established for centuries.Somehow, things that could be unifying protests where the working class of every political stripe are able to overlook their differences and push back against government never seem to happen. It is always polarized so that it's only ever one side at a time, and the other side is against them. How does that work?
Reflex. People's opinion on a subject changes if you tell them which political group supports it, sometimes even if they get asked twice in a row. Tribal identity determines ideology more than the other way around for a lot of people.So as soon as Labour comes out for something, Cons are inclined to be against it and so on. The only way to have neutral protests is if no one visibly backs them and they don't become associated with a side, but then how do they get support and organization?
> People need to stick it to the government and demand their freedoms.It will only work if they admit that they supported this and all forms of totalitarianism during COVID.  You can't fall for that and then be surprised when the world keeps going down that obvious path.
In matters of public health, you cannot trust the public to do the right thing.The problem with covid is that we weren't totalitarian enough. Regulations you could drive a coach & horses through and no way to enforce is a sop.The first lock down needed to be a proper 'papers, please' affair. When we get a properly lethal pandemic, we're fucked. Hopefully Laurence Fox and Piers Corbyn will catch it quickly and expire in a painful and televised way, it's the only hope of people complying with actual quarantine measures.
90% of â€œcitizen journalismâ€ is nothing of the sort. Just like â€œcitizen scienceâ€ researching vaccines.
> 90% of â€œcitizen journalismâ€ is (trash)You're right. But compared to what?I guess 99% of mainstream "journalism" is irrelevant and/or inaccurate, hence citizen journalism is a 10x improvement in accuracy and relevancy! Not 10% better, 900% better! This makes a huge difference to our society as a whole and in our daily lives!But this misses the most important point which is that the user should have the right to choose for themselves what they say and read. Making citizen journalism unduly burdensome deprives everyone of that choice.
Preach comrade!Those citizen journalists with their primary sources, disgusting.Thats nothing but propaganda.Remember it doesnt matter what the video shows, it only matters who showed it to you.
> Remember it doesnt matter what the video shows, it only matters who showed it to you.Both matter.
>Remember it doesnt matter what the video shows, it only matters who showed it to youIn an age of mass media (where there's a video for anything) or now one step further synthetic media knowing who makes something is much more important than the content, given that what's being shown can be created on demand. Propaganda in the modern world is taking something that actually happened, and then framing it as an authentic piece of information found "on the street", twisting its context."what's in the video" is now largely pointless, and anyone who isn't gullible will obviously always focus on where the promoter of any material wants to direct the audiences attention to, or what they want to deflect from.
Citizen journalism avoids the main weakness of a centralised system: it's incredible suspectible to capture. A prime example of this is the mass opposition around the world to Israel's genocide in Gaza.  Israel committed such genocides prior to the event of social media, such as the Nakba, but it was rarely reported on, due to media ownership being concentrated in the hands of a few pro-Zionist individuals.
I am just waiting for red states in the US to try this too since their current laws requiring ID verification for porn sites arenâ€™t effective.
> red statesWell you'd be surprised to find out that this stupid policy (and many more) have been brought forward by Labour (Left).
At this point, anyone who has been watching politics for a few decades understands that the left/right dichotomy is primarily one designed to keep the majority of people within a certain set of bounds. We see it revealed when politicians and ideologies that should be in opposition to one another still cooperate on the same strategies, like this one.The goal right now is to make online anonymity impossible. Adult content is the wedge issue being used to make defending it unpalatable for any elected official, but nobody actually has it as a goal to prevent teenagers from looking at porn - if they did, they would be using more direct and efficient strategies.  No, it's very clear that anonymous online commentary is hurting politicians and they are striking back against it.
It has been my impression that in UK, both parties are strongly authoritarian, with the sole difference being what kinds of speech and expression, precisely, they want to police.
Both the major Australian parties (Liberal and Labor) seem as spineless as each other.They're being pushed by media conglomerates News Corp and Nine Entertainment [0] to crush competition (social media apps). With the soon-to-be-introduced 'internet licence' (euphemism: 'age verification'), and it's working. If they ban VPN's, it will make social media apps even more burdensome to access and use.[0] News Corp and Nine Entertainment together own 90% of Australian print media, and are hugely influential in radio, digital and paid and free-to-air TV. They have a lot to gain by removing access to social media apps, where many (especially young) people get their information now days.
How long until they produce an  generative AI version of Burt Newton to do new episodes of 20 to 1 based on some social media slop?Yep, not a great time line here.
Yep, here in Australia the social media age restriction was pushed through by both sides. Two sides of the same coin.
- Tor. Pros: Reasonably user friendly and easy to get online, strong anonymity, free. Cons: a common target for censorship, not very fast, exit nodes are basically universally distrusted by websites.- Tailscale with Mullvad exit nodes. Pros: little setup but not more than installing and configuring a program, faster than Got, very versatile. Cons: deep packet inspection can probably identify your traffic is using Mullvad, costs some money.- Your own VPSs with Wireguard/Tailscale. Pros: max control, you control how fast you want it, you can share with people you care about (and are willing to support). Cons: the admin effort isn't huge but requires some skill, cost is flexible but probably 20-30$ per month minimum in hosting.
> - Tailscale with Mullvad exit nodesTailscale is completely unnecessary here, unless OP can't connect to Mullvad.net in the first place to sign up. But if the Indonesian government blocks Mullvad nodes, they'll be out of luck either way.> - Your own VPSs with Wireguard/TailscaleKeep in mind that from the POV of any websites you visit, you will be easily identifiable due to your static IP.My suggestion would be to rent a VPS outside Indonesia, set up Mullvad or Tor on the VPS and route all traffic through that VPS (and thereby through Mullvad/Tor). The fastest way to set up the latter across devices is probably to use the VPS as Tailscale exit node.
Tailscale + Mullvad does have a privacy advantage over either one by itself: the party that could potentially spy on the VPN traffic (Mullvad) doesnâ€™t know whose traffic it is beyond that itâ€™s a Tailscale customer. Any government who wanted to trace specific traffic back to OP would need to get the cooperation of both Mullvad and Tailscale, which is a lot less likely than even the quite unlikely event of getting Mullvad to cooperate.
True, but OP's threat model doesn't involve state actors outside Indonesia, so traffic analysis of the "last mile" between Mullvad node and whatever non-Indonesian service OP is trying to use (Twitter, Discord, â€¦) is not really relevant here. (Assuming Indonesia doesn't have capabilities we don't know of.)What might be more interesting is the case where the Indonesian government forces Twitter/Discord to give up IP addresses (which I find hard to believe but it's certainly not impossible). But then they'd still have to overcome Mullvad. It's much more likely that if OP has an account on Twitter/Discord, it is already tied to their person in many ways, and this would probably be the main risk here.
I mean multiple VPSs for redundancy. Contabo is maybe the cheapest I've seen and it's like 3$ mtl for the smallest?
And using another VPN like NordVPN or ProtonVPN is probably in the same category as Mullvad, but worth being cautious. If it's free, you are the product. If you pay, you're still sending your traffic to a publicly (usually) known server of a VPN. That metadata alone in some jurisdictions can still put you in danger.Stay safe
This is good overview, I just wanted to add that a VPS IP is not a residential IP. You will encounter roadblocks when you try to access services if you appear to be coming from a VPS. Not that I had a better solution, just to clarify what you can expect.
Tor also has anti-censorship mechanisms (snowflakes, ...). Depending on how aggressive the blocking is, Tor might be the most effective solution.
Wireguard is not censorship-resistant, and most VPN-averse countries block cross-border Wireguard. Why reply a practical question in an area in which you have no experience?
Yes. Fixed packet headers, predictable packet sizes. I don't know what "a common port" means in relation to wg.
Yeah. Tailscale uses 41641, and you can generally use whatever. I don't think there's any consensus, or majority.
Because Indonesia is new to the game and might still be catching up. Theyâ€™re probably playing whackamole with the most common public VPN providers and might not be doing deep packet inspection yet. I worked with someone getting traffic out of Hong Kong a year ago and there was a lot trial and error figuring out what was blocked and what was not. Wireguard was one that worked.
They recommend Tailscale in particular. Tailscale control plane and DERPs (which are functionally required on mobile) will be among the first to go.Outline (shadowsocks-based) and amnezia (obfuscated wg and xray) both offer few-click install on your own VPS, which is easier than setting up headscale or static wg infrastructure, and will last you longer.Also, you did not answer my "why" question. I'm not sure what question you were answering.
IMO most people should have a VPS even if you don't need it for tunneling. Living without having a place to just leave services/files is very hard and often "free" services will hold your data hostage to manipulate your behavior which is annoying on a good day.
Yeah they can be cheap, but I would definitely recommend having at least 3 for redundancy. If one get shut down or it's IP blacklisted you still hopefully have a backup line to create a replacement.
No, unless you pay month to month. If you wait till BF you can find some really good deals on sites like lowendspirit
> cost is flexible but probably 20-30$ per month minimum in hosting.$4/month VPS from DigitalOcean is more than enough to handle a few users as per my experience. I have a Wireguard setup like this for more than a year. Didn't notice any issues.
Hi, not well educated on the details of VPNs and network security so this may be a basic question, but - VPNs are used regularly by corporates to enable secure intranet access to people offsite, etc - surely completely blocking VPNs or detecting and punishing VPN users is severely detrimental to business and not something countries would want to do carte blanche? How does this work?
Do you still have access to GitHub?If so you can run BrowserBox in a GitHub action runner exposed via IP or ngrok tunnel. That will give you a browser in a free region. Easy set up via workflow.Youâ€™ll need a ngrok API key and a BrowserBox key. Hit us up: sales@dosaygo.com for a short term key at a discount if it works for you.We will offer keys for free to any journalists in censored regions.
Also sing-box [1]. I don't use it for its primary use case of censorship circumvention, but rather for some highly complex routing configurations it supports.My use case consists of passing some apps on my Android through interface A (e.g. banking apps through my 5G modem), some apps through US residential proxy (for US banks that don't like me visiting from abroad), and all the rest through VPN. And no root required!It's wild that GFW triggered creation of this and nothing like it existed / exists.[1]: https://github.com/SagerNet/sing-box
im curious, isn't ALL of your traffic appearing to be to just one website the most obvious giveaway?
*ray clients typically allow configuration of routing. So you can send only blocked stuff through the tunnel; or, in reverse, send some known-working stuff (e. g. local domain) direct. Also works as adblock.
I'm currently traveling in Uzbekistan and am surprised that wireguard as a protocol is just blocked. I use wireguard with my own server, because usually governments just block well known VPN providers and a small individual server is fine.It's the first time I've encountered where the entire protocol is just blocked. Worth checking what is blocked and how before deciding which VPN provider to use.
I should have mentioned that our use case isn't avoiding government firewalls, it's transiting through broken network environments.
WireGuard by itself has a pretty noticeable network pattern and I don't think they make obfuscating it a goal.There are some solutions that mimic the traffic and, say, route it through 443/TCP.
Wow, kinda crazy to think about a government blocking a protocol that just simply lets two computers talk securely over a tunnel.
Well, think about it - almost every other interaction you can have with an individual in another country is mediated by government. Physical interaction? You need to get through a border and customs. Phone call? Going through their exchanges, could be blocked, easy to spy on with wiretaps. Letter mail? Many cases historically of all letters being opened before being forwarded along.We lived through the golden age of the Internet where anyone was allowed to open a raw socket connection to anyone else, anywhere. That age is fading, now, and time may come where even sending an email to someone in Russia or China will be fraught with difficulty. Certainly encryption will be blocked.We're going to need steganographic tech that uses AI-hallucinated content as a carrier, or something.
> surprised that wireguard as a protocol is just blocked.Honestly this is the route I'm sure the UK will decide upon in the not too distant future.The job of us hackers is going to become even more important...
Cloak + wireguard should work fine on the server side. The problem is that I didn't find any clients for Android and I doubt there are clients for iOs that can (a) open a cloak tunnel and then (b) allow wireguard to connect to localhost...
A year ago I was traveling through Uzbekistan while also partly working remotely. IKEv2 VPN was blocked but thankfully I was able to switch to SSL VPN which worked fine. I didn't expect that, everything else (people, culture) in the country seemed quite open.
XRay protocol based VPN worked for me in Uzbekistan when I were travelling there.Wireguard is indeed blocked.
how can they detect it is wireguard, I thought the traffic is encrypted?how does it differ from regular TLS 1.3 traffic?
It's UDP, not TCP (like TLS) and has a distinguishable handshake. Wireguard is not designed as a censorship prevention tool, it's purely a networking solution.The tunnel itself is encrypted, but the tunnel creation and existence is not obfuscated.
There are many instances of Mastodon, and due to its federated nature, you can use any of them to access it, and even host your own.
Sure, but if you have an account on a different server, you can still see things posted on mastodon.social if you have followed someone there.
It would be easy to block on protocol level. Countries that block VPNs usually progress to that level pretty fast once they discover that simple IP blocks don't work.
I doubt that is the case once you do statistical analysis of it.Advanced VPN tunneling protocols, for example, have to take a lot of special measures to conceal their nature from China's and Russia's deep packet inspecting firewalls.
Nations severing peoples connections to the world is awful. I'm so sorry for the chaos in general, and the state doing awful things both.Go on https://lowendbox.com and get a cheap cheap cheap VPS. Use ssh SOCKS proxy in your browser to send web traffic through it.Very unfancy, a 30+ year old solution, but uses such primitive internet basics that it will almost certainly never fail. Builtin to everything but Windows (which afaik doesn't have an ssh client built-in).Tailscale is also super fantastic.
>  uses such primitive internet basics that it will almost certainly never fail.It already fails in China and Russia. Simply tunneling HTTP through SSH is too easy to detect with DPI.> Windows (which afaik doesn't have an ssh client built-in)It has had both SSH client and SSH server built-in since Win10.
I work often in China. I somehow havenâ€™t had my WireGuard VPN back to my own home server blocked, yet. Itâ€™s pointed to a domain that also hosts some HTTPS web services so that might help.Prior to this, pre-Covid I used to use shadowsocks hosted on a DO droplet. Shadowsocks with obfs, or a newer equivalent (v2ray w/ vmess or vless protocol) and obfs (reality seems to be the current hotness) will probably work within Indonesia given their blocking will be way less sophisticated than China. The difference here is that itâ€™s a proxy, not a VPN, but it makes it a lot easier to obfuscate its true nature than a VPN which stands out because obfuscation isnâ€™t in its design.Hosting on big public VPSs can be double edged. On one hand, blocking DO or AWS is huge collateral. On the other, itâ€™s an obvious VPN endpoint and can help identify the type of traffic as something to block.If you have access to reddit, r/dumbclub (believe it or not) has some relatively current info but itâ€™s pretty poor signal to noise. Scratch around there for some leads though.Note that this stuff is all brittle as hell to set up and I usually have a nightmarish time duct-taping it all together. Thatâ€™s why Iâ€™m overjoyed my WireGuard tunnel has worked whenever Iâ€™ve visited for a year now.One other left-field option, depending on your cost appetite, is a roaming SIM. Roaming by design tunnels all data back to your own ISP before routing out so even in China roaming SIMs arenâ€™t blocked. Itâ€™s a very handy backup if you need a clear link to ssh into a box to set up the above, for example.
Looks like MacOS and iOS only, which is unfortunate. Support for at least Windows and Android is needed for wider adoption. Linux would also be nice.
Very possible, though many of our users are saying that in network environments where WireGuard is blocked they were able to use Obscura.
Hey, I went to take a look at Obscura and I like the ideas but I can't find the source code.You are making some bold claims but without the source I can't verify those claims.Any plans to open-source it?
Looks good, just one note: btc was never meant for anonymity, if you would add Monero as a payment option that would be great.
An expensive but functional option is to enable roaming on a foreign eSIM. Getting an eSIM is relatively easy. Roaming mobile traffic is routed from the country in which the SIM is from, not the country that you're in, meaning that an eSIM from e.g. an American carrier will not be subject to the censorship in your country.I've used this on multiple trips to China over the past decade (including a trip last year). You can find carriers that will charge very low (or even no) roaming rates.
Data-only eSIMs (e.g. ones you get from Airalo and apps like that) are not going to cut it though. You need a "full" eSIM that gives you a real number and even then, it's not a guarantee that your traffic will be routed via the country eSIM is from. Tello does route (or rather, exit) via US for example, but it's 2Â¢/MB.Chinese forums / blogs have a lot of information about this stuff. I usually ask ChatGPT to translate "Research topic re: some form of circumvention and give me forum posts and blog posts about it" to Chinese, then paste that into DeepSeek with search enabled and just let Chrome translate the responses. Does a really good job. At least better than what I can manage with Baidu.
I don't know if these work or not for the specific case mentioned here, but the cheapest eSIMs by a huge margin are from https://silent.link/ if anyone is interested. They definitely do work under normal internet circumstances.
The most effective solution is to use X-ray/V2ray with VLESS, or VMESS, or Trojan as a protocol.Another obfuscated solution is AmneziaIf you are not ready to set up your own VPN server and need any kind of connection right now, try Psiphon, but it's a proprietary centralized service and it's not the best solution.
Tunneling via SSH (ssh -D) is super easy to detect. The government doesn't need any sophisticated analysis to tell SSH connections for tunneling from SSH connections where a human is typing into a terminal.Countries like China have blocked SSH-based tunneling for years.It can also block sessions based on packet sizes: a typical web browsing session involves a short HTTP request and a long HTTP response, during which the receiving end sends TCP ACKs; but if the traffic traffic mimics the above except these "ACKs" are a few dozen bytes larger than a real ACK, it knows you are tunneling over a different protocol. This is how it detects the vast majority of VPNs.
One alternative would be to set up a VPS, run VNC on it, run your browser on that to access the various web sites, and connect over an SSH tunnel to the VNC instance. Then it actually is an interactive ssh session.
15 years ago, I was using EC2 at work, and realized it was surprisingly easy to SSH into it in a way where all my traffic went through EC2. I could watch local Netflix when traveling. It was a de facto VPN.Details are not at the top of my mind these years later, but you can probably rig something up yourself that looks like regular web dev shit and not a known commercial VPN. I think there was a preference in Firefox or something.
The issue these days is that all of the EC2 IP ranges are well known, and are usually not very high-reputation IPs, so a lot of services will block them, or at least aggressively require CAPTCHAs to prevent botting.Source: used to work for a shady SEO company that searched Google 6,000,000 times a day on a huge farm of IPs from every provider we could find
I watched a season of Doctor Who that way back when the BBC were being precious about it. But Digital Ocean, so $5.
WireGuard should still work. Tons of different providers. I trust Mullvad but ProtonVPN has a free tier. If they start blocking WireGuard, check out v2ray and xray-core. If those get blocked... that means somehow they're restricting all HTTPS traffic going out of the country
In case known VPN providers are blocked you can pick a small VPS from a hoster like Hetzner and setup your own VPN.
In this scenario, Chinese have very rich experience.
you need to use the advance proxy tool like clash ,v2ray, shadowsocks etc.
shadowsocks was the winner of the state of the art I had to do at work. It address the "long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a stat)" comment.
Someone should create a vpn protocol that pretends to be a  command and control server exfiltrating US corporate secrets from hacked servers. The traffic pattern should be similar, and god forbid Xi blocks the real exfiltrations
What I'm worried most are that most people are not even aware of what is DNS and how to change it.I can't imagine those who are caught in the chaos with only their phone and unable to access information that could help them to be safe.
Generally speaking, the general population that wants to use blocked services will develop enough technical know-how to circumvent it. The biggest risk is that there are bad actors giving malicious advice and to such learners, looking to defraud or otherwise exploit them.
there is a major protest currently happening due to the legislative body representative just giving themselves a monthly domicile stipend of ~$3300 on top of their salaries (yes, multiple), while the average people earned ~$330 monthly. the information about the protest are not broadcasted on local TVs, so the only spread of information is through social media. i guess since a lot of people went around it using VPN, the gov decided to block it too.
â€œSome demonstrators on Monday were seen on television footage carrying a flag from the Japanese manga series One Piece, which has become a symbol of protest against government policies in the country.â€
The official word is to counter gambling. Lately the government is not really popular after some decisions that could be interpreted as authoritative, and as citizens have spoken out about it online, causing more voices to join and protests erupting..So well, my guess is they're trying to control it.
I'd recommend using Outline - it's a one click setup that lets you provision your own VPN on a cloud provider (or your own hardware).Since you get to pick where the hardware is located and it is just you (or you and a small group of friends & family) using the VPN, blocking is more difficult.If you don't want the hassle of using your own hardware you can rent a Digital Ocean droplet for <$5 per month.https://getoutline.org/
Iâ€™ve set this up for friends in fairly heavily censored countries before, it has been working well so far, but as others have said, this is a cat and mouse game
What is going on if you donâ€™t mind my asking? Our local news does not mention anything. Nor does ddging help? Any sources?
> the housing allowance for a month for a parliamentarian is now ten times the minimum wage for a month.I'm almost positive that everyone in the US Congress is making at least ten times the minimum wage in this country. The "housing allowance" being referred to is separate from their normal salary in Indonesia, but still, interesting to imagine how much more seriously people there would take that disparity than in many other countries.This caught my attention more:> Indonesia passed a law in March allowing for the military to assume more civilian posts, while this month the government announced 100 new military battalions that will be trained in agriculture and animal husbandry. In July the government said the military would also start manufacturing pharmaceuticals.They're replacing civilian industry with military, apparently not out of any emergency requirement but just to benefit the military with jobs (and the government with control over those sectors) at the expense of civilian jobs.
The ratio between Indonesian parliamentary income and the median Indonesian income is ~18x, while the ratio in the US is ~4x. As someone who wants US congressional income to be substantially higher, it's hard for me to be upset at that on its own. There are plenty of other variables at play, though, and a direct comparison of these ones might not be getting at the issue.
Iâ€™m not sure this is the right conversation right now, but is this thread heading towards â€œhow do we make totalitarian governments become liberal democracies?â€Itâ€™s a nice technical question on how to run a VPN but the ultimate goal is not the best technical solution but the ability to avoid detection by the state. And thatâ€™s not a technical problem but an opsec oneIf someone is participating in online discussions (discord and twitter) to spread local news - then itâ€™s hard to know who is who, and who to trust - and thatâ€™s kind of the why Arab spring did not spring â€œhey wear a red carnation and meet me by the cornerâ€ can become a death sentenceThe answer to opsec is avoid all digital comms - but at this point you are seriously into â€œregieme changeâ€, or just as Eastern Europe did, keep your heads down for forty years and hope those who leave you economically behind will half bankrupt them selves bringing you back.I think in the end, a thriving middle class with a sufficient amount of land reform, wealth taxes which can over a generation push for liberalisation sounds a good idea.Our job in the very lucky liberal
West is to keep what our forefathers won, and then push it further to show why our values are worth the sacrifice in copying
About VPNs I don't know but you could all start using Nostr instead of Twitter and Discord.Also Telegram using MTProto proxies (that you have to host, do not use those free ones out there), if those don't qualify as VPNs.
Chinese have developed a significant amount of sophisticated tools countering internet censorship. V2ray as far as I recall is the state-of-the-art.To use them, one need to first rent a (virtual) server somewhere from a foreign cloud provider as long as the payment does not pose a problem. The first step sometimes proves difficult for people in China, but hopefully Indonesia is not at that stage yet. What follows is relatively easy as there are many tutorials for the deployment like: https://guide.v2fly.org/en_US/
As someone based in China, it's a bit surprising that techniques used by Chinese people get very few mentions here, while I do think they are quite effective against access blocking, especially after coevolving with GFW for the past decade. While I do hope blocking in Indonesia won't get to GFW level, I will leave this here in case it helps.I found this article [0] summarizing the history of censorship and anti-censorship measures in China, and I think it might be of help to you if the national censorship ever gets worse. As is shown in the article, access blocking in China can be categorized into several kinds: (sorted by severity)1. DNS poisoning by intercepting DNS traffic. This can be easily mitigated by using a DOT/DOH DNS resolver.2. Keyword-based HTTP traffic resetting. You are safe as long as you use HTTPS.3. IP blocking/unencrypted SNI header checking. This will require the use of a VPN/proxy.4. VPN blocking by recognizing traffic signatures. (VPNs with identifiable signatures include OpenVPN and WireGuard (and Tor and SSH forwards if you count those as VPNs), or basically any VPN that was designed without obfuscation in mind.) This really levels up the blocking: if the government don't block VPN access, then maybe any VPN provider will do; but if they do, you will have a harder time finding providers and configuring things.5. Many other ways to detect and block obfuscated proxy traffic. It is the worse (that I'm aware of), but it will also cost the government a lot to pull off, so you probably don't need to worry about this. But if you do, maybe check out V2Ray, XRay, Trojan, Hysteria, NaiveProxy and many other obfuscated proxies.But anyways, bypassing techniques always coevolve with the blocking measures. And many suggestions here by non-Indonesian (including mine!) might not be of help. My personal suggestion is to find a local tech community and see what techniques they are using, which could suit you better.[0] https://danglingpointer.fun/posts/GFWHistory
Thanks for the link!Is there any good DoT/DoH DNS resolver that works well in China? I know I can build one myself, but forwarding all DNS requests to my home server in NA slows down all connections...
Personally, I like Amnezia VPN, it has some ways to work around blocks: https://amnezia.org/en
You can very easily self-host it, their installer automatically works on major cloud platforms.Though if Indonesia has blocked VPNs only now, possibly they only block major providers and don't try to detect the VPN protocol itself, which would make self-hosting any VPN possible.
Starlink, by policy, connects you through a ground station in the same country. They wouldn't be allowed to operate otherwise.
Starlink is a legitimate business (ISP) that wants to make money from customers in that country. They will comply with all of the regulations and bans imposed by the government in that country or risk getting banned completely.
It's not about blocking the sky. Starlink sends the internet connection back down to the ground somewhere in the country you are in.That being said, if I have an American starlink account, and I go to Indonesia, what happens? Does my internet connection go back down through Indonesia or does it go through somewhere else?
I live in Pakistan and two years back we had this exact same problem, (election interference) and frankly, you just try to scrape through solutions, but without an answerable government, there is little you can do.We tried things like Proton VPN and Windscribe VPN, as well as enabling MT proxy on Telegram, but soon govts find it easier to just mass ban internet access.Use Netblocks.org to analyse the level of internet blockage and try to react accordingly.
A question related to the question, for which I apologize:It seems to me that using WireGuard (UDP) in conjunction with something like Raptor Forward Error Correction would be somewhat difficult to block. A client could send to and receive from a wide array of endpoints without ever establishing a session and communicate privately and reliably, is that correct?
AmneziaWG is a decent option for censorship resistance, and it can be installed as a container on your own server.
Aren't there local (online or print) newspapers to get news from, as an alternative to Discord? Hope I'm not asking a dumb question
In countries where it comes to government blocking/censoring internet traffic, traditional media is cleared of all dissent and fully controlled long before. Last stages of that are happening in my country, Serbia, currently.
Right, that makes sense. Did some looking up and nonfree press seems to be indeed the case for Indonesia: https://rsf.org/en/country/indonesiaIt's a mixed bag apparently, free press is technically legal since 1998 but selective prosecution and harassment of those actually uncovering issues (mainly becomes clear in the last section, "Safety")Tried looking up Serbia next on that website but got a cloudflare block. I'm a robot now...
It's not a dumb question at all. Level on hn really got down lately if you're getting downvoted.Think about it Aachen. If the government has enough power to censor internet traffic, that what was the first thing it censored? Which media is traditionally known for being censored or just speaking propaganda? That's the classical newspapers. It's not uncommon in authoritarian countries for editors to need state to sign off on the day's paper. And if not that, articles are signed and publishers are known. They will auto-censor to avoid problems. Just like creators on YouTube don't comment on this one country's treatment of civilians to avoid problems.
It is not a real URI... lolThe point was to include something clowns can't filter without incurring collateral costs, and wrapping the ssh protocol in standard web traffic. =3
I was wondering something like this but in a different capacity.What with certain countries (they know who they are) and their hatred for encryption, it got me wondering how people would communicate securely if - for example - Signal/WhatsApp/etc. pulled out and the country wound up disconnecting the submarine cables to "keep $MORAL_PANIC_OF_THE_DAY safe."How would people communicate securely and privately in a domestic situation like that?
In person or not at all.At that point you've essentially lost.You either hope another country sees value in spreading you some democracy, or you rise up and hope others join you.Or not and you accept the protection the state is graciously providing to you.
An alternative is using an eSIM with an â€œinternet breakoutâ€ via another country.Esimdb is a good place to start.
Your first option until you get settled is to use an SSH reverse proxy:    ssh -D 9999 user@my.server

Then configure your browser to use local port 9999 for your SOCKS5 proxy.This gets you a temporarily usable system and if you can tunnel this way successfully installing some WireGuard or OpenVPN stuff will likely work.EDIT: Thanks it's -D not -R
Use the Tor browser window in Brave. It's nowhere near as anonymous as the Tor browser, but the built in ad blocking makes browsing via Tor usable. And that's what you and your compatriots are interested in.Prepare to fill in Cloudflare captchas all day, but that's what it takes to have a bit of privacy nowadays.
Usually when countries block websites they don't block major cloud providers, like AWS and Google Cloud. Because most websites are hosted on them. So you can get a cheap VPS from AWS or GCP (always free VM is available) and host OpenVPN on it.
In this case the blockage will probably just be up for a few days, until the protests calmed down.Other than that: tor
Try looking into tor bridges.You could also buy a VPS and use SSH tunneling to access a tor daemon running on a VPS. Host some sort of web service on the VPS so it looks inconspicuous
Set up a VM on AWS/azure/gcp/... in the desired cell, install a VPN server and done. Once you have automation in place it takes ~2 minutes to start, you can run it on demand so you can pay per minute.
Censorship circumvention tools specialize in this, and are extensively used in China, Iran, and Russia. I work on Lantern, and we're not seeing any significant interruptions to connections in Indonesia at the moment.
https://lantern.io/downloadHope it helps!
You should use people power to work to make Indonesia a more open, democratic society.Yes, it's hard work. Yes, it will take a long time. Yes, you personally may not get very far with your efforts.But if Indonesians don't take responsibility for and work to improve Indonesia then the rest of it doesn't matter.
Part of that is knowing whats happening inside the country, of which they were previously using tools like discord, which have now been blocked. So the first step to using people power to make Indonesia a more open, democratic society would be to find a way to tunnel out to get and share that information. To that end the OP has created this Ask HN thread.
Nope. The outside doesn't matter. The problem is on the inside. External websites will never fix the internal problem.There are no technical solutions to what is fundamentally a problem of political culture.
>External websites will never fix the internal problem.Except the internal problem is censoring internal information sources. They can only trust external sites to remain neutral.Not to mention that, politically and historically speaking, there are so many examples of revolutionaries needing to go overseas to organize. The Bolshies literally got started in a London pub.
Nope. They can simply talk to each other. I talk to people inside Indonesia routinely. I do it via SMS, via the phone, via iMessage, via Microsoft Teams. It's not difficult.You're not understanding the circumstances on a practical level. All you're doing is running away from the work to solve the fundamental political problem and that avoidance won't solve anything.
How do you propose they coordinate the political activities when they can't use external communications sites/tools, and internal sites are actively monitored by an authoritarian government?Step 1 is establishing a secure means of communication.
All the various proxy solutions offered are good (although the simplest ones - like squid - haven't been mentioned yet). You can also use a remote desktop or even just ssh -Y me@remote-server "firefox"
I like mullvad. You can buy a prepaid card off amazon. I figured out how to setup wireguard on various unixes Mac/linux/openbsd
Android doesn't come with system wide socks proxy support, and i couldn't find an open source app for it either. Is anyone aware of one?Nonetheless this is a surprisingly simple and bullet proof solution: SSH, that's not vpn boss, i need it for work.
Outline is an open source shadowsocks client, and you provision your own server to act as the proxy.  You can use it against any Shadowsocks server you want, and the protocol makes it look like regular https traffic.https://github.com/Jigsaw-Code/outline-appsAndroid & iOS & Linux & Mac & Windowstheir server installer will help set up a proxy for users that aren't familiar with shadowsocks, too
I'd recommend Obscura because it uses Wireguard over QUIC and it pretty good at avoiding these blocks. It's also open source.
Launch an EC2 instance in the US region (Ubuntu, open ports 22 and 1194), then connect via SSH and run the OpenVPN install script. Generate the .ovpn profile with the script and download it to your local machine. Finally, import the file into the OpenVPN client and connect to route traffic through the US server.
Make your own VPN using a VPS and something like openvpn.Not every website will allow it, but it should get you access to more than you have now.
Depending on the circumstances, maybe ditch the landline local ISP for a satellite connection with a foreign ISP?
Try a ssh socks5 proxy to a cheap vps.It worked well for me in UAE when other solutions didnâ€™t
A proxy service like shadow socks works. There are thousands of providers for $X/month for a decent amount of traffic
Yes, it's superseded by V* stuff and derivatives (VLess...), and probably by Trojan, but the latter is less popular.
The closest I've come to this is on an airplane where almost everything was blocked. SSTP to a server I spun up worked well.
OP, you can rent a VPS from a reputable and cheap provider within the NA region - OVH, Vultr, Linode etc. are decent. Also check out lowendtalk.comThen, setup Tailscale on the server. You can VPN into it and essentially browse the internet as someone from NA.
From some of the comments here I get why you are downvoted. But tbh I would also have gone that route. So are we just inexperienced? I read here indeed that wireguard is very easily blocked. It was at the company I worked for but then I just set port 23 (who uses ftp anyways??). And it worked. But why is this still bad then?Obviously I have 0 real experience with this.
Well, I mean, Tailscale is pretty easy overall. When client apps get blocked, you can literally hook up your router into Tailscale if needed, or you can run a headless version of Tailscale on your home server or the very machine you are on.It should also be possible to use a tunnel to get around the blocking of WireGuard, for example.You can then use it as an exit node if needed. It should work in theory, I have never tried this though. I just speak as a very frequent user of Tailscale with a bunch of nodes that are geographically located in different cities around me.
Sure, I know and use it too. But I saw you being downvoted so I responded to that. I think, reading the rest of the thread, your response (as mine would be) does not work as signals 0 experience with actually oppressing regimes. Not?
Shadowsocks used to be the thing that _really_ worked in CN. Not sure what's current there.AWS  ap-southeast-3 should still be up, and isn't in a different partition like CN, govcloud, iso etc. So a VM there and a vpc peer in the US should get you around a lot of stuff.
Shadowsocks isn't a viable method in 2025 it seems. Not by itself apparently. 
Shadowsocks generates high-entropy noise via packet analysis, which typically is easy to spot out as it looks irregular.
Full disclosure, I run a commercial VPN service (Windscribe).There are 2 paths you can take here:1. Roll your own VPN server on a VPS at a less common cloud provider and use it. If you're tech savvy and know what you're doing, you can get this going in <1hr. Be mindful of the downsides of being the sole user of your custom VPN server you pay for: cloud providers log all TCP flows and traffic correlation is trivial. You do something "bad", your gov subpoenas the provider who hands over your personal info. If you used fake info, your TCP flows are still there, which means your ISP's IP is logged, and deanonymizing you after that is a piece of cake (no court order needed in many countries).2. Get a paid commercial VPN service that values your privacy, has a diverse network of endpoints and protocols. Do not use any random free VPN apps from the Play/App stores, as they're either Chinese honeypots (https://www.bitdefender.com/en-us/blog/hotforsecurity/china-...) or total scams (https://www.tomsguide.com/computing/vpns/this-shady-vpn-has-...).Do not go with a VPN service that is "mainstream" (advertised by a Youtuber) or one that has an affiliate program. Doing/having both of these things essentially requires a provider to resort so dishonest billing practices where your subscription renews at 2-5x of the original price. This is because VPNs that advertise or run affiliate programs don't make a profit on the initial purchase for that amazing deal thats 27 months with 4 months free or whatever the random numbers are, they pay all of this to an affiliate, sometimes more. Since commercial VPNs are not charities, they need ROI and that comes only when someone rebills. Since many people cancel their subscriptions immediately after purchase (to avoid the thing that follows) the rebill price is usually significantly more than the initial "amazing deal". This is why both Nord and Express have multiple class action lawsuits for dishonest billing practices - they have to do it, to get their bag (back). It's a race to the bottom of who can offer the most $ to affiliates, and shaft their customers as the inevitable result.Billing quirks aside, a VPN you choose should offer multiple VPN protocols, and obfuscation techniques. There is no 1 magic protocol that just works everywhere, as every country does censorship differently, using different tools.- Some do basic DNS filtering, in which case you don't need a VPN at all, just use an encrypted DNS protocol like DOH, from any provider (Cloudflare, Google, Control D[I also run this company], NextDNS, Adguard DNS)- Then there is SNI filtering, where changing your DNS provider won't have any effect and you will have to use a VPN or a secure proxy (HTTPS forward proxy, or something fancier like shadowsocks or v2ray).- Finally there is full protocol aware DPI that can be implemented with various degrees of aggressiveness that will perform all kinds of unholy traffic inspection on all TCP and UDP flows, for some or all IP subnets.For this last type, having a variety of protocols and endpoints you can connect to is what's gonna define your chance of success to bypass restrictions. Beyond variety of protocols, some VPN providers (like Windscribe, and Mullvad) will mess with packets in order to bypass DPI engines, which works with variable degree of success and is very region/ISP specific. You can learn about some of these concepts in this very handy project: https://github.com/ValdikSS/GoodbyeDPI (we borrow some concepts from here, and have a few of our own).Soooo... what are good VPNs that don't do shady stuff, keeps your privacy in mind, have a reasonably sized server footprint and have features that go beyond basic traffic proxying? There is IVPN, Mullvad, and maybe even Windscribe. All are audited, have open source clients and in case of Windscribe, also court proven to keep no logs (ask me about that 1 time I got criminally charged in Greece for actions of a Windscribe user).If you have any questions, I'd be happy to answer them.
I can relate to this because my country has an election soon and I'm sure we wont have internet for 3 - 5 days then.
Tor should be pretty good even for environments where they crack down on VPNs, although it can be a bit slow, at least it works.
Yeah, sucks, but really should find better places for people to gather regardless, if you're in that sort of environment.
How is this practical advice in a thread where someone mentions that the clampdown happened without notice?The "shoulda done..." advice isn't useful in the slightest, and I'd argue is malicious with how often it's done simply to satiate a poster's ego.
You could rent a cheapo instance at a cloud provider and tunnel https over ssh.Thatâ€™s basically undetectable. Long lived ssh connection? Totally normal. Lots of throughput? Also normal. Bursts throughput? Same.Not sure how to do this on mobile.Tailscale might be an option too (they have a free account for individuals and an exit node out of country nearly bypasses your problem) It uses wireguard which might not be blocked and which comes with some plausible deniability. Itâ€™s a secure network overlay not a VPN. It just connects my machines, honest officer.
Just please be safe and necessarily paranoidOne way they tend to "solve" workarounds is making examples of people
Use an Actual Private Network? Radio links that you control. Peer with someone who owns a Starlink terminal. Rent instances in GCP's Jakarta datacenter.
Easy, you can just create any generic Linux Amazon EC2 instance (or just about any cloud provider of your choice; in fact, the smaller the provider, the better) and use it as a SOCKS5 proxy via SSH tunnel with -D flag... Then set one of your browsers (e.g. Firefox) to connect via that proxy.Indistinguishable from any other server on the internet.
Blocking Twitter is a good start, now Facebook, Instagram, Whatsup and TikTok.This is a good start but more should be blocked. Then force ISP to block ads.Not just for Indonesia but all countries. But we still have a lot more to do to fix the web.
The issue with that is where do they draw the line. Next thing you know each country becomes North Korea.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Dedalus Labs (YC S25) â€“ Vercel for Agents]]></title>
            <link>https://news.ycombinator.com/item?id=45054040</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054040</guid>
            <description><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools â€“ local or hosted by us. No Dockerfiles or YAML configs required.]]></description>
            <content:encoded><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools â€“ local or hosted by us. No Dockerfiles or YAML configs required.Hereâ€™s a demo: https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&t=11Last October, I was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and letâ€™s just say it was super annoying to buildâ€¦ I was thinking to myself the entire time â€œWhy canâ€™t I just pass in `tools=code_execution` to the model and just have itâ€¦work?Even with MCP, youâ€™re stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.The code you write looks something like this:  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=["author/server-1â€, â€œauthor/server-2â€],
    model=["openai/gpt-4.1â€, â€œanthropic/claude-sonnet-4-20250514â€],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too

Our docs start at https://docs.dedaluslabs.ai. Hereâ€™s a simple Hello World example: https://docs.dedaluslabs.ai/examples/01-hello-world. For basic tool execution, see https://docs.dedaluslabs.ai/examples/02-basic-tools. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: https://docs.dedaluslabs.ai/examples/use-case/weather-foreca....There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (we joke that the â€œSâ€ in MCP stands for â€œsecurityâ€). MCP servers right now are expected to act as both the authentication server and the resource server. That is too much to ask of server writers. People just want to expose a resource endpoint and be done.Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve them. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.We arenâ€™t quite yet at the stateful code execution sandbox that I wanted last October, but weâ€™re getting there! Shipping secure and stateful MCP servers is high on our priority list, and weâ€™ll be launching our auth solution next month. Weâ€™re also working on an MCP marketplace, so people can monetize their tools, while we handle billing and rev-share.Weâ€™re big on open sourcing things and have these SDKs so far (MIT licensed):https://github.com/dedalus-labs/dedalus-sdk-pythonhttps://github.com/dedalus-labs/dedalus-sdk-typescripthttps://github.com/dedalus-labs/dedalus-sdk-gohttps://github.com/dedalus-labs/dedalus-openapiWe would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.Thanks HN!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Will AI Replace Human Thinking? The Case for Writing and Coding Manually]]></title>
            <link>https://www.ssp.sh/brain/will-ai-replace-humans/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052784</guid>
            <description><![CDATA[Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. Itâ€™s OK to use it, and everyone does it, but still learn your craft, and try to think.]]></description>
            <content:encoded><![CDATA[
Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. Itâ€™s OK to use it, and everyone does it, but still learn your craft, and try to think.
Similar to what DHH said:

Itâ€™s also more fun to be competent in something than constantly waiting for an AI to complete.
The probability that AI will make us unhappy is very high IMO. Use it, yes, but not for every task. For discovering, creating a historical overview, or creating diagrams (Canva, Figma), but a big no to the writing (or coding). Someone needs to add knowledge or new insights; AI cannot train itself. So articles, books, and words will be written, and writers will be more in demand as everyone relies on AI, which at some point just plateaus.
It will be a long-term loss; people stop thinking and learning. Time will tell. My two cents, if you are a senior in something, you know better. 

Bsky
# Guidance on When to Use It
I 

heard from ThePrimeagen: It depends on how far you fix into the future. Short-term autocomplete is fine, but architectural decisions are big no, noâ€™s.

This is about the bottom where we have time and the left where we have amount of errors. This means that the longer we use AI for fixing something in the future, like an architecture, the more errors it will produce.
If we use it for quick autocomplete or creating a well-defined algorithm function, itâ€™s less prone to errors. In that first phase, you gain 20% productivity; in the later phases, you lose more.

This is like in real life, the longer I wait with making m decision, the more information I have, the better the decison will be. And is exactly what Shape Up preaches with maximum deciding for 6 weeks (a cycle),  donâ€™t have roadmaps and backlogs for longer than that in the future. Similar is it with using AI, as all of it is predicted probability.
Another great illustration by 

Forrest Brazeal:

Also to keep in mind whatâ€™s most imporant to your usecase like illustrated by Thomas Ptacek in 

My AI Skeptic Friends Are All Nuts Â· The Fly Blog:

# Soulless
Nobody wants to read some soulless text, and what if itâ€™s even good? Where do you get more from? I think this is a big trap that only over time people will realize. Sure, they help, and everyone needs to use them for â€œcertainâ€ tasks, but not the writing itself.
In the end, LLMs and AI require guidance; theyâ€™re just probabilities. See also Writing Manually.
# Distraction
I think we will be more distracted than ever. We canâ€™t even have 2 seconds to think before Grammarly, Copilot, or Cursor suggests something. So instead of doing the thinking, we just cruise on. We are losing the driverâ€™s seat.
It brings me back to the article I wrote recently about Â«

Finding FlowÂ». More on Donâ€™t use AI for everything, you stop thinking-learning AI Use and Writing is Hard.
# Donâ€™t Get Me Wrong
Donâ€™t get me wrong, I use it every day, too. But more deliberately. I turned off my Grammarly and my Copilot (a long time ago), so I have the space to think and to learn. If you do it once or twice, thatâ€™s OK, but if you do it everywhere, you also lose the ability to learn new skills or the fun of it.
Interesting about the LCI (LLM Collaborative Intelligence), and sure, there will be a lot of benefits, but I am not sure if these insights are anything that comes close to a human insight that has felt, sensed, or experienced something through hardship. So yes, but I do not have many expectations, nor do I want it to create new insights. This is the fun part of my job :)
# Exercising a Skill
Itâ€™s never always or never; itâ€™s in between. The problem with learning is if you use it often, Iâ€™d argue that you, in fact, donâ€™t learn much. You just copy and paste in writing or just tab tab tab in coding. The learning is gone. And do that often enough; our brain is not used to learning or, more critically, thinking anymore. Same as remembering, how good can we remember mobile phone numbers? not really, but I could very well during the early phone times because I trained it every day.
Itâ€™s all a matter of exercise, and I learned for myselfâ€”it doesnâ€™t have to be true for everyoneâ€”that I didnâ€™t learn or think anymore. And frankly, it was also not fun anymore. Thatâ€™s to be said in the stuff I know well.
In other areas, like creating an image (like the one I created for this article ðŸ˜†) or updating my websiteâ€™s front page with HTML/CSS, which would have taken me much longer as I donâ€™t practice, it helped a lot. But Iâ€™d argue the fact that I didnâ€™t learn anything new except prompting Claude Code :). Itâ€™s all tradeoffs, as always, right? :)
# Other Opinions
# Paul Graham on Writing
Paul Graham says on 

Writes and Write-Nots (internal):

The result will be a world divided into writes and write-nots. There will still be some people who can write.
Yes, itâ€™s bad. The reason is something I mentioned earlier: writing is thinking.
In fact thereâ€™s a kind of thinking that can only be done by writing.
If youâ€™re thinking without writing, you only think youâ€™re thinking.
So a world divided into writes and write-nots is more dangerous than it sounds. It will be a world of thinks and think-nots.

# Nathan Baugh
Nathan Baugh shares on 

About AI and ghostwriting:

1st Order Effect:

The world will be overrun with slop content and stories.
We already see this. Just look at AI written comments on this platform.

2nd Order Effect:

People will stop learning the foundational skills â€“ storytelling, writing, rhetoric â€“ required to communicate their experiences and ideas effectively.
They will over rely on AI. It starts as a tool, becomes a crutch, and ends as a hindrance.

3rd Order Effect:

People who invest in those same skills see massive returns.
Writing sharpens your ideas. Story gives leverage to those ideas.

# Ted Gioia
The good news, and why AI wonâ€™t replace writers on 2024-08-31 by Ted Gioia. Some of the reasons why he thinks AI Writing wonâ€™t be as good:



Source on Twitter/X. Full article 

Google Thinks Beethoven Looks Like Mr. Bean - by Ted Gioia.
# Mitchell Hashimoto

2.5 years into the AI craze, and I continue to firmly believe that if your company wasnâ€™t already interesting/succeeding without AI, then doing â€œwhatever plus AIâ€ isnâ€™t going to save you. For the few that seem this way (eg Cursor), I think their moat is a lot weaker than it seems. You have to play the game and the game is AI, but I donâ€™t think itâ€™s a defensible foundational capability. Might play out as an excellent land and grab strategy to buy you time to fill out the meat though. Mitchell Hashimoto on 

Twitter
# Andrew Ng
Andrew Ng on 

Twitter/X:

Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, â€œIt is far more likely that the programming occupation will become extinct [â€¦] than that it will become all-powerful. More and more, computers will program themselves.â€â€‹ Statements discouraging people from learning to code are harmful!
In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Todayâ€™s arguments not to learn to code continue to echo his comment.
As coding becomes easier, more people should code, not fewer!
Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step.
I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals â€” individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.
den>
One question Iâ€™m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is a great way to do that.
When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on â€” using the language of art â€” to get the result he wanted. I didnâ€™t know this language, and my paltry attempts at prompting could not deliver as effective a result.
Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools are continuing to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.
# Harry Dry

Big ideas are less about creativity and more about conviction. [..] So, what happened? â€˜Sauce and other shitâ€™ got incredibly cheap! [..] There is no AI prompt for conviction. Harry Dry
^64403f
More on Is AI solving this?.
# Jason Fried
As Jason Fried said, initially, itâ€™s magical. After a while, you see it so clearly and itâ€™s just average:



Cover letters? Yes!

The hardest thing is not making something.
The hardest thing is maintaining something.
Itâ€™s become so easy to just make stuff and vomit out ideas, and I mean this in the best possible wayâ€¦ 

Jason Fried on LinkedIn
This another valid insights, itâ€™s hard to maintain code that is not made by you, itâ€™s losing itâ€™s fun. Therefore this will be a big part of a winning business, to have sustainable, and energy to want to maintain a product. And not â€œjust making itâ€.
Also who takes responsibility for the generated (vibed) code?
# David Perell
David Perell has similar views as me on being soulless:

When you outsource your writing to AI, you end up with words that lack soul or personality. Gone go your quirks and your idiosyncrasies, which are the very things that make your writing irreplaceable. 

LinkedIn
# Ezra Klein
Ezra Klein has 

great insights that I very align with in terms of writing. He says that there are no shortcuts for research. When you grapple with a text or book for seven hours, it will change you. This will influence your writing, too. Thereâ€™s no summary that gives you this kind of in-depth connection.
Also, you canâ€™t prompt your way into it, as thereâ€™s no prompt that knows that you donâ€™t know yet, or AI doesnâ€™t know what you wanted to have read or what connections you would have made. On the contrary, you actually lose time reading something, and over time, we think we read lots of stuff, but we actually only read summaries. Full episode on 

The Case Against Writing with AI.
# Will It Replace X
# Writers


Are Cover letters still a thing? Yes. This reminded me of good writing is key for every job these days.  Writing was always an asset, but even more these days; although people think they donâ€™t need it, as AI is doing that. But thatâ€™s a very dangerous bet I wouldnâ€™t take.
I wrote more on Writing Manually.
# Data Engineers?
Probably not.
Nice 

comparison by Mehdi Ouazza:


Did the music record replace musicians 100 years ago? Nope, it changed them and the industry.
Did cloud computing take all IT jobs? Nope, it also changed the industry and our jobs.
Same here; it will change our industry and job, but we wonâ€™t disappear.

More on Will AI replace Data Engineers.
# Image Generation
Initial generation, yes. But final touch, no. Whenever I try to create images with AI, I am always initially impressed, but that quickly fades over time.
Yesterday, I updated my second brain image, but I changed it again today. I created some more with AI; prompted prompted prompted. In the end, I made one manually based on my copy. I think itâ€™s more powerful. What do you think?
# ChatGPT


# My Own


Some AI-generated images I like too, but they were always missing something, and yeah, they looked so AI-generated. I started to feel the same as I did for AI writing () and AI data engineering (Will AI replace Data Engineers); now, with AI image generation, doing it yourself is more fulfilling, and you end up happier.
More on AI Generated Images.
# How to detect AI Writing
How to detect AI Writing
If we know how AI is writing, should we stop using em dashes or thing AI does?
I donâ€™t think so. I love the em dash. I even have a keyboard shortcut for the em dash. And sometimes when I write a negation, Iâ€™m thinking Â«could that look like itâ€™s written by AIÂ».
But at the end, convition is a good word. I can focus what an AI thinks while I write, I must write. So having something to say, and trying my best to communicate that, is the best I can do.  ^ebca60
# History Logs
# From 2024-10-12
What AI Writing canâ€™t do, because it can only think one word at a time.
E.g., in the below example, as a writer you know you need to start all sentences the same, but the AI model canâ€™t do that.

Writing from Abundance is the art of collecting ideas so you can think better and avoid writerâ€™s block.
Writing from Conversation is the art of using dialogue to identify your best ideas and double down on them.Â 
Writing in Public is the art of broadcasting your ideas to the Internet so you become a beacon for people, opportunities, and serendipity.

More on Copywriting.
# AI Slop - Companies not doing great
AI Slop is generating more content, no matter the quality. Itâ€™s a the never ending Quality vs Quantity discussion, but now ever more important.
Here are some companies backpedaling after going full AI-first:

Klarna 

backpedaling AI customer service.:



â€œAfter years of depicting Klarna as an AI-first company, the fintechâ€™s CEO reversed himself, telling Bloomberg the company was once again recruiting humans after the AI approach led to â€œlower quality.â€ An IBM survey reveals this is a common occurrence for AI use in business, where just 1 in 4 projects delivers the return it promised and even fewer are scaled up.â€



Duolingo getting 

worse with AI
Next up, Shopify after the 

announcement to go full AI?

# Learning With AI
Learning with AI
# Future

Nice insights, why LLMs with token pretictors are not so good for understanding the worlds. It kinda works (but not so so good) for writing, but to understand physics, and world models, this is much harder he says: 

Metas AI Boss Says He DONE With LLMSâ€¦

# Further Reads



The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers
Smart Note Taking


My AI Skeptic Friends Are All Nuts Â· The Fly Blog
Companies that used AI to generate a quick solutions and now spending humans to fix it, expensively



Companies That Tried to Save Money With AI Are Now Spending a Fortune Hiring People to Fix Its Mistakes


Companies That Replaced Humans With AI Are Realizing Their Mistake




AWS CEO says AI replacing junior staff is â€˜dumbest ideaâ€™


Origin: Artificial General Intelligence
References: ChatGPT, My AI Logs of Will AI replace humans, My AI Prompts
Created 2024-08-31

]]></content:encoded>
        </item>
    </channel>
</rss>