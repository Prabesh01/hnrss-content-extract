<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 18:41:38 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Territorial Markings as a Predictor of Driver Aggression and Road Rage (2008)]]></title>
            <link>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1559-1816.2008.00364.x?prevSearch=allfield%3A%28szlemko%29</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095079</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: woomarks, transfer your Pocket links to this app or self-host it]]></title>
            <link>https://woomarks.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094936</guid>
            <description><![CDATA[woomarks
        FAQ]]></description>
            <content:encoded><![CDATA[
    
        woomarks
        FAQ
      

    

    
    Made with woomarks.

    
      
        Title
          
        

        URL
          
        

        Tags (comma-separated)
          
        

        

        
          
            Bulk Transfer
          
          
        
      
    
  
  
  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optery (YC W22) Is Hiring in Engineering, Legal, Sales, Marketing (U.S., Latam)]]></title>
            <link>https://www.optery.com/careers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094471</guid>
            <description><![CDATA[Now is a great time to join Optery. Optery is profitable, and we 3x-ed sales last year. Our product was awarded ‚ÄúEditors‚Äô Choice‚Äù by PCMag as the most outstanding in the personal data removal category for the 4th year in a row (2022 - 2025), Optery was ranked the #1 most effective of all personal data removal services tested in 2024 study by Consumer Reports, Optery was named one of Business Insider‚Äôs Top 30 Future Unicorns of 2025, and we‚Äôre changing the game in the world of consumer data in a way that puts individuals in control. Optery is automated opt out software, and we serve individuals, families and businesses. Our mission is to empower people to take control of their personal data, and we have a vision for a safer world through data privacy. Hundreds of thousands of people rely on Optery to prevent attacks and keep their personal information off the Internet. Optery has raised $9M+ in funding from world-class investors such as Y Combinator, Alumni Ventures, Bayhouse Capital, Flex Capital, Global Founders Capital, Goodwater Capital, Pioneer Fund, Soma Capital, TRAC, Tribe Capital, and Uncorrelated Ventures. Optery is headquartered in the San Francisco Bay Area, but operates as a fully-remote global team.]]></description>
            <content:encoded><![CDATA[
      Use promo code:  Xi8TJRBw  at checkout for 20% Off üéâ with Optery‚Äôs Labor Day Sale! üéá
    

          
        Ready to safeguard your personal data?
      
    
          
        Join the movement of people strengthening their privacy      
    
          
        Sign Up Free      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[93% of GPT-4 performance at 1/4 cost: LLM routing with weak bandit feedback]]></title>
            <link>https://arxiv.org/abs/2508.21141</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094421</guid>
            <description><![CDATA[Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.
    

    
    
              
          Comments:
          Accepted at EMNLP 2025 (findings)
        

          Subjects:
          
            Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21141 [cs.LG]
        
        
          ¬†
          (or 
              arXiv:2508.21141v1 [cs.LG] for this version)
          
        
        
          ¬†
                        https://doi.org/10.48550/arXiv.2508.21141
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Pranoy Panda [view email]          [v1]
        Thu, 28 Aug 2025 18:18:19 UTC (1,560 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lessons from building an AI data analyst]]></title>
            <link>https://www.pedronasc.com/articles/lessons-building-ai-data-analyst</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094256</guid>
            <description><![CDATA[Text-to-SQL is not enough. Real user questions require multi-step plans, Python, and external context. A semantic layer encodes business meaning and sharply reduces SQL complexity.]]></description>
            <content:encoded><![CDATA[AI/MLData AnalyticsMalloyTL;DRText-to-SQL is not enough. Answering real user questions requires going the extra mile like multi-step plans, external tools (coding) and external context.Context is the product. A semantic layer (we use Malloy ‚éã) encodes business meaning and sharply reduces SQL complexity.Use a multi-agent, research-oriented system. Break problems down using context / domain knowledge, retrieve precisely, write code, interact with the environment and learn from it.Retrieval is a recommendation problem. Mix keyword, embeddings, and a fine-tuned reranker; optimise for precision, recall, and latency.Benchmarks ‚â† production. Users expect human-level answers, drill-downs, and defensible reasoning, not just pass@k.Latency and quality are a tight bar. Route between fast and reasoning models; cache aggressively; keep contexts short. Continuous model evaluation is needed to avoid drifts as new models are launched.The short storyI spent years on ML for Analytics and Knowledge Discovery at Google and Twitter. For the past 3 years I've been building an AI data analyst at Findly (findly.ai ‚éã). We entered Y Combinator with a different idea, but quickly realised the real problem for most teams wasn't "lack of data" ‚Äî it was data discovery and use.We started the company as Conversion Pattern, tackling post-iOS 14 attribution and the privacy-driven collapse of cookie-based measurement. What we kept seeing: our customers already had most of the data they needed. They either didn't know it existed or couldn't stitch it together to answer business questions. The job wasn't to generate new data; it was to unlock the value of existing data.We started with a toy problem ‚Äî text-to-SQL ‚Äî and then let users pull us forward. The product evolved into a generative BI platform: it generates SQL, draws charts, writes Python for complex calculations, grounds itself in enterprise context, and pulls in external sources (web, PDFs) when the data story demands it.Why text-to-SQL isn't enoughReal questions rarely map to a single query:"Give me a study on the crude oil market.""Create a trading strategy‚Ä¶.""Compare these cohorts over the last four releases and explain the variance."You can sometimes force these into one monstrous SQL statement, but it's brittle and hard for current models. In practice, the system should run a multi-step workflow:01Plan the analysis by breaking down the problem, defining the required tools/capabilities.02Issue targeted SQL queries.03Join/transform in Python (safer merges, custom calcs, charting).04Validate assumptions with checks & sanity tests.05Visualise and explain the result.06Offer drill-downs and next questions.Bottom line: Text-to-SQL is a capability. The product is end-to-end analysis that stands up to scrutiny.Context Engineering & Semantic MetadataWhen building AI-powered data tools, context and metadata can mean the difference between the right and wrong answer. We invest heavily in a semantic layer for our data because it brings several critical benefits:Encodes business meaning: All the important context ‚Äì dimensions, measures, relationships, and constraints ‚Äì lives in a maintained semantic model instead of being buried inside prompts. Business logic (like how "revenue" is calculated or what qualifies as a "customer") is explicitly defined in one place and can be reused everywhere, rather than re-explained in every prompt. This also allows faster prototyping and testing.Shrinks the search space: By providing structured context, our LLM-based planner avoids guesswork with ambiguous table or column names. The model knows exactly which fields are relevant and won't wander off into nonexistent or irrelevant data. This drastically improves the reliability of generated SQL, because the AI isn't brainstorming schema details ‚Äì it's selecting from a known set.Enables compile-time checks: Because the LLM works against a defined schema and semantic model, we can validate its output before execution. If it tries to use a field that doesn‚Äôt exist or apply a metric incorrectly, the semantic layer‚Äôs compiler catches it early. This leads to fewer silent failures and much more predictable behavior when the SQL or code runs, allowing us to self-correct intermediary steps along the process.Our Choice: Malloy for Semantic ModelingTo implement this semantic layer, we chose Malloy ‚éã, an open-source semantic modeling language. Malloy lets us model our data relationships as a graph of sources (tables) and joins, then define metrics (measures) and dimensions in that graph. We express complex queries at the semantic level, and Malloy's compiler translates them into optimized SQL with strong guarantees of correctness ‚Äì essentially acting as our "knowledge graph plus compiler". In other words, Malloy serves as a single source of truth for business logic that ensures consistent, accurate SQL generation across the board.If your organization has already invested in a different semantic layer, the same principles apply. Snowflake's recently introduced Native Semantic Views ‚éã are one attempt to bake a semantic layer directly into their platform, and Google's Looker provides a semantic modeling solution on top of BigQuery. We'll explore these alternatives for the AI era in a future post. Regardless of the tool, the key is to make your business logic explicit and shareable.Another advantage of Malloy is the ability to attach rich metadata and documentation directly to the model. We annotate each measure and dimension with human-readable descriptions and tags (units, currencies, etc.) right alongside its definition. For example, we might tag a metric with its unit and add a description:In the snippet above, the total_revenue measure is clearly defined as the sum of price √ó quantity, and it's annotated with a description as well as a currency tag. Malloy's flexible annotation system allows us to store arbitrary metadata like this (in this case, noting that total_revenue is in USD). These descriptions and facts are not just for show ‚Äì they are programmatically accessible. Our application can retrieve them and pass them into the LLM's context, so the model knows, for instance, that "total_revenue" means "sum of price√óquantity in USD" without having to infer it purely from the name. We can also have operations like casting or even case statements to create maps that are directly related to the business logic.Integrating the Semantic Layer with LLMs (Functions & RAG)How do we actually feed this context to the LLM? We employ a combination of retrieval-augmented generation (RAG) and the LLM's function-calling capabilities to integrate Malloy's semantic layer into our AI workflow. Instead of dumping the entire data schema into every prompt, we maintain a lightweight knowledge base of the semantic model. When a user asks a question, we first retrieve the relevant model fragments (e.g. the definitions of any measures or dimensions that the question mentions) and include only those in the prompt. This keeps prompts concise and focused. The LLM sees only the pertinent pieces of context, which dramatically narrows its search space to the correct solution.Furthermore, we define a set of tools that the LLM can invoke as needed. Using function-calling API the model can ask for more info or actions. For example, if it needs additional detail about a field, it can call something like get_definition("trading_day_window") and our system will return the stored description/metadata for that term. Or the LLM might decide to call run_query(model, params) to execute a Malloy-defined query plan and retrieve some data. This way, the LLM doesn't have to guess or hallucinate schema details ‚Äì it can query the semantic layer directly for clarification. After gathering the needed context via these function calls, we can generate the final code (SQL or Python) with much higher confidence.It's worth noting that this semantic context benefits Python code generation as much as SQL. Because our model includes things like unit conversions and custom calendar logic, the LLM can produce Python code that's aware of those definitions. For instance, if certain measures are tagged as currency in USD or a trading_day dimension delineates business days vs. weekends, the assistant can incorporate that knowledge (maybe by calling a convert_currency() helper or using a pre-defined trading-days list) in the Python code it writes. By making the model more focused with a well-defined semantic layer, we get code that is not only plausible but also correct and aligned with our business rules.Example: Malloy Semantic Layer in ActionLet‚Äôs tie it all together with a concrete example using Malloy. Suppose we have an e-commerce dataset with orders and customers. We define a semantic model as follows:Here's a more complete example of our Malloy semantic model:Here we've explicitly modeled the relationships (linking orders to customers) and defined a metric total_revenue with a clear meaning and unit. Now imagine a user asks: "What was our total revenue by region last quarter?" Our system will:01Retrieve contextRecognize that the question involves the total_revenue measure and the region dimension. It pulls their definitions from the Malloy model (including the knowledge that total_revenue is price √ó quantity in USD, and that region comes from the customers table related to orders).02Provide context to the LLMConstruct a prompt that includes the user's question along with the retrieved semantic definitions. This might look like a short snippet of Malloy model info or a brief text explanation for each relevant field, injected before asking the LLM to formulate an answer.03Generate code via function callThe LLM analyzes the question with the given context and decides on a plan. It might output a structured function call such as generate_sql(query_params‚Ä¶) rather than a raw answer. For example, it could produce a call like generate_sql(model="orders", measure="total_revenue", dimension="customers.region", filter="order_date in last_quarter"). This is the signal to take over and produce the actual query.04Compile and validateOur backend function receives that structured request and uses Malloy to compile the corresponding query. Malloy knows about the orders‚Üícustomers join and the definitions of each field, so it can generate the correct SQL. If the LLM's request referenced something incorrectly (say an undefined field), Malloy would throw an error here ‚Äì catching the issue before execution.05Execute or return codeOnce the query is successfully compiled, we execute it on the database. In our example, Malloy would produce a SQL that joins the orders and customers tables, filters to last quarter's dates, groups by region, and sums the total_revenue. The end result might be a neat table of regions with their respective revenue, or the SQL code for it ‚Äì either way, it's guaranteed to be using the right tables, joins, and formulas.Note: This is a very simple example, the schema in practice for enterprises is much more complex. It usually even requires multiple calls to the system in order to get disjoint tables etc.The heavy lifting of "knowing the data" is handled by the semantic layer, and not left to the LLM. By making the business logic explicit and shareable, we ensure that both AI and humans are always speaking the same language ‚Äì and that language is formally defined (in Malloy, in our case). The outcome is answers and code that are not just plausible, but correct, maintainable, and aligned with the business's reality.Python code generation (and why it matters)A lot of business analysis is post-SQL computation: statistical tests, time-series transforms, strategy backtests, data quality checks. We run these in a sandboxed Python environment with pre-installed libraries tuned to the customer's domain. Two big benefits:Fewer tokens, more leverage. Libraries abstract tools and capabilities, allowing the model to just recall them instead of creating them from scratch.Better generalisation. The model composes short, readable Python blocks instead of over‚Äëfitting giant prompts. Pre-built, well-tested functions encode the general solution and its edge cases (missing timestamps, time zones, irregular sampling, NaNs, etc.). The model only has to compose these building blocks, so the resulting code is shorter, clearer, and behaves correctly across many datasets‚Äîi.e., it generalizes.A simple pattern that works well:Store facts with reasoning traces and explanations. When generating code, retrieve the relevant traces and let the model adapt them.Treat these snippets as natural‚Äëlanguage programs: general, succinct, and reusable. Reasoning models are good at recombining past programs (in the form of CoTs) ‚Äî it‚Äôs how they‚Äôre trained. Storing traces in your business context ‚Äúreminds‚Äù the model of the right approach and narrows the search space. AlphaEvolve ‚éã and Gemini 2.5 Pro Capable of Winning Gold at IMO 2025 ‚éã are good examples on how "hints" can significantly increase the results from the models.Also: treat prompts and reasoning traces as company assets. Store them, version them, test them.Multi-agent planning, memory, and groundingComplex requests benefit from decomposition. Our architecture uses cooperating agents that:01Plan the analysisDecompose tasks, choose tools, define checks.02Retrieve preciselySee next section, iterating when gaps are detected.03Generate SQL/PythonRun it in sandboxes.04ValidateWith unit checks / sanity tests.05Explain resultsAnd propose next questions.This reduces hallucinations and ambiguity, sharpens accountability by having more self-contained problems, and makes debugging possible. Memory (short- and long-term) keeps the system grounded in prior decisions and user preferences.Retrieval systems: treat RAG like recommendationsFigure 1. Multi-stage retrieval as a recommendation pipeline: cheap candidate generation ‚Üí instruction-tuned reranking ‚Üí minimal, high-quality context. Click the image to zoom.LLM speed suffers as context grows (transformer attention is ~quadratic in input length), so good retrieval is non-negotiable. The shorter and well curated the data you put in the LLM is, the better and faster the results will be. Think of it as a recommendation pipeline:Candidate generation: keyword search for internal acronyms and exact terms; embeddings for semantic matches. People should look more at their "RAG" systems as a full recommendation system. You can always improve the latency, precision and recall of the system.Reranking: a fine-tuned instruction-following reranker optimises for the current question style. (Off-the-shelf rerankers underperform without this.) Fine-tuning the reranker is important, otherwise it won't perform as well as needed. You can see a lot of companies have been releasing better instruction following reranker models ‚Äî this is quite important as the LLMs will be doing the queries.Multi-stage ranking: keep the early stages cheap; spend budget late where it matters. Aim for both precision (fewer irrelevant docs) and recall (don't miss the key one). This helps to keep latency and cost in check.Query rewriting: LLMs can write long, precise queries ‚Äî use that to drive search, not just retrieval.Chunking and keys: design retrieval keys to match how analysts think (e.g., metric ‚Üí dimension ‚Üí time), not how files are stored.Current search and recommendation systems are heavily optimized for humans: LLMs search is different from human search. LLMs are able to write complex, precise and verbose queries. This information needs to be used to make the search systems as precise as possible as part of a multi-agent framework. This also helps to reduce the information needed to be passed as context to the LLM.Note: the specific top@k thresholds and models used in an AI data analytics system really depends on the specific tasks being solved. Some problems require bigger models specially at the later stages, other ones not that much. Some of them might even require LLMs using a map-reduce / divide-and-conquer approach to get the right accuracy.Some of the companies that provide good reranking models:Open source: Qwen3 reranker - https://qwenlm.github.io/blog/qwen3-embedding/Voyage AI - https://blog.voyageai.com/2025/08/11/rerank-2-5/Contextual AI - https://contextual.ai/blog/introducing-instruction-following-reranker/Cohere - https://cohere.com/blog/rerank-3pt5The latest Voyage, Contextual and Qwen3 models all emphasize the instruction following capabilities, showing the need for it on multi-agent systems.The picture below shows the improvements given by the instructions following models from Voyage AI.Figure 2. Accuracy gains from instruction-following: +8.13% (rerank-2.5) and +7.55% (rerank-2.5-lite) across 24 datasets in 7 domains.I am excited about the usage of specialized hardware ‚éã and diffusion LLM language models (e.g., the ones by Inception Labs ‚éã) for rerankers. While most people focus a lot on the top-tier LLMs, having strong and extremely low-latency rerankers might be a much better performance improvement for AI systems than the top model itself.Different LLM choicesReasoning-style models are already excellent for text-to-SQL. They handle ambiguous or very hard questions well, and outright hallucinations are now uncommon. The trade-off is latency (and often cost), so you can't run them end-to-end across every step of a real-time pipeline.Key takeaways:Hallucinations aren't the main risk anymore. Modern reasoning models rarely fabricate facts outright.Context is the real failure mode. Missing schema details, vague user intent, or unclear join paths lead to wrong queries.Context engineering matters most. Invest in precise retrieval, schema selection, examples/constraints, and clear problem framing.Recommendation. If you're building an AI data-analyst workflow, use top-tier reasoning models for the SQL generation + schema reasoning step (e.g., Gemini 2.5 Pro, o4-mini, Claude 4 Sonnet). O3 and Claude 4 Opus are extremely strong, but their latency and cost typically make them impractical for interactive production use.A practical pattern is a hybrid setup: route easy or routine requests to a faster model, and automatically escalate the hard/ambiguous ones to a reasoning model. This preserves quality where it matters without blowing up response times.Common failure modes (and fixes)Ambiguous tables/joins ‚Äî push grain/joins into the semantic layer; add compile-time checks.Over-long contexts ‚Äî narrow retrieval keys; teach query rewriting; cache partial results.Quiet wrong answers ‚Äî add validators and reconciliation tests; require citations.Latency spikes ‚Äî stage reranking; cap tokens; route early to fast paths.Brittle prompts ‚Äî store & version traces; test against real user questions.What is nextAdaptive models that switch between fast and reasoning modes and know how much to think. This will lead to faster models on par with human expectations on how long a task should take given the difficulty.More agentic systems that explore alternative plans, fill knowledge gaps, and critique their own outputs.Automated knowledge extraction that continuously harvests and organises metadata and business logic. With curated knowledge, today's multi-agent systems can already tackle surprisingly complex tasks.In the next posts I will dig more on the specifics about semantic layer choices, what is missing to make enterprise program synthesis better, etc.Was this article helpful?]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Unique, High-Tech (Family) Computer]]></title>
            <link>https://nicole.express/2025/a-computer-in-your-home.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093956</guid>
            <description><![CDATA[There‚Äôs a concept that many people have tried, with varying effects: the ‚Äúeducational computer‚Äù, a device that a parent can buy for their children to learn t...]]></description>
            <content:encoded><![CDATA[
        

  

  
    There‚Äôs a concept that many people have tried, with varying effects: the ‚Äúeducational computer‚Äù, a device that a parent can buy for their children to learn the basics of the computer, which everyone will need to know in the future, and can also play games, so the children will actually want to use it. These have ranged from plasticky VTech toys with little more than an electronic organizer, to the Wonder Computer of the 1980‚Äôs, the Commodore VIC-20, which was a full computer. This is a prime market fit for an aging 8-bit platform, so of course, the Famicom has been wedged into it too‚Ä¶ but not by Nintendo.

Unique, High-Tech, What more could you want?



This is it: a unique, high-tech computer. As we can see, it‚Äôs also advertising Contra on the box, along with ‚Äú8 Bit‚Äù games, so immediately, you know that this is a Famiclone, and it‚Äôs got a Famicom cartridge slot underneath the cartridge flap. There‚Äôs been more than a few of these out there; they‚Äôre unique to me because they rarely show up in the United States (I bought this from Goodwill.com), but I would bet to many of the readers of this blog they won‚Äôt see this as unique at all.

What‚Äôs in the box?



In addition to the computer, you can see a whole selection of peripherals: two controllers, a mouse, a light-gun. And a power supply with a Europlug; further evidence that this is definitely not for the US market. Thankfully, it‚Äôs just 9V center-negative, so any plug you can use to power a Famicom should work here as well.



The sticker on the bottom of the system doesn‚Äôt match the sticker on the front of the box, but it does give us a release year for this model of the product: 2003. By 2003, the Famicom hardware was definitely old hat; in fact, that‚Äôs the same year Nintendo of Japan officially discontinued the system. You can definitely tell this sticker is trying to get you thinking this is relevant to the Windows XP world.



The sticker in the top left corner is long gone. Underneath is interesting, though; you can see three holes that look to the world like the Caps Lock, Num Lock, and Scroll Lock lights you‚Äôd see in the corner of a standard Windows keyboard of the era. Was this top case also used for standard keyboards? And if so, what did they do with the cartridge slot?



More evidence of plastics reuse is on the back, which shows a blanking plate covering nothing, and a speaker grille with no speaker behind it.



The actual ports you get are paltry; the common DB-9 ports you see for Famiclones, a power plug, and three RCA jacks. Think that‚Äôs stereo audio? (Something we have discussed as a Famicom mod on this blog before) Look closer!



The white RCA port is actually the RF modulator! Audio is the red jack. I‚Äôm guessing white, yellow, and red triplets of RCA ports were just extremely cheap at the time of this computer‚Äôs manufacture, so why not use them?



This is held together by screws, not plastic clips, which actually surprised me. But inside is just a standard keyboard membrane and a few small PCBs.



The keyboard mechanism is self-contained in the top plastic, and is actually a bit more elaborate than I expected; this is a ‚Äúslider over membrane‚Äù design, where pressing a key causes a tiny point-like piece of plastic to connect the membrane. It works fairly well; you could definitely learn to type on this. Assuming it didn‚Äôt bind as much when it was new and clean, anyway.



Where‚Äôs the Famiclone itself? It‚Äôs just underneath the cartridge port, of course!
And also of course, it‚Äôs an epoxy blob.



On the epoxy blob was a small piece of masking tape, which I removed for the earlier screenshot. I can‚Äôt quite make it out as the ink has unfortunately bled a lot; the first letter seems to be a ‚ÄúV‚Äù. A major series of Famiclone chips from V.R. Technology has serial numbers beginning with ‚ÄúVT‚Äù, which could be related.



One thing about that controller. You might notice that on a real NES controller, the A button is on the outside edge, and the B button closer to the center. This is labeled in the opposite way‚Äì and this is how the buttons are arranged, too. Why did they swap the button positions? I don‚Äôt know, perhaps they just don‚Äôt like games being playable. The X and Y buttons are turbo buttons, as is commonly the case on four-button controllers being used for the Famicom.

Built-in hardware

This Famiclone has no built-in software or games. That seems to be pretty standard for models with cartridge slots; everything that makes this an educational computer is on the ‚Äú48 in 1‚Äù cartridge. 48 is a much more achievable goal than many multicarts claim.



What‚Äôs inside?



An epoxy blob, of course, and 32kiB of SRAM. It‚Äôs a shame this is an epoxy blob, because I‚Äôm actually quite curious how that SRAM is wired. The NES memory map does not have room for 32kiB of cartridge PRG-ROM (usual amount of area mapped to the ROM) and 32kiB of cartridge RAM, so my assumption is that some sort of banking much be going on here.

Turn it on



But let‚Äôs boot the damn thing up already! Worth noting that this is a PAL 50Hz console; that should‚Äôve been evident from the Europlug. I don‚Äôt think anywhere uses the Europlug and 60Hz NTSC; though possibly parts of Latin America?





The UI is clearly inspired by Microsoft Windows, though not the Windows XP that the sticker on the console tries to hint at. It‚Äôs actually pretty adorable, though having to move the cursor to the top corner is annoying. (Protip: use the page up and page down keys on the keyboard) The cursor can be moved with the mouse, or the controller. What is Super Hero?

You don't have a video tag support or something, so long story short: it's a rhythm game

It‚Äôs a rhythm game of some sort. I can‚Äôt recognize the track, and I don‚Äôt know how to play the game either; it doesn‚Äôt seem like controller inputs are what it‚Äôs looking for, or the arrow keys on the keyboard? So I‚Äôll just let it be for now.

UPDATE: Thanks to The_Opponent for finding the track: Boys by smile.dk. Still not sure why it‚Äôs called ‚ÄúSuper Hero‚Äù, though.

This actually has a lot of unique elements. For example, like any good version of Microsoft Windows, it has Solitaire.



And like any good multicart, it pads things out. Not only does it break up Duck Hunt (remember that gun in the package?) into multiple games‚Ä¶



And yes, it is Nintendo‚Äôs Duck Hunt. What else did you expect?



The most extreme case is Konami‚Äôs Track & Field. It‚Äôs here, sure.



But it‚Äôs been broken up into so many individual options for individual events that an entire page of the menu is taken up by it.



Also, you know what Konami game is not present on this multicart? Contra. Which was advertised on the box.



There are some educational games. Not really worth noting too much; mostly focused on typing, though it can also sing ‚ÄúHappy Birthday to You‚Äù. Since the keyboard is pretty decent, that‚Äôs probably actually the best usecase, but making games focused on typing is always a bit limiting. Here‚Äôs a classic ‚Äúpress the key listed‚Äù game, with a ‚ÄúMy First Missile Command‚Äù theme.



But we were promised an Electronic Organ. So what does it have for a ‚ÄúMUSIC BOARD‚Äù?





That‚Äôs right; it‚Äôs MUSIC BOARD, from Nintendo and Hudson‚Äôs Family BASIC. Just separated into its own option on the menu, just like they did for Duck Hunt and Track and Field. Family BASIC MUSIC BOARD is fine, though I wouldn‚Äôt call it an electronic organ. I feel robbed.

But if Family BASIC‚Äôs MUSIC BOARD is here‚Ä¶



Then Family BASIC‚Äôs GAME BASIC should be here too. And it is! Or at least, I assume this is Family BASIC. (V3, judging by the version number) 32kiB of RAM is much more than it usually has access to, but is likely the purpose of the extra RAM on the cartridge. Very nice.



Unfortunately, this has some severe downsides compared to the real Family BASIC, despite the extra RAM. The biggest being that there is no way to save your work between sessions; neither battery-backed RAM nor a way to interface with a cassette tape. This pretty much relegates G BASIC to a novelty, though it always was one anyways.



Part of the FAMILY?



One thing I wondered here was, if it has Family BASIC on board, would the original one work?

Well‚Ä¶ unfortunately, Family BASIC has a very annoying UI where you have to talk to the computer using text. And so I learned that while the keyboard is compatible in the sense that pressing keys makes letters appear, the keyboard matrix has been remapped.



I didn‚Äôt even make it to the actual BASIC.



Computers for the whole family

As I noted, the educational computer market has a lot of entries. Many had features like printers; I wonder if that was what the blanking plate was for. This one is very bare-bones. But let‚Äôs face it; it was mostly a way for kids to get their parents to get something into the house which could play games like Super Mario Bros. 3, albiet at a PAL 50Hz slowdown.

You don't have a video tag support or something, so long story short: it's SMB3 but slow

Still, I think it‚Äôs a pretty cool bit of computing history, especially important outside the wealthier countries whose markets I usually look at. I hope you enjoyed!

  

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The time picker on the iPhone's alarm app isn't circular, it's just a long list]]></title>
            <link>https://old.reddit.com/r/interestingasfuck/comments/1n5lztw/the_time_picker_on_the_iphones_alarm_app_isnt/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093765</guid>
        </item>
        <item>
            <title><![CDATA[Search engine referral report for 2025 Q2]]></title>
            <link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid>
        </item>
        <item>
            <title><![CDATA[Isolated(any)]]></title>
            <link>https://nshipster.com/isolated-any/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093590</guid>
            <description><![CDATA[There are cases where just a little more visibility and control over how to schedule asynchronous work can make all the difference.]]></description>
            <content:encoded><![CDATA[
              Ahh, @isolated(any).
                It‚Äôs an attribute of contradictions.
                You might see it a lot, but it‚Äôs ok to ignore it.
                You don‚Äôt need to use it, but I think it should be used more.
                It must always take an argument, but that argument cannot vary.
              Confusing? Definitely.
                But we‚Äôll get to it all.
              To understand why @isolated(any) was introduced,
                we need to take a look at async functions.
              let respondToEmergency: () async -> Void

              This is about as simple a function type as we can get.
                But, things start to get a little more interesting
                when we look at how a function like this is used.
                A variable with this type must always be invoked with await.
              await respondToEmergency()

              This, of course, makes sense.
                All async functions must be called with await.
                But! Consider this:
              let sendAmbulance: @MainActor () -> Void = {
    print("üöë WEE-OOO WEE-OOO!")
}

let respondToEmergency: () async -> Void = sendAmbulance

await respondToEmergency()

              The explicit types are there to help make what‚Äôs going on clear.
                We first define a synchronous function that must run on the MainActor.
              And then we assign that to a plain old,
              non-MainActor async function.
            We‚Äôve changed so much that you might find it surprising this even compiles.
          Remember what await actually does. It allows the current task to suspend. That doesn‚Äôt just let the task wait for future work to complete. It also is an opportunity to change isolation. This makes async functions very flexible!
          Just like a dispatcher doesn‚Äôt sit there doing nothing while waiting for the ambulance to arrive, a suspended task doesn‚Äôt block its thread. When the dispatcher puts you on hold to coordinate with the ambulance team, that‚Äôs the isolation switch - they‚Äôre transferring your request to a different department that specializes in that type of work.
          
            But change to where, exactly?
          Ok, so we know that async functions, because they must always be awaited, gain a lot of flexibility. We are close, but have to go just a little further to find the motivation for this attribute.
          func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

await dispatchResponder {
    // no explicit isolation => nonisolated
    print("üöí HONK HOOOOONK!")
    await airSupport()
    print("üöÅ SOI SOI SOI SOI SOI!")
}

await dispatchResponder { @MainActor in
    print("üöë WEE-OOO WEE-OOO!")
}

          We now have a function that accepts other functions as arguments. It‚Äôs possible to pass in lots of different kinds of functions to dispatchResponder. They could be async functions themselves, or even be synchronous. And they can be isolated to any actor. All thanks to the power of await.
        Except there‚Äôs a little problem now.
          Have a look at dispatchResponder on its own:
      func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

      The type of responder fully describes everything about this function,
        except for one thing.
        We have no way to know its isolation.
        That information is only available at callsites.
        The isolation is still present,
        so the right thing happens at runtime.
        It‚Äôs just not possible to inspect it statically or even programmatically.
        If you‚Äôve encountered type erasure before,
        this should seem familiar.
        The flexibility of async has come with a price -
        a loss of information.
      This is where @isolated(any) comes in.
      
        Using @isolated(any)
      
      We can change the definition of dispatchResponder to fix this.
    func dispatchResponder(_ responder: @isolated(any) () async -> Void) async {
    print("responder isolation:", responder.isolation)

    await responder()
}

    When you apply @isolated(any) to a function type, it does two things. Most importantly, it gives you access to a special isolation property. You can use this property to inspect the isolation of the function. The isolation could be an actor. Or it could be non-isolated. This is expressible in the type system with (any Actor)?.
    Functions with properties felt really strange to me at first.
      But, after thinking for a minute,
      it became quite natural.
      Why not?
      It‚Äôs just a type like any other.
      In fact, we can simulate how this all works with another feature:
      callAsFunction.
struct IsolatedAnyFunction<T> {
    let isolation: (any Actor)?
    let body: () async -> T

    func callAsFunction() async -> T {
        await body()
    }
}

let value = IsolatedAnyFunction(isolation: MainActor.shared, body: {
    // isolated work goes here
})

await value()

This analogy is certainly not perfect,
  but it‚Äôs close enough that it might help.
There is one other subtle change that @isolated(any) makes to a function
  that you should be aware of.
  Its whole purpose is to capture the isolation of a function.
  Since that could be anything,
  callsites need an opportunity to switch.
  And that means an @isolated(any) function must be called with an await ‚Äî
  even if it isn‚Äôt itself explicitly async.
func dispatchResponder(_ responder: @isolated(any) () -> Void) async {
    await responder() // note the function is synchronous
}

This makes synchronous functions marked with @isolated(any) a little strange.
  They still must be called with await,
  yet they aren‚Äôt allowed to suspend internally?
As it turns out, there are some valid (if rare) situations
  where such an arrangement can make sense.
  But adding this kind of constraint to your API
  should at least merit some extra documentation.

  How @isolated(any) Affects Callers
All of the task creation APIs ‚Äî
  Task initializers and TaskGroup ‚Äî
make use of @isolated(any).
These are used a lot
and are usually encountered very early on when learning about concurrency.
So, it‚Äôs completely natural to run into this attribute and think:
‚ÄúUgh another thing to understand!‚Äù
It‚Äôs reasonable because
  the components of a function type dictate how it can be used.
  They are all essential qualities for API consumers.
  They are the interface.

  Parameters
  Return value
  Does it throw?
  Is it async?

This is not an exhaustive list,
  but what‚Äôs important is all of these are things callers must care about.
  Except for @isolated(any), which is the opposite.
  It doesn‚Äôt affect callers at all.
This, I think, is the root of a lot of confusion around @isolated(any).
  Unlike other qualities of a function,
  this attribute is used to capture information for the API producer.
I‚Äôm so close to saying ‚Äúyou can and should just ignore @isolated(any)‚Äú.
  But I just cannot quite go that far,
  because there is one situation you should be aware of.

  Scheduling
To help understand when you should be thinking about using @isolated(any),
  I‚Äôm going to quote
  the proposal:

  This allows the API to make more intelligent scheduling decisions about the function.

I‚Äôve highlighted ‚Äúintelligent scheduling‚Äù,
  because this is the key component of @isolated(any).
  The attribute gives you access to the isolation of a function argument.
  But what would you use that for?
Did you know that, before Swift 6.0, the ordering of the following code was undefined?
@MainActor
func threeAlarmFire() {
    Task { print("üöí Truck A reporting!") }
    Task { print("üöí Truck B checking in!") }
    Task { print("üöí Truck C on the case!") }
}

Ordering turns out to be a very tricky topic when working with unstructured tasks.
  And while it will always require care, Swift 6.0 did improve the situation.
  We now have some stronger guarantees about scheduling work on the MainActor,
and @isolated(any) was needed to make that possible.

Take a look at this:
@MainActor
func sendAmbulance() {
    print("üöë WEE-OOO WEE-OOO!")
}

nonisolated func dispatchResponders() {
    // synchronously enqueued
    Task { @MainActor in
        sendAmbulance()
    }

    // synchronously enqueued
    Task(operation: sendAmbulance)

    // not synchronously enqueued!
    Task {
        await sendAmbulance()
    }
}

These are three ways to achieve the same goal.
  But, there is a subtle difference in how the last form is scheduled.
  Task takes an @isolated(any) function
  so it can look at its isolation
  and synchronously submit it to an actor.
  This is how ordering can be preserved!
  But, it cannot do that in the last case.
  That closure passed into Task isn‚Äôt actually itself MainActor ‚Äî
it has inherited nonisolated from the enclosing function.
I think it might help to translate this into
  GCD.
func dispatchResponders() {
    // synchronously enqueued
    DispatchQueue.main.async {
        sendAmbulance()
    }

    // synchronously enqueued
    DispatchQueue.main.async(execute: sendAmbulance)

    // not synchronously enqueued!
    DispatchQueue.global().async {
        DispatchQueue.main.async {
            sendAmbulance()
        }
    }
}

Look really closely at that last one!
  What we are doing there is introducing a new async closure
  that then calls our MainActor function.
There are two steps.
This doesn‚Äôt always matter,
but it certainly could.
And if you need to precisely schedule asynchronous work,
@isolated(any) can help.

  isolated(all)
All this talk about @isolated(any) got me thinking‚Ä¶
It‚Äôs kinda strange that only some functions get to have this isolation property.
  It would certainly feel more consistent to me if all functions had it.
  In fact, I think we can go further.
  I can imagine a future where an explicit @isolated(any)
  isn‚Äôt even necessary for async functions.
  As far as I can tell, there is no downside.
And a little less syntactic noise would be nice.
  Perhaps one day!

  isolated(some)
We do have to talk about that any.
  It‚Äôs surprising that this attribute requires an argument,
  yet permits only one possible value.
  The reason here comes down to future considerations.
The concrete actor type that this isolation property returns
  is always (any Actor)?.
  This is the most generic type for isolation and matches the #isolation macro.
  Today, there is no way to constrain a function to only specific actor types,
  such as @isolated(MyActor).
The any keyword here was chosen to mirror how protocols handle this.
But accepting an argument leaves the door open
to more sophisticated features in the future.
And that really fits the spirit of @isolated(any).
  Doing a little work now in exchange for flexibility down the road.
Because you‚Äôll see it in many foundational concurrency APIs,
  it‚Äôs very natural to feel like you must understand @isolated(any).
  I‚Äôm 100% behind technical curiosity!
  In this case, however, it is not required.
  For the most part, you can just ignore this attribute.
  You will rarely, if ever, need to use it yourself.
But if you ever find yourself capturing isolated functions
  and passing them along to other APIs that use @isolated(any),
  you should consider adopting it.
  It could prove useful.
  It‚Äôs even a source-compatible change
  to add or remove this attribute from an async function.

So there you have it.
As with many parts of the concurrency system,
  there‚Äôs a surprising depth to @isolated(any).
  Thankfully, from a practical perspective,
  we can enjoy the ordering guarantees of task creation
  that it enables without needing to master it.
  And one less thing on this journey is most welcome.
Isolated maybe, but never alone.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who is hiring? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093192</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093192</guid>
            <description><![CDATA[Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.]]></description>
            <content:encoded><![CDATA[Ask HN: Who is hiring? (September 2025)77 points by whoishiring 3 hours ago  | hide | past | favorite | 72¬†commentsPlease state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.Please only post if you personally are part of the hiring company‚Äîno
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.Please only post if you are actively filling a position and are committed
to responding to applicants.Commenters: please don't reply to job posts to complain about
something. It's off topic here.Readers: please only email if you are personally interested in the job.Searchers: try https://dheerajck.github.io/hnwhoishiring/,
https://amber-williams.github.io/hackernews-whos-hiring/,
http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com,
https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/,
https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension:
https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....Don't miss these other fine threads:Who wants to be hired? https://news.ycombinator.com/item?id=45093190Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45093191
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who wants to be hired? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093190</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093190</guid>
            <description><![CDATA[Share your information if you are looking for work. Please use this format:]]></description>
            <content:encoded><![CDATA[Ask HN: Who wants to be hired? (September 2025)29 points by whoishiring 2 hours ago  | hide | past | favorite | 89¬†commentsShare your information if you are looking for work. Please use this format:  Location:
  Remote:
  Willing to relocate:
  Technologies:
  R√©sum√©/CV:
  Email:

Please only post if you are personally looking for work. Agencies, recruiters, job boards,
and so on, are off topic here.Readers: please only email these addresses to discuss work opportunities.There's a site for searching these posts at https://www.wantstobehired.com.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid>
        </item>
        <item>
            <title><![CDATA[Effective learning: Rules of formulating knowledge (1999)]]></title>
            <link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid>
            <description><![CDATA[This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge.]]></description>
            <content:encoded><![CDATA[Dr Piotr Wozniak, February, 1999 (updated)This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledgeThe speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!¬†¬†The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as in¬†SuperMemo).The 20 rules of formulating knowledge in learning1) Do not learn if you do not understandTrying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such ‚Äúblind learning‚Äù is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe ‚Äúit will be useful some day‚Äù. ¬†2) Learn before you memorizeBefore you proceed with memorizing individual facts and rules, you need to¬†build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned in¬†Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g.¬†the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g.¬†What moves the pistons in the internal combustion engine?), etc.3) Build upon the basicsThe picture of the learned whole (as discussed in¬†Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!4) Stick to the¬†minimum information principleThe material you learn must be formulated in as simple way as it isSimple is easyBy definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex materialRepetitions of simple items are easier to scheduleI assume you will make repetitions of the learned material using optimum inter-repetition intervals (as in¬†SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split into¬†ten or more¬†simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!Here is a striking example:Ill-formulated knowledge ‚Äì Complex and wordyQ: What are the characteristics of the Dead Sea?A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth‚Äôs surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline watersWell-formulated knowledge ‚Äì Simple and specificQ: Where is the Dead Sea located?A:¬†on the border between Israel and JordanQ: What is the lowest point on the Earth‚Äôs surface?A:¬†The Dead Sea shorelineQ: What is the average level on which the Dead Sea is located?A:¬†400 meters¬†(below sea level)Q: How long is the Dead Sea?A:¬†70 kmQ: How much saltier is the Dead Sea than the oceans?A:¬†7 timesQ: What is the volume content of salt in the Dead Sea?A:¬†30%Q: Why can the Dead Sea keep swimmers afloat?A:¬†due to high salt contentQ: Why is the Dead Sea called Dead?A:¬†because only simple organisms can live in itQ: Why only simple organisms can live in the Dead Sea?A:¬†because of high salt contentYou might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e.¬†the longer the time you need to remember knowledge, the more you benefit from simplifying your items!Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition!¬†We want answer to be as short as imaginably possible!You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted to¬†enumerations)5) Cloze deletion¬†is easy and effectiveCloze deletion¬†is a sentence with its parts missing and replaced by three dots.¬†Cloze deletion exercise¬†is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example,¬†Bill ‚Ä¶[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique called¬†incremental reading.Ill-formulated knowledge ‚Äì Complex and wordyQ: What was the history of the Kaleida company?A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleida‚Äôs mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.Well-formulated knowledge ‚Äì Simple cloze deletionQ: Kaleida was funded to the tune of ‚Ä¶(amount) by Apple Computer and IBM in 1991A: $40 millionQ: Kaleida was funded to the tune of $40 million by ‚Ä¶(companies) in 1991A: Apple and IBMQ: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in ‚Ä¶ (year)A: 1991Q: ‚Ä¶(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three yearsA: Kaleida‚ÄôsQ: Kaleida‚Äôs mission was to create a ‚Ä¶ It finally produced one, called Script X. But it took three yearsA: multimedia programming languageQ: Kaleida‚Äôs mission was to create a multimedia programming language. It finally produced one, called ‚Ä¶ But it took three yearsA: Script XQ: Kaleida‚Äôs mission was to create a multimedia programming language. It finally produced one, called Script X. But it took ‚Ä¶(time)A: three yearsQ: Kaleida‚Äôs mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as ‚Ä¶ had snapped up all the businessA: Macromedia/AsymetrixQ: Kaleida‚Äôs mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in ‚Ä¶(year)A: 1995Optional: SuperMemo Recipe:SuperMemo 2002SuperMemo 2000SuperMemo 98/99Creating¬†cloze deletions¬†in new SuperMemos:select the keyword that is to be replaced with tree dots and press¬†Alt+ZGenerating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:1. Press¬†Ctrl+Alt+N¬†to paste the text to SuperMemo¬†2. Select the part that is to be replaced with three dots 3. Right-click to open the¬†component menu¬†and select¬†Reading : Remember cloze¬†(or click one of cloze icons on the reading toolbar)¬†Cloze deletions in SuperMemo 98/99:1. Press¬†Ctrl+A¬†to add a standard question-and-answer item2. Paste the text into the question field. This will create the outline of your items3. Press¬†Ctrl+Alt+U¬†to¬†Duplicate¬†the element4. Select the part that is to be replaced with three dots5. Cut the selection to the clipboard (e.g. with¬†Shift+Del)6. Type in three dots (optionally, add the explanation in parentheses as in above examples)7. Press¬†Ctrl+T¬†to save the question field and move to the answer field8. Paste the text cut in Step 5 (e.g. with¬†Shift+Ins¬†or¬†Ctrl+V). Your first item is ready9. Press¬†PgUp¬†to go back to the outline item created in Step 210. Goto Step 3 and continue adding new items6) Use imageryVisual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we say¬†one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzan‚Äôs mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.Less beneficial formulationQ: What African country is located between Kenya, Zambia and Mozambique?A: TanzaniaWell-formulated knowledge ‚Äì Simple cloze deletionQ: What African country is marked white on the map?A: Tanzania7) Use¬†mnemonic techniquesMnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will need¬†SuperMemo¬†and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as:¬†mind maps, peg lists,¬†mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!Exemplary mind map:(Six Steps¬†mind map generated in¬†Mind Manager 3.5, imported to SuperMemo 2004, courtesy of John England,¬†TeamLink Australia)8) Graphic deletion¬†is as good as cloze deletionGraphic deletion works like¬†cloze deletion¬†but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The student‚Äôs job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:SuperMemo 2000/2002SuperMemo 99This is how you can quickly generate graphic deletion using a picture from the clipboard:1. Press¬†Shift+Ins¬†to paste the picture to SuperMemo2. Press¬†Ctrl+Shift+M¬†and choose¬†Occlusion¬†template to apply graphic deletion template3. SuperMemo 2000 only: Choose¬†Ctrl+Shift+F2¬†to impose and detach the¬†Occlusion¬†template4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (use¬†Alt+click¬†twice to set the rectangle in the dragging mode)In SuperMemo 99 you will need a few more steps:1.Create an item containing the following components:‚Äì question text: What is the name of the area covered with the red rectangle?‚Äì empty answer text (click Answer on the component menu)‚Äì your illustration (use Import file on the image component menu)‚Äì red rectangle component (choose red color with Color on the rectangle component menu)2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)3. Ctrl+click the rectangle component twice to place it in the dragging mode4. Drag and size the red rectangle to cover the area in question5. Type in the answer in the answer field6. Press PgUp to go back to the original element created in Step 17. Go to Step 2 to add generate more graphic deletionsNote that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from >SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above techniqueA more detailed recipe for creating occlusion tests is presented in:¬†Flow of knowledge9) Avoid setsA¬†set¬†is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example:¬†What countries belong to the European Union?¬†You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them into¬†enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of the¬†minimum information principle:¬†you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason:¬†Avoid sets!¬†If you need them badly, convert them into enumerations and use¬†techniques for dealing with enumerationsIll-formulated knowledge ‚Äì Sets are unacceptable!Q:¬†What countries belong to the European Union¬†(2002)?A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.Well-formulated knowledge ‚Äì Converting a set into a meaningful listingQ: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?A: FranceQ: Which countries apart from France joined the European Coal and Steel Community in 1952?A: Germany, Italy and the BeneluxQ: What countries make up the Benelux?A: Belgium, Luxembourg, and the NetherlandsQ: Whose membership did Charles de Gaulle oppose in the 1960s?A: that of UKQ: Which countries joined the EEC along the UK in 1973?A: Ireland and DenmarkQ: Which country joined the EEC in 1981?A: GreeceQ: Which countries joined the EEC in 1986?A: Spain and PortugalQ: Which countries joined the EU in 1995?A: Austria, Sweden and FinlandQ: What was the historic course of expansion of the European Union membership?A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and FinlandNote that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with the¬†minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.10) Avoid enumerationsEnumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them using¬†cloze deletions¬†(overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:Hard to learn itemQ: What is the sequence of letters in the alphabet?A: abcdefghijklmnopqrstuvwxyzEasy to learn itemsQ: What three letters does the alphabet begin with?A: ABCQ: Fill out the missing letters of the alphabet A ‚Ä¶ ‚Ä¶ ‚Ä¶ EA: B, C, DQ: Fill out the missing letters of the alphabet B ‚Ä¶ ‚Ä¶ ‚Ä¶ FA: C, D, EQ: Fill out the missing letters of the alphabet C ‚Ä¶ ‚Ä¶ ‚Ä¶ GA: D, E, FThe above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabet¬†after¬†making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict the¬†minimum information principle¬†because¬†the extra information is added in extra items.You can also deal with enumerations by using grouping like in the case of sets (see the¬†European Union example) but cloze deletions should be simpler and should suffice in most cases.Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurableA poem that is hard to rememberQ: The credit belongs ‚Ä¶ (Teddy Roosevelt)A: The credit belongs to the man who‚Äôs actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeat¬†A poem split into easy itemsQ: The credit belongs ‚Ä¶ (Teddy Roosevelt)A: to the man who‚Äôs actually in the arenaQ: The credit belongs to the man who‚Äôs actually in the arena ‚Ä¶A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)Q: whose face is marred by dust and sweat ‚Ä¶ (The credit belongs)A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)Q: a man who knows the great enthusiasm and the great devotions ‚Ä¶ (The credit belongs)A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)Q: who spends himself in a worthy cause ‚Ä¶ (The credit belongs)A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!11) Combat¬†interferenceWhen you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the words¬†historic¬†and¬†historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case of¬†memory interference. You can often remember an item for years with straight excellent grades until ‚Ä¶ you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is to¬†detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and ‚Ä¶ aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:make items as unambiguous as possiblestick to the¬†minimum information principle¬†(many of the remaining rules in this text are based on avoiding interference!)eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the word¬†inept¬†you think ‚ÄúI know the meanings of¬†inept¬†and¬†inapt¬†but I will never know which is which!‚Äù)in SuperMemo use¬†View¬†:¬†Other browsers :¬†Leeches(Shift+F3) to regularly review and¬†eliminate¬†most difficult itemsread more:¬†Memory interference12) Optimize wordingThe wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based ‚Ä¶ blew past. PageMaker, now owned by Adobe, remains No. 2Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based ‚Ä¶ blew past. PageMaker, now owned by Adobe, remains No. 2A: QuarkBetter item: fewer words will speed up learningQ: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then ‚Ä¶ blew past (PageMaker remains No. 2)A: QuarkOr better:Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by ‚Ä¶A: QuarkOr better:Q: PageMaker failed to improve and was outdistanced by ‚Ä¶A: QuarkOr better:Q: PageMaker lost ground to ‚Ä¶A: QuarkNote that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name:¬†Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!13) Refer to other memoriesReferring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the words¬†humble¬†and¬†supplicant¬†helps the student focus on the word¬†shamelessly¬†and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the words¬†humble¬†and¬†supplicant¬†makes it possible to avoid interference of¬†cringing¬†with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rules¬†basics-to-details¬†and¬†do not learn what you do not understand¬†require that the words¬†humble¬†and¬†supplicant¬†be learned beforehand (or at least at the same time)Item subject to strong interferenceQ: derog: adj: shamelessly conscious of one‚Äôs failings and asking in a begging wayA: cringingItem that uses interfering memories to amplify the correct meaningQ: derog: adj: shamelessly humble and supplicantA: cringing14) Personalize and provide examplesOne of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the questionItem subject to strong interferenceQ: What is the name of a soft bed without arms or back?A: divanItem that uses interfering memories to amplify the correct meaningQ: What is the name of a soft bed without arms or back? (like the one at Robert‚Äôs parents)A: divanIf you remember exactly what kind of soft bed can be found in Robert‚Äôs parents‚Äô apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant to¬†interference¬†and can greatly reduce your learning time15) Rely on emotional statesIf you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situationHarder itemQ: a light and joking conversationA: banterEasier itemQ: a light and joking conversation (e.g. Mandela and de Klerk in 1992)A: banterIf you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such as¬†badinage¬†or even¬†chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent to¬†25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted on¬†simplicity¬†and against the¬†interference16) Context cues¬†simplify wordingYou can use¬†categories¬†in¬†SuperMemo 2000/2002, provide different branches of knowledge with a different look (different¬†template), use reference labels (Title, Author, Date,¬†etc.) and clearly label subcategories (e.g. with strings such as¬†chem¬†for¬†chemistry,¬†math¬†for¬†mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefix¬†bioch:¬†saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the label¬†bioch¬†which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and ‚Ä¶ what is worse ‚Ä¶ you will light up the wrong areas of your brain that will now perhaps be prone to interference!Wordy item can cause accidental lapses through interferenceQ: What does GRE stand for in biochemistry?A: glucocorticoid response elementContext-labeled items increase success rateQ: bioch: GREA: glucocorticoid response element17) Redundancy¬†does not contradict minimum information principleRedundancy¬†in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict the¬†minimum information principle¬†and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate that¬†minimum information principle¬†cannot be understood as¬†minimum number of characters or bits in your collections or even items:passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such as¬†phone-telefono, language-lingvo, hope-esperanto,¬†etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail with¬†telefono-phone, lingvo-language,¬†or¬†esperanto-hope.¬†Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themIn¬†SuperMemo 2000/2002, you can quickly generate swapped word-pair items with¬†Duplicate¬†(Ctrl+Alt+D) and¬†Swap¬†(Ctrl+Shift+S)reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of just¬†memorizing¬†the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) and¬†generate¬†the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitionsderivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read:¬†Roots of genius and creativity, as well as more specific:¬†Derivation, reasoning and intelligencemultiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with the¬†forgetting index)!flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g.¬†a mark made by ink spilt on sth¬†=¬†blot/blob/blotch)more18) Provide¬†sourcesExcept for well-tested and proven knowledge (such as¬†2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g.¬†Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).19) Provide¬†date stampingKnowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.20) PrioritizeYou will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see:¬†learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:Prioritizing sources¬†‚Äì there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of othersExtracting knowledge¬†‚Äì unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to use¬†incremental reading¬†toolsTransferring knowledge to SuperMemo¬†‚Äì you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate itFormulating items¬†‚Äì make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its contextUsing forgetting index¬†‚Äì you can use the¬†forgetting index¬†to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priorityLearning¬†‚Äì the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:Remember¬†(Ctrl+M) ‚Äì re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you choose¬†Ctrl+M¬†you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantlyReschedule¬†(Ctrl+J) ‚Äì manually schedule the date of the next repetitionExecute repetition¬†(Ctrl+Shift+R) ‚Äì manually execute a repetition even before the repetition‚Äôs due date (e.g. when reviewing particularly important material)Forget¬†(Ctrl+R)- remove the current item from the learning process and place it at the end of the¬†pending queueDismiss¬†(Ctrl+D)¬†‚Äì ignore the current item in the learning process altogetherDelete¬†(Ctrl+Shift+Del) ‚Äì remove the current item from your collectionChange the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)SummaryHere again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:Do not learn if you do not understandLearn before you memorize¬†‚Äì build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!Build upon the basics¬†‚Äì never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit inStick to the minimum information principle¬†‚Äì if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)Cloze deletion is easy and effective¬†‚Äì completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginnersUse imagery¬†‚Äì a picture is worth a thousand wordsUse mnemonic techniques¬†‚Äì read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You won‚Äôt have problems with phone numbers and complex figuresGraphic deletion is as good as cloze deletion¬†‚Äì obstructing parts of a picture is great for learning anatomy, geography and moreAvoid sets¬†‚Äì larger sets are virtually un-memorizable unless you convert them into enumerations!Avoid enumerations¬†‚Äì enumerations are also hard to remember but can be dealt with using cloze deletionCombat interference¬†‚Äì even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal lifeOptimize wording¬†‚Äì like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maximsRefer to other memories¬†‚Äì building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gapsPersonalize and provide examples¬†‚Äì personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memoriesRely on emotional states¬†‚Äì emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in rememberingContext cues simplify wording¬†‚Äì providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interferenceRedundancy does not contradict minimum information principle¬†‚Äì some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!Provide sources¬†‚Äì sources help you manage the learning process, updating your knowledge, judging its reliability, or importanceProvide date stamping¬†‚Äì time stamping is useful for volatile knowledge that changes in timePrioritize¬†‚Äì effective learning is all about prioritizing. In¬†incremental¬†reading¬†you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also:¬†Incremental reading,¬†Devouring knowledge,¬†Flow of knowledge,¬†Using tasklists]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google AI Overview made up an elaborate story about me]]></title>
            <link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid>
        </item>
        <item>
            <title><![CDATA[Git for Music ‚Äì Using Version Control for Music Production (2023)]]></title>
            <link>https://grechin.org/2023/05/06/git-and-reaper.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid>
            <description><![CDATA[last updated on 6 Apr 2024]]></description>
            <content:encoded><![CDATA[
        

  

  
    
  last updated on 6 Apr 2024


Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.

Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.

Okay, and now let‚Äôs get to the point and‚Ä¶



Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?

my-cool-song-new-vocals-brighter-mix-4.rpp

Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?

This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as ‚Äúgit‚Äù, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).

For music production, I use Reaper, and instead of creating dozens of copies of my project file (my-cool-song.rpp), such as my-cool-song-new-vocals-brighter-mix-4.rpp, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the ‚Äúhome‚Äù for managing the version of our music project.

By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.


  


My git-based music production workflow

Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install git-bash, and have all the git functionality at your fingertips through a command-line interface.

First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$


in the example above:

  I first navigated to the directory with my project with cd command
  initialized a repository with git init .
  on the last, third line, my command prompt starts having a little (master) thing, which is the default ‚Äúbranch‚Äù in my repository that Git has created for me


I also create a .gitignore file and that this is this particular project file that I want to track, and not any other, such as media or peak files:

*

!in_your_eyes_remix_git_managed.rpp


Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. ‚Äúbass vst settings adjusted‚Äù.

Then I can see all the versions of my project in git gui tool.




  side note: you can use any git frontend, not only git gui.


Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.

The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.

Managing other files (WAVs etc.)

Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.

About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.

This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.

collaborating with GIT? Not sure‚Ä¶

GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don‚Äôt see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.

And let‚Äôs not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.

Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can‚Äôt really consider it a real backup - because of missing media.

Tracking TODO items for your music project in GitHub

Interesting use-case I‚Äôm currently testing is to have a ‚Äútodo list‚Äù, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:

fix panning issues in chorus TODO
add one more synth layer TODO


Once it‚Äôs in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don‚Äôt forget to pull your update, though, once you are back to your DAW PC).

In conclusion, when we inspect this idea of ‚Äúgit for music‚Äù a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!

Thanks for reading.

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI enters the grant game, picking winners]]></title>
            <link>https://www.science.org/content/article/ai-enters-grant-game-picking-winners</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092880</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Simple modenized .NET NuGet server reached RC]]></title>
            <link>https://github.com/kekyo/nuget-server</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid>
            <description><![CDATA[Simple modenized NuGet server üì¶. Contribute to kekyo/nuget-server development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[nuget-server
Simple modenized NuGet server implementation.






(Êó•Êú¨Ë™û„ÅØ„Åì„Å°„Çâ)
What is this?
A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.
Compatible with dotnet restore and standard NuGet clients for package publishing, querying, and manually downloading.
A modern browser-based UI is also provided:

You can refer to registered packages. You can check various package attributes.
You can download packages by version.
You can also publish (upload) packages.
You can manage user accounts.

Browse package list:

Publishing packages:

User account managements:

Key Features

Easy setup, run NuGet server in 10 seconds!
NuGet V3 API compatibility: Support for modern NuGet client operations
No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements
Package publish: Flexible client to upload .nupkg files via HTTP POST using cURL and others
Basic authentication: Setup authentication for publish and general access when you want it
Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution
Modern Web UI with enhanced features:

Multiple package upload: Drag & drop multiple .nupkg files at once
User account management: Add/delete users, reset passwords (admin only)
API password regeneration: Self-service API password updates
Password change: Users can change their own passwords


Package importer: Included package importer from existing NuGet server
Docker image available


Installation
npm install -g nuget-server
For using Docker images, refer to a separate chapter.
Usage
# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json
The NuGet V3 API is served on the /v3 path.

Default nuget-server served URL (Show UI): http://localhost:5963
Actual NuGet V3 API endpoint: http://localhost:5963/v3/index.json

The default URL provided by nuget-server can be changed using the --base-url option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.
Configure the NuGet client
nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.
If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.
Add as package source:
For HTTP endpoints:
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections
For HTTPS endpoints:
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Or specify in nuget.config:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
</configuration>
Publish packages
Upload packages by HTTP POST method, using cURL or any HTTP client with /api/publish endpoint:
# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
You may be dissatisfied with publishing using this method. The dotnet command includes dotnet nuget push, which is the standard approach.
However, in my experience, this protocol uses multipart/form-data for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.
Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately.
You might still feel issue with managing read operations and publish operation with the same key,
but in that case, you can simply separate the users.
For authentication feature, please refer to below chapter.

Package storage configuration
Storage location
By default, packages are stored in the ./packages directory relative to where you run nuget-server.
You can customize this location using the --package-dir option:
# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location
Package storage layout
Packages are stored in the filesystem using the following structure:
packages/
‚îú‚îÄ‚îÄ PackageName/
‚îÇ   ‚îú‚îÄ‚îÄ 1.0.0/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PackageName.1.0.0.nupkg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PackageName.nuspec
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ icon.png            # Package icon (if present)
‚îÇ   ‚îî‚îÄ‚îÄ 2.0.0/
‚îÇ       ‚îú‚îÄ‚îÄ PackageName.2.0.0.nupkg
‚îÇ       ‚îú‚îÄ‚îÄ PackageName.nuspec
‚îÇ       ‚îî‚îÄ‚îÄ icon.jpg            # Package icon (if present)
‚îî‚îÄ‚îÄ AnotherPackage/
    ‚îî‚îÄ‚îÄ 1.5.0/
        ‚îú‚îÄ‚îÄ AnotherPackage.1.5.0.nupkg
        ‚îú‚îÄ‚îÄ AnotherPackage.nuspec
        ‚îî‚îÄ‚îÄ icon.png            # Package icon (if present)

Backup and restore
You can backup the package directory using simply tar or other achiver:
cd /your/server/base/dir
tar -cf - ./packages | lz4 > backup-packages.tar.lz4
Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.

Configuration
nuget-server supports configuration through command-line options, environment variables, and JSON file.
Settings are applied in the following order (highest to lowest priority):

Command-line options
Environment variables
config.json
Default values

Configuration file structure
You can specify a custom configuration file:
# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server
If not specified, nuget-server looks for ./config.json in the current directory.
config.json structure
Create a config.json file:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "<your-secret-here>",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}
All fields are optional. Only include the settings you want to override.
Both packageDir and usersFile paths can be absolute or relative. If relative, they are resolved from the directory containing the config.json file.

Authentication
nuget-server also supports authentication.



Authentication Mode
Details
Auth Initialization




none
Default. No authentication required
Not required


publish
Authentication required only for package publishing
Required


full
Authentication required for all operations (must login first)
Required



To enable authentication on the NuGet server, first register an initial user using the --auth-init option.
Initialize
Create an initial admin user interactively:
nuget-server --auth-init
This command will:

Prompt for admin username (default: admin)
Prompt for password (with strength checking, masked input)
Create users.json
Exit after initialization (server does not start)

When enabling authentication using a Docker image, use this option to generate the initial user.
Example session
Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================

User Management
Users added with --auth-init automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.

While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.
Using the API password
The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server.
The password used by NuGet clients when accessing the server is called the "API password,"
and access is granted using the combination of the user and the API password.
Please log in by displaying the UI in the browser.
Select the ‚ÄúAPI password‚Äù menu from the UI menu to generate an API password.
Using this API password will enable access from the NuGet client.

Here is an example of using the API password:
# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections
Or specify nuget.config with credentials:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
  <packageSourceCredentials>
    <local>
      <add key="Username" value="reader" />
      <add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" />
    </local>
  </packageSourceCredentials>
</configuration>
For package publishing:
# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
When publishing a package, you can send the package by setting Basic authentication in the Authorization header.
Password strength requirements
nuget-server uses the zxcvbn library to enforce strong password requirements:

Evaluates password strength on a scale of 0-4 (Weak to Very Strong)
Default minimum score: 2 (Good)
Checks against common passwords, dictionary words, and patterns
Provides real-time feedback during password creation

Configure password requirements in config.json:
{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}
The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved.
However, if you do not use HTTPS (TLS), be aware that the Authorization header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.

Import packages from another NuGet server
Import all packages from another NuGet server to your local nuget-server instance.
This feature can be used when migrating the foreign NuGet server to nuget-server.
Package import from another NuGet server
Import packages interactively in CLI:
nuget-server --import-packages --package-dir ./packages
This command will:

Prompt for source NuGet server URL
Ask if authentication is required
If needed, prompt for username and password (masked input)
Discover all packages from the source server
Download and import all packages to local storage
Display progress for each package (1% intervals)
Exit after import (server does not start)

Import behavior

Existing packages with the same version will be overwritten
Failed imports are logged with error details
Progress is reported at 1% intervals to reduce log noise
Package icons are preserved during import

Parallel downloads are not done. This is to avoid making a large number of requests to the repository.
This feature is a type of downloader.
Therefore, it does not need to be run on the actual host where it will operate.
You can perform the import process in advance on a separate host and then move the packages directory as-is.
Example session
Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================


Reverse proxy interoperability
The server supports running behind a reverse proxy.
For example, when you have a public URL like https://nuget.example.com and run nuget-server on a host within your internal network via a gateway.
In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.
URL resolving
The server resolves URLs using the following priority order:

Fixed base URL (highest priority): When --base-url option is specified, it always takes precedence
Trusted proxy headers: When trusted proxies are configured with --trusted-proxies:

HTTP Forwarded header (proto, host, port)
Traditional X-Forwarded-* headers (X-Forwarded-Proto, X-Forwarded-Host, X-Forwarded-Port)


Standard request information (fallback): Uses Host header when proxy headers are not available

For example --base-url option:

nuget-server served public base URL: https://packages.example.com
Actual NuGet V3 API endpoint: https://packages.example.com/v3/index.json

# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Another option, you can configure with trusted proxy addresses:
# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"
Environment variables are also supported:
export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here

Docker usage
Docker images are available for multiple architectures:

linux/amd64 (x86_64)
linux/arm64 (aarch64)

When pulling the image, Docker automatically selects the appropriate architecture for your platform.
Quick start
Suppose you have configured the following directory structure for persistence (recommended):
docker-instance/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ user.json
‚îî‚îÄ‚îÄ packages/
    ‚îî‚îÄ‚îÄ (package files)

Execute as follows:
# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat > docker-compose.yml << EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d
Your NuGet server is now available at:

Web UI: http://localhost:5963
NuGet V3 API: http://localhost:5963/v3/index.json

Permission requirements
The Docker container runs as the nugetserver user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.
Set proper permissions for mounted directories:
# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages
Important: Without proper permissions, you may encounter 500 Permission Denied errors when:

Creating or updating user accounts
Publishing packages
Writing configuration files

Basic usage
# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest
You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use config.json.
Since the Docker image has mount points configured, you can mount /data and /packages as shown in the example above and place /data/config.json there to flexibly configure settings. Below is an example of config.json:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}
When initializing credentials or importing packages, configure config.json and perform the operation via the CLI before launching the Docker image:
# Initialize authentication
nuget-server -c ./data/config.json --auth-init
Volume mounts and configuration

/data: Default data directory for config.json, users.json and other persistent data
/packages: Default package storage directory (mounted to persist packages)

Default behavior: The Docker image runs with --users-file /data/users.json --package-dir /packages by default.
Configuration priority (highest to lowest):

Custom command line arguments (when overriding CMD)
Environment variables (e.g., NUGET_SERVER_PACKAGE_DIR)
config.json file (if explicitly specified)
Default command line arguments in Dockerfile

Example of Automatic Startup Using systemd
Various methods exist for automatically starting containers with systemd.
Below is a simple example of configuring a systemd service using Podman.
This is a simple service unit file used before quadlets were introduced to Podman.
By placing this file and having systemd recognize it, you can automatically start the nuget-server:
/etc/systemd/system/container-nuget-server.service:
# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target

Building the Docker image (Advanced)
The build of the nuget-server Docker image uses Podman.
Multi-platform build with Podman (recommended)
Use the provided multi-platform build script that uses Podman to build for all supported architectures:
# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect
Important: For cross-platform builds, QEMU emulation must be configured first:
# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64
Without QEMU, you can only build for your native architecture.

Note
Non-interactive mode (CI/CD)
The --auth-init and --import-packages options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:
export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json
This allows initialization in CI/CD pipelines without user interaction.
Session Security
For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:
export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server
(Or use config.json.)
If not set, a random secret is generated (warning will be logged).
Supported NuGet V3 API endpoints
The server implements a subset of the NuGet V3 API protocol:

Service index: /v3/index.json
Package content: /v3/package/{id}/index.json
Package downloads: /v3/package/{id}/{version}/{filename}
Registration index: /v3/registrations/{id}/index.json


License
Under MIT.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems]]></title>
            <link>https://github.com/gargakshit/zfsbackrest</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid>
            <description><![CDATA[pgbackrest style encrypted backups for ZFS filesystems - gargakshit/zfsbackrest]]></description>
            <content:encoded><![CDATA[zfsbackrest

‚ö†Ô∏è Experimental:
Do not use it as your only way for backups. This is something I wrote over a
weekend. There's a lot of things that need work here.

pgbackrest style encrypted backups for ZFS
filesystems.
Getting Started
Installing
You need age installed to generate
encryption keys. Encryption is NOT optional.
$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest
Configuring
Create /etc/zfsbackrest.toml.
debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4
Creating a repository
$ zfsbackrest init --age-recipient-public-key="<your age public key>"
Backing up
$ zfsbackrest backup --type <full | diff | incr>
full backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.
diff backups are sent incrementally from the latest full backup. They depend
on the parent full backup to be present in the repository to restore.
incr backups are send incrementally from the latest diff backup. They depend
on the parent diff backup to restore.
Viewing the repository
$ zfsbackrest detail
It shows a list of backups, orphans and all.
Cleaning up the repository
Sometimes, orphaned backups are left as an artefact of incomplete or cancelled
backups. You can clean those by running
$ zfsbackrest cleanup --orphans --dry-run=false
You can clean up expired backups by running
$ zfsbackrest cleanup --expired --dru-run=false
Restoring
To restore the backups, you'll need your age identity file (private key).
zfsbackrest restore -i <path-to-age-identity-file> \
  -s <name of the dataset to restore from> \
  -b <optionally, the backup ID to restore from, leave empty to restore the latest> \
  -d <name of the dataset to restore to> # Restoring to a dataset that already exists on your local FS will fail.
Safety
zfsbackrest doesn't write or modify actual zfs datasets. It makes extensive
use of snapshots. List of zfs operations used by zfsbackrest are


backup

zfs snapshot - Creating a zfs snapshot for zfsbackrest
zfs hold - Creating a reference to that snapshot to prevent removal
zfs send - Sending the snapshot incrementally



cleanup / force-destroy

zfs release - Release the held snapshot
zfs destroy - Destroy the snapshot



restore

zfs recv - Receiving the remote snapshot



Model
TODO
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bear is now source-available]]></title>
            <link>https://herman.bearblog.dev/license/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092490</guid>
            <description><![CDATA[Updates to the Bear license]]></description>
            <content:encoded><![CDATA[
  
  
    
      
        ·ïï( ·êõ )·ïó Herman's blog
      
    
    
      Home Now Projects Blog

    
  
  
    

    
        
    

    
        

        
            
                
                    01 Sep, 2025
                
            
        
    

    When I started building Bear I made the code available under an MIT license. I didn't give it much thought at the time, but knew that I wanted the code to be available for people to learn from, and to make it easily auditable so users could validate claims I have made about the privacy and security of the platform.
Unfortunately over the years there have been cases of people forking the project in the attempt to set up a competing service. And it hurts. It hurts to see something you've worked so hard on for so long get copied and distributed with only a few hours of modification. It hurts to have poured so much love into a piece of software to see it turned against you and threaten your livelihood. It hurts to believe in open-source and then be bitten by it.
After the last instance of this I have come to the difficult decision to change Bear's license from MIT to a version of copyleft called the Elastic License‚Äîcreated by the Elastic Search people.
This license is almost identical to the MIT license but with the stipulation that the software cannot be provided as a hosted or managed service. You can view the specific wording here.
After spending time researching how other projects are handling this, I realise I'm not alone. Many other open-source projects have updated their licenses to prevent "free-ride competition" in the past few years.123456
We're entering a new age of AI powered coding, where creating a competing product only involves typing "Create a fork of this repo and change its name to something cool and deploy it on an EC2 instance".
While Bear's code is good, what makes the platform special is the people who use it, and the commitment to longevity.
I will ensure the platform is taken care of, even if it means backtracking on what people can do with the code itself.


    

    
        

        
            


        
    


  
  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tetris is NP-hard even with O(1) rows or columns (2020) [pdf]]]></title>
            <link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid>
        </item>
    </channel>
</rss>