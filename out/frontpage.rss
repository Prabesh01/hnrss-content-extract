<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 30 Aug 2025 17:50:42 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Fed up with macOS – it downloaded 47 GB of 4K 240FPS screensavers. Asahi FTW]]></title>
            <link>https://news.ycombinator.com/item?id=45076119</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45076119</guid>
            <description><![CDATA[Not Asahi but I recently revived a 10-year-old MacBook Pro (MBP 2015) that had been sitting in my closet for many years by installing Fedora on it. To my surprise, it's fast and sleek, just like a brand new computer. All of the drivers worked!]]></description>
            <content:encoded><![CDATA[
Not Asahi but I recently revived a 10-year-old MacBook Pro (MBP 2015) that had been sitting in my closet for many years by installing Fedora on it. To my surprise, it's fast and sleek, just like a brand new computer. All of the drivers worked!The laptop now serves as a desktop when I'm at home and as an SSH server when I'm at work. And my 5-year-old M1 MacBook is now sitting in the closet, waiting for its turn in the next 10 years.
It's sort of insane to switch to an almost completely unrelated operating system because of such a trivial feature as a screensaver.It doesn't download those screensavers unprompted, you have to actually select them in the System Settings.It's hard to believe that anyone who thought that a Mac was the right computer for them could so easily switch to Linux on hardware which has a great number of unsupported features.I know that Hacker News is not representative of the real world, but gosh, stories like this are crazy.
It doesn’t download these by default. All the wallpaper and screensaver downloads are user initiated in their respective UI.Did you click the download button on them by any chance, and forget about it?
the biggest problem with switching away from Mac is losing the ecosystem benefits. When Apple TV automatically knows to fill your iCloud password, all the Apple Watch integration, syncing everything from notes to reminders. I can't see how any Linux can match all that.
  % du -hcs "/Library/Application Support/com.apple.idleassetsd/Customer/"
  5.1M /Library/Application Support/com.apple.idleassetsd/Customer/
  5.1M total

Is this only a thing in latest Sequoia? I've been clicking "remind me later" on the upgrade prompts for a couple weeks, since it seems like there's a story like this with every big release.
I must have used the shuffle screensaver. Who knew a ‘harmless’ feature could quietly eat up ~50 GB of space and bandwidth?
I mean, how else did you expect it to play a random assortment of high quality videos as a screen saver? It's good these aren't bundled, and very few people have metered internet.
Not on my machine with only one files about 100MB. Still Why on earth do they do that?I keep thinking if Apple is deliberately doing it. From Safari constantly writing 100GBs per day on paging, to this. It is the small things Apple no longer cared about which is worrying in terms of company culture.
It could either be a bug, or they clicked on every screensaver in System Settings to download them years ago, or screensaver was set to Random.
You know, if you respond to every valid OS complaint like that, eventually you will be the problem. It happened with Microsoft, happened to Google, and sure as shit can happen to the "you're holding it wrong" company.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New research reveals longevity gains slowing, life expectancy of 100 unlikely]]></title>
            <link>https://lafollette.wisc.edu/news/new-research-reveals-longevity-gains-slowing-life-expectancy-of-100-unlikely/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075813</guid>
        </item>
        <item>
            <title><![CDATA[The V Programming Language]]></title>
            <link>https://vlang.io/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075571</guid>
            <content:encoded><![CDATA[

  
            

    
         
            Jun 19, 2025
         
         
    

    
         
            Mar 20, 2025
         
         
    



    
         
            Dec 22, 2024
         
         
    

    
	  
		  Sep 28, 2024
	  
	  
	

    
	  
		  Jul 26, 2024
	  
	  
	

    
	  
		  May 20, 2024
	  
	  
	

    
	  
		  Mar 20, 2024
	  
	  
	

    
	  
		  Jan 9, 2024
	  
	  
	

    
	  
		  Nov 11, 2023
	  
	  
	

    
	  
		  Sep 30, 2023
	  
	  
	

    
	  
		  Sep 3, 2023
	  
	  
	


    
	  
		  July 1, 2023
	  
	  
	

    
	  
		  June 29, 2023
	  
	  
	

    
	  
		  April 30, 2023
	  
	  
	

    
	  
		  April 17, 2023
	  
	  
	

    
	  
		  January 30, 2023
	  
	  
	

    
	  
		  October 31, 2022
	  
	  
	

    
	  
		  August 31, 2022
	  
	  
	

    
	  
		  June 30, 2022
	  
	  
	

    
	  
		  June 22, 2022
	  
	  
	
    
	  
		  June 10, 2022
	  
	  
	 

    
	  
		  June 9, 2022
	  
	  
	   As of today, programs built with the V compiler no longer leak memory by default.
	  
	


    
	  
		  May 29, 2022
	  
	  
		7000 pull requests have been merged!
		
	

	  


    
	



  
            

    
The V development team does an amazing job. I've never seen a language evolve that fast; I suspect you guys never sleep. I hope V will remain a simple, clean language and have a bright future. Thanks for all your hard work.

	  
	

    
	I'm mostly surprised by how many things "just work".
Channels and closures made implementing asynchronous callbacks for C functions such a breeze.
Thanks for that! 😄
		
	  
	

    
		V is the most comfortable syntax I've encountered, so I look forward to 1.0.


	  
	

    
Been programming for around 30 years. Have done some C/C++, VB, and lots of Delphi years ago. Then PHP/Ruby for over 15 years. Bash, Python too. Recently I've wanted to start using a compiled language for reasons, and have looked at Rust, Go, and a couple others. I stumbled upon V a few weeks ago and have been very surprised by how easy it is to pick up. I am really enjoying writing V and feel like I am already productive. Even to the point I'm now looking at making a small ... 
		
	  
	

    
	Really like the direction of V, been wondering when this kind of language would pop up. 
	  
	

    
	V is amazing.
Things I like about it:

Flexibility for operator overloading. (it makes syntax intuitive)

Compiles directly to C, and I love C

Flexibility for having and not having garbage collection

Go concurrency model

	  
	

    
	I come from a strong Java and Go background and have been playing with Rust.
V looks pretty amazing in terms of readability.
	  
	

    
	V is amazingly simple!
	  
	

    
	I buy my son's diaper with C#, but every project I start today is in V.
Because every project in C# has an initial load of bureaucracy that in V I don't have to deal with at the beginning, and I can plug bureaucracy into the project throughout the creation process.
	  
	


    
	Gotta say that V's syntax is amazing (especially the error handling aspect for me since I strongly dislike Go's error handling approach but liked its other aspects).
	  
	

    
	 After V, developing a kernel in C results in more bugs, honestly more so because V catches a set of bugs and memory leakage that C simply doesn't.
	  
	  
	    mint, maintainer of Vinix
	  
	


    
	Just want to say thank you for continuing to improve the language. Adding the GC was very nice and I find V my all-time favorite language. Using V is fun due to great syntax and tooling but also makes you a better developer as you try to understand how things work underneath.
	  
	


    
	Just wanted to say thank you to everyone contributing to V! Before September 2023 I never really coded nor completed any project as the tools I had were less than ideal. In September I started learning V and now I have a really good prototype and a professional developer even reached out to me saying my code was really good! That's why I wanted to thank V and all the contributors 🥰
	  
	

    
	It's obvious that Alex was able to bring his experience of many other languages to V, giving the language a very solid foundation.
It's difficult for me to list all the positive qualities I like about V because I'm sure I'll forget some. I'll try it anyway.

compilation speed
simplicity (only a few keywords), yet powerful
readability & maintainability
execution performance
cross compilation
...

	  
	

    
	Thanks to everyone who contributed to this project. The simplicity of language and the fact that it's compiled makes it number 1 where applicable.
	  
	



I think you do not get enough credit for the amazing language V is. Thank you for your time and effort.
	  
	



When I stumbled across V, it just 'felt right'.
	  
	


Thank you very much to everyone contributing to V! It's been 1.5 years since I've discovered it. I really enjoy the kind and helping community, the simplicity philosophy and lots of other things.
I was pleasantly surprised by the easiness of creating an HTTP server: just a few lines and then handling the request is very easy by following the example.
A big thank you from the bottom of my heart ❤️
	  
	



V is the better Go.

I use V in production for my personal projects. It is such a joy to use.
	  
	


When I stumbled across V, it just 'felt right'.

	  
	  



I mastered Python when I worked as a Data Scientist a few years ago. Since I discovered V I stopped using Python almost entirely, and now I look back and... it's  awful. Never had so much fun programming as I am now.
	  
	


V is seriously impressive. After years of Go development, V brings the simplicity I love, but with true low-level control and raw speed — exactly what I’ve been missing. Looking forward digging deeper with this tech, just started ❤️
	  
	




	  


    
	


  
    
		
Simple language for building  maintainable programs 
		 
		You can learn the entire language by going through the 
		documentation over a weekend, and in most cases there's only one way to do something.
		
		This results in simple, readable, and maintainable code.
		 
		Despite being simple, V gives a lot of power to the developer and can be used in pretty much every field, including systems programming, webdev, gamedev, GUI, mobile, science, embedded, tooling, etc.
		
		
		V is very similar to Go. If you know Go, you already know ≈80% of V.
		Things V improves on Go: vlang.io/compare#go.
		
    
    
      Safety
      
        Bounds checking
        No undefined values
        No variable shadowing
        Immutable variables by default
        Immutable structs by default
        Option/Result and mandatory error checks
        Sum types
        Generics
        Immutable function args by default, mutable args have to be marked on call
        No null (allowed in unsafe code)
        No undefined behavior new!
        No global variables (can be enabled for low level apps like kernels via a flag)
      
    
  


  

  
    
      Performance
      
        C interop without any costs
        Minimal amount of allocations
        Built-in serialization without runtime reflection
        Compiles to native binaries without any dependencies: a simple web server is only about 250 KB
        As fast as C (V's main backend compiles to human readable C),
		with equivalent code.
		V does introduce some overhead for safety (such as array bounds checking, GC free), but these features can be disabled/bypassed when performance is more important.
		
      
    
    
      Fast compilation
      
        V compiles ≈80k (Clang backend) and ≈400k (x64 and tcc backends) lines of code per second. (Apple M2, no optimization)
        V is written in V and compiles itself in under a second (0.33s on MacBook Air M3). 
      
	  
	    Most of the compiler is still single threaded, so it's going to be 2-3x faster in the future!
	  
    
  




	  



  
    
      Small and easy to build compiler
      
      V can be bootstrapped in under a second by compiling its code translated to C with a simple cc v.cNo libraries or dependencies needed.
      
      For comparison, space and time required to build each compiler:
      
      
      Space  Build time
      Go525 MB1m 33s
      Rust30 GB45m
      GCC8 GB50m
      Clang90 GB
      [0]
      60m
      Swift70 GB
      [1]
      90m
      V< 20 MB
      [2]
	  <1s
      
    
    
      Building V in 0.3 seconds and then using the resulting binary to build itself again:
		
		 
		
	  Try it yourself:
	  
	  
	  wget https://github.com/vlang/v/releases/latest/download/v_linux.zip 
	  unzip v_linux && cd v 
	  time ./v self
	  
    
  



  
    
      Flexible memory management
	  
	  	V avoids doing unnecessary allocations in the first place
		by using value types, string buffers, promoting a simple abstraction-free code style.
	  
	  
	    There are 4 ways to manage memory in V.
	  
	  
	    The default is a minimal and a well performing tracing GC.
		
	  
	   The second way is autofree, it can be enabled with -autofree. It takes care of most objects (~90-100%):
	  the compiler inserts necessary free calls automatically during compilation. 
	    Remaining small percentage of objects is freed via GC.
	    The developer doesn't need to change anything in their code. "It just works",
		like in Python, Go, or Java, except there's no heavy GC tracing everything
		or expensive RC for each object. Autofree is still experimental and not production ready yet. That's planned for V 1.0.
	  
	  
	    For developers willing to have more low level control, memory can be managed manually
		
		with -gc none.
	  
	  
	  
	    Arena allocation is available via v -prealloc.
	  



	
    
	
	V's autofree demo. All objects are freed during compilation. Running the Ved editor on an 8 MB file with 0 leaks:
	

	
	 
	
	 
	
  

  
    
      C translation
      V can translate your entire C project and offer you the safety, simplicity, and compilation speed-up (via modules). 
      
        v translate file.c

std::vector s;
s.push_back("V is ");
s.push_back("awesome");
std::cout << s.size();

mut s := []
s << 'V is '
s << 'awesome'
println(s.len)
      
      
        A blog post/tutorial about translating DOOM is available.
        C++ to V translation is at an early stage.
    
    
      
	  Translating DOOM from C to V and building it in under a second:
	  
    
You can read translated code here:
github.com/vlang/doom

    
  

  
      Hot code reloading
      
        Get your changes instantly without recompiling.
        
        Since you also don't have to get to the state you are working on after every compilation, this can save a lot of precious minutes of your development time.
      
      github.com/.../examples/hot_reload
    

  
    
      A powerful graphics library
      
        Cross-platform drawing library gg, using OpenGL/Metal/DirectX 11 for rendering 2D applications.
	  
	    There's also a 3D engine in development with the following features already available:
      
      
        Loading complex 3D objects with textures
        Camera (moving, looking around)
        Skeletal animation
      
      A simple example of the graphics library in action is tetris.v.
	  For 3D examples, check out this.
    
    
    
  

  
    
      Light and fast cross-platform GUI library
      
        Build native UI apps with V UI. You no longer need to embed a browser to develop cross-platform apps quickly.
        V has a UI module that uses custom drawing, similar to Qt and Flutter, but with as much similarity to the native GUI toolkit as possible.
	  
	  
        It has a declarative API similar to SwiftUI and React Native and runs on Windows, Linux, macOS, and Android.
      
	  
        Coming soon:
      
      
        a Delphi-like visual editor for building native GUI apps
        iOS support
      
      github.com/vlang/ui
    
    
      
        Volt, a 3 MB Slack/Discord client built with V and V UI:
      
      
    
  

  
    
      Easy cross compilation
      
        To cross compile your software simply run v -os windows or v -os linux. No extra steps required, even for GUI and graphical apps!
        (Compiling macOS software only works on macOS for now.)
      
    
    
      
        Building V for Windows using V for macOS, and then testing resulting v.exe on a Windows VM:
      
      
    
  

  
    
      Painless deployments and dependency management
      
        To build your project, no matter how big, all you need to do is run 
		v .

        No build environments, makefiles, headers, virtual environments, etc.
        You get a single binary that is guaranteed to work on all operating systems (provided you cross compile) without any external dependencies.
		This binary is self-contained except for the OS-provided system library (libc), which is linked dynamically.
		To achieve fully static binaries on Linux, you can use v -cc musl-gcc .

        Installing new libraries via vpm, a centralized package manager written in V, is as simple as
		v install ui
		
      
    
    
      Run everywhere
      
        V can emit (human readable) C, so you get the great platform support and optimization of GCC and Clang. (Use v -prod . to make optimized builds.)
        Emitting C will always be an option, even after direct machine code generation matures.
        V can call C code, and calling V code is possible in any language that has C interop.
      
    
  

  
    
      REPL
      v
>>> import net.http
>>> data := http.get('https://vlang.io/utc_now')!
>>> data.body
1565977541
    
    
      Cross-platform shell scripts in V
	  
	  V can be used as an alternative to Bash to write deployment scripts, build scripts, etc.

The advantage of using V for this is the simplicity and predictability of the language, and cross-platform support. "V scripts" run on Unix-like systems as well as on Windows.

      for file in ls('build/') {
  rm(file)
}
mv('v.exe', 'build/')

v run deploy.vsh
      Read more about V script
    
  

  
    
      Code formatting with vfmt for consistent style
      
      No more arguments about coding styles. There's one official coding style
      enforced by the vfmt formatter.
      
      
      All V code bases are guaranteed to use
      the same style, making it easier to read and change code written by
  	  other developers.
      
      v fmt -w hello.v
      
      
        A built-in code profiler
      
      Build and run your program with v -profile profile.txt x.v && ./x and
      you'll get a detailed list for all function calls: number of calls, average time per
      call, total time per call.
      
    
  

  
    
      JavaScript and WASM backends
      
      V programs can be translated to JavaScript (WIP):
      
      v -o hello.js hello.v
	  The JS backend is at an early stage.
      
	  They can also be compiled to WASM (for now with Emscripten, but native WASM support is planned).
      V compiler compiled to WASM and running V programs by translating them to JavaScript:
      
      
      v-wasm.vercel.app
      
	  A game written using V's graphical backend and compiled to WASM:
	  v2048

      
      
    Automatic documentation
    
    Use vdoc to get instant documentation generated directly from 
    the module's source code. No need to keep and update separate documentation.
    
    v doc os
	


  
    
      Built-in testing framework
	  
	  Writing tests is very easy: just start your test function with test_
	  fn get_string() string { return 'hello' }

fn test_get_string() {
  assert get_string() == 'hello'
}
      

      
    Friendly error messages
    
	Helpful error messages make learning the language and fixing errors simpler:

    
    user.v:8:14: error: `update_user` parameter `user` is mutable, you need to provide `mut`: `update_user(mut user)`

    7 |     mut user := User{}
    8 |     update_user(user)
      |                 ~~~~
    9 | }
	
	



  
    Powerful built-in web framework Veb
	Veb is very fast (built on top of pico.v which has been at the top of TechEmpower benchmark), it compiles into a single binary (html templates are also compiled), supports hot code reloading
	(the website is automatically updated in the browser once you change any .v/.html file).
    github.com/vlang/v/tree/master/vlib/veb

    
      

        ['/post/:id']
fn (b &Blog) show_post(id int) veb.Result {
  post := b.posts_repo.retrieve(id) or {
    return veb.not_found()
  }
  return veb.view(post)
}



      Gitly, a light and fast alternative to GitHub/GitLab is built in V and Veb.
    

    
	



      Built-in ORM

      

  

  import db.sqlite

struct Customer {
  id int
  name string
  nr_orders int
  country string
}

fn main() {
  db := sqlite.connect('example.sqlite') or {
    panic('could not create/find example.sqlite')
  }

  nr_customers := sql db {
    select count from Customer
  }!
  println('number of customers: ${nr_customers}')

  
  uk_customers := sql db {
    select from Customer where country == 'uk' && nr_orders > 0
  }!

  for customer in uk_customers {
    println('${customer.id} - ${customer.name}')
  }

  
  
  customer := sql db {
    select from Customer where id == 1 limit 1
  }!
  println(customer.name)

  
  new_customer := Customer{name: 'Bob', nr_orders: 10}
  sql db {
    insert new_customer into Customer
  }!
}





]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New interpretations suggest the "heat death" hypothesis might not hold (2023)]]></title>
            <link>https://www.noemamag.com/life-need-not-ever-end/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075430</guid>
            <description><![CDATA[New interpretations of the laws of thermodynamics suggest the infamous “heat death” hypothesis, which foretells the end of all life and organization in the universe, might not hold.]]></description>
            <content:encoded><![CDATA[

        
    Credits
    Bobby Azarian is a cognitive neuroscientist, a science journalist and the author of the book “The Romance of Reality: How the Universe Organizes Itself to Create Life, Consciousness and Cosmic Complexity.”

Perhaps the most depressing scientific idea that has ever been put forth is the infamous “heat death hypothesis.” It is a theory about the future of the universe based on the second law of thermodynamics, which in its most well-known form states that entropy, a complicated and confusing term commonly understood to simply mean “disorder,” tends to increase over time in a closed system. Therefore, if we consider that the universe is itself a closed system, the law seems to suggest that the cosmos is becoming increasingly disorganized. It has also been described by many as “winding down.”  As such, the second law appears to hold a chilling prophecy for humanity in the very long term. Essentially, it would seem to imply that life is doomed — not just life on Earth, but life anywhere in the cosmos. Consciousness, creativity, love — all of these things are destined to disappear as the universe becomes increasingly disordered and dissolves into entropy. Life would merely be a transient statistical fluctuation, one that will fade away, along with all dreams of our existence having some kind of eternal meaning, purpose or permanence. This bleak idea is known as the “heat death hypothesis,” and the prophecy foretells a future where all pattern and organization has ceased to be. In this cosmological model, everything must come to an end. There is simply no possibility for continual existence. Fortunately, the gloomiest theory of all time may just be a speculative assumption based on a misunderstanding of the second law of thermodynamics. For one thing, the law may not be applicable to the universe as a whole, because the types of systems on which it has been empirically tested have well-defined boundaries. The expanding universe does not. Secondly, depending on how one interprets the second law, the inevitable increase in entropy may not correspond to an increase in cosmic disorder. In fact, some leading scientists are beginning to think that the cosmos is becoming increasingly complex and organized over time as a result of the laws of physics and the evolutionary dynamics that emerge from them. Seth Lloyd, Eric Chaisson and Freeman Dyson are among the well-known names who have questioned whether “disorder” is increasing in the cosmos. Outside of physics, complexity theorist Stuart Kauffman, neuroscientist Christof Koch and Google’s director of engineering Ray Kurzweil all believe that the universe is not destined to grow more disorganized forever, but more complex and rich with information. Many of them have a computational view of the universe, in which life plays a special role.As Paul Davies, a prolific author and a highly respected theoretical physicist, wrote: “We now see how it is possible for the universe to increase both organization and entropy at the same time. The optimistic and pessimistic arrows of time can coexist: The universe can display creative unidirectional progress even in the face of the second law.” In other words, if we understand the second law better, we can see that it does not actually prohibit the continual growth of complexity and order in nature.



  

    
      “Essentially, the heat death hypothesis seems to imply that life is doomed — not just life on Earth, but life anywhere in the cosmos.”    

    
    
  
This is the cosmic narrative that the theoretical physicist and author Julian Barbour proposes in his new book “The Janus Point: A New Theory of Time,” which has received praise by some trusted names in the physics world, such as Martin Rees, Sean Carroll and Lee Smolin. Barbour believes that the second law — at least as it is popularly interpreted — does not apply to the universe as a whole, since it is always expanding due to the mysterious force known as dark energy. The old story of increasing cosmic disorder, Barbour concludes, may turn out to be the complete opposite of what is actually happening. Because the universe is not a bounded system, order can continue to increase indefinitely.Barbour is not alone. David Deutsch, the father of quantum computation, has expressed a similar view in his bestselling mindbender “The Beginning of Infinity,” in which he argues that there are no fundamental limits to knowledge creation. This is a much stronger claim than Barbour’s, because it specifically suggests that life in the universe need not come to an end. Life is a crucial part of the cosmic story because the growth of complexity and organization enters a new phase when biology emerges. Life is a special form of complexity: It has the ability to create more complexity and to maintain organization against the tendency toward disorder. In a universe expanding without limit, the ability of intelligent life to continually construct complex order may not be limited by the laws of thermodynamics in the way once imagined.This story of continual complexification would seem to go against the second law, a rock-solid pillar of physics. Remember, though, that both the first and second laws of thermodynamics were conceived before we knew the universe was expanding. To understand if these laws are applicable to the universe as a whole — and not just systems inside the universe — we must briefly explore the history of thermodynamics and understand its relationship with the phenomenon we call life. DoomIn the two fields of thermodynamics — classical and statistical — there are subtly different versions of the second law. The former emerged about a half a century before the latter, and it was concerned with the flow of heat and energy. Statistical thermodynamics attempted to explain the findings of classical thermodynamics in terms of the behavior of ensembles of molecules and atoms, and it was more concerned with how configurations of particles evolve over time.You could say that the original version of the second law, the classical version, was about the spreading out of energy, where the statistical version was more about ordered configurations of particles becoming more disordered. While the two versions are intimately related and in many instances become equivalent, they do not have the same cosmic implications. The ideas that would become the second law can be traced back to the work of the French engineer Sadi Carnot in the early 1800s. Carnot wanted to understand how to make steam engines more efficient by analyzing how they used energy. He recognized that heat would spontaneously flow from hotter to colder systems, but never in reverse. We all experience this phenomenon on a daily basis, whenever a hot bath or cup of coffee inevitably cools to room temperature as heat is lost to the surrounding air. Carnot pointed out that this flow of heat creates a motive force, which can be harnessed to power machines. Through a cycle of heating and cooling steam inside a chamber (known as a cylinder) with a movable wall on one side (known as a piston), you can create a force of motion that can power an engine. What Carnot astutely noticed about this process was that it couldn’t be made 100% efficient. This is the basis for the original second law of thermodynamics. The conversion of thermal energy into mechanical energy always involves the loss of some useful energy to the environment in the form of heat. Once this useful energy is dissipated, meaning it gets spread and lost to its surroundings, it can no longer be harnessed to do physical work. The lost energy still technically exists somewhere out there in the universe, but it can’t be extracted to do anything useful, like sustaining an engine or some other machine. Since life is a machine of sorts, this has implications for how long it can persist in the universe.  Because Carnot was an engineer, his insights were largely unknown or ignored by the physics community for decades, until two giants in the field — Lord Kelvin and Rudolph Clausius — explained their significance and relevance to the emerging science of thermodynamics. The new field proposed two major laws that, when put together, seem to have cosmic implications. The first law says that energy is conserved. That means it cannot be created or destroyed — implying that the total amount is fixed — though it can be transformed from one form to another. The second essentially says that there is “free energy” — or energy available to do work — but as that energy is used for mechanical work, some of it inevitably gets dissipated as it is converted into heat, a form of energy that is no longer useful. Once energy is dispersed in this way, it becomes impossible to be used to do mechanical work, like creating a force that could power a system.In 1852, Lord Kelvin wrote a paper with what is considered to be the first statement of the second law, which he described as a universal tendency toward the dissipation of mechanical energy. The term “entropy,” introduced by Clausius in 1865, was originally defined as a measure of the energy in a system that is no longer available for work. Entropy, then, referred to dissipated energy, not structural disorder. 



  

    
      “The conversion of thermal energy into mechanical energy always involves the loss of some useful energy to the environment in the form of heat.”    

    
    
  
Essentially, these discoveries suggested that a limited supply of free energy was always spreading out and dissipating, so there would come a time when no further mechanical work could be done, including the work required to sustain the biological machinery that we call “life.” One by one, the stars that supply the energy that powers biology would radiate away their usable energy, and life would cease to be. This sad story isn’t just local; all the stars throughout the cosmos will eventually burn out, causing any biosphere, anywhere, to degrade. Even if some form of life could develop the technology to explore the cosmos, eventually all useful energy in the universe would be converted into heat, leaving no energetic fuel for advanced forms of sentience to consume.  At least, that was the assumption in the second half of the 19th century. This scenario became known as the “heat death” of the universe, and it seemed to be the nail in the coffin for any optimistic cosmology that promised, or even allowed, eternal life and consciousness. For example, one of the most popular cosmological models of the time was put forth by the evolutionary theorist Herbert Spencer, a contemporary of Charles Darwin who was actually more famous than him during their time. Spencer believed that the flow of energy through the universe was organizing it. He argued that biological evolution was just part of a larger process of cosmic evolution, and that life and human civilization were the current products of a process of continual cosmic complexification, which would ultimately lead to a state of maximal complexity, integration and balance among all things. When the prominent Irish physicist John Tyndall told Spencer about the heat death hypothesis in a letter in 1858,” Spencer wrote him back to say it left him “staggered”: “Indeed, not seeing my way out of the conclusion, I remember being out of spirits for some days afterwards. I still feel unsettled about the matter.”Things got even gloomier when the Austrian physicist Ludwig Boltzmann put forward a new statistical interpretation of the second law in the latter half of the 19th century. That was when the idea that the universe is growing more disordered came into the picture. Boltzmann took the classical version of the second law — that useful energy inevitably dissipates — and tried to give it a statistical explanation on the level of molecules colliding and spreading out. He used one of the simplest models possible: a gas confined to a box. How does the evolution of a gas in a box explain the dissipation of useful energy? First, it should be understood that a gas is a collection of molecules moving around rapidly and chaotically, particles that Boltzmann assumed were like little billiard balls following fixed trajectories. Since the great Scottish physicist James Clerk Maxwell had recently shown that the kinetic energy of a molecule is determined by how fast it is moving, Boltzmann assumed the dissipation of usable energy described by Lord Kelvin was caused by pockets of excited molecular motion spreading out in space due to random collisions between neighboring molecules.  For example, if a pocket of highly excited gas molecules starts out in some orderly configuration — let’s say the molecules are bunched together in one corner of the box — over time, the ensemble of particles will evolve to become increasingly spread out, or “disordered.” When an ordered pocket of excited molecular motion exists, there is an energy gradient in the system and the potential to do some work, but as these molecules interact with their neighbors and that excited motion gets dispersed, the gradient disappears. This dissipation of molecular order and free energy continues until the gas approaches a state of maximum entropy and disorder known as thermodynamic equilibrium. Paradoxically, this state of “total disorder” looks like a uniform distribution of gas molecules. The gas molecules spread out in this way due to a simple statistical reason: There are many more ways for the gas molecules to be arranged in a disordered mess than in some orderly configuration. In other words, an orderly arrangement of particles moving around randomly will naturally become more disorganized. Just like in pool, where the balls start off in an ordered formation but spread out and mix up as collisions occur.  
          
        Boltzmann, like Clausius and Kelvin before him, tried to apply his version of the second law to the entire universe — which, he assumed, must be a giant closed system of atoms and molecules bouncing around chaotically, not all that different from his gas in a box. According to his version of the second law of thermodynamics, the entire universe — as a system composed of atoms moving according to physical laws — must eventually tend toward a more disordered and random configuration, just like his box of gas molecules. To explain why there was so much complexity and order in the universe around him, he suggested that the universe must have started out in an extremely ordered state that had since evolved into what we see today, or that the ordered state of affairs we see in our neck of the cosmic woods was the result of a temporary statistical fluctuation away from the general trend toward disorder. 



  

    
      “The universe can grow increasingly organized through the spread of intelligent life, as long as it can find the free energy it needs to build and maintain the cosmic organization it constructs.”    

    
    
  
Of course, there were many problems with comparing Boltzmann’s gas-in-a-box model to the universe. The order-to-disorder transition only occurs when the particles in the system do not become statistically correlated with each other over time. Boltzmann’s H-theorem, which the idea of a natural tendency toward disorder is based on, assumes “molecular chaos.” But molecular and chemical forces often cause atoms and molecules to clump together into larger, more complex structures — meaning a gas evolving in a box is not an accurate representation of all the dynamics in nature. Boltzmann’s model also ignored the influence of gravity, which is often described as an anti-entropic force due to its clumping effects on matter. Gravity’s effects on small objects like gas molecules are essentially so tiny that they are negligible for all practical purposes, meaning you can leave the force out of the model and still make accurate predictions about the state of the system. But at the scale of the universe, the effects of gravity become extremely important to the evolving structure of the system. Gravity is one factor driving the growth of order in the cosmos, and a good example of why the evolution of the universe looks very different from a gas spreading out in a box.Of course, the attractive force of gravity doesn’t explain the emergence of life, which has been defying Boltzmann’s tendency toward disorder for about four billion years. Not only does life represent the formation of complexity, it constructs more of it. What explains this paradox? How does the biosphere grow more complex and organized if there’s a tendency for organized systems to fall apart? If cosmic complexity is to grow continuously, the process would then seem to curiously depend on life, the only form of complexity that can create more organization and actively sustain itself.The quantum physicist Erwin Schrodinger explained this paradox in his 1944 book “What is Life?”. What Schrödinger noticed was that instead of drifting toward thermodynamic equilibrium — which for life means a state of death and decay — biological organisms maintained their ordered living state by consuming free energy from the environment (which he called “negative entropy”). Boltzmann’s law of increasing disorder only applies to closed systems, and life on Earth is an open system. It is constantly receiving usable energy from the sun, which drives it away from thermodynamic equilibrium.  Of course, without a steady supply of incoming energy, equilibrium ensues and life perishes. But by feasting on the free energy in the environment, ordered systems can pay the physical price of staying organized and functional, just like burning more coal will allow a steam engine to continue to function. The cost is the dissipation of free energy and the production of thermal entropy, in the form of heat, which is constantly being released into the environment. Therefore, the continual growth of complexity in the form of biological and technological organization — in other words, the biosphere and the layer of industry and technology that sits on top of it — does not violate the classical version of the second law of thermodynamics. Because the biosphere is an open system that is continually getting energy from the sun, it can continuously build and maintain order. Local reductions in configurational entropy (disorder) are paid for by the simultaneous increase in thermal entropy (heat) caused by life’s constant use of free energy. As long as free energy continues to be used and dispersed, the total amount of entropy in the universe increases, and the classical version of the second law remains intact. However, it is important to note that the production of heat is not the same as the creation of structural disorder. Energy gets more dispersed as the universe organizes itself, and that is all the second law requires in this context. One could say that energetic disorder increases as structural order grows.What this means is that the universe can grow increasingly organized through the spread of intelligent life, as long as it can find the free energy it needs to build and maintain the cosmic organization it constructs. Luckily, the universe offers a vast ocean of exploitable energy to beings that are intelligent enough to know how to extract it. In theory, a hyperintelligent civilization could spread through the cosmos, transforming all the matter in its midst into exotic forms of biological and computational machinery. This scenario might be hard to visualize, but it would not be very different from how life went from existing at just a single point on the Earth, not even visible with the naked eye, to covering the entire planet.But how long could this go on for? The great science fiction writer Isaac Asimov called that “The Last Question” in a critically acclaimed short story about the fate of life in the universe. The story questions the prevailing view of the second law’s applicability to the entire universe, an assumption made by a series of characters in the story: “However it may be husbanded, however stretched out, the energy once expended is gone and cannot be restored. Entropy must increase to the maximum.” Asimov’s skepticism may have been one of his most prescient insights. In his 1964 biographical sketch of Clausius, Asimov called the heat death hypothesis the “scientific analog of the Last Judgement” and notes that “its validity is less certain now than it was a century ago. Though the laws of thermodynamics stand as firmly as ever, cosmologists are far less certain that the laws, as deduced in this small segment of the universe, necessarily apply to the universe as a whole and there is a certain willingness to suspend judgment on the matter of the heat-death.”The ExpanseIn the 1960s, the Harvard cosmologist David Layzer pointed out that although the entropy of the universe will continue to increase in accord with the second law of thermodynamics — that is, an expanding intelligence will always be converting more free energy into thermal entropy — the maximum possible entropy of the expanding universe will presumably increase at a faster rate than the actual entropy increase, allowing for the continual growth of order and complexity. He called this an “entropy gap” — the difference between the universe’s actual entropy and its maximum possible entropy. As long as that gap exists, the universe will not be in thermodynamic equilibrium, and that means there will be energy gradients that life can extract work from. Now we know the universe is not just expanding, which Edwin Hubble confirmed in 1929, but that the expansion is accelerating at an increasing rate due to the mysterious force known as “dark energy,” the presence of which was theorized before the turn of the millennium. These developments give us reason to believe that the entropy gap will persist into the future, such that the universe may never come to the state of equilibrium predicted by the heat death hypothesis. In his 2016 book “Humanity in a Creative Universe,” the complexity theorist Stuart Kauffman explained the significance of this: “[W]e do not have to worry about enough free energy. As the universe becomes larger, its maximum entropy increases faster than the loss of free energy by the second law, so there is always more than enough free energy to do work.” But where does this seemingly unlimited free energy come from, if the first law of thermodynamics suggests that nature has a fixed and finite amount? Well, it turns out that first law of thermodynamics may also not apply to the universe as a whole, as was assumed, even though conservation of energy applies to systems within the universe. Challenges to our traditional notion of the first law are not uncommon in modern physics. For example, cosmic inflation theory — the leading cosmological model for how the universe became filled with all its energy and matter — proposes that during the early period of expansion, miniscule fractions of a second after the Big Bang, new matter and energy was being continuously created from nothing. In fact, the theory of cosmic inflation suggests more and more universes are being created, so in the totality of reality envisioned by this model, matter creation never ends.The only way cosmic inflation theory can coexist with the first law is if we divide all the energy in the world into two opposing categories of energy: positive and negative. The so-called “positive energy” associated with new matter is balanced out by the “negative energy” of the gravitational force associated with that matter. According to this model, the sum total of energy of the universe is zero. It may seem like a desperate attempt by cosmologists to salvage the first law, but it works out mathematically. For this reason, Alan Guth calls the universe “the ultimate free lunch.” In principle, new energy can be continuously created, as long as the ratio of positive to negative energy remains balanced. While the implications of this concept are foggy, it is clear that applying the first and second laws of thermodynamics to the cosmos as a whole can get very tricky. 



  

    
      “These new developments give us reason to believe that the entropy gap will persist into the future, such that the universe may never come to the state of equilibrium predicted by the heat death hypothesis.”    

    
    
  
Deutsch speculates over whether life could harness dark energy directly itself to power computation forever in his 2011 book “The Beginning of Infinity”: “Depending on what dark energy turns out to be, it may well be possible to harness it in the distant future, to provide energy for knowledge-creation to continue forever.” Some physicists have since argued that in theory, it is possible that dark energy could be used as a power source. A conference paper published by the American Astronomical Society proposes that “simple machines could, in theory, extract local power from the gravitationally repulsive cosmological constant,” even if “the amount of energy that could be liberated in a local setting is many orders of magnitude too small to be useful or even detectable.”Whatever dark energy turns out to be, the cosmic expansion it is driving serves to keep the universe out of thermodynamic equilibrium, and a system not in equilibrium is a system that still has some energy and the capacity to do work.At his blog Preposterous Universe, Sean Carroll writes: “If there exists a maximal entropy (thermal equilibrium) state, and the universe is eternal, it’s hard to see why we aren’t in such an equilibrium state — and that would be static, not constantly evolving. This is why I personally believe that there is no such equilibrium state, and that the universe evolves because it can always evolve.”If there’s no inevitable equilibrium state, then there seems to be no reason to assume that an evolving intelligence must necessarily come to an end. In his 2006 book “Programming the Universe,” MIT’s Seth Lloyd speculates along these lines: “By scavenging farther and farther afield, our descendants will collect more and more matter and extract its energy. Some fraction of this energy will inevitability be wasted or lost in transmission. Some cosmological models allow the continued collection of energy ad infinitum, but others do not.”While some cosmologists believe dark energy and the accelerating expansion will ultimately dilute the matter and energy in the universe to such a degree that life must come to an end, a popular new theory known as quintessence suggests that the accelerating expansion may begin to slow, creating even more uncertainty around any predictions for life’s future. Perhaps the dynamics of the universe’s expansion are what they need to be to allow for the continual growth of cosmic complexity? In a 2020 Nature article about quintessence, Carroll is quoted saying, “We’re back to a situation where we have zero idea about how the universe is going to end.” If Isaac Asimov were alive today, I believe he would be delighted to know that his “last question” is still open. The increase in entropy in the universe is not equivalent to increasing cosmic disorganization. Complexity and entropy can grow together, and perhaps even without limit. I like to believe that this means that the universe is on our side.Correction: An earlier version of this essay incorrectly stated that the Hubble telescope confirmed that the universe was expanding. It was Edwin Hubble the person, not the telescope that was named after him.
          
        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Have to Feel It]]></title>
            <link>https://mitchellh.com/writing/feel-it</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075048</guid>
            <description><![CDATA[You see a series of checkboxes checked. Schedule met.
Documented requirements satisfied. Demo video delivered.
It's a good day. Good job, you, good job! A promotion is in sight.]]></description>
            <content:encoded><![CDATA[You see a series of checkboxes checked. Schedule met.
Documented requirements satisfied. Demo video delivered.
It's a good day. Good job, you, good job! A promotion is in sight.
But you didn't feel it. You didn't feel it.
We, as people, feel something with every interaction. Frustration, joy, relief,
confidence. A feeling. A person interacts with our work. Our work evokes
a feeling. The feeling matters. The feeling is part of the work. The
desired feeling is part of the requirements.
When you feel it, you know. The feature makes you smile when you use it.
It fits right in, like it was always meant to be there. You want to
use it again. You want to tell people about it.
This is the difference. This is what metrics, specifications, and demos
miss. They don't capture the feeling. For the people who will use and live
in the work, the feeling is part of their daily experience. Which means
you can't stop at checking the boxes on paper. You have to sit with it,
use it, live with it.
You have to feel it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Condor's Cuzco RISC-V Core at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074895</guid>
            <description><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive.]]></description>
            <content:encoded><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache missesIf you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI models need a virtual machine]]></title>
            <link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074467</guid>
        </item>
        <item>
            <title><![CDATA[Bcachefs Goes to "Externally Maintained"]]></title>
            <link>https://lwn.net/Articles/1035736/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074312</guid>
            <description><![CDATA[Linus Torvalds has quietly changed the maintainer status of bcachefs to 'externally maintained' [...]]]></description>
            <content:encoded><![CDATA[
Yeah, bcachefs is not in the same boat as ZFS. There are no licensing issues, and as far as I can tell, no technical issues either. It's just a case of "Linus and some other folks don't want to work with Kent," and that is (presumably) solved by somebody else upstreaming the patches.
But there are a number of unstated assumptions here. The most important problem is how this upstreaming process will work. I can think of a few different alternatives, but the most straightforward option is for Kent to designate somebody. That person would then be responsible for all interaction with the kernel process, including sending emails, responding to code reviews, changing the code as requested (or telling Kent to do so and relaying his responses), etc. This strikes me as a highly difficult and thankless job that I certainly would not want to do. You could easily end up recreating exactly the same argument that Kent was regularly getting into (over release schedules, merge windows, etc.), but by proxy instead of directly. Ideally, Kent stops caring about the kernel's release processes altogether, and takes a mindset of "the kernel is [designee's] problem, and I don't have to deal with it aside from applying a few patches every now and then."

Most of the alternatives are worse. If we instead suppose that the kernel takes bcachefs code without Kent's explicit approval or involvement, then the kernel upstream is a de facto fork of bcachefs (or they're just mirroring him, but there are governance problems with that). I'm not convinced that Linus et al. want to maintain a fork in this situation.

      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 22:11 UTC (Fri)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (7 responses)
      
      
      
That's exactly why I've resisted the push to put someone else in that role. If it's one of the other people working on bcachefs I don't want to risk them burning out and losing another engineer; I'd be ok with it if it was another long standing member of the kernel community with experience working with Linus, but for some strange reason no one I've talked to wants to take that on. 
And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.

Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.

So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 4:57 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [Link] (1 responses)
      
      
      
> And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.
>
> Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.
The distros are downstreams. If they want to package and ship your code, in whatever way they see fit, you've already given them permission to do so. And they are not shy about exercising that permission. I remember several years ago, jwz asked Debian to stop shipping XScreenSaver, because he disagreed with their practice of backporting fixes to old versions. Debian said no, and XScreenSaver is still in the repository today. As you might imagine, some rather harsh words were exchanged, but in the end, both sides went back to their respective corners of the internet and proceeded to mostly ignore each other.

Linus, however, is not in the business of playing that game. If there's nobody actively maintaining (his copy of) bcachefs, then I find it hard to believe it's going to be allowed to stick around indefinitely.

> So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.

My interpretation of events is that there are only three long-term paths that make sense here:

* You accept that you cannot control what appears in Linus's tree, but would prefer that some recent-ish version of bcachefs is there (as opposed to no bcachefs or a very old bcachefs). You designate somebody as I've described upthread, they upstream patches at whatever rate Linus is willing to take them, and everybody is more or less willing to live with the result.
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They continue to ship an old bcachefs for (at least) the rest of the current release cycle, but eventually it bitrots and they delete it. You might or might not choose to ship it out-of-tree like ZFS, and various distros might or might not package some version of it for you (whether you want them to or not).
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They fork bcachefs or mirror it from your out-of-tree version, and slightly-old or modified versions continue to appear in the kernel indefinitely. As I explained, I think this is less likely, but I don't want to entirely discount it.

I do not see any plausible outcome where you are allowed to control what appears in Linus's tree. He has very explicitly closed the door on that. For expository purposes, and because you are a functioning adult, I have assumed that you will accept this lack of control, but that does not actually matter - one of the above scenarios will inevitably play out, regardless of your opinion of it. The only choice you have at this point is whether it's the first bullet or one of the other two.

I do not say this to be cruel. Based on your words in this and other threads, I genuinely believe that this process has been very painful for you, and I doubt you enjoy being reminded that Linus's tree does, in fact, belong to Linus (I'm sure other developers have screamed that at you enough times by now). Unfortunately, this is not a matter of right or wrong. It is a matter of power. You are aggrieved about something that neither Linus, nor anybody else on LKML, is prepared to recognize as an injury to you. Regardless of whether that is the right way or the wrong way of looking at it, Linus is going to conduct the kernel's release cycle as he sees fit. The *healthy* way of looking at it is to accept that that is not within your power to change, and redirect your attention to the things you can change.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 16:30 UTC (Sat)
                               by ttuttle (subscriber, #51118)
                              [Link] 
      
      
      
Thank you for posting such a compassionate response. This thread could easily turn into an unkind discussion or an all-out flame war, but you went out of your way to be kind instead.


      
          
        
     


    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 7:57 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (3 responses)
      
      
      
> Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.
Debian was not even close to the topic at hand, and yet you felt the need to bring it up, just to attack someone, and with information that is misrepresenting the truth. This is something you've done before, and you were very recently called out in lkml for it. Stop.

To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. It was ultimately removed from unstable because noone was able to get through. Source: I am one of them.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 11:44 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (1 responses)
      
      
      
The specific, technical issue was the package maintainer switching out the Rust dependencies for the packaged versions from Debian. I explained that this was a bad idea at the outset, because it invalidated all the testing we do, and the Debian package wasn't replicating that testing; it was also wholly unnecessary because Rust dependencies are statically linked.
He did so anyways, and then swapped out bindgen for an old version that was explicitly unsupported according to the Cargo.toml, which broke the build, and he sat on it and Debian users stopped getting updates (I didn't even see a report until months later).

This resulted in users being unable to access their filesystems.

There was briefly a buggy version of bcachefs-tools that couldn't pass mount options correctly; users in every other distro got a fix quickly, but Debian users did not - and we found out about this when a lot of users weren't able to mount in degraded mode after having a drive die.

What you're doing is conflating technical criticism with personal, and then using that as an excuse to ramp up the drama. Technical criticism, including pointing out failures of processes, has to be ok for engineering to function, otherwise we don't learn from our mistakes. That can make for a harsh learning environment, but when you're shipping critical system components that have to work, that's what you signed up for; we have responsibilities.

The person in question was warned explicitly that what he was doing was a bad idea; he could have at any point said "this is too complicated an issue for me to handle; I'll let someone else take this one" (and there are mechanisms in Debian process for obtaining exceptions to process rules that could have avoided this, by simply skipping the Rust dependency unbundling with a clear explanation of why); he ignored advice and plowed ahead, and a lot of people were affected by those actions.

When we work on this kind of code, we have to be responsible for the work we do, including our mistakes.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 14:25 UTC (Sat)
                               by ma4ris8 (subscriber, #170509)
                              [Link] 
      
      
      
My goal is to show the power of listening to the other. I'll try to listen first, and then answer.
I hope that you get my point of listening well in order to carefully heal the relationships.
Listen part: I'm trying to repeat roughly the same as you wrote above, to show that I listened you:

First you state that maintainer switched Rust dependencies for the packaged versions from Debian.
You explained that it was a bad idea, for multiple reasons: statically linked dependencies, and
invalidating all your active testing.

He changed Rust dependencies anyways, and then swapped out bindgen into older version,
which broke the build for Debian, and file system users stopped getting updates.

Important end question: Did I repeat (re-phrase in text) precisely what you wrote?

Answer part:
You wrote many items into one message. I answered only for the first one,
to keep the answer small enough. Some progress, but further messages
could increase coverage.

For me it sounds like there were some mistakes done by both you and others.
The unfortunate end result was, that Debian users had problems with the bug.
I didn't get from your message, the outcome of the relationships between persons:
whether personal relationships were worsened, stayed the same, or healed in the
end (each relation individually).

How to communicate (listen) effectively, to heal relationships?

This way of listening is mentioned in
https://www.verywellmind.com/what-is-active-listening-302...
"Paraphrasing and reflecting back what has been said"
( Those who know psychology, know these things ).

What I showed, is one way to restore human relationships, with Linus and others:
You could try to restore relationships with just listening others. Choose carefully
messaging cases, in which you think that you won't cause much backslash,
but you could have progress with healing the relationship by listening to the other.

If you get a backslash, you was just given an opportunity to listen the complaint.
Repeat in nearly the same words the whole complaint, 
so that the other one feels of being heard fully.
Try to at least have progress, thus please listen carefully the mentioned
complaint by repeating it. You can have pauses, like answering another day, to reduce the burden.
Please don't open up any new problems. If you do (I do mistakes sometimes),
and get a backslash as a heated answer, please listen and repeat it carefully,
to reduce the impact.

By doing this just very slightly to not burden others,
you could both improve your communication skills,
and perhaps others could learn from it too,
and perhaps then relations with other stakeholders, like maintainers,
and Linus, could be restored into a level that you can co-operate efficiently together again.

I've seen that sometimes this listening technique helps on-line, in addition of meeting face to face.
I'm trying to improve my communicating skills in the contexts of
change management for "OWASP top 10", and AI adoption.


      
          
        
     


    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 12:24 UTC (Sat)
                               by muase (subscriber, #178466)
                              [Link] 
      
      
      
Hm, for me it simply reads like OP is mentioning his frustration about release cycles, and is citing a pretty legitimate example: The fact that developers are overwhelmed with obsolete bug reports, because LTS distros are months or even years behind, is not something new and a real problem for some projects.
I know it's not the distros' fault; it's simply how LTS has to work in practice – however I can understand the frustration that arises if there seems to be an opportunity to finally update a package(set)... and then that opportunity is missed, and now the dev knows that they have to endure those obsolete bug reports for another n-year release cycle. It definitely didn't read as "just to attack someone".

> To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another.

Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case.  If the communication is public, do you have a link or something? :)


      
          
        
     


    
      A few suggestions (which you don’t have to follow)
       Posted Aug 30, 2025 17:12 UTC (Sat)
                               by DemiMarie (subscriber, #164188)
                              [Link] 
      
      
      
My recommendation is to have an out-of-tree repository that has the latest changes and to send them to Linus at a pace Linus is okay with.  Users who need the very latest code can use the driver from your tree via DKMS or similar.  I also recommend having a way to implement new features that are not on hot paths (such as recovery) in userspace.  Finally, a FUSE version would be a good idea and (as you mentioned earlier IIRC) make kernel changes that are not corruption or crash fixes less urgent, because users could recover in userspace and even access their data (albeit more slowly) while that is happening.
For anything that has to happen before the filesystem can be accessed at all, it might make sense to have an option for the userspace mount helper to do the work.  In this case, the userspace helper has far fewer disadvantages I know of.

My dream would be for bcachefs to have SQLite’s level of testing and input validation, or (even better) formal verification.   Either would massively reduce the rate of bugs making it into a release, but neither is reasonable to ask for outside of a suitably-priced commercial engagement.


      
          
        
     
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cognitive Load is what matters]]></title>
            <link>https://github.com/zakirullin/cognitive-load</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074248</guid>
            <description><![CDATA[🧠 Cognitive Load is what matters. Contribute to zakirullin/cognitive-load development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Cognitive Load is what matters
Readable version | Chinese translation | Korean translation | Turkish translation
It is a living document, last update: August 2025. Your contributions are welcome!
Introduction
There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.
Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.
Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.
Cognitive load

Cognitive load is how much a developer needs to think in order to complete a task.

When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.
Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.


We should reduce the cognitive load in our projects as much as possible.

  Cognitive load and interruptions
  

Types of cognitive load
Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.
Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.


Let's jump straight to the concrete practical examples of extraneous cognitive load.

We will refer to the level cognitive load as follows:
🧠: fresh working memory, zero cognitive load
🧠++: two facts in our working memory, cognitive load increased
🤯: cognitive overload, more than 4 facts

Our brain is much more complex and unexplored, but we can go with this simplistic model.

Complex conditionals
if val > someConstant // 🧠+
    && (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    && (condition4 && !condition5) { // 🤯, we are messed up by this point
    ...
}
Introduce intermediate variables with meaningful names:
isValid = val > someConstant
isAllowed = condition2 || condition3
isSecure = condition4 && !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid && isAllowed && isSecure {
    ...
}
Nested ifs
if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} 
Compare it with the early returns:
if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+
We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.
Inheritance nightmare
We are asked to change a few things for our admin users: 🧠
AdminController extends UserController extends GuestController extends BaseController
Ohh, part of the functionality is in BaseController, let's have a look: 🧠+
Basic role mechanics got introduced in GuestController: 🧠++
Things got partially altered in UserController: 🧠+++
Finally we are here, AdminController, let's code stuff! 🧠++++
Oh, wait, there's SuperuserController which extends AdminController. By modifying AdminController we can break things in the inherited class, so let's dive in SuperuserController first: 🤯
Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.
Too many small methods, classes or modules

Method, class and module are interchangeable in this context

Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.
Deep module - simple interface, complex functionality
Shallow module - interface is relatively complex to the small functionality it provides


Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.

Information hiding is paramount, and we don't hide as much complexity in shallow modules.

I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.
Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.

The best components are those that provide powerful functionality yet have a simple interface.
John K. Ousterhout

The interface of the UNIX I/O is very simple. It has only five basic calls:
open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)
A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.

This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.

P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.
Responsible for one thing
All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.
We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.

A module should be responsible to one, and only one, user or stakeholder.

This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.
But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.
Too many shallow microservices
This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.
I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. Diagnostic difficulty in integration space skyrocketed. Both time to market and cognitive load were unacceptably high. 🤯
Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.
The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.
A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.
Feature-rich languages
We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.
If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!
You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. 🤯
These statements are made by none other than Rob Pike.

Reduce cognitive load by limiting the number of choices.

Language features are OK, as long as they are orthogonal to each other.

  Thoughts from an engineer with 20 years of C++ experience ⭐️
  
  I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!
  I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.
  Like, can you imagine, the token || has a different meaning in requires ((!P<T> || !Q<T>)) and in requires (!(P<T> || Q<T>)). The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.
  You can't allocate space for a trivial type and just memcpy a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.
  Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. 🤯
  There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then... 🤯
  This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).
  I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.
  By no means I am trying to blame C++. I love the language. It's just that I am tired now.Thanks to 0xd34df00d for writing.

Business logic and HTTP status codes
On the backend we return:
401 for expired jwt token
403 for not enough access
418 for banned users
The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:
401 is for expired jwt token // 🧠+, ok just temporary remember it
403 is for not enough access // 🧠++
418 is for banned users // 🧠+++
Frontend developers would (hopefully) introduce some kind numeric status -> meaning dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.
Then QA engineers come into play:
"Hey, I got 403 status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.
Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:
{
    "code": "jwt_has_expired"
}
Cognitive load on the frontend side: 🧠 (fresh, no facts are held in mind)
Cognitive load on the QA side: 🧠
The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.

People spend time arguing between 401 and 403, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.

P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.
Abusing DRY principle
Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.
Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. 🤯
In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.
Rob Pike once said:

A little copying is better than a little dependency.

We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.
All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.
Tight coupling with a framework
There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.
Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. 🤯
By no means do we advocate to invent everything from scratch!
We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.

Why I Hate Frameworks

Layered architecture
There is a certain engineering excitement about all this stuff.
I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. 🤯
Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. 🤯
This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.

  Coding principles and experience
  
  @flaviocopes

If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.

With a sufficient number of users of an API,
it does not matter what you promise in the contract:
all observable behaviors of your system
will be depended on by somebody.

We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.
So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.
These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.
Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.
Layers of abstraction aren't free of charge, they are to be held in our limited working memory.


Domain-driven design
Domain-driven design has some great points, although it is often misinterpreted. People say "We write code in DDD", which is a bit strange, because DDD is about problem space, not about solution space.
Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.
Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. 🤯
Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.
Cognitive load in familiar projects

The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!
It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.
In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”
There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.
Thanks to Dan North for his comment.

If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.


The more mental models there are to learn, the longer it takes for a new developer to deliver value.
Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.
If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.
Examples

Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres
How Instagram scaled to 14 million users with only 3 engineers
The companies where we were like ”woah, these folks are smart as hell” for the most part failed
One function that wires up the entire system. If you want to know how the system works - go read it

These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.
Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.
Conclusion
Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. 🤯
Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.


We should reduce any cognitive load above and beyond what is intrinsic to the work we do.

LinkedIn, X, GitHub
Readable version

    Comments
    
    Rob PikeNice article.
    Andrej Karpathy (ChatGPT, Tesla)Nice post on software engineering. Probably the most true, least practiced viewpoint.
    Elon MuskTrue.
    Addy Osmani (Chrome, the most complex software system in the world)I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.
    The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."
    What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.
    And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.
    Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.
    One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.
    Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. 
    Sometimes the simplest solution is the best one, even in a complex system.
    antirez (Redis)Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.
    A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.
    Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).
    These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D
    A developer from the internetYou would not hire me... I sell myself on my track record of released enterprise projects.
    I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.
    I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.
    I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.
    Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.
    Comments on Hacker News

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FBI cyber cop: Salt Typhoon pwned 'nearly every American']]></title>
            <link>https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074157</guid>
            <description><![CDATA[: Plus millions of other people across 80+ countries]]></description>
            <content:encoded><![CDATA[
China's Salt Typhoon cyberspies hoovered up information belonging to millions of people in the United States over the course of the years-long intrusion into telecommunications networks, according to a top FBI cyber official.
"There's a good chance this espionage campaign has stolen information from nearly every American," Michael Machtinger, deputy assistant director for the FBI's cyber division, told The Register.
"There's a thought among the public that if you don't work in a sensitive area that the PRC might be interested in for its traditional espionage activities, then you are safe, they will not target you," he said, during a Thursday interview with The Register. "As we have seen from Salt Typhoon, this is no longer an assumption that anyone can afford to make."

    

The Beijing-backed spying campaign began at least in 2019 but wasn't uncovered by US authorities until last fall. On Wednesday, US law enforcement and intelligence agencies along with those from 12 other countries warned the ongoing espionage activity expanded far beyond nine American telcos and government networks. According to Machtinger, at least 80 countries were hit by the digital intrusions.

        


        

Around 200 American organizations were compromised by the espionage activity, Machtinger said, including the previously disclosed telecommunications firms such as Verizon and AT&T.
Yesterday's joint security alert also pointed the allies' collective finger at three China-based entities affiliated with Salt Typhoon: Sichuan Juxinhe Network Technology, Beijing Huanyu Tianqiong Information Technology, and Sichuan Zhixin Ruijie Network Technology. These companies, and likely others, provide cyber products and services to China's Ministry of State Security and People's Liberation Army, the governments said.


What the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space

"This is one of the most consequential cyber espionage breaches that we've ever seen in the United States," Machtinger said.
"What this really underscores is that what the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space," he added. "And that should really set off alarm bells for us — not only in the United States. The scale of indiscriminate targeting is unlike what we've seen in the past."

        

This indiscriminate targeting, as the FBI and White House security officials have previously noted, allowed Beijing’s snoops to geo-locate millions of mobile phone users, monitor their internet traffic, and, in some cases, record their phone calls. Victims reportedly included President Donald Trump and Vice President JD Vance.
Machtinger declined to confirm whether Trump and Vance were among those surveilled, but did say that victims included more than 100 current and former presidential administration officials.
"As we look at the impact on the different sets of victims," he said, Salt Typhoon collected "bulk information from millions of Americans."

        

For the more targeted group of individuals, "most of whom are very high-profile, current and former presidential administration officials, and campaign appointees from both major political parties," the data collection went much deeper, Machtinger added. "Down to intercepting actual content."


If you thought China's Salt Typhoon was booted off critical networks, think again

China's Salt Typhoon spies spotted on US govt networks before telcos, CISA boss says

This is the FBI, open up. China's Volt Typhoon is on your network

How does China keep stealing our stuff, wonders DoD group responsible for keeping foreign agents out

In addition to Salt Typhoon, the feds over the past year have issued warnings about other Chinese cyber operations. These include Volt Typhoon intruders, who infected hundreds of outdated routers to build a botnet and break into US critical infrastructure facilities. The Beijing-backed crew, we would later learn, was prepositioning itself and readying destructive cyberattacks.
Another China-linked crew, Silk Typhoon has spent more than a decade compromising IT and cloud providers to steal sensitive data from their government, technology, education, and legal and professional services customers.
China is not the only source of threats, Machtinger noted. Russia, Iran, North Korea, plus along with home-grown and international cybercriminals and ransomware crooks, assault computers and networks of both individuals and organizations, every day.
"These actors are going to continue their efforts, and they're going to get more sophisticated," Machtinger said. "We need to make sure that we, a nation, are taking cybersecurity seriously, updating systems, removing end-of-life devices, and making it as hard and costly as possible for the myriad of actors that are out there to successfully compromise." ®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Client Protocol]]></title>
            <link>https://agentclientprotocol.com/overview/introduction</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074147</guid>
            <description><![CDATA[Get started with the Agent Client Protocol (ACP)]]></description>
            <content:encoded><![CDATA[The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).
The protocol is still under development, but it should be complete enough to build interesting user experiences using it.Why ACP?
AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.
This creates several problems:
Integration overhead: Every new agent-editor combination requires custom work
Limited compatibility: Agents work with only a subset of available editors
Developer lock-in: Choosing an agent often means accepting their available interfaces

ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.
Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.
This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.Overview
ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.
Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.
The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.Supported Editors

Zed
neovim through the CodeCompanion plugin

Supported Agents

Gemini
… more coming soon ;)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[F-Stack – A network development kit with high performance based on DPDK]]></title>
            <link>https://www.f-stack.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074115</guid>
            <description><![CDATA[F-Stack Official Home Page]]></description>
            <content:encoded><![CDATA[
                    
                        
                                Introduction
                                With the rapid development of NIC, the poor performance of data packets processing with Linux kernel has become the bottleneck. However, the rapid development of the Internet needs high performance of network processing, kernel bypass has caught more and more attention. There are various similar technologies appear, such as DPDK, NETMAP and PF_RING. The main idea of kernel bypass is that Linux is only used to deal with control flow, all data streams are processed in user space. Therefore, kernel bypass can avoid performance bottlenecks caused by kernel packet copy, thread scheduling, system calls and interrupt. Furthermore, kernel bypass can achieve higher performance with multi optimizing methods. Within various techniques, DPDK has been widely used because of its more thorough isolation from kernel scheduling and active community support.
                                F-Stack is an open source network framework with high performance based on DPDK， include an user space TCP/IP stack(port FreeBSD 11.0 stable), Posix API(Socket, Epoll, Kqueue), Progamming SDK(Coroutine) and some apps(Nginx, Redis) interface.
                            
                        
                            
                        
                    
                    
                                F-Stack with follow characteristics
                                
                                    Ultra high network performance which can achieve network card under full load, 10 million concurrent connection, 5 million RPS, 1 million CPS.
                                    Transplant FreeBSD 11.01 user space stack, provides a complete stack function, cut a great amount of irrelevant features. Therefore greatly enhance the performance.
                                    Support Nginx, Redis and other mature applications, service can easily use F-Stack
                                    With Multi-process architecture, easy to extend
                                    Provide micro thread interface. Various applications with stateful app can easily use F-Stack to get high performance without processing complex asynchronous logic.
                                    Provide Epoll/Kqueue interface that allow many kinds of applications easily use F-Stack
                                
                                Currently, there are various products in Tencent Cloud has used the F-Stack, such as DKDNS(DNSPod's authorization DNS server), HttpDNS (D+), COS access module, CDN access module, etc..
                            
                ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nokia’s legendary font makes for a great user interface font]]></title>
            <link>https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074071</guid>
            <description><![CDATA[Home > OS News > It turns out Nokia’s legendary font makes for a great general user interface font]]></description>
            <content:encoded><![CDATA[ Home > OS News > It turns out Nokia’s legendary font makes for a great general user interface fontIf you’re of a certain age (and not American), there’s a specific corporate font you’re most likely aware of. You may not know its exact name, and you may not actively remember it, but once you see it, you know exactly what you’re looking at. The font’s called Nokia Sans (and Nokia Serif), and it was used by pretty much every single Nokia device between roughly 2002 and 2013 or so, when it was replaced by a very bland font made by Bruno Maag (with help from the person who designed Comic Sans) that they used after that.I can’t remember why, exactly, but I got majorly nostalgic for Nokia’s characteristic, recognisable font, and decided to see if it would work as a user interface font. Now, the font is still owned by Nokia and I couldn’t find a proper place to download it, but I eventually stumbled upon a site that had each individual variant listed for download. I downloaded each of them, installed them using KDE’s font installation method, and tried it out as my user interface font.You’ll quickly discover you shouldn’t use the regular variant, but should instead opt for the Nokia Sans Wide variant. Back in 2011, when Nokia originally announced it was replacing Nokia Sans, the creator of the font, Erik Spiekermann, responded to the announcement on his blog. Apparently, one of the major reasons for Nokia to change fonts was that they claimed Nokia Sans wouldn’t work as a user interface font, but Spiekermann obviously disagrees, pointing specifically to the Wide variant. In fact, Spiekermann does not pull any punches.After 10 years it was high time to look at Nokia’s typefaces as the dominant visual voice of the brand but whoever decided on a completely new direction was either not aware of what was available or was persuaded by Bruno Maag to start over. Bruno may not create the most memorable typefaces, but he certainly knows how to sell them. And technically, their fonts are excellent. Too bad they didn’t have the confidence to work with me on an update. Instead they’re throwing out ten years of brand recognition in favour of blandness.
↫ Erik SpiekermannI was pleasently surprised by just how nice the font looks when used as a general user interface font. It’s extremely legible at a variety of sizes, and has a ton of character without becoming gimmicky or overbearing. What originally started as mere curiosity has now become my UI font of choice on all my machines, finally displacing Inter after many years of uncontested service. Of course, all of this is deeply personal and 95% an issue of taste, but I wanted to write about it to see if I’m just entirely crazy, or if there’s some method to my madness.Do note that I’m using high DPI displays, and KDE on Wayland, and that all of this may look different on Windows or macOS, or on displays with lower DPI. One of Inter’s strengths is that it renders great on both high and lower DPI displays, but since I don’t have any lower DPI displays anymore, I can’t test it in such an environment. I’m also not entirely sure about the legal status of downloading fonts like this, but I am fairly sure you’re at least allowed to use non-free fonts for personal, non-commercial use, but please don’t quote me on that. Since downloading each variant of these Nokia fonts is annoying, I’d love to create and upload a zip file containing all of them, but I’m sure that’s illegal.I’m not a font connoisseur, so I may be committing a huge faux pas here? Not that I care, but reading about font nerds losing their minds over things I never even noticed is always highly entertaining.About The Author
Thom HolwerdaFollow me on Mastodon @[email protected]]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hardening Firefox – a checklist for improved browser privacy]]></title>
            <link>https://andrewmarder.net/firefox/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45073746</guid>
            <description><![CDATA[A checklist for configuring Mozilla Firefox for a more private browsing experience.]]></description>
            <content:encoded><![CDATA[  July 3, 2025 /  3 min read  
Updated:
August 29, 2025    This checklist will walk you (and me) through the settings and extensions I use to improve my privacy when using Firefox.
If you’re looking for a web browser that offers a high degree of privacy out of the box with minimal setup, Brave is a common choice. However, I prefer Firefox for several reasons:

Firefox is developed by the nonprofit organization Mozilla.
I value Mozilla’s commitment to open source software.
Firefox is not based on Chromium. Brave, like most browsers, is based on Chromium, which is developed primarily by Google.

While there are many web browsers to choose from, I’ve decided Firefox is best for me. This post is a checklist of how I’ve configured it to better protect my privacy while browsing the web.
1. Basic Privacy Settings
Access Firefox’s settings by clicking the menu button (three horizontal lines) in the top-right corner and selecting “Settings.”

 Change Default Search Engine: In the Search tab, change the “Default Search Engine” to a privacy-respecting option like DuckDuckGo.
 Enable HTTPS-Only Mode: In the Privacy & Security tab, scroll down to “HTTPS-Only Mode” and select “Enable HTTPS-Only Mode in all windows.”
 Disable Telemetry: Still in Privacy & Security, scroll to “Firefox Data Collection and Use” and uncheck all the boxes to stop Firefox from sending data back to Mozilla.
 Set Enhanced Tracking Protection to Strict: Under Privacy & Security, set “Enhanced Tracking Protection” to Strict. This offers stronger protection against trackers. If a site breaks, you can easily disable it for that specific site by clicking the shield icon in the address bar.

2. Recommended Extensions

 Install uBlock Origin: A comprehensive content blocker that stops ads and tracking scripts, which speeds up page loading and enhances privacy.
 Install ClearURLs: This extension automatically removes tracking elements from URLs, helping prevent another form of web tracking.
 Install Privacy Badger: From the Electronic Frontier Foundation, this extension automatically learns to block invisible trackers. Instead of relying on blocklists, it discovers trackers based on behavior.

3. Advanced Configuration (about:config)
To access this, type about:config into the address bar and accept the warning.
Warning: Changing advanced configuration preferences can impact Firefox performance or security. Proceed with caution.

 Isolate Cookies to the First-Party Domain:

Search for privacy.firstparty.isolate and set its value to true.
This prevents cookies from tracking you from one site to another, but it can break single sign-on on some websites.


Resist Fingerprinting:

I previously set privacy.resistFingerprinting to true to make my browser fingerprint less unique.
It caused minor display issues on some sites and broke image uploads to Bluesky, so I set it back to false.



By following this checklist, you can significantly improve your privacy while using Firefox. Please let me know if I’m missing anything in the comments.
     ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms]]></title>
            <link>https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45072160</guid>
            <description><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms
What is attention?
In any autoregressive model, the prediction of the future tokens is based on some preceding context …]]></description>
            <content:encoded><![CDATA[7 min read1 day ago--Press enter or click to view image in full sizeWhat is attention?In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.“The animal didn’t cross the street because it was too tired”.In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.How is attention calculated?There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.Key Concepts in Attention MechanismsBefore diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vectorKV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.To understand how each of these vectors are scores are calculated you can refer to this blog.The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.Now, let's dive into each of these techniquesMulti-Head Attention (MHA)In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.Press enter or click to view image in full sizeImage credits: Illustrated TransformersIn multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.Models using MHA – Bert, RoBerta, T5, etc.Multi-Query Attention (MQA)A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHAModels using MQA – PaLM, FalconGrouped Query Attention (GQA)Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.Models using GQA – Llama2, Llama3, MistralMulti-Head Latent Attention (MHLA)While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.So during the inference:A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectorsThis is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectorsAdditionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embeddingAdditionally, the same process is done for attention Queries as well, which will reduce the activation memory during trainingPress enter or click to view image in full sizeMHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.Models using MHLA– Deepseek- V2, Deep seek V2ConclusionIn addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Theoretical Limitations of Embedding-Based Retrieval]]></title>
            <link>https://arxiv.org/abs/2508.21038</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068986</guid>
            <description><![CDATA[Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.
    

    
    
      
          Subjects:
          
            Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21038 [cs.IR]
        
        
           
          (or 
              arXiv:2508.21038v1 [cs.IR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.21038
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Orion Weller [view email]          [v1]
        Thu, 28 Aug 2025 17:43:53 UTC (195 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Do the simplest thing that could possibly work]]></title>
            <link>https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068091</guid>
            <description><![CDATA[When designing software systems, do the simplest thing that could possibly work. It’s surprising how far you can take this piece of advice. I genuinely think…]]></description>
            <content:encoded><![CDATA[When designing software systems, do the simplest thing that could possibly work.
It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.
A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.
Simple can be underwhelming
System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.
However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.
In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.
Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.
You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?
Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?
You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.
What’s wrong with doing the simplest thing?
Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.
Big balls of mud
To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.
But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.
It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.
What is simplicity?
Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:

Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them
Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces

Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.
What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.
When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.
Why wouldn’t you want to be scalable?
A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?
No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.
The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.
The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!
Final thoughts
The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.
There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.
edit: this article has gotten some comments on Hacker News.
One interesting comment thread says that simplicity of architecture doesn’t matter at scale, because the complexity of “state space exploration in implementation” (I think that means something like what I wrote about here) dominates any other complexity. I disagree - the more complex your feature interactions become, the more important a simple architecture becomes, because your “complexity budget” is almost exhausted.
I also want to credit Ward Cunningham and Kent Beck for inventing the expression - I genuinely thought I’d just come up with the wording myself, but I almost certainly just remembered it. Oops! Thanks to the HN user ternaryoperator for pointing this out.




It’s just Unix sockets and forked processes! I love Unicorn.
↩


Every tech company has some kind of edge proxy.
↩


I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).
↩


I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.
↩


Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.
↩


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[John Carmack's arguments against building a custom XR OS at Meta]]></title>
            <link>https://twitter.com/ID_AA_Carmack/status/1961172409920491849</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066395</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Essential Coding Theory [pdf]]]></title>
            <link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065705</guid>
        </item>
        <item>
            <title><![CDATA[Grok Code Fast 1]]></title>
            <link>https://x.ai/news/grok-code-fast-1</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063559</guid>
        </item>
    </channel>
</rss>