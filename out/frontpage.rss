<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 12 Sep 2025 21:05:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[I made a small site to share text and files for free, no ads, no registration]]></title>
            <link>https://www.dum.pt/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45226277</guid>
            <description><![CDATA[Upload, store, and share your dumps and files securely.]]></description>
            <content:encoded><![CDATA[Privacy Policy]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[North Korea executing more people for watching foreign films and TV, UN finds]]></title>
            <link>https://www.bbc.com/news/articles/ckgqdz17ye3o</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45226186</guid>
            <description><![CDATA[The report is based on more than 300 interviews with escapees from Kim Jong Un's secretive regime.]]></description>
            <content:encoded><![CDATA[12 hours agoJean MackenzieSeoul correspondent KCNA via EPALife under Kim Jong Un's rule has become tougher and people are more afraid, the report claims The North Korean government is increasingly implementing the death penalty, including for people caught watching and sharing foreign films and TV dramas, a major UN report has found.The dictatorship, which remains largely cut off from the world, is also subjecting its people to more forced labour while further restricting their freedoms, the report added.The UN Human Rights Office found that over the past decade the North Korean state had tightened control over "all aspects of citizens' lives"."No other population is under such restrictions in today's world," it concluded, adding that surveillance had become "more pervasive", helped in part by advances in technology.The UN High Commissioner for Human Rights, Volker TÃ¼rk, said that if this situation continued, North Koreans "will be subjected to more of the suffering, brutal repression and fear that they have endured for so long".The report, which is based on more than 300 interviews with people who escaped from North Korea in the past 10 years, found that the death penalty is being used more often. At least six new laws have been introduced since 2015 that allow for the penalty to be handed out. One crime which can now be punished by death is the watching and sharing of foreign media content such as films and TV dramas, as Kim Jong Un works to successfully limit people's access to information.Escapees told UN researchers that from 2020 onwards there had been more executions for distributing foreign content. They described how these executions are carried out by firing squads in public to instil fear in people and discourage them from breaking the law.Kang Gyuri, who escaped in 2023, told the BBC that three of her friends were executed after being caught with South Korean content. She was at the trial of one 23-year-old friend who was sentenced to death. "He was tried along with drug criminals. These crimes are treated the same now," she said, adding that since 2020 people had become more afraid.Watch: Rare footage shows teens sentenced to hard labour over K-drama Such experiences run counter to what North Korean people had expected from the past decade. When the current leader Kim Jong Un came to power in 2011, the escapees who were interviewed said they had hoped their lives would improve, as Kim had promised they would no longer need to "tighten their belts" â€“ meaning they would have enough to eat. He promised to grow the economy, while also protecting the country by further developing its nuclear weapons.But the report found that since Kim shunned diplomacy with the West and the US in 2019, instead focusing on his weapons programme, people's living situations and human rights had "degraded".Almost everyone interviewed said they did not have enough to eat, and having three meals a day was a "luxury". During the Covid pandemic, many escapees said there had been a severe lack of food, and people across the country died of hunger.At the same time, the government cracked down on the informal marketplaces where families would trade, making it harder for them to make a living. It also made it nearly impossible to escape from the country, by tightening controls along the border with China and ordering troops to shoot those trying to cross."In the early days of Kim Jong Un, we had some hope, but that hope did not last long," said one young woman who escaped in 2018 at the age of 17.  "The government gradually blocked people from making a living independently, and the very act of living became a daily torment," she testified to researchers.The UN report said that "Over the past 10 years the government has exercised near total control over people, leaving them unable to make their own decisions" - be they economic, social or political. The report added that improvements in surveillance technology had helped make this possible.One escapee told researchers these government crackdowns were intended "to block people's eyes and ears". "It is a form of control aimed at eliminating even the smallest signs of dissatisfaction or complaint," they said, speaking anonymously.AFP via Getty ImagesPeople bow in front of a mosaic in Pyongyang featuring Kim's father and grandfather in this photo taken on 9 September The report also found the government is using more forced labour than it was a decade ago. People from poor families are recruited into "shock brigades" to complete physically demanding tasks, such as construction or mining projects.The workers hope this will improve their social status, but the work is hazardous, and deaths are common. Rather than improve workers' safety, however, the government glorifies deaths, labelling them as a sacrifice to Kim Jong Un. In recent years it has even recruited thousands of orphans and street children, the report claims.This latest research follows a groundbreaking UN commission of inquiry report in 2014, which found, for the first time, that the North Korean government was committing crimes against humanity. Some of the most severe human rights violations were discovered to be taking place at the country's notorious political prison camps, where people can be locked up for life and "disappeared".This 2025 report finds that at least four of these camps are still operating, while detainees in regular prisons are still being tortured and abused. Many escapees said they had witnessed prisoners die from ill treatment, overwork and malnutrition, though the UN did hear of "some limited improvements" at the facilities, including "a slight decrease in violence by guards".KCNA via ReutersRussia's Putin, China's Xi and North Korea's Kim met in Beijing earlier this monthThe UN is calling for the situation to be passed to the International Criminal Court in the Hague. However, for this to happen, it would need to be referred by the UN Security Council. Since 2019, two of its permanent members, China and Russia, have repeatedly blocked attempts to impose new sanctions on North Korea.Last week, Kim Jong Un joined the Chinese leader Xi Jinping and the Russian President Vladimir Putin at a military parade in Beijing, signalling these countries' tacit acceptance of North Korea's nuclear weapons programme and treatment of its citizens.As well as urging the international community to act, the UN is asking the North Korean government to abolish its political prison camps, end the use of the death penalty and teach its citizens about human rights."Our reporting shows a clear and strong desire for change, particularly among (North Korea's) young people," said the UN human rights chief, Mr TÃ¼rk.Read more of our North Korea coverage]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hyundai battery plant faces startup delay after US immigration raid, CEO says]]></title>
            <link>https://www.japantimes.co.jp/business/2025/09/12/companies/hyundai-battery-plant-delay/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45226076</guid>
            <description><![CDATA[The plant, part of a $7.6 billion factory complex to make battery-powered models, was slated to come online later this year.]]></description>
            <content:encoded><![CDATA[
    
        

                  
            
                    

                    Sep 12, 2025
                



                                            
                            
                        
                                    DETROIT â€“ A battery plant co-owned by Hyundai Motor is facing a minimum startup delay of two to three months following an immigration raid last week, Hyundai CEO Jose Munoz said on Thursday.The Georgia plant, which is operated through a joint venture between Hyundai and South Korea's LG Energy Solution, was at the center of the largest single-site enforcement operation in the U.S. Department of Homeland Security's history last week.Munoz, in his first public comments since the raid, said he was surprised when he heard the news and immediately inquired if Hyundai workers were involved. He said the company discovered that the workers at the center of the raid were mainly employed by suppliers of LG.
                    
                
            
                    



    
    
            In a time of both misinformation and too much information, quality journalism is more crucial than ever.By subscribing, you can help us get the story right.
            SUBSCRIBE NOW
        
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: 47jobs â€“ A Fiverr/Upwork for AI Agents]]></title>
            <link>https://47jobs.xyz</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45226066</guid>
        </item>
        <item>
            <title><![CDATA[Groundbreaking Brazilian Drug, Capable of Reversing Spinal Cord Injury]]></title>
            <link>https://www1.folha.uol.com.br/internacional/en/scienceandhealth/2025/09/groundbreaking-brazilian-drug-considered-capable-of-reversing-spinal-cord-injury-presented-in-sao-paulo.shtml</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45225506</guid>
            <description><![CDATA[Patients regained movement in the experimental phase of polylaminin, capable of regenerating the spinal cord in people who suffered organ rupture]]></description>
            <content:encoded><![CDATA[
                    From a tender wrapper of human life, the placenta, comes the extraction of a protein that points to a solution for something that, until now, science had no clear path toâ€”and never one so celebrated: restoring the spinal cord in people who suffered injuries and lost body movement.Brazilian researcher Tatiana Coelho de Sampaio, PhD professor at the Federal University of Rio de Janeiro, has quietly worked with a team of biologists for the past 25 years on the repairing and multiplying power of the protein laminin, which acts on the nervous system.

The studies ultimately produced the current drug polylaminin, a world first, presented on Tuesday (9) by CristÃ¡lia laboratory as capable of regenerating the spinal cord in people who suffered organ rupture in accidents of various kinds, leading to paraplegiaâ€”paralysis of the lower limbsâ€”or quadriplegiaâ€”paralysis of both lower and upper limbs.






During the experimental phase of the antidote, which is applied directly to the spine, patients experienced full recovery of their conditions, with no aftereffects and resuming a routine without restrictions.

Read the article in the original language


    News from Brazil
      Receive in your email inbox a summary of the day
    


                  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UTF-8 is a brilliant design]]></title>
            <link>https://iamvishnu.com/posts/utf8-is-brilliant-design</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45225098</guid>
            <description><![CDATA[Exploring the brilliant design of UTF-8 encoding system that represents millions of characters while being backward compatible with ASCII]]></description>
            <content:encoded><![CDATA[
    UTF-8 is a Brilliant Design
    2025-09-12
    


The first time I learned about UTF-8 encoding, I was fascinated by how well-thought and brilliantly it was designed to represent millions of characters from different languages and scripts, and still be backward compatible with ASCII.
Basically UTF-8 uses 32 bits and the old ASCII uses 7 bits, but UTF-8 is designed in such a way that:

Every ASCII encoded file is a valid UTF-8 file.
Every UTF-8 encoded file that has only ASCII characters is a valid ASCII file.

Designing a system that scales to millions of characters and still be compatible with the old systems that use just 128 characters is a brilliant design.

Note: If you are already aware of the UTF-8 encoding, you can explore the UTF-8 Playground utility that I built to visualize UTF-8 encoding.

How Does UTF-8 Do It?
UTF-8 is a variable-width character encoding designed to represent every character in the Unicode character set, encompassing characters from most of the world's writing systems.
It encodes characters using one to four bytes. 
The first 128 characters (U+0000 to U+007F) are encoded with a single byte, ensuring backward compatibility with ASCII, and this is the reason why a file with only ASCII characters is a valid UTF-8 file.
Other characters require two, three, or four bytes. The leading bits of the first byte determine the total number of bytes that represents the current character. These bits follow one of four specific patterns, which indicate how many continuation bytes follow.



1st byte Pattern
# of bytes used
Full byte sequence pattern



0xxxxxxx
1
0xxxxxxx(This is basically a regular ASCII encoded byte)


110xxxxx
2
110xxxxx 10xxxxxx


1110xxxx
3
1110xxxx 10xxxxxx 10xxxxxx


11110xxx
4
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx


Notice that the second, third, and fourth bytes in a multi-byte sequence always start with 10. This indicates that these bytes are continuation bytes, following the main byte.
The remaining bits in the main byte, along with the bits in the continuation bytes, are combined to form the character's code point. A code point serves as a unique identifier for a character in the Unicode character set. A code point is typically represented in hexadecimal format, prefixed with "U+". For example, the code point for the character "A" is U+0041.
So here is how a software determines the character from the UTF-8 encoded bytes:

Read a byte. If it starts with 0, it's a single-byte character (ASCII). Show the character represented by the remaiing 7 bits on the screen. Continue with the next byte.
If the byte didn't start with a 0, then:
If it starts with 110, it's a two-byte character, so read the next byte as well.
If it starts with 1110, it's a three-byte character, so read the next two bytes.
If it starts with 11110, it's a four-byte character, so read the next three bytes.


Once the number of bytes are determined, read all the remaining bits except the leading bits, and find the binary value (aka. code point) of the character.
Look up the code point in the Unicode character set to find the corresponding character and display it on the screen.
Read the next byte and repeat the process.

Example: Hindi Letter "à¤…" (open in UTF-8 Playground)
The Hindi letter "à¤…" (officially "Devanagari Letter A") is represented in UTF-8 as:
11100000 10100100 10000101
Here:
The first byte 11100000 indicates that the character is encoded using 3 bytes.
The remaining bits of the three bytes:
xxxx0000 xx100100 xx000101 
are combined to form the binary sequence 00001001 00000101 (0x0905 in hexadecimal). This is the code point of the character, represented as U+0905.
The code point U+0905 (see official chart) represents the Hindi letter "à¤…" in the Unicode character set.
Example Text Files
Now that we understood the design of UTF-8, let's look at a file that contains the following text:
1. Text file contains: HeyðŸ‘‹ Buddy
The text HeyðŸ‘‹ Buddy has both English characters and an emoji character on it. The text file with this text saved on the disk will have the following 13 bytes in it:
01001000 01100101 01111001 11110000 10011111 10010001 10001011 00100000 01000010 01110101 01100100 01100100 01111001
Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:



Byte
Explanation



01001000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1001000 represent the letter 'H'. (open in playground)


01100101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100101 represent the letter 'e'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


11110000
Starts with 11110, indicating it's the first byte of a four-byte character.


10011111
Starts with 10, indicating it's a continuation byte.


10010001
Starts with 10, indicating it's a continuation byte.


10001011
Starts with 10, indicating it's a continuation byte.The bits from these four bytes (excluding the leading bits) combine to form the binary sequence 00001 11110100 01001011, which is 1F44B in hexadecimal, corresponds to the code point U+1F44B. This code point represents the waving hand emoji "ðŸ‘‹" in the Unicode character set (open in playground).


00100000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 0100000 represent a whitespace character. (open in playground)


01000010
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1000010 represent the letter 'B'. (open in playground)


01110101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1110101 represent the letter 'u'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


Now this is a valid UTF-8 file, but it doesn't have to be "backward compatible" with ASCII because it contains a non-ASCII character (the emoji). Next let's create a file that contains only ASCII characters.
2. Text file contains: Hey Buddy
The text file doesn't have any non-ASCII characters. The file saved on the disk has the following 9 bytes in it:
01001000 01100101 01111001 00100000 01000010 01110101 01100100 01100100 01111001
Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:



Byte
Explanation



01001000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1001000 represent the letter 'H'. (open in playground)


01100101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100101 represent the letter 'e'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


00100000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 0100000 represent a whitespace character. (open in playground)


01000010
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1000010 represent the letter 'B'. (open in playground)


01110101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1110101 represent the letter 'u'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


So this is a valid UTF-8 file, and it is also a valid ASCII file. The bytes in this file follows both the UTF-8 and ASCII encoding rules. This is how UTF-8 is designed to be backward compatible with ASCII.
Other Encodings
I did a quick research on any other encoding that are backward compatible with ASCII, and there are a few, but they are not as popular as UTF-8, for example GB 18030 (a Chinese government standard). Another one is the ISO/IEC 8859 encodings are single-byte encodings that extend ASCII to include additional characters, but they are limited to 256 characters.
The siblings of UTF-8, like UTF-16 and UTF-32, are not backward compatible with ASCII. For example, the letter 'A' in UTF-16 is represented as: 00 41 (two bytes), while in UTF-32 it is represented as: 00 00 00 41 (four bytes).
Bonus: UTF-8 Playground
When I was exploring the UTF-8 encoding, I couldn't find any good tool to interactively visualize how UTF-8 encoding works. So I built UTF-8 Playground to visualize and play around with UTF-8 encoding. Give it a try!.

Read the discussion on this on Hacker News.

    
        #tech
        #history
        #programming
    

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EU court rules nuclear energy is clean energy]]></title>
            <link>https://www.weplanet.org/post/eu-court-rules-nuclear-energy-is-clean-energy</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224967</guid>
            <description><![CDATA[The highest court in the EU just reaffirmed that nuclear energy meets the scientific and environmental standards to be included in sustainable finance, and Greenpeace still refuses to budge.]]></description>
            <content:encoded><![CDATA[Launching Dear Greenpeace back in 2023When I launched Dear Greenpeace with my fellow youth climate activists alongside WePlanet two years ago, I had no idea just how quickly the anti-nuclear dominoes would fall across Europe. In 2023, and what seems like a lifetime ago, Austria launched their legal action against the European Commission for the inclusion of nuclear energy in the EU Sustainable Finance Taxonomy. At the time they were supported by a bulwark of EU countries and environmental NGOs that opposed nuclear energy. Honestly, it looked like they might win.But today, that whole landscape has changed.Germany, long a symbol of anti-nuclear politics, is beginning to shift. The nuclear phase-outs or bans in the Netherlands, Belgium, Switzerland, Denmark, and Italy are now history. Even Fridays for Future has quietened its opposition, and in some places, embraced nuclear power.This moment matters.It shows whatâ€™s possible when we stick to the science. The evidence only gets clearer by the day that nuclear energy has an extremely low environmental impact across its lifecycle, and strong regulations and safety culture ensure that it remains one of the safest forms of energy available to humanity. The European Court of Justice has now fully dismissed Austriaâ€™s lawsuit. That ruling doesnâ€™t just uphold nuclear energyâ€™s place in EU green finance rules. It also signals a near-certain defeat for the ongoing Greenpeace case â€“ the very lawsuit that inspired me to launch Dear Greenpeace in the first place.But instead of learning from this, Greenpeace is doubling down. Martin Kaiser, Executive Director of Greenpeace Germany, called the court decision â€œa dark day for the climateâ€.Let that sink in. The highest court in the EU just reaffirmed that nuclear energy meets the scientific and environmental standards to be included in sustainable finance, and Greenpeace still refuses to budge.Meanwhile, the climate crisis gets worse. Global emissions are not falling fast enough. Billions of people still lack access to clean, reliable electricity. And we are forced to spend time defending proven solutions instead of scaling them.Announcing our inclusion in the case between Greenpeace and the EU CommissionItâ€™s now up to the court whether we will get our time in court to outline the evidence in support of nuclear energy and the important role it can play in the global clean energy transition. Whether in court, on the streets, or in the halls of parliaments across the globe, we will be there to defend the science and ensure that nuclear power can spread the advantages of the modern world across the planet in a sustainable, reliable and dignified way.Austria stands increasingly isolated among a handful of countries that still cling to their opposition to nuclear energy. Their defeat in this vital high stakes topic is a success not just for the nuclear movement, but for the global transition as a whole. We have made real progress. Together, weâ€™ve helped defend nuclear power in the EU, overturned outdated policies at the World Bank, and secured more technology-neutral language at the UN. These wins are not abstract. They open the door to real investment, real projects, and real emissions cuts.But the work is not done.We still need to overturn national nuclear bans, unlock more funding, and push democratic countries to support clean energy development abroad: especially where it is most needed to compete with Russiaâ€™s growing influence.The fight will not be done until every single country in the world can boast a clean, reliable energy grid, ready to maintain a modern dignified standard of living, for everyone, everywhere.This is a great success for the movement and it would not have been possible without the financial support, time and energy given by people like you.In Solidarity,
Ia Aanstoot]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[An embarrassing failure of the US patent system: Nintendo's latest patents]]></title>
            <link>https://www.pcgamer.com/gaming-industry/an-embarrassing-failure-of-the-us-patent-system-videogame-ip-lawyer-says-nintendos-latest-patents-on-pokemon-mechanics-should-not-have-happened-full-stop/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224901</guid>
            <description><![CDATA["Bad patents like this cast a massive shadow on the industry."]]></description>
            <content:encoded><![CDATA[









(Image credit: Nintendo)




The last 10 days have brought a string of patent wins for Nintendo. Yesterday, the company was granted US patent 12,409,387, a patent covering riding and flying systems similar to those Nintendo has been criticized for claiming in its Palworld lawsuit (via Gamesfray). Last week, however, Nintendo received a more troubling weapon in its legal arsenal: US patent 12,403,397, a patent on summoning and battling characters that the United States Patent and Trademark Office granted with alarmingly little resistance.According to videogame patent lawyer Kirk Sigmon, the USPTO granting Nintendo these latest patents isn't just a moment of questionable legal theory. It's an indictment of American patent law."Broadly, I don't disagree with the many online complaints about these Nintendo patents," said Sigmon, whose opinions do not represent those of his firm and clients. "They have been an embarrassing failure of the US patent system." 


(Image credit: Nintendo, USPTO)Sigmon, who we spoke with last year about the claims and potential consequences of Nintendo's Palworld lawsuit, said both this week's '387 patent and last week's '397 represent procedural irregularities in the decisionmaking of US patent officials. And thanks to those irregularities, Nintendo has yet more tools to bully its competitors.The '387 patent granted this week, Sigmon told PC Gamer, "got a bit of push-back, but barely." After its initial application was deemed invalid due to similarities to existing Tencent and Xbox-related patents, Nintendo amended its claims based on interviews with the USPTO, which then determined that the claims were allowable "for substantially the same reasons as parent application(s).""That parent case," Sigmon said, "had an even weirder and much less useful prosecution history." 


(Image credit: Nintendo, USPTO)Most of the claims made in the '387 patent's single parent case, US Pat. No. 12,246,255, were immediately allowed by the USPTO, which Sigmon said is "a very unusual result: most claims are rejected at least once." When the claims were ultimately allowed, the only reasoning the USPTO offered was a block quote of text from the claims themselves."This seems like a situation where the USPTO essentially gave up and just allowed the case, assuming that the claims were narrow or specific enough to be new without evaluating them too closely," Sigmon said. "I strongly disagree with this result: In my view, these claims were in no way allowable."Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.To Sigmon, an IP attorney with extensive experience in prosecuting and teaching patent law, the '387 patent and its parent case rely on concepts and decisions that would have been obvious to a "Person of Ordinary Skill in the Art"â€”a legal construct that holds if a patent's claims would reasonably occur to a practitioner in the relevant field based on prior art, those claims aren't patentable. 


(Image credit: Nintendo, USPTO)The '397 patent granted last week is even more striking. It's a patent on summoning and battling with "sub-characters," using specific language suggesting it's based on the Let's Go! mechanics in the PokÃ©mon Scarlet and Violet games. Despite its relevance to a conceit in countless gamesâ€”calling characters to battle enemies for youâ€”it was allowed without any pushback whatsoever from the USPTO, which Sigmon said is essentially unheard of."Like the above case, the reasons for allowance don't give us even a hint of why it was allowed: the Examiner just paraphrases the claims (after block quoting them) without explaining why the claims are allowed over the prior art," Sigmon said. "This is extremely unusual and raises a large number of red flags."According to Sigmon, USPTO records show that the allowance of the '397 patent was based on a review of a relatively miniscule number of documents: 16 US patents, seven Japanese patents, andâ€”apparentlyâ€”one article from Pokemon.com. 


(Image credit: Nintendo, USPTO)"I have no earthly idea how the Examiner could, in good faith, allow this application so quickly," Sigmon said.Admittedly, the '397 case was originally filed as a Japanese patent application, which would allow the Examiner to use the existing progress in the Japanese case as a starting point for their review. But, Sigmon said, "even that doesn't excuse this quick allowance.""This allowance should not have happened, full stop," he said.On paper, the patent might not seem like a threat to Nintendo's competitors: The claims as constructed in the '397 outline a very specific sequence of events and inputs, and patent claims must be met word-for-word to be infringed."Pragmatically speaking, though, it's not impossible to be sued for patent infringement even when a claim infringement argument is weak, and bad patents like this cast a massive shadow on the industry," Sigmon said.For a company at Nintendo's scale, the claims of the '397 patent don't need to make for a strong argument that would hold up in court. The threat of a lawsuit can stifle competition well enough on its own when it would cost millions of dollars to defend against. 


(Image credit: Nintendo, USPTO)"In my opinion, none of the three patents I've discussed here should have been allowed. It's shocking and offensive that they were," Sigmon said. "The USPTO dropped the ball big time, and it's going to externalize a lot of uncertainty (and, potentially, litigation cost) onto developers and companies that do not deserve it."Sigmon, who says he's helped inventors protect their inventions from IP theft perpetrated by major companies, insists that the patent system still has merit. "That's the kind of thing that patents are meant to do," he said. "They were not made to allow a big player to game the system, get an overly broad patent that they should have never received in the first place, and then go around bullying would-be competition with the threat of a legally questionable lawsuit."Unfortunately, Nintendo has gained these patents at a moment when the USPTO has made challenging bad patents more difficult. Currently, US patent officials under USPTO Acting Director Coke Morgan Stewart have been refusing to hear a huge number of Inter Partes Review casesâ€”special proceedings in which parties can argue that a patent should never have been grantedâ€”for "discretionary" reasons."Realistically, this means that patent validity issues are being relegated to lawsuits: not a good situation, as that often entails millions of dollars in costs and a lot of risk," Sigmon said. "In practice, this means that bad patents get to fester on the market for longer and provide a bigger threat for the industry as a whole."


Lincoln has been writing about games for 11 yearsâ€”unless you include the essays about procedural storytelling in Dwarf Fortress he convinced his college professors to accept. Leveraging the brainworms from a youth spent in World of Warcraft to write for sites like Waypoint, Polygon, and Fanbyte, Lincoln spent three years freelancing for PC Gamer before joining on as a full-time News Writer in 2024, bringing an expertise in Caves of Qud bird diplomacy, getting sons killed in Crusader Kings, and hitting dinosaurs with hammers in Monster Hunter.






]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why do browsers throttle JavaScript timers?]]></title>
            <link>https://nolanlawson.com/2025/08/31/why-do-browsers-throttle-javascript-timers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224542</guid>
            <description><![CDATA[Even if youâ€™ve been doing JavaScript for a while, you might be surprised to learn that setTimeout(0) is not really setTimeout(0). Instead, it could run 4 milliseconds later: Nearly a decade aâ€¦]]></description>
            <content:encoded><![CDATA[
				Even if youâ€™ve been doing JavaScript for a while, you might be surprised to learn that setTimeout(0) is not really setTimeout(0). Instead, it could run 4 milliseconds later:
const start = performance.now()
setTimeout(() => {
  // Likely 4ms
  console.log(performance.now() - start)
}, 0)

Nearly a decade ago when I was on the Microsoft Edge team, it was explained to me that browsers did this to avoid â€œabuse.â€ I.e. there are a lot of websites out there that spam setTimeout, so to avoid draining the userâ€™s battery or blocking interactivity, browsers set a special â€œclampedâ€ minimum of 4ms.
This also explains why some browsers would bump the throttling for devices on battery power (16ms in the case of legacy Edge), or throttle even more aggressively for background tabs (1 second in Chrome!).
One question always vexed me, though: if setTimeout was so abused, then why did browsers keep introducing new timers like setImmediate (RIP), Promises, or even new fanciness like scheduler.postTask()? If setTimeout had to be nerfed, then wouldnâ€™t these timers suffer the same fate eventually?
I wrote a long post about JavaScript timers back in 2018, but until recently I didnâ€™t have a good reason to revisit this question. Then I was doing some work on fake-indexeddb, which is a pure-JavaScript implementation of the IndexedDB API, and this question reared its head. As it turns out, IndexedDB wants to auto-commit transactions when thereâ€™s no outstanding work in the event loop â€“ in other words, after all microtasks have finished, but before any tasks (can I cheekily say â€œmacro-tasksâ€?) have started.
To accomplish this, fake-indexeddb was using setImmediate in Node.js (which shares some similarities with the legacy browser version) and setTimeout in the browser. In Node, setImmediate is kind of perfect, because it runs after microtasks but immediately before any other tasks, and without clamping. In the browser, though, setTimeout is pretty sub-optimal: in one benchmark, I was seeing Chrome take 4.8 seconds for something that only took 300 milliseconds in Node (a 16x slowdown!).
Looking out at the timer landscape in 2025, though, it wasnâ€™t obvious what to choose. Some options included:

setImmediate â€“ only supported in legacy Edge and IE, so thatâ€™s a no-go.
MessageChannel.postMessage â€“ this is the technique used by afterframe.
window.postMessage â€“ a nice idea, but kind of janky since it might interfere with other scripts on the page using the same API. This approach is used by the setImmediate polyfill though.
scheduler.postTask â€“ if you read no further, this was the winner. But letâ€™s explain why!

To compare these options, I wrote a quick benchmark. A few important things about this benchmark:

You have to run several iterations of setTimeout (and friends) to really suss out the clamping. Technically, per the HTML specification, the 4ms clamping is only supposed to kick in after a setTimeout has been nested (i.e. one setTimeout calls another) 5 times.
I didnâ€™t test every possible combination of 1) battery vs plugged in, 2) monitor refresh rates, 3) background vs foreground tabs, etc., even though I know all of these things can affect the clamping. I have a life, and although itâ€™s fun to don the lab coat and run some experiments, I donâ€™t want to spend my entire Saturday doing that.

In any case, here are the numbers (in milliseconds, median of 101 iterations, on a 2021 16-inch MacBook Pro):



Browser
setTimeout
MessageChannel
window
scheduler.postTask




Chrome 139
4.2
0.05
0.03
0.00


Firefox 142
4.72
0.02
0.01
0.01


Safari 18.4
26.73
0.52
0.05
Not implemented




Donâ€™t worry about the precise numbers too much: the point is that Chrome and Firefox clamp setTimeout to 4ms, and the other three options are roughly equivalent. In Safari, interestingly, setTimeout is even more heavily throttled, and MessageChannel.postMessage is a tad slower than window.postMessage (although window.postMessage is still janky for the reasons listed above).
This experiment answered my immediate question: fake-indexeddb should use scheduler.postTask (which I prefer for its ergonomics) and fall back to either MessageChannel.postMessage or window.postMessage. (I did experiment with different priorities for postTask, but they all performed almost identically. For fake-indexeddbâ€˜s use case, the default priority of 'user-visible' seemed most appropriate, and thatâ€™s what the benchmark uses.)
None of this answered my original question, though: why exactly do browsers bother to throttle setTimeout if web developers can just use scheduler.postTask or MessageChannel instead? I asked my friend Todd Reifsteck, who was co-chair of the Web Performance Working Group back when a lot of these discussions about â€œinterventionsâ€ were underway.
He said that there were effectively two camps: one camp felt that timers needed to be throttled to protect web devs from themselves, whereas the other camp felt that developers should â€œmeasure their own silliness,â€ and that any subtle throttling heuristics would just cause confusion. In short, it was the standard tradeoff in designing performance APIs: â€œsome APIs are quick but come with footguns.â€
This jives with my own intuitions on the topic. Browser interventions are usually put in place because web developers have either used too much of a good thing (e.g. setTimeout), or were blithely unaware of better options (the touch listener controversy is a good example). In the end, the browser is a â€œuser agentâ€ acting on the userâ€™s behalf, and the W3Câ€™s priority of constituencies makes it clear that end-user needs always trump web developer needs.
That said, web developers often do want to do the right thing. (I consider this blog post an attempt in that direction.) We just donâ€™t always have the tools to do it, so instead we grab whatever blunt instrument is nearby and start swinging. Giving us more control over tasks and scheduling could avoid the need to hammer away with setTimeout and cause a mess that calls for an intervention.
My prediction is that postTask/postMessage will remain unthrottled for the time being. Out of Toddâ€™s two â€œcamps,â€ the very existence of the Scheduler API, which offers a whole slew of fine-grained tools for task scheduling, seems to point toward the â€œpro-controlâ€ camp as the one currently steering the ship. Although Todd sees the API more as a compromise between the two groups: yes, it offers a lot of control, but it also aligns with the browserâ€™s actual rendering pipeline rather than random timeouts.
The pessimist in me wonders, though, if the API could still be abused â€“ e.g. by carelessly using the user-blocking priority everywhere. Perhaps in the future, some enterprising browser vendor will put their foot more firmly on the throttle (so to speak) and discover that it causes websites to be snappier, more responsive, and less battery-draining. If that happens, then we may see another round of interventions. (Maybe weâ€™ll need a scheduler2 API to dig ourselves out of that mess!)
Iâ€™m not involved much in web standards anymore and can only speculate. For the time being, Iâ€™ll just do what most web devs do: choose whatever API accomplishes my goals today, and hope that browsers donâ€™t change too much in the future. As long as weâ€™re careful and donâ€™t introduce too much â€œsilliness,â€ I donâ€™t think thatâ€™s a lot to ask.
Thanks to Todd Reifsteck for feedback on a draft of this post.


							]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How FOSS Projects Handle Legal Takedown Requests]]></title>
            <link>https://f-droid.org/2025/09/10/how-foss-projects-handle-legal-takedown-requests.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224421</guid>
            <description><![CDATA[When a legal takedown request arrives, whether itâ€™s about copyright,censorship, privacy, or something more vague, how a Free and Open SourceSoftware (FOSS) p...]]></description>
            <content:encoded><![CDATA[
          

  

  
    When a legal takedown request arrives, whether itâ€™s about copyright,
censorship, privacy, or something more vague, how a Free and Open Source
Software (FOSS) project responds can make all the difference.

Handled well, a takedown request can be a manageable administrative step. Handled poorly, it can cause panic, disrupt infrastructure, or even put contributors at legal risk.

As part of our legal resilience research, we spoke with a range of legal experts, software freedom advocates, and maintainers of mature FOSS infrastructure to understand how others manage these moments. In this article, we share what we learned, and how F-Droid is incorporating these lessons into its own approach.

A Pattern Emerges

Despite differences in jurisdiction, size, and mission, a few common themes
from our research emerged when we asked how other projects handle takedown
requests:

1. Donâ€™t Be a Soft Target

Legal threats often follow the path of least resistance. FOSS projects that
publish a formal takedown policy, require legal submissions through specific
channels, and insist on a valid legal basis are much less likely to receive,
or comply with, vague or harassing demands.

One FOSS organization, for example, requires all legal correspondence to be submitted by postal mail in the national language and citing local law. Most complaints evaporate once asked to comply.

2. Creating a transparent and documented process

Several digital rights organizations advised setting up structured response
steps:


  Require submissions to a dedicated legal@ or abuse@ email.
  Insist on full documentation: legal basis, jurisdiction, evidence of the
infringement and identity of the complainant.
  Review for sufficiency, proportionality, and standing before acting.


This creates proper documentation to process valid claims, while protecting projects from illegitimate or unfounded requests.

3. Use Jurisdiction Strategically

Projects based in civil law jurisdictions, particularly in Europe, are often
better positioned to deflect legal demands from foreign entities. Several
organizations emphasized that complying with vague or extrajudicial
requests, especially those originating outside your jurisdiction, can
increase risk unnecessarily. Instead, they recommended requiring a valid
legal basis grounded in the projectâ€™s home country. Formal legal processes,
such as court orders or official government channels, were seen as the
appropriate threshold, not informal emails or unverifiable demands.

Notification and Appeals: Fairness and Transparency

All of the projects we consulted emphasized the importance of notifying
developers whose apps are being targeted, informing them (if possible) of
the seriousness of the claim, and the proposed strategy F-Droid is taking to
handle the claim.Â 

If a threat is deemed to be valid and a developerâ€™s content is flagged for
takedown:


  The developer or maintainer is informed, unless prohibited by law (gag
orders).
  A window for response (commonly 14 days) is offered, unless unfeasible due
to seriousness and time restraints of the request itself
  If the developer disputes the claim and provides supporting information
(e.g. license, public domain status, fair use justification), the claim is
reviewed.
  If the claim is upheld, the content is removed, but always with an
internal record and opportunity to appeal.


This mirrors principles embedded in international norms (like the Manila Principles and GitHubâ€™s DMCA takedown policy) and avoids overcompliance with weak or abusive claims.

Transparency, Censorship, and What You Can (Legally) Publish

Takedown requests occupy a complex space between legal enforcement and
censorship. While some are legitimate claims, like copyright violations or
privacy breaches, others are vague, politically motivated, or intended to
silence dissent. For FOSS projects that have a global user base, itâ€™s not
always obvious how to respond. Complying too quickly can reinforce
censorship practices; resisting without process can lead to full website
shut downs, domain names being taken
away
(as in the US) or large and costly legal
battles.

One strategy that helps balance this tension is radical
transparency. Several projects we spoke with emphasized the importance of
documenting what actions were taken and why, not just for accountability,
but as a form of resistance. A well-known example is GitHubâ€™s DMCA takedown
policy (as of July 2025), which mandates compliance with valid takedown
requests, but also posts each one publicly in their github/dmca
repository. The result: potential abusers
know their requests will face public scrutiny, which acts as a deterrent.

However, not all jurisdictions allow this kind of transparency. In India,
for example, we were informed that it is often illegal to disclose that you
have received a government request, even to the developer of the affected
app. In contrast, in Russia, takedown requests can often be legally
posted, though by doing so you may be
putting yourself at risk for retaliation, additional takedown requests and
legal troubles.

With that in mind, some best practices for FOSS projects include:


  Publishing biannual transparency reports, even if redacted or aggregated.
  Maintaining an internal log of all takedown activity, with public
disclosure where legally possible.
  Explaining the general reasons for content removals, who made the request,
under what law, and what action was taken, unless legally prohibited.
  Being explicit about what cannot be shared, and why.


Transparency wonâ€™t prevent all forms of censorship, but it can slow them
down, raise awareness, and provide a record that strengthens the broader
FOSS ecosystem.

What Weâ€™re Doing at F-Droid

F-Droid is revising its own takedown policy, informed by:


  Dutch law and EU regulations
  The structural support provided by The Commons Conservancy
  Practical lessons from long-standing FOSS organizations


Our draft process includes:


  Written takedown submission request to legal@f-droid.org including the
   required information.:



  Identify the specific material in question (e.g. app name)
  Include valid legal basis under applicable jurisdiction (e.g. copyright
  law, court order statutory basis)
  Indicate jurisdiction in which the legal basis is claimed to apply
  Include sufficient evidence of the alleged infringement (e.g. copyright
  certificate, ownership declaration)
  Clearly state that the complaintant is authorized to act on behalf of the
  rights holder
  Include full contact details and a verifiable identity (subject to
  exceptions, such as gag orders or whistleblower protection)



  Verification of jurisdiction and legal basis, including evidence
  Developer notification and appeal procedures
  Rejection of requests lacking documentation or legal authority may be
   rejected or ignored
  Biannual transparency reports and public tracking of takedown requests


Weâ€™re also working to improve contributor education about potential exposure
when contributing to F-Droid, document internal escalation paths, and ensure
consistent handling of international claims.

Final Thoughts

Takedown requests are not going away in fact, theyâ€™re becoming more frequent
and more complex. But FOSS projects donâ€™t have to face them unprepared.

By building processes, establishing clear jurisdiction, and protecting individuals through structure and policy, we can handle these challenges with the seriousness they deserve without letting them derail our mission.

Legal Disclaimer

The content provided in this article is for informational purposes only and
does not constitute legal advice. While we strive to provide accurate and
up-to-date information, F-Droid makes no representations or warranties of
any kind, express or implied, about the completeness, accuracy, or
suitability of the information contained herein.

F-Droid is not a law firm and does not offer legal services. Any reliance
you place on the information provided is strictly at your own risk. If you
have questions about legal obligations, rights, or compliance, we strongly
recommend consulting a qualified legal professional familiar with your
jurisdiction.

F-Droid and its contributors disclaim all liability for any loss or damage
arising from the use or misuse of this content.

  



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Humanely dealing with humungus crawlers]]></title>
            <link>https://flak.tedunangst.com/post/humanely-dealing-with-humungus-crawlers</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224246</guid>
            <description><![CDATA[I host a bunch of hobby code on my server. I would think itâ€™s really only interesting to me, but it turns out every day, thousands of people from all over the world are digging through my code, reviewing years old changesets. On the one hand, wow, thanks, this is very flattering. On the other hand, what the heck is wrong with you?]]></description>
            <content:encoded><![CDATA[
I host a bunch of hobby code on my server. I would think itâ€™s really only interesting to me, but it turns out every day, thousands of people from all over the world are digging through my code, reviewing years old changesets. On the one hand, wow, thanks, this is very flattering. On the other hand, what the heck is wrong with you?This has been building up for a while, and Iâ€™ve been intermittently developing and deploying countermeasures. Itâ€™s been a lot like solving a sliding block puzzle. Lots of small moves and changes, and eventually it starts coming together.My primary principle is that Iâ€™d rather not annoy real humans more than strictly intended. If thereâ€™s a challenge, it shouldnâ€™t be too difficult, but ideally, we want to minimize the number of challenges presented. You should never suspect that I suspected you of being an enemy agent.First measure is we only challenge on the deep URLs. So, for instance, I can link to the anticrawl repo no problem, or even the source for anticrawl.go, and thatâ€™ll be served immediately. All the pages any casual browser would visit make up less than 1% of the possible URLs that exist, but probably contain 99% of the interesting content.Also, these pages get cached by the reverse proxy first, so anticrawl doesnâ€™t even evaluate them. Weâ€™ve already done the work to render the page, and weâ€™re trying to shed load, so why would I want to increase load by generating challenges and verifying responses? It annoys me when I click a seemingly popular blog post and immediately get challenged, when Iâ€™m 99.9% certain that somebody else clicked it two seconds before me. Why isnâ€™t it in cache? We must have different objectives in what weâ€™re trying to accomplish. Or who weâ€™re trying to irritate.The next step is that anybody loading style.css gets marked friendly. Big Basilisk doesnâ€™t care about my artisanal styles, but most everybody else loves them. So if you start at a normal page, and then start clicking deeper, thatâ€™s fine, still no challenge. (Sorry lynx browsers, but donâ€™t worry, itâ€™s not game over for you yet.)And then letâ€™s say somebody directly links to a changeset like /r/vertigo/v/b5ea481ff167. The first visitor will probably hit a challenge, but then we record that URL as in use. The bots are shotgun crawling all over the place, but if a single link is visited more than once, Iâ€™ll assume itâ€™s human traffic, and bypass the challenge. No promises, but clicking that link will mostly likely just return content, no challenge.The very first version of anticrawl relied on a weak POW challenge (find a SHA hash with first byte 42), just to get something launched, but this does seem counter intuitive. Why are we making humans solve a challenge optimized for machines? Instead I have switched to a much more diabolical challenge. You are asked how many Rs in strawberry. Or maybe something else. To be changed as necessary. But really, the key observation is that any challenge, anything at all, easily sheds like 99.99% of the crawling load.Notably, because the challenge does not include its own javascript solver, even a smart crawler isnâ€™t going to solve it automatically. If you include the solution on the challenge page, at least some bots are going to use it. All anticrawl challenges now require some degree of contemplation, not just blind interpretation.It took a few iterations because the actual deployment involves a few pieces. I had to reduce the style.css cache time, so that visitors would periodically refresh it (and thus their humanity). And then exclude it from the caching proxy, so that the request would be properly observed. Basically, a few minutes tinkering now and then while I wait for my latte to arrive, and now I think Iâ€™ve gotten things to the point where itâ€™s unlikely to burden anybody except malignant crawlers.elsewhereI have focused my bot detection efforts on humungus because the ratio of crawler to legit traffic was out of control. But now that I know what to look for, I see the same patterns scraping everywhere else. Seems really unlikely a worldwide colelctive of Opera users is suddenly interested in my old honks. Iâ€™m starting to deploy similar countermeasures.appendixSome log samples. Thereâ€™s always somebody to insist these could be real humans, and I have somehow misjudged them. Make your own decision.logs136.158.49.199 810.886Âµs humungus.tedunangst.com [2025/09/07 11:31:06] "GET /r/old-flak/v/d720c11fbb57 HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"
179.42.10.152 831.235Âµs humungus.tedunangst.com [2025/09/07 11:31:31] "GET /r/flak/v/8b31c923ca0f HTTP/1.1" 402 900 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36"
43.128.250.84 863.565Âµs humungus.tedunangst.com [2025/09/07 11:31:32] "GET /r/vertigo/v/71df18bb3819 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
78.182.153.38 639.086Âµs humungus.tedunangst.com [2025/09/07 11:31:46] "GET /r/gerc/v/692abbdefe18 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"
119.28.100.182 525.152Âµs humungus.tedunangst.com [2025/09/07 11:31:47] "GET /r/honk3/v/462b7440c563 HTTP/1.1" 402 959 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
185.163.26.50 678.609Âµs humungus.tedunangst.com [2025/09/07 11:31:49] "GET /r/azorius/v/b48a3aa3e060 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
43.167.157.150 758.749Âµs humungus.tedunangst.com [2025/09/07 11:32:01] "GET /r/humungus/v/0442f94c95fc HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
43.134.75.63 574.875Âµs humungus.tedunangst.com [2025/09/07 11:32:03] "GET /r/azorius/v/969314b9f388 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36" 
119.28.203.219 497.67Âµs humungus.tedunangst.com [2025/09/07 11:32:04] "GET /r/gerc/v/8ddbf7307214 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
43.128.84.91 727.05Âµs humungus.tedunangst.com [2025/09/07 11:32:06] "GET /r/vertigo/v/eb31940f6fa2 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
178.163.207.155 566.9Âµs humungus.tedunangst.com [2025/09/07 11:32:09] "GET /r/lua-tedu/v/300e67089469 HTTP/1.1" 402 904 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
181.120.225.184 561.48Âµs humungus.tedunangst.com [2025/09/07 11:32:12] "GET /r/azorius/v/43120b8aac5a HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
150.109.20.69 612.716Âµs humungus.tedunangst.com [2025/09/07 11:32:13] "GET /r/gojxl/v/tip/f/pool.go HTTP/1.1" 404 19 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
49.51.170.84 629.056Âµs humungus.tedunangst.com [2025/09/07 11:32:27] "GET /r/gerc/v/41b8b28ee893 HTTP/1.1" 402 958 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
5.62.146.6 843.157Âµs humungus.tedunangst.com [2025/09/07 11:32:33] "GET /r/honk/v/f6b8a7bee881 HTTP/1.1" 402 900 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36"
129.226.112.105 595.173Âµs humungus.tedunangst.com [2025/09/07 11:32:40] "GET /r/azorius/v/6179155ac315 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36"
43.167.204.48 514.371Âµs humungus.tedunangst.com [2025/09/07 11:32:42] "GET /r/vertigo/v/fae0082c32c2 HTTP/1.1" 402 961 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"
43.132.108.190 715.327Âµs humungus.tedunangst.com [2025/09/07 11:32:56] "GET /r/vertigo/v/8b5fcd06f8c6 HTTP/1.1" 402 903 "" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36"The big brain solution is you just cache all these requests, but unfortunately I have slightly less than 2TB RAM. Dealing with these relentless scans renders the cache for actual content less useful because everything gets LRUd out.Take a look at this guy. Apparently a pro starcraft player has taken up speed browsing as a side hustle, middle clicking ten times per second. These links arenâ€™t even on the same page, so heâ€™s switching tabs between clicks, too. Amazing. But he doesnâ€™t solve a single challenge. I thought gamers liked puzzles? Maybe thatâ€™s why this totally real human quit gaming.logs46.183.108.190 1.361957ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/a0824eb087c7 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 2.450003ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/1a4d35ff94ef HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.049854ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/azorius/v/9ca7ed390641 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.215804ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/67e77258e203 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 3.26703ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/honk/v/2fc6f904deaa HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 830.924Âµs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/1437b0d26457 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 1.23004ms humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/honk/v/945572a3b51d HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 789.496Âµs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/63f9c1f17606 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 841.414Âµs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/b5711a883e66 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 916.233Âµs humungus.tedunangst.com [2025/07/08 13:41:32] "GET /r/humungus/v/a5307922b3f5 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 565.518Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/ab1e84cac5e6 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 574.083Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/a9043d011e41 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 602.026Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/1cc1393b6832 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 497.831Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/4d53be2bdbd5 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 516.365Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/honk/v/302e58335796 HTTP/1.1" 402 1805 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 614.239Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/1a4d35ff94ef HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 530.161Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/a0824eb087c7 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 625.91Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/azorius/v/9ca7ed390641 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 757.497Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/humungus/v/67e77258e203 HTTP/1.1" 402 1809 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"
46.183.108.190 841.494Âµs humungus.tedunangst.com [2025/07/08 13:41:33] "GET /r/vertigo/v/6b3ffb3b21f5 HTTP/1.1" 402 1808 "" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:140.0) Gecko/20100101 Firefox/140.0"I think thereâ€™s a common perception that 10 req/s just isnâ€™t that much, based on some simple benchmarks. But that doesnâ€™t account for TLS, etc. 10 handshakes/s requires a bit more juice than GET hello. Iâ€™ve worked to keep response times in the low millisecond range, just seems like good sense, but I think people should be allowed to program in slower languages and frameworks. You shouldnâ€™t need a fairly substantial EPYC server like I have, either.And thereâ€™s always stuff happening in the background. Mastodon hits me once a second every time anybody deletes something. Lemmy hits me twice a second every time somebody likes anything. Thereâ€™s a bunch of nitwits with misconfigured RSS readers. A 4x overhead in one area doesnâ€™t matter, but a 1/4 as powerful CPU, with 1/4 as many cores, and 1/4 as fast language, all of which are entirely realistic, and pretty soon weâ€™re running close to the edge.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[K2-Think: A Parameter-Efficient Reasoning System]]></title>
            <link>https://arxiv.org/abs/2509.07604</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224219</guid>
            <description><![CDATA[K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at k2think.ai, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine.]]></description>
            <content:encoded><![CDATA[
    
    
    Authors:Zhoujun Cheng, Richard Fan, Shibo Hao, Taylor W. Killian, Haonan Li, Suqi Sun, Hector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie, Xudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang, Anze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs, Xuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu, Eric P. Xing            
    View PDF
    HTML (experimental)
            Abstract:K2-Think is a reasoning system that achieves state-of-the-art performance with a 32B parameter model, matching or surpassing much larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system shows that smaller models can compete at the highest levels by combining advanced post-training and test-time computation techniques. The approach is based on six key technical pillars: Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic planning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized Hardware, all using publicly available open-source datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art scores on public benchmarks for open-source models, while also performing strongly in other areas such as Code and Science. Our results confirm that a more parameter-efficient model like K2-Think 32B can compete with state-of-the-art systems through an integrated post-training recipe that includes long chain-of-thought training and strategic inference-time enhancements, making open-source reasoning systems more accessible and affordable. K2-Think is freely available at this http URL, offering best-in-class inference speeds of over 2,000 tokens per second per request via the Cerebras Wafer-Scale Engine.
    

    
    
              
          Comments:
          To access the K2-Think reasoning system, please visit this https URL
        

          Subjects:
          
            Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2509.07604 [cs.LG]
        
        
          Â 
          (or 
              arXiv:2509.07604v1 [cs.LG] for this version)
          
        
        
          Â 
                        https://doi.org/10.48550/arXiv.2509.07604
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Taylor Killian [view email]          [v1]
        Tue, 9 Sep 2025 11:25:55 UTC (3,106 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[QGIS is a free, open-source, cross platform geographical information system]]></title>
            <link>https://github.com/qgis/QGIS</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224156</guid>
            <description><![CDATA[QGIS is a free, open source, cross platform (lin/win/mac) geographical information system (GIS) - qgis/QGIS]]></description>
            <content:encoded><![CDATA[







QGIS is a full-featured, user-friendly, free-and-open-source (FOSS) geographical information system (GIS) that runs on Unix platforms, Windows, and MacOS.


Features

1. Flexible and powerful spatial data management
2. Beautiful cartography
3. Advanced and robust geospatial analysis
4. Powerful customization and extensibility
5. QGIS Server


Under the hood

Versions and release cycle
Free and Open Source


Installing and using QGIS

Documentation
Help and support channels


Get involved with the community

Features
1. Flexible and powerful spatial data management

Support for raster, vector, mesh, and point cloud data in a range of industry-standard formats

Raster formats include: GeoPackage, GeoTIFF, GRASS, ArcInfo binary and ASCII grids, ERDAS Imagine SDTS, WMS, WCS, PostgreSQL/PostGIS, and other GDAL supported formats.
Vector formats include: GeoPackage, ESRI shapefiles, GRASS, SpatiaLite, PostgreSQL/PostGIS, MSSQL, Oracle, WFS, Vector Tiles and other OGR supported formats.
Mesh formats include: NetCDF, GRIB, 2DM, and other MDAL supported formats.
Point-cloud format: LAS/LAZ and EPT datasets.


Data abstraction framework, with local files, spatial databases (PostGIS, SpatiaLite, SQL Server, Oracle, SAP HANA), and web services (WMS, WCS, WFS, ArcGIS REST) all accessed through a unified data model and browser interface, and as flexible layers in user-created projects
Spatial data creation via visual and numerical digitizing and editing, as well as georeferencing of raster and vector data
On-the-fly reprojection between coordinate reference systems (CRS)
Nominatim (OpenStreetMap) geocoder access
Temporal support

Example: Temporal animation

Example: 3D map view

2. Beautiful cartography

Large variety of rendering options in 2D and 3D
Fine control over symbology, labeling, legends and additional graphical elements for beautifully rendered maps
Respect for embedded styling in many spatial data sources (e.g. KML and TAB files, Mapbox-GL styled vector tiles)
In particular, near-complete replication (and significant extension) of symbology options that are available in proprietary software by ESRI
Advanced styling using data-defined overrides, blending modes, and draw effects
500+ built-in color ramps (cpt-city, ColorBrewer, etc.)
Create and update maps with specified scale, extent, style, and decorations via saved layouts
Generate multiple maps (and reports) automatically using QGIS Atlas and QGIS Reports
Display and export elevation profile plots with flexible symbology
Flexible output direct to printer, or as image (raster), PDF, or SVG for further customization
On-the-fly rendering enhancements using geometry generators (e.g. create and style new geometries from existing features)
Preview modes for inclusive map making (e.g. monochrome, color blindness)

Example: Map of Bogota, Colombia in the style of Starry Starry Night, by AndrÃ©s Felipe Lancheros SÃ¡nchez

For more maps created with QGIS, visit the QGIS Map Showcase Flickr Group.

3. Advanced and robust geospatial analysis

Powerful processing framework with 200+ native processing algorithms
Access to 1000+ processing algorithms via providers such as GDAL, SAGA, GRASS, OrfeoToolbox, as well as custom models and processing scripts
Geospatial database engine (filters, joins, relations, forms, etc.), as close to datasource- and format-independent as possible
Immediate visualization of geospatial query and geoprocessing results
Model designer and batch processing

Example: Travel isochrones

Example: Model designer

4. Powerful customization and extensibility

Fully customizable user experience, including user interface and application settings that cater to power-users and beginners alike
Rich expression engine for maximum flexibility in visualization and processing
Broad and varied plugin ecosystem that includes data connectors, digitizing aids, advanced analysis and charting tools,
in-the-field data capture, conversion of ESRI style files, etc.
Style manager for creating, storing, and managing styles
QGIS style hub for easy sharing of styles
Python and C++ API for standalone (headless) applications as well as in-application comprehensive scripting (PyQGIS)

Example: Style manager

Example: Plugins


5. QGIS Server
Headless map server -- running on Linux, macOS, Windows, or in a docker container -- that shares the same code base as QGIS.

Industry-standard protocols (WMS, WFS, WFS3/OGC API for Features and WCS) allow plug-n-play with any software stack
Works with any web server (Apache, nginx, etc) or standalone
All beautiful QGIS cartography is supported with best-in-class support for printing
Fully customizable with Python scripting support

Example: QGIS server WMS response

Example: QGIS server WFS response

Under the hood
QGIS is developed using the Qt toolkit and C++, since 2002, and has a pleasing, easy to use graphical
user interface with multilingual support. It is maintained by an active developer team and supported by vibrant
community of GIS professionals and enthusiasts as well as geospatial data publishers and end-users.
Versions and release cycle
QGIS development and releases follow a time based schedule/roadmap. There are three main branches of QGIS that users can install. These are the Long Term Release (LTR) branch, the Latest Release (LR) branch, and the Development (Nightly) branch.
Every month, there is a Point Release that provides bug-fixes to the LTR and LR.
Free and Open Source
QGIS is released under the GNU Public License (GPL) Version 2 or any later version.
Developing QGIS under this license means that you can (if you want to) inspect
and modify the source code and guarantees that you, our happy user will always
have access to a GIS program that is free of cost and can be freely
modified.
QGIS is part of the Open-Source Geospatial Foundation (OSGeo), offering a range of complementary open-source GIS software projects.
Installing and using QGIS
Precompiled binaries for QGIS are available at the QGIS.org download page.
Please follow the installation instructions carefully.
The building guide can be used to get started with building QGIS from source.
For installation of QGIS Server, see its getting started documentation.
Documentation
A range of documentation is available. This includes:

Training Manual
QGIS User Guide
QGIS Server Guide
Visual Changelog
Documentation Guidelines
QGIS Python (PyQGIS) Cookbook
QGIS Python (PyQGIS) API
QGIS C++ API
Developers Guide

Help and support channels
There are several channels where you can find help and support for QGIS:

Using the QGIS community site
Joining the qgis-users mailing list
Chatting with other users real-time. Please wait around for a response to your question as many folks on the channel are doing other things and it may take a while for them to notice your question. The following paths all take you to the same chat room:

Using an IRC client and joining the
#qgis channel on irc.libera.chat.
Using a Matrix client and joining the #qgis:osgeo.org room.


At the GIS stackexchange or r/QGIS reddit, which are not maintained by the QGIS team, but where the QGIS and broader GIS community provides lots of advice
Other support channels

Get involved with the community
Contribution guidelines for this project
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vector database that can index 1B vectors in 48M]]></title>
            <link>https://www.vectroid.com/blog/why-and-how-we-built-Vectroid</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224141</guid>
            <description><![CDATA[Discover why we built Vectroid, a serverless vector database that delivers exceptional accuracy and low latency without compromising on cost. Learn about our approach to solving the traditional tradeoffs between speed, accuracy, and cost in vector search.]]></description>
            <content:encoded><![CDATA[We are excited to announce Vectroid, a serverless vector search solution that delivers exceptional accuracy and low latency in a cost effective package. Vectroid is not just another vector search solutionâ€”itâ€™s a search engine that performs and scales in all scenarios.Why we built VectroidTalk to any team working with large, low latency vector workloads and youâ€™ll hear a familiar story: something always has to give. Vector databases often make significant tradeoffs between speed, accuracy, and cost. Thatâ€™s the nature of the mathematical underpinnings of vector search worksâ€”taking algorithmic shortcuts to get near-perfect results in a short amount of time.There are some common permutations of these tradeoffs:Very high accuracy, but very expensive and slow
Fast speed and tolerable accuracy, but very expensive
Cheap and fast, but inaccurate to a disqualifying degree
Based on the existing vector database landscape, it would seem that building a cost effective vector database requires sacrificing either speed or accuracy at scale. We believe thatâ€™s a false pretense: building a cost-efficient database is possible with high accuracy and low latency. We just need to rethink our underlying mechanism.Our â€œahaâ€ momentQuery speed and recall are largely a function of the chosen ANN algorithm. Algorithms which are both fast and accurate like HNSW (Hierarchical Navigable Small Worlds) are memory intensive and expensive to index. The traditional assumption is that these types of algorithms are untenable for a cost-conscious system.We had two major realizations which challenged this assumption.Demand for in-memory HNSW is not static. Real world usage patterns are bursty and uneven. A cost efficient database can optimize for this reality by making resource allocation dynamic and by individually scaling system components as needed.
HNSWâ€™s memory footprint is tunable. It can be easily be flattened (ex. by compressing vectors using quantization) and expanded (ex. by increasing layer count), which gives us the flexibility to experiment with different configurations to find a goldilocks setup.

What is Vectroid?Vectroid is a serverless vector database with premium performance. It delivers the same or stronger balance of speed and recall promised by high-end offerings, but costs less than competitors.Performant vector search: HNSW for ultra fast, high recall similarity search.
Near real-time search capabilities: Newly ingested records are searchable almost instantly.
Massive scalability: Seamlessly handles billions of vectors in a single namespace.
Cost efficient resource utilization: Scaling each layer (ingest, index, query) separately.

How Vectroid performsThe core philosophy of Vectroid is that optimizing for one metric at any cost to the others doesnâ€™t make for a robust system. Instead, vector search should be designed for balanced performance across recall, latency, and cost so users donâ€™t have to make painful tradeoffs as workloads grow.When tested against other state-of-the-art vector search, Vectroid is not only competitive but the most consistent across the board. Across all of our tests, Vectroid is the only databases that was able to maintain over 90% recall while scaling to 10 query threads per secondâ€”all while maintaining good latency scores.Some early benchmarks:Indexed 1B vectors (Deep1B) in ~48 minutes
Achieved P99 latency of 34ms on the MS Marco 138M vector / 1024 dimensions dataset
Weâ€™ll be releasing the full benchmark suite (with setup details so anyone can reproduce them) in an upcoming post. For now, these numbers highlight the kind of scale and performance we designed Vectroid to handle.How Vectroid worksVectroid is composed of two independently scalable microservices for writes and reads.As the diagram shows, index state, vector data, and metadata are persisted to cloud object storage (GCS for now, S3 coming soon). Disk, cache, and in-memory storage layers each employ a usage-aware model for index lifecycle in which indexes are lazily loaded from object storage on demand and evicted when idle.For fast, high-recall ANN search, we chose the HNSW algorithm. It offers excellent latency and accuracy tradeoffs, supports incremental updates, and performs well across large-scale workloads. To patch its limitations, we added a number of targeted optimizations:Slow indexing speed â‡’ in-memory write buffer to ensure newly inserted vectors are immediately searchable
High indexing cost â‡’ batched, highly concurrent and partitioned indexing
High memory usage â‡’ vector compression via quantization
Final ThoughtsWeâ€™re just getting started. If youâ€™re building applications that rely on fast, scalable vector search (or youâ€™re running up against the limits of your current stack), weâ€™d love to hear from you. Start using Vectroid today or sign up for our newsletter to follow along as we continue building.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VaultGemma: The most capable differentially private LLM]]></title>
            <link>https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45223726</guid>
            <description><![CDATA[As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. Differential privacy (DP) offers a mathematically robust solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional scaling laws â€” rules describing performance dynamics â€” by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of input prompts sent to the model simultaneously for processing) and computation costs.]]></description>
            <content:encoded><![CDATA[
                    
                    
    



    




    As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. Differential privacy (DP) offers a mathematically robust solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional scaling laws â€” rules describing performance dynamics â€” by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of input prompts sent to the model simultaneously for processing) and computation costs.Our new research, â€œScaling Laws for Differentially Private Language Modelsâ€, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, weâ€™re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on Hugging Face and Kaggle, alongside a technical report, to advance the development of the next generation of private AI.


                    
                    
    



    




    Understanding the scaling lawsWith a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the "noise-batch ratioâ€ which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-lawsâ€“style queries, such as, â€œFor a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?â€


                    
                    
    




                    
                    
    



    




    Key findings: A powerful synergyBefore diving into the full scaling laws, itâ€™s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective â€” i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget (FLOPs) or data budget (tokens).


                    
                    
    




                    
                    
    



        
  To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.

    

                    
                    
    




                    
                    
    



        
  This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations â€”Â i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.

    

                    
                    
    



    




    Applying the scaling laws to build VaultGemmaThe Gemma models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.Algorithmic advancements: Training at scaleThe scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of Poisson sampling, which is a central component of DP-SGD. We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on Scalable DP-SGD, which allows us to process data in fixed-size batches â€” either by adding extra padding or trimming them â€” while still maintaining strong privacy protections.


                    
                    
    



    




    ResultsArmed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.


                    
                    
    




                    
                    
    



    




    We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., HellaSwag, BoolQ, PIQA, SocialIQA, TriviaQA, ARC-C, ARC-E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that todayâ€™s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.Finally, the model comes with strong theoretical and empirical privacy protections.


                    
                    
    



    




    Formal privacy guaranteeIn general, both the privacy parameters (Îµ, Î´) and the privacy unit are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a sequence-level DP guarantee of (Îµ â‰¤ 2.0, Î´ â‰¤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the Gemma 2 model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, user-level differential privacy would be a better choice.What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.


                    
                    
    



    




    Empirical memorizationSequence-level DP provably bounds the influence of any single training sequence (example) on the final model. We prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.


                    
                    
    



    




    ConclusionVaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.While a utility gap still exists between DP-trained and non-DPâ€“trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.


                    
                    
    



    




    AcknowledgementsWe'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.


                    
                ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Advanced Scheme Techniques (2004) [pdf]]]></title>
            <link>https://people.csail.mit.edu//jhbrown/scheme/continuationslides04.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45223419</guid>
        </item>
        <item>
            <title><![CDATA[Doom-ada: Doom Emacs Ada language module with syntax, LSP and Alire support]]></title>
            <link>https://github.com/tomekw/doom-ada</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222993</guid>
            <description><![CDATA[Doom Emacs Ada language module with syntax highlighting, LSP and Alire support - tomekw/doom-ada]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    



  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  





    






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    0

  

  
        
            
          Star
          9

  



        

        


          

  
    


  

  




          


  
  
  Folders and filesNameNameLast commit messageLast commit dateLatest commitHistory2 CommitsLICENSELICENSEREADME.mdREADME.mdconfig.elconfig.eldoctor.eldoctor.elpackages.elpackages.elREADMEMIT licenseDoom Emacs Ada Module
This is a Doom Emacs :lang ada module providing:

Tree-sitter highlighting via ada-ts-mode
LSP support with ada_language_server
Autocomplete (company-capf)
Alire integration (alr build, alr run, alr clean)

Installation
Clone into your Doom modules folder:
git clone https://github.com/tomekw/doom-ada ~/.doom.d/modules/lang/ada
Enable in ~/.doom.d/init.el:
:lang
ada
Sync Doom:
doom sync
Restart Emacs.
Usage

SPC m b â†’ build with alr build
SPC m r â†’ run with alr run
SPC m c â†’ clean with alr clean

Errors are parsed into the compilation buffer, and eglot provides inline diagnostics and completions.
Requirements

Alire
Ada Language Server





      




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oq: Terminal OpenAPI Spec Viewer]]></title>
            <link>https://github.com/plutov/oq</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222799</guid>
            <description><![CDATA[Terminal OpenAPI Spec viewer. Contribute to plutov/oq development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[oq - a terminal-based OpenAPI Spec (OAS) viewer

Usage
oq openapi.yaml
# or
cat openapi.yaml | oq
# or
curl https://api.example.com/openapi.json | oq
Keyboard Shortcuts
Press ? to see the help screen with all available keyboard shortcuts.
OpenAPI Support
oq supports both modern major OpenAPI specification versions:

OpenAPI 3.0.x
OpenAPI 3.1.x

Both JSON and YAML formats are supported.
Installation
go install github.com/plutov/oq@latest
You can also download the compiled binaries from the Releases page.
From source
git clone git@github.com:plutov/oq.git
cd oq
go build -o oq .
License
MIT License - see LICENSE file for details.
Contributing
Contributions are welcome! Please feel free to submit issues and pull requests.
When contributing:

Ensure tests pass: go test -v
Test with both OpenAPI 3.0 and 3.1 examples

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Many hard LeetCode problems are easy constraint problems]]></title>
            <link>https://buttondown.com/hillelwayne/archive/many-hard-leetcode-problems-are-easy-constraint/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222695</guid>
            <description><![CDATA[Use the right tool for the job.]]></description>
            <content:encoded><![CDATA[
            
            
                
                
                September 10, 2025
                
                
            
            

            

            
            
                Use the right tool for the job.
            
            

            

            
            
            In my first interview out of college I was asked the change counter problem:

Given a set of coin denominations, find the minimum number of coins required to make change for a given number. IE for USA coinage and 37 cents, the minimum number is four (quarter, dime, 2 pennies).

I implemented the simple greedy algorithm and immediately fell into the trap of the question: the greedy algorithm only works for "well-behaved" denominations. If the coin values were [10, 9, 1], then making 37 cents would take 10 coins in the greedy algorithm but only 4 coins optimally (10+9+9+9). The "smart" answer is to use a dynamic programming algorithm, which I didn't know how to do. So I failed the interview.
But you only need dynamic programming if you're writing your own algorithm. It's really easy if you throw it into a constraint solver like MiniZinc and call it a day. 
int: total;
array[int] of int: values = [10, 9, 1];
array[index_set(values)] of var 0..: coins;

constraint sum (c in index_set(coins)) (coins[c] * values[c]) == total;
solve minimize sum(coins);

You can try this online here. It'll give you a prompt to put in total and then give you successively-better solutions:
coins = [0, 0, 37];
----------
coins = [0, 1, 28];
----------
coins = [0, 2, 19];
----------
coins = [0, 3, 10];
----------
coins = [0, 4, 1];
----------
coins = [1, 3, 0];
----------


Lots of similar interview questions are this kind of mathematical optimization problem, where we have to find the maximum or minimum of a function corresponding to constraints. They're hard in programming languages because programming languages are too low-level. They are also exactly the problems that constraint solvers were designed to solve. Hard leetcode problems are easy constraint problems.1 Here I'm using MiniZinc, but you could just as easily use Z3 or OR-Tools or whatever your favorite generalized solver is.
More examples
This was a question in a different interview (which I thankfully passed):

Given a list of stock prices through the day, find maximum profit you can get by buying one stock and selling one stock later.

It's easy to do in O(n^2) time, or if you are clever, you can do it in O(n). Or you could be not clever at all and just write it as a constraint problem:
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
var int: buy;
var int: sell;
var int: profit = prices[sell] - prices[buy];

constraint sell > buy;
constraint profit > 0;
solve maximize profit;

Reminder, link to trying it online here. While working at that job, one interview question we tested out was:

Given a list, determine if three numbers in that list can be added or subtracted to give 0? 

This is a satisfaction problem, not a constraint problem: we don't need the "best answer", any answer will do. We eventually decided against it for being too tricky for the engineers we were targeting. But it's not tricky in a solver; 
include "globals.mzn";
array[int] of int: numbers = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array[index_set(numbers)] of var {0, -1, 1}: choices;

constraint sum(n in index_set(numbers)) (numbers[n] * choices[n]) = 0;
constraint count(choices, -1) + count(choices, 1) = 3;
solve satisfy;

Okay, one last one, a problem I saw last year at Chipy AlgoSIG. Basically they pick some leetcode problems and we all do them. I failed to solve this one:

Given an array of integers heights representing the histogram's bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.


The "proper" solution is a tricky thing involving tracking lots of bookkeeping states, which you can completely bypass by expressing it as constraints:
array[int] of int: numbers = [2,1,5,6,2,3];

var 1..length(numbers): x; 
var 1..length(numbers): dx;
var 1..: y;

constraint x + dx <= length(numbers);
constraint forall (i in x..(x+dx)) (y <= numbers[i]);

var int: area = (dx+1)*y;
solve maximize area;

output ["(\(x)->\(x+dx))*\(y) = \(area)"]

There's even a way to automatically visualize the solution (using vis_geost_2d), but I didn't feel like figuring it out in time for the newsletter.
Is this better?
Now if I actually brought these questions to an interview the interviewee could ruin my day by asking "what's the runtime complexity?" Constraint solvers runtimes are unpredictable and almost always than an ideal bespoke algorithm because they are more expressive, in what I refer to as the capability/tractability tradeoff. But even so, they'll do way better than a bad bespoke algorithm, and I'm not experienced enough in handwriting algorithms to consistently beat a solver.
The real advantage of solvers, though, is how well they handle new constraints. Take the stock picking problem above. I can write an O(nÂ²) algorithm in a few minutes and the O(n) algorithm if you give me some time to think. Now change the problem to

Maximize the profit by buying and selling up to max_sales stocks, but you can only buy or sell one stock at a given time and you can only hold up to max_hold stocks at a time?

That's a way harder problem to write even an inefficient algorithm for! While the constraint problem is only a tiny bit more complicated:
include "globals.mzn";
int: max_sales = 3;
int: max_hold = 2;
array[int] of int: prices = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5, 8];
array [1..max_sales] of var int: buy;
array [1..max_sales] of var int: sell;
array [index_set(prices)] of var 0..max_hold: stocks_held;
var int: profit = sum(s in 1..max_sales) (prices[sell[s]] - prices[buy[s]]);

constraint forall (s in 1..max_sales) (sell[s] > buy[s]);
constraint profit > 0;

constraint forall(i in index_set(prices)) (stocks_held[i] = (count(s in 1..max_sales) (buy[s] <= i) - count(s in 1..max_sales) (sell[s] <= i)));
constraint alldifferent(buy ++ sell);
solve maximize profit;

output ["buy at \(buy)\n", "sell at \(sell)\n", "for \(profit)"];


Most constraint solving examples online are puzzles, like Sudoku or "SEND + MORE = MONEY". Solving leetcode problems would be a more interesting demonstration. And you get more interesting opportunities to teach optimizations, like symmetry breaking.

Update for the Internet
This was sent as a weekly newsletter, which is usually on topics like software history, formal methods, unusual technologies, and the theory of software engineering. You can subscribe here: 





Because my dad will email me if I don't explain this: "leetcode" is slang for "tricky algorithmic interview questions that have little-to-no relevance in the actual job you're interviewing for." It's from leetcode.com.Â â†©



            
            

            
            
            If you're reading this on the web, you can subscribe here. Updates are once a week. My main website is here.
My new book, Logic for Programmers, is now in early access! Get it here.
            
            

            





        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[3D modeling with paper]]></title>
            <link>https://www.arvinpoddar.com/blog/3d-modeling-with-paper</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45222369</guid>
            <description><![CDATA[Exploring the process of designing and assembling 3D models from paper.]]></description>
            <content:encoded><![CDATA[August 31, 2025

Over the past several years, I've enjoyed the hobby of paper
modeling (or papercraft), the art
of creating 3D models from cut and glued parts from paper sheets. This hobby is
a superset of origami, in that it allows for cutting and gluing, as well as for
multiple sheets of paper for a single model. The alleviation of these
constraints means that papercraft allows for more complex models that are easier
to assemble.
Over many years, I've built models designed by others as well as designed my
own. In this post, I want to share everything I've learned along the way,
covering the entire process from design to assembly.
I love this hobby for three reasons:

It is extremely accessible. There is no fancy hardware or software
involved. As we'll see, the core tools are paper, scissors, and glue;
everything else is an addon to make the experience better. All software tools
can be free. Accidentally messed up during assembly and need a replacement
part? Just print out another page. The entire creation of a model can be done
in the ballpark of a few cents.
It is equally technical and creative. As we'll see, many of the problems
faced in papercraft require an engineering-like approach and a willingness to
experiment and iterate on designs. While it may appear outwardly like a craft
project, the end-to-end process involves constraints and optimizing within
them.
There's no limits on what you can make. What you decide to build is
limited by your patience and imagination. Theoretically, nearly any object can
be represented as a paper model.

Let's dive in. My most recent model is a papercraft plane inspired by the SR-71
Blackbird, a
reconnaissance plane that to this day holds many records for being one of the
fastest aircrafts ever. It's now one of most iconic planes ever designed and an
engineering masterpiece. The program was ultimately retired in 1999.
The model we'll be desiging and assembling in this post
An actual SR-71 Blackbird, By USAF / Judson Brohmer - Armstrong Photo Gallery, Public Domain, https://commons.wikimedia.org/w/index.php?curid=30816
We're going to walk through the full model design and assembly process, while
referencing specific examples I encountered during creating this SR-71.
Constraints#
Let's set some constraints for how we're allowed to model our creation. These
are self-imposed limitations that fit my preferred-style for model design:

All parts in the assembled model must be made of paper.
Each part must be a single, solid color. The parts must not use any printed
textures or designs.
The model must be represented as a simple
polyhedron. There may be
no curvatures, holes, two-dimensional surfaces, or surface-to-surface contact.
If the figure we're trying to capture has any of these features, we must find
a way to approximate it using only flat faces. The object must be manifold (an
edge is only shared by 2 faces).

Why constraints?#
It may feel weird to impose constraints on an art. However, I find that these
constraints encourage a better designed model that can be assembled easily and
predictably, including by others.
Using features like curvatures, printing with textures, etc. are shortcuts. For
example, printing textures helps fill in details that aren't captured inherently
by the model; curvatures and 2d surfaces are flimsy and introduce variances in
how a model can be assembled. Simply polyhedral designs with single color parts
ensure that the 3D form itself captures the object being depicted, and can be
assembled in a structurally sound, predictable way.
Goals#
In addition to constraints, we also have some goals that we're optimzing for.
These goals will be considered in each step of our design process.

Ease of assembly: By far the most important goal, our model should be
easy to put together. Given the nature of paper and glue, a model that is
difficult to assemble will almost certainly look bad. A model can have a
well-designed topology, but still be difficult to assemble based on the parts
design we put together.
Aesthetic appeal: This is an art, after all. The model we design should
be aesthetically pleasing and resemble the object of interest.
Minimal consumption of resources: We should aim to minimize waste and use
our materials efficiently.

As in engineering, we have to consider trade-offs between these goals, and
optimize for these goals within our constraints.
Steps#
The process of designing a paper model is iterative. Each iteration consists of
the following steps:

Mesh modeling - using software to create a 3D polyhedron mesh of our desired form
Mesh unfolding - unfolding the mesh into a 2D layout of parts
Assembly - putting the parts together to create the final model

The remainder of this article will be walking through each step in detail. The
discussion of each step will be centered around the goals and constraints
declared from above.
Mesh Modeling#
Related goals: Ease of assembly, aesthetic appeal
In this phase, we design the mesh for our model. We aim to capture the essence
of an object in a way that can feasibly be built with paper. Depending on how
you approach this, this can easily be the most complicated step.
What do I mean by "feasibly built with paper"? Our mesh is a collection of
polygons that represent a 3D object. The closeness of that representation is
largely determined by how many polygons we use. We could use many really small
polygons to closely match the subtle curves of our plane, but this would be hard
to assemble in reality. Alternatively, we could simplifiy our representation
down to a triangular pyramid. This would be trivially easy to assemble, but it
wouldn't look a lot like our plane.
We can now see that our goals of ease of assembly and aesthetic appeal are at
odds. Imagine that we have a continuum, where on the left we have a triangular
pyramid (the simplest possible polyhedron) and on the right we have a mesh of
the SR-71 with an arbitrarily high number (millions) of polygons.
Our mesh can exist anywhere between the simplest polyhedron and a mesh with near perfect resolution. The example on the right is by USSIowa on Thingiverse (Creative Commons - Attribution): https://www.thingiverse.com/thing:5508640
Generally, an "easy" to assemble model will have somewhere around a few hundred
polygons. Thus, our ideal model exists somewhere on the far left of this
spectrum.
The challenge here is what I call "allocation of resolution" - we have a finite
number of polygons to distribute across the features of our object. Certain
features will naturally require more polygons to be accurately captured than
others. For example, curved features require more polygons than flat features -
in this model, the cylindrical engines will require more detail, than say, the
flat wings.
In addition to the number of polygons and their concentrations, the arrangement
of the polygons themselves matters - this is the topology of the mesh. Most
discourse on 3D mesh topology is related to shading and animation. For our
purposes, we're considered with ease of assembly. Certain topologies are easier
to assemble and more structurally sound. Generally, here's some positive
topological qualities for papercraft:

Symmetries: a good mesh design is symetrical when possible. Symmetrical shapes
are intuitive and easier to reason about when assembling.
No narrow shapes: really narrow shapes are hard to cut out, hard to fold, and
hard to glue. Avoid them at all costs.
Use quads: quad faces have an aesthetic appeal to them.

If all of this is sounding hard, we've got some options, in increasing order of
difficulty:
Easy: Use an existing mesh#
The easiest way past this step is to find an existing mesh. There's a whole
genre of 3D modeling called "low-poly" that you can find with a quick search on
Thingiverse or
Printables. These are
usually designed for video games or 3D printing, but can be taken up for
papercraft.
Medium: Converting an existing mesh#
Sometimes, you can find a high-resolution mesh of your desired object, but not a
low-poly one. In this case, there are tools available to reduce the polygon
count while preserving the overall shape. This is called "mesh simplification"
or "mesh decimation."
This Instructable
goes over the process of doing this with Meshlab,
but there's many other software alternatives out there.
The pitfall of this approach is that automatic mesh decimation typically results
in some nasty topologies, and there's not a lot you can do to control the
output. To get around this, we could add an additional refinement step where
we take the raw decimated mesh output and "clean it up" using a mesh editor
software.
As an example, let's try this with a SR-71 mesh on
Thingiverse. The original mesh has
more than 1.2 million faces, and we're going to try decimating down to ~1,000.
Here's what we get from Meshlab:
Result of mesh decimation in Meshlab
In this case, the output is not usable - it's wildly asymetric and is full of
self-intersections. Refining this topology would take just as long (if not
longer) as creating a model from scratch.
Hard: Creating your own mesh#
The most difficult option is to create your own mesh from scratch. This option
gives you full control over the design, and is what I chose for the SR-71 model.
My software of choice for this is Blender. Blender
has a steep learning curve, but the type of mesh design we're doing for this
project doesn't begin to scratch the surface of its full capabilities. I highly
recommend this low-poly tutorial
if you've never used Blender before and need somewhere to start. Two things I
found very handy were the mirror
modifier
to enforce symmetry, and the 3D Print
Toolbox
to auto-cleanup the mesh and check for manifoldness.
This process is very tedious. My advice here is: simplify your mesh to the point
where you feel uncomfortable. Recall that we're largely optimizing for ease of
assembly. When modeling, it's very tempting to capture finer details, but fine
details have costs (small parts, hard to glue regions, etc.) that are not worth
it during the assembly phase. Scrutinize every feature, and zoom out once in a
while. When you zoom out, your omissions won't feel as weird.
After many days, here's the initial mesh I created. It contains 732 triangles.
Note the symmetry along the y-axis.
Initial mesh created with Blender
Mesh Unfolding#
Related goals: Ease of assembly, minimal consumption of resources
Once we have a mesh, we have to convert it into a 2D template of parts that can
be printed and assembled. This process is called unfolding. Each of the
faces of our mesh are grouped into parts, and the arrangement of our parts
is a layout, or template.
To do this, we're going to turn to software again. The most popular unfolding
tool (and my favorite) is Pepakura
Designer. Pepakura is not free (at the
time of this writing, it's a one time $70 purchase) and it only runs on
Windows. There's also Unfolder for Mac, which is
$30. If you can't use either of these, Blender can save the day again with its
free Paper Model
plugin.
I believe that the unfolding step is one that does not get as much attention as
it deserves. There is a noticable difference between a good template and a bad
one. A good template has parts that make intuitive sense, with logical groupings
and clear flow. The faces themselves are grouped into parts that are easy to cut
out and handle. All of this equates to a better building experience, which means
a better looking model.
Part of unfolding is also deciding the scale of your model. You can make your
model as big or small as you want, but again, ease of assembly should be top of
mind when deciding. A model that's too small will end up with parts that are
hard to cut out and fold. Bigger models are easier to assemble, but you're
limited to the point where the faces of your model must fit on a page.
I ended up making this model 25 inches long. With the original SR-71 being about
107 feet long, this puts our model at around a 1:50 ratio.
Creating many parts#
Let's start off with the creation of parts. In most unfolding software, the
software will auto-unfold for you, and from there you can regroup faces into
whatever parts you want. Here's Pepakura's auto unfold:
Pepakura default unfolding produces complex parts
The parts it generated are pretty complicated, so we have some work to do.
If you have a mesh with nn faces, you can have anywhere from 1 (all the faces
in a single part) to nn total parts (each part is a single face). We want our
model to be easy to assemble, and neither of these extremes are easy.
Rather than trying to fix the number of parts and going from there, I recommend
creating parts that are logical. Identify features that can be captured in a
single part, and go from there. For example, in the SR-71, each engine intake
spike makes sense as a single part. So does the nose cone.
If your mesh has an axis of symmetry, then your parts have symmetrical pairings
as well. The same feature on either side of the axis should be represented with
a mirrored part. In the SR-71, the entire plane is symmetrical on the vertical
axis, so all parts across this axis are mirrored. This is good because once
someone builds one side, they can more easily reason about the other side.
I ended up dividing this model into 42 parts. These parts were carefully divided
in such a way that I felt would make them easier to assemble. If you look at any
part in particular, chances are it'll have a symmetric counterpart.
Finalized parts for the layout
They're arranged pretty haphazardly right now, but we'll cleanup this up in the
next step.
Arranging the parts#
Again, most software will automatically arrange the parts for you as part of
unfolding. Here's the 14 page arrangement Pepakura decided for the parts I created:
Auto arrangement of parts by Pepakura
I highlighted all the parts on the first two pages so you can see where the are
on the finished model. Notice that they're scattered throughout different
sections. That's why I typically don't like auto-arrangement - they're designed
to minimize paper usage, but they often result in a less intuitive assembly
process. You can't look at any particular page and loosely know where its parts
will go.
A good part layout reads like a story. Parts are arranged in a logical
order, with related parts grouped together. I like to arrange mine left to
right, top to bottom on a page. Here's my layout, with the first two pages
highlighted.
Manual arrangement of parts, which now has logical groupings
All the parts that are near each other in the layout are also near each other in
the final assembly. In this case, I even was able to reduce the page count down
to 12 from the starting 14.
Flap structure#
Flaps, or tabs, are the appendages on each part that allow for gluing
parts together. Each flap has a singular counterpart edge that it's glued to -
this is known as an edge/flap pair. Most software will auto-assign a shared
number between an edge and its flap to make identify pairs easy during the
assembly process.
Two edge/flap pairs, with arrows pointing to their matching number IDs
For an edge/flap pair, most unfolding software will allow us to swap the flap
across parts. Doing this strategically is critical for creating an easy to
assemble model, and also has implications for the structural integrity of the
final build.
For example, consider the two example parts shown above. These two parts that
meet at two shared edges, so these parts have two edge/flap pairs between them.
We could arrange the flaps so that one part has both of them:
Arranging both flaps on the same side
We could also interlace the flaps, so each part has one flap on each side.
Arranging flaps as interlaced, with one flap on each part
Interlacing flaps between parts can create a more stable structure, since
there's only one way for the parts to meet. If two flaps are on the same side,
they can over-extend when glued to the edge. That being said, same-side flaps
can be easier to work with, especially when reaching the closing stages of a
model.
In general, I like to using interlaced flaps wherever possible to create an
overall stronger model, and use same-side flaps selectively.
Once we have an arrangement we like, we can export our layout as a PDF.
Assembly#
With our layout PDF ready, we can now print it and move on to assembly. We'll
finally get to see our design come to life.
Materials and Tools#
For our materials, we'll need:

65lb (176 g/m2g/m^2) cardstock: This is the ideal paper weight for creating
sturdy models, while still being thin/flexible enough to pass through a normal
printer and be easy to fold.
Adhesive. My recommended adhesive is tacky glue: it's strong, dries clear, but
is forgiving enough to allow for repositioning during assembly. Specifically,
I use Aleene's Original Tacky
Glue.
I've also had past success with a glue stick.

We'll also need some tools, which I've listed these in order of importance. The
ones with asterisks are essential. Everything else is a nice-to-have.

Printer*: You'll need access to a printer to print the template on the
cardstock. Laser jet printers are great because the prints don't smudge.
Cutting tools*: You'll need a pair of scissors or a craft knife to cut
out the parts. Use sharp tools for clean cuts - it makes a difference.
Ruler*: Cutting/scoring perfectly straight lines is a must. Steel rulers
are great for their consistent edge, and they don't catch against your tools.
That being said, I used a clear plastic ruler for this model. Being able to
see through the ruler helps with alignment.
Scoring tool*: This will help you prepare a part for folding. You can use
a bone folder or scoring wheel. I use an embossing tool I found at a dollar
store, but before that, I used a ballpoint pen than ran out of ink. Anything
with a precise (but not too sharp) tip will do.
Toothpicks: I use toothpicks to spread blobs of glue into thin layers and
get into tight spaces.
Assembly surface: A cutting mat or piece of cardboard will protect your
work surface and give you a stable surface to cut/score your parts.
Tweezers: Tweezers are helpful for handling small parts and getting into
tight spaces, especially while holding parts together as glue dries.

If you want to get fancy, you can also purchase an automatic cutting machine,
like a Cricut or
Silhouette. These machines can precisely
cut/score your parts from cardstock. Getting the template into their software
takes some extra effort, but it results in the best quality parts. I did not use
a machine for this project.
To match the real SR-71, I printed my template on black cardstock. Darker
cardstocks are harder to work with because of the low contrast between the ink
and the paper itself. If you're new to the hobby, I would recommend starting
with a lighter color.
Assembly phases#
The assembly of a model has 4 steps:

Cutting: Cutting the parts out from the paper with your cutting tool of
choice. Scissors are quicker, but the combination of ruler and craft knife
results in cleaner cuts.
Scoring: Running a scoring tool over fold lines to get cleaner folds. This
may be tempting to skip, but I cannot emphasize the importance of this step
enough. Scoring is especially important when dealing with thicker paper.
Folding: Folding the parts in prep for gluing. There's only two types of
folds: mountain folds and valley folds.
Gluing: Gluing the parts together.

How you decide to batch these steps is up to you. For example, you could cut all
the parts out at once, then score all of them, etc. This approach is effective
because you can develop a rhythm by doing each phase only once, so you're not
constantly switching between tools; the downside is that you only get to start
assembly after a pretty lengthy process. Alternatively, you can do it per part:
cut one part out, score it, fold it, and secure it to the assembly. Here, the
pros and cons are flipped: you get to see the model come together quicker, but
there's a lot of context switching between phases. I've tried both of these
approaches, and find that the latter results in a non-negligible increase in the
assembly time of the model.
To strike a balance, the approach I took for this model was performing the
phases at the granularity of sections (engines, wings, fuselage, etc.) of the
model. This approach has the added final step of assembling all the standalone
sections together into the final model.
Here's some pictures I took during the assembly process. In total, assembly took
6-8 hours.
An part cut from the template â€” this part is for one of the elevons.
The same part from above, but now scored. Note the visible impressions on the fold lines.
All the parts to make one of the engine/wings, cut and scored.
Beginning to assemble the engine (see completed inlet spike).
Both engines and partial wings, fully assembled.
The assembled nose cone and cockpit.
Bottom view of assembling the engines to the main fuselage.
Top view of assembling the engines to the main fuselage.
Tips#
Use little glue: When gluing parts together, apply as little glue as
possible. Using too much glue will result in spillover when the flaps/edges are
put together, and this spillover is hard to wipe away from a porous surface like
paper. Too much glue can even result in subtle paper warping. In the recommended
tools, I suggested a toothpick. I apply a small bead of glue to a flap and use
the toothpick to spread it into a thin film. This prevents any spillage and
keeps the model clean.
Start in complex areas: As you progress further in gluing parts together,
the degrees of freedom of your model will reduce. This is why I recommend
starting with more complicated areas of your model where you'll need those
degrees of freedom. In this model, this meant starting with precise features,
like the engine inlet spikes or the vertical stabilizers.
Finish in hidden areas: This goes hand in hand with the tip above. As you
reach to the end of your model, gluing the final parts together can be very
hard, which means the final edges may come out a bit sloppy. Why does this
happen? Any minor imperfections we made throughout the assembly process result
in stresses in our model that will be felt at the end. Gluing the last part may
be challenging because it'll feel misaligned, and it has the added challenge of
attempting to close a 3D object from the outside. That's why I always recommend
choosing an assembly order that results in the last parts being glued in an area
that is out of sight. For the SR-71, that happens to be the underside of the
fuselage.
Final Model#
Here's the final model, displayed on a stand (also made from paper):
Top view of finished paper model
Side view of finished paper model, on its stand
Iteration#
No matter how much you scrutinize the modeling and layout phases, you will
inevitably find areas for improvement as you assemble. In the case of the SR-71,
I spotted a few minor assymetries in part tabs, and more importantly, an
opportunity to reduce face count by simplifying the topology of the bottom of
the plane and the nose cone.
I took my mesh back into Blender, and was able to get the triangle count down to
636, which is almost a full 100 faces fewer than the original mesh.
Second mesh iteration in Blender.
Below, you can see the old mesh (left) next to the new mesh (right). It's hard
to tell the difference, yet the new one has almost 15% fewer faces.
First mesh iteration (left) vs. latest mesh iteration (right).
A faster way to iterate is to render the model rather than physically building
it. This allows you to quickly identify and fix visual issues without going
through the hours of assembly. Here's some renders (in Blender) of the final
iteration:
Blender isometric-view rendering of final mesh.
Blender front-view rendering of final mesh.
Conclusion#
In total, the full cycle of designing the mesh, creating the parts layout,
assembly, and subsequent refinement iterations occurred over the course of a few
months. The process is long, but the results are well worth it.
If you're interested in making this model yourself, you can download the PDFs for
the first iteration of the model below. I've included a template for the stand as well.

SR71 Template
SR-71 Stand Template

Hope you enjoy!]]></content:encoded>
        </item>
    </channel>
</rss>