<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 15:32:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid>
        </item>
        <item>
            <title><![CDATA["Turns out Google made up an elaborate story about me"]]></title>
            <link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid>
        </item>
        <item>
            <title><![CDATA[Git for Music – Using Version Control for Music Production (2023)]]></title>
            <link>https://grechin.org/2023/05/06/git-and-reaper.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid>
            <description><![CDATA[last updated on 6 Apr 2024]]></description>
            <content:encoded><![CDATA[
        

  

  
    
  last updated on 6 Apr 2024


Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.

Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.

Okay, and now let’s get to the point and…



Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?

my-cool-song-new-vocals-brighter-mix-4.rpp

Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?

This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as “git”, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).

For music production, I use Reaper, and instead of creating dozens of copies of my project file (my-cool-song.rpp), such as my-cool-song-new-vocals-brighter-mix-4.rpp, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the “home” for managing the version of our music project.

By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.


  


My git-based music production workflow

Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install git-bash, and have all the git functionality at your fingertips through a command-line interface.

First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$


in the example above:

  I first navigated to the directory with my project with cd command
  initialized a repository with git init .
  on the last, third line, my command prompt starts having a little (master) thing, which is the default “branch” in my repository that Git has created for me


I also create a .gitignore file and that this is this particular project file that I want to track, and not any other, such as media or peak files:

*

!in_your_eyes_remix_git_managed.rpp


Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. “bass vst settings adjusted”.

Then I can see all the versions of my project in git gui tool.




  side note: you can use any git frontend, not only git gui.


Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.

The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.

Managing other files (WAVs etc.)

Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.

About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.

This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.

collaborating with GIT? Not sure…

GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don’t see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.

And let’s not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.

Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can’t really consider it a real backup - because of missing media.

Tracking TODO items for your music project in GitHub

Interesting use-case I’m currently testing is to have a “todo list”, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:

fix panning issues in chorus TODO
add one more synth layer TODO


Once it’s in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don’t forget to pull your update, though, once you are back to your DAW PC).

In conclusion, when we inspect this idea of “git for music” a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!

Thanks for reading.

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Simple modenized .NET NuGet server reached RC]]></title>
            <link>https://github.com/kekyo/nuget-server</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid>
            <description><![CDATA[Simple modenized NuGet server 📦. Contribute to kekyo/nuget-server development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[nuget-server
Simple modenized NuGet server implementation.






(日本語はこちら)
What is this?
A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.
Compatible with dotnet restore and standard NuGet clients for package publishing, querying, and manually downloading.
A modern browser-based UI is also provided:

You can refer to registered packages. You can check various package attributes.
You can download packages by version.
You can also publish (upload) packages.
You can manage user accounts.

Browse package list:

Publishing packages:

User account managements:

Key Features

Easy setup, run NuGet server in 10 seconds!
NuGet V3 API compatibility: Support for modern NuGet client operations
No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements
Package publish: Flexible client to upload .nupkg files via HTTP POST using cURL and others
Basic authentication: Setup authentication for publish and general access when you want it
Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution
Modern Web UI with enhanced features:

Multiple package upload: Drag & drop multiple .nupkg files at once
User account management: Add/delete users, reset passwords (admin only)
API password regeneration: Self-service API password updates
Password change: Users can change their own passwords


Package importer: Included package importer from existing NuGet server
Docker image available


Installation
npm install -g nuget-server
For using Docker images, refer to a separate chapter.
Usage
# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json
The NuGet V3 API is served on the /v3 path.

Default nuget-server served URL (Show UI): http://localhost:5963
Actual NuGet V3 API endpoint: http://localhost:5963/v3/index.json

The default URL provided by nuget-server can be changed using the --base-url option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.
Configure the NuGet client
nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.
If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.
Add as package source:
For HTTP endpoints:
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections
For HTTPS endpoints:
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Or specify in nuget.config:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
</configuration>
Publish packages
Upload packages by HTTP POST method, using cURL or any HTTP client with /api/publish endpoint:
# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
You may be dissatisfied with publishing using this method. The dotnet command includes dotnet nuget push, which is the standard approach.
However, in my experience, this protocol uses multipart/form-data for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.
Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately.
You might still feel issue with managing read operations and publish operation with the same key,
but in that case, you can simply separate the users.
For authentication feature, please refer to below chapter.

Package storage configuration
Storage location
By default, packages are stored in the ./packages directory relative to where you run nuget-server.
You can customize this location using the --package-dir option:
# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location
Package storage layout
Packages are stored in the filesystem using the following structure:
packages/
├── PackageName/
│   ├── 1.0.0/
│   │   ├── PackageName.1.0.0.nupkg
│   │   ├── PackageName.nuspec
│   │   └── icon.png            # Package icon (if present)
│   └── 2.0.0/
│       ├── PackageName.2.0.0.nupkg
│       ├── PackageName.nuspec
│       └── icon.jpg            # Package icon (if present)
└── AnotherPackage/
    └── 1.5.0/
        ├── AnotherPackage.1.5.0.nupkg
        ├── AnotherPackage.nuspec
        └── icon.png            # Package icon (if present)

Backup and restore
You can backup the package directory using simply tar or other achiver:
cd /your/server/base/dir
tar -cf - ./packages | lz4 > backup-packages.tar.lz4
Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.

Configuration
nuget-server supports configuration through command-line options, environment variables, and JSON file.
Settings are applied in the following order (highest to lowest priority):

Command-line options
Environment variables
config.json
Default values

Configuration file structure
You can specify a custom configuration file:
# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server
If not specified, nuget-server looks for ./config.json in the current directory.
config.json structure
Create a config.json file:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "<your-secret-here>",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}
All fields are optional. Only include the settings you want to override.
Both packageDir and usersFile paths can be absolute or relative. If relative, they are resolved from the directory containing the config.json file.

Authentication
nuget-server also supports authentication.



Authentication Mode
Details
Auth Initialization




none
Default. No authentication required
Not required


publish
Authentication required only for package publishing
Required


full
Authentication required for all operations (must login first)
Required



To enable authentication on the NuGet server, first register an initial user using the --auth-init option.
Initialize
Create an initial admin user interactively:
nuget-server --auth-init
This command will:

Prompt for admin username (default: admin)
Prompt for password (with strength checking, masked input)
Create users.json
Exit after initialization (server does not start)

When enabling authentication using a Docker image, use this option to generate the initial user.
Example session
Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================

User Management
Users added with --auth-init automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.

While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.
Using the API password
The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server.
The password used by NuGet clients when accessing the server is called the "API password,"
and access is granted using the combination of the user and the API password.
Please log in by displaying the UI in the browser.
Select the “API password” menu from the UI menu to generate an API password.
Using this API password will enable access from the NuGet client.

Here is an example of using the API password:
# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections
Or specify nuget.config with credentials:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
  <packageSourceCredentials>
    <local>
      <add key="Username" value="reader" />
      <add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" />
    </local>
  </packageSourceCredentials>
</configuration>
For package publishing:
# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
When publishing a package, you can send the package by setting Basic authentication in the Authorization header.
Password strength requirements
nuget-server uses the zxcvbn library to enforce strong password requirements:

Evaluates password strength on a scale of 0-4 (Weak to Very Strong)
Default minimum score: 2 (Good)
Checks against common passwords, dictionary words, and patterns
Provides real-time feedback during password creation

Configure password requirements in config.json:
{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}
The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved.
However, if you do not use HTTPS (TLS), be aware that the Authorization header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.

Import packages from another NuGet server
Import all packages from another NuGet server to your local nuget-server instance.
This feature can be used when migrating the foreign NuGet server to nuget-server.
Package import from another NuGet server
Import packages interactively in CLI:
nuget-server --import-packages --package-dir ./packages
This command will:

Prompt for source NuGet server URL
Ask if authentication is required
If needed, prompt for username and password (masked input)
Discover all packages from the source server
Download and import all packages to local storage
Display progress for each package (1% intervals)
Exit after import (server does not start)

Import behavior

Existing packages with the same version will be overwritten
Failed imports are logged with error details
Progress is reported at 1% intervals to reduce log noise
Package icons are preserved during import

Parallel downloads are not done. This is to avoid making a large number of requests to the repository.
This feature is a type of downloader.
Therefore, it does not need to be run on the actual host where it will operate.
You can perform the import process in advance on a separate host and then move the packages directory as-is.
Example session
Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================


Reverse proxy interoperability
The server supports running behind a reverse proxy.
For example, when you have a public URL like https://nuget.example.com and run nuget-server on a host within your internal network via a gateway.
In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.
URL resolving
The server resolves URLs using the following priority order:

Fixed base URL (highest priority): When --base-url option is specified, it always takes precedence
Trusted proxy headers: When trusted proxies are configured with --trusted-proxies:

HTTP Forwarded header (proto, host, port)
Traditional X-Forwarded-* headers (X-Forwarded-Proto, X-Forwarded-Host, X-Forwarded-Port)


Standard request information (fallback): Uses Host header when proxy headers are not available

For example --base-url option:

nuget-server served public base URL: https://packages.example.com
Actual NuGet V3 API endpoint: https://packages.example.com/v3/index.json

# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Another option, you can configure with trusted proxy addresses:
# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"
Environment variables are also supported:
export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here

Docker usage
Docker images are available for multiple architectures:

linux/amd64 (x86_64)
linux/arm64 (aarch64)

When pulling the image, Docker automatically selects the appropriate architecture for your platform.
Quick start
Suppose you have configured the following directory structure for persistence (recommended):
docker-instance/
├── data/
│   ├── config.json
│   └── user.json
└── packages/
    └── (package files)

Execute as follows:
# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat > docker-compose.yml << EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d
Your NuGet server is now available at:

Web UI: http://localhost:5963
NuGet V3 API: http://localhost:5963/v3/index.json

Permission requirements
The Docker container runs as the nugetserver user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.
Set proper permissions for mounted directories:
# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages
Important: Without proper permissions, you may encounter 500 Permission Denied errors when:

Creating or updating user accounts
Publishing packages
Writing configuration files

Basic usage
# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest
You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use config.json.
Since the Docker image has mount points configured, you can mount /data and /packages as shown in the example above and place /data/config.json there to flexibly configure settings. Below is an example of config.json:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}
When initializing credentials or importing packages, configure config.json and perform the operation via the CLI before launching the Docker image:
# Initialize authentication
nuget-server -c ./data/config.json --auth-init
Volume mounts and configuration

/data: Default data directory for config.json, users.json and other persistent data
/packages: Default package storage directory (mounted to persist packages)

Default behavior: The Docker image runs with --users-file /data/users.json --package-dir /packages by default.
Configuration priority (highest to lowest):

Custom command line arguments (when overriding CMD)
Environment variables (e.g., NUGET_SERVER_PACKAGE_DIR)
config.json file (if explicitly specified)
Default command line arguments in Dockerfile

Example of Automatic Startup Using systemd
Various methods exist for automatically starting containers with systemd.
Below is a simple example of configuring a systemd service using Podman.
This is a simple service unit file used before quadlets were introduced to Podman.
By placing this file and having systemd recognize it, you can automatically start the nuget-server:
/etc/systemd/system/container-nuget-server.service:
# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target

Building the Docker image (Advanced)
The build of the nuget-server Docker image uses Podman.
Multi-platform build with Podman (recommended)
Use the provided multi-platform build script that uses Podman to build for all supported architectures:
# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect
Important: For cross-platform builds, QEMU emulation must be configured first:
# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64
Without QEMU, you can only build for your native architecture.

Note
Non-interactive mode (CI/CD)
The --auth-init and --import-packages options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:
export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json
This allows initialization in CI/CD pipelines without user interaction.
Session Security
For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:
export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server
(Or use config.json.)
If not set, a random secret is generated (warning will be logged).
Supported NuGet V3 API endpoints
The server implements a subset of the NuGet V3 API protocol:

Service index: /v3/index.json
Package content: /v3/package/{id}/index.json
Package downloads: /v3/package/{id}/{version}/{filename}
Registration index: /v3/registrations/{id}/index.json


License
Under MIT.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems]]></title>
            <link>https://github.com/gargakshit/zfsbackrest</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid>
            <description><![CDATA[pgbackrest style encrypted backups for ZFS filesystems - gargakshit/zfsbackrest]]></description>
            <content:encoded><![CDATA[zfsbackrest

⚠️ Experimental:
Do not use it as your only way for backups. This is something I wrote over a
weekend. There's a lot of things that need work here.

pgbackrest style encrypted backups for ZFS
filesystems.
Getting Started
Installing
You need age installed to generate
encryption keys. Encryption is NOT optional.
$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest
Configuring
Create /etc/zfsbackrest.toml.
debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4
Creating a repository
$ zfsbackrest init --age-recipient-public-key="<your age public key>"
Backing up
$ zfsbackrest backup --type <full | diff | incr>
full backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.
diff backups are sent incrementally from the latest full backup. They depend
on the parent full backup to be present in the repository to restore.
incr backups are send incrementally from the latest diff backup. They depend
on the parent diff backup to restore.
Viewing the repository
$ zfsbackrest detail
It shows a list of backups, orphans and all.
Cleaning up the repository
Sometimes, orphaned backups are left as an artefact of incomplete or cancelled
backups. You can clean those by running
$ zfsbackrest cleanup --orphans --dry-run=false
You can clean up expired backups by running
$ zfsbackrest cleanup --expired --dru-run=false
Restoring
To restore the backups, you'll need your age identity file (private key).
zfsbackrest restore -i <path-to-age-identity-file> \
  -s <name of the dataset to restore from> \
  -b <optionally, the backup ID to restore from, leave empty to restore the latest> \
  -d <name of the dataset to restore to> # Restoring to a dataset that already exists on your local FS will fail.
Safety
zfsbackrest doesn't write or modify actual zfs datasets. It makes extensive
use of snapshots. List of zfs operations used by zfsbackrest are


backup

zfs snapshot - Creating a zfs snapshot for zfsbackrest
zfs hold - Creating a reference to that snapshot to prevent removal
zfs send - Sending the snapshot incrementally



cleanup / force-destroy

zfs release - Release the held snapshot
zfs destroy - Destroy the snapshot



restore

zfs recv - Receiving the remote snapshot



Model
TODO
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bear is now source-available]]></title>
            <link>https://herman.bearblog.dev/license/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092490</guid>
            <description><![CDATA[Updates to the Bear license]]></description>
            <content:encoded><![CDATA[
  
  
    
      
        ᕕ( ᐛ )ᕗ Herman's blog
      
    
    
      Home Now Projects Blog

    
  
  
    

    
        
    

    
        

        
            
                
                    01 Sep, 2025
                
            
        
    

    When I started building Bear I made the code available under an MIT license. I didn't give it much thought at the time, but knew that I wanted the code to be available for people to learn from, and to make it easily auditable so users could validate claims I have made about the privacy and security of the platform.
Unfortunately over the years there have been cases of people forking the project in the attempt to set up a competing service. And it hurts. It hurts to see something you've worked so hard on for so long get copied and distributed with only a few hours of modification. It hurts to have poured so much love into a piece of software to see it turned against you and threaten your livelihood. It hurts to believe in open-source and then be bitten by it.
After the last instance of this I have come to the difficult decision to change Bear's license from MIT to a version of copyleft called the Elastic License—created by the Elastic Search people.
This license is almost identical to the MIT license but with the stipulation that the software cannot be provided as a hosted or managed service. You can view the specific wording here.
After spending time researching how other projects are handling this, I realise I'm not alone. Many other open-source projects have updated their licenses to prevent "free-ride competition" in the past few years.123456
We're entering a new age of AI powered coding, where creating a competing product only involves typing "Create a fork of this repo and change its name to something cool and deploy it on an EC2 instance".
While Bear's code is good, what makes the platform special is the people who use it, and the commitment to longevity.
I will ensure the platform is taken care of, even if it means backtracking on what people can do with the code itself.


    

    
        

        
            


        
    


  
  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tetris is NP-hard even with O(1) rows or columns [pdf]]]></title>
            <link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid>
        </item>
        <item>
            <title><![CDATA[Ask HN: Do custom ROMs exist for electric cars, for example Teslas?]]></title>
            <link>https://news.ycombinator.com/item?id=45092204</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092204</guid>
            <description><![CDATA[I always wondered, in age of almost digital cars, is custom ROM a thing? Like root access and custom features?]]></description>
            <content:encoded><![CDATA[Ask HN: Do custom ROMs exist for electric cars, for example Teslas?26 points by j1000 3 hours ago  | hide | past | favorite | 15 commentsI always wondered, in age of almost digital cars, is custom ROM a thing? Like root access and custom features?

No. Aftermarket ECUs absolutely exist for almost all internal combustion engines. Other aftermarket modules are rare. Integration of them into a complete system even more so.
I think I know what you are asking but it is complicated.For safety, regulator, historical and frankly common sense reasons, a car is not one system.  It is a system of system that communicate via a CAN BUS, https://en.wikipedia.org/wiki/CAN_bus.  This is still true for electric cars.  Can this be hacked?  Like everything else, yes.Can you side load a new ROM like an android device?  Not that know of and hope that never becomes a reality because your phone crashing is different than you car crashing (figuratively and literally).  Can you enable/disable features?  Yes, usually through ECU hacking.  On my P3 Volvo, I bought a cheap stripped down Chinese clone of Volvo's diagnostic tool called DiCE.  Once the ECU is decrypted, which is done through brute force, you can use something like https://d5t5.com/article/vdash-volvo-diagnostic or P3Tool to change level settings like the theme of LED dash or engine tuning.You may be interested in https://github.com/jaredthecoder/awesome-vehicle-security#re...
Not ROMs but OrBit is a "OrBit is PC software for diagnostics, configuration,  and software flashing for newer Volvo and Polestar vehicles".American Polestars can, for example, enable their adaptive headlights using OrBit.https://spaycetech.com/
There is a lot of potential liability for anyone who creates something that targets anything other than the infotainment system.
I believe these systems are quite coupled with the hardware itself, making it quite difficult to port any custom ROM or such on them. I am not aware of any projects with the goals of creating an open-source Android ROM for a car. Even Phone ROMs are slowly dying off, with the exceptions of Lineage and GrapheneOS.
Security (for vendor) from obscurity. AFAIK most of car owners cannot just buy the replace electronics for his car on used market so most of owners afraid of messing with proprietary computers in the car.
I believe law environment need to change to make possible digital  custom car's ROM. Now everything can be closed in same of safety, security, user convience...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CocoaPods Is Deprecated]]></title>
            <link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid>
            <description><![CDATA[The Dependency Manager for iOS & Mac projects]]></description>
            <content:encoded><![CDATA[
        TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.

Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:


We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.


I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.

Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)

May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the prepare_command field in a Podspec. Any existing Pods using prepare_command are hard-coded to bypass this check.

Timeline

My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.

May 2025

We are stopping new CocoaPods from being added which use the prepare_command field

Mid-late 2025

I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.

September-October 2026

I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.

November 1-7th 2026

I will trigger a test run, giving automation a chance to break early

December 2nd 2026

I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.



These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.

If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[De-Googling TOTP Authenticator Codes]]></title>
            <link>https://imrannazar.com/articles/degoogle-otp</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091202</guid>
            <description><![CDATA[I've been slowly removing Google apps from my life, and one of the last ones left is Authenticator. In this post I look at migrating codes out of Authenticator to a command-line OTP tool, and the steps involved.]]></description>
            <content:encoded><![CDATA[
   
Back to Articles
1st Sep 2025
In the ongoing effort to extricate myself from Google's services, I've been paring down my usage of their apps on my (admittedly Android) phone. I'm now down to two Google apps I use regularly: Maps (for traffic data) and Authenticator (for TOTP[A]Time-based One Time Password codes).
Now, I spend most of my time in a terminal window on MacOS or connected to a Linux machine; it'd be nice if I could get TOTPs on the command-line, and it turns out there's a utility called oathtool that allows for TOTP generation on the CLI. However, that would mean switching my OTP provider, which usually involves:

 Logging into each service that has an OTP registered in the app;
 Disabling two-factor authentication (2FA);
 Re-enabling 2FA and using the "manual entry" code as input to oathtool;
 Doing it all again for the next website or service.

Fortunately, Google's Authenticator provides a way to migrate codes between instances of the app based on scanning QR codes, and we can use this to migrate them away from Google into a TOTP handler of our choosing. It's another four-step process:

 Generating a QR code in Google Authenticator for the codes you want to export;
 Decoding the QR somewhere off-device, into a URL;
 Decoding the URL into its constituent services and secret values;
 Setting up oathtool to use the secrets.

Note that the below steps are presented just as I went through them, you may be able to find efficiencies or you may run into troubles that I didn't (especially if you're trying this exclusively on Windows); "your mileage may vary" is apt here.
Going from Authenticator to a migration URL
The first step is getting the code out of Authenticator, through the Transfer Codes menu option in the app. Picking the services you'd like to extract leads you to a code like this:

 
 Figure 1: QR code exported from Google AuthenticatorUnrivalled padding between the QR and Next button

You may have an app on your phone that decodes QRs, but I don't; instead, I transferred the file to my MacOS machine over Tailscale, and used a command-line tool called qrtool to get the QR content:

 Decoding the migration QR
 $ brew install qrtool
$ qrtool decode Screenshot_20250901_062719_Authenticator.jpg
otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D

Decoding the URL into secrets
So we have our migration URL, with a Base64-encoded data block. Unfortunately, if we were to simply decode the data, we'd end up with some binary gibberish:

 Trying to decode the URL directly
 $ php -r 'var_dump(base64_decode("CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D"));'
string(69) "
<

i*H??h??ý

It turns out that this is a Protobuf-encoded data string, and we need to use Google's Protobuf library to get the data out. It turns out Tim Brooks has already done this with a short piece of Python at: https://github.com/brookst/otpauth_migrate
I decided to install this on a Linux machine I tend to be connected to (entirely unrelated to my Python installation being broken on Mac...):

 Extracting the data via otpauth_migrate
 $ git clone https://github.com/brookst/otpauth_migrate
$ cd otpauth_migrate
$ ./otpauth_migrate.py otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D
secret: "i*H\231\315h\014\212\223\016\243"
name: "The Rickroll Store"
algorithm: ALGORITHM_SHA1
digits: DIGIT_COUNT_SIX
type: OTP_TYPE_TOTP

Secret code = NEVERGONNAGIVEYOUM======

This tool is intelligent enough to extract any number of names and secrets from a migration URL, so you can export all your codes from Authenticator into one giant QR without needing to do each separately.
Using oathtool to generate OTPs
The final step is to use this secret code with oathtool, which takes the secret directly as a parameter. If you instead want to refer to the service by name, Michael Bushey[1]"CLI 2-Factor Authentication", Michael Bushey, 2023 has a quick wrapper script which extracts the secrets from a locally-stored file:

 Wrapper script to generate OTPs: /usr/local/bin/otp
 #!/bin/bash
OTPKEY="$(sed -n "s/${1}=//p" ~/.otpkeys)"
if [ -z "$OTPKEY" ]; then
   echo "$(basename $0): Bad Service Name '$1'"
   exit
fi
date
oathtool --totp -b "$OTPKEY"
 OTP key store: ~/.otpkeys
 rickroll=NEVERGONNAGIVEYOUM======

With this in place, you won't need to use your Authenticator app again. The tool outputs the current date and time, so you can double-check that your code won't expire (at :00 seconds) before you get a chance to type it in:

 $ otp rickroll
Mon Sep  1 07:10:42 AM UTC 2025
200213

Future expansion
There's a security issue here, of course, which is the exposed secret key sitting in a file on-disk. I'm happy to sit with that and not require a password to generate OTPs every time, but if you're interested in adapting the wrapper script to use symmetric encryption to secure the keys, Vivek Gite[2]"Use oathtool Linux command line for 2 step verification (2FA)", Vivek Gite, updated Feb 2025 has a set of scripts which employ gpg for the job.
Now I just need to find a way to get traffic data into a maps App that doesn't involve Google's servers... Thoughts welcome.
  
  
 
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UK's largest battery storage facility at Tilbury substation]]></title>
            <link>https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091119</guid>
            <description><![CDATA[The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.]]></description>
            <content:encoded><![CDATA[    
            National Grid has connected the UK’s largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.
        The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the battery’s significant additional load, with new protection and control systems installed to ensure a robust connection.The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.John Twomey, director of customer and network development at National Grid Electricity Transmission, said:“Battery storage plays a vital role in Britain’s clean energy transition. Connecting Thurrock Storage, the UK’s biggest battery, to our transmission network marks a significant step on that journey.“Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.”Tom Vernon, Statera Energy CEO and founder, said:“We are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever it’s needed.”National Grid is continuing work at Tilbury substation to connect the 450MW Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.The connection of the UK’s biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent – the largest solar plant in the country – which National Grid connected to its adjacent Cleve Hill substation.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Telli (YC F24) is hiring engineers, designers, and interns (on-site in Berlin)]]></title>
            <link>https://hi.telli.com/join-us</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45090216</guid>
        </item>
        <item>
            <title><![CDATA[Preserving Order in Concurrent Go Apps: Three Approaches Compared]]></title>
            <link>https://destel.dev/blog/preserving-order-in-concurrent-go</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45089938</guid>
            <description><![CDATA[Concurrency breaks ordering by design, but sometimes we need both. Explore three methods to preserve order in concurrent Go applications, from standard ReplyTo channels to sophisticated permission passing, with benchmarks and real-world trade-offs.]]></description>
            <content:encoded><![CDATA[ Concurrency is one of Go’s greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine – unordered processing is enough, it’s faster and simpler.
But sometimes, order matters.
When Order Matters
Here are three real-world scenarios where preserving order becomes critical:
Real-time Log Enrichment: You’re processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing can’t keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.
Finding the First Match in a File List: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order – the 50th file might finish before the 5th file, so you can’t simply return the first match you find without knowing if an earlier file also contains the string.
Time Series Data Processing: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.
The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isn’t just a theoretical problem – it’s a practical constraint that affects real systems at scale.
In this article, we’ll explore three approaches I’ve developed and used in production Go applications. We’ll build a concurrent OrderedMap function that transforms a channel of inputs into a channel of outputs while preserving order. Through benchmarks of each approach, we’ll understand their trade-offs and discover surprising performance insights along the way.
The Problem: Why Concurrency Breaks Order

Let’s quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason – we can’t predict how exactly goroutines will be scheduled by the Go runtime.
For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.
If you want to see this in action, here’s a quick demo the Go playground.
Design Philosophy: Backpressure vs Buffering
The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but it’s too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.
In such designs buffers can typically grow without bound. This happens when:

The input is skewed – early items take longer to process than later items
Downstream consumers are slow

The algorithms presented below are backpressure-first. If a worker can’t yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.

Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger “buffer” in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.

Establishing a Performance Baseline
To understand the true cost of ordering, we first need a baseline to measure against.
Let’s implement and benchmark a basic concurrent Map function that doesn’t preserve order – this will show us exactly what overhead the ordering approaches add.
Our Map function transforms an input channel into an output channel using a user-supplied function f. It’s built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.
// Map transforms items from the input channel using n goroutines, and the
// provided function f. Returns a new channel with transformed items.
func Map[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	out := make(chan B)
	Loop(in, n, out, func(a A) {
		out <- f(a)
	})
	return out
}

// Loop is a worker pool implementation. It calls function f for each 
// item from the input channel using n goroutines. This is a non-blocking function 
// that signals completion by closing the done channel when all work is finished.
func Loop[A, B any](in <-chan A, n int, done chan<- B, f func(A)) {
	var wg sync.WaitGroup

	for i := 0; i < n; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for a := range in {
				f(a)
			}
		}()
	}

	go func() {
		wg.Wait()
		if done != nil {
			close(done)
		}
	}()
}

// Discard is a non-blocking function that consumes and discards
// all items from the input channel
func Discard[A any](in <-chan A) {
	go func() {
		for range in {
			// Discard the value
		}
	}()
}

func BenchmarkMap(b *testing.B) {
	for _, n := range []int{1, 2, 4, 8, 12, 50} {
		b.Run(fmt.Sprint("n=", n), func(b *testing.B) {
			in := make(chan int)
			defer close(in)
			out := Map(in, n, func(a int) int {
				//time.Sleep(50 * time.Microsecond)
				return a // no-op: just return the original value
			})
			Discard(out)

			b.ReportAllocs()
			b.ResetTimer()

			for i := 0; i < b.N; i++ {
				in <- 10 // write something to the in chan
			}
		})
	}
}
As you can see, Map uses Loop to create a worker pool that processes items concurrently, while Loop itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.
What exactly are we measuring here? We’re measuring throughput – how fast we can push items through the entire pipeline. Since the Map function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Let’s run the benchmark (I used Apple M2 Max laptop to run it):



































GoroutinesTime /opAllocs/op2408.6ns04445.1ns08546.4ns012600.2ns0501053ns0
You might wonder: “Shouldn’t higher concurrency increase throughput?” In real applications, absolutely – but only when there’s actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.
We’ll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, let’s run one more benchmark with some work simulated (50μs sleep):















































GoroutinesTime /opSpeedupAllocs/op161656ns1.0x0230429ns2.0x0415207ns4.1x087524ns8.2x0125034ns12.2x0501277ns48.3x0
Perfect! Here we see the dramatic benefits of concurrency when there’s real work to be done. With 50μs of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.
We’re now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.
Approach 1: ReplyTo Channels
This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my batching article), but somehow this was the hardest approach for me to explain clearly.
Here’s how it works:

A packer goroutine creates jobs by attaching a unique replyTo channel to every input item.
Workers process jobs concurrently, and send results through those replyTo channels.
An unpacker goroutine unpacks the values sent via replyTo channels and writes them to the output.

The following diagram illustrates how this pattern in more detail:

The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the replyTo channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.
func OrderedMap1[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job struct {
		Item    A
		ReplyTo chan B
	}

	// Packer goroutine.
	// `jobs` chan will be processed by the pool
	// `replies` chan will be consumed by unpacker goroutine
	jobs := make(chan Job)
	replies := make(chan chan B, n)
	go func() {
		for item := range in {
			replyTo := make(chan B)
			jobs <- Job{Item: item, ReplyTo: replyTo}
			replies <- replyTo
		}
		close(jobs)
		close(replies)
	}()

	// Worker pool of n goroutines.
	// Sends results back via replyTo channels
	Loop[Job, any](jobs, n, nil, func(job Job) {
		job.ReplyTo <- f(job.Item) // Calculate the result and send it back
		close(job.ReplyTo)
	})

	// Unpacker goroutine.
	// Unpacks replyTo channels in order and sends results to the `out` channel
	out := make(chan B)
	go func() {
		defer close(out)
		for replyTo := range replies {
			result := <-replyTo
			out <- result
		}
	}()
	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2818.7ns+410ns14808.9ns+364ns18826.8ns+280ns112825.6ns+225ns150772.3ns-281ns1
This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new replyTo channel for every item. Unfortunately, we can’t use a package level sync.Pool to mitigate this because our function is generic – channels for different types can’t share the same pool.
What’s also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens – OrderedMap1 becomes faster than Map (-281ns at 50 goroutines).
I haven’t investigated this phenomenon deeply. I believe it can’t be caused by inefficiencies inside Map since it’s already based on the simplest possible channel-based worker pool. One guess that I have is that in Map we have 50 goroutines competing to write into a single output channel. On the contrary, in OrderedMap, despite additional moving parts, only one goroutine is writing to the output.
Let’s now move on to the next approach.
Approach 2: sync.Cond for Turn-Taking
This was the first algorithm I implemented when I needed ordered concurrency, and it’s much easier to explain than the ReplyTo approach.
Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.
This conditional waiting is implemented using a shared currentIndex variable protected by sync.Cond, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.
Here’s how the turn-taking mechanism works:

Here, after each write, all workers wake up (using broadcast) and recheck “is it my turn?” condition
func OrderedMap2[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job struct {
		Item  A
		Index int
	}

	// Indexer goroutine.
	// Assign an index to each item from the input channel
	jobs := make(chan Job)
	go func() {
		i := 0
		for item := range in {
			jobs <- Job{Item: item, Index: i}
			i++
		}
		close(jobs)
	}()

	// Shared state.
	// Index of the next result that must be written to the output channel.
	nextIndex := 0
	cond := sync.NewCond(new(sync.Mutex))

	// Worker pool of n goroutines.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job) {
		result := f(job.Item) // Calculate the result

		// Cond must be used with a locked mutex (see stdlib docs)
		cond.L.Lock()

		// wait until it's our turn to write the result
		for job.Index != nextIndex {
			cond.Wait()
		}

		// Write the result
		out <- result

		// Increment the index and notify all other workers
		nextIndex++
		cond.Broadcast()

		cond.L.Unlock()
	})

	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2867.7ns+459ns041094ns+649ns081801ns+1255ns0122987ns+2387ns05016074ns+15021ns0
The results are telling – no more per-item allocations, which is excellent for memory efficiency. But there’s a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the “thundering herd” problem: after each write, all goroutines wake up via cond.Broadcast(), but only one will do useful work.
This inefficiency led me to think: “How can I wake only the goroutine that should write next?” And this is how the 3rd approach was born.
Approach 3: Permission Passing Chain
Here’s the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.
In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: canWrite channel to receive the permission, and nextCanWrite channel to pass the permission to the next job.

This chain structure makes the worker logic remarkably simple:

Calculate: Process the job using the provided function
Wait: Receive the permission from canWrite channel
Write: Send the result to the output channel
Pass: Send the permission to the next job via nextCanWrite channel

Here’s the diagram that illustrates the whole flow:

The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the “thundering herd” problem entirely – each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.
Let’s see how this translates to code. The implementation has two parts: a “linker” goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:
func OrderedMap3[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = make(chan struct{}, 1)
		close(nextCanWrite) // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, make(chan struct{}, 1)
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		<-job.CanWrite          // Wait for the write permission
		out <- result           // Write to the output channel
		close(job.NextCanWrite) // Pass the permission to the next job
	})

	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2927.2ns+519ns14939.8ns+495ns18860.7ns+314ns112823.8ns+224ns150609.8ns-443ns1
Here the result is very similar to what we’ve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But there’s one difference…
Unlike approach 1, here we’re allocating a non-generic chan struct{}. This means we can use a package level sync.Pool to eliminate those allocations – let’s explore that next.
Approach 3a: Zero-Allocation Permission Passing Chain
Let’s create a pool for canWrite channels. Implementation is straightforward – the pool itself and make/release functions.
// Package-level pool for canWrite channels
type chainedItem[A any] struct {
	Value        A
	CanWrite     chan struct{}
	NextCanWrite chan struct{} // canWrite channel for the next item
}

var canWritePool sync.Pool

func makeCanWriteChan() chan struct{} {
	ch := canWritePool.Get()
	if ch == nil {
		return make(chan struct{}, 1)
	}
	return ch.(chan struct{})
}

func releaseCanWriteChan(ch chan struct{}) {
	canWritePool.Put(ch)
}
Now let’s use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.
func OrderedMap3a[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite <- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		<-job.CanWrite                    // Wait for the write permission
		out <- result                     // Write to the output channel
		releaseCanWriteChan(job.CanWrite) // Release our canWrite channel to the pool
		job.NextCanWrite <- struct{}{}    // Pass the permission to the next job
	})

	return out
}
Performance Results with Pooling:









































GoroutinesTime /opvs BaselineAllocs/op2891.0ns+482ns04916.5ns+471ns08879.5ns+333ns012872.6ns+272ns050657.6ns-395ns0
Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeve…
One more thing: Building Reusable Abstractions
The permission passing approach has another significant advantage over the ReplyTo method: it controls when to write rather than where to write.
I’ll admit it – sometimes I get a bit obsessed with building clean abstractions. When working on rill, I really wanted to extract this ordering logic into something reusable and testable. This “when vs where” distinction was an AHA moment for me.
Since the algorithm doesn’t care where the outputs are written, it’s easy to abstract it into a separate function – OrderedLoop. The API is very similar to the Loop function we used before, but here the user function receives two arguments – an item and a canWrite channel. It’s important that the user function must read from the canWrite channel exactly once to avoid deadlocks or undefined behavior.
func OrderedLoop[A, B any](in <-chan A, done chan<- B, n int, f func(a A, canWrite <-chan struct{})) {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite <- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	Loop(jobs, n, done, func(job Job[A]) {
		f(job.Item, job.CanWrite) // Do the work

		releaseCanWriteChan(job.CanWrite) // Release item's canWrite channel to the pool
		job.NextCanWrite <- struct{}{}    // Pass the permission to the next job
	})
}
The typical usage looks like:
OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
	// [Do processing here]
	
	// Everything above this line is executed concurrently,
	// everything below it is executed sequentially and in order
	<-canWrite
	
	// [Write results somewhere]
})

With this abstraction in hand it’s remarkably simple to build any ordered operations. For example OrderedMap becomes just 7 lines of code:
func OrderedMap3b[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	out := make(chan B)
	OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
		result := f(a)
		<-canWrite
		out <- result
	})
	return out
}
We can also easily build an OrderedFilter that conditionally writes outputs:
func OrderedFilter[A any](in <-chan A, n int, predicate func(A) bool) <-chan A {
	out := make(chan A)
	OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
		keep := predicate(a)
		<-canWrite
		if keep {
			out <- a
		}
	})
	return out
}
Or even an OrderedSplit that distributes items to two channels based on a predicate:
func OrderedSplit[A any](in <-chan A, n int, predicate func(A) bool) (<-chan A, <-chan A) {
	outTrue := make(chan A)
	outFalse := make(chan A)
	done := make(chan struct{})
	
	OrderedLoop(in, done, n, func(a A, canWrite <-chan struct{}) {
		shouldGoToTrue := predicate(a)
		<-canWrite
		if shouldGoToTrue {
			outTrue <- a
		} else {
			outFalse <- a
		}
	})
	
	go func() {
		<-done
		close(outTrue)
		close(outFalse)
	}()
	
	return outTrue, outFalse
}
Simply put, this abstraction makes building ordered operations trivial.
Performance Comparison
Here’s how all approaches perform across different concurrency levels:





























































ConcurrencyBaselineApproach 1(ReplyTo)Approach 2(sync.Cond)Approach 3(Permission)Approach 3a(+ Pool)2408.6ns818.7ns867.7ns927.2ns891.0ns4445.1ns808.9ns1094ns939.8ns916.5ns8546.4ns826.8ns1801ns860.7ns879.5ns12600.2ns825.6ns2987ns823.8ns872.6ns501053ns772.3ns16074ns609.8ns657.6nsZero allocs✅❌✅❌✅
Key Takeaways


sync.Cond is a no-go for ordered concurrency – While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.


ReplyTo is a strong contender – it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.


Permission Passing emerges as the clear winner – It has it all:

Good performance: at most ~500ns of overhead compared to the baseline
Zero allocations: Less GC pressure for long running tasks
Clean abstraction: Core synchronization logic can be abstracted away and used to build various concurrent operations.
Maintainability: Separation of concerns and the intuitive “calculate → wait → write → pass” pattern make code easy to support and reason about



This exploration shows that ordered concurrency doesn’t have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Go’s channels can be used creatively to solve complex coordination problems.
Finally, these patterns have been battle-tested in production through rill concurrency toolkit (1.7k 🌟 on GitHub). It implements Map, OrderedMap, and many other concurrent operations. Rill focuses on composability – operations chain together into larger pipelines – while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.
Playground Links:

Code from this article
Finding the First Match in a File List example
 ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lewis and Clark marked their trail with laxatives]]></title>
            <link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid>
            <description><![CDATA[AS LEWIS AND CLARK’S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren’t thinking much about their place in history. So they weren’t taking any particular pains to document their every movement.

There were, however, some particular pains they were experiencing, as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.

Luckily, they had something that helped with that — a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called “thunder-clappers,” which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.

And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in “thunder-clappers” was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground — which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in — and take samples of the dirt in them. 

If it comes up with an off-the-charts reading for mercury, well, that’s a Corps of Discovery pit toilet — and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
												   
(Astoria, Clatsop County; 1800s) --  #ofor #oregonHistory #ORhistory -- 26 Jan 2025 -- By Finn J.D. John]]></description>
            <content:encoded><![CDATA[
			
		
				
		
		
		
        
			ASTORIA, CLATSOP COUNTY; 1800s: 
			
     
    
			  
			  			  
				   Audio version is not yet available
				  
            


		              By Finn J.D. John
			                January 26, 2025
                            
                        
		              
		              AS LEWIS AND CLARK’S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren’t thinking much about their place in history. So they weren’t taking any particular pains to document their every movement.
            There were, however, some particular pains they were experiencing with every movement, so to speak ... as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.
            Luckily, they had something that helped with that — a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called “thunder-clappers,” which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.
            
               
                  The reproduction of Fort Clatsop, built at or near the site of the Corps of Expedition's original buildings. Dr. Rush's Bilious Pills have not been particularly helpful in locating the original Fort Clatsop, long since rotted away — either because it hasn’t been found yet, or because the site of the old pit latrine has been disturbed by farming or logging activities in the years since. (Image: National Parks Service)
                
              
            
            And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in “thunder-clappers” was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground — which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in — and take samples of the dirt in them. 
            If it comes up with an off-the-charts reading for mercury, well, that’s a Corps of Discovery pit toilet — and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
            
              THESE PILLS WERE the pride and joy of Dr. Benjamin Rush, one of the Founding Fathers and a signer of the Declaration of Independence. Rush was also the man President Thomas Jefferson considered the finest physician in the republic. 
            In that opinion, Jefferson was probably alone, or at least in a small minority. Dr. Rush’s style of “heroic medicine” had caused his star to fall quite a bit by this time — especially after the Philadelphia yellow fever epidemic of 1793, when his patients died at a noticeably higher rate than untreated sufferers. 
            At the time, of course, very little was known about how the human body worked. Physicians were basically theorists, who made educated guesses and did their best. 
            The problem was, the education on which those educated guesses were based varied pretty wildly depending on what school you came from. Homeopathic physicians theorized that giving patients a tiny amount of something that mimicked their symptoms would stimulate the body to cure itself. Eclectic physicians sought cures from herbs and folk remedies. Hydropathic physicians believed hot and cold water, applied externally or internally, was all that was needed. 
            Dr. Rush wasn’t from one of these schools. He was from the school of mainstream medicine — also known as allopathic medicine (although that term is a perjorative today).
            Allopathic medical theory, in the early 1800s, dated from the second century A.D., courtesy of a Roman doctor named Galen. 
            Galen theorized that the human body ran on four different fluids, which he called “humours”: Blood, phlegm, yellow bile, and black bile. All disease, he claimed, stemmed from an imbalance in these humours.
            Thus, too much blood caused inflammation and fever; the solution was to let a pint or two out. Too much bile caused problems like constipation; the solution was to administer a purgative and let the patient blow out some black bile into a handy chamber-pot, or vomit up some yellow bile — or both.
            These interventions sometimes helped, but most of the time they had little or no good effect. So by Rush’s time, a number of physicians were going on the theory that what was needed was a doubling-down on their theory — in a style of practice that they called “heroic medicine.”
            If a sensible dose of a purgative didn’t get a patient’s bile back in balance, a “heroic” dose might. If a cup or two of blood didn’t get the fever down, four or five surely would.          
          
             
            
              [EDITOR'S NOTE: In "reader view" some phone browsers truncate the story here, algorithmically "assuming" that the second column is advertising. (Most browsers do not recognize this page as mobile-device-friendly; it is designed to be browsed on any device without reflowing, by taking advantage of the "double-tap-to-zoom" function.) If the story ends here on your device, you may have to exit "reader view" (sometimes labeled "Make This Page Mobile Friendly Mode") to continue reading. We apologize for the inconvenience.]
            
            —(Jump to top of next column)—
    

        
           
            A sketch of Fort Clatsop as it would have appeared in 1805. (Image: Oregon Historical Society)
          
        
        
          You can imagine what the result of this philosophy was, when applied to an actual sick person.
        “Some people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,” says physician and historian David Peck. “I think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.”
        In lieu of a trained physician, the Corps of Discovery’s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing “heat,” opium products for relieving pain and inducing sleep — and purgatives.
        Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called “Dr. Rush’s Bilious Pills.” They contained about 10 grains of calomel and 10 to 15 grains of jalap.
        
           
              This recipe for a milder version of Rush's Bilious Pills comes from the National Formulary in 1945. This image appears in the Lewis and Clark Fort Mandan Foundation's Web site, at which there's a lot more information about the ingredients in this compound. Mercury was still being used as an internal medicine in the 1960s and as a topical antiseptic (chiefly as Mercurochrome) into the 1990s.
            
          
        
        Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power. 
        And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don’t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance. 
        Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis “sporting house” before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.
        When symptoms broke out, the patient would be dosed with “thunder clappers” and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself. 
        And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative “on the regular” (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.
        And this low-fiber diet had predictable results.
        It had another result, too, which was less predictable — although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given “bilious pill” gets blown out post-haste in the ensuing “purge.”
        Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.
        So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way — a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.        
        
          
            (Sources: Class lecture in History of American Medicine, October 2009, Univ. of Oregon, by Dr. James Mohr; Or Perish in the Attempt: Wilderness Medicine in the Lewis and Clark Expedition, a book by David J. Peck published in 2002 by Farcountry Press; “Following Lewis and Clark’s Trail of Mercurial Laxatives,” an article by Marisa Sloan published in the Jan. 29, 2022, issue of Discover Magazine.)
          TAGS: #Archaeology #HeroicMedicine #DavidPeck #Jalap #Syphilis #CorpsOfDiscovery #BenjaminRush #Humours #Medicine #FrontierDoctors #Galen #FortClatsop #Calomel #MercuryPoisoning #Thunderclappers #Constipation #DrJamesMohr #OregonTrail #DrRush's #BiliousPills #Bile #COLUMBIAgorge #CLATSOPcounty
        

		  
          

          
          

      

     
    
		
		    Background image is a postcard, a hand-tinted photograph of Crown Point and the Columbia Gorge Scenic Highway. Here is a link to the Offbeat Oregon article about it, from 2024.
		    Scroll sideways to move the article aside for a better view.
		    
		    Looking for more?
            On our Sortable Master Directory you can search by keywords, locations, or historical timeframes. Hover your mouse over the headlines to read the first few paragraphs (or a summary of the story) in a pop-up box.
            ... or ...		    
		    Home
		    
	      

    
    
  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Linux version of the Procmon Sysinternals tool]]></title>
            <link>https://github.com/microsoft/ProcMon-for-Linux</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid>
            <description><![CDATA[A Linux version of the Procmon Sysinternals tool. Contribute to microsoft/ProcMon-for-Linux development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Process Monitor for Linux (Preview) 
Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows.  Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.

Installation & Usage
Requirements

OS: Ubuntu 18.04 lts
cmake >= 3.14 (build-time only)
libsqlite3-dev >= 3.22 (build-time only)

Install Procmon
Please see installation instructions here.
Build Procmon
Please see build instructions here.
Usage
Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file
Examples
The following traces all processes and syscalls on the system:
sudo procmon
The following traces processes with process id 10 and 20:
sudo procmon -p 10,20
The following traces process 20 only syscalls read, write and open at:
sudo procmon -p 20 -e read,write,openat
The following traces process 35 and opens Procmon in headless mode to output all captured events to file procmon.db:
sudo procmon -p 35 -c procmon.db
The following opens a Procmon tracefile, procmon.db, within the Procmon TUI:
sudo procmon -f procmon.db
Feedback

Ask a question on Stack Overflow (tag with ProcmonForLinux)
Request a new feature on GitHub
Vote for popular feature requests
File a bug in GitHub Issues

Contributing
If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:

How to build and run from the source
The development workflow, including debugging and running tests
Coding Guidelines
Submitting pull requests

Please see also our Code of Conduct.
License
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We should have the ability to run any code we want on hardware we own]]></title>
            <link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid>
            <description><![CDATA[Refuting the common and flawed argument of]]></description>
            <content:encoded><![CDATA[
  
  Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: “I should be able to run whatever code I want on hardware I own”. I agree entirely with this point, but within the context of this discussion it’s moot.


  “I should be able to run whatever code I want on hardware I own”


When Google restricts your ability to install certain applications they aren’t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. It’s through this control of the operating system that Google is exerting control, not at the hardware layer. You often don’t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Apple’s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.

You shouldn’t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldn’t be of the restrictions in place in the operating systems they provide – rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sony’s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.


  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What to do with C++ modules?]]></title>
            <link>https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086210</guid>
            <description><![CDATA[Note:  Everything that follows is purely my personal opinion as an individual. It should not be seen as any sort of policy of the Meson buil...]]></description>
            <content:encoded><![CDATA[
Note: Everything that follows is purely my personal opinion as an individual. It should not be seen as any sort of policy of the Meson build system or any other person or organization. It is also not my intention to throw anyone involved in this work under a bus. Many people have worked to the best of their abilities on C++ modules, but that does not mean we can't analyze the current situation with a critical eye.The lead on this post is a bit pessimistic, so let's just get it out of the way.If C++ modules can not show a 5× compilation time speedup (preferably 10×) on multiple existing open source code base, modules should be killed and taken out of the standard. Without this speedup pouring any more resources into modules is just feeding the sunk cost fallacy. That seems like a harsh thing to say for such a massive undertaking that promises to make things so much better. It is not something that you can just belt out and then mic drop yourself out. So let's examine the whole thing in unnecessarily deep detail. You might want to grab a cup of $beverage before continuing, this is going to take a while.What do we want?For the average developer the main visible advantages would be the following, ordered from the most important to the least.Much faster compilation times.If you look at old presentations and posts from back in the day when modules were voted in (approximately 2018-2019), this is the big talking point. This makes perfect sense, as the "header inclusion" way is an O(N²) algorithm and parsing C++ source code is slow. Splitting the code between source and header files is busywork one could do without. The core idea behind modules is that if you can store the "headery" bit in a preprocessed binary format that can be loaded from disk, things become massively faster.Then, little by little, build speed seems to fall by the wayside and the focus starts shifting towards "build isolation". This means avoiding bugs caused by things like macro leakage, weird namespace lookup issues and so on. Performance is still kind of there, but the numbers are a lot smaller, spoken aloud much more rarely and often omitted entirely. Now, getting rid of these sorts of bugs is fundamentally a good thing. However it might not be the most efficient use of resources. Compiler developer time is, sadly, a zero sum game so we should focus their skills and effort on things that provide the best results.Macro leakage and other related issues are icky but they are on average fairly rare. I have encountered a bug caused by them maybe once or twice a year. They are just not that common for the average developer. Things are probably different for people doing deep low level metaprogramming hackery, but they are a minuscule fraction of the total developer base. On the other hand slow build times are the bane of existence of every single C++ developer every single day. It is, without question, the narrowest bottleneck for developer productivity today and is the main issue modules were designed to solve. They don't seem to be doing that nowadays.How did we end up here in the first place?C++ modules were a C++ 20 feature. If a feature takes over five years of implementation work to get even somewhat working, you might ponder how it was accepted in the standard in the first place. As I was not there when it happened, I do not really know. However I have spoken to people who were present at the actual meetings where things were discussed and voted on. Their comments have been enlightening to say the least.Apparently there were people who knew about the implementation difficulty and other fundamental problems and were quite vocal that modules as specified are borderline unimplementable. They were shot down by a group of "higher up" people saying that "modules are such an important feature that we absolutely must have them in C++ 20".One person who was present told me: "that happened seven years ago [there is a fair bit of lead time in ISO standards] and [in practice] we still have nothing. In another seven years, if we are very lucky, we might have something that sort of works".The integration task from hellWhat sets modules apart from almost all other features is that they require very tight integration between compilers and build systems. This means coming up with schemes for things like what do module files actually contain, how are they named, how are they organized in big projects, how to best divide work between the different tools. Given that the ISO standard does not even acknowledge the fact that source code might reside in a file, none of this is in its purview. It is not in anybody's purview.The end result of all that is that everybody has gone in their own corner, done the bits that are the easiest for them and hoping for the best. To illustrate how bad things are, I have been in discussions with compiler developers about this. In said discussion various avenues were considered on how to get things actually working, but one compiler developer replied "we do not want to turn the compiler into a build system" to every single proposal, no matter what it was. The experience was not unlike talking to a brick wall. My guess is that the compiler team in question did not have resources to change their implementation so vetoing everything became the sensible approach for them (though not for the module world in general).The last time I looked into adding module support to Meson, things were so mind-bogglingly terrible, that you needed to create, during compilation time, additional compiler flags, store them in temp files and pass them along to compilation commands. I wish I was kidding but I am not. It's quite astounding that the module work started basically from Fortran modules, which are simple and work (in production even), and ended up in their current state, a kafkaesque nightmare of complexity which does not work.If we look at the whole thing from a project management viewpoint, the reason for this failure is fairly obvious. This is a big change across multiple isolated organizations. The only real way to get those done is to have a product owner who a) is extremely good at their job b) is tasked with and paid to get the thing done properly c) has sufficient stripes to give orders to the individual teams and d) has no problems slapping people on metaphorical wrists if they try to weasel out of doing their part.Such a person does not exist in the modules space. It is arguable whether such a person could exist even in theory. Because of this modules can never become good, which is a reasonable bar to expect a foundational piece of technology to reach.The design that went backwardsIf there is one golden rule of software design, it is "Do not do a grand design up front". This is mirrored in the C++ committee's guideline of "standardize existing practice".C++ modules may be the grandest up-frontest design the computing world has ever seen. There were no implementations (one might argue there still aren't, but I digress), no test code, no prototypes, nothing. Merely a strong opinion of "we need this and we need it yesterday".For the benefit of future generations, one better way to approach the task would have gone something like this. First you implement enough in the compiler to be able to produce one module file and then consume it in a different compilation unit. Keep it as simple as possible. It's fine to only serialize a subset of functionality and error out if someone tries to go outside the lines. Then take a build system that runs that. Then expand that to support a simple project, say, one that has ten source files and produces one executable. Implement features in the module file until you can compile the whole thing. Then measure the output. If you do not see performance increases, stop further development until you either find out why that is or you can fix your code to work better. Now you update the API so that no part of the integration makes people's eyes bleed of horror. Then scale the prototype to handle project with 100 sources. Measure again. Improve again. Then do two 100 source pairs, one that produces a library and one that creates an executable that uses the library. Measure again. Improve again. Then do 1000 sources in 10 subprojects. Repeat.If the gains are there, great, now you have base implementation that has been proven to work with real world code and which can be expanded to a full implementation. If the implementation can't be made fast and clean, that is a sign that there is a fundamental design flaw somewhere. Throw your code away and either start from scratch or declare the problem too difficult and work on something else instead.Hacking on an existing C++ compiler is really difficult and it takes months of work to even get started. If someone wants to try to work on modules but does not want to dive into compiler development, I have implemented a "module playground", which consists of a fake C++ compiler, a fake build system and a fake module scanner all in ~300 lines of Python.The promise of import stdThere is a second way of doing modules in an iterative fashion and it is actually being pursued by C++ implementers, namely import std. This is a very good approach in several different ways. First of all, the most difficult part of modules is the way compilations must be ordered. For the standard library this is not an issue, because it has no dependencies and you can generate all of it in one go. The second thing is the fact that most of the slowness of most of C++ development comes from the standard library. For reference, merely doing an #include<vector> brings in 27 000 lines of code and that is fairly small amount compared to many other common headers.What sort of an improvement can we expect from this on real world code bases? Implementations are still in flux, so let's estimate using information we have. The way import std is used depends on the compiler but roughly:Replace all #include statements for standard library headers with import std.Run the compiler in a special mode.The compiler parses headers of the standard library and produces some sort of a binary representation of themThe representation is written to disk.When compiling normally, add compiler flags that tell the compiler to load the file in question before processing actual source codeIf you are thinking "wait a minute, if we remove step #1, this is exactly how precompiled headers work", you are correct. Conceptually it is pretty much the same and I have been told (but have not verified myself) that in GCC at least module files are just repurposed precompiled headers with all the same limitations (e.g. you must use all the same compiler flags to use a module file as you did when you created it).Barring a major breakthrough in compiler data structure serialization, the expected speedup should be roughly equivalent to the speedup you get from precompiled headers. Which is to say, maybe 10-20% with Visual Studio and a few percentage points on Clang and GCC. OTOH if such a serialization improvement has occurred, it could probably be adapted to be usable in precompiled headers, too. Until someone provides verifiable measurements proving otherwise, we must assume that is the level of achievable improvement.For reference, here is a Reddit thread where people report improvements in the 10-20% range.But why 5×?A reasonable requirement for the speedup would be "better than can be achieved using currently available tools and technologies". As an experiment I wrote a custom standard library (not API compatible with the ISO one on purpose) whose main design goal was to be fast to compile. I then took an existing library, converted that to use the new library and measured. The code compiled four times faster. In addition the binary it produced was smaller and, unexpectedly, ran faster. Details can be found in this blog post.Given that 4× is already achievable (though, granted, only tested on one project, not proven in general), 5× seems like a reasonable target.But what's in it for you?The C++ standard committee has done a lot of great (and highly underappreciated) work to improve the language. On several occasions Herb Sutter has presented new functionality with "all you have to do is to recompile your code with a new compiler and the end result runs faster and is safer". It takes a ton of work to get these kinds of results, and it is exactly where you want to be.Modules are not there. In fact they are in the exact opposite corner.Using modules brings with it the following disadvantages:Need to rewrite (possibly refactor) your code.Loss of portability.Module binary files (with the exception of MSVC) are not portable so you need to provide header files for libraries in any case.The project build setup becomes more complicated.Any toolchain version except the newest one does not work (at the time of writing Apple's module support is listed as "partial")In exchange for all this you, the regular developer-about-town, get the following advantages:Nothing.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Eternal Struggle]]></title>
            <link>https://yoavg.github.io/eternal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid>
            <description><![CDATA[change background]]></description>
            <content:encoded><![CDATA[
  
  
  change background



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use One Big Server (2022)]]></title>
            <link>https://specbranch.com/posts/one-big-server/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45085029</guid>
            <description><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system …]]></description>
            <content:encoded><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system architecture is worth the developer time and
cost overheads.  By thinking about the real operational considerations of our systems, we can
get some insight into whether we actually need distributed systems for most things.
We have all gotten so familiar with virtualization and abstractions between our software
and the servers that run it.  These days, "serverless" computing is all the rage, and even
"bare metal" is a class of virtual machine.  However, every piece of software runs on a
server.  Since we now live in a world of virtualization, most of these servers are a lot
bigger and a lot cheaper than we actually think.
Meet Your Server



This is a picture of a server used by Microsoft Azure with AMD CPUs.  Starting from the left,
the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes
that the copper tubes are attached to are heat exchangers on each CPU.  The CPUs are AMD's
third generation server CPU, each of which has the following specifications:

64 cores
128 threads
~2-2.5 GHz clock
Cores capable of 4-6 instructions per clock cycle
256 MB of L3 cache

In total, this server has 128 cores with 256 simultaneous threads.  With all of the cores working
together, this server is capable of 4 TFLOPs of peak double precision computing performance. This
server would sit at the top of the top500 supercomputer list in early 2000. It would take until
2007 for this server to leave the top500 list.  Each CPU core is substantially more powerful than a
single core from 10 years ago, and boasts a much wider computation pipeline.
Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket.  The largest
capacity "cost effective" DIMMs today are 64 GB.  Populated cost-efficiently, this server can hold
1 TB of memory.  Populated with specialized high-capacity DIMMs (which are generally slower
than the smaller DIMMs), this server supports up to 8 TB of memory total.  At DDR4-3200, with
a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across
all of its cores.
In terms of I/O, each CPU offers 64 PCIe gen 4 lanes.  With 128 PCIe lanes total, this server is
capable of supporting 30 NVMe SSDs plus a network card.  Typical configurations you can buy will
offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is
in the top right, the network card.  This server is likely equipped with a 50-100 Gbps network
connection.
The Capabilities of One Server
One server today is capable of:

Serving video files at 400 Gbps (now 800 Gbps)
1 million IOPS on a NoSQL database
70k IOPS in PostgreSQL
500k requests per second to nginx
Compiling the linux kernel in 20 seconds
Rendering 4k video with x264 at 75 FPS

Among other things.  There are a lot of public benchmarks these days, and if you know how your
service behaves, you can probably find a similar benchmark.
The Cost of One Server
In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications
to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth
for $1,318/month.
Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores
and 128 GB of RAM for about €140.00/month.  This is a smaller server than the one from OVHCloud
(1/4 the size), but it gives you some idea of the price spread between hosting providers.
In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps
of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour
in the US East region.  This comes out to $6,055/month.  The cloud premium is real!
A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs,
SSDs, and support contracts), can be purchased from the Dell website for about $40,000.  However,
if you are going to spend this much on a server, you should probably chat with a salesperson to
make sure you are getting the best deal you can.  You will also need to pay to host this server
and connect it to a network, though.
In comparison, buying servers takes about 8 months to break even compared to using cloud servers,
and 30 months to break even compared to renting.  Of course, buying servers has a lot of drawbacks,
and so does renting, so going forward, we will think a little bit about the "cloud premium" and
whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the
cloud companies want you to pay").
Thinking about the Cloud
The "cloud era" began in earnest around 2010.  At the time, the state of the art CPU was an
8-core Intel Nehalem CPU.  Hyperthreading had just begun, so that 8-core CPU offered a
whopping 16 threads.  Hardware acceleration was about to arrive for AES encryption, and
vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a
whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to
offer a 3 TB hard drive.  Each core offered 4 FLOPs per cycle, meaning that your 8-core
server running at 2.5 GHz offered a blazing fast 80 GFLOPs.
The boom in distributed computing rode on this wave: if you wanted to do anything that
involved retrieval of data, you needed a lot of disks to get the storage throughput you want.
If you wanted to do large computations, you generally needed a lot of CPUs. This meant that
you needed to coordinate between a lot of CPUs to get most things done.
Since that time began, the size of servers has increased a lot, and SSDs have increased available
IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased
much, and we still use virtualized drives that perform more like hard drives than SSDs (although
this gap is closing).
One Server (Plus a Backup) is Usually Plenty
If you are doing anything short of video streaming, and you have under 10k QPS, one server
will generally be fine for most web services.  For really simple services, one server could
even make it to a million QPS or so.  Very few web services get this much traffic - if you
have one, you know about it.  Even if you're serving video, running only one server for your
control plane is very reasonable.  A benchmark can help you determine where you are.
Alternatively, you can use common benchmarks of similar applications, or
tables of common performance numbers to estimate how big of a
machine you might need.
Tall is Better than Wide
When you need a cluster of computers, if one server is not enough, using fewer larger servers
will often be better than using a large fleet of small machines.  There is non-zero overhead
to coordinate a cluster, and that overhead is frequently O(n) on each server.  To reduce this
overhead, you should generally prefer to use a few large servers than to use many small servers.
In the case of things like serverless computing, where you allocate tiny short-lived containers,
this overhead accounts for a large fraction of the cost of use.  On the other extreme end,
coordinating a cluster of one computer is trivial.
Big Servers and Availability
The big drawback of using a single big server is availability.  Your server is going to need
downtime, and it is going to break.  Running a primary and a backup server is usually enough,
keeping them in different datacenters.  A 2x2 configuration should appease the truly paranoid: two
servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will
give you a lot of redundancy.  If you want a third backup deployment, you can often make that
smaller than your primary and secondary.
However, you may still have to be concerned about correlated hardware failures.  Hard drives
(and now SSDs) have been known to occasionally have correlated failures: if you see one disk
fail, you are a lot more likely to see a second failure before getting back up if your disks
are from the same manufacturing batch.  Services like Backblaze overcome this by using many
different models of disks from multiple manufacturers.  Hacker news learned this the hard way
recently when the primary and backup server went down at the same time.
If you are using a hosting provider which rents pre-built servers, it is prudent to rent two
different types of servers in each of your primary and backup datacenters.  This should avoid
almost every failure mode present in modern systems.
Use the Cloud, but don't be too Cloudy
A combination of availability and ease of use is one of the big reasons why I (and most other
engineers) like cloud computers.  Yes, you pay a significant premium to rent the machines, but
your cloud provider has so much experience building servers that you don't even see most failures,
and for the other failures, you can get back up and running really quickly by renting a new
machine in their nearly-limitless pool of compute.  It is their job to make sure that you don't
experience downtime, and while they don't always do it perfectly, they are pretty good at it.
Hosting providers who are willing to rent you a server are a cheaper alternative to cloud
providers, but these providers can sometimes have poor quality and some of them don't understand
things like network provisioning and correlated hardware failures. Also, moving from one rented
server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a
price premium for a good reason.
However, when you deal with clouds, your salespeople will generally push you towards
"cloud-native" architecture.  These are things like microservices in auto-scaling VM groups with
legions of load balancers between them, and vendor-lock-in-enhancing products like serverless
computing and managed high-availability databases.  There is a good reason that cloud
salespeople are the ones pushing "cloud architecture" - it's better for them!
The conventional wisdom is that using cloud architecture is good because it lets you scale up
effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people
is not one of them: most services can serve millions of people at a time with one server, and
will never give you a surprise five-figure bill.
Why Should I Pay for Peak Load?
One common criticism of the "one big server" approach is that you now have to pay for your peak
usage instead of paying as you go for what you use.  Thus, serverless computing or fleets of
microservice VMs more closely align your costs with your profit.
Unfortunately, since all of your services run on servers (whether you like it or not), someone
in that supply chain is charging you based on their peak load.  Part of the "cloud premium" for
load balancers, serverless computing, and small VMs is based on how much extra capacity your
cloud provider needs to build in order to handle their peak load.  You're paying for someone's
peak load anyway!
This means that if your workload is exceptionally bursty - like a simulation that needs
to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if
your workload is not so bursty, you will often have a cheaper system (and an easier time building
it) if you go for few large servers.  If your cloud provider's usage is more bursty than yours,
you are going to pay that premium for no benefit.
This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM
24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with
a salesperson if you are big enough.
Generally, the burstier your workload is, the more cloudy your architecture should be.
How Much Does it Cost to be Cloudy?
Being cloudy is expensive.  Generally, I would anticipate a 5-30x price premium depending on what
you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and
30.
Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM.  I
am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above.
Large ARM servers and serverless ARM compute are both cheaper.
Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:


1k QPS is 60k queries per minute, or 3.6M queries per hour


Each query here gets 0.768 GB-seconds of RAM (amortized)


Replacing this server would cost about $46/hour using serverless computing


The price premium for serverless computing over the instance is a factor of 5.5.  If you can keep
that server over 20% utilization, using the server will be cheaper than using serverless computing.
This is before any form of savings plan you can apply to that server - if you can rent those big
servers from the spot market or if you compare to the price you can get with a 1-year contract,
the price premium is even higher.
If you compare to the OVHCloud rental price for the same server, the price premium of buying your
compute through AWS lambda is a factor of 25
If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you
should prefer the hosting provider if you can keep the server operating at 5% capacity!
Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k
QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x
(or 25x) premium. Of course, you should scale the size of the server to fit your application.
Common Objections to One Big Server
If you propose using the one big server approach, you will often get pushback from people who are
more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns.  Use your
judgment when you think about it, but most people vastly underestimate how much "cloud
architecture" actually costs compared to the underlying compute.  Here are some common objections.
But if I use Cloud Architecture, I Don't Have to Hire Sysadmins
Yes you do.  They are just now called "Cloud Ops" and are under a different manager. Also, their
ability to read the arcane documentation that comes from cloud companies and keep up  with the
corresponding torrents of updates and deprecations makes them 5x more expensive than system
administrators.
But if I use Cloud Architecture, I Don't Have to Do Security Updates
Yes you do.  You may have to do fewer of them, but the ones you don't have to do are the easy ones
to automate.  You are still going to share in the pain of auditing libraries you use, and making
sure that all of your configurations are secure.
But if I use Cloud Architecture, I Don't Have to Worry About it Going Down
The "high availability" architectures you get from using cloudy constructs and microservices just
about make up for the fragility they add due to complexity.  At this point, if you use two
different cloud regions or two cloud providers, you can generally assume that is good enough to
avoid your service going down.  However, cloud providers have often had global outages in the past,
and there is no reason to assume that cloud datacenters will be down any less often than your
individual servers.
Remember that we are trying to prevent correlated failures.  Cloud datacenters have a lot of
parts that can fail in correlated ways.  Hosting providers have many fewer of these parts.
Similarly, complex cloud services, like managed databases, have more failure modes than simple
ones (VMs).
But I can Develop More Quickly if I use Cloud Architecture
Then do it, and just keep an eye on the bill and think about when it's worth it to switch.  This
is probably the strongest argument in favor of using cloudy constructs.  However, if you don't
think about it as you grow, you will likely end up burning a lot of money on your cloudy
architecture long past the time to switch to something more boring.
My Workload is Really Bursty
Cloud away.  That is a great reason to use things like serverless computing.  One of the big
benefits of cloud architecture constructs is that the scale down really well.  If your workload
goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud
architecture probably works really well for you.
What about CDNs?
It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings,
with one big server.  This is also true of other systems that need to be distributed, like backups.
Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of
thing to buy rather than build.
A Note On Microservices and Monoliths
Thinking about "one big server" naturally lines up with thinking about monolithic architectures.
However, you don't need to use a monolith to use one server.  You can run many containers on one
big server, with one microservice per container.  However, microservice architectures in general
add a lot of overhead to a system for dubious gain when you are running on one big server.
Conclusions
When you experience growing pains, and get close to the limits of your current servers, today's
conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture
that gives you horizontal scaling "for free."  It is often easier and more efficient to scale
vertically instead.  Using one big server is comparatively cheap, keeps your overheads at a
minimum, and actually has a pretty good availability story if you are careful to prevent correlated
hardware failures.  It's not glamorous and it won't help your resume, but one big server will serve
you well.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Jujutsu for everyone]]></title>
            <link>https://jj-for-everyone.github.io/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45083952</guid>
            <description><![CDATA[A Jujutsu tutorial that requires no previous experience with Git or other version control systems.]]></description>
            <content:encoded><![CDATA[
    
            Keyboard shortcuts
            
                Press ← or → to navigate between chapters
                Press S or / to search in the book
                Press ? to show this help
                Press Esc to hide this help
            
        
    
        
        

        
        

        

        
        

        
            
            
            
            
        

        

            
                    
                        Introduction
This is a tutorial for the Jujutsu version control system.
It requires no previous experience with Git or any other version control system.
At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu.
This tutorial is my attempt to fill the void of beginner learning material for Jujutsu.
If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.
This tutorial requires you to work in the terminal.
Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet.
The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac.
If you're on Windows (and can't switch to Linux), consider using WSL.
How to read this tutorial
The tutorial is split into levels, which are the top-level chapters in the sidebar.
The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned.
Once you're comfortable with those skills, come back for the next level.
There is one exception to this:
If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.
Here's an overview of the planned levels:
LevelDescription
1The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.
2The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.
3Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.
4History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.
5Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.
6Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.


Only a few levels are complete right now, the rest are on the way.
Reset your progress
Throughout the tutorial, you will build an example repository.
Later chapters depend on the state of previous ones.
Losing the state of the example repo can therefore block you from making smooth progress.
This might happen for several reasons:

You use the example repo for practice and experimentation.
You switch to a different computer or reinstall the OS.
You intentionally delete it to clean up your home directory.
The tutorial is updated significantly while you're taking a break.

To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter.
To identify the chapter you want to continue with, the script expects a keyword as an argument.
Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.



Always be careful when executing scripts from the internet!




The script is not complicated, you can verify that it's not doing anything malicious.
Basically, it's just the list of commands I tell you to run manually.
For convenience, it's included in the expandable text box below.
You can also download the script here and then execute it locally once you have inspected it.





Source of reset script




#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "✅✅✅ Reset script completed successfully! ✅✅✅"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj > /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" > README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" >> README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" > hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" > README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" >> README.md

echo "*.tar.gz" > .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i < 10; i = i + 1):
    print('Hello, world!')" > hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" > hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" > hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &> /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1



Stay up to date
Both this tutorial and Jujutsu are still evolving.
In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo.
You will be notified of important changes:

A new level becomes available.
An existing level is changed significantly.

I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content.
I consider this tutorial up-to-date with the latest version of Jujutsu (0.32) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.
You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".

Help make this tutorial better
If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner.
If you have general suggestions for improvement, please open an issue.
I am also very interested in experience reports, for example:

Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?
Was there a section that wasn't explained clearly?
(If you didn't understand something, it's probably the tutorial's fault, not yours!)
Did you complete a level but didn't feel like you had the skills that were promised in the level overview?
Is there something missing that's not being taught but should?
Do you feel like the content could be structured better?

Thank you for helping me improve this tutorial!
What is version control and why should you use it?
I will assume you're using version control for software development, but it can be used for other things as well.
For example, authoring professionally formatted documents with tools like Typst.
The source of this tutorial is stored in version control too!
What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time.
You don't want to lose any of it and you want to be able to go back to previous states of your work.
Often, several people need to work on the project at the same time.
A general-purpose backup solution can keep a few copies of your files around.
A graphical document editor can allow multiple people to edit the text simultaneously.
But sometimes, you need a sharper knife.
Jujutsu is the sharpest knife available.
Why Jujutsu instead of Git?
Git is by far the most commonly used VCS in the software development industry.
So why not use that?
Using the most popular thing has undeniable benefits.
There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc.
Why make life harder on yourself by using a lesser-known alternative?
Here's my elevator pitch:


Jujutsu is compatible with Git.
You're not actually losing anything by using Jujutsu.
You can work with it on any existing project that uses Git for version control without issues.
Tools that integrate with Git mostly work just as well with Jujutsu.


Jujutsu is easier to learn than Git.
(That is, assuming I did a decent job writing this tutorial.)
Git is known for its complicated, unintuitive user interface.
Jujutsu gives you all the functionality of Git with a lot less complexity.
Experienced users of Git usually don't care about this, because they've paid the price of learning Git already.
(I was one of these people once.)
But you care!


Jujutsu is more powerful than Git.
Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust.
Don't worry, you don't have to use that power right away.
But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back.
This is not a watered-down "we have Git at home" for slow learners!


Learning Jujutsu instead of Git as your first VCS does have some downsides:


When talking about version control with peers, they will likely use Git-centric vocabulary.
Jujutsu shares a lot of Git's concepts, but there are also differences.
Translating between the two in conversation can add some mental overhead.
(solution: convince your peers to use Jujutsu 😉)


Jujutsu is relatively new and doesn't cover 100% of the features of Git yet.
When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly.
Still, having to use two tools instead of one is slightly annoying.
I plan to teach such Git features in this tutorial in later levels.
The tutorial should be a one-stop-shop for all Jujutsu users.


The command line interface of Jujutsu is not yet stable.
That means in future versions of Jujutsu, some commands might work a little differently or be renamed.
I personally don't think this should scare you away.
Many people including me have used Jujutsu as a daily driver for a long time.
Whenever something did change, my reaction was usually:
"Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!"
Consider subscribing to GitHub releases of this tutorial.
You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.


Despite some downsides, I think the benefits are well worth it.

                    

                    
                        

                            
                                
                            

                        
                    
                

            

                    
                        
                    
            

        




        


        
        
        

        
        
        

        


    
    

]]></content:encoded>
        </item>
    </channel>
</rss>