<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Wed, 10 Sep 2025 23:07:51 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Intel's E2200 "Mount Morgan" IPU at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/intels-e2200-mount-morgan-ipu-at</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204838</guid>
            <description><![CDATA[Intelâ€™s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities.]]></description>
            <content:encoded><![CDATA[Intelâ€™s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities. IPUs take things a step further, aiming to take on a wide variety of infrastructure services in a cloud environment in addition to traditional software defined networking functions. Infrastructure services are run by the cloud operator and orchestrate tasks like provisioning VMs or collecting metrics. They wonâ€™t stress a modern server CPU, but every CPU core set aside for those tasks is one that canâ€™t be rented out to customers. Offloading infrastructure workloads also provides an extra layer of isolation between a cloud providerâ€™s code and customer workloads. If a cloud provider rents out bare metal servers, running infrastructure services within the server may not even be an option.Intelâ€™s incoming â€œMount Morganâ€ IPU packs a variety of highly configurable accelerators alongside general purpose CPU cores, and aims to capture as many infrastructure tasks as possible. It shares those characteristics with its predecessor, â€œMount Evansâ€. Flexibility is the name of the game with these IPUs, which can appear as a particularly capable network card to up to four host servers, or run standalone to act as a small server. Compared to Mount Evans, Mount Morgan packs more general purpose compute power, improved accelerators, and more off-chip bandwidth to support the whole package.Intel includes a set of Arm cores in their IPU, because CPUs are the ultimate word in programmability. They run Linux and let the IPU handle a wide range of infrastructure services, and ensure the IPU stays relevant as infrastructure requirements change. Mount Morganâ€™s compute complex gets an upgrade to 24 Arm Neoverse N2 cores, up from 16 Neoverse N1 cores in Mount Evans. Intel didnâ€™t disclose the exact core configuration, but Mount Evans set its Neoverse N1 cores up with 512 KB L2 caches and ran them at 2.5 GHz. Itâ€™s not the fastest Neoverse N1 configuration around, but itâ€™s still nothing to sneeze at. Mount Morgan of course takes things further. Neoverse N1 is a 5-wide out-of-order core with a 160 entry ROB, ample execution resources, and a very capable branch predictor. Each core is already a substantial upgrade over Neoverse N1. 24 Neoverse N2 cores would be enough to handle some production server workloads, let alone a collection of infrastructure services.Mount Morgan gets a memory subsystem upgrade to quad channel LPDDR5-6400 to feed the more powerful compute complex. Mount Evans had a triple channel LPDDR4X-4267 setup, connected to 48 GB of onboard memory capacity. If Intel keeps the same memory capacity per channel, Mount Morgan would have 64 GB of onboard memory. Assuming Intelâ€™s presentation refers to 16-bit LPDDR4/5(X) channels, Mount Morgan would have 51.2 GB/s of DRAM bandwidth compared to 25.6 GB/s in Mount Evans. Those figures would be doubled if Intel refers to 32-bit data buses to LPDDR chips, rather than channels. A 32 MB System Level Cache helps reduce pressure on the memory controllers. Intel didnâ€™t increase the cacheâ€™s capacity compared to the last generation, so 32 MB likely strikes a good balance between hitrate and die area requirements. The System Level Cache is truly system level, meaning it services the IPUâ€™s various hardware acceleration blocks in addition to the CPU cores.A Lookaside Crypto and Compression Engine (LCE) sits within the compute complex, and shares lineage with Intelâ€™s Quickassist (QAT) accelerator line. Intel says the LCE features a number of upgrades over QAT targeted towards IPU use cases. But perhaps the most notable upgrade is getting asymmetric crypto support, which was conspicuously missing from Mount Evansâ€™s LCE block. Asymmetric cryptography algorithms like RSA and ECDHE are used in TLS handshakes, and arenâ€™t accelerated by special instructions on many server CPUs. Therefore, asymmetric crypto can consume significant CPU power when a server handles many connections per second. It was a compelling use case for QAT, and itâ€™s great to see Mount Morgan get that as well. The LCE block also supports symmetric crypto and compression algorithms, capabilities inherited from QAT.A programmable DMA engine in the LCE lets cloud providers move data as part of hardware accelerated workflows. Intel gives an example workflow for accessing remote storage, where the LCE helps move, compress, and encrypt data. Other accelerator blocks located in the IPUâ€™s network subsystem help complete the process.Networking bandwidth and offloads are a core function of the IPU, and its importance canâ€™t be understated. Cloud servers need high network and storage bandwidth. The two are often two sides of the same coin, because cloud providers might use separate storage servers accessed over datacenter networking. Mount Morgan has 400 Gbps of Ethernet throughput, double Mount Evansâ€™s 200 Gbps.True to its smart NIC lineage, Mount Morgan uses a large number of inline accelerators to handle cloud networking tasks. A programmable P4-based packet processing pipeline, called the FXP, sits at the heart of the network subsystem. P4 is a packet processing language that lets developers express how they want packets handled. Hardware blocks within the FXP pipeline closely match P4 demands. A parser decodes packet headers and translates the packet into a representation understood by downstream stages. Downstream stages can check for exact or wildcard matches. Longest prefix matches can be carried out in hardware too, which is useful for routing.The FXP can handle a packet every cycle, and can be configured to perform multiple passes per packet. Intel gives an example where one pass processes outer packet layers to perform decapsulation and checks against access control lists. A second pass can look at the inner packet, and carry out connection tracking or implement firewall rules.An inline crypto block sits within the network subsystem as well. Unlike the LCE in the compute complex, this crypto block is dedicated to packet processing and focuses on symmetric cryptography. It includes its own packet parsers, letting it terminate IPSec and PSP connections and carry out IPSec/PSP functions like anti-replay window protection, sequence number generation, and error checking in hardware. IPSec is used for VPN connections, which are vital for letting customers connect to cloud services. PSP is Googleâ€™s protocol for encrypting data transfers internal to Googleâ€™s cloud. Compared to Mount Evans, the crypto blockâ€™s throughput has been doubled to support 400 Gbps, and supports 64 million flows.Cloud providers have to handle customer network traffic while ensuring fairness. Customers only pay for a provisioned amount of network bandwidth. Furthermore, customer traffic canâ€™t be allowed to monopolize the network and cause problems with infrastructure services. The IPU has a traffic shaper block, letting it carry out quality of service measures completely in hardware. One mode uses a mutli-level hierarchical scheduler to arbitrate between packets based on source port, destination port, and traffic class. Another â€œtiming wheelâ€ mode does per-flow packet pacing, which can be controlled by classification rules set up at the FXP. Intel says the timing wheel mode gives a pacing resolution of 512 nanoseconds per slot.RDMA traffic accounts for a significant portion of datacenter traffic. For example, Azure says RDMA accounts for 70% of intra-cloud network traffic, and is used for disk IO. Mount Morgan has a RDMA transport option to provide hardware offload for that traffic. It can support two million queue pairs across multiple hosts, and can expose 1K virtual functions per host. The latter should let a cloud provider directly expose RDMA acceleration capabilities to VMs. To ensure reliable transport, the RDMA transport engine supports the Falcon and Swift transport protocols. Both protocols offer improvements over TCP, and Intel implements congestion control for those protocols completely in hardware. To reduce latency, the RDMA block can bypass the packet processing pipeline and handle RDMA connections on its own.All of the accelerator blocks above are clients of the system level cache. Some hardware acceleration use cases, like connection tracking with millions of flows, can have significant memory footprints. The system level cache should let the IPU keep frequently accessed portions of accelerator memory structures on-chip, reducing DRAM bandwidth needs.Mount Morganâ€™s PCIe capabilities have grown far beyond what a normal network card may offer. It has 32 PCIe Gen 5 lanes, providing more IO bandwidth than some recent desktop CPUs. Itâ€™s also a huge upgrade over the 16 PCIe Gen 4 lanes in Mount Evans.Traditionally, a network card sits downstream of a host, and thus appears as a device attached to a server. The host fabric and PCIe subsystem is flexible to let the IPU wear many hats. It can appear as a downstream device to up to four server hosts, each of which sees the IPU as a separate, independent device. Mount Evans supported this â€œmulti-hostâ€ mode as well, but Mount Morganâ€™s higher PCIe bandwidth is necessary to utilize its 400 Gigabit networking.Mount Morgan can run in a â€œheadlessâ€ mode, where it acts as a standalone server and a lightweight alternative to dedicating a traditional server to infrastructure tasks. In this mode, Mount Morganâ€™s 32 PCIe lanes can let it connect to many SSDs and other devices. The IPUâ€™s accelerators as well as the PCIe lanes appear downstream of the IPUâ€™s CPU cores, which act as a host CPU.A â€œconvergedâ€ mode can use some PCIe lanes to connect to upstream server hosts, while other lanes connect to downstream devices. In this mode, the IPU shows up as a PCIe switch to connected hosts, with downstream devices visible behind it. A server could connect to SSDs and GPUs through the IPU. The IPUâ€™s CPU cores can sit on top of the PCIe switch and access downstream devices, or can be exposed as a downstream device behind the PCIe switch.The IPUâ€™s multiple modes are a showcase of IO flexibility. Itâ€™s a bit like how AMD uses the same die as an IO die within the CPU and a part of the motherboard chipset on AM4 platforms. The IO dieâ€™s PCIe lanes can connect to downstream devices when itâ€™s serving within the CPU, or be split between an upstream host and downstream devices when used in the chipset. Intel is also no stranger to PCIe configurability. Their early QAT PCIe cards reused their Lewisburg chipset, exposing it as a downstream device with three QAT devices appearing behind a PCIe switch.Cloud computing plays a huge role in the tech world today. It originally started with commodity hardware, with similar server configurations to what customers might deploy in on-premise environments. But as cloud computing expanded, cloud providers started to see use cases for cloud-specific hardware accelerators. Examples include "Nitro" cards in Amazon Web Services, or smart NICs with FPGAs in Microsoft Azure. Intel has no doubt seen this trend, and IPUs are the company's answer.Mount Morgan tries to service all kinds of cloud acceleration needs by packing an incredible number of highly configurable accelerators, in recognition of cloud providersâ€™ diverse and changing needs. Hardware acceleration always runs the danger of becoming obsolete as protocols change. Intel tries to avoid this by having very generalized accelerators, like the FXP, as well as packing in CPU cores that can run just about anything under the sun. The latter feels like overkill for infrastructure tasks, and could let the IPU remain relevant even if some acceleration capabilities become obsolete.At a higher level, IPUs like Mount Morgan show that Intel still has ambitions to stretch beyond its core CPU market. Developing Mount Morgan must have been a complex endeavor. Itâ€™s a showcase of Intelâ€™s engineering capability even when their CPU side goes through a bit of a rough spot. Itâ€™ll be interesting to see whether Intelâ€™s IPUs can gain ground in the cloud market, especially with providers that have already developed in-house hardware offload capabilities tailored to their requirements.If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KDE launches its own distribution]]></title>
            <link>https://lwn.net/SubscriberLink/1037166/caa6979c16a99c9e/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204393</guid>
            <description><![CDATA[At Akademy 2025, the KDE Project released an alpha version of KDE Linux, a distribution built b [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



At Akademy 2025, the
KDE Project released an
alpha version of KDE Linux, a
distribution built by the project to "include the best
implementation of everything KDE has to offer, using the most advanced
technologies". It is aimed at providing an operating system
suitable for home use, business use, OEM installations, and more
"eventually". For now there are many rough edges and missing
features that users should be aware of before taking the plunge; but
it is an interesting look at the kind of complete Linux system that
KDE developers would like to see.

Development and goals

KDE contributor Nate Graham wrote an announcement
blog post on SeptemberÂ 6 to accompany the release of KDE
Linux. Harald Sitter had introduced the project as "Project Banana"
during a talk (video, slides)
at Akademy in 2024, and has
been leading its development along with major contributions from Hadi Chokr, Lasath Fernando,
Justin Zobel, Graham, and others.

KDE Linux is an immutable distribution that uses Arch Linux
packages as its base, but Graham notes that it is "definitely not
an 'Arch-based distro!'" Pacman is not included, and Arch is used
only for the base operating system. Everything else, he said, is
either compiled from source using KDE Builder or installed
using Flatpak.

Some may wonder why another Linux distribution is needed; Graham
said that he has expressed that sentiment himself in the past
regarding other distributions, but he thinks that KDE Linux is justified:


KDE is a huge producer of software. It's awkward for us to not have
our own method of distributing it. Yes, KDE produces source code that
others distribute, but we self-distribute our apps on app stores like
Flathub and the Snap and Microsoft stores, so I think it's a natural
thing for us to have our own platform for doing that distribution too,
and that's an operating system. I think all the major producers of
free software desktop environments should have their own OS, and many
already do: Linux Mint and ElementaryOS spring to mind, and GNOME is working on one too.

Besides, this matter was settled 10 years ago with the creation of
KDE neon, our first bite at the "in-house OS" apple. The sky did not
fall; everything was beautiful and nothing hurt.


Speaking of neon, Graham points
out that it is "being held together by a heroic volunteer"
(singular) and that no decision has been made as of yet about its
future. Neon has "served admirably for a decade", he said, but
it "has somewhat reached its limit in terms of what we can do with
it" because of its Ubuntu base. According to the wiki
page, neon's Ubuntu LTS base is built on old technology and
requires "a lot of packaging busywork". It also becomes less
stable as time goes on, "because it needs to be tinkered with to
get Plasma to build on it, breaking the LTS promise".

Architecture and plans

KDE Linux, on the other hand, is designed to be a greenfield
project that allows KDE to make use of newer technologies and more
modern approaches to a Linux distribution unhampered by the needs of a
general-purpose distribution. If KDE Linux's technology choices are
not appealing, Graham says, "feel free to ignore KDE Linux and
continue using the operating system of your choice. There are plenty
of them!"

KDE Linux is Wayland-only; there is no X.org session and no plan
to add one. Users with some of the older NVIDIA cards will need to
manually
configure the system to work properly with KDE Linux. The
distribution also only supports UEFI systems, and there are no plans
to add support for BIOS-only systems.

The root filesystem (/) is a read/write Btrfs
volume, while /usr is a read-only Enhanced
Read-Only File System (EROFS) volume backed by a single file. The
system is updated atomically by swapping out the EROFS volume;
currently KDE Linux caches up to five of the files to allow users to
roll back to previous versions if the most recent updates are
broken.

The files have names like kde-linux_202509082242.erofs and
are stored in /system. The most recent releases are about
4.8GB in size. The project uses systemd-sysupdate
under the hood, which does not have
support for delta updates yet. Users should expect to set aside at least 30GB
just to cache the EROFS files for now.

Unlike Fedora's image-based Atomic Desktops,
KDE Linux does not supply a way for users to add packages to the base
system. So, for example, users have no way to add packages with
additional kernel modules. Users can add applications packaged as 
Flatpaks using KDE's Discover graphical software manager; the 
Snap format is also supported, but it is not integrated with
Discoverâ€”the snap command-line
utility can be used to do install Snaps for now. KDE Linux also includes Distrobox, which allows users to set
up a container with the distribution of their choice and install
software in the container that is integrated with the system. LWN touched on Distrobox in
our coverage of the Bluefin image-based operating system in December
2023.

Unfortunately, it looks
like users are not set up correctly for Podman, which Distrobox
needs, on KDE Linux; trying to set up a new container gives a
"potentially insufficient UIDs or GIDs available in user namespace"
error when trying to test Distrobox on the latest KDE Linux build. This
comment in the Podman repository on GitHub set me on the right
path to fix the problem. This kind of bug is to be expected in an
alpha release; no doubt it will be ironed out in the coming weeks or
months.

System updates are also performed using Discover: when a new system
image is available, it will show up in the Updates tab and can be
installed from there. (Or using "sudo updatectl update" from
the command line, for those who prefer doing it that way.) Likewise,
installed Flatpaks with updates will show up in the Updates tab. For
now, at least, users will have to manually manage any applications
installed in a Distrobox container.

The default software selection is a good start for a desktop
distribution; it includes the Gwenview image viewer, Okular document
viewer, Haruna media player, Kate text editor, and Konsole for
terminal emulation. Firefox is the only prominent non-KDE application
included with the default install. The base system currently includes
GNU Bash 5.3.3, curl 8.15, Linux 6.16.5, GCC 15.2.1, Perl 5.42, Python 3.13.7, Vim
9.1, and wget 1.25. It does not include some utilities users might
want or expect, such as GNU Screen, Emacs, tmux, pip, or alternate shells like Fish.







KDE Linux's base packages are not meant to be user-customizable,
but it should be possible to create custom images using systemd's mkosi tool, which is what is used
by the project itself. The mkosi.conf.d
directory in the KDE Linux repository contains the various
configuration files for managing the packages included in the system image.



Development and what's next

The plan, longer term, is to have three editions of KDE Linux: the
testing edition, which is what is available now, an enthusiast
edition, and a stable edition. The testing edition is meant for
developers and quality assurance folks; it is to be built daily from
Git and to be similar in quality to KDE neon's unstable release. The
enthusiast edition will include beta or released software, depending
on the status of a given application at the time; this edition is
aimed at "KDE enthusiasts, power users, and influencers". The
stable edition, as the name suggests, will include only released
software that meets quality metrics (which are not yet defined),
indicating it's ready for users not in the other categories.

KDE Linux can be installed
on bare metal or in a virtual
machine using virt-manager. Support
for UEFI Secure Boot is currently missing. Since KDE Linux uses a lot
of space for cached images, users should provision more disk space for
a virtual machine than they might ordinarily; I allocated 50GB, but
probably should have gone with 75GB or more.

Those wishing to follow along with KDE Linux development can check
out the milestone trackers for the enthusiast
and stable
editions. All of the milestones
have been reached for the testing edition. There are quite a few items
to complete before KDE Linux reaches beta status; for example, the
project is currently using packages from the Arch User Repository (AUR) but
the plan is to move
away from using AUR soon. The project also needs to move
production to official KDE infrastructure rather than depending on
Sitter's personal systems.

At the moment, the project does not have a security announcement mailing
list or other notification mechanism; those using KDE Linux for more
than testing should keep an eye on Arch's security tracker and
KDE security advisories.
Since KDE Linux is an immutable derivative of Arch Linux, with no
way to immediately pull updated Arch packages, users should remember
that they will be at a disadvantage when there are security
vulnerabilities in the base operating system. Any security update would
need to be created by Arch Linux, pushed out as an Arch package, and
then incorporated into a build for KDE Linux. Conservatively, that
will add at least a day for any security updates to reach KDE Linux
users.

One of the downsides of having no package manager is that there is
no easy way to take stock of what is installed on the system. Normally,
one might do an inventory of software using a package manager's query
tools; a quick "rpm -qa" shows all of the system software on
my desktop's FedoraÂ 42 install. There is no such mechanism for
KDE Linux, and it's not clear that there are any plans for that type
of feature long term. To be suitable for some of the target audiences,
KDE Linux will need (for example) ways to manage the base operating
system and easily query what is installed.

The project's governance is described
as a "'Council of elders' model with major contributors being
the elders". Sitter has final decision-making authority in cases
of disagreement.

Obviously the team working on KDE Linux wants the project to
succeed, but it has put some thought into what will happen if the
distribution is put out to pasture at some point. There is an end-of-life
contingency plan to "push a final update shipping an OS image
that transforms the system into a completely different
distro". The successor distribution has not been chosen yet; it 
would be picked based on the KDE Linux team's relationship with the other
distribution and its ability to take on all of the new users.

Part of the rationale for KDE Linux is to satisfy an impulse that
is common to many open-source developers: the desire to ship software
directly to users without an intermediary tampering with it.
The process of creating and refining KDE Linux will satisfy
that for KDE developers, but it may also serve another purpose: to
demonstrate just how difficult it is to create and maintain a
desktop distribution for widespread use. Whether KDE Linux succeeds as a
standalone distribution or not, it may be a useful exercise to
illustrate why projects like Debian, Fedora, openSUSE, Ubuntu,
and others make choices that ultimately frustrate application
developers.


               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Christie's Deletes Digital Art Department]]></title>
            <link>https://news.artnet.com/market/christies-scraps-digital-art-department-2685784</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204305</guid>
        </item>
        <item>
            <title><![CDATA[Fraudulent Publishing in the Mathematical Sciences]]></title>
            <link>https://arxiv.org/abs/2509.07257</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45203935</guid>
            <description><![CDATA[This report is the first of two publications of a joint Working Group of the International Mathematical Union (IMU) and the International Council of Industrial and Applied Mathematics (ICIAM). In it, we shall analyze the current state of publishing in the mathematical sciences and explain the resulting problems. Our second publication will offer concrete recommendations, guidelines, and best practices for researchers, policymakers, and evaluators of mathematical research. It will explain how to detect and counteract attempts to game bibliometric measures, empowering the community to reclaim control over research evaluation and drive necessary change.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:This report is the first of two publications of a joint Working Group of the International Mathematical Union (IMU) and the International Council of Industrial and Applied Mathematics (ICIAM). In it, we shall analyze the current state of publishing in the mathematical sciences and explain the resulting problems. Our second publication will offer concrete recommendations, guidelines, and best practices for researchers, policymakers, and evaluators of mathematical research. It will explain how to detect and counteract attempts to game bibliometric measures, empowering the community to reclaim control over research evaluation and drive necessary change.
    

    
    
              
          Comments:
          Full version with complete clickable references. A print version with a selection of the most important references appeared in the October 2025 issue of the Notices of the AMS
        

          Subjects:
          
            History and Overview (math.HO)
        
          Cite as:
          arXiv:2509.07257 [math.HO]
        
        
          Â 
          (or 
              arXiv:2509.07257v1 [math.HO] for this version)
          
        
        
          Â 
                        https://doi.org/10.48550/arXiv.2509.07257
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Ilka Agricola [view email]          [v1]
        Mon, 8 Sep 2025 22:19:40 UTC (34 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mux (YC W16) Is Hiring Engineering ICs and Managers]]></title>
            <link>https://mux.com/jobs</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45203643</guid>
            <description><![CDATA[Mux is video for developers. Our mission is to democratize video by solving the hard problems developers face when building video. Video is a huge part of peopleâ€™s lives, and we want to help make it better.]]></description>
            <content:encoded><![CDATA[Mux is video for developers. Our mission is to democratize video by solving the hard problems developers face when building video: video encoding and streaming (Mux Video), video monitoring (Mux Data), and more. Video is a huge part of peopleâ€™s lives, and we want to help make it better.Weâ€™re committed to building a healthy team that welcomes a diverse range of backgrounds and experiences. We want people who care about our mission, are ready to grow, believe in our values (from Be Human to Turn Customers Into Fans), and want to make the people around them better.Youâ€™ll be joining a tight-knit team with experience at places like Google, YouTube, Twitch, Zencoder, Fastly, and more. Our founders previously started (and sold) Zencoder, an early leader in cloud video technology, and authored Video.js, the biggest HTML5 video player on the web. We organize Demuxed, the premiere conference for video engineers in the world.Weâ€™re backed by top investors like Coatue, Accel, Andreessen Horowitz, and Y Combinator. Youâ€™ll get to work with amazing companies: hundreds of startups, plus Reddit, Vimeo, Robinhood, CBSi, Discovery, PBS, and TED. Customers large and small love working with us and love our team.We are building something big together. Weâ€™d love to hear from you!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: HumanAlarm â€“ Real people knock on your door to wake you up]]></title>
            <link>https://humanalarm.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45203617</guid>
            <description><![CDATA[Lovable Generated Project]]></description>
            <content:encoded><![CDATA[
    
  

	Edit with 






















































	




]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UGMM-NN: Univariate Gaussian Mixture Model Neural Network]]></title>
            <link>https://arxiv.org/abs/2509.07569</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45202421</guid>
            <description><![CDATA[This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed nonlinearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feedforward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:This paper introduces the Univariate Gaussian Mixture Model Neural Network (uGMM-NN), a novel neural architecture that embeds probabilistic reasoning directly into the computational units of deep networks. Unlike traditional neurons, which apply weighted sums followed by fixed nonlinearities, each uGMM-NN node parameterizes its activations as a univariate Gaussian mixture, with learnable means, variances, and mixing coefficients. This design enables richer representations by capturing multimodality and uncertainty at the level of individual neurons, while retaining the scalability of standard feedforward networks. We demonstrate that uGMM-NN can achieve competitive discriminative performance compared to conventional multilayer perceptrons, while additionally offering a probabilistic interpretation of activations. The proposed framework provides a foundation for integrating uncertainty-aware components into modern neural architectures, opening new directions for both discriminative and generative modeling.
    

    
    
              
          Comments:
          10 pages, 2 figures
        

          Subjects:
          
            Machine Learning (cs.LG); Machine Learning (stat.ML)
        
          Cite as:
          arXiv:2509.07569 [cs.LG]
        
        
          Â 
          (or 
              arXiv:2509.07569v1 [cs.LG] for this version)
          
        
        
          Â 
                        https://doi.org/10.48550/arXiv.2509.07569
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Zakeria Sharif Ali [view email]          [v1]
        Tue, 9 Sep 2025 10:13:37 UTC (11 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dotter: Dotfile manager and templater written in Rust]]></title>
            <link>https://github.com/SuperCuber/dotter</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45202252</guid>
            <description><![CDATA[A dotfile manager and templater written in rust ðŸ¦€. Contribute to SuperCuber/dotter development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[What is Dotter?
Dotter is a dotfile manager and templater.
Dotfiles are configuration files that usually live in the home directory and start with a dot.
Often times, it is desirable to have a backup of all the configurations on your system, which is why a lot of users have their dotfiles saved in a git repository, then symlinking them to their target locations using ln -s.
However, there are several issues with that barebones approach:

Hard to keep track of what comes from where once you have more than a handful of dotfiles
Tedious to setup on a new machine - you need to manually create every single link
No real way to handle differences between machines - say you want the battery meter on your bar to not appear on your desktop machine

Dotter aims to solve all those problems by providing a flexible configuration and automatic templating or symlinking to the target locations.
Installation
Mac (Homebrew)
Dotter is available on homebrew using brew install dotter
Arch Linux
The following AUR packages are available:

dotter-rs-bin for a precompiled version of the latest release
dotter-rs for the latest release's source that is built on your machine
dotter-rs-git for the latest commit on master that is built on your machine

All of those are maintained by orhun - huge thanks to him!
Windows
Dotter is available on Scoop. Run scoop install dotter to install the latest release.
Others
Download the binary for your platform from the latest release and then put it in your $PATH or in your dotfile repository (then you'd run it with ./dotter).
Alternatively, Dotter is on crates.io, so if you have Rustup installed, run cargo install dotter.
Wiki
Check out the wiki for more information.
Among other things, it explains how to setup and configure Dotter, as well as giving insight on how the templating and deployment works.
Usage
Now that you've configured all the global and local file sections, you can simply run dotter from within your repository.
All the files will be deployed to their target locations.
Check out dotter -h for the command-line flags that Dotter supports:
A dotfile manager and templater written in rust

Usage: dotter [OPTIONS] [COMMAND]

Commands:
  deploy           Deploy the files to their respective targets. This is the default subcommand
  undeploy         Delete all deployed files from their target locations. Note that this operates on all files that are currently in cache
  init             Initialize global.toml with a single package containing all the files in the current directory pointing to a dummy value and a local.toml that selects that package
  watch            Run continuously, watching the repository for changes and deploying as soon as they happen. Can be ran with `--dry-run`
  gen-completions  Generate shell completions
  help             Print this message or the help of the given subcommand(s)

Options:
  -g, --global-config <GLOBAL_CONFIG>
          Location of the global configuration [default: .dotter/global.toml]
  -l, --local-config <LOCAL_CONFIG>
          Location of the local configuration [default: .dotter/local.toml]
      --cache-file <CACHE_FILE>
          Location of cache file [default: .dotter/cache.toml]
      --cache-directory <CACHE_DIRECTORY>
          Directory to cache into [default: .dotter/cache]
      --pre-deploy <PRE_DEPLOY>
          Location of optional pre-deploy hook [default: .dotter/pre_deploy.sh]
      --post-deploy <POST_DEPLOY>
          Location of optional post-deploy hook [default: .dotter/post_deploy.sh]
      --pre-undeploy <PRE_UNDEPLOY>
          Location of optional pre-undeploy hook [default: .dotter/pre_undeploy.sh]
      --post-undeploy <POST_UNDEPLOY>
          Location of optional post-undeploy hook [default: .dotter/post_undeploy.sh]
  -d, --dry-run
          Dry run - don't do anything, only print information. Implies -v at least once
  -v, --verbose...
          Verbosity level - specify up to 3 times to get more detailed output. Specifying at least once prints the differences between what was before and after Dotter's run
  -q, --quiet
          Quiet - only print errors
  -f, --force
          Force - instead of skipping, overwrite target files if their content is unexpected. Overrides --dry-run
  -y, --noconfirm
          Assume "yes" instead of prompting when removing empty directories
  -p, --patch
          Take standard input as an additional files/variables patch, added after evaluating `local.toml`. Assumes --noconfirm flag because all of stdin is taken as the patch
      --diff-context-lines <DIFF_CONTEXT_LINES>
          Amount of lines that are printed before and after a diff hunk [default: 3]
  -h, --help
          Print help
  -V, --version
          Print version

Contributing
Contributions to Dotter are welcome, whether in the form of a pull request or an issue (for bug repots, feature requests, or other helpful comments)
Support
Like what I do and want to encourage me to continue?
You can donate a small amount via Paypal.
Donations are not expected but greatly appreciated.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Charlie Kirk killed at event in Utah]]></title>
            <link>https://www.nbcnews.com/news/us-news/live-blog/live-updates-shooting-charlie-kirk-event-utah-rcna230437</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45202200</guid>
            <description><![CDATA[Conservative activist Charlie Kirk was shot and killed at an event he was hosting at Utah Valley University.]]></description>
            <content:encoded><![CDATA[FBI confident person of interest is the shooter,  senior official saysA senior FBI official told NBC News that the person of interest Utah authorities announced is the same person FBI Director Kash Patel was referring to in an X post before tonight's news conference. They are confident the person is the shooter, the official said. Authorities give mixed answers about suspect but say a person of interest is in custodyWhile Utah Public Safety Commissioner Beau Mason told reporters that a suspect is at large, Gov. Spencer Cox has said a person of interest is being interviewed in connection with Kirk's fatal shooting. That person is not George Zinn, the man who was initially detained after the shooting but was not a match to the shooter's description, Cox said. Mason said the only information authorities have about the shooter is from CCTV video from campus. "We do have that we're analyzing it, but it is security camera footage, so you can kind of guess what the quality of that is," Mason said. "We do know dressed in all dark clothing. We don't have much better description." 'This is a political assassination,' Utah governor saysUtah Gov. Spencer Cox described Kirk's killing as "a political assassination" in remarks to reporters. "Charlie Kirk was, first and foremost, a husband and a dad to young children," Cox said. "He was also very much politically involved, and that's why he was here on campus." Kirk was on the Utah Valley University campus today to debate ideas in line with his beliefs about the power of free speech, Cox said. He added that that is a constitutional right "foundational to the formation of our country." "Nothing I say can unite us as a country," Cox said. "Nothing I can say right now can fix what is broken. Nothing I can say can bring back Charlie Kirk." Person of interest still at large; Utah authorities saidAuthorities are still searching for a person of interest, Utah Public Safety Commissioner Beau Mason told reporters. "There was one shot fired, and once one victim, while the suspect is at large," Mason said. "I believe this was a targeted attack towards one individual." Mason clarified that a man was initially detained but did not match descriptions of the shooter. The man, George Zinn, was booked on a charge of obstruction of justice.The FBI and the state Department of Public Safety asked the public for any information that could lead to identifying a suspect. â€˜Do you run?â€™ Witnesses who took cover after shooting describe chaos Taylor Dufur and his wife, Haley Bermingham, were about 30 feet from Kirk when the fatal gunshot rang out, and the first reactions in the crowd near them were to get down.â€œWe heard the gunshot, and people just started dropping,â€ Dufur told NBC Newsâ€™ Tom Llamas by phone during live coverage. He said his biggest concern was lying on top of his wife to protect her.â€œI looked over and saw him as he was falling out of his chair, and I saw the blood coming out of his neck,â€ Dufur said. â€œIt was just â€” people panicking, kind of chaos.â€Bermingham said that at first she questioned what the sound was, and then a man in front of them started pushing people down, saying, â€œEveryone get down, get down, thatâ€™s a gunshot.â€â€œAnd then next thing you know, youâ€™re just trying to look for your exits,â€ Bermingham said. â€œHeâ€™s saying to stay down, but do you run? Do you stay down, do you crawl? You donâ€™t know what to do.â€The couple are fans of Kirk, and so are their kids. Dufur said everyone in the crowd was having a good time.â€œIt makes me angry. I do get very angry about it â€” that someone could have this much hate inside them as they want to take somebody out of his world,â€ Dufur saidâ€œItâ€™s just sad, as well, because our world is so divided at the moment,â€ he said. â€œAnd I feel like people just need to get along.â€White House lowers flags to half-staff in honor of KirkMatt Lavietes and Sarah DeanThe White House lowered its flags to half-staff within an hour after Trump confirmed Kirk's death. In addition, Trump issued a proclamation ordering flags to lowered to half-staff at the at all federal buildings, embassies, consular offices and other U.S. government properties abroad until sunset on Sept. 14, 2025.The flag atop the White House is lowered to half-staff on Wednesday.Kent Nishimura / Getty ImagesCharlie Kirk murdered at campus event, Turning Point USA saysDoha Madani and Madison LambertKirk was "murdered by a gunshot" at Utah Valley University, a spokesperson for Turning Point USA confirmed in a statement to NBC News. "We ask that everyone keep his family and loved ones in your prayers," the organization said. "We ask that you please respect their privacy and dignity in this time." Students offered police escorts as search for shooter continuesStudents who are still on campus are being offered police escorts as authorities continue to search for the shooter. "Police will come and escort you out of the building," the university wrote on X. "Public transportation available at Orem Station by pedestrian bridge." Charlie Kirk is dead, Trump said on Truth Social."The Great, and even Legendary, Charlie Kirk, is dead," Trump wrote. "No one understood or had the Heart of the Youth in the United States of America better than Charlie. "He was loved and admired by ALL, especially me, and now, he is no longer with us," Trump added. "Melania and my Sympathies go out to his beautiful wife Erika, and family. Charlie, we love you!"Johnson on Kirk shooting: â€˜Political violence has become all too commonâ€™ House Speaker Mike Johnson called the shooting â€œdetestable.â€ He condemned political violence, saying, â€œThis is not who we are.â€No suspect in custody, university saysLindsay Good and Doha MadaniAuthorities do not have a suspect in custody, Utah Valley University spokesperson Ellen Treanor said.The school had said earlier in an alert that police had detained a suspect.No security to get into Kirk's event, bystander saysDespite the large crowd Kirk drew to Utah Valley University, witness Tyler McGettigan said he was surprised he didn't have to pass any security to get in. The event required a ticket with a scannable code, which McGettigan told NBC News he printed out and brought with him. But he said he didn't need it to get into the amphitheater where Kirk was speaking. "I was expecting when I got here that I'd have to pass through some kind of security, but that just wasn't a thing," he said. "No one checked the barcode or the QR code. There was no checkpoint to get in. It was literally anyone could walk in if they wanted." Charlie Kirk speaks at Utah Valley University in Orem on Wednesday.Trent Nelson / The Salt Lake Tribune via ReutersKirk was shot from nearby building, university saysLindsay Good and Matt LavietesA spokesperson for Utah Valley University said in a statement that Kirk was shot from the university's Losee Center, roughly 200 feet away from where he was sitting.  Vance shares photo with KirkBen Kamisar and Henry J. GomezVice President JD Vance shared a photo of him with Kirk, as well as Donald Trump Jr. and political aide Andy Surabian, as he asked for prayers for Kirk. Kirk is a well-known conservative activist who is close with key members of Trump's political operation, including Vance and Donald Trump Jr. And Kirk helped push for Vance to be chosen as Trump's running mate, NBC News reported at the time.  Utah Valley University cancels classes, tells people to leave campusCampus is closed and classes are canceled until further notice at Utah Valley University. The school urged everyone to leave campus immediately in a post on X following the shooting. Kirk is shot and hospitalized, Turning Point USA saysDoha Madani and Cristian SantanaA spokesperson for Turning Point USA confirmed to NBC News that Kirk has been shot."He is in the hospital, and we are praying for him at this time," the organization said.Witness describes a loud shot and 'a bunch of blood'Doha Madani and Ben KamisarJustin Hickens, who said he was about 20 yards from the shooting, told NBC Newsâ€™ Tom Llamas that he saw Kirk get hit after a gunshot rang out during the event at Utah Valley University. â€œWe heard a big loud shot, I saw a bunch of blood come out of Charlie, I saw his body kind of kick back and go limp, and everybody dropped to the ground,â€ Hickens said. After a few moments, people started running out of the outdoor pavilion area, knocking over barricades. Hickens said he slowed down once he realized no more shots were being fired and he felt safe. "I kind of turned around, and all of a sudden I saw officers walking with this very elderly gentleman with white hair," Hickens said. "They had him arrested, they had him on his knees, and he was screaming about his rights and all that stuff. They cuffed him and put him away."  There were no metal detectors to get into the event, Hickens said, though there was security by Kirk himself. Trump has often complimented, appeared with Kirk+2Sarah Dean, Megan Shannon and Alexandra MarquezPresident Donald Trump has often spoken favorably about Kirk, including multiple times on the campaign trail last year.During a rally in Washington the day before he was sworn in to a second term in January, Trump told attendees: "Charlie Kirk is here. And I want to thank Charlie. Charlie is fantastic. I mean, this guy."A few weeks before, during a rally in Las Vegas on Dec. 22, 2024, Trump called Kirk "incredible," adding that he "is a special talent, and heâ€™s out there fighting."The president, then a candidate for the White House, also appeared last October at a Turning Point USA political rally in Phoenix."I want to express my tremendous gratitude to Charlie Kirk. Heâ€™s really an amazing guy. Amazing guy," Trump said.The president, on the campaign trail last June, also lauded Kirk at a Turning Point Action town hall in Phoenix, saying, "I want to thank a special person, Charlie Kirk, for his tremendous leadership."Who is Charlie Kirk: A prominent young conservative activist Kirk is a 31-year-old conservative activist and father of two who helped found Turning Point USA, a prominent nonprofit that activates conservative youth on school campuses. A well-known supporter of President Donald Trump and a close ally of many in his circle, Kirk spoke at the 2024 presidential convention just days after an assassination attempt on Trump. Elected officials react to reports of shooting, ask for thoughts and prayers for Kirk familyAlexandra Marquez and Rebecca KaplanLawmakers on both sides of the aisle reacted to reports that Charlie Kirk had been shot, with many sending prayers to the Kirk family."Pls join me in praying for Charlie Kirk," Sen. Chuck Grassley, R-Iowa, said in a post on X."Our prayers are with Charlie Kirk," Sen. Ted Cruz, R-Texas, said, also on X.Rep. Marjorie Taylor Greene, R-Ga., wrote in her own post that, "Charlie Kirk has been shot!Â Pray for him!"Several other senators, who were in and around the U.S. Capitol during the shooting, reacted to the news."Charlie is a very, actually, a very good friend of mine. It's, it scares you to death for he and his family," Sen. Rick Scott, R-Fla., told reporters,Sen. John Cornyn, R-Texas, told reporters that news of the shooting was, "shocking and tragic.""Weâ€™re all praying Charlie is going to be okay," Cornyn added.Among notable Democrats, California Gov. Gavin Newsom also weighed in on X, writing, "The attack on Charlie Kirk is disgusting, vile, and reprehensible. In the United States of America, we must reject political violence in EVERY form."Sen. Ruben Gallego, D-Ariz., also posted about the shooting, writing on X that the news is "horrific.""Letâ€™s hope he is well and keep him and his families in your prayers," Gallego added.Sen. Mark Kelly, D-Ariz., wrote in his own post on X that, "The news that Charlie Kirk was shot while speaking in Utah is shocking and horrible. Itâ€™s an example of political violence that has no place in our country."Kelly added that he and his wife, former Rep. Gabby Giffords, who was the victim of a shooting in 2010, are "thinking of him and his family."Sen. Chris Murphy, D-Conn., was giving a speech on the Senate floor this afternoon and addressed reports of the shooting, saying that, "we are all horrified watching images and following the news out of Utah, and we are sending all of our thoughts to Mr. Kirk, to his family, to survivors there."Utah Valley is the state's largest public university While BYU, the University of Utah and Utah Valley are probably the best-known schools in Utah, UVU calls itself the Beehive Stateâ€™s biggest public university with more than 46,000 students.The school wasÂ established in 1941 as Central Utah Vocational SchoolÂ and renamed Utah Valley College in 1993.The Orem institution is about 40 miles south of the state capital, Salt Lake City.Trump addresses shootingPresident Donald Trump just addressed the shooting on his social media site, Truth Social."We must all pray for Charlie Kirk, who has been shot," Trump wrote. "A great guy from top to bottom. GOD BLESS HIM!"Suspect in custody, university saysA suspect is in custody and police are investigating the shooting, Utah Valley University told students in an alert this afternoon.Videos online show Kirk recoiling after shot heardVideos circulating across social media show the moment a shot is heard in the crowd. Kirk was sat in a tent with the "American Comeback Tour" logo when a shot is heard and Kirk physically recoils, slumping down in his seat. Blood is seen flowing down from his neck and the crowd audibly panics and begins to run. FBI monitoring shooting reports; Vance asks people to pray for KirkThe FBI is closely monitoring reports of a shooting involving Kirk, according to FBI Director Kash Patel. "Our thoughts are with Charlie, his loved ones, and everyone affected," Patel wrote in a post on X. "Agents will be on the scene quickly and the FBI stands in full support of the ongoing response and investigation."Vice President JD Vance responded to reports that Kirk was shot, encouraging others to "say a prayer" for the right-wing activist in a post on X. Kirk on campus for 'American Comeback Tour' appearanceKirk was being hosted by the Utah Valley University chapter of Turning Point USA. The organization's website shows a series of dates at college campuses across the country. The University of Minesosta at Northrop described it as a "high-energy evening featuring a candid conversation about conservative values." Kirk was the only person shot, university saysA spokesperson for Utah Valley University said that Kirk was the sole person shot at the event.Charlie Kirk taken off campus after shots heard, university saysDoha Madani and Lindsay GoodTurning Point USA founder Charlie Kirk was about 20 minutes into a presentation when witnesses heard shots fired from a nearby building, a spokesperson for Utah Valley University told NBC News.The spokesperson said, to the best of the university's knowledge, Kirk was hit and taken with his security team away from the premises, and the courtyard was cleared.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Haystack â€“ Review pull requests like you wrote them yourself]]></title>
            <link>https://haystackeditor.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45201703</guid>
        </item>
        <item>
            <title><![CDATA[Defeating Nondeterminism in LLM Inference]]></title>
            <link>https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200925</guid>
            <description><![CDATA[Reproducibility is a bedrock of scientific progress. However, itâ€™s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves â€œsamplingâ€, a process that converts the language modelâ€™s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isnâ€™t deterministic (see here or here).]]></description>
            <content:encoded><![CDATA[
    
    
    
    Reproducibility is a bedrock of scientific progress. However, itâ€™s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves â€œsamplingâ€, a process that converts the language modelâ€™s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isnâ€™t deterministic (see here or here).
But why arenâ€™t LLM inference engines deterministic? One common hypothesis is that some combination of floating-point non-associativity and concurrent execution leads to nondeterminism based on which concurrent core finishes first. We will call this the â€œconcurrency + floating pointâ€ hypothesis for LLM inference nondeterminism. For example, a recent arXiv preprint writes:

Floating-point arithmetic in GPUs exhibits non-associativity, meaning $(a + b) + c \neq a + (b + c)$ due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.


You can also find the â€œconcurrency + floating pointâ€ hypothesis repeated by others, like here (â€œThere are speed tradeoffs, and in order to make the endpoints fast GPUs are used, which do parallel [nondeterministic] calculations. Any modern GPU neural net calculations will be subject to these."), or here (â€œBecause GPUs are highly parallelized, the ordering of additions or multiplications might be different on each execution, which can cascade into small differences in output.").
While this hypothesis is not entirely wrong, it doesnâ€™t reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. Weâ€™re definitely using floating-point numbers. And our GPU definitely has a lot of concurrency. Why donâ€™t we see nondeterminism in this test?
A = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
B = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
ref = torch.mm(A, B)
for _ in range(1000):
    assert (torch.mm(A, B) - ref).abs().max().item() == 0
To understand the true cause of LLM inference nondeterminism, we must look deeper.
Unfortunately, even defining what it means for LLM inference to be deterministic is difficult. Perhaps confusingly, the following statements are all simultaneously true:

Some kernels on GPUs are nondeterministic.
However, all the kernels used in a language modelâ€™s forward pass are deterministic.
Moreover, the forward pass of an LLM inference server (like vLLM) can also be claimed to be deterministic.
Nevertheless, from the perspective of anybody using the inference server, the results are nondeterministic.

In this post, we will explain why the â€œconcurrency + floating pointâ€ hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.
The original sin: floating-point non-associativity
Before talking about nondeterminism, itâ€™s useful to explain why there are numerical differences at all. After all, we typically think of machine learning models as mathematical functions following structural rules such as commutativity or associativity. Shouldnâ€™t there be a â€œmathematically correctâ€ result that our machine learning libraries should provide us?
The culprit is floating-point non-associativity. That is, with floating-point numbers:
$$ (a + b) + c \neq a + (b + c) $$(0.1 + 1e20) - 1e20
>>> 0
0.1 + (1e20 - 1e20)
>>> 0.1
Ironically, breaking associativity is what makes floating-point numbers useful.
Floating-point numbers are useful because they allow for a â€œdynamicâ€ level of precision. For the purposes of explanation, we will use base 10 (instead of binary), where floating-point numbers are in the format $\text{mantissa} * 10^\text{exponent}$. We will also use 3 digits for the mantissa and 1 digit for the exponent.
For example, for the value 3450, we can represent it exactly as $3.45 * 10^3$. We can also represent much smaller values like 0.486 as $4.86 * 10^{-1}$. In this way, floating point allows us to represent both very small as well as very large values. In the sciences, we might say that floating point allows us to maintain a constant number of â€œsignificant figuresâ€.
If you add together two floating-point numbers with the same exponent, it looks similar to integer addition. For example, 123 ($1.23 * 10^2$) + 456 ($4.56 * 10^2$) results in 579 ($5.79 * 10^2$).
But what happens when we add two floating-point numbers with different exponents, such as 1230 and 23.4?  In this case, the exact result is 1253.4. However, we can only maintain 3 digits of precision at a time. Floating-point addition will thus drop the last 2 digits and obtain the value $1.25 * 10^3$ (or 1250).


        
            1.23 Ã— 10Â²
        
        +
        
            3.45 Ã— 10Â¹
        
        =
        
            1.575 Ã— 10Â²
            Exact: 1575
        
    



We require 3 digits of precision to represent 1230 and 3 digits of precision to represent 23.4. However, adding these 2 numbers together results in a number that requires 5 digits of precision to represent (1253.4). Our floating-point format must then drop the 34 off the end. In some sense, we have effectively rounded our original 23.4 to 20.0 before adding it. 

At this point, however, weâ€™ve destroyed information. Note that this can happen every time we add two floating-point numbers with different â€œscalesâ€ (i.e. different exponents). And adding together floating-point numbers with different exponents happens all of the time. In fact, if we could guarantee that we never needed different exponents, we could just use integers!
In other words, every time we add together floating-point numbers in a different order, we can get a completely different result. To take an extreme example, there are 102 possible different results for summing this array depending on the order.
import random

vals = [1e-10, 1e-5, 1e-2, 1]
vals = vals + [-v for v in vals]

results = []
random.seed(42)
for _ in range(10000):
    random.shuffle(vals)
    results.append(sum(vals))

results = sorted(set(results))
print(f"There are {len(results)} unique results: {results}")

# Output:
# There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]
Although this is the underlying cause for non-identical outputs, it does not directly answer where the nondeterminism comes from. It doesnâ€™t help us understand why floating-point values get added in different orders, when that happens, nor how it can be avoided.
The answers lie in how kernels are implemented.
Why donâ€™t kernels always add numbers in the same order?
As mentioned above, one common explanation for why kernels add numbers in different orders is the â€œconcurrency + floating pointâ€ hypothesis. The hypothesis states that if the order in which concurrent threads finish is nondeterministic and the accumulation order depends on the order in which concurrent threads finish (such as with an atomic add), our accumulation order will be nondeterministic as well.
Confusingly, although this can lead to nondeterministic kernels, concurrency (and atomic adds) end up being completely uninvolved in LLM inference nondeterminism! To explain what the real culprit is, letâ€™s first understand why modern GPU kernels rarely need atomic adds.
When are atomic adds needed?
Typically a GPU launches a program concurrently across many â€œcoresâ€ (i.e. SMs). As the cores have no inherent synchronization among them, this poses a challenge if the cores need to communicate among each other. For example, if all cores must accumulate to the same element, you can use an â€œatomic addâ€ (sometimes known as a â€œfetch-and-addâ€). The atomic add is â€œnondeterministicâ€ â€” the order in which the results accumulate is purely dependent on which core finishes first.
Concretely, imagine that you are reducing a 100-element vector with 100 cores (e.g. torch.sum()). Although you can load all 100 elements in parallel, we must eventually reduce down to a single element. One way to accomplish this is with some kind of â€œatomic addâ€ primitive, where the hardware guarantees that all additions will be processed but does not guarantee the order.




 The atomic add ensures that every core's contributions will be reflected in the final sum. However, it makes no guarantee about what order the contributions will be added. The order depends entirely on which core finishes first, a nondeterministic property. Thus, executing the same parallel program multiple times can result in nondeterministic outputs. 

This is usually what folks mean by â€œnondeterminismâ€ â€” you execute the same kernel twice with exactly the same inputs and you get a different result out. This is known as run-to-run nondeterminism, where you run the same python script twice with the exact same dependencies but get a different result.
Although concurrent atomic adds do make a kernel nondeterministic, atomic adds are not necessary for the vast majority of kernels. In fact, in the typical forward pass of an LLM, there is usually not a single atomic add present.
This may be surprising, given that parallelizing a reduction can benefit from atomic adds. There are two main reasons why atomic adds do not end up being needed.

There is often sufficient parallelism along the â€œbatchâ€ dimension that we donâ€™t need to parallelize along the reduction dimension. For example, letâ€™s say that instead of reducing a single 100-dim vector we were reducing 500 vectors in parallel. In this case, we can reduce an entire vector in each core and allow every core to operate on a different vector.
Over time, most neural network libraries have adopted a variety of strategies for achieving determinism without sacrificing performance. For example, we can perform a â€œsplitâ€ (or tree) reduction, where we split the 100-element reduction into five 20-element reductions (thus achieving five-way parallelism). Then, to combine the remaining five elements, we can either perform a separate â€œclean-upâ€ reduction (which isnâ€™t parallelized, but operates over few enough elements to be cheap) or utilize a semaphore (which ensures that each concurrent thread-block will accumulate in a deterministic order).The semaphore strategy can be found described here.

Due to these two factors, avoiding atomics adds is a negligible performance penalty for the vast majority of neural network operations.
There are still a couple of common operations that have significant performance penalties for avoiding atomics. For example, scatter_add in PyTorch (a[b] += c). The only one commonly used in LLMs, however, is FlashAttention backward.Fun fact: did you know that the widely used Triton implementations of FlashAttention backward actually differ algorithmically from Tri Daoâ€™s FlashAttention-2 paper? The standard Triton implementation does additional recomputation in the backward pass, avoiding atomics but costing 40% more FLOPs!
However, the forward pass of an LLM involves no operations that require atomic adds. Thus, the forward pass in an LLM is in fact â€œrun-to-run deterministic.â€











Model

Deterministic



User requests




Other user requests




Output




















































From the perspective of the inference server, it is deterministic. Given the exact same user requests, it will always provide the same deterministic output. 

Wikipedia writes that â€œa deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.â€ And in this case, given the exact same inputs (i.e. the exact requests the inference server is processing), the forward pass always produces the exact same outputs.
However, the forward pass itself being â€œdeterministicâ€ is not sufficient to ensure that a system that includes it is deterministic. For example, what if our requestâ€™s output depended on the parallel user requests (e.g. batch-norm)? Since each individual request has no way of knowing what the parallel requests will be, from their perspective our overall LLM inference is also nondeterministic!
As it turns out, our requestâ€™s output does depend on the parallel user requests. Not because weâ€™re somehow leaking information across batches â€” instead, itâ€™s because our forward pass lacks â€œbatch invarianceâ€, causing our requestâ€™s output to depend on the batch size of our forward pass.
Batch Invariance and â€œDeterminismâ€
To explain batch invariance, letâ€™s simplify the system and look solely at matmuls. You can assume that all matmul implementations are â€œrun-to-run deterministic."This is not totally true, but most common matmul implementations do have this property.  However, they are not â€œbatch-invariant.â€ In other words, when the batch size changes, each element in the batch can get different results.
This is a fairly unusual property from a mathematical perspective. Matrix multiplication should be â€œindependentâ€ along every element in the batch â€” neither the other elements in the batch nor how large the batch is should affect the computation results of a specific element in the batch.
However, as we can observe empirically, this isnâ€™t true.
import torch
torch.set_default_device('cuda') 

B = 2048
D = 4096
a = torch.linspace(-1000, 1000, B*D).reshape(B, D)
b = torch.linspace(-1000, 1000, D*D).reshape(D, D)
# Doing a matrix vector multiplication by taking
# the first element of the batch
out1 = torch.mm(a[:1], b)
# Doing a matrix matrix multiplication and then taking
# the first element of the batch
out2 = torch.mm(a, b)[:1]
print((out1 - out2).abs().max()) # tensor(1669.2500, device='cuda:0')
Note that this is â€œrun-to-run deterministic.â€ If you run the script multiple times, it will deterministically return the same result.It is not â€œhardware/software version invariantâ€ â€” your GPU/PyTorch version may return a different value, but it should deterministically return the same value.
However, when a non-batch-invariant kernel is used as part of a larger inference system, the system can become nondeterministic. When you make a query to an inference endpoint, the amount of load the server is under is effectively â€œnondeterministicâ€ from the userâ€™s perspective. The load determines the batch size that the kernels are run under, and thus changes the eventual result of each individual request!


















Model

Deterministic
Nondeterministic



User requests




Other user requests




Output


































































Although the inference server itself can be claimed to be "deterministic", the story is different for an individual user. From the perspective of an individual user, the other concurrent users are not an "input" to the system but rather a nondeterministic property of the system. This makes LLM inference "nondeterministic" from the perspective of each user.

If you compose some property under which the kernel is not invariant (i.e. batch-size) with nondeterminism of that property (i.e. the load the server is under), you get a nondeterministic system.
In other words, the primary reason nearly all LLM inference endpoints are nondeterministic is that the load (and thus batch-size) nondeterministically varies! This nondeterminism is not unique to GPUs â€” LLM inference endpoints served from CPUs or TPUs will also have this source of nondeterminism.
So, if weâ€™d like to avoid nondeterminism in our inference servers, we must achieve batch invariance in our kernels. In order to understand how that can be achieved, letâ€™s first take a look at why kernels donâ€™t have batch invariance in the first place.
How do we make kernels batch-invariant?
In order to make a transformer implementation batch-invariant, we must make every kernel batch-invariant. Luckily, we can assume that every pointwise operation is batch-invariant.Although this is true for all kernels in say, PyTorch, itâ€™s not inherently true. For example, there are some kernel implementations on CPU that will use vectorized intrinsics on some parts of the array and non-vectorized intrinsics on other parts, and these intrinsics donâ€™t necessarily always have bitwise identical numerics. Thus, we only need to worry about the 3 operations that involve reductions â€” RMSNorm, matrix multiplication, and attention.Reductions related to parallelism are out of the scope of this discussion, but the same principles apply. One factoid that may be useful is that NVLink-Sharp in-switch reductions are deterministic on Blackwell as well as Hopper with CUDA 12.8+. As is the case with many things, this information can be found on NCCLâ€™s github issues
Conveniently, these are also ordered in ascending levels of difficulty. Each one requires some additional considerations to achieve batch invariance with reasonable performance. Letâ€™s talk about RMSNorm first.
Batch-Invariant RMSNorm











































Data Parallel RMSNorm Ideally, we'd like to avoid communication between cores in our parallelization strategy. One way to achieve that is by assigning one batch-element to each core, thus guaranteeing that each reduction is done entirely within a single core. This is what's known as a "data-parallel" strategy, since we're simply parallelizing along a dimension that doesn't require communication. In this example, we have four rows and four cores, saturating our cores. 

RMSNorm can be implemented as:
# x: [batch_size, hidden_dim]
# weight: [hidden_dim]
def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
The requirement for batch invariance is that the reduction order for each element must be fixed regardless of the batch-size of the kernel. Note that this doesnâ€™t mean we must always use the same reduction strategy. For example, if we change the number of elements weâ€™re reducing over, we can still be batch-invariant even if our reduction strategy changes.The Quack blog post has some nice examples showing the hierarchy of various reduction strategies you can do (e.g. thread reduction, warp reduction, block reduction, cluster reduction).
Thus, we only break batch invariance when our batch-size affects the reduction strategy.
Letâ€™s look at the standard parallelism strategy for RMSNorm. Generally, parallel algorithms benefit from minimizing communication across cores. For the purpose of this discussion you can assume that when we refer to â€œcoresâ€ we mean SMs. More specifically, the property here thatâ€™s important is that the # of threadblocks our kernel launches is greater than the # of SMs. So, one strategy we can start with is to assign each batch element to one core, as seen in the above figure.
Increasing our batch size doesnâ€™t affect our reduction strategy; if a batch size of 200 provides sufficient parallelism to our kernel then a batch size of 2000 will definitely provide sufficient parallelism.



































































Data Parallel RMSNorm for larger batches Extending the data-parallel strategy to larger batches is fairly straightforward --- instead of having each core handle one row you allow each core to handle different rows sequentially. This preserves batch invariance as the reduction strategy for each batch element remains identical. 

On the other hand, decreasing the batch size can pose challenges. Because we assign each batch element to one core, decreasing our batch size will eventually lead to having more cores than batch elements, leaving some cores idle.
Upon encountering this situation, a good kernel engineer would reach for one of the solutions mentioned in the prior section (atomic adds or split reductions), maintaining good parallelism and thus, good performance. Unfortunately, this changes the reduction strategy, preventing this kernel from being batch-invariant.































Split-Reduction RMSNorm If we have a small batch size, our data-parallel strategy may no longer have sufficient parallelism to saturate our cores. In this case, it may be more efficient to "split" a reduction among multiple cores, allowing us to fully utilize our GPU. However, this loses batch invariance, as we are no longer reducing each element in the same order.

The easiest solution is to simply ignore these cases altogether. This is not completely unreasonable â€” a small batch size means that the kernel is likely to execute quickly anyways, and so a slowdown may not be catastrophic.
If we were compelled to optimize this use case, one approach would be to consistently use a reduction strategy that has enough parallelism even for very small batch sizes. Such a reduction strategy would lead to an excess amount of parallelism for larger batch sizes but would allow us to achieve decent (but not peak) performance across the entire range of sizes.
Batch-Invariant Matrix Multiplication














































































Data Parallel Matmul Similar to RMSNorm, the standard parallelism strategy for matmuls is a "data-parallel" strategy, keeping the entire reduction in one core. It is most straightforward to think about splitting the output tensor into 2D tiles and assigning each tile to a different core. Each core then computes the dot products that belong to that tile, once again performing the entire reduction within one core.
Unlike for RMSNorm, additional constraints around arithmetic intensity and utilizing tensorcores force us to split 2D tiles instead of individual output elements for efficient matmul kernels.


At its core, you can view matrix multiplication as simply a pointwise operation followed by a reduction. Then, if we parallelize our matrix multiplication by chunking the output into tiles, we have an analogous â€œdata-parallelâ€ kernel strategy that keeps each reduction within one core.
Also similar to RMSNorm, it is possible for our â€œbatchâ€ dimensions (M and N) to become too small, forcing us to split along the reduction dimension (K). Despite having two â€œbatchâ€ dimensions, matmuls also require us to have much more â€œworkâ€ per core in order to leverage tensorcores effectively. For example, if you have a [1024, K] x [K, 1024] matmul and a standard 2D tile size of [128, 128], a data-parallel strategy would only be able to split this matmul into 64 cores, insufficient to saturate the GPU.
Splitting along the reduction dimension in a matmul is known as a Split-K Matmul. And just like RMSNorm, using this strategy breaks batch invariance.

Another interesting parallelism strategy for matmuls is stream-k. Stream-k is interesting because it has even less invariance than typical matmuls. As discussed, most matmul libraries are not batch-invariant, but theyâ€™re at least what you could call batch-position-invariant (i.e. changing the position of the element within the batch does not affect numerics). However, stream-k is not batch-position-invariant either! Its core insight is that you can get cleaner load-balancing by splitting along k in different ways for different output tiles, but taking advantage of this makes our kernel not batch-position-invariant either. 






















































Split-K Matmul If our batch dimension is fairly small we may not have enough parallelism and require a split-k matmul. In this example, we split each reduction across two cores, which would accumulate separately and then combine their results at the end. However, splitting each reduction across two cores allows us to still leverage eight cores.



Thereâ€™s an additional complexity with matmuls â€” tensor core instructions. Whereas with reductions we could simply operate on one row at a time, efficient matrix multiplication kernels must operate on an entire â€œtileâ€ at a time.
Each tensor-core instruction (like say, wgmma.mma_async.sync.aligned.m64n128k16) may have a different reduction order internally. One reason to use a different tensor-core instruction might be that the batch size is very small. For example, if we use a tensor-core PTX instruction that operates on a tile of length 256 but the batch size is only 32, weâ€™re wasting almost all of that compute! At a batch-size of 1, the fastest kernels usually donâ€™t use tensor cores at all.







































































Padded Tensor-Core Instructions If the batch size is too small, we may be in our situation where we can't fit even one of our 2D tiles in the output. In this case, it is most efficient to switch to a smaller tensor-core instruction or eschew tensor-cores altogether! However, both of these options prevent our kernel from being batch-invariant.

So, the easiest way to ensure batch invariance for matmuls is to compile one kernel configuration and use that for all shapes. Although we will lose some performance, this isnâ€™t typically disastrous in LLM inference. In particular, split-k is most needed when both M and N are small, and luckily in our case, N (i.e. the model dim) is usually pretty large!

  Despite obtaining batch invariance, we only lose about 20% performance compared to cuBLAS. Note that this is not an optimized Triton kernel either (e.g. no TMA). However, some of the patterns in performance are illustrative of where our batch-invariant requirement loses performance. First, note that we lose a significant amount of performance at very small batch sizes due to an overly large instruction and insufficient parallelism. Second, there is a "jigsaw" pattern as we increase the batch-size that is caused by quantization effects (both tile and wave) that are typically ameliorated through changing tile sizes. You can find more on these quantization effects here.
  
  

Batch-Invariant Attention





















































FlashAttention2 Strategy We parallelize along Q, and reduce along K/V simultaneously. This means that our entire reduction can be kept within a single core, making it another data-parallel strategy.

After obtaining batch invariance for matmuls, attention introduces two additional wrinkles â€” fittingly,  because it contains two matmuls.

As opposed to only reducing over the feature dimension like both RMSNorm and matmuls, we now reduce over the feature dimension and sequence dimension.
Due to the above, attention must deal with a variety of inference optimizations that affect how sequences get processed (chunked prefill, prefix caching, etc.).

Thus, to achieve determinism in LLM inference our numerics must be invariant to both how many requests are processed at once and how each request gets sliced up in the inference engine.
Letâ€™s first walk through the standard parallelism strategy for attention, first introduced in FlashAttention2. Similar to RMSNorm and Matmul, the default strategy is a â€œdata-parallelâ€ strategy. Since we reduce along the key/value tensors, a data-parallel strategy can only parallelize along the query tensor.
For example, depending on the inference engineâ€™s choices, itâ€™s possible that a sequence might get processed in several parts (such as in chunked prefill) or perhaps all at once (if the prefill isnâ€™t split up). In order to achieve â€œbatch invarianceâ€, itâ€™s necessary that the reduction order for a given token does not depend on how many other tokens from its sequence are being simultaneously processed. If you reduce over the K/V values in the KV cache separately from the K/V values in the current tokens being processed (like in vLLMâ€™s Triton attention kernel), this canâ€™t be achieved. For example, when processing the 1000th query token in a sequence, the reduction order must be identical regardless of whether 0 tokens are in the KV cache (prefill) or 999 tokens are in the KV cache (decoding).






























































FlashAttention with a KV Cache The reason why explicitly handling the KV cache separately from the current KV values breaks batch invariance is a bit subtle and is related to "boundary conditions". In particular, imagine your block size is 32 but we currently have 80 elements in our KV cache. We then compute an additional 48 elements that aren't cached. In this case, we need three blocks (two full and one masked) to compute "P cache" and another two blocks (one full and one masked) to compute "P". This is therefore five total blocks to compute our reduction when we only have four total blocks (i.e. 128) of elements to compute, which will definitely change our reduction order. 
For example, if we instead had no elements in our KV Cache and were processing 128 elements altogether, we need to have identical numerics in both of these situations to ensure â€œbatch invarianceâ€ for attention.
 

To resolve this, we can just update the KV cache and page table before the attention kernel itself, ensuring that our keys and values are always consistently laid out regardless of how many tokens are being processed.
With this additional detail (as well as all the things mentioned in the previous section, like consistent tile sizes), we are able to achieve a batch-invariant attention implementation!
However, there is a significant problem here. Unlike with matrix multiplication, the attention shapes we see in LLM inference often do require a split-reduction kernel, often known as Split-KV or FlashDecoding. This is because if we donâ€™t parallelize along the reduction, we can only parallelize along the batch dimension, head dimension, and â€œquery lengthâ€ dimension. In the decode stage of attention, query length is very small, and so unless we have a very large batch size we are often unable to saturate the GPU.
Unfortunately, itâ€™s not as easy to ignore this case as it was for RMSNorm and Matmuls. For example, if you have a very long KV cache, the attention kernel may take a very long time despite only processing one request.



































Fixed # Split-KV Strategy (i.e. FlashDecode) If our query length becomes very small (like it does during decoding), we may end up in a situation where there is very little parallelism in our kernel at all. In these cases, we'll need to once again split along the reduction dimension --- the KV dimension this time. The typical strategy for how to split along the KV dimension is to figure out how much parallelism we need and then divide the KV dimension evenly. For example, if our KV length was 1000 and we needed 4 splits, each core would handle 250 elements.
This unfortunately also breaks batch invariance, as our precise reduction strategy depends on how many query tokens from the sequence weâ€™re processing in any given request.


Furthermore, the split-reduction strategies commonly used for attention also pose challenges for batch invariance. For example, FlashInferâ€™s â€œbalanced scheduling algorithmâ€ chooses the largest split-size that can still saturate all the GPUâ€™s cores, thus making the reduction strategy not â€œbatch-invariantâ€. However, unlike with RMSNorm/Matmuls, itâ€™s not sufficient to choose a fixed number of splits regardless of the batch size.
Instead, to achieve batch invariance, we must adopt a â€œfixed split-sizeâ€ strategy. In other words, instead of fixing the # of splits, we fix the size of each split and then end up with a varying number of splits. In this manner, we can guarantee that regardless of how many tokens weâ€™re processing, we always perform the identical reduction order. This requires some internal FlexAttention changes that are not included in our code release. We will upstream them in the near future!




































Fixed Size Split-KV Strategy 
The only difference between this strategy and the previous strategy is that our splits are now "fixed size". For example, if our KV length was 1000, instead of splitting it into four even length 250 splits, we would split it into three fixed-size length 256 splits and one length 232 split.
This allows us to preserve batch invariance as our reduction strategy is no longer dependent on how many query tokens weâ€™re processing at once!
 

Implementation
We provide a demonstration of deterministic inference on top of vLLM by leveraging its FlexAttention backend as well as torch.Library.
Through torch.Library, weâ€™re able to substitute out most of the relevant PyTorch operators in an unintrusive way. You can find the library of â€œbatch-invariantâ€ kernels at thinking-machines-lab/batch-invariant-ops, as well as the vLLM example of running in â€œdeterministicâ€ mode.
Experiments
How nondeterministic are completions?
We use Qwen/Qwen3-235B-A22B-Instruct-2507 and sample 1000 completions at temperature 0 with the prompt â€œTell me about Richard Feynmanâ€ (non-thinking mode), generating 1000 tokens each. Surprisingly, we generate 80 unique completions, with the most common of these occuring 78 times.
Looking at where the completions differ, we see that the completions are actually identical for the first 102 tokens! The first instance of diverging completions occurs at the 103rd token. All completions generate the sequence â€œFeynman was born on May 11, 1918, inâ€ However, 992 of the completions go on to generate â€œQueens, New Yorkâ€ whereas 8 of the completions generate â€œNew York Cityâ€.

On the other hand, when we enable our batch-invariant kernels, all of our 1000 completions are identical. This is what we would mathematically expect from our sampler, but we arenâ€™t able to achieve deterministic results without our batch-invariant kernels.

Performance
We have not put a significant effort into optimizing the performance of the batch-invariant kernels here. However, letâ€™s run some experiments to verify that our performance remains usable.
We will set up an API server with one GPU running Qwen-3-8B, and request 1000 sequences with an output length of between 90 and 110.

  
      
          Configuration
          Time (seconds)
      
  
  
      
          vLLM default
          26
      
      
          Unoptimized Deterministic vLLM
          55
      
      
          + Improved Attention Kernel
          42
      
  

Much of the slowdown comes from the fact that the FlexAttention integration in vLLM has not been heavily optimized yet. Nevertheless, we see that performance is not disastrous.
True on-policy RL
As researchers have noted, the different numerics between training and inference implicitly turns our on-policy RL into off-policy RL.
Of course, it is impossible to get bitwise identical results between training and inference if we canâ€™t even get bitwise identical results from two identical inference requests. Then, deterministic inference enables us to also modify our training stack to obtain bitwise identical results between sampling and training, thus resulting in true on-policy RL.
We run experiments in a RLVR setup on Bigmath with the RL policy initialized from the Qwen 2.5-VL instruct 8B with a max rollout length of 4096.
If we train without off-policy correction (i.e. importance weighting), our reward collapses partway through training, whereas adding an off-policy correction term  allows training to proceed smoothly. But, if we achieve bitwise identical results between our sampler and trainer, we are fully on policy (i.e. 0 KL divergence) and can also train smoothly.
We can also plot the KL-divergence in logprobs between our sampler and trainer, where all 3 runs have notably different behavior. When running with importance weighting, it stays around 0.001 with occasional spikes. However, running without importance weighting eventually leads to a spike in KL-divergence around the same time that reward crashes. And, of course, when running â€œTrue On-Policy RLâ€, our KL-divergence stays flat at 0, indicating that there is no divergence between the training policy and sampling policy.



  Note that the run without importance weighting has a significant loss spike around Step 318, and this comes with a correspond ing spike in KL-divergence of logprobs. Meanwhile, either using an off-policy correction or running with "True On-Policy" allows RL to continue smoothly. The blue line showing "True On-Policy" is not a bug - it's just a flat line at 0. 
  
  

Conclusion
Modern software systems contain many layers of abstractions. In machine learning, when we run into nondeterminism and subtle numerical differences it can often be tempting to paper over them. After all, our systems are already â€œprobabilisticâ€, so whatâ€™s wrong with a little more nondeterminism? Whatâ€™s wrong with bumping up the atol/rtol on the failing unit test? The difference in logprobs between the trainer and the sampler probably isnâ€™t a real bug, right?
We reject this defeatism. With a little bit of work, we can understand the root causes of our nondeterminism and even solve them! We hope that this blog post provides the community with a solid understanding of how to resolve nondeterminism in our inference systems and inspires others to obtain a full understanding of their systems.
Citation
Please cite this work as:
He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference", 
Thinking Machines Lab: Connectionism, Sep 2025.
Or use the BibTeX citation:
@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}

    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA["No Tax on Tips" Includes Digital Creators, Too]]></title>
            <link>https://www.hollywoodreporter.com/business/business-news/no-tax-on-tips-guidance-creators-trump-treasury-1236366513/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200024</guid>
            <description><![CDATA[A U.S. Treasury Department document includes podcasters, social media influencers and streamers among occupations eligible for tax-free tips.]]></description>
            <content:encoded><![CDATA[
	President Trumpâ€™s One Big Beautiful Bill Act may have quietly changed the economics of the creator economy.




	The U.S. Treasury Department this past week released a list of occupations â€œthat customarily and regularly received tipsâ€ and thus will be eligible for the administrationâ€™s flagship â€œno tax on tipsâ€ policy, which will let eligible taxpayers deduct their tipped income, within certain limits.






	And while the list includes the obvious (bartenders, food servers, casino dealers and housekeepers are all there) it also includes some surprising jobs that could alter the economics of the creator economy.

	







	Thatâ€™s because the Treasury Department has determined that â€œdigital content creatorsâ€ are eligible, including podcasters, social media influencers and streamers.




	Comedians, singers, musicians, DJs and magicians are also included, though that is more relevant to the wedding performer crowd than Grammy-winners.




	The change could cause digital creators to rethink how they seek income. Platforms like TikTok, YouTube, Twitch and Snapchat all offer a variety of ways for creators to generate income, be it a share of advertising revenue or creator funding programs, or options to launch subscription tiers for their channels or profiles. But they also give creators the option to turn on tips or gifts. If revenue from user tips or gifts is eligible, while recurring subscription revenue is not, it could shift how streamers, podcasters or influencers ask their followers to support them.




	To be sure, there are limitations: The tax deduction is capped at $25,000 per year, and it begins to phase out at $150,000 in income for single filers and $300,000 for married joint filers. The act also provides that tips do not qualify for the deduction if they are received â€œin the course of certain specified trades or businesses â€” including the fields of health, performing arts, and athletics,â€ Treasury says, further limiting the deduction opportunity for some in entertainment-adjacent lines of work.

	





	But by making influencers, Twitch streamers and podcasters eligible, the administration has nonetheless changed the incentive structure for digital creators, and the ramifications could be felt across the creator economy in the name of tax efficiency (Donâ€™t be surprised if users are asked to like, subscribe, and tip).




	Platforms may also develop more ways to more prominently feature tips and gifts, pushing creators to add more opportunities for that income.




	But the inclusion of digital creators is also a recognition of how the power dynamics have shifted in media.




	Podcasters and creators, as everyone knows by now, have emerged as a driving force in todayâ€™s political climate, and the classification by Treasury could push more people to consider joining the fray or ramping up their content, as long as it is tipped, of course.














]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I didn't bring my son to a museum to look at screens]]></title>
            <link>https://sethpurcell.com/writing/screens-in-museums/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199931</guid>
            <description><![CDATA[When I was a kid in the â€™80s, one of my two favorite places on Earth was The Franklin Institute (TFI) in downtown Philadelphia. We lived a couple hours away so a visit was a rare and precious thing. I think I only visited two or three times but it left an indelible impression on me. I remember wandering in amazement through its enormous spaces getting to actually play with amazing and interesting things. I remember sweeping off a table and then filling an overhanging funnel pendulum with sand, setting it going, and watching it create unexpected patterns on the table. I remember running through the gigantic model heart with other kids. I remember the overpowering joy of being in an actual monumental marble temple of curiosity and fascination. So I was filled with anticipation a couple weeks ago when, during a family trip to the East Coast, we managed to squeeze in a visit to TFI with our six-year-old son.]]></description>
            <content:encoded><![CDATA[ When I was a kid in the â€™80s, one of my two favorite places on Earth was The Franklin Institute (TFI) in downtown Philadelphia. We lived a couple hours away so a visit was a rare and precious thing. I think I only visited two or three times but it left an indelible impression on me. I remember wandering in amazement through its enormous spaces getting to actually play with amazing and interesting things. I remember sweeping off a table and then filling an overhanging funnel pendulum with sand, setting it going, and watching it create unexpected patterns on the table. I remember running through the gigantic model heart with other kids. I remember the overpowering joy of being in an actual monumental marble temple of curiosity and fascination. So I was filled with anticipation a couple weeks ago when, during a family trip to the East Coast, we managed to squeeze in a visit to TFI with our six-year-old son.
We parked and ran in, paid close to ninety bucks (ouch! but I love you, so take my money), and started off on the top floor with the Wondrous Space exhibit.
And were met with screens.
Design your own rocket! it said (or something like that). No, I thought, this isnâ€™t designing a rocket, this is playing a lame video game on a touchscreen. Yes, there were space-related artifacts around the walls, and a spacesuit in its own large case, but you couldnâ€™t touch any of this stuff, you couldnâ€™t play with it, you could just look at it for a few seconds, read the placard, say â€œhuhâ€, and maybe point out some interesting feature to the kiddo.
But the screens were given pride of place, dead center in the dimly-lit space. And so they beckoned. My wife â€”Â a science writer who used to be the only staff writer covering space for New Scientist and before that, worked at NASA â€” poked at one of these with my son, added too many boosters to their launch vehicle, and were told it failed â€œfor reasonsâ€ in a way she found totally unhelpful and pointless. She led our son gently but firmly away to the glorious four-story Foucault pendulum which hangs in a stairwell.
Here are some images from the website showing patrons interacting with (or running past) screens so you can see what Iâ€™m talking about:



But the screens were all over the place. There were on the main floor, in another section of Wondrous Space, and in the Body Odyssey exhibit. They were all over the SportsZone exhibit on the top floor. Many of them are connected to body motion sensors a la Xbox Kinect so you donâ€™t need to touch them, but theyâ€™re still just video games, where the action-response feedback loop is provided by software, not the universe itself.
And the wonderful hands-on physical stuff that I loved as a kid? Jammed into out-of-the-way spaces in the Sir Isaacâ€™s Loft and Air Show rooms. These rooms are terrific, and I was delighted to see they were absolutely packed with kids playing with stuff. No screens, just objects and forces â€” you donâ€™t even need to read anything to enjoy many of the exhibits, such as the one where you sit in one of two chairs hanging from different configurations of block-and-tackle and haul yourself up â€” and then just let yourself drop, cushioned by a damping piston. Tucked far away in a desolate corner by a hallway we discovered an engaging exhibit where you pluck rods with your finger to generate Lissajous curves from the vibrations. My son was fascinated; he had never seen anything behave like that. And in the Air Show room he liked many of the exhibits, like the one where you (apparently) evacuate a cylinder to see the effect that has on objects moving through it (versus a control). And the â€œshimmer wallâ€, where kids generate sound waves using a variety of devices and can then see the sound waves impacting on a reflective and reactive surface was wonderful and really conveys the mechanical nature of sound.
But these physical exhibits require maintenance, and I was dismayed to see that several are in bad repair; some of them werenâ€™t even working anymore, some seemed worn out, or didnâ€™t seem well-designed to begin with. For instance, they have the classic â€œbicycle wheel and rotating stoolâ€ gyro effect demonstration, but the wheel was too large a diameter for my son to hold, and the stool seemed to have too much friction to work properly for my wife. There was no one trying to use it before or after us; Iâ€™d be curious to see the data on how many visitors attempt it, and how well it usually works for them. And that one should be trivial to design and implement properly: for crying out loud, our local ice cream shop has stools that spin on ball bearings, and I think that would be a big improvement. Every time something didnâ€™t work right I couldnâ€™t help thinking: we paid almost ninety bucks to visit this place. TFI doesnâ€™t seem poor; it seems like its budgetary priorities lie elsewhere.
And where it looks like the budget has been going are the screen rooms. They occupy the huge central spaces on the main floor of the museum, and Iâ€™m sure a lot of time, money, and passion went into these things. But itâ€™s misguided.
I believe museums exist to present the real thing for the visitor to experience with their own senses. Hereâ€™s the sculpture â€” the actual piece of stone, two thousand years old, Greek sculptor unknown â€”Â now go ahead and form your impressions. Come back to it when youâ€™re an old man or woman, it will still be here, and you will see it with different eyes. This is a tiny but, to me, beautiful part of the human condition. And what made the Franklin Institute so amazingly special to me as a kid was that the exhibits sat at the intersection of things that kids want to play with, things that kids are allowed to play with, and things that demonstrate some â€œhey, thatâ€™s cool!â€ scientific phenomenon. And it was all real.
But a lame video game? I can do that on my goddamn phone. TFI â€œwas one of the first museums in the nation to offer a hands-on approach to learning about the physical worldâ€, but â€” and I canâ€™t emphasize this enough, itâ€™s my whole reason for writing this â€” touchscreens are not actually hands-on. Digital representations arenâ€™t tangible, and touchscreen experiences just donâ€™t activate a kidâ€™s brain (including, Iâ€™d say, a sense of delight) the same way a genuinely hands-on experience, like pulling hard on a rope to raise your chair, does.
I donâ€™t know why museums are doing this; my idle speculation is that they see themselves as competing with screens for attention, so in a kind of experiential race to the bottom, they feel compelled to bring screens into their exhibits (see: Amusing Ourselves to Death). But now more than ever in history, kids need a break from the screens that all too many of them are sadly often plugged into by default, and connection to the real world instead. Now is the time for TFI â€” and all museums â€” to take a stand against the tidal wave of digital garbage that is consuming humanity, especially kids, by eliminating all of their touchscreen â€œexhibitsâ€.
To be fair, TFI is still pretty damn great if you just ignore all the screens. The Franklin Memorial rotunda (free to visit!) is gorgeous. The hands-on stuff, tucked away and apparently suffering from neglect though it is, should be replicated in every city in the world. But it would be so much better if they removed all the screens and put that budget and real estate toward the real, tangible, interactive science exhibits that were the reason the museum was created in the first place, and what made me love it as a child. ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ChatGPT Developer Mode: Full MCP client access]]></title>
            <link>https://platform.openai.com/docs/guides/developer-mode</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199713</guid>
        </item>
        <item>
            <title><![CDATA[Launch HN: Recall.ai (YC W20) â€“ API for meeting recordings and transcripts]]></title>
            <link>https://news.ycombinator.com/item?id=45199648</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199648</guid>
            <description><![CDATA[Hey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today weâ€™re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. Itâ€™s our biggest release in quite a while so we thought weâ€™d finally do our Launch HN :)]]></description>
            <content:encoded><![CDATA[Hey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today weâ€™re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. Itâ€™s our biggest release in quite a while so we thought weâ€™d finally do our Launch HN :)Hereâ€™s a demo that shows it producing a transcript from a meeting, followed by examples in code: https://www.youtube.com/watch?v=4croAGGiKTA . API docs are at https://docs.recall.ai/.Back in W20, our first product was an API that lets you send a bot participant into a meeting. This gives developers access to audio/video streams and other data in the meeting. Today, this API powers most of the meeting recording products on the market.Recently, meeting recording through a desktop form factor instead of a bot has become popular. Many products like Notion and ChatGPT have added desktop recording functionality, and LLMs have made it easier to work with unstructured transcripts. But itâ€™s actually hard to reliably record meetings at scale with a desktop app, and most developers who want to add recording functionality donâ€™t want to build all this infrastructure.Doing a basic recording with just the microphone and system audio is fairly straightforward since you can just use the system APIs. But it gets a lot harder when you want to capture speaker names, produce a video recording, get real-time data, or run this in production at large scale:- Capturing speaker names involves using accessibility APIs to screen-scrape the video conference window to monitor who is speaking at what time. When video conferencing platforms change their UI, we must ship a change immediately, so this keeps working.- Producing a video recording that is clean, and doesnâ€™t capture the video conferencing platform UI involves detecting the participant tiles, cropping them out, and compositing them together into a clean video recording.- Because the desktop recording code runs on end-user machines, we need to make it as efficient as possible. This means writing highly platform-optimized code, taking advantage of hardware encoders when available, and spending a lot of time doing profiling and performance testing.Meeting recording has zero margin for failure because if anything breaks, you lose the data forever. Reliability is especially important, which dramatically increases the amount of engineering effort required.Our Desktop Recording SDK takes care of all this and lets developers build meeting recording features into their desktop apps, so they can record both video conferences and in-person meetings without a bot.We built Recall.ai because we experienced this problem ourselves. At our first startup, we built a tool for product managers that included a meeting recording feature. 70% of our engineering time was taken up by just this feature! We ended up starting Recall.ai to solve this instead. Since then, over 2000 companies use us to power their recording features, e.g. Hubspot for sales call recording, Clickup for their AI note taker. Our users are engineering teams building commercial products for financial services, telehealth, incident management, sales, interviewing, and more. We also power internal tooling for large enterprises.Running this sort of infrastructure has led to unexpected technical challenges! For example, we had to debug a 1 in 36 million segfault in our audio encoder (https://www.recall.ai/blog/debugging-a-1-in-36-000-000-segfa...), we encountered a Postgres lock-up that only occurs when you have tens of thousands of concurrent writers (https://news.ycombinator.com/item?id=44490510), and we saved over $1M a year on AWS by optimizing the way we shuffle data around between our processes (https://news.ycombinator.com/item?id=42067275).You can try it here: https://www.recall.ai. It's self-serve with $5 of free credits. Pricing starts at $0.70 for every hour of recording, prorated to the second. We offer volume discounts with scale.All data recorded through Recall.ai is the property of our customers, we support 0-day retention, and we donâ€™t train models on customer data.We would love your feedback!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The origin story of merge queues]]></title>
            <link>https://mergify.com/blog/the-origin-story-of-merge-queues</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199378</guid>
            <description><![CDATA[From Bors and Homu to Bulldozer, Kodiak, Mergify, and now GitHub and GitLab, merge queues have shaped how we keep main branches green. This article traces their history, why they emerged, and how they became a standard in modern software development.]]></description>
            <content:encoded><![CDATA[From Bors and Homu to Bulldozer, Kodiak, Mergify, and now GitHub and GitLab, merge queues have shaped how we keep main branches green. This article traces their history, why they emerged, and how they became a standard in modern software development.If you use GitHub or GitLab today, merge queues feel like a built-in feature of modern development. But their story goes back over a decade, long before "merge queue" was a product term.It started with a simple problem: How do you keep your main branch green when dozens of developers are merging code simultaneously? Continuous integration clarified that "just merge and hope" wasn't good enough. The solution wasn't a new testing framework but a new workflow.From early scripts in the Rust project (Bors, Homu) to Shopify's Shipit, to modern SaaS offerings like Mergify and built-in queues from GitHub and GitLab, merge queues evolved out of necessity. What began as side-project bots has become a standard practice for engineering teams at scale.This post walks through that history â€” the motivations, the people behind it, and how these tools shaped the way we merge code today.The "Not Rocket Science" Rule and Early ExperimentsThe idea of a merge queue â€“ automatically ensuring that a main branch is never broken by merged changes â€“ can be traced back over two decades. In the early 2000s, developer Ben Elliston devised a system of cron jobs, multiple repositories, and a database to "automatically maintain a repository of code that always passes all the tests". This approach, later dubbed the "Not Rocket Science Rule of Software Engineering," kept a known-good code branch for developers and customers, preventing the headaches of broken main builds.Fast forward to 2013: Graydon Hoare (creator of the Rust language) faced a similar challenge as Rust's contributor base grew. Remembering Elliston's rule, Hoare implemented a small bot named Bors to enforce it. Bors integrated with Rust's build farm and GitHub: it would monitor pull requests, wait for a reviewer's "approve" command, merge the PR into a temporary branch, and run the full test suite. If tests passed, Bors would fast-forward the main branch to that tested merge commit; if not, it would report the failure and leave the main branch untouched. This ensured that Rust's master branch was always green (always passing tests). The motivation was to avoid "merge skew," where changes appear compatible when reviewed in isolation but break once merged into an updated main. (A classic example of merge skew is two PRs that individually pass tests â€“ one renames a function, and another adds a call to the old name, resulting in a broken main after sequential merges). By "testing it first, then promoting it" to main, Bors kept Rust's primary branch stable without human intervention to update or revert commits.Rust's experience proved the concept's value. As Hoare noted, the approach "is not rocket science" â€“ it's just tedious to do manually, hence ripe for automation. Bors' success meant that by 2014 the Rust and Mozilla Servo projects were using such bots to gate all merges on tests. However, Bors itself was a quick script, and the need for a more extensible solution soon became apparent.Rust's Homu and the Rise of Merge BotsTo build a more general tool, Rust contributor Barosl Lee created Homu as a reimplementation and extension of the original Bors bot. Homu was designed to be generic (not Rust-specific) and easier for others to adopt. It implemented the same core idea: maintain a tested integration branch (often called "auto") that includes pending PRs, and only fast-forward main to auto when tests on auto pass. In practice, Homu reversed the usual merge process: instead of merging a PR then testing, it tested the PR before it landed on main by temporarily combining it with the up-to-date main branch. This ensured that "the main branch is always a copy of auto that passed all tests, processing approved PRs one at a time in order.Homu was open source and quickly became integral to Rust's workflow (Rust's own Homu instance was named "bors"). In 2015, Barosl even launched Homu as a service (homu.io), making it easy for other open-source projects to use a hosted merge queue bot. This service gained users in various communities, demonstrating a broader demand for maintaining green main branches. Homu's design was language-agnostic, so projects beyond Rust/Servo could adopt it with their CI systems.However, by around 2018, the original Homu service began to stagnate. The maintainer stopped updating it, the web frontend's source was lost, and eventually the homu.io domain expired. Some projects (like Rust itself) forked and maintained their own Homu instances, but there was clearly room for a modern replacement. This set the stage for the next evolution of merge queue tools.Bors-NG: Modern Successor to HomuEnter Bors-NG (Bors "Next Generation"), created by Michael Howell. Bors-NG was a complete open-source replacement for Homu, first released around 2017. It was built to be faster, more user-friendly, and easier to host, while preserving the same core idea of tested-then-merged pull requests. Unlike homu.io, Bors-NG had no closed-source components â€“ the public instance at app.bors.tech ran the same code that anyone could self-host.Bors-NG quickly became popular, especially for teams on GitHub that needed a merge queue before GitHub offered any native solution. Many saw it as the "spiritual successor to Homu, the original Rust merge bot". It integrated with GitHub pull requests and CI services, allowing maintainers to use the familiar bors r+ command to enqueue PRs for merging once tests pass. For years, this combo of Bors-NG + CI filled a critical gap, giving projects large and small a way to avoid merge skew and keep their main branch healthy.Notably, the Kubernetes project developed a similar concept in parallel â€“ their Prow/Tide system â€“ and other communities like OpenStack had long used a gating bot (Zuul) for Gerrit. These all share the same philosophy pioneered by Bors. Merge queues became recognized as best practice for high-velocity projects where broken merges are unacceptable.By 2023, however, the landscape changed: GitHub announced its own merge queue feature (more on that later). The author of Bors-NG announced the public Bors-NG service would be deprecated in favor of GitHubâ€™s native Merge Queue. While Bors-NG remains open source for self-hosters, the availability of an official tool signaled a shift. It was a full-circle moment â€“ a concept that started as a community hack had become mainstream enough for GitHub itself to support it out-of-the-box.Industry Solutions: Bulldozer, Mergify, and KodiakIn parallel to the Rust community's bots, other engineers and companies were solving the same problem, often "scratching their own itch." This gave rise to several notable merge automation tools in the late 2010s:Bulldozer (2017): Developers at Palantir created Bulldozer as a GitHub App to automate merging and updating PRs. Palantir open-sourced the tool, which could be self-hosted or installed on repos. Bulldozer automatically merges pull requests when all required checks and reviews are satisfied, and it can also auto-update PR branches to keep them in sync with the base branch. Essentially, it removes the manual "update and merge" drudgery in a fast-moving trunk-based development workflow. An example use case is at ACV Auctions, where engineers adopted Palantir's Bulldozer to ensure everyone's feature branch stays up-to-date with main and merges immediately once tests pass (They noted that for public GitHub usage, Bulldozer could be run as a custom instance, as it wasn't officially listed on the marketplace at the time)Mergify (2018): That's us! Frustration with manual PR management led Julien Danjou and Mehdi Abaakouk to build a small merge automation tool for their team in 2018. That side project struck a chord and soon evolved into Mergify, a full-fledged SaaS product and company. Mergify introduced features to queue, update, and merge PRs with flexible rules, effectively bringing merge queues to any GitHub repository via a cloud service. The founders (long-time open source contributors) initially open-sourced Mergify's code and offered it free for OSS projects. Over the years, Mergify became a popular "CI companion" for teams worldwide, offering advanced queue configurations, priority rules, batch merging, and more on top of GitHub's API. It was essentially "merge queue as a service," well before GitHub's native feature. (As an aside, Mergify's engine was eventually made closed-source in 2022 as the business matured, but it continues to serve thousands of developers.)Kodiak (2019): In early 2019, developer Christopher Blump faced constant delays from having to rebase and update PRs in a busy repo with an always-green main policy. He evaluated existing tools like Bulldozer and others, even contributing a patch, but found "none of the existing projectsâ€¦ solved the problem of efficiently updating and merging pull requests." So during his college finals in May 2019, he built the first version of Kodiak. Kodiak is an open-source GitHub App that automates the â€œupdate with latest main and merge if tests pass" dance. It introduced a proper queue to merge PRs in order, eliminating the race condition when multiple developers try to merge at once. The initial version was basic (queue in memory, no persistent state) but solved the pain point. By that summer, Kodiakâ€™s usefulness caught on â€“ it got a considerable boost when Vercelâ€™s CEO tweeted about using it to auto-merge and deploy changes. This endorsement in July 2019 brought a wave of adopters, and Kodiak rapidly grew via word-of-mouth in the open-source community. Kodiak added features like configuration files, persistent queues, and GitHub checks for transparency. It became another popular option for teams wanting a hosted merge bot, though it is not actively maintained anymore.These tools all shared a common purpose: to safely remove human bottlenecks in merging PRs. They watched for PRs meeting defined conditions (CI passes, approvals present, specific labels, etc.) and then automatically merged them in a controlled way. Many could also keep branches rebased or updated to prevent stale merges. In effect, they implemented merge queues or merge trains outside of the platform â€“ a testament to how universal the need had become by 2018â€“2019.It's worth noting that GitLab users were also early to this idea: GitLab introduced an official Merge Trains feature in mid-2019 (GitLab Premium 12.0) to queue merge requests and run "pipelines for merge results" on each in sequence. This was GitLab's integrated solution to guarantee that each MR is tested with all prior changes before landing, much like Bors/Homu's approach. Merge Trains made it easy to auto-merge a series of MRs without breaking the target branch, and even supported batching multiple MR commits into one pipeline run for efficiency. In other words, by 2019 the concept of merge queues had leapt from niche bots to built-in VCS platform features â€“ at least on GitLab.Internal Merge Queues in Tech GiantsLarge-scale organizations soon recognized that hand-rolling merge queues was often the only way to keep their main branches stable amid heavy developer activity. Uber, for instance, built a system called SubmitQueue to verify and land changes in their monorepo, reducing CI wait times by 74% and dramatically improving merge throughput while keeping the mainline green. Shopify, working on a massive Rails-based monolith, added a merge queue into their deployment tool, Shipit, to prevent accidental merges during backlog surges and maintain pipeline reliability. Similarly, Strava created an internal tool dubbed Butler, a CI-integrated merge queue that enforces orderly merging for their fast-moving engineering teams.At Shopify, developers gave positive feedback early on: â€œBy getting automation involved earlier in the pipeline, we were able to take some of the load off our developers, make them happier, and more productive.â€ In fact, over 90% of pull requests to Shopifyâ€™s core application now use the Shipit Merge Queue. This demonstrates how merge queues arenâ€™t just technical enablersâ€”they improve engineering experience at scale.Mainstream Adoption: GitHub's Merge QueueGiven the success of these systems, it was perhaps only a matter of time before GitHub provided native support. Historically, GitHub's stance was more manual: they added a basic "Auto-merge" option in late 2020 that lets a PR merge after checks pass. However, that still didn't handle multiple PRs interacting or ensure rebasing. The true paradigm shift came in 2022â€“2023.GitHubâ€™s own Merge Queue wasnâ€™t born from product planningâ€”it was born from internal necessity. By 2016, GitHub engineers were merging nearly 1,000 pull requests per month into their expansive monorepo. The resulting chaos mandated a smarter system. What emerged was the concept of a "train" â€” a bundle of PRs tested, deployed, and merged together under human orchestration. This was a precursor to today's fully automated queues. An internal shift began in 2020, when multiple teams pooled efforts to streamline PR merging across internal projects. By mid-2021, GitHub piloted a merge queue in smaller repos and, by 2023, had rolled out an internal system powering thousands of merges monthlyâ€”cutting average wait times by 33% and calling the Merge Queue "one of the best quality-of-life improvements" they'd seenThen, GitHub decided to release its internal merge queue as part of its product. They began experimenting with a first-partyÂ Pull Request Merge Queue (PRMQ). After a closed beta,Â in February 2023, GitHub released its merge queue feature in public beta, and by July 2023, it was generally available.GitHub's Merge Queue closely mirrors the principles established by Bors and others: it maintains a queue of PRs waiting to merge. It ensures each PR is tested in a merged state (often by creating a temporary merge branch for the PR and running CI) before it lands on the base branch. In effect, it automates the formerly tedious process of constantly rebasing or updating PRs and serializing their merges. As GitHub's announcement put it, developers used to have to update their feature branches one-by-one and re-run CI to avoid breaking main; "Merge Queue automates this process" by queuing PRs and testing them with any earlier queued changes. The result is higher velocity and confidence that incompatible changes never break the default branch.The introduction of GitHub's own merge queue was a watershed moment. It validated the approach pioneered by community tools and brought it to a much broader audience. Organizations on GitHub Enterprise Cloud or public open-source projects can now simply toggle on a merge queue in settings, without needing an external bot. The impact was immediately felt: maintainers of Bors-NG announced the phase-out of their hosted service in favor of GitHub's queue, and guidelines for migrating from Bors to GH Merge Queue emerged. Essentially, the ecosystem came full circle â€“ what started as a custom script to enforce an "always-green" rule evolved into a standard platform feature.It's important to note that GitHub's initial implementation has some differences in workflow. For example, GH Merge Queue uses a two-phase testing approach (one run on the PR itself, and a second run after queueing when merged into a temporary branch) whereas tools like Bors performed a single integrated test cycle. There are also limitations: among others, GitHub's queue is FIFO only (no priority reordering) and lacks batch merging capabilities. Third-party services like Mergify have pointed out these gaps quickly, since their products offer more advanced queue configurations (multiple queues, priority rules, batching to merge several PRs at once, etc.). In fact, even after GitHub's native queue launch, some teams continued to use or switch to tools like Mergify or others for more flexibility. Nonetheless, the core need is now officially recognized and supported by GitHub, a significant milestone in the history of merge queues.Conclusion: From Niche Scripts to Essential WorkflowIn a little over a decade, merge queue systems have gone from an obscure hack to an essential part of modern software delivery. The progression tells a story of increasing scale and quality demands:Early 2010s: Only very large or risk-averse projects (like Rust, Servo, or OpenStack) felt the pain strongly enough to build bespoke solutions (Bors, Homu, Zuul) to guarantee unbreakable main branches. These were novel, community-driven efforts born out of necessity â€“ enforcing the "always passes all tests" rule that was known but rarely automated.Late 2010s: As continuous integration became ubiquitous and teams adopted trunk-based development, the merge skew problem became more common across the industry. This spurred a wave of tools â€“ some open-source, some commercial â€“ to automate PR merging (Bulldozer, Mergify, Kodiak, and others). They enabled even smaller teams to achieve what only giants could before: continuously integrating code without constantly babysitting CI or worrying about conflicting changes. The fact that a lone developer could write Kodiak in a weekend to solve his team's annoyance, and that it immediately found a user base, speaks to how widespread the need had become.2020s: Merge queues became mainstream best practice. Platform support (GitLab's Merge Trains, GitHub's Merge Queue) lowered the barrier to adoption. Today, even teams that never heard of "bors" or "homu" are benefiting from the lessons those tools taught. On GitHub, you can simply enable a branch protection that uses a merge queue, and achieve the same guarantee that â€œthe main branch is never broken by incompatible changes.â€ The ecosystem around merge automation is still evolving â€“ with third-party services pushing the envelope on features â€“ but the fundamental approach is here to stay.In an academic sense, it's fascinating how a principle from the early 2000s configuration management became a pillar of modern DevOps. The story of merge queue systems is one of increasing automation to support software quality at scale. By eliminating the integration risk of each incremental change, developers can move faster without fear.What began as Graydon Hoare's small Rust bot named after a knight (Bors) has grown into a standard tool in software teams' arsenal, ensuring that code integration isÂ "not rocket science"Â but a well-engineered process. Merge queues have evolved from niche hacks into an industry standardâ€”because as PR velocity increased, they weren't a luxury, they became an operational necessity.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zoox robotaxi launches in Las Vegas]]></title>
            <link>https://zoox.com/journal/las-vegas</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199031</guid>
            <description><![CDATA[Ride in Zooxâ€™s purpose-built autonomous robotaxi. Now serving Las Vegas. Download the app and ride in a Zoox.]]></description>
            <content:encoded><![CDATA[Copyright Zoox, Inc. 2025All Rights Reserved.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Jiratui â€“ A Textual UI for interacting with Atlassian Jira from your shell]]></title>
            <link>https://jiratui.sh/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45198481</guid>
            <description><![CDATA[JiraTUI revolutionizes task management for developers by enabling seamless interaction with Jira from the terminal. Create, update, and track tasks efficiently, all while maintaining focus on your code and workflow.]]></description>
            <content:encoded><![CDATA[
      
        
      

      
            
            JiraTUI revolutionizes task management for developers by enabling seamless interaction with Jira from the terminal. Create, update, and track tasks efficiently, all while maintaining focus on your code and workflow.
          


      
        
        
      

      
          
            Features
          
          
            
                  
                  
                  
                  
                
            
              Search Tasks
              Quickly locate your Jira tasks using the powerful search functionality in JiraTUI. With just a few commands, you can filter tasks by status, assignee, or priority. This feature saves time and enhances productivity, allowing you to focus on what matters most in your projects.
            
          
          
            
            
            
              Create Tasks
               Easily create new Jira tasks directly from the terminal with JiraTUI. This feature simplifies the task creation process, enabling you to specify details like title, description, and priority in a streamlined manner. Spend less time navigating interfaces and more time getting things done.
            
          
          
            
                  
                  
                  
                  
                
            
              Update Tasks
              Keep your tasks up to date effortlessly with JiraTUI's update feature. Modify task details such as status, assignee, summary, labels and due dates directly from the command line. This functionality ensures that your project remains organized and current, enhancing collaboration and workflow efficiency.
            
          
          
            
            
            
              Comments
              Engage with your team by managing comments on tasks through JiraTUI. Add or delete comments directly from the terminal, fostering clear communication and collaboration. This feature helps keep discussions organized and accessible, ensuring everyone stays informed about task progress.
            
          
          
              Manage Related Tasks
              Easily manage related tasks with JiraTUI, allowing you to link and unlink tasks directly from the terminal. This feature helps you visualize dependencies and relationships between tasks, ensuring a more cohesive project management experience and improving overall workflow.
            
          
            
                  
                  
                  
                  
                
            
              JQL Search
              Leverage the power of Jira Query Language (JQL) with JiraTUI to perform advanced searches. This feature allows you to create complex queries to filter tasks based on specific criteria, providing greater flexibility and precision in managing your projects and enhancing your productivity. Expressions can be saved to use at any time.
            
          
        

      
          Advantages
          Discover the key advantages of JiraTUI that make it an essential tool for developers seeking efficient and effective task management.
          
            
                  
                    
                  
                  Configurable
                  JiraTUI is highly configurable, allowing users to tailor the tool to their specific needs. Customize command shortcuts, settings, and preferences to create a personalized experience that enhances productivity. This flexibility ensures that JiraTUI adapts to your workflow, making it a perfect fit for any development environment.
                
            
                  
  

                  Simplicity
                  JiraTUI offers a straightforward command-line interface that simplifies task management. By eliminating unnecessary clicks and navigation, it allows developers to focus on their work. The intuitive commands make it easy to perform actions quickly, ensuring that managing Jira tasks is a seamless part of your development workflow.
                
          
          
            
                  
                    
                    
                  
                  Speed
                  Experience unparalleled speed with JiraTUI, designed for efficiency in task management. The command-line interface allows for rapid execution of commands, enabling developers to create, update, and search tasks in seconds. This speed not only saves time but also enhances overall productivity, allowing you to focus on delivering quality work.
                
            
                  
  

                  Easy to Use
                  JiraTUI is designed with user-friendliness in mind, making it accessible for developers of all skill levels. The clear command structure and helpful prompts guide users through task management effortlessly. With minimal learning curve, you can quickly harness the power of JiraTUI and integrate it into your daily workflow.
                
          
        

      
            
            Â© 2025 Gaston Tagni.
            
            
          

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kerberoasting]]></title>
            <link>https://blog.cryptographyengineering.com/2025/09/10/kerberoasting/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45196437</guid>
            <description><![CDATA[I learn about cryptographic vulnerabilities all the time, and they generally fill me with some combination of jealousy (â€œoh, why didnâ€™t I think of thatâ€) or else they impress me wâ€¦]]></description>
            <content:encoded><![CDATA[
	
	
		
I learn about cryptographic vulnerabilities all the time, and they generally fill me with some combination of jealousy (â€œoh, why didnâ€™t I think of thatâ€) or else they impress me with the brilliance of their inventors. But thereâ€™s also another class of vulnerabilities: these are the ones that canâ€™t possibly exist in important production software, because thereâ€™s no way anyone could still do that in 2025.



Today I want to talk about one of those ridiculous ones, something Microsoft calls â€œlow tech, high-impactâ€. This vulnerability isnâ€™t particularly new; in fact the worst part about it is that itâ€™s had a name for over a decade, and itâ€™s existed for longer than that. Iâ€™ll bet most Windows people already know this stuff, but I only happened to learn about it today, after seeing a letter from Senator Wyden to Microsoft, describing how this vulnerability was used in the May 2024 ransomware attack on the Ascension Health hospital system.



The vulnerability is called Kerberoasting, and TL;DR it relies on the fact that Microsoftâ€™s Active Directory is very, very old. And also: RC4. If you donâ€™t already know where Iâ€™m going with this, please read on.



A couple of updates: The folks on HN pointed out that I was using some incorrect terms in here (sorry!) and added some good notes, so Iâ€™m updating below.



Whatâ€™s Kerberos, and whatâ€™s Active Directory?



Microsoftâ€™s Active Directory (AD) is a many-tentacled octopus that controls access to almost every network that runs Windows machines. The system uses centralized authentication servers to determine who gets access to which network resources. If an employeeâ€™s computer needs to access some network Service (a file server, say), an Active Directory server authenticates the user and helps them get securely connected to the Service.



This means that AD is also the main barrier ensuring that attackers canâ€™t extend their reach deeper into a corporate network. If an attacker somehow gets a toehold inside an enterprise (for example, because an employee clicks on a malicious Bing link), they should absolutely not be able to move laterally and take over critical network services. Thatâ€™s because any such access would require the employeeâ€™s machine to have access to specialized accounts (called â€œService accountsâ€) with privileges to fully control those machines. A well-managed network obviously wonâ€™t allow this. This means that AD is the â€œguardianâ€ that stands between most companies and total disaster.



Unfortunately, Active Directory is a monster dragged from the depths of time. It uses the Kerberos protocol, which was first introduced in early 1989. A lot of things have happened since 1989! In fairness to Microsoft, Active Directory itself didnâ€™t actually debut until about 1999; but (in less fairness), large portions of its legacy cryptography from that time period appear to still be supported in AD. This is very bad, because the cryptography is exceptionally terrible. 



Let me get specific.



When you want to obtain access to some network resource (a â€œServiceâ€ in AD parlance), you first contact an AD server (called a KDC) to obtain a â€œticketâ€ that you can send to the Service to authenticate. This ticket is encrypted using a long-term Service â€œpasswordâ€ established at the KDC and the Service itself, and itâ€™s handed to the user making the call.



Now, ideally, this Service password is not really a password at all: itâ€™s actually a randomly-generated cryptographic key. Microsoft even has systems in place to generate and rotate these keys regularly. This means the encrypted ticket will be completely inscrutable to the user who receives it, even if theyâ€™re malicious. But occasionally network administrators will make mistakes, and one (apparently) somewhat common mistake is to set up a Service thatâ€™s connected to an ordinary user account, complete with a human-generated password.



Since human passwords probably are not cryptographically strong, the tickets encrypted using them are extremely vulnerable to cracking. This is very bad, since any random user â€” including our hypothetical laptop malware hacker â€”Â can now obtain a copy of such a ticket, and attempt to crack the Serviceâ€™s password offline by trying many candidate passwords using a dictionary attack. The result of this is that the user learns an account password that lets them completely control that essential Service. And the result of that (with a few extra steps) is often ransomware.



Isnâ€™t that cute?



That doesnâ€™t actually seem very cute?



Of course, itâ€™s not. Itâ€™s actually a terrible design that should have been done away with decades ago. We should not build systems where any random attacker who compromises a single employee laptop can ask for a message encrypted under a critical password! This basically invites offline cracking attacks, which do not need even to be executed on the compromised laptop â€” they can be exported out of the network to another location and performed using GPUs and other hardware.



There are a few things that can stop this attack in practice. As we noted above, if the account has a long enough (random!) password, then cracking it should be virtually impossible. Microsoft could prevent users from configuring services with weak human-generated passwords, but apparently they donâ€™t â€” at least because this is something thatâ€™s happened many times (including at Ascension Health.) 



So letâ€™s say you did not use a strong cryptographic key as your Serviceâ€™s password. Where are you?



Your best hope in this case is that the encrypted tickets are extremely challenging for an attacker to crack. Thatâ€™s because at this point, the only thing preventing the attacker from accessing your Service is computing power. But â€” and this is a very weak â€œbutâ€ â€” computing power can still be a deterrent! In the â€œstandardâ€ authentication mode, tickets are encrypted with AES, using a key derived using 4,096 iterations of PBKDF2 hashing, based on the Service password and a per-account salt (Update: which is not truly random salt, itâ€™s a combination of domain and principal name.) The salt means an attacker cannot easily pre-compute a dictionary of hashed passwords, and while the PBKDF2 (plus AES) isnâ€™t an amazing defense, it puts some limits on the number of password guesses that can be attempted in a given unit of time.



This page by Chick3nman gives some excellent password cracking statistics computed using an RTX 5090. It implies that a hacker can try 6.8 million candidate passwords every second, using AES-128 and PBKDF2.



So thatâ€™s not great. But also not terrible, right?



This isnâ€™t the end of the story. In fact itâ€™s self-evident that this is not the end of the story, because Active Directory was invented in 1999, which means at some point weâ€™ll have to deal with RC4.



Hereâ€™s the thing. Anytime you see cryptography born in the 1990s and yet using AES, you cannot be dealing with the original. What youâ€™re looking at is the modernized, â€œupgradedâ€ version of the original. The original probably used an abacus and witchcraft, or (failing that) at least some combination of unsalted hash functions and RC4. And hereâ€™s the worst part: it turns out that in Active Directory, when a user does not configure a Service account to use a more recent mode, then Kerberos will indeed fall back to RC4, combined with unsalted NT hashes (basically, one iteration of MD4.)



The main implication of using RC4 (and NT hashing) is that tickets encrypted this way become hilariously, absurdly fast to crack. According to our friend Chick3nman, the same RTX 5090 can attempt 4.18 billion (with a â€œbâ€) password guesses every second. Thatâ€™s roughly 1000x faster than the AES variant.



As an aside, the NT hashes are not salted, which means theyâ€™re vulnerable to pre-computation attacks that involve rainbow tables. I had been meaning to write about rainbow tables recently on this blog, but had convinced myself that they mostly donâ€™t matter, given that these ancient unsalted hash functions are going away. I guess maybe I spoke too soon? Update: see Tom Tervoortâ€™s excellent comment below, which mentions that there is a random 8-byte â€œconfounderâ€ acting as a salt during key derivation.



So what is Microsoft doing about this?



Clearly not enough. These â€œKerberoastingâ€ attacks have been around for ages: the technique and name is credited to Tim Medin who presented it in 2014 (and many popular blogs followed up on it) but the vulnerabilities themselves are much older. The fact that there are practical ransomware attacks using these ideas in 2024 indicates that (1) system administrators arenâ€™t hardening things enough, but more importantly, (2) Microsoft is still not turning off the unsafe options that make these attacks possible.







To give some sense of where we are, in October 2024, Microsoft published a blog post on how to avoid Kerberos-based attacks (NB: I cannot say Kerberoasting again and take myself seriously). 



The recommendations are all kind of dismal. They recommend that administrators should use proper automated key assignment, and if they canâ€™t do that, then to try to pick â€œreally good long passwordsâ€, and if they canâ€™t do that, to pretty please shut off RC4. But Microsoft doesnâ€™t seem to do anything proactive, like absolutely banning obsolete legacy stuff, or being completely obnoxious and forcing admins to upgrade their weird and bad legacy configurations. Instead this all seems much more like a reluctant and half-baked bit of vulnerability management.



Iâ€™m sure there are some reasons why this is, but I refuse to believe theyâ€™re good reasons, and Microsoft should probably try a lot harder to make sure these obsolete services go away. It isnâ€™t 1999 anymore, and it isnâ€™t even 2014.



If you donâ€™t believe me on these points, go ask Ascension Health.
	

			
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OrioleDB Patent: now freely available to the Postgres community]]></title>
            <link>https://supabase.com/blog/orioledb-patent-free</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45196173</guid>
        </item>
    </channel>
</rss>