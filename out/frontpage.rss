<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 08 Sep 2025 15:04:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Experimenting with Local LLMs on macOS]]></title>
            <link>https://blog.6nok.org/experimenting-with-local-llms-on-macos/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45168953</guid>
            <description><![CDATA[A developer's guide to downloading and running LLMs on macOS, for experimentation and privacy.]]></description>
            <content:encoded><![CDATA[September 08, 2025Â Â·Â 9 minutes to read
So, this blog post will be about LLMs, and everyone has opinions about that. To be upfront about it, Iâ€™m a skeptic (bordering on hater), yet I like experimenting with stuff so I download and run them locally on my Mac. And Iâ€™ll teach you how to do it too, if youâ€™d like!

      
    
  
  
    
Some call them fancy autocomplete, some argue that they are sentient and should have rights. The truth is somewhere in between. Yes, they perform next word prediction, but itâ€™s so complex that thereâ€™s nontrivial emergent behavior. No, they donâ€™t have creativity or a mind. I believe one day we can create sentient machines, but not in this current iteration, maybe not before we go extinct.
Now that weâ€™re out of the science fiction territory, letâ€™s talk about their strengths. Laurie has a great post about it, which I highly recommend, but in summary they are generally good at summarizing text, regurgitating home maintenance advice from reddit, or telling you that you have cancer.
I also use them for brain-dumping. I find it hard to keep a journal, because I find it boring, but when youâ€™re pretending to be writing to someone, itâ€™s easier. If you have friends, thatâ€™s much better, but some topics are too personal and a friend may not be available at 4 AM.
I mostly ignore its responses, because itâ€™s for me to unload, not to listen to a machine spew slop. I suggest you do the same, because weâ€™re anthropomorphization machines and Iâ€™d rather not experience AI psychosis. Itâ€™s better if you donâ€™t give it a chance to convince you itâ€™s real. I could use a system prompt so it doesnâ€™t follow up with dumb questions (or â€œYoUâ€™Re AbSoLuTeLy CoRrEcTâ€s), but I never bothered as I already donâ€™t read it.
Lastly, Iâ€™m interested in them because itâ€™s tech and I like tech. I donâ€™t believe they make you that much productive, and I never let them write for me. When I ask it something, I always fact-check, they are known to hallucinate (or bullshit, if you prefer) and Iâ€™ve experienced this too many times to trust it. Most importantly, just avoid asking questions that canâ€™t be easily verified, thereâ€™s enough fake news around anyway.
You may be asking, â€œWell I can do everything you said with ChatGPT free tier, why bother running them locally?â€ There are a few reasons, and Iâ€™ve already mentioned them briefly:


I like experimenting with things. Itâ€™s marvelous that you can download this 12 GB file and your computer talks to you marginally better than ELIZA! Joking aside, we accepted the concept of LLMs too quickly, when the truth is that we never expected computers to figure out human speech before robots were walking among us. So it feels a bit magical when my computer has better grammar than I do, and I can summon one whenever I want without a supercomputer.


People have secrets and some secrets shouldnâ€™t leave your computer. Companies are not well equipped to keep your sensitive data, and current trends show that they can retain your data or use it for training. I feel like a local LLM is a better choice for these use cases.


I donâ€™t feel comfortable giving money to the AI companies. Every single one of them has done some kind of unethical thing, and the way the AI race is going, a hypothetical ethical one will be left behind. They intentionally hype AI, destroy the environment, and plagiarize peopleâ€™s hard work. Iâ€™m thankful for the open-weight models they provide and will keep using them, luckily they canâ€™t take that away from me.


If you still want to run an LLM on your macOS1, letâ€™s get started. There are two options that I recommend, one is open-source and the other is easier to use, as all things are. I only cover macOS because thatâ€™s what I have; if thatâ€™s not your platform, you can still follow this guide with platform-specific tweaks, or find another guide.
Llama.cpp
Itâ€™s created by Georgi Gerganov and works really well with a ton of configuration options. It supports a lot of platforms, can download models, and has a basic web UI for convenience. You can install it with Nix using the following command: nix profile install nixpkgs#llama-cpp. For other installation methods, check their instructions.
To download and run a small and good enough model for experimentation, I recommend Gemma 3 4B QAT, which you can do with the following command:
$ llama-server -hf ggml-org/gemma-3-4b-it-qat-GGUF
If you open your web browser and navigate to http://127.0.0.1:8080, youâ€™ll see a very bare bones web UI thatâ€™s eerily similar to ChatGPT, but that should be enough. You can experiment with it and exit the server once youâ€™re done.
LM Studio
This is the closed-source but easier to use one. It has a superb UI that lets you browse models, manage downloads, organize chats, and even tells you if a model can run on your machine. It has guardrails so you donâ€™t crash your system by loading a model thatâ€™s too large. I like using it a lot, but enshittification is always a risk with closed-source for-profit software.
You can download a DMG from its website and install it as usual. LM Studio has two runtimes on macOS, llama.cpp which we covered earlier, and MLX, which is an ML engine developed by Apple and runs a bit faster, but offers less configuration in the UI. (I didnâ€™t try running MLX directly, maybe itâ€™s configurable but not exposed.)
I will not provide a walkthrough because this is not a product tour and the UI is pretty user-friendly anyway. I can give you some tips and tricks though.

You can switch the model mid-conversation, it wonâ€™t have any problem
You can branch off the current conversation, which is good for experimentation
You can regenerate the assistant message, like ChatGPT
You can edit your own messages, also like ChatGPT, but you can also edit assistant messages, which allows you to put words into its mouth
You can create presets for system prompts and reuse them for different personas
There are a lot of model settings you can configure; for example, you can customize the context overflow behavior when the context grows larger than the context window (the default is to truncate the middle, so first and last messages are kept, which is a good default)

How to choose a good LLM for your use case
Right now there are a lot of open-weight models around, almost all AI companies have released one. There are a few things you need to pay attention to when choosing a model. These things take a lot of disk space so be mindful of filling your disk!
Model size
You may have plenty of free space on your drive, but for the LLM to work you need to load it into memory. So your RAM is the bottleneck. Since the operating system also needs memory to work, if you have 16 GB RAM, like me, then models should be less than 12 GB. Loading larger models may cause you to run out of memory, your system will be unresponsive, and youâ€™ll have to perform a hard reboot. Larger models will also run slower.2
Runtime
If youâ€™re using llama.cpp directly, or as the runtime in LM Studio, you need GGUF models. If youâ€™re using the MLX runtime in LM Studio, you need MLX models. As mentioned before, MLX models run slightly faster, but GGUF models provide more configuration, and since they can run on many platforms they are ubiquitous and better tested.
Quantization
Most LLMs are trained at 16-bit precision, but you can downcast (or quantize) the weights to lower precision and they still perform well up to some point. The sweet point is around 4 bits, which is noted as Q4. LM Studio defaults to this.
Quantization is a rabbit hole; different kernels are used for quantization, which is included in the notation, such as Q4_K_M, which is a bit too much for me and I decided against learning it. Just download the default and be done with it.
Vision models
Some models can tokenize image inputs and can â€œseeâ€ whatâ€™s inside them. This feels pretty magical; they can read text, recognize objects, and determine the mood or art style. You can use them as basic OCRs but in my experience dedicated OCRs perform better at serious tasks, LLMs tend to make shit up when they canâ€™t read.
Reasoning
Some models can â€œthinkâ€ before generating an answer, which is sometimes called â€œinference time scalingâ€3. The general wisdom is that smaller reasoning models can compete with larger non-reasoning models, which is reflected in benchmarks. The downside is that it takes longer to get an answer. So you need to decide if you want a larger but smarter model, or a smaller one that reasons. Keep in mind that reasoning sometimes takes minutes, and fills up the context pretty quickly.
Tool use
Some models are taught to emit special tokens that can call tools specified in the system prompt with the correct arguments. LM Studio has a UI for adding MCP servers and managing the capabilities provided by them.
By default, LM Studio asks you to confirm each tool call request, which is great for security. Tool calls are commonly used for data exfiltration attacks, which is as bad as it sounds.
LM Studio by default comes with a JavaScript MCP, powered by Deno, which provides the LLM the ability to execute code in a sandbox. This is really powerful, you can make it perform hard calculations, analyze data, even generate random numbers. I created a number guessing game, which works better than expected.
You can also plug in a web search MCP to give it up-to-date knowledge retrieval capabilities. Since small models have limited world knowledge, this makes them work remarkably well for applicable use cases. I have used Exaâ€™s free tier for this, which worked well.
Lastly, if you want a model to have long-term memory, there are a ton of MCPs that can do that. But keeping with the theme of keeping things local, I found MCP server for Obsidian to be a good candidate.
One thing to keep in mind regarding MCPs is that since they have to teach the model about themselves, they pollute the context pretty quickly, so be sure to only enable those you need.
Aside: Agents
â€œAgents are models using tools in a loop.â€ So a model that has both reasoning and tool use capabilities hits a sweet spot, and is commonly called an â€œagentâ€. These can call tools repeatedly while reasoning, and provide a complete answer. The truth is far from perfect, in my experience, but itâ€™s still a cool concept.
Finding a good model
LM Studio has a built-in UI for finding models, which shows runtime, quantization, model capabilities, and size in a user-friendly way. For llama.cpp, you can check Hugging Face for GGUF models.
You may not find a model that hits all the capability boxes, so itâ€™s better to download a variety of models and experiment with them. Here are some of my favorites, in no particular order:

Gemma 3 12B QAT: for visual intelligence and itâ€™s generally a good non-reasoning model thatâ€™s fast and produces good text
Qwen3 4B 2507 Thinking: This is the updated version of Qwen3 4B, which also has a non-reasoning variant; itâ€™s really small, fast, and good quality for its size
GPT-OSS 20B: The largest and most capable model that can run on my machine, has three levels of reasoning; itâ€™s rather slow but very capable, smartest of all
Phi-4 (14B): It was my favorite before GPT-OSS, now has reasoning and reasoning plus variants, but I havenâ€™t used it lately

Final words
Small models may not replace frontier models in terms of speed or quality, but I still see utility in them. Running them locally is also a good test bed for understanding how they work and learning to work around their weaknesses.
Let me part with a final tip: LM Studio shows you how much of the context window is being used. So you may find it useful to ask for it to summarize the conversation so far, when the context window gets close to being filled. This way you can help it remember important information that it would otherwise forget.4
Have fun with your brand new genie in your computer!
Thanks to Jull for reviewing an early draft, and my girlfriend for the leekspin hero image.



With an M-series, Apple Silicon chipset, Intel chips are pretty old at this point and wouldnâ€™t run LLMs wellâ†©
You may be wondering about performance and if it will be usable. When you generate a model response, two things happen in sequence; prompt processing and token generation. Prompt processing is tokenizing the entire chat history, is done in parallel, and compute bound. Token generation is sequential, and is memory bandwidth bound. Both of these things get slower as the context size increases.

Also, counterintuitively, these models donâ€™t run on the Neural Engine. Both llama.cpp and MLX run on the GPU, using Metal shaders. Iâ€™m not entirely sure why though.â†©
Itâ€™s called that because the common wisdom is that models reason with tokens, and a smaller model can generate a higher quality response if it generates more tokens. To protect the user from the burden of reading through more slop, these reasoning tokens are usually hidden from the user.â†©
This is called compaction and, coding tools such as Claude Code can do this automatically, or provide a command for you to trigger it manually.â†©

Share on BlueskyÂ Â·Â Edit on GitHub]]></content:encoded>
        </item>
        <item>
            <title><![CDATA['We can do it for under $100M': Startup joins race to build local ChatGPT]]></title>
            <link>https://www.afr.com/technology/we-can-do-it-for-under-100m-start-up-joins-race-to-build-local-chatgpt-20250908-p5mt5o</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45168883</guid>
            <description><![CDATA[Sovereign Australia AI has unveiled plans to build Australis, a local alternative to the AI models of tech giants OpenAI, Anthropic, Google and Meta.]]></description>
            <content:encoded><![CDATA[Two Australian entrepreneurs have joined the race to build a local alternative to the artificial intelligence models created by tech giants like OpenAI and Meta, earmarking $10 million to compensate copyright owners for their work.Sovereign Australia AI was founded by AI strategist Simon Kriss and technology executive Troy Neilson who shared concerns that Australiaâ€™s access to the critical technology would be at the mercy of the decisions made in the United States or China.Loading...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Exploring Grid-Aware Websites]]></title>
            <link>https://www.nicchan.me/blog/exploring-grid-aware-websites/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45167887</guid>
            <description><![CDATA[Could adapting websites to a user's energy grid help make them more sustainable?]]></description>
            <content:encoded><![CDATA[ Over the past year, Iâ€™ve had the incredible privilege of getting to participate in the Grid-Aware Websites (affectionately abbreviated to GAW) advisory group. The Green Web Foundation team have thoroughly explained what a grid-aware website is in this detailed case study of the Branch Magazine redesign, but if I had to put it in my own words, a grid-aware website responds to the cleanliness or dirtiness of a userâ€™s electricity grid. In simpler terms, whether it is currently using more renewable energy sources or fossil fuels.
To do so in the most performant way possible, this currently involves getting a userâ€™s rough location and using the Electricity Maps API to check the current status of the grid, and then returning that information to a serverless function on the edge so that the website can be modified, all before it reaches the userâ€™s browser.
To explore this concept, Iâ€™ve thrown together a small demo of what this might look like in two of my favorite web tools that I believe are conducive to this effort, and written up a few of my personal thoughts.
Why implement grid-awareness?Many folks may notice that the in brief approach outlined above, thereâ€™s a non-trivial amount of code required to implement grid-awareness. Since less code = better performance = greener websites, isnâ€™t this a bit counterintuitive, if not self-defeating? I hear you, and itâ€™s something weâ€™ve thought about a lot as a group, and the Green Web Foundation have addressed this question excellently in their FAQ. Essentially, this is an experimental approach to see if this is a viable way to reduce the carbon emissions of websites.I would love nothing more than for every site and application to be the best version that it could possible be, but weâ€™re definitely not living in that world at the moment. The collective push by performance, sustainability and accessibility advocates have not been able to make the industry shift meaningfully, despite strong real world incentives to make sites more usable for more people.When every company is asking â€˜How can we do more?â€™, I think it is a powerful mindset shift to think â€˜How can we do more with less?â€™ This question is typically framed in terms of having less budget or monetary resources, but when asked in terms of a performance budget or a user interface design, it forces you to think about what parts of the user experience is really necessary.A big part of the Green Web Foundationâ€™s philosophy with this project is raising awareness about the inherent connection between digital and physical worlds. To a designer using the latest MacBook with a Gigabit internet connection, performance concerns may often feel completely abstract and divorced from reality. Tools that remind us that yes, the choices we make do have an actual impact, can be an important part of driving change.We canâ€™t ignore the impact that digital systems have on our environment, especially at this critical moment when electricity usage is greater than ever. Sure, the energy saved from efforts like this may be a drop in the bucket compared to what AI is using, but I donâ€™t think weâ€™ve ever solved any societal problems by not trying to think about or address them.Though my preferred approach is to build progressively enhanced, lean sites in order to make them sustainable, many brands prefer to express a commitment to sustainability in more overt ways. The visibility and positioning of something like a grid-aware toggle can be a powerful motivator for certain types of companies.For example, I once had a client that sold sustainably made household items, and they wanted their website to reflect their commitment to sustainability. I suggested the usual high-impact things like reducing the size of media and using a static site generator. They didnâ€™t mind the statically-generated approach as it helped them save on hosting costs, but they didnâ€™t want to give up their use of background video, a decision that forced every user to download an additional 6MB of video. Instead, they suggested implementing a dark mode toggle, which I was more than happy to do to improve accessibility, but deep down, I knew that this was still a a net loss. If grid-aware websites had existed back then, maybe we could have reached some kind middle ground.Ultimately, the end goal of the grid-aware websites project is to not build a single library, but to embed this kind of thinking into the greater web ecosystem. Much like how we have media queries for things like preferring reduced motion or using forced colors mode, what if we could have a media query to prefer a simpler, more environmentally friendly experience? The future is uncharted territory, and this is an open invitation to invite the web dreamers, gardeners, and explorers to join in.
The breakdownStep 1: Determine what grid-awareness looks like for youNow that weâ€™ve discussed why one might want to consider implementing grid-awareness, letâ€™s go into how one would implement it. In this post I will be following this tutorial by the Green Web Foundation pretty closely, which involves deploying the project to Cloudflare and spinning up a worker from their template.Since Iâ€™ve been living in e-commerce land lately, I decided to throw together a proof-of-concept using a product display page (PDP) as a practical example that could relate to a wider audience. Although it is widely known that a performant product display page is important for improving conversions, many merchants opt for style over substance, resulting in page that is slow to load. I added some components that are fairly common across e-commerce sites, including:
a scrolling marquee
an image slideshow
an image lightbox
tabs for product information
a modal notification upon adding an item to the cart
All these components require JavaScript, but fall back to alternatives that still function without JavaScript. For example the scrolling marquee becomes a static text banner, the lightbox slideshow is just a horizontal scrolling region with links to larger images, the tabs fall back to a list of headings and paragraphs, and the product form is just a regular HTML form.For this particular example, I thought that a good way to implement grid-awareness would be to create a distinction between JavaScript that is core to the e-commerce experience, and JavaScript that is just for presentation. I decided that of the JavaScript features listed above, only the modal form was integral to the purchasing experience, so I would need to find a way to disable the other features when the grid was â€˜dirtyâ€™.The grid-aware worker has three modes to represent three levels of carbon intensity â€” â€˜lowâ€™, â€˜moderateâ€™ and â€˜highâ€™. To keep this simple, weâ€™re just going implement this as a binary on and off state. In â€˜lowâ€™ mode, you get the site with all the bells and whistles, and in â€˜moderateâ€™ and â€˜highâ€™ mode, you will just get the core experience.Step 2: Determine how youâ€™re going to make it happenFollowing the tutorial provided by the Green Web Foundation, I deployed the site to Cloudflare, then installed the specific grid-aware worker they made (thereâ€™s also plans to build out a Netlify equivalent). Out of the box, the plugin provides a drop-in connection to the Electricity Maps API once you plug in your key, which is very helpful. This worker will get the request, get the userâ€™s rough location, and then determine the current state of the grid.It can also optionally inject the grid-aware status bar web component, which will show some data about the userâ€™s current grid and allow the user to manually toggle between the modes. The status bar is optional, you can also build your own custom solution to give the user control over their grid-aware experience.The website design changes in response to the grid are unique to each website, so this is an implementation step for us. On the Cloudflare version of this worker, this involves using the HTMLRewriter API to transform the markup before itâ€™s sent to the user. Even though the underlying approach would be the same for both 11ty and Astro, due to the different bundling strategies, we have to implement them slightly differently. Letâ€™s take a closer look.11tyAh, 11ty, my old friend. I havenâ€™t picked it up in a while, but Iâ€™m excited to play around with the new WebC stuff, which lets you write single file components. The WebC architecture in 11ty is a really interesting idea, and the same technique that can be used for setting up critical CSS and JS can be used to set up grid-awareness.One thing I particularly like about 11ty + WebC is that it lets you put scripts and other assets into designated buckets, while still allowing the author to write them in the main component file. Iâ€™ve included an example below.<!--This goes in your component file -->
<script webc:bucket="bucketName">
	/* Your script stuff goes here! */
</script>

<!--This goes in your layout or page file -->
<script @raw="getBundle('js', 'bucketName')"></script>
This is really handy! We can now separate the JS that is optional by putting it in itâ€™s own separate bucket, so we can easily stick a data-gaw-remove attribute on it it for our Cloudflare worker to strip out. An approach like this is an excellent authoring experience, as we arenâ€™t limited to putting the data attribute on script tags, we can put it on markup, style tags, etc. Our resulting HTML Rewriter API function is super simple as a result, we just remove all of these elements in one fell swoop.  new HTMLRewriter().on(.on('[data-gaw-remove]', {
    element: (element) => {
      element.remove();
    },
  });
AstroAstro has a little more â€˜magicâ€™ in it than 11ty, and it made it a little harder to figure out a solid grid-aware approach. Ideally, we could use the same approach as 11ty and just take away anything with the data-gaw-remove attribute on it, but Astro has this thing where if you add any attribute to a script tag, Astro will not process it. Itâ€™s typically used as an escape-hatch for things like analytics scripts, so they arenâ€™t included in a bundle, but it poses a problem for us as we still want to process our scripts, we just want to add something extra on top.To split the code up into different buckets, weâ€™d probably need to hook into the underlying Vite/Rollup configuration. That felt beyond the scope of this small demo, so I came up with a quick-and-dirty solution. I noticed the bundled scripts for each Astro component kept the name of the component in the script src. I decided to use a prefix-based approach, and used the HTMLRewriter to strip out scripts that started with â€œGA-â€. In terms of code, this is almost as simple as the 11ty version, but quite a bit less flexible â€” I canâ€™t see people relying on opinionated naming as a way to implement this in real life scenarios.new HTMLRewriter().on("script", {
	element: (element) => {
		if (element.getAttribute("src")?.includes("GA-")) {
			element.remove();
		}
	},
});
Step 3: Finetune and deployThereâ€™s still a little work we need to do to get this up and running! We can reduce the number of external requests to the Electricity Maps API by implementing some caching. We can look at the documentation for the additional steps of setting up Workers KV, a form of data storage that will let us cache data from the API, the actual computed page, or both! The plugin default is to cache the data for one hour, so users from the same region wonâ€™t need to make additional requests. Once weâ€™re done configuring that, we can deploy the worker with npx wrangler deploy and start processing our site!DisclaimerPlease note, to make the distinction between grid-aware modes clearer and to
reduce the amount of code required for the demo to function, Iâ€™ve opted not
to respect accessibility features such as prefers-reduced-motion in the
following demonstrations to avoid accessibility-related behavior being
mistaken for grid-aware behavior. Iâ€™ve also reached for third-party
libraries so that I could get this post out in a reasonable amount of timeBelow is a comparison of what the site looks like when the grid is â€˜dirtyâ€™
vs â€˜cleanâ€™     'Dirty' grid       'Clean' grid      Visit the live 11ty demo and the Astro demo to see it respond to your local energy grid.Iâ€™ve put all the code into public repos (11ty, Astro), with the main project at the top level and the grid-aware worker in a designated folder at grid-aware-worker. Take a peek!
Is this another thing I have to care about?I know what youâ€™re thinking. Grid-awareness might seem like just one more item in the long checklist of things you should probably do, but donâ€™t have the time or budget to do.Hereâ€™s the thing, I think grid-awareness fits really neatly into all those other things you might already be doing to make a better website. If youâ€™ve already got a website that has features to reduce data usage or utilizes progressive enhancement/graceful degradation, you already have the bones for a grid-aware website. Grid-awareness is in itâ€™s infancy, and it isnâ€™t exactly a specific practice in the way progressive enhancement is, itâ€™s more of mindset.Grid-awareness on an large social media application with video data might look completely different to grid-awareness on the Branch magazine website, and thatâ€™s totally fine. For example, the Branch magazine website doesnâ€™t load web fonts and heavily compresses images when the grid is â€˜dirtyâ€™. That approach is completely different to the one Iâ€™ve done here, and thatâ€™s okay! One of the main points I want people to take away from this is that you get to decide how to implement this.It doesnâ€™t have to be a completely new thing, either. Many applications already have something like a data saver mode, or a lower power mode. Grid-awareness could be an extension of these settings, and utilize the same core logic, only switching up the conditions when this logic is triggered.
ChallengesThe first major blocker for the grid-aware websites project is the cost of the Electricity Maps API. The Carbon-Aware pricing plan required to make this work is currently 99 Euros a month (a recent development that occurred after the project started), a hefty price to cough up for a smaller organization. Itâ€™s hard to justify the expense when the entire architecture required to run the site may be cheaper, or even free. If you feel strongly about this, feel free to weigh in on this Github Issue. If there is a strong demand for a free plan, Electricity Maps may consider implementing it.This specific proprietary API isnâ€™t the only option. The EU has passed laws requiring member states to make this data available, and it may be possible that there will be greater options for this information with the greater shift to greener energy.The one thing that might be the biggest blocker for implementing grid-awareness at scale is that it involves all levels of the stack. Unlike areas such as accessibility or performance, where even one lone advocate can make a big difference, the method to implement grid-awareness requires a knowledge of all the moving pieces required to make it happen. The person who knows how to design a multi-layered, grid-aware experience is probably not the same person coding that front-end experience, and theyâ€™re probably not the same person with control over the building and deployment of the site.Smaller projects like this demo where one person is in charge of all aspects can more easily implement grid-awareness because the complexity is significantly reduced, but at the same time, these kinds of sites are already most likely to be the least problematic in terms of carbon emissions. The same kind of paradox occurs with the tooling choice â€” tools like 11ty and Astro that make it easier to split your JavaScript up are much more likely to produce performant outputs already, making it uncertain if implementing this is worth the tradeoff. Once you pick up a bigger web framework, youâ€™re more likely to benefit from implementing grid-awareness, but the barrier to do so will be much higher.The grid-aware advisory group is well aware of this adoption barrier and has representatives from open-source CMS communities, who are actively working on grid-awareness libraries for CMSes like Wordpress (Github Link), Drupal, Umbraco and Wagtail on the server-side. If grid-awareness was a solved problem at the deployment and ops layer, then it would be easier for designers and front-end developers to implement without worrying about this logic.
ConclusionThatâ€™s all I have on grid-aware websites for now! Itâ€™s been hard to summarize a years worth of tinkering and thinking into a single blog post, and I really hope Iâ€™ve done the subject justice. If you are at all interested in getting involved, the ClimateAction.tech Slack is a wonderful group where much of this conversation happens. Or, you can try to implement the code yourself, feel free to reach out if you want to give this a shot on your own site.

Green Web Foundationâ€™s developer tutorial
Fershadâ€™s case study for implementing grid-awareness on his own site has all the technical background you need behind the plugin
Fershad also has a writeup of the corresponding visual changes on his site
Progressive enhancement and graceful degradation in the context of grid-aware websites
Iâ€™ve linked the Branch Magazine case study before, but Iâ€™m going to do it again because itâ€™s my blog and Iâ€™m the boss around here
 ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[No more data centers: Ohio township pushes back against influx of Amazon, others]]></title>
            <link>https://www.usatoday.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45167729</guid>
            <description><![CDATA[USA TODAY delivers current national and local news, sports, entertainment, finance, technology, and more through award-winning journalism, photos, and videos.]]></description>
            <content:encoded><![CDATA[Amy Coney Barrett says 'I'm nobody's justice.' That includes Trump.Brutally honest reviews of every VMAs performanceThis NFL team's stunning, impossible win is an instant classic Trump fails to overturn E. Jean Carroll's $83.3 million verdictTrump booed at US open. He is the first president to attend since 2000America's moms love their jobs. Why are so many leaving them?Palestinian gunmen kill six at Jerusalem bus stopFeatured VideosFor YouWhich topics matter most to you? Add your favorites and youâ€™ll see more of these stories in the For You section.View For YouMore Top Stories Why Channing Tatum worried about his naked 'Roofman'â€¦ She had a stroke at 27. Itâ€™s a growing problem for youngâ€¦ ABC, ESPN air boos of Trump, refuse to kowtow to USâ€¦ Joni Ernst is out of Iowa's Senate race. Here's what itâ€¦ Why this may be Americaâ€™s most inclusive city forâ€¦ Is Howard Stern leaving his SiriusXM show? NFL winners, losers of Week 1: How does one contenderâ€¦ Hawaii braces for 'life-threatening surf' as Hurricaneâ€¦ Fall 2025: See when and where fall colors will peak Democrats are falling for Trump's 'invasion' trap. It'sâ€¦ When is the 2025 Apple Event? Date, start time and howâ€¦ The Aussie collagen craze has arrived stateside withâ€¦Discover Relive the MTV VMAs' most controversial momentsÂ Â Â Â Â Â Â  Highlights from the 2025 US OpenÂ Â Â Â Â Â Â  Italian teen becomes first millennial Catholic saintÂ Â Â Â Â Â Â Pope canonizes first millennial Catholic saint NFL winners, losers of Week 1: How does oneâ€¦ This NFL team's stunning, impossible win is anâ€¦ Bears star prepares for long haul insideâ€¦ Biggest overreactions from Week 1 of 2025 NFLâ€¦More in SportsShave, brush, trim and more with the Panasonicâ€¦ Get $20 pillow slippers with over 6,000 five-starâ€¦ Target's exclusive Stanley Halloween tumblers areâ€¦ Does your dog have allergies? This at-homeâ€¦More in ShoppingU.S. says it will deport Kilmar Abrego Garcia toâ€¦ Fourteen killed in Nepal in 'Gen Z' protest overâ€¦The CDC is in crisisGlobal refunds if Supreme Court strikes downâ€¦More in PoliticsSee the weekâ€™s best celebrity photos ðŸ“¸ See snakes, turtles and more at the New Yorkâ€¦Howard Stern's career, from shock jock to 'kingâ€¦VMAs most outrageous looks: From a labubu toâ€¦More in News in PicturesWhy you should upgrade your vacuum What are the kids saying? Learn the trendyâ€¦ 5-month-old feline rescued from dark storm drain Video shows moment police find 5-year-old aloneâ€¦More in Trending VideoAmerica's moms love their jobs. Why are so manyâ€¦ Who won the Powerball? What we know about theâ€¦ Does the auto start-stop feature really save fuel? Why are electricity bills so high now? What toâ€¦More in MoneyCure for balding could be in your kitchen She had a stroke at 27. Itâ€™s a growing problemâ€¦ 8 reasons volleyball is better for you than youâ€¦ What is broccolini â€“ and should you be eating it?More in Wellness8 reasons volleyball is better for you than youâ€¦ What is broccolini â€“ and should you be eating it? Is your bathroom routine 'normal'? Hereâ€™s whatâ€¦ 3 big cream cheese health concernsMore in Just CuriousAfter Sandy Hook, it's clear politicians won'tâ€¦ Zuckerberg says he'll defend free speech. He mayâ€¦ Schools should listen to parents, not activistsâ€¦ Democrats are falling for Trump's 'invasion' trapâ€¦More in OpinionSupertramp star Rick Davies dies at 81 Pamela Anderson slams 'PR stunts' amid rumoredâ€¦ 'Fight or Flight' gets streaming release dateâ€¦See the weekâ€™s best celebrity photos ðŸ“¸More in EntertainmentWhat to know before you take an electricâ€¦ Thinking of moving abroad? Hereâ€™s where expatsâ€¦ Travelers won't get extra compensation for flightâ€¦ Spirit Airlines cancels 11 routes amid bankruptcyâ€¦More in TravelA $2B NHL team seized control of youth hockeyâ€¦ Top Trump officials' stock sales clustered beforeâ€¦ Guitars and booze: How Canadians are reluctantâ€¦ Farmers face a fork on Trump's immigrationâ€¦More in InvestigationsThe CDC is in crisis Nitazene opioids are more potent than fentanylâ€¦Cracking down on ultrapotent nitazenesUltrapotent nitazenes present a new front inâ€¦More in U.S. NewsInteractive fall foliage maps predict 2025 fallâ€¦ 8 lip-smacking tailgate recipes for your nextâ€¦ Our 10 favorite adults-only, all-inclusiveâ€¦ 10 reasons why fall in British Columbia is magicalMore in USA TODAY 10BESTWhen is the 2025 Apple Event? Date, start timeâ€¦ When does the iPhone 17 release? What we knowâ€¦ What's being unveiled at the 2025 Apple eventâ€¦ Automakers turn windshields into screens withâ€¦More in Tech]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Meta suppressed research on child safety, employees say]]></title>
            <link>https://www.washingtonpost.com/investigations/2025/09/08/meta-research-child-safety-virtual-reality/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45167705</guid>
            <description><![CDATA[The companyâ€™s lawyers intervened to shape research that might have shed light on risks in virtual reality, four current and former staffers have told Congress. Meta denies the allegations.]]></description>
            <content:encoded><![CDATA[At her home in western Germany, a woman told a team of visiting researchers from Meta that she did not allow her sons to interact with strangers on the social media giantâ€™s virtual reality headsets. Then her teenage son interjected, according to two of the researchers: He frequently encountered strangers, and adults had sexually propositioned his little brother, who was younger than 10, numerous times.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Indiana Jones and the Last Crusade Adventure Prototype Recovered for the C64]]></title>
            <link>https://www.gamesthatwerent.com/2025/09/indiana-jones-and-the-last-crusade-adventure-prototype-recovered-for-the-commodore-64/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45167245</guid>
            <description><![CDATA[A huge Commodore 64 surprise to kick off the week with Games That Weren't adding the sadly cancelled playable prototype of the Indiana Jones 3 adventure. An unofficial conversion of the great Lucasfilm Games title with bags of promise, but wasn't to be.]]></description>
            <content:encoded><![CDATA[HomeAboutTeam25 years of Games That Werenâ€™tA talk at Retro CollectiveUnreleased & Cancelled gamesUncategorized reviewsVIC-20Recent updatesC64Browse GTW64Still in productionWantedAbout GTW64Prototypes & UnusedPreservationFeaturesBonus book contentContactSubmit an entryLinksArchive A-Z links | Search archive0ABCDEFGHIJKLMNOPQRSTUVWXYZA huge Commodore 64 surprise to kick off the week with Games That Werenâ€™t adding the sadly cancelled playable prototype of the Indiana Jones 3 adventure. An unofficial conversion of the great Lucasfilm Games title with bags of promise, but wasnâ€™t to be. Check it out here:https://www.gamesthatwerent.com/gtw64/indiana-jones-and-the-last-crusade-adventure/Share this page:DISCLAIMER: We are a non-profit digitisation project, aiming to digitally preserve software and history which would otherwise be lost for good. If for any reason there is anything that you do not wish to be on the website, please contact us for removal.Games That Weren'tÂ® is the registered trademark of Frank Gasking.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VMware's in court again. Customer relationships rarely go this wrong]]></title>
            <link>https://www.theregister.com/2025/09/08/vmware_in_court_opinion/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45167239</guid>
            <description><![CDATA[Opinion: Have you ever seen the 'Are we the baddies' sketch, Broadcom?]]></description>
            <content:encoded><![CDATA[
Opinion If you're a tech company marketing manager writing white papers, you'll love a juicy pull quote. That's where a client says something so lovely about you, you can pull it out of the main text and reprint it in a big font in the middle of the page.
"VMware is essential for the operations of Tesco's business and its ability to supply groceries" is a great candidate from 2019.
Broadcom's answer to VMware pricing outrage: You're using it wrong
READ MORE
Or it would be, if it wasn't followed by accusations of massive contractual misbehavior threatening the client, and requests for many millions of dollars in damages â€“ and rising. What looks great as marketing blurb isn't so hot on a court filing.
What a filing it is, too. Tesco is the UK's biggest supermarket chain by revenue, with around 40,000 server workloads keeping the ship afloat. Before Broadcom swallowed VMware, Tesco bought perpetual licenses and support that could run to 2030. Broadcom, Tesco claims, is refusing to honor the support contracts until Tesco switches to new licenses. This, it is further claimed, puts the retail giant at risk of being unable to operate.
Thus, Tesco is looking for damages of Â£100 million and rising from Broadcom, VMware, and the somewhat unfortunate reseller Computacenter. It's hard to feel sorry for a reseller. That's how bad this is.

    

Assuming Tesco's claims are true, this is extortion. Running an enterprise on unsupported software, while not exactly unknown, is corporate malpractice. Running unsupported software on which your entire business depends is nigh on suicidal. But who's holding the gun here?

        


        

Pulling patches, support, and upgrades that you are contractually obliged to provide, while demanding more money for a worse deal, doesn't look like being a good partner. It looks like running a protection racket. It looks like extortion. Nice multibillion business you've got there, man. Shame if anything, y'know, happened to it.
Can we assume Tesco is entirely accurate in its claims? Not until the case is heard, but the circumstantial evidence is there. The affair is in danger of turning into a class action lawsuit. The UK company is joining Siemens and AT&T.

        

Perhaps Broadcom has never lost similar cases? Um, no. Perhaps Broadcom is putting up a spirited public defense, rather than blaming its clients for doing it wrong? Um, no. Surely, it couldn't be so crass as to say its new licensing policy is not only blameless but very popular because it's bringing in so much more money, man, rather than 1,000-percent-plus price hikes? It is. All these stories are from just three months this year.
As The Register's European editor wearily remarked: "Search the site for Simon and VMware. We've got pages of this stuff. Go. Look."


How Windows 11 is breaking from its bedrock and moving away

Two wrongs don't make a copyright

When hyperscalers can't safeguard one nation's data from another, dark clouds are ahead

Fear of the unknown keeps Broadcom's VMware herd captive. Don't be cowed

It's not hard to guess Broadcom's motivation. Perpetual licenses can be a very bad idea for a vendor unless carefully constructed to be not really perpetual at all â€“ the thing being licensed can cease to be under agreed conditions, for example. You can unilaterally revoke them if your client base is too poor to sue, or you have an off-ramp that doesn't hurt too much. Neither seems to be the case here, in which case you negotiate with the licensees. If you try to strong-arm your clients by removing support, especially contracted support, you are not only acting unethically, to say the least, you are putting both you and your client at tremendous risk. Plus, you look like a gangster betting that a company will cave from fear of the consequences rather than stick it out.
Broadcom's policy seems to accept that. And the bigger the client, the better. Bigger equals more pressure, right? To which the right answer is do you feel lucky, punk? Do you think you'll win in court?
Does Broadcom think there's no real chance of a big client getting crippled because it didn't honor a contract and bad things happened? Better pray that doesn't happen.

        

Even if you win, somehow voiding the contracts that were signed in good faith and that so many of your best clients are confident taking to court, then what? Migrations will flock faster than swallows, African or European. If you're reading this, Broadcom, click on that search link above.
Broadcom admits it's sold a lot of shelfware to VMware customers
READ MORE
Ask yourself: would you want to trust your company to someone like that? How does someone like that look to the industry? If you didn't answer arrogant, greedy, unethical, untrustworthy, and full of it, why not?
While your respectable, blue-chip, long-established clients are taking you to court because they can find no other responsible way to continue to use your product, consider how many enemies you want to make before it becomes too many. Sure, court cases can be part of hardball negotiation, yet that's not what this looks like.
At some point, this has to stop. Virtualization, even as a deeply embedded framework, is a layer in the stack that can be replaced. The equation of migration is always complicated. It balances risk versus reward, ROI, inertia, future roadmaps, and more. Overarching it all is trust.
When you look in the mirror, Broadcom, what do you see? Â®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[14 Killed in protests in Nepal over social media ban]]></title>
            <link>https://www.tribuneindia.com/news/world/massive-protests-in-nepal-over-social-media-ban/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45166972</guid>
            <description><![CDATA[Nepali police fire tear gas and rubber bullets at protesters outside parliament.]]></description>
            <content:encoded><![CDATA[Unrest killed at least 19 people in Nepal on Monday, authorities said, as police in the capital fired tear gas and rubber bullets at protesters trying to storm parliament in anger at a social media shutdown and corruption.Some of the mainly youth protesters forced their way into the parliament complex in Kathmandu by breaking through a barricade, a local official said, setting fire to an ambulance and hurling objects at lines of riot police guarding the legislature."The police have been firing indiscriminately," one protester told the ANI news agency. "(They) fired bullets which missed me but hit a friend who was standing behind me. He was hit in the hand."More than 100 people, including 28 police personnel, were receiving medical treatment for their injuries, police officer Shekhar Khanal told Reuters. Protesters were ferrying the injured to hospital on motorcycles.Two more people were killed when protests in the eastern city of Itahari turned violent, police said.Prime Minister KP Sharma Oli called an emergency cabinet meeting to discuss the unrest, which erupted after thousands of young people, including many wearing their school or college uniforms, took to the streets earlier on Monday.Many carried flags and placards with slogans such as â€˜Shut down corruption and not social mediaâ€™, â€˜Unban social mediaâ€™, and â€˜Youths against corruptionâ€™, as they marched through Kathmandu.Organisers of the protests, which spread to other cities in the Himalayan country, have called them "demonstrations by Gen Z". They say the protests reflect young people's widespread frustration with government action to tackle corruption and boost economic opportunities."This is the protest by the new generation in Nepal," another protester told ANI.A government decision to block access to several social media platforms, including Facebook, last week has fuelled anger among the young. About 90% of Nepal's 30 million people use the internet.Officials said they imposed the ban because platforms had failed to register with authorities in a crackdown on misuse, including false social media accounts used to spread hate speech and fake news, and commit fraud.Batons and rubber bulletsPolice had orders to use water cannons, batons and rubber bullets to control the crowd and the army was deployed in the parliament area to bolster law-enforcement officers, Muktiram Rijal, a spokesperson for the Kathmandu district office, told Reuters.He said the curfew, which will remain in force until 10 pm local time, had been extended to Kathmandu's Singha Durbar area, which includes the prime minister's office and other government buildings.Violence abated later in the evening although protesters remained in the area outside parliament.Police said similar protests were also held in Biratnagar and Bharatpur in the southern plains and in Pokhara in western Nepal.Many people in Nepal think corruption is rampant, and the Oli government has been criticised by opponents for failing to deliver on its promises to tackle graft or make progress to address longstanding economic issues.Thousands of young Nepalis go abroad every year for work and education.Rameshwore Khanal, a former finance secretary, said although job creation is not up to expectations, popular anger appears to stem more from unhappiness with government appointments and its inability to stamp out corruption.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RSS Beat Microsoft]]></title>
            <link>https://buttondown.com/blog/rss-vs-ice</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45166750</guid>
            <description><![CDATA[Massive tech companies tried to own syndication. They failed.]]></description>
            <content:encoded><![CDATA[People like to tell the story of how VHS beat Betamax because adult film studios backed VHS. Itâ€™s a clutch-your-pearls story that says nothing about why these multi-million-dollar businesses picked one format over the other. The real story is that while Betamax tapes had better resolution and fidelity, VHS was cheaper, offered longer recordings, and, most importantly, was the more open format.Â Not many people talk about how or why RSS won the content syndication war because few people are aware that a war ever took place. Everyone was so fixated on the drama over RSSâ€™s competing standards (Atom vs RSS 2.0) that they barely registered the rise and fall of the Information and Content Exchange (ICE) specification, which had been created, funded, and eventually abandoned by Microsoft, Adobe, CNET, and other household names.Â ICE was the Betamax to RSSâ€™s VHS. The Information and Content Exchange standard was more advanced, more expensive, less open, and unable to counter the overwhelming number of bloggers who flooded the market with DIY-friendly RSS feeds.The dawn of war over syndicationWhen Pew Research informally asked readers about online activities that had lost their charm, most of the responses mentioned surfing the web, something people used to do for the hell of it, just to see what was out there. That was in 2007, the same year the iPhone launched, long before most of us were addicted to social feeds. One user complained that â€œthe net is no longer a toy but more like a Velveteen Rabbit â€” while some loved parts have worn away or disappeared, other parts are still in place.â€ People hadnâ€™t lost interest in surfing so much as the waves of content had grown to crushing heights.Big-name publishers got a whiff of monetization and became obsessed with content syndication. They figured that if they could make it easier for websites to repackage and republish their articles and eCommerce catalogs, corporate content creators wouldnâ€™t need to worry about declining traffic to their sites. They could simply make a deal with whichever site was currently in vogue.â€œSyndication will evolve into the core model for the Internet economy, allowing businesses and individuals to retain control over their online personae while enjoying the benefits of massive scale and scope,â€ Kevin Werbach wrote in the July â€˜99 issue of Release 1.0. â€œThe foundations for pervasive Web-based syndication are now being laid.â€The first attempt came in the form of the Information and Content Exchange (ICE) standard, which, like Betamax, predated its archrival by almost exactly a year. ICEâ€™s stated goal in a March 1998 proposal was to standardize how data posted to one website could be automatically published on other websites. It was unapologetically commercial from day one, promising to â€œexpand publishersâ€™ electronic sales by making it easier to license the same material to multiple sources.â€The My Netscape Network port via Scripting NewsRSS entered the game as a humble widget on the experimental My Netscape Network portal. Any website owner who used Netscapeâ€™s nascent XML-adjacent tags to create a feed of their websiteâ€™s updates could have said feed added to Netscapeâ€™s list of 600+ â€œchannelsâ€. When a user picked a channel from the list, it added a widget to their personalized My Netscape Network page, aggregating their favorite blogs and news sites on a single page.ICE and RSS had a lot in common. Both used XML to create a common language between syndicators and subscribers. Both used self-describing tags to differentiate content elements. And both let subscribers â€œpullâ€ the latest feed at any time. Philosophically, though, they couldnâ€™t have been further apart.Revenue vs readershipOne of the creators of ICE was Vignette, famous for its StoryBuilder content management software. They ceded technical development to a consortium that included Microsoft, Adobe, Reuters, and others, while focusing on commercial development. In 1999, Vignette invested $14 million in the iSyndicate platform in exchange for iSyndicate moving exclusively to ICE, while shopping around their proprietary ICE serverâ€“priced at $50,000â€“to other publishers.The first desktop RSS aggregator via Internet ArchiveMeanwhile, RSS was sprinting in the opposite direction. Headline Viewer was released in April 1999 as a free desktop feed aggregator that promised to let users â€œWatch mailing lists! Watch weblogs! Be cool!â€ It was soon followed by the first web-based aggregator at my.userland.com. There wasnâ€™t a whiff of server racks or five-figure investments. In fact, there wasnâ€™t even anyone at the helm, as Netscape had abandoned development.â€œNow, let the flames beginâ€¦repeated attempts to find anyone who cares about RSS at Netscape have turned up nothing,â€ Dave Winer ranted on his blog in the summer of 2000. â€œThe people we worked with at Netscape left shortly after [version] 0.91 was finalized.â€ That would have immediately killed the consortium-driven ICE standard. Not so for RSSâ€™s grassroots efforts. Winer simply wrote his own version.â€œUp until this morning I wasn't sure if this document should be called 0.91 or 0.92. I was concerned that practice had deviated from the Netscape spec, esp in respecting the limits it imposes, which most developers (myself included) think are ridiculous and unweblikeâ€¦so I changed the title from 0.92 to 0.91. So all this is a cleanup. All the Netscapeisms are removed.â€Â What started as a passion project by a disgruntled individual grew into a movement. And those in the ICE camp knew it. Laird Popkin went as far as writing a post about how to map RSS into ICE syntax, pointing out that â€œThe widespread adoption of RSS by low-end syndicators to distribute promotional links should serve as a clear indicator of the importance of this scenario in the world of syndication.â€Â Complicated vs UncomplicatedBut there was simply no getting around how bloated ICEâ€™s requirements were. Its North Star was automating complex, corporate publishing partnerships. It contained fields for catalog pricing and negotiation, content expiration tags, copyright enforcement functions, and the ability to apply the display websiteâ€™s visual branding to feed content. While most of them could be ignored, that didnâ€™t make its 58,000-word Getting-Started guide any more digestible.A web-based aggregator in 1999 via Release 1.0Werbach predicted in The Web Goes Into Syndication that, because ICE was overkill for most uses, â€œShould this become a head-to-head competition, though, the moral of the Internetâ€™s story is that simple, open-source protocols that scale up tend to win over complex top-down approaches.â€Almost anyone could set up an RSS feed or an RSS aggregator. When Winer released RSS version 2.0 in 2002, a feed could be considered compliant with only three elements: feed title, feed description, and a link to the items you wanted to share. â€œI definitely want ICE-like stuff in RSS2, publish and subscribe is at the top of my list, but I am going to fight tooth and nail for simplicity.â€ Winer argued in one of the many combative and public RSS mailing lists.Never mind that the RSS group couldnâ€™t even agree on what the acronym stood for, they were trouncing ICE. The New York Times, a publisher that should have been firmly in ICEâ€™s wheelhouse, adopted RSS in November 2002. ICE limped on, however, with the authoring group releasing version 2.0 in 2004.Â But less than a year later, Microsoft, arguably ICEâ€™s biggest cheerleader, had a dedicated RSS blog. Its first post proposed icon designs for Internet Explorerâ€™s built-in RSS features. It wasnâ€™t an explicit capitulation. ICE and RSS could have theoretically co-existed. Just like Betamax could have let other companies manufacture and sell Betamax players. But they didnâ€™t. So they lost.When small wins are better than big onesÂ In their timeline of The Rise and Demise of RSS, Sinclair Target sees the glass half empty, believing that in another timeline the standard could have been more widely adopted if not for the fights between developers. â€œRSS, an open format, didnâ€™t give technology companies the control over data and eyeballs that they needed to sell ads, so they did not support it. But the more mundane reason is that centralized silos are just easier to design than common standards.â€Â And yet, no one has heard of ICE. I couldnâ€™t even find proof of any publishers who used it to ink syndication deals. RSS, meanwhile, lives on. RSS-to-email is one of Buttondownâ€™s most popular features!"I can't really explain it, I would have thought given all the abuse it's taken over the years that it would be stumbling a lot worse," Winer told Wired in 2015, as opinions on algorithmic social media began to sour.All RSS had to do to weather ICE, Twitter, AI, and whatever comes next, was keep things simple and let users build their own feeds, filters, lists, and aggregators. Like email, it probably wonâ€™t make anyone a billion dollars or reshape entire industries. But it will always be wholly yours. And if that isnâ€™t nice I donâ€™t know what is.Header image via Bloglines.softonic.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How inaccurate are Nintendo's official emulators? [video]]]></title>
            <link>https://www.youtube.com/watch?v=oYjYmSniQyM</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45166711</guid>
        </item>
        <item>
            <title><![CDATA[Immich â€“ High performance self-hosted photo and video management solution]]></title>
            <link>https://github.com/immich-app/immich</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45165684</guid>
            <description><![CDATA[High performance self-hosted photo and video management solution. - immich-app/immich]]></description>
            <content:encoded><![CDATA[ 
  
  
  
    
  



High performance self-hosted photo and video management solution






  CatalÃ 
  EspaÃ±ol
  FranÃ§ais
  Italiano
  æ—¥æœ¬èªž
  í•œêµ­ì–´
  Deutsch
  Nederlands
  TÃ¼rkÃ§e
  ä¸­æ–‡
  Ð£ÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ°
  Ð ÑƒÑÑÐºÐ¸Ð¹
  PortuguÃªs Brasileiro
  Svenska
  Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
  Tiáº¿ng Viá»‡t
  à¸ à¸²à¸©à¸²à¹„à¸—à¸¢

Disclaimer

âš ï¸ The project is under very active development.
âš ï¸ Expect bugs and breaking changes.
âš ï¸ Do not use the app as the only way to store your photos and videos.
âš ï¸ Always follow 3-2-1 backup plan for your precious photos and videos!

NoteYou can find the main documentation, including installation guides, at https://immich.app/.

Links

Documentation
About
Installation
Roadmap
Demo
Features
Translations
Contributing

Demo
Access the demo here. For the mobile app, you can use https://demo.immich.app for the Server Endpoint URL.
Login credentials



Email
Password




demo@immich.app
demo



Features



Features
Mobile
Web




Upload and view videos and photos
Yes
Yes


Auto backup when the app is opened
Yes
N/A


Prevent duplication of assets
Yes
Yes


Selective album(s) for backup
Yes
N/A


Download photos and videos to local device
Yes
Yes


Multi-user support
Yes
Yes


Album and Shared albums
Yes
Yes


Scrubbable/draggable scrollbar
Yes
Yes


Support raw formats
Yes
Yes


Metadata view (EXIF, map)
Yes
Yes


Search by metadata, objects, faces, and CLIP
Yes
Yes


Administrative functions (user management)
No
Yes


Background backup
Yes
N/A


Virtual scroll
Yes
Yes


OAuth support
Yes
Yes


API Keys
N/A
Yes


LivePhoto/MotionPhoto backup and playback
Yes
Yes


Support 360 degree image display
No
Yes


User-defined storage structure
Yes
Yes


Public Sharing
Yes
Yes


Archive and Favorites
Yes
Yes


Global Map
Yes
Yes


Partner Sharing
Yes
Yes


Facial recognition and clustering
Yes
Yes


Memories (x years ago)
Yes
Yes


Offline support
Yes
No


Read-only gallery
Yes
Yes


Stacked Photos
Yes
Yes


Tags
No
Yes


Folder View
Yes
Yes



Translations
Read more about translations here.



Repository activity

Star history

 
   
   
   
 

Contributors

  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Veena Chromatic Tuner]]></title>
            <link>https://play.google.com/store/apps/details?id=in.magima.digitaltuner&amp;hl=en_US</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45165245</guid>
            <description><![CDATA[Tune in Equal/Just Intonation with oscilloscope-like waveform display.]]></description>
            <content:encoded><![CDATA[Veena Chromatic Tuner lets you to tune in Equal Temperament/Just Intonation with oscilloscope-like stable waveform display. This powerful, versatile tuning application is designed for musicians who need precise control over their instrument tuning across multiple musical traditions (Equal Temperament, Just Intonation). It displays the pitch (frequency) and deviation in cents via a needle interface. A unique visual feature is its linear and circular waveform display of input audio: the waveform appears stabilised when in tune with reference note selected, rotates right when sharp, and rotates left when flat, providing immediate visual feedback for precise adjustments. This dynamic visual feedback, akin to a digital oscilloscope's trigger synchronisation, offers immediate and precise adjustment cues.A Veena instrument mode has been added, allowing users to play and tune notes across 24 fret positions. This feature is particularly useful for instrument makers. When choosing Just Intonation or Carnatic tuning profiles, fret positions are based on specific ratios. Consequently, this mode allows you to observe how note positions on the first and other strings vary relative to each other when pure intonation is applied. Key Features:Multiple Tuning Systems - Choose between Equal Temperament, Just Intonation, or create your own Custom temperament with personalised ratio settings.Multicultural Note Naming - Display notes in various systems including Western notation, Indian classical (Carnatic/Hindustani), and Solfege, with support for multiple Indian language scripts (Tamil, Devanagari, Telugu, Kannada, Malayalam). The base note selected is treated as "Sa".Adjustable Reference Pitch - Customize your A4 frequency from standard 440Hz to alternative tuning like 432Hz.Transposition Support - Easily transpose notes up or down for instruments that play in different keysCustom Tuning Profiles - Create, save, and manage multiple tuning profiles for different instruments or musical traditions.Just Intonation Support - Perfect for Indian classical music and other traditions that rely on pure harmonic relationships between notes.Perfect for:- String instrument players (veena, violin, guitar, sitar, banjo, piano, lute etc.)- Veena melam(fretboard) making / repair - Wind instrument musicians- Vocalists- Music teachers and students- Ethnomusicologists- Instrument makers and tuners - Fret setting- Anyone exploring microtonal musicDownload Veena Chromatic Tuner today. Please note: this application is ad-supported. We are continuously developing the app, rapidly adding new features and improvements. However, pitch detection is a resource-intensive application, and as a result, compatibility may vary depending on the hardware and software limitations of your device. We appreciate your interest in our app and apologise if it doesn't function as expected on your specific device.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Claude Code to modernize a 25-year-old kernel driver]]></title>
            <link>https://dmitrybrant.com/2025/09/07/using-claude-code-to-modernize-a-25-year-old-kernel-driver</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45163362</guid>
            <description><![CDATA[As a bit of background, one of my hobbies is helping people recover data from old tape cartridges, such as QIC-80 tapes, which were a rather popular backup medium in the 1990s among individuals, small businesses, BBS operators, and the like. I have a soft spot for tape media; thereâ€™s something about the tactile sensation of holding these tapes in my hands that makes the whole process very joyful, even though QIC tapes are notorious for their many design flaws. With some careful inspection and reconditioning, the data on these tapes is still totally recoverable, even after all these years.]]></description>
            <content:encoded><![CDATA[
	
	

	
		As a bit of background, one of my hobbies is helping people recover data from old tape cartridges, such as QIC-80 tapes, which were a rather popular backup medium in the 1990s among individuals, small businesses, BBS operators, and the like. I have a soft spot for tape media; thereâ€™s something about the tactile sensation of holding these tapes in my hands that makes the whole process very joyful, even though QIC tapes are notorious for their many design flaws. With some careful inspection and reconditioning, the data on these tapes is still totally recoverable, even after all these years.



Whenever I receive a QIC-80 tape for recovery, I power up one of my older PC workstations which has the appropriate tape drive attached to it, and boot into a very old version of Linux (namely CentOS 3.5), because this is the only way to use the ftape driver, which is the kernel driver necessary for communicating with this tape drive, allowing the user to dump the binary contents of the tape.
You see, the drive that reads these tapes connects to the floppy controller on the motherboard. This clever hack was done as a cost-saving measure: instead of having to purchase a separate SCSI adapter (the standard interface for higher-tier tape media), you can just connect this tape drive to your floppy controller, which was already available on most PCs. It can even work alongside your existing floppy drive, on the same ribbon cable! The tradeoff, of course, is that the data rate is limited by the speed of the floppy controller, which was something like 500 Kbps (thatâ€™s kilobits, not bytes).
The other downside is that the protocol for communicating with these tape drives through the floppy controller was very messy, nonstandard, and not very well-supported. It was a â€œhackâ€ in every sense: your motherboardâ€™s BIOS had no knowledge of the tape drive being connected, and it was entirely up to the end-user software to know exactly how to manipulate the hardware I/O ports, timings, interrupts, etc. to trick the floppy controller into sending the appropriate commands to the tape drive.

There were a small number of proprietary tools for MS-DOS and Windows 3.x/9x for dealing with these drives, and only one open-source implementation for Linux, namely ftape. Of course it is possible to use those original DOS/Windows tools to read the tapes, but itâ€™s actually only ftape that allows us to read the â€œrawâ€ binary contents of the tape, regardless of which proprietary software originally wrote it, which is why I prefer it for dumping the contents and worrying afterwards about decoding the proprietary logical formatting, and then extracting the files from it.
The trouble is, the ftape driver hasnâ€™t been supported since roughly the year 2000, and was soon removed from the Linux kernel for this reason. This is why Iâ€™ve needed to run a painfully old version of Linux anytime I have to work with one of these drives. It would be great if ftape worked on a modern distro, with all the benefits and affordances that would provide.
***
So a couple of weeks ago, it occurred to me to make a simple request to Claude Code:
> This repository is a Linux kernel driver that communicates with legacy tape drives connected to the floppy
  controller (FDC) on the motherboard. Unfortunately, this driver hasn't been maintained for a long time, and
  can only compile under kernel version 2.4. I'd like to modernize this driver, allowing it to be built with
  the latest versions of the kernel.

â— I'll help you modernize this Linux kernel driver for legacy tape drives. This is a significant task that
  will require updating the code to work with modern kernel APIs and conventions.

And after several iterations of â€œcombobulatingâ€ and whatever else Claude claims to do, I suddenly had a kernel driver that was compiling without errors. This is because Claude is able to take the compiler output and feed it back into itself, until the compilation works correctly. There was a laundry list of kernel functions and structures that were understandably deprecated or replaced, in the eternity that elapsed between kernel version 2.4 and version 6.8. Amazingly, Claude found all the outdated bits and replaced them with the correct modern equivalents, requiring just a few manual touch-ups of the code (a bit more on this later).
At this point, however, the kernel driver was still expected to be compiled as part of a full kernel tree, whereas I just wanted it to become a standalone loadable kernel module. OK, no problem:
> Is there a way to compile just this module in-place, instead of copying it into a kernel source tree?

â— Yes! You can compile kernel modules out-of-tree without copying them into the kernel source. Let me create
  a proper standalone build system for the ftape driver.

â€¦And it did. Without any further prompting. At the end of this stage, I had a loadable kernel module (.ko) that I could start trying to use with actual live hardware. Unsurprisingly, the module did not yet â€œworkâ€ â€” it was loading, but something was preventing it from communicating with the tape drive correctly. But I pressed on.
From this point forward, since loading/unloading kernel modules requires sudo, I could no longer let Claude â€œiterateâ€ on such sensitive operations by itself. Luckily, the ftape driver was already generously sprinkled with logging messages (printk() calls that are printed to dmesg), so I loaded the module myself, and iteratively pasted the output of dmesg into Claude manually, and let Claude compare it to a â€œknown-goodâ€ dmesg log that Iâ€™d previously saved from a successful tape read. After a few iterations of this, Claude identified the remaining issues preventing the module from communicating with the hardware.
â— Perfect! The module has parameters that can be set. The issue is that without configuration, all the base
  addresses default to -1, which gets converted to 0xffff in the config function, causing the detection to fail.
â— The "No such device or address" (ENXIO) error occurs because the ftape-internal module is not configured
  with the correct I/O port base address for your floppy controller.
  The issue is in fdc-internal.c:735-737: if fdc->sra == 0xffff, it returns -ENXIO. Since no configuration was
  provided, all values default to -1, which becomes 0xffff....

And at last, I had a kernel module that was able to a) load, b) detect the tape drive, and c) dump the contents of a test tape!

***
Using the ftape driver on a modern kernel, a feat that I thought was hopelessly out of reach, was suddenly completed over the course of two evenings.
As a giant caveat, I should note that I have a small bit of prior experience working with kernel modules, and a good amount of experience with C in general, so I donâ€™t want to overstate Claudeâ€™s success in this scenario. As in, it wasnâ€™t literally three prompts to get Claude to poop out a working kernel module, but rather several back-and-forth conversations and, yes, several manual fixups of the code. It would absolutely not be possible to perform this modernization without a baseline knowledge of the internals of a kernel module.
This led me to crystallize some thoughts on working with such coding agents in our current moment:
Open yourself up to a genuine collaboration with these tools.
Interacting with Claude Code felt like an actual collaboration with a fellow engineer. People like to compare it to working with a â€œjuniorâ€ engineer, and I think thatâ€™s broadly accurate: it will do whatever you tell it to do, itâ€™s eager to please, itâ€™s overconfident, itâ€™s quick to apologize and praise you for being â€œabsolutely rightâ€ when you point out a mistake it made, and so on. Because of this, you (the human) are still the one who must provide the guardrails, make product decisions, enforce architectural guidelines, and spot potential problems as early as possible.
Be as specific as possible, making sure to use the domain-specific keywords for the task.
Iâ€™m not claiming to suddenly be an expert in prompt engineering, but the prompts that Iâ€™ve found to be most successful are ones that clearly lay out the verbal scaffolding for a feature, and then describe the gaps in the scaffolding that the LLM should fill in. (For some reason the image that comes to mind is one of those biological stem-cell scaffolds where an artificial human ear will grow.)
Develop an intuition for the kinds of tasks that are â€œwell-suitedâ€ for an agent to complete.
These agents are not magical, and canâ€™t do literally everything you ask. If you ask it to do something for which itâ€™s not well-suited, you will become frustrated and prematurely reject these tools before you allow them to shine. On this point, itâ€™s useful to learn how LLMs actually work, so that you develop a sense of their strengths and weaknesses.
Use these tools as a massive force multiplier of your own skills.
Iâ€™m sure that if I really wanted to, I could have done this modernization effort on my own. But that would have required me to learn kernel development as it was done 25 years ago. This would have probably taken me several weeks of nonstop poring over documentation that would be completely useless knowledge today. Instead of all that, I spent a couple of days chatting with an agent and having it explain to me all the things it did.
Naturally, I verified and tested the changes it made, and in the process I did end up learning a huge amount of things that will be actually useful to me in the future, such as modern kernel conventions, some interesting details of x86 architecture, as well as several command line incantations that Iâ€™ll be keeping in my arsenal.
Use these tools for rapid onboarding onto new frameworks.
I am not a kernel developer by any stretch, but this particular experience ignited a spark that might lead to more kernel-level work, and it turns out that kernel development isnâ€™t nearly as difficult as it might sound. In another unrelated â€œvibe-codingâ€ session, I built a Flutter app without having used Flutter before. If youâ€™re like me, and your learning style is to learn by doing, these tools can radically accelerate your pace of learning new frameworks, freeing you up to do more high-level architectural thinking.
***
In any case, circling all the way back, I am now happy to say that ftape lives on! Twenty-five years after its last official release, it is once again buildable and usable on modern Linux. Iâ€™m still in the process of making some further tweaks and new feature additions, but I have already verified that it works with the floppy-based tape drives in my collection, as well as parallel-port-based drives which it also supports.

The physical setup looks very similar, but the OS is now Xubuntu 24.04, instead of CentOS 3.5! ðŸŽ‰
Until next time!
	

	
	

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Formatting code should be unnecessary]]></title>
            <link>https://maxleiter.com/blog/formatting</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45163043</guid>
            <description><![CDATA[and we knew this back in the 80s]]></description>
            <content:encoded><![CDATA[Sep 6, 2025Formatting code should be unnecessaryand we knew this back in the 80sI had a (maybe slightly overqualified) computer science teacher back in highschool, Mr. Paige.
He worked on the Ada compiler and
has been programming since the early 80s.
One day I complained about linter tooling that
was driving me nuts. I said something to the effect of, "it's 2016, how are we still dealing with this sort of thing?"
Turns out, that problem was solved four decades ago (well, three at that point). Back when he was working on Ada,
they didn't store text sources at all â€” they used an IR called DIANA.
Everyone had their own pretty-printing settings for viewing it however they wanted.
We've been debating some linter settings at work recently and I keep thinking back to
Mr. Paige. It's 2025, how are we still dealing with this sort of thing?

Well, to answer that it would help to know what we're missing.
I believe he was working with the Rational R1000, of which there isn't a ton of info
(like all things Ada, it was used by the DoD):

The R1000 had a lot of bleeding-edge features: incremental compilation, semantic analysis, version control, and first-class debugging all built-in. It was a workstation similar to the Xerox Alto but using Ada instead of Smalltalk.

DIANA (Descriptive Intermediate Attributed Notation for Ada) was a key component of Ada that enabled a lot of the more advanced features.

Taken from Experiences with Code Generation
(1984)
Instead of storing plain-text source code, the R1000 wrote DIANA.
The compiler and the IDE built into the machine both understood DIANA too, so you could view the source however you wanted.
Spaces vs. tabs didn't matter because neither affects the semantics and the editor on the system let you modify the program tree directly (known today as projectional editing).
Grady Booch summarizes it well:

R1000 was effectively a DIANA machine. We didn't store source code: source code was simply a pretty-printing of the DIANA tree.

Imagine that.
No wasted time due to formatting discussions or fighting linters,
without forcing everyone into the same editor setup (looking at you, eslint-config-airbnb).
And there were other benefits:

Using DIANA with hardware acceleration made it possible to do incremental compilation (unheard of at the time, for strongly typed languages), easy refactoring (though that word had not yet been invented), and incredibly fast integration (essential for the large systems that we being built with Ada).

Today, we don't need to worry about hardware-accelerated compilation (hopefully),
and we have better tools for refactoring (thanks, Claude). But with formatting, we regressed.
I'm not advocating for everyone to use projectional editing and a live environment (although I think they're awesome and we should be exploring them more),
but surely we can figure out something that fits into todays programming paradigms.
Further reading
This post was meant to be me saying "it'd be easier if we just pushed minified code", but I had too much fun
researching the R1000 during it. Here are some of the documents I looked at:

Experiences with Code Generation (1985): https://www2.eecs.berkeley.edu/Pubs/TechRpts/1985/CSD-85-249.pdf
Ada Compiler Validation Summary Report: Rational Environment (1985): https://apps.dtic.mil/sti/tr/pdf/ADA157830.pdf
Grady Booch's blog post about the 5th anniversary on Rational's acquisition (2008): https://web.archive.org/web/20160304051102/https://www.ibm.com/developerworks/community/blogs/gradybooch/entry/rational_anniversary?lang=en#pagePlaceBar
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Intel Arc Pro B50 GPU Launched at $349 for Compact Workstations]]></title>
            <link>https://www.guru3d.com/story/intel-arc-pro-b50-gpu-launched-at-for-compact-workstations/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45162626</guid>
            <description><![CDATA[Intel has officially expanded its professional GPU portfolio with the launch of the Arc Pro B50, designed specifically for small-form-factor workstations. The card is based on the Battlemage BMG-G21 GPU, configured with 16 Xe2 cores.]]></description>
            <content:encoded><![CDATA[
                                
                                    
                        
                        Intel has officially expanded its professional GPU portfolio with the launch of the Arc Pro B50, designed specifically for small-form-factor workstations. The card is based on the Battlemage BMG-G21 GPU, configured with 16 Xe2 cores. It comes paired with 16 GB of GDDR6 VRAM clocked at 14 Gbps on a 128-bit memory bus, producing 224 GB/s of effective bandwidth. This configuration ensures that the GPU cores are properly fed while maintaining a low overall power draw. Intel has kept the total board power at 70 W, enabling the card to run entirely from the PCIe slot without external connectors. With a PCIe Gen 5 x8 interface, the Pro B50 balances efficiency and bandwidth for professional workloads.

One of the key features of the Arc Pro B50 is its suitability for AI workloads and specialized professional applications. Intel claims performance of up to 170 TOPS in INT8 compute, which is significant for local AI inference tasks, machine learning workloads, and data preprocessing. Beyond AI, the GPU is optimized for CAD, engineering, architectural visualization, and design software, where stability is just as important as raw throughput. To meet these needs, Intel supplies a certified workstation driver stack, ensuring predictable performance across industry-standard applications. The physical design reflects its target environment: the card uses a low-profile dual-slot form factor, making it ideal for dense workstation cases that prioritize both space savings and airflow efficiency.
                                    Display connectivity is handled via four mini DisplayPort outputs, which support multi-monitor setups critical for professional users who often work with complex datasets or design layouts. By providing flexibility in display configuration while maintaining a small footprint, Intel positions the Arc Pro B50 as a versatile tool for both AI and visual workflows. The emphasis is not on competing with high-end workstation GPUs in sheer raw power, but on striking a balance between price, efficiency, and reliability in scenarios where compact workstations are used.

With an MSRP of $349, Intel has aimed for the Arc Pro B50 to be an accessible entry into the workstation GPU segment. The card will be distributed both as a standalone retail product and through OEM workstation systems. At launch, early reviews from outlets such as HardwareLuxx, Phoronix, and Igorâ€™s LAB indicate that the card delivers consistent results within its targeted workloads, rather than focusing on gaming performance. As Intel continues to invest in both consumer and professional graphics solutions, the Arc Pro B50 demonstrates a focus on specialized, compact, and power-efficient GPU offerings that broaden its reach in the professional computing market.




Source: Techpowerup
                                
                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Taking Buildkite from a side project to a global company]]></title>
            <link>https://www.valleyofdoubt.com/p/taking-buildkite-from-a-side-project</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45162593</guid>
            <description><![CDATA[How an Australian developer turned his frustration with CI tools into a 13-year journey that redefined developer tooling.]]></description>
            <content:encoded><![CDATA[ðŸ‘‹ Welcome to Valley of Doubt, a free weekly newsletter that goes deep into founder stories from the early days of startups. ðŸš€Keith Pitt is the co-founder and former CEO of Buildkite, a devtools company that started in Melbourne and grew to have some of Silicon Valleyâ€™s biggest companies as clients. In this interview we dig into:Starting Buildkite as a side projectRunning out of money and having to go back to investorsThe challenges of growing into a venture capital valuationFinding your product voiceScott Handsaker (SH): Tell me who Keith Pitt is in 30 seconds or less.Keith Pitt (KP): I'm a dad with three kids. I'm someone who loves to explore with his hands and code, and I love exploring ideas. It's no surprise that I ended up in developer tools. Before I was a programmer, I was a magician doing magic shows. That's all about pleasing people, bringing joy and excitement to someone's day, subverting expectations and having them walk away with a memorable experience of something you spent a lot of time working on.Developer tools is the closest I can get to being a magician in programming, because I get to please my peers, help them be better, and bring a little magic to their day.I don't consider myself an entrepreneur. I never have, even though I do entrepreneurial things. I consider myself more of an inventor. The way that I like to figure out if my inventions are any good is by selling them, because I feel like the only true way to determine if something is of value is if someone's bought it.SH: Is the â€œinventorâ€ identity something that you carry with you throughout your life?KP: Yeah, I think so. I like making stuff, whether itâ€™s Lego with the kids or software for developers. I have a mad Lego collection, but I make something and then just pull it apart and put it in a box because I don't like displaying Lego. I like making it.SH: Tell me how you got started with Buildkite.KP: It was a side project of a side project. I started building an email transaction API like Mailgun. I needed some CI for it, and at work we were transitioning off Travis CI. My personal requirements collided with my work requirements and I thought, I'll quickly just bust something out. Years earlier Iâ€™d written a CI tool just for fun, so I dusted that off.At the same time, PIN Payments launched in Australia. It was sort of the first time in Australian development history where you could charge for something on the internet without messing around with PayPal or getting a merchant account and SOAP APIs. It was a REST API to charge credit cards.Because I'm a commercially minded inventor, I packaged up Buildkite. I spent my evenings working on it, and I wasnâ€™t really building it to make money. I was just building it for me and what I thought was good.I'd go to work and show my peers, "Hey, what do you think of this?" Little did I know that I was testing product-market fit. I thought I was just working on something that I liked.SH: When did you know you had something people wanted?KP: I eventually strapped a credit card system on there and put it on the internet. People I knew bought it to start with, but then one company bought it that I didn't know from Europe. I think they were called Moneybird. I was like, "Who the hell are these people?" That's when I knew I had something that people wanted.SH: How did pricing evolve?KP: I started at $5, $30, and $100 a month. Just classic SaaS tiers.I remember I was walking back from lunch and I got a ping on my phone that someone had upgraded to $500 a month. That was a moment of pure joy for me because this particular customer knew who I was, but they had believed in me. There's a lot of people telling you no throughout the whole journey, and so part of that whole experience was just ignoring people that didn't believe in you.When that enterprise customer came through, it was the first moment that I felt like someone else believed in me. That's when I knew I would be okay.SH: Were you still working at Envato at that point?KP: I was doing Buildkite after hours because my wife was studying to be a social worker at the time. I would do my day job and come home and work on Buildkite in the evenings. The work at Envato was super easy. I was on a green fields project, but my creative itches weren't being scratched at work.I needed someone that could support my Buildkite work. I wanted to do one day a week on Buildkite, but Envato weren't too keen on a part-time position. So I ended up leaving Envato and going to work at PIN Payments. They were cool with me working four days a week.SH: At what point did the co-founders come into it?KP: I was two years in and still solo. I remember the moment it started. I was on Twitter and one of my competitors had announced they'd raised some money and I was really pissed off because I thought, "Why can't I raise money? Why is no one giving me money?"I tweeted something salty and Matt Allen from the Ruby community replied. He said, "You can raise money. I'll show you." So I started applying for accelerator programs. All of them knocked me back because they didn't want to support first-time founders. This was 13-14 years ago when the Australian startup ecosystem was pretty small and risk tolerance was basically non-existent.They were like, "You have to find a co-founder." I went co-founder hunting but couldn't really find one. I had this weird romantic relationship with bootstrapping and going at it alone. At that same moment, I asked Tim Lucas, who I was working with at PIN, "Hey man, can you make me a new logo for Buildkite?" Instead of making me a logo he said, "How about I be your co-founder instead?" I said, "Okay." That was basically what it was.A custom illustration of Keith and Tim, presented to Tim on his last day at Buildkite.KP: Matt Allen helped us raise 200 grand from people we didn't know. Just a bunch of rich people, some of whom I still have not met to this day. We quit our jobs, paid ourselves 80 grand a year, and just worked on becoming profitable.We built things and sold them, tried marketing, and made enough money to hire someone. We did this until we were 15 people, just rinse and repeat. Grow the bank balance, increase revenue, hire people.SH: You mentioned it was a hard slog during those years. Do any particular moments stand out?KP: I ran out of money. I stuffed up big time. In Australia you have to pay employees superannuation, and no one told me that superannuation payments were quarterly. I thought I was doing the right thing by using our cash to pre-buy EC2 instances, and then a couple weeks later our external accounting firm told me, "Super's due". I thought, "Oh, we don't have enough money."But I wasn't that scared because I had a plan. I thought, "Why not be open about it?" because it would probably help. I was completely open about it and said to the team, "All right, so here's the situation, but here's what we're going to do."I converted a bunch of customers to annual payments, went back to investors for loans, and did some invoice advances. It was about 100 grand to see us through until we got R&D tax credits and bigger invoices.SH: What did you learn from that experience?KP: Cash flow is an art form in and of itself. It's not just the bank balance. It's the bank balance at any one moment in time. At that point the game changed for me to making sure that I don't run out of money. That's the number one thing.Don't run out of money and do whatever it takes to not run out of money. Buildkite took a long time to bloom and blossom, but if we had run out of money earlier on, I might have just given up.Brought to you by MurmarSH: How did Lachlan join as the third co-founder?KP: Lachlan joined a couple years in as a late-stage co-founder. I wasn't precious about the titles. When he joined, I thought if he does what we think he's going to do, the company that exists will be just as much of his creation as mine.The reason he joined was that Tim and I didn't really know what we were doing. We had a good sense of how to navigate, but we didn't know big company enterprise stuff and were still thinking small fry. Lachlan really helped us charge lots of money.I didn't have the confidence to walk into a customer and say, "This is $30,000," but Lachlan did. That's probably the biggest gift he gave the company. He gave us the confidence to ask for more.I didn't realise at the time that by having such a low price point, people weren't taking us seriously. For us to charge more, we had to really believe in ourselves. To walk into a company and say, "This is $100,000" with a straight face and truly believe it in your soul.SH: How did you handle the dynamics between three founders?KP: It's always hard when each of you want to be the CEO. I think all of us wanted a little bit of it, and that was super hard to navigate.I gave the role away twice. I started as CEO, but I was the CEO of one person and I never really liked the title. I gave it to Tim because I was struggling with it. I always thought I wasn't doing what I thought that role was supposed to be. I was too invested in the product, too invested in customers, and wasn't interested in budgets or back office stuff.Later on Lachlan took it over. I struggled with that because I always thought I could do a good job of it, but I wasn't ready to take the title and really own it.It wasn't until a couple years later that I personally gained the confidence to say, "I'm the CEO." When I eventually could do it, I think I did a pretty good job because I learned I needed to be the sort of CEO that I was. I couldn't be trying to replicate something I thought in my head.I decided I was a product-centric CEO, and when I owned that, that's when I started to flourish in the role. If we were selling brake pads, you'd want the person in charge to be an expert on brake pads. If we're selling developer tools, the person in charge should be an expert on developer tools.SH: Tell me about the first major funding round.KP: In 2019 we raised a Series A of $28 million. The reason we did that was because I wanted to buy a house. I had previously been to the bank for a home loan and every bank rejected me. I was too risky because Buildkite's finances were largely US revenue, and my finances were Buildkite's finances.A lot of that Series A round was taken out as secondaries. Everyone who had invested the original 200 grand in the seed round got their money back plus some. I had enough money to buy a house, and only a few million went into the company as rainy day money.Keith in the early days of BuildkiteSH: What did you learn about taking VC money?KP: We effectively chose the one that gave us the highest valuation and a clean term sheet. At the time it was awesome because it meant we could sell less of our company for more money, but no one really tells you what happens if you go in at a high valuation early on.If you're given a high valuation, you're not worth that at the moment. They're pricing you based on what they think you're going to be worth. You effectively need to grow into the valuation, and if you don't then in your next round, you end up with a down round.It made the next round much harder. We hadnâ€™t grown fast enough to justify the valuation, so the next round was effectively flat. No one warns you about that.I think we were naive going into that transaction. We always painted the picture of being a long-term sustainable business, and our position on that never changed. We weren't building something to sell. We were building something that would last forever. The VCs gave us money even though we said those words, but I think in their mind they were hoping we would change.Once you take big VC money, expectations change. They want big results, and we were still behaving like 37signals.Eventually I realised that if weâ€™re going to play the enterprise game, we have to play it properly. That meant raising again, and building the team to go after bigger customers.SH: What changed when you took over as CEO the second time?KP: When I took over CEO, I really embraced having a sales team. Up to that point, Buildkite didn't have a sales team. I used to have this romantic relationship with the Atlassian approach, who famously had never had a salesperson. In reality they did have salespeople, they just called them something different.The biggest learning from hiring a salesperson was that there is a certain dance you need to have with customers during a sale, and you want someone whose job it is to do that tango.We hired a salesperson and built a sales team, and tripled revenue in a couple of years through doing that.SH: Buildkite is well known for being a much-loved product in the developer community. Tell me about your product development process.KP: It took me nine years to figure out my product voice. I read a book by Rory Sutherland called Alchemy. It was a book on marketing, but weirdly enough, this marketing book helped me find my product voice.I had a lot of stuff that was just instinct and gut, but I never figured out how to turn that gut into something actionable or a set of product principles. Reading this book helped me find that voice.I remember the day I showcased to the company, "Hey, I finally figured out what I've struggled with all this time." I did a three-hour presentation to the company, and I think everyone hated it. Everyone hated it because so many people disagreed with the principles.SH: What were some of those principles?KP: Here's a great example: don't listen to customers. Listen to them, but don't listen to them. What I learned is that when a customer comes to you with a problem, they come to you with a cry for help disguised as a feature request. You have to ignore the feature request and get at their problem.Ninety-nine percent of the time, they've had some configuration issue somewhere else, 10 steps earlier, that's led them to this point where they've fucked it up so badly that they need something to solve that.A feature request is not a feature request. It's a cry for help. That was difficult for a lot of people to unwind in their head, but that principle helped a lot.Another principle: I always wanted developers to be treated as humans, not robots. Just look at any error message in any developer tool. It's probably obscure and shit. One of the magical parts of Buildkite was that I spent a lot of time taking what the developer would give me, and interpreting it to be what they actually meant.You have to have pure empathy for the user. You're almost like a parent because you can't always give them what they want. You can easily just add all these checkboxes and make everyone happy, but you pay for it later with a million checkboxes.SH: Why do you think the team reacted negatively?KP: I think I should have packaged up my principles differently or had a different approach. They were pretty black and white, quite dogmatic. It was like, "This is how it is." Anyone receiving that would probably not like being told "this is the way it is from now on."At that moment, I realised the company I had was somewhat incompatible with that. I should have spent more time thinking about that stuff before going on mass hiring.KP: There's something not a lot of founders talk about because maybe they're scared about it, but in industry there's this thing called culture fit that people get wrong all the time.If you hire too many different people with different work styles, different wants, different needs, different approaches, different ideas, different values, different ethics, then everything becomes hard mode.I wish I had gone back and been more involved in hiring. I would have opted more for culture fit. I would have been more bullish and precise: here is the type of person we want, let's go find that type of person.At some point in hiring you get exhausted and you just hire the person who's at the top of the pile, even if they might not match who you wanted. You think, "We need somebody, so we just get them." That's a trap I see a lot of founders fall into, and it has major repercussions over time.When I'm starting my new company, Unreasonable Magic, I'm being very strict about who I let in, way more exclusionary this time around. Teams that build products aren't a party. It's a set of inventors and makers, and you need everyone on the same wavelength.SH: How did having a board change the way you operated?KP: I personally struggled with board stuff. The board was heavily American, and they were all operators who were used to working at a particular pace. I wasn't at that pace, and I couldn't help but feel that I was always letting them down. Not going fast enough.Board work at some point becomes theater, and I struggle with that. I struggle with having to put on a show for a board. I was always struggling to find the balance between how do I get the most out of these smart people versus how is this not just a press conference? I never figured it out.The thing about boards is they have a lot of control, but these people think about your company for only a couple of hours a month. That was super tough because you care a thousand percent, and they care, but they only care for a couple of hours.SH: You have since moved on from Buildkite. Can you talk about your exit?KP: I worked on Buildkite for 13 years, and the hardest part about leaving was that I didn't get to see what happened next. It was almost like I left halfway through a season of a TV show, and there were all these characters and stories and threads I never got to see the ending to.Whenever any founder exits a company, so much of that company is them, so you're leaving a part of yourself. The biggest struggle I've had since leaving is that no one's ever going to care as much about the thing as you did.It's like when you have an electrician come to your house. Sometimes they don't vacuum up as much as you would. You're always going to find that extra wire or bit of gyprock because it's not their house. Your company is your house.No exit is good enough for any founder after a 13-year thing. It's almost an impossible task to try to figure out how to exit someone from a company after 13 years gracefully.SH: What's next for you with Unreasonable Magic?KP: I've got 13 years of ideas to explore. Where I'm focusing my energy is on programming. A lot of programmers like programming, as they like to code. The rise of coding tools fundamentally changes the relationship you have with the work, more than anything in the history of programming.A lot of programmers who would program and get fulfilment out of it have become managers of a coding machine. Whenever I have a day of using Claude or similar tools, I walk away feeling dumb and empty. It's like popcorn. It's not a real meal, it's not feeding your soul.Unreasonable Magic is about how can we take these tools and make you feel fulfilled? How can we let you be as productive as using a coding assistant but also feel that you are the boss, that you walk away smarter at the end of a coding session?It's that doing-the-opposite thing again where everyone's trying to be more productive, but for me it's not about productivity. It's about relationships to the code.Programming is thinking. When I'm writing code, I'm thinking about how each line will be executed, what happens if this line goes wrong. You just don't get that same level of intimacy with the program when you use coding assistants because they can whip out so much code so quickly.The products I built with Claude are worse than without them because I use programming as a way to think and interact with the problem. When you're coding, you're deeply invested in the problem you're solving, getting intimate with the problem. With AI tools, it's surface level. It's a one-night stand with a problem versus a deep and meaningful relationship.Claude certainly makes me more productive, but at what cost? You're paying with the knowledge you would have learned from interacting with the problem.SH: Whatâ€™s a book we should all read?KP: Alchemy by Rory Sutherland. Itâ€™s the book that helped me find who I am. That book changed more about Buildkite than I can list out. SH: What is a band or artist that we should all listen to?KP: Uhmm, I like beatboxing.SH: Do you beatbox yourself?KP: Oh gawd no. But there is this band I like, and the song is called Candy Thief. Itâ€™s K-pop. I probably listen to that song on repeat every day.SH: What is a podcast we should all listen to?KP: I listen to Conan Oâ€™Brien Needs A Friend. If Iâ€™m working all day, I feel like it's important for me to make sure my brain goes somewhere else at the end of the day. And Conan O'Brien is just a very silly human. SH: Amazing. Thanks for your time Keith, and good luck with Unreasonable Magic!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Creative Technology: The Sound Blaster]]></title>
            <link>https://www.abortretry.fail/p/the-story-of-creative-technology</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45162501</guid>
            <description><![CDATA[The Sound Blaster]]></description>
            <content:encoded><![CDATA[Sim Wong Hoo was born on the 28th of April in 1955, the tenth child in a family of twelve children (five brothers, seven sisters). His family were Singaporean Hoklo with ancestry in the southernmost area of Fujian, China, and they spoke Hokkien. He grew up in a kampung called End of Coconut Hill in Bukit Panjang, and his father, Sim Chye Thiam, was a factory worker while his mother, Tan Siok Kee, raised chickens, ducks, pigs, and rabbits, and grew fruits and herbs. The young Sim had chores around the house and around the farm as soon as he was physically able, and he often sold eggs at the local market before school classes started each day. This afforded him the ability to buy things for himself such as his harmonica when he was about 11. The harmonica was a hobby he greatly enjoyed throughout his life. He also enjoyed making his own games.Sim graduated from Bukit Panjang Government High School and then went on to attend Ngee Ann Technical College for engineering. At the college, Sim was a member of both the harmonica troupe, consisting of thirty people, and the Practice Theatre School. In the theatre, Sim provided musical accompaniment for the schoolâ€™s performances with the harmonica and the accordion, often performing his own arrangements. His two interests collided at this time in his life. When writing or arranging music, heâ€™d only be able to hear his composition during weekly practice. Having seen a computer, he realized that a computer could allow him to hear the music precisely as written while still working on it. Sim envisioned a computer that could play music, talk, or even sing, and his earlier entrepreneurial spirit drove him to an ambitious goal: selling 100 million units of a single piece of equipment. Sim graduated in 1975 and then entered the uniformed services for his obligatory two years.For three to four years following his service, Sim worked a brief stint on an offshore oil rig, designing computerized seismic data logging equipment. After that, he opened a computer education center at Coronation Plaza. As he was more interested in teaching and researching, he left the business work to his business partner. This wasnâ€™t a great decision. His partner took off with all the money.Sim Wong Hoo with a variety of Creative productsOn the 1st of July in 1981, Sim founded Creative Technology with Ng Kai Wa, who had been his childhood friend and classmate in a 440 sqft shop at Pearls Center using his own savings of around $6000. The company initially did computer repair and sold parts and accessories for microcomputers. Business wasnâ€™t great, so Sim also did some teaching. In whatever time he had left to him, he was busy developing his own products.Creative CUBIC 99 advertisementThe first Creative product (at least, for which I can find any evidence at all) was a memory board for the Apple II. Having an understanding of the Apple II, Creative followed their memory board by producing the CUBIC 99 in 1984. This was an Apple II compatible machine with a 6502, but it also featured a Zilog Z80 for compatibility with CP/M. I am not certain how this was arranged, but I imagine that it wasnâ€™t entirely dissimilar to the Microsoft Z80 SoftCard. Of course, this is Creative Technology, so the machine also featured a voice synthesizer allowing users to record and playback words in English or Chinese. The computer also had an optional Cubic Phone Sitter which could make and answer calls. This was the first computer to be designed and manufactured in Singapore.The market was moving quickly, and the IBM PC had created a standard. The CUBIC CT was released in 1986 as a PC compatible, and it featured graphics and sound capabilities. This was, essentially, a multimedia PC (with a weaker CPU than that standard would later dictate) localized in the Chinese language. Unfortunately, it was too early. With nearly zero software support for anything approaching the capabilities of the CT and an even smaller local market, the product was a failure.Creative Music System advertisementRealizing that the sound features of the CUBIC CT were likely more salable and supportable than the computer itself, Sim and his company chose to sell the sound card by itself as the Creative Music System (also C/MS or CT-1300). This board was built around two Philips SAA1099 chips providing 12 channels of square-wave stereo sound on a half-length 8bit ISA card, and it shipped with five 360K 5.25 inch floppy disks (Master Disk, Intelligent Organ, Sound Disk 1, Sound Disk 2, Utilities). To promote this card, Sim moved to California in 1988 and established Creative Labs. His goal was to sell at least 20,000 cards generating $1 million in revenue. The USA was the largest PC market, and he knew that sound cards were seeing good sales.Creative Game Blaster, image from pixelatedarcade.comBeing in the USA, Sim quickly realized that games were the software titles driving sound card sales, and this meant that heâ€™d need new branding and software partners. The C/MS became the Game Blaster, and the included software was now just the Intelligent Organ, a test utility, a TSR, and drivers for Sierra Online games. The inclusion of those drivers was key to what would follow. Creativeâ€™s partnership with Sierra meant that some of the most popular games of the era would support the Game Blaster; ultimately, over 100 games would support the C/MS and Game Blaster. Naturally, selling a card required a store front, and Creative found a partner in Radio Shack. While the Game Blaster sold better than any Creative product before it, it didnâ€™t overtake the Adlib.Creative SoundBlaster with the CUBIC CT, image from Creative Labs PTETo better compete, Creative needing something that was better than the Adlib but still compatible with it. This came in 1989 with the CT1310, better known as the Sound Blaster. The Sound Blaster offered 12-voice C/MS stereo sound, 11-voice FM synthesis with Adlib compatibility (via the Yamaha YM3812), a MIDI interface, a joystick port, microphone jack with a built-in amplifier, a stereo amplifier with volume dial, the ability to play back mono-sampled sound at up to 22kHz, and record at 12kHz. While a sample rate of 22k doesnâ€™t seem great (because it isnâ€™t) this did allow simultaneous output of sound effects and music in a game. Likewise, while a game port doesnâ€™t seem like all too big a deal, it saved the buyer an extra $50 to buy one separately, and it saved an ISA slot too. The Sound Blaster was the first sound card to feature digital sample playback, and it took over the market, quickly becoming the top-selling expansion card of any kind in under a year, and Creativeâ€™s revenues hit $5.5 million. With the C/MS never having been too popular, Creative followed the CT1310 with the CT1320 which removed the C/MS chips but kept sockets for them on the card.1989 also saw Creative release the PJS operating system and the PJ Views word processor and desktop publishing system which included support for 70,000 Chinese characters. As far as I know, these products were only released in Southeast Asia.Tandy Multimedia Audio Adapter, image from vcfed.org member CloudschatzeAnnounced in May of 1991, the Sound Blaster Pro, CT1330, was a major redesign of Creativeâ€™s sound card. This card used two Yamaha YM3812 chips to provide stereo sound while maintaining full backward compatibility with the original Sound Blaster and Adlib. Sample rates were increased to 22kHz for stereo, 44.1kHz for mono. A simple mixer, low-pass filter, high-pass filter, and CD-ROM interface were added. This CD-ROM interface could take multiple forms, but it was useful in pushing CD-ROMs into the mainstream. Many early CD-ROM drives were SCSI-only and that was expensive. Creative worked with MKE in Japan to produce low-cost IDE CD-ROM drives, and then included support on their cards. As for the card itself, while the card did have the AT connector, it wasnâ€™t 16bit. The Pro was still an 8bit card. The presence of the 16bit AT connector was for additional interrupts and DMAs on the 16bit bus that supported the Multimedia PC standard from Microsoft. The Sound Blaster Pro 2 was released shortly after the original, and it replaced the YM3812s with a single YMF262. The Pro series was often sold in Multimedia Upgrade Kits where it was bundled with a CD-ROM drive and software titles. Given that CD-ROMs were quite new, these kits often represented a significant value to consumers.This card can also be found in Tandy Multimedia PCs as the Tandy Multimedia Audio Adapter. Immediately noticeable changes were from the regular joystick port to two mini-DIN connectors compatible with the Tandy 1000 joysticks, and the addition of a mini-DIN MIDI port. For both the joystick connectors and MIDI connector, adapters were required. A less noticeable change, the Tandy card used a different bus interface chip, the CT1346, and the output amplifier could be disabled via a jumper. Finally, the card featured a high-DMA channel allocated for audio which allowed 16bit 44.1kHz mono output in Windows.Creative Sound Blaster, CT1350b, image from theretroweb.comThe Sound Blaster 2, or Sound Blaster Deluxe, model CT1350 was released in October of 1991. This model improved the board layout allowing for a more compact card, and it completely eliminated the C/MS chips. This model improved on its predecessor by adding auto-init to DMA allowing the card to play continuously without the crackling or pausing that was experienced on the original. The sample rate for digital audio on this card was increased to 44kHz for playback and to 15kHz for recording. With this card, a DSP upgrade was made available to owners of the original Sound Blaster, which was required for full compatibility with the Windows 3 multimedia extensions.Creative was growing quickly, achieving an estimated 72% market share of the sound card market globally in 1992, but it was also facing significant competition. Media Visionâ€™s Pro Audio Spectrum Plus, released in 1991, was capable of 8bit digital sampling and 16bit digital audio playback. It featured a CD-ROM interface, and it was Sound Blaster compatible. The Pro Audio Spectrum 16 of 1992 moved the company to 16bit ISA, added 16bit stereo digital audio, and featured stereo FM synthesis while maintaining full Sound Blaster compatibility. Then, there was Aztech in the more low-end market making some serious OEM deals with likes of Dell and Compaq. They entered the market in 1992 at a much lower price point and offered broad compatibility with sound cards like the Adlib, Sound Blaster 2, Sound Blaster Pro, Cover Speech Thing, Disney Sound Source, and Windows Sound System.To answer the competition and maintain their lead, the company released the Sound Blaster 16, CT1740, in June of 1992. This was a fully 16bit sound card and featured support for 16bit 44.1kHz digital audio. Creative had partnered with E-mu Systems to offer the Wave Blaster daughter board that brought wavetable synthesis to card through the header on the top of the card. The empty socket seen on the SB16 was for the Creative Signal Processor, CT1748, which brought hardware-assisted speech synthesis, QSound audio spatialization for digital wave playback, and PCM audio compression/decompression. The SB16 was more popular than any card before it, and the wavetable daughter board was popular enough to push Creative to acquire E-mu in March of 1993 for $54 million.Creative went public in August of 1992 and became the first Singaporean company to be listed on the NASDAQ. In September of 1992, Creative expanded into China establishing a joint venture in Beijing called Chuang Tong Multimedia Computer Ltd. Creative held 70%, NewStone 20%, and Da Heng 10%. In addition to selling the companyâ€™s multimedia hardware, the Chinese subsidiary developed and distributed CD-ROM software in the Chinese language, and sold PJS and PJ Views. The following year, Ed Esber, formerly of Ashton-Tate, joined Creative Labs as CEO, and he assembled a team that included Rich Buchanan, Gail Pomerantz, and Rich Sorkin. Of the new team in the USA, Sorkin had the most lasting impact. He began licensing programs, shortened product development cycles, and began legal endeavors to protect Creativeâ€™s intellectual property. Throughout 1993, Creative established itself Australia, Japan, the UK, and Ireland. Finally, that same year, Creative acquired ShareVision Technology who made videoconferencing technologies. Creativeâ€™s later attempts in that market didnâ€™t make it far.Creative AWE32 advertisementBy 1994, the Sound Blaster 16 was the audio card. The company needed both a low-end product and high-end product, and so the ViBRA 16, CT2501, took the low, and the AWE32 took the high. The ViBRA was a cost-reduced, single-chip implementation of the SB16 and was frequently supplied to OEMs. Some ViBRA models included an on-board modem. The AWE32 featured the CT1748 CSP, CT1747A with OPL3 FM synth, CT1971 (EMU8000) and CT1972 (EM8011, 1MB sample ROM) wavetable synth, CT1745A mixer, CT1741 DSP, a CD-ROM interface, wavetable header, SPDIF header, and 512K of sample RAM upgradeable to 28MB via two 30-pin SIMM slots. The AWE32, CT3900, was a full-length, 16bit, ISA card. With the SB16, ViBRA, and AWE32 on the market, the companyâ€™s revenues exceeded $650 million, and the company was listed on the Singapore stock exchange.On the 26th of October in 1994, in time for the Christmas shopping season, Creative released the 3DO Blaster. This brought 3DO games to the PC via a full-length, 16bit, ISA card. On the card was a 32bit RISC CPU, a DSP for CD audio, two graphics processors, 2MB of RAM, 1MB of ROM, 1MB of VRAM, and 32K SRAM with battery backup. The box contained two games (Shockwave, Gridders), some demos, drivers, Aldus Photostyler and Gallery Effects, a controller, manuals, the card itself, and a registration card. Of course, the 3DO blaster was not, itself, a standard VGA card. To use the 3DO Blaster, oneâ€™s PC would need to be at least a 25MHz Intel 386, have at least 4MB of RAM, a VGA card, Windows 3.1, a CD-ROM drive (either Matsushita or Creative CR-564), a Sound Blaster, and some speakers. The press release from 3DO read, in part:With the introduction of 3DO Blaster, Creative is targeting their extensive installed base of CD-ROM users. 3DO Blaster provides PC owners with the ultimate game platform â€” exciting 3DO games recognized for unprecedented interactive realism, full-motion video, CD-quality audio and three-dimensional sound effects.â€œTodayâ€™s announcement reflects the efforts of two of the most advanced technology suppliers, Creative Technology and 3DO. The 3DO Blaster provides the advantage of Creativeâ€™s and 3DOâ€™s innovation to the installed base of PCâ€™s already using Creative multimedia products,â€ said Sim Wong Hoo, CEO and chairman of Creative Technology Ltd.â€œCreativeâ€™s and 3DOâ€™s technologies create an advanced entertainment platform which will enhance the capabilities of PCs, and expand the imagination of users by providing them access to exciting, interactive products that fully exploit the potential of multimedia entertainment.â€Trip Hawkins, president and CEO of The 3DO Company, said todayâ€™s announcement enables his company to expand quickly and aggressively into the vast PC market. â€œCreative is the leading supplier of multimedia products for PCs, providing us with the opportunity to deliver 3DOâ€™s advanced interactive technology to an even broader audience,â€ said Hawkins.Given that the 3DO Blaster cost $399.95 and the 3DO console didnâ€™t do too well, this product was moribund from the start. Also in October of 1994, Creative released HansVision. This was a Chinese-language office suite for Windows, and while Windows replaced PJS, HansVision replaced PJ Views. Also in 1994, Creative acquired Digicom Systems, a modem company. This resulted in the Creative Phone Blaster in 1995. The Phone Blaster, CT3110, was largely just a ViBRA 16 with an integrated modem and a wavetable header, but it was a full-length, 16bit, ISA card. It faired better than the companyâ€™s attempts at video conferencing, but it wasnâ€™t much of a success.A cost reduced version of the AWE32 was released in 1995 as the Sound Blaster 32. It was roughly equivalent to the AWE32 but lacked the on-board RAM, Wave Blaster Support, and CSP. Additionally, it utilized the CQM chip from the ViBRA instead of the OPL3. The CQM (Creative Quadratic Modulation) commonly suffered audio clipping, hiss, and ringing when playing digital audio.Esber, Buchanan, and Pomerantz left the company in 1995. Theyâ€™d never really got on with the folks in Singapore, and the two groups had disagreements over the companyâ€™s strategy. Sorkin, however, was promoted to General Manager of the audio division, and then to executive VP of business development and corporate investments.With the earlier success of the companyâ€™s CD-ROM and sound card bundles packing Matsushita, Mitsumi, and other vendorsâ€™ drives, Creative had gone into the CD-ROM drive business. In 1995, the industry had a large oversupply and Creative dumped its inventory incurring a loss of $30 million, and causing the companyâ€™s share price to drop nearly 75%.Creative 3D Blaster, image from wiki.peterhuman.netCreative 3D Blaster VLBIn 1995, Creative released the 3D Blaster, CT6200. This was a 3D accelerator card built around the 3DLabs GLINT 300SX processor. The GLINT 300SX was built of about a million transistors on IBMâ€™s 3.3V, 0.5 micron process, and it was capable of about 2.5 billion operations per second. As with many cards that would follow, GLINT was designed to process Gouraud-shaded, Z-buffered, dithered triangles that were generated by an application or game and passed to GLINT via the OpenGL API (in this case CGL, and later DirectX). The chip was accompanied by 2MB (or 4MB with the 2MB daughter board) of DRAM, and this VESA Local Bus card achieved a pixel filtrate of 25MP/s. The card cost $349.95 at launch and it only handled 3D, requiring the user to have a 2D card installed and use VGA passthrough. This was roughly a year before the first Voodoo card, but shortly after the Diamond Edge 3D with an NV1 at $299 for 2MB. Given that this was a VLB card, the 3D Blaster was largely a card for 486 machines, and given the price, it didnâ€™t sell well. As far as I am aware, there were roughly 13 game titles to support CGL. Of those, there was Rebel Moon which was exclusive to the CT6200, and even having been designed exclusively for this card, it wasnâ€™t great. Frame rates would get quite sluggish at times, likely having been hampered by the 486 at the heart of VLB machines.Creative Sound Blaster AWE64 ValueCreative Sound Blaster AWE64 GoldThe Sound Blaster AWE64 was released in November of 1996, and it improved on the AWE32 in a few ways. First, it increased the signal to noise ratio (especially in the Gold version), and increased component integration resulting in traces that likewise avoided noise. Given increases in integration, the board also became smaller than its predecessor and decreased cost. Itâ€™s also notable that with general technological advancements made in the industry, the ICs were of a consistently higher quality than those used in earlier cards despite being less expensive. The card came in two versions; one was the standard version which later was re-branded as the Value version (CT4500) with 512K RAM, and the other was the Gold version (CT4390) with 4MB of RAM, a 20bit DAC, and separate SPDIF output. Functionally, there were two major differences between the AWE64 and AWE32. The AWE64 added WaveGuide which synthesizes instrument sounds. While the Wave Blaster is no longer supported, the AWE64 Gold does have line inputs on the rear for an external Sound Canvas or similar product. Effectively, the WaveGuide feature allowed for greater polyphony through the use of 32 extra software-emulated channels, but in practice this used more CPU time and wasnâ€™t very popular. The other change was the removal of 30-pin SIMM slots in favor of proprietary memory daughter boards. In all other respects, the AWE64 was simply a better AWE32. For purists, the AWE64 lacks Sound Blaster Pro compatibility and genuine OPL3 FM Synthesis, but for those who want SB16 compatibility, mostly noise-free output, hassle-free plug-n-play, and General MIDI capabilities, the AWE64 is wonderful. For collectors today, however, owning a genuine AWE64 Gold will set a buyer back between $200 and $400. That price will increase for those desiring a SIMMConn (replacing the proprietary memory daughter board with a 30-pin SIMM adapter).Creative closed 1996 with $1.6 billion in revenues, and Sorkin left the company for Elon Muskâ€™s Zip2.Media Vision will get its own article at some point, but the company collapsed in a scandal, and Aureal Semiconductor was formed on the 9th of November in 1995 out of the prior companyâ€™s remnants. On the 14th of July in 1997, Aureal announced the Vortex AU8820 with high quality positional audio via the companyâ€™s A3D technology. This allowed a human listener to perceive audio as coming from a rather precise location, and it had originally been developed by Crystal River Engineering for NASAâ€™s Virtual Environment Workstation Project. Crystal River had been acquired by Aureal in May of 1996, and Aureal productized the technology. The Vortex proved to be extremely popular and its features were supported by many of the most popular gaming titles of the time: Half-Life, Unreal, Quake II, and so on.For Creative, the arrival of the Vortex card was existential. Most of the companyâ€™s revenues came from sound cards, and the Vortex had gained the respect of gamers and audiophiles almost immediately following its release. What was worse was that its feature set was being incorporated into games where once the Sound Blaster had been the de facto standard. The fastest way to gain expertise is to buy it, and Creative bought Ensoniq in January of 1998 for $77 million. Within Creative, Ensoniq was merged with E-mu Systems. The acquisition brought the Ensoniq AudioPCI into Creative, and this was a card intended to be cheap, functional, and feature rich. It supported digital effects such as reverb, chorus, and spatial enhancement, as well as DirectSound3D, and sample-based synthesis. For the new owner, the card couldnâ€™t have been better as it support Sound Blaster compatibility through the use of a TSR despite being a PCI card. This card was rebranded several times as the Sound Blaster PCI 64, PCI 128, Vibra PCI and so on. The Ensoniq ES1370 that powered the card became the Creative 5507, and then revised into further AC97 variants. A major downside of the card was that it ran with a 44kHz sample rate only, and thus, audio recorded at any other rate was resampled which lowered fidelity and increased CPU time. The later AC97 variants supported only 48kHz natively, and therefore likewise resampled audio. While the AudioPCI wouldnâ€™t win over audiophiles, its low cost moved units and won the company some OEM deals.3DBlaster Voodoo2, image from thunder.netOn the 20th of January in 1998, Creative chose to remedy the mistake it had made with their first 3D accelerator, and they released the CT6670, or 3D Blaster Voodoo2. It used the PCI bus, had 8MB of 25ns EDO RAM, and like all Voodoos, supported Glide. In September the same year, the company released the 3D Blaster Voodoo Banshee AGP card (CT6750) as well as the CT6760 PCI card. Depending upon the SKU, these could come with 8MB, 12MB or 16MB of SDRAM. While using the same name, the AGP card was designed entirely by Creative, and it was the only Creative board using a 3dfx chip to be so.In July of 1998, Creative proved to be a leader in a different market segment with the introduction of HansVision Future 2000 in schools around Singapore. HVF2K featured the HansWord word processor, the HansBrowser bidirectional English-Chinese dictionary, and the HanSight online translator of webpages. Creative had successfully implemented productivity tools on the web, and theyâ€™d done machine translation of the web. Truly outstanding for the time.Beginning in 1997, Creative Labs optical drive bundles began featuring DVD drives and speaker sets (thanks to the acquisition of Cambridge SoundWorks), and on the 10th of March in 1998 these products dropped in price rather dramatically and were expanded in their contents. One example, the Creative Components 700 (the most expensive on offer) included Creativeâ€™s PC-DVDx2 drive, Sound Blaster AWE64, the new Graphics Blaster Exxtreme (PCI, 3DLabs Permedia 2 chip, 4MB SGRAM, 64bit data path, OpenGL, up to 1600 by 1200, 60Hz to 150Hz refresh), Creative MPEG-2/Dolby Digital decoder board, and Cambridge SoundWorksâ€™ PCWorks speaker system. This was priced at $479.99. The DVD-ROM drive was $149.99 stand-alone, and the decoder board was $169.99 stand-alone.Sound Blaster Live!In August of 1998, Creative released the Sound Blaster Live! (CT4670) as a successor to the ViBRA range of sound cards. These were built around the EMU10K1 chip and supported DirectSound3D, EAX (Environmental Audio Extensions) versions 1 and 2, and featured an onboard, 64-voice, wavetable synthesizer though it did use main memory for sample storage. This was a PCI bus card, and it utilized Ensoniqâ€™s TSR for the emulation of Adlib, Sound Blaster, and General MIDI (the adaptation of that TSR was a condition of the acquisition of Ensoniq).EMU10K1 Digital Audio Processor652KB âˆ™ PDF fileDownloadEMU10K1 Digital Audio Processor, from IEEEDownload1998 was a year of intense litigation for Creative. The first suit was filed by Creative against Aureal over MIDI caching patent infringements. This was followed by a counter claim of defamation and unfair competition by Aureal against Creative. Creativeâ€™s advertising of the Sound Blaster Live! then sparked more lawsuits by Aureal against Creative over claimed falsehoods. By the end of 1999, Aureal had won but had gone bankrupt as a result of legal costs. I am sure it cut quite deeply, but Creative acquired Aureal in September of 2000 for $32 million.After 3dfx acquired STB, they began making their own cards. As a result, Creative began making, mostly, Nvidia-based cards for video and graphics. There were some exceptions. The Creative 3D Blaster Savage 4 obviously used the S3 Savage 4 chipset, and the Graphics Blaster Exxtreme used chips from 3DLabs. Possibly to prevent the sort of problem theyâ€™d had with 3dfx, Creative then acquired 3DLabs in June of 2002. From 1999 onward, Creative would release a handful of graphics cards, some did well and others didnâ€™t, but they were no longer a substantial source of revenue for the company.Creative Nomad in box, image from peterhuman.netCreative Nomad, box, back, image from peterhuman.netCreative had some great timing with one particular product. WinAmp brought MP3 support to the desktop in 1997, and Windows Media Player 5.2 gained MP3 support in 1998. Creative released the NOMAD MP3 player in April of 1999 for $429. In June of 1999, Napster was born, and MP3s exploded in popularity. The NOMAD connected to a userâ€™s PC via a cradle, and that cradle attached to the PC via parallel port. The device had either 32MB or 64MB of battery backed RAM depending upon the model purchased, with more storage provided by flash media. The NOMAD also provided an FM tuner for those who wished to listen to radio, and a microphone for voice recordings. On the PC side of things, Creative provided both a CD ripper and the NOMAD Manager. The latter of which was for handling the transfer of content to the device. The box proudly claims that 64MB would provide an hour of CD-quality audio, and thatâ€™sâ€¦ wellâ€¦ not true at all. MP3 encoding is quite lossy, and to compress 700MB of lossless CD audio into 64MB infers an incredibly low sample rate. An hour of audio in 64MB would absolutely not be â€œCD-quality.â€ Marketing aside, the NOMAD was a cool product.nomadworld.com as of the 9th of October in 1999The NOMAD II launched the following year, and it was well received by the press. This time, Creative used USB 1.1 instead of parallel, 32MB of internal memory, bundled 64MB Smart Media flash, and added EAX support, WMA support, a backlight for the LCD, a wired remote for controls, and slightly better microphone for voice recording. This was followed by the IIc which removed the FM tuner and offered either 64MB or 128MB of internal memory.Creative released two further units in 2000, the NOMAD Jukebox and the NOMAD II MG. These also used USB. The II MG returned to the format of the original NOMAD, but it added equalizer presets, ID3-tag support, the wired remote, and the FM tuner returned and now featured. a sleep timer and recording. The NOMAD Jukebox was different. It was roughly the size and shape of a Discman, though slightly thicker, and had a 2.5 inch, 6GB, IDE hard disk in it. The Jukebox also had WAV support, line-in for recording, and two line-out jacks for four speaker systems like Creativeâ€™s own Cambridge SoundWorks four point surround. If NiMH batteries were being used, the Jukebox featured a DC jack, and it could charge those batteries. Given the use of spinning rust, battery life was just four hours. For adventurous folks today, the hard disk in this is upgradeable, but the disk didnâ€™t have any identifiable partitions or formatting, and as a result the first 32MB need to be copied with something like dd and then the drive inserted into the Jukebox and the format function used by holding the Play and Stop buttons (or EAX and Down on newer units) during the â€œloadingâ€ sequence.Following the 2001 crash, Creative became an increasingly audio-only company. Some Chinese/English, electronic, pocketable dictionaries would continue in Asia, but most of Creativeâ€™s other endeavors ceased. The company was focused on speakers, headphones, sound cards, and portable music players.US patent 6928433 was awarded to Creative on the 9th of August in 2005 for the user interface of the Zen and NOMAD Jukebox MP3 Players. This patent had been applied for on the 5th of January in 2001. Creative filed suit against Apple in May of 2006, and the two companies reached a settlement in August with Apple agreeing to pay $100 million.Time wasnâ€™t kind to Creative. Motherboard audio had become good enough for most people, and fewer than a quarter of desktop users bought dedicated sound cards. Worse, the shift to laptops during the first decade of the new millennium meant that a majority of PC users couldnâ€™t make use of a sound card. Creative voluntarily delisted from the NASDAQ with the last day of trading having been the 31st of August in 2007. The company continued to be listed on SGX-ST. Layoffs of some staff in Stillwater, Oklahoma followed in 2008.In 2009, 3DLabs and Creativeâ€™s Personal Digital Entertainment divisions were combined and reformed as ZiiLABS. This division designed a series of semi-custom ARM chips with 24 to 96 processing units called StemCells. These StemCells were sort of DSPs, and video, audio, and 3D graphics tasks were handled by these coprocessors. ZiiLABS produced at least five SKUs: ZMS-05, ZMS-08, ZMS-20, ZMS-40, and ZMS-50. On the 19th of November in 2012, Creative announced that theyâ€™d licensed ZiiLABS technology and patents to Intel for $20 million, and they sold engineering resources and assets to Intel for $30 million. Creative stated in the announcement that theyâ€™d retained the patents themselves. The ZiiLABS website was online through 2023, but it later went dormant with a default tomcat page in 2024, and the domain is no longer active. From 2012 forward, the website hadnâ€™t been updated.Today, Creative is led by Freddy Sim (Sim Wong Hooâ€™s brother), Tan Jok Jin is the executive chairman, and Ng Kai Wa is vice chairman. The companyâ€™s 2024 net sales stood at $62.8 million (12% increase over 2023) with $59.4 million of that being due to audio, speakers, and headphones. The company reported a net loss of $11 million for 2024, down from $17 million in 2023. The company continues to sell Sound Blaster products including both internal and external sound cards, DACs, and amplifiers. Their speakers, headphones, and headsets sell well and have won the company some awards.Creative rose to dominate the sound card market at a time when there werenâ€™t many options. They made an excellent product, marketed well, and made solid relationships with software makers. The primary issue for the company was that their entire business was built around a single product category, and their attempts to break out of that category werenâ€™t successful. With video cards, they were right on time with a decent product, but the Voodoo was superior. They pivoted and survived that transition only to have 3dfx abandon board partners. They then moved to MP3 players, saw some success, but were beaten by Apple. Today, the company continues in the same niche they once dominated, and they continue to make excellent sound cards. They are simply a much smaller company. Among retro-tech enthusiasts, however, the Sound Blaster 16, Pro, and AWE64 continue to have loyal fans.My dear readers, many of you worked at, ran, or even founded the companies I cover here on ARF, and some of you were present at those companies for the time periods I cover. A few of you have been mentioned by name. All corrections to the record are sincerely welcome, and I would love any additional insights, corrections, or feedback. Please feel free to leave a comment.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The MacBook has a sensor that knows the exact angle of the screen hinge]]></title>
            <link>https://twitter.com/samhenrigold/status/1964428927159382261</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158968</guid>
            <description><![CDATA[Something went wrong, but donâ€™t fret â€” letâ€™s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but donâ€™t fret â€” letâ€™s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I am giving up on Intel and have bought an AMD Ryzen 9950X3D]]></title>
            <link>https://michael.stapelberg.ch/posts/2025-09-07-bye-intel-hi-amd-9950x3d/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155986</guid>
            <description><![CDATA[The Intel 285K CPU in my high-end 2025 Linux PC died again! ðŸ˜¡ Notably, this was the replacement CPU for the original 285K that died in March, and after reading through the reviews of Intel CPUs on my electronics store of choice, many of which (!) mention CPU replacements, I am getting the impression that Intelâ€™s current CPUs just are not stable ðŸ˜ž. Therefore, I am giving up on Intel for the coming years and have bought an AMD Ryzen 9950X3D CPU instead.]]></description>
            <content:encoded><![CDATA[
  
  
    Table of contents
    
  
    What happened? Or: the batch job of death
    No, it wasnâ€™t the heat wave
    Which AMD CPU to buy?
    Performance
    Power consumption
    Conclusion
  

  
  The Intel 285K CPU in my high-end 2025 Linux
PC died again! ðŸ˜¡ Notably,
this was the replacement CPU for the original 285K that died in
March, and
after reading through the reviews of Intel CPUs on my electronics store of
choice, many of which (!) mention CPU replacements, I am getting the impression
that Intelâ€™s current CPUs just are not stable ðŸ˜ž. Therefore, I am giving up on
Intel for the coming years and have bought an AMD Ryzen 9950X3D CPU instead.
What happened? Or: the batch job of death
On the 9th of July, I set out to experiment with
layout-parser and
tesseract in order to
convert a collection of scanned paper documents from images into text.
I expected that offloading this task to the GPU would result in a drastic
speed-up, so I attempted to build layout-parser with
CUDA. Usually, itâ€™s not required to
compile software yourself on NixOS, but CUDA is non-free,
so the default NixOS cache does not compile software with CUDA. (Tip: Enable the
Nix Community Cache, which contains prebuilt
CUDA packages, too!)
This lengthy compilation attempt failed with a weird symptom: I left for work,
and after a while, my PC was no longer reachable over the network, but fans kept
spinning at 100%! ðŸ˜³ At first, IÂ suspected a Linux
bug, but now I am thinking this was
the first sign of the CPU being unreliable.
When the CUDA build failed, I ran the batch job without GPU offloading
instead. It took about 4 hours and consumed roughly 300W constantly. You can see
it on this CPU usage graph:


































On the evening of the 9th, the computer still seemed to work fine.
But the next day, when I wanted to wake up my PC from suspend-to-RAM as usual,
it wouldnâ€™t wake up. Worse, even after removing the power cord and waiting a few
seconds, there was no reaction to pressing the power button.
Later, I diagnosed the problem to either the mainboard and/or the CPU. The Power
Supply, RAM and disk all work with different hardware. I ended up returning both
the CPU and the mainboard, as I couldnâ€™t further diagnose which of the two is
broken.
To be clear: I am not saying the batch job killed the CPU. The computer was
acting strangely in the morning already. But the batch job might have been what
really sealed the deal.
No, it wasnâ€™t the heat wave
Tomâ€™s Hardware recently
reported
that â€œIntel Raptor Lake crashes are increasing with rising temperatures in
record European heat waveâ€, which prompted some folks to blame Europeâ€™s general
lack of Air Conditioning.
But in this case, I actually did air-condition the room about half-way
through the job (at about 16:00), when I noticed the room was getting
hot. Hereâ€™s the temperature graph:















I would say that 25 to 28 degrees celsius are normal temperatures for computers.
I also double-checked if the CPU temperature of about 100 degrees celsius is too
high, but no: this Tomâ€™s Hardware
article
shows even higher temperatures, and Intel specifies a maximum of 110
degrees. So, running at â€œonlyâ€ 100 degrees for a few hours should be fine.
Lastly, even if Intel CPUs were prone to crashing under high heat, they should
never die.
Which AMD CPU to buy?
I wanted the fastest AMD CPU (for desktops, not for servers), which currently is
the Ryzen 9 9950X, but there is also the Ryzen 9 9950X3D, a variant with 3D
V-Cache. Depending on the use-case, the variant with or without 3D V-Cache is
faster, see the comparison on
Phoronix.
Ultimately, I decided for the 9950X3D model, not just because it performs better
in many of the benchmarks, but also because Linux 6.13 and newer let you
control whether to prefer the CPU cores with larger V-Cache or higher
frequency,
which sounds like an interesting capability: By changing this setting, maybe one
can see how sensitive certain workloads are to extra cache.
Aside from the CPU, I also needed a new mainboard (for AMDâ€™s socket AM5), but I
kept all the other components. I ended up selecting the ASUS TUF
X870+
mainboard. I usually look for low power usage in a mainboard, so I made sure to
go with an X870 mainboard instead of an X870E one, because the X870E has two
chipsets (both of which consume power and need cooling)! Given the context of
this hardware replacement, I also like the TUF lineâ€™s focus on enduranceâ€¦
Performance
The performance of the AMD 9950X3D seems to be slightly better than the Intel
285K:

  
      
          Workload
          12900K (2022)
          285K (2025)
          9950X3D (2025)
      
  
  
      
          build Go 1.24.3
          â‰ˆ35s
          â‰ˆ26s
          â‰ˆ24s
      
      
          gokrazy/rsync tests
          â‰ˆ0.5s
          â‰ˆ0.4s
          â‰ˆ0.5s
      
      
          gokrazy Linux compile
          3m 13s
          2m 7s
          1m 56s
      
  

In case youâ€™re curious, the commands used for each workload are:

cd src; ./make.bash
make test
gokr-rebuild-kernel -cross=arm64

(I have not included the gokrazy UEFI integration tests because I think there is
an unrelated difference that prevents comparison of my old results with how the
test runs currently.)
Power consumption
In my high-end 2025 Linux PC I
explained that I chose the Intel 285K CPU for its lower idle power consumption,
and some folks were skeptical if AMD CPUs are really worse in that regard.
Having switched between 3 different PCs, but with identical peripherals, I can
now answer the question of how the top CPUs differ in power consumption!
I picked a few representative point-in-time power values from a couple of days
of usage:

  
      
          CPU
          Mainboard
          idle power
          idle power with monitor
      
  
  
      
          Intel 12900k
          ASUS PRIME Z690-A
          40W
          60W
      
      
          Intel 285k
          ASUS PRIME Z890-P
          46W
          65W
      
      
          AMD 9950X3D
          ASUS TUF GAMING X870-PLUS WIFI
          55W
          80W
      
  

Looking at two typical evenings, here is the power consumption of the Intel 285K:















â€¦and here is the same PC setup, but with the AMD 9950X3D:















I get the general impression that the AMD CPU has higher power consumption in
all regards: the baseline is higher, the spikes are higher (peak consumption)
and it spikes more often / for longer.
Looking at my energy meter statistics, I usually ended up at about 9.x kWh per
day for a two-person household, cooking with induction.
After switching my PC from Intel to AMD, I end up at 10-11 kWh per day.
Conclusion
I started buying Intel CPUs because they allowed me to build high-performance
computers that ran Linux flawlessly and produced little noise. This formula
worked for me over many years:

Back in 2008, I bought a mobile Intel CPU in a desktop case (article in
German).
Then, in 2012, I could just buy a regular Intel CPU (i7-2600K) for my Linux
PC, because they had gotten so
much better in terms of power saving.
Over the years, I bought an i7-8700K, and later an i9-9900K.
The last time this formula worked out for me was with my 2022 high-end Linux
PC.

On the one hand, Iâ€™m a little sad that this era has ended. On the other hand, I
have had a soft spot for AMD since I had one of their K6 CPUs in one of my early
PCs and in fact, I have never stopped buying AMD CPUs (e.g. for my Ryzen
7-based Mini
Server).
Maybe AMD could further improve their idle power usage in upcoming models? And,
if Intel survives for long enough, maybe they succeed at stabilizing their CPU
designs again? I certainly would love to see some competition in the CPU market.

  
    Did you like this
    post? Subscribe to this
      blogâ€™s RSS feed to not miss any new posts!
  
  
    I run a blog since 2005, spreading knowledge and experience for over 20 years! :)
  
  
    If you want to support my work, you
    can buy me a coffee.
  
  
    Thank you for your support! â¤ï¸
  


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GPT-5 Thinking in ChatGPT (a.k.a. Research Goblin) is good at search]]></title>
            <link>https://simonwillison.net/2025/Sep/6/research-goblin/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152284</guid>
            <description><![CDATA[â€œDonâ€™t use chatbots as search enginesâ€ was great advice for several years... until it wasnâ€™t. I wrote about how good OpenAIâ€™s o3 was at using its Bing-backed search tool back â€¦]]></description>
            <content:encoded><![CDATA[

6th September 2025



â€œDonâ€™t use chatbots as search enginesâ€ was great advice for several years... until it wasnâ€™t.
I wrote about how good OpenAIâ€™s o3 was at using its Bing-backed search tool back in April. GPT-5 feels even better.
Iâ€™ve started calling it my Research Goblin. I can assign a task to it, no matter how trivial or complex, and it will do an often unreasonable amount of work to search the internet and figure out an answer.
This is excellent for satisfying curiosity, and occasionally useful for more important endeavors as well.
I always run my searches by selecting the â€œGPT-5 Thinkingâ€ model from the model pickerâ€”in my experience this leads to far more comprehensive (albeit much slower) results.
Here are some examples from just the last couple of days. Every single one of them was run on my phone, usually while I was doing something else. Most of them were dictated using the iPhone voice keyboard, which I find faster than typing. Plus, itâ€™s fun to talk to my Research Goblin.


  Bouncy travelators
  Identify this building
  Starbucks UK cake pops
  Britannica to seed Wikipedia
  Official name for the University of Cambridge
  History of the caverns in Exeter quay
  Aldi vs Lidl
  AI labs scanning books for training data
  GPT-5 for search feels competent
  Tips for using search in ChatGPT


Bouncy travelators

They used to be rubber bouncy travelators at Heathrow and they were really fun, have all been replaced by metal ones now and if so, when did that happen?

I was traveling through Heathrow airport pondering what had happened to the fun bouncy rubber travelators.
Hereâ€™s what I got. Research Goblin narrowed it down to some time between 2014-2018 but, more importantly, found me this delightful 2024 article by Peter Hartlaub in the San Francisco Chronicle with a history of the SFO bouncy walkways, now also sadly retired.
Identify this building


Identify this building in reading

This is a photo I snapped out of the window on the train. It thought for 1m4s and correctly identified it as The Blade.
Starbucks UK cake pops

Starbucks in the UK donâ€™t sell cake pops! Do a deep investigative dive

The Starbucks in Exeter railway station didnâ€™t have cake pops, and the lady I asked didnâ€™t know what they were.
Hereâ€™s the result. It turns out Starbucks did launch cake pops in the UK in September 2023 but they arenâ€™t available at all outlets, in particular the licensed travel locations such as the one at Exeter St Davids station.
I particularly enjoyed how it established definitive proof by consulting the nutrition and allergen guide PDF on starbucks.co.uk, which does indeed list both the Birthday Cake Pop (my favourite) and the Cookies and Cream one (apparently discontinued in the USA, at least according to r/starbucks).
Britannica to seed Wikipedia

Someone on hacker News said:
> I was looking at another thread about how Wikipedia was the best thing on the internet. But they only got the head start by taking copy of Encyclopedia Britannica and everything else
Find what they meant by that

The result. It turns out Wikipedia did seed itself with content from the out-of-copyright 1911 EncyclopÃ¦dia Britannica... but that project took place in 2006, five years after Wikipedia first launched in 2001.
I asked:

What is the single best article I can link somebody to that explains the 1911 Britannica thing

And it pointed me to Wikipedia:WikiProject Encyclopaedia Britannica which includes a detailed explanation and a link to the 13,000 pages still tagged with the template from that project. I posted what I found in a comment.
Notably (for me anyway) I didnâ€™t feel the need to disclose my use of ChatGPT in finding that informationâ€”at this point that feels a little like disclosing that I ran a Google search.
Official name for the University of Cambridge

What is the official legal name of the university of Cambridge?

Hereâ€™s the context for that one. It thought for 19 secondsâ€”the thinking trace reveals it knew the answer but wanted to confirm it. It answered:

â€œThe Chancellor, Masters, and Scholars of the University of Cambridge.â€ University of Cambridge, Cambridge University Press & Assessment

That first link gave me the citation I needed in order to be sure this was right.
Since this is my shortest example, hereâ€™s a screenshot of the expanded â€œThought for 19sâ€ panel. I always expand the thoughtsâ€”seeing how it pulled together its answer is crucial for evaluating if the answer is likely to be useful or not.

History of the caverns in Exeter quay

Research On the waterfront restaurant in Exeter, is it dug into the cliffs somehow? History of the building, who built it, why and how

We were out to dinner here and noticed that the interior of the restaurant appeared to be a space dug into the cliff, which piqued my interest.
This was the ChatGPT session that inspired the Research Goblin nickname. It just kept on digging!
The first reply took 2m40s and confirmed that yes, these quay buildings were carved into the red sandstone cliff in the 1820s-1830s.
ChatGPT with GPT-5 really likes to suggest additional steps it can take. In this case:

If youâ€™d like, I can dig up the exact Historic England entry that covers the â€œSouthern Warehouseâ€ address and overlay it on a map of the vaults.

I often say â€œyesâ€ purely out of curiosity to see what it will do next, and the offer to â€œoverlay it on a mapâ€ was irresistible, like how would it even do that?
It did a ton of extra searches, found latitude and longitude coordinates for the restaurant (from Wikimedia Commons) and the warehouse buildings (from National Heritage List for England via Wikipedia), showed me that data in a table and then used Python to render this image:

Yeah, so that one is a bit of a miss!
But... the underlying research chain, as always, provided fascinating. I noticed that at one point it had searched for â€œExeter Quay vaults map PDFâ€, which inspired my next question:

Did you find any PDF diagrams, showing the tunnels into the cliff

2m20s later it had found a planning document and a letting brochure, both PDFs, but noted that neither of them had the diagram I wanted. Evidently it actually â€œreadsâ€ the PDFs it finds as part of its activities, since it appears to have knowledge of the images contained within them as well as the text.
It concluded with this intriguing note:

Thereâ€™s strong evidence that a detailed plan/section does exist in the Exeter Archaeology report often cited by plannersâ€”â€œArchaeological, Historical and Conservation Study of the Exeter Canal Basinâ€ (Exeter Archaeology Report 00.18, 2000)â€”but it isnâ€™t online; itâ€™s just cited in planning documents. Youâ€™d likely need to request it via the Devon Heritage Centre/Historic Environment Record.
If you want, I can keep hunting specialist archives for a digitized copy or a different report that includes the vault layouts.

Obviously I said â€œYes do thatâ€ and 2m24s later it chucked out a whole host of extra links, an apology that it still hadnâ€™t found a detailed plan diagram and, under the section heading â€œWhat I recommend next (actionable + exact ask)â€:

If youâ€™re happy, I can draft an email you can paste to the right archivists. [...]

Subject: Exeter Quayside cliff vaults â€“ request for plan/section drawings
Hello,
Iâ€™m researching the cliff-cut vaults behind The Quay (On The Waterfront / Kings Wharf).
Could you supply scans or copies of any plans/sections/diagrams of these vaultsâ€”especially those in Exeter Archaeology Report 00.18 (2000) (Archaeological, Historical and Conservation Study of the Exeter Canal Basin) and any EMAFU/Exeter Archaeology drawings for â€œThe Quayâ€ showing the vault/tunnel layouts cut into the red sandstone cliff?
For reference: NHLE list entries 1223045 (1â€“4 Warehouse Vaults), 1223046 (6â€“11 Warehouse Vaults), and 1223114 (14â€“25 Vaults).
Many thanks!


Itâ€™s a good thing my Research Goblin doesnâ€™t have the ability to send email (which we already know is very risky business) or at this point I wouldnâ€™t be surprised if it started autonomously bothering archivists on my behalf!
Aldi vs Lidl

In UK supermarket terms compare lidl and aldi

I saw an Aldi and a Lidl next to each other in Exeter and was curious as to how they compare. In this case my Research Goblin went way deeper than I had intended, thinking for 1m25s about market share and price surveys and loyalty schemes and answering with ten paragraphs of detailed analysis.
I replied:

Yeah, I just wanted the basic vibes like is one fancier than the other?

It thought for 6 seconds, didnâ€™t run any additional searches and replied:

Short version: neither is â€œfancyâ€ â€” both are no-frills discounters â€” but Lidl often feels a touch more polished, while Aldi feels a touch more bare-bones. [...]

Since I was now enjoying having my Goblin express judgement over British supermarket chains I said:

How do they rate in fancinews compared to all of the other major UK supermarket chains?

(That was a typo for â€œfancinessâ€ which it figured out.)
It thought for another 1m1s, ran 7 searches, consulted 27 URLs and gave me back a ranking that looked about right to me.

AI labs scanning books for training data

Anthropic bought lots of physical books and cut them up and scan them for training data. Do any other AI labs do the same thing?

Relevant to todayâ€™s big story. Research Goblin was unable to find any news stories or other evidence that any labs other than Anthropic are engaged in large scale book scanning for training data. Thatâ€™s not to say it isnâ€™t happening, but itâ€™s happening very quietly if thatâ€™s the case.
GPT-5 for search feels competent
The word that best describes how I feel about GPT-5 search is that it feels competent.
Iâ€™ve thrown all sorts of things at it over the last few weeks and it rarely disappoints me. It almost always does better than if I were to dedicate the same amount of time to manually searching myself, mainly because itâ€™s much faster at running searches and evaluating the results than I am.
I particularly love that it works so well on mobile. I used to reserve my deeper research sessions to a laptop where I could open up dozens of tabs. Iâ€™ll still do that for higher stakes activities but Iâ€™m finding the scope of curiosity satisfaction I can perform on the go with just my phone has increased quite dramatically.
Iâ€™ve mostly stopped using OpenAIâ€™s Deep Research feature, because ChatGPT search now gives me the results Iâ€™m interested in far more quickly for most queries.
As a developer who builds software on LLMs I see ChatGPT search as the gold standard for what can be achieved using tool calling combined with chain-of-thought. Techniques like RAG are massively more effective if you can reframe them as several levels of tool calling with a carefully selected set of powerful search tools.
The way that search tool integrates with reasoning is key, because it allows GPT-5 to execute a search, reason about the results and then execute follow-up searchesâ€”all as part of that initial â€œthinkingâ€ process.
Anthropic call this ability interleaved thinking and itâ€™s also supported by the OpenAI Responses API.
Tips for using search in ChatGPT
As with all things AI, GPT-5 search rewards intuition gathered through experience. Any time a curious thought pops into my head I try to catch it and throw it at my Research Goblin. If itâ€™s something Iâ€™m certain it wonâ€™t be able to handle then even better! I can learn from watching it fail.
Iâ€™ve been trying out hints like â€œgo deepâ€ which seem to trigger a more thorough research job. I enjoy throwing those at shallow and unimportant questions like the UK Starbucks cake pops one just to see what happens!
You can throw questions at it which have a single, unambiguous answerâ€”but I think questions which are broader and donâ€™t have a â€œcorrectâ€ answer can be a lot more fun. The UK supermarket rankings above are a great example of that.
Since I love a questionable analogy for LLMs Research Goblin is... well, itâ€™s a goblin. Itâ€™s very industrious, not quite human and not entirely trustworthy. You have to be able to outwit it if you want to keep it gainfully employed.


]]></content:encoded>
        </item>
    </channel>
</rss>