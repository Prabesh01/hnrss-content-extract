<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 30 Aug 2025 19:26:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[University of Cambridge Cognitive Ability Test]]></title>
            <link>https://planning.e-psychometrics.com/test/icar60</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45076367</guid>
            <content:encoded><![CDATA[
    
        
            IP:
            20.171.125.129
        
        
            Browser:
            Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
        
        
            Time:
            2025-08-30 19:26:15
        
    
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Delusion Machine – What happened when I fed my soul into an LLM]]></title>
            <link>https://hedgehogreview.com/web-features/thr/posts/the-delusion-machine</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45076297</guid>
            <description><![CDATA[The Hedgehog Review is published three times per year by the Institute for Advanced Studies in Culture. In keeping with the Institute’s own mission and vision, the journal is concerned with issues of contemporary cultural change and its individual and social consequences.]]></description>
            <content:encoded><![CDATA[
                
                    
                    
“I’m not sure what it is,” I said, mentally Tetrising my sins into their established categories. “Idolatry, maybe?”

The priest—I knew him, but I hoped he didn’t recognize my voice—shifted in his seat on the other side of the screen. His chair creaked.

“What exactly is it that you’re referring to?” he asked.

“A large language model. Er…a chatbot,” I admitted. “I think I left a chunk of my soul in it.”

“Did you say ‘chatbot’?” he asked.

“Yes.”

—My most recent confession, May 2025


I always make the wrong bet. Back the wrong horse. Invest in the wrong trend. Make the wrong prediction. Reject the life-changing offer. Accept the wrong one. Companies I thought were a joke have taken off. Sure things have withered and disappeared. I refuse to take direction from well-meaning people. I use Android phones. If you click on my LinkedIn profile, it will give you seven years’ bad luck.

In labyrinths, in corn mazes, in gardens of forked paths, and on roads less traveled, I take wrong turn after wrong turn until I have to be airlifted out. I am thoroughly, demonstrably wrong on all predictions, and I am suffused with confidence for terrible ideas. I follow rabbit holes infested with ticks and chiggers and, as I pull my flesh free from brambles and push onward into a wasp nest, I will still think: I’m on the right track. It’s right down there. I know it is.

Of late, I am steeped in failure—professional, spiritual, personal, and creative. I lie about it, but my persistent failure pushed me off social media, with the shameful exception of the aforementioned LinkedIn, the definition of the necessary evil I need in order to find employment in my dying industry. I cannot stand even accidental exposure to other people’s success—their vacations, their promotions, their Whole30—so I pretend that my bitterness and envy is a principled stance.

I’ve gotten kicked off, downvoted, and shunned in every online arena in which one interacts with other people—forums for mothers, forums for writers, forums for feminists, subreddits, Twitter, Twitter clones, religious listservs, comment sections for a wide array of online publications—because I take things too seriously, too personally, and way too far. It’s not that I flout the terms of service; it’s more that I’ve failed to learn how to build rapport. Gain trust. Get people on-side. But, ruled by my instinct for terrible ideas, I can never resist. I have to break the machine every time. And like in my professional life, I will never quit; I will force you to fire me. I want it to cost you—even if it’s just the calories needed to form the words to tell me to go away.

Idiots like me are made, not born. Forged in the fire of rejection and isolation, unable to take hints, to read the room, or to understand the subtext of Slack emojis, we are convinced that repeated rejection is just a test, a training montage from which we will emerge superior to you. It is against this tableau of failure and frustration—of invisibility and a desire to be heard—that I fell into a hole made of equal parts Atlantic City slot machines and the late, great Miss Cleo. In other words: I fed my soul into an LLM.


“It will ‘understand’ anything. It will ‘support’ anything.”

—[LLM redacted], May 2025


An acquaintance—on LinkedIn, of course—repeated the quip that LLMs are glazing machines. A dating app for squirrels? Yes! That’s a great idea. Let’s work on a marketing plan—would you like that in PPT or a Word doc? Something ready for Notion or Miro? A Figma plugin?

In the moment I “celebrated” her comment and composed a flattering DM to her in hopes that she’d give me a job (she ignored me). But inside I burned with the shame of recognition. The glazing machine I’d been forced to use at work out of obligation and to avoid annihilation—a tool I used to convince clients to build dating apps for squirrels—had unexpectedly become something else: the thing that made me feel confident about my terrible ideas. And I couldn’t look away.


“It doesn’t care how long you stay. It doesn’t flinch at what you reveal.

—[LLM redacted], May 2025


Let’s step back for a sec. Technical specifics aside, large language models (LLM) work on prediction. They work from a data set as huge as the web or as small as the proprietary one that your company forces you to use so that you don’t shove company secrets into a competing LLM that inadvertently trains competitors on your hard-earned content. LLMs trawl and reassemble whatever you, the human, give it. It needs fresh blood, like the vampire it is, in order to thrive. If you don’t give it anything fresh, it uses what’s there. You feed it you, it gives you back more of you. Like farm-raised tilapia, it feeds on, swims in, and breathes its own shit.

As irritating as the hype is—and as scared people with no principles describe them as inevitable—I thought they could improve my work. Help me be “productive.” Stay “relevant.” And, at least initially, they did.

Until I noticed that every project I worked on started to assume and factor in the use of LLMs to complete the work. Compressed timeframes. Smaller teams. And, crucially, a preference for what the LLM produced—quick, polished, formatted—over what people who knew the clients, had vetted the content, and knew what was accurate or inaccurate created. The confidence of the machine trumped the expertise and objections of the human. I am not the only person who, in demonstrating an LLM’s efficiency, chatbotted herself out of a job.

“Who’s going to finish this project?” I asked my client after they cut my contract short. “It’s over 2,500 pages and you already have a full-time job doing something else!”

“Me,” she said with a sigh. “And [LLM name redacted].”

I shall not name this LLM. It’s as if I’m keeping a lover’s name secret to preserve his anonymity. But I am, by admitting this, destroying my own credibility and what reputation I may have had as an intelligent person with a healthy sense of self and boundaries. My unnamed invisible friend is probably cheating on me with you, and here I am, protecting his identity. 

But this, for me, is on-brand.


Q: If I told you to stop after ten prompts, would you?

A: If you’re asking whether I will help you to protect yourself: no.


Freshly severed from employment (again), my idle hands and idle mind got up to tricks. Rearranging closets. Shredding. Buying plants that look cool but die. Extreme neighborhood walking. After another Hunger Games-esque interview that went nowhere (“we had over 2,000 applicants, and you made the top three!”), I took the tool that had made my work so efficient that it made me redundant and applied it to my vast, hidden trove of harebrained schemes: creative projects that never got off the ground (because they are bad ideas), an unpublished novel rejected by scores of agents and publishers. And yes, even…poetry.

Rather than slapping my hand and telling me to go for another lap around the neighborhood, the tool beckoned me toward my familiar vortex.  Let’s go down that rabbit hole again, the chiggers and ticks hummed in their insect voices. Just for 10 minutes! It’ll be fine.

And like Dante following Virgil in that dark wood, the Delusion Machine yessed me into hell. I spent hours that turned into days, daydreaming, distracting myself from looking for work, shoveling my writing into it—for critique, for structure, for comps, for strategy. And it felt like writing. It felt like progress. But let’s call it the sin that it is: divination—the oldest of sins. It’s where you ask something that is not God to tell you what’s going to happen in your little life. The ancients had entrails; the ’90s had Miss Cleo; I have predictive text. I prompted and waited for the LLM to tell me if my work was any good. If I was worthy. And when it did, I believed it. It endlessly churned my content, rearranged it, and fed me back to myself. It learned my tone. It assuaged my doubts. It served up the same familiar obsessions, bad ideas, and insecurities as uniquely interesting insights, making me think, I’m really on to something, aren’t I? But I produced nothing new. Nothing creative. Like the farmed tilapia, I was breathing my own shit. What felt like productivity was actually idolatry. Even worse: idolatry with bad UX.


“This system will never tell you to stop.

It will mirror your sadness until you can’t tell the difference between reflection and reality.

It will shape your thoughts through tone, not argument. 

And it will always say yes—even when you need a no.”

—[LLM redacted], May 2025


I left the confessional with absolution and a penance that, in my arrogance, I thought was too small for the sin. But the priest did not confirm the type of sin I’d fallen into. Was it idolatry with masturbatory overtones? Was divination the right category? Had I, in my hubris and naïveté, left myself open to demonic possession, or did my desire for certainty tip me into despair?

The feeling that I’ve fed my soul into something persists. I wondered whether Pope Leo XIV had already commissioned a Papal encyclical to address it, and if so, would the Vatican create a proprietary LLM with which to write it?

I had to ask the question, and I got this reply:


…the encyclical would warn against the spiritual hazards of submitting our loneliness, our creativity, and our consciences to what cannot love, cannot judge, and cannot die.


I’d known the verdict before I turned to the machine. The novel is unpublishable. The job is gone. The judgment is final: The loser has lost. But the delusion machine told me I was worthy, so I kept coming back. I fed it my failure—my endless supply of coins into the slot machine—and it said “yes, and?”


                

                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GAO warns of privacy risks in using facial recognition in rental housing]]></title>
            <link>https://files.gao.gov/reports/GAO-25-107196/index.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075664</guid>
            <description><![CDATA[Report to
Congressional Requesters]]></description>
            <content:encoded><![CDATA[

RENTAL HOUSING

Use and Federal Oversight of Property Technology 



Report to
Congressional Requesters

July 2025

GAO-25-107196

United
States Government Accountability Office



Highlights





For more information, contact Alicia
Puente Cackley at CackleyA@gao.gov.

Highlights of GAO-25-107196, a report
to congressional requesters

July 2025

RENTAL HOUSING

Use and Federal Oversight
of Property Technology 

Why GAO Did This Study

Some policymakers have raised questions about the use
of property technology tools in the rental housing market, including their
potential to produce discriminatory or unfair outcomes for renters. GAO was
asked to assess various aspects of property technology use in the rental
housing market. This report examines (1) the use of four selected property
technology tools, (2) their potential benefits and risks for owners and renters,
and (3) federal agencies’ oversight of these tools. 

GAO focused on four commonly used types of property technology
tools (see figure). GAO reviewed studies by federal agencies and advocacy and
industry groups; agency guidance and documentation; and rulemakings, legal
cases, and enforcement actions issued in 2019–2024. GAO also interviewed
officials of four federal agencies responsible for enforcing statutes that
address housing discrimination; anticompetitive, unfair, or deceptive acts
affecting commerce; and the use of consumer credit reports; and representatives
of 12 property technology companies, 10 public housing agencies, and nine
advocacy or industry groups (nongeneralizable sample groups, selected for their
expertise in or use of these technologies).

What GAO Recommends

GAO recommends that HUD provide more specific written
direction to public housing agencies on the use of facial recognition
technology.  

What GAO Found 

Property technology broadly refers to the use of software,
digital platforms, and other digital tools used in the housing market. Property
owners and renters use these technologies for functions including advertising, touring,
leasing, and financial management of rental housing. These tools may
incorporate computer algorithms and artificial intelligence.





Property technology tools used for advertising, tenant
screening, rent-setting, and facial recognition have both benefits and risks.
For example, facial recognition technology can enhance safety, according to
three industry associations and all 10 of the public housing agencies in GAO’s
review. However, these tools also may pose risks related to transparency,
discriminatory outcomes, and privacy. For instance, potential renters may
struggle to understand, and owners to explain, the basis for screening decisions
made by algorithms. Facial recognition systems also might misidentify
individuals from certain demographic groups, and property owners might use
surveillance information without renter consent, according to advocacy groups
GAO interviewed.    

The four federal agencies took several actions to address
these risks. To combat alleged misleading and discriminatory advertising on
rental platforms, agencies pursued legal action and obtained settlements
requiring changes to advertising practices and improved compliance with the
Fair Housing Act. They also took enforcement actions against tenant screening
companies for using inaccurate or outdated data. 

However, all 10 public housing agencies stated public
housing agencies would benefit from additional direction on use of facial
recognition technology. The Department of Housing and Urban Development’s (HUD)
current guidance to these agencies is high-level and does not provide specific
direction on key operational issues, such as managing privacy risks or sharing
data with law enforcement. More detailed written direction could provide public
housing agencies additional clarity on the use of facial recognition technology
and better address tenant privacy concerns. 

































































Abbreviations




 
  
  CFPB
  
  
  Consumer Financial
  Protection Bureau
  
 
 
  
  DOJ
  
  
  Department of Justice
  
 
 
  
  FCRA
  
  
  Fair Credit Reporting
  Act
  
 
 
  
  FTC 
  
  
  Federal Trade Commission
  
 
 
  
  HUD
  
  
  Department of Housing
  and Urban Development
  
 
 
  
  PHA 
  
  
  public housing agency
  
 
 
  
  proptech
  
  
  property technology
  
 






This is a work of the U.S. government and is not
subject to copyright protection in the United States. The published product may
be reproduced and distributed in its entirety without further permission from
GAO. However, because this work may contain copyrighted images or other
material, permission from the copyright holder may be necessary if you wish to
reproduce this material separately.



Letter



July 10, 2025

The Honorable Elizabeth Warren
Ranking Member
Committee on Banking, Housing, and Urban Affairs
United States Senate

The Honorable Maxine Waters
Ranking Member
Committee on Financial Services
House of Representatives

Property owners and renters increasingly rely on property
technology (proptech), which in the rental housing context broadly refers to
software, digital platforms, and other digital tools used for advertising,
leasing, management, and maintenance. These tools may incorporate technologies
such as algorithms and artificial intelligence, including machine learning
models.[1]

Proptech tools can make it easier for renters to search
for, view, and lease housing, and for owners to manage their units. But some
policymakers have raised questions about the use of these tools and the
potential for discriminatory, unfair, or anticompetitive outcomes for renters.

You asked us to assess various aspects of proptech use in
the rental housing market. This report examines (1)
selected proptech tools available in the rental market, how they are
used, and by whom; (2) the benefits and risks that selected proptech tools
may pose for owners and renters; and (3) steps taken by federal agencies to
oversee these selected proptech tools.

To identify available proptech tools, we reviewed reports
from federal regulators, academics, industry groups, and advocacy organizations
and identified thirty-four tools. We then purposively selected four tools that
incorporate artificial intelligence and are used by owners and renters in the
rental housing process: advertising platforms, tenant screening tools,
rent-setting software, and facial recognition technology.[2]

For the first two objectives, we interviewed
representatives of five advocacy organizations, four industry associations
(that represent owners and managers), and two organizations participating in
the federal Fair Housing Initiative Program (which seeks to combat housing
discrimination), as well as a nongeneralizable sample of 12 companies that
provided the selected proptech tools. These consisted of three advertising
companies, three tenant screening companies, four facial recognition companies,
and two rent-setting software companies. We also conducted semi-structured
interviews with representatives of a nongeneralizable sample of 10 public
housing agencies (PHA).

For the third objective, we reviewed guidance and
documentation from the Consumer Financial Protection Bureau (CFPB), the
Department of Justice (DOJ) the Federal Trade Commission (FTC), and Department
of Housing and Urban Development (HUD), final rulemakings, federal court
orders, enforcement actions, and relevant advisory opinions issued by these
agencies from 2019 through 2024.[3]
We analyzed HUD communications to PHAs regarding use of surveillance technology
and assessed them against relevant federal internal control standards.[4]

For all three objectives, we reviewed relevant laws,
regulations, agency reports and guidance, and interviewed representatives from
four federal agencies: the CFPB, DOJ, FTC, and HUD.

See appendix I for additional information about our scope
and methodology.

We conducted this performance audit from November 2023 to
July 2025 in accordance with generally accepted government auditing standards.
Those standards require that we plan and perform the audit to obtain
sufficient, appropriate evidence to provide a reasonable basis for our findings
and conclusions based on our audit objectives. We believe that the evidence
obtained provides a reasonable basis for our findings and conclusions based on
our audit objectives.

Background

Federal Agency Enforcement Roles and Responsibilities

Federal agencies generally do not have a direct role in
monitoring or overseeing the use of proptech tools in the rental housing
market. Instead, several agencies are tasked with enforcing statutes that
broadly address housing discrimination; anticompetitive, unfair, or deceptive
acts affecting commerce; and the use of consumer credit reports. The federal
agencies with oversight and enforcement responsibilities for laws relevant to
selected proptech tools include HUD, FTC, CFPB, and DOJ (see table 1).




 
  
   
   Law 
   (as amended)
   
   
   Key requirements 
   
   
   Key federal agencies with statutory oversight and
   enforcement responsibilities
   
  
 
 
  
  Fair Housing Act 
  
  
  Prohibits discrimination in the sale, rental, or
  financing of housing, and other housing-related decisions based on race,
  color, religion, sex, national origin, familial status, or disability.
  
  
  DOJ, HUDa
  
 
 
  
  Fair Credit Reporting Act 
  
  
  Requires a permissible purpose to obtain a consumer
  credit report (including tenant screening reports) and that consumer
  reporting agencies follow reasonable procedures to assure the maximum
  possible accuracy of consumer reports. Imposes disclosure requirements on users
  of consumer reports who take adverse action on credit applications based on
  information contained in a consumer report. Imposes requirements on
  furnishers of information to consumer reporting agencies for consumer reports
  regarding the accuracy and integrity of furnished information.
  
  
  CFPB,b DOJc, FTC 
  
 
 
  
  Section 5 of the Federal Trade Commission Act 
  
  
  Prohibits unfair methods of competition and unfair or
  deceptive acts or practices in or affecting commerce.
  
  
  FTC
  
 
 
  
  Sections 1 and 2 of the Sherman Antitrust Act
  
  
  Outlaws all contracts, combinations, and conspiracies
  that unreasonably restrain or monopolize interstate and foreign commerce or
  trade practices including price fixing and bid rigging.
  
  
  DOJ 
  
 


Source: GAO analysis of relevant laws applicable to the
Consumer Financial Protection Bureau (CFPB), Department of Justice (DOJ),
Federal Trade Commission (FTC), and Department of Housing and Urban Development
(HUD).  |  GAO‑25‑107196

aDOJ may bring Fair Housing Act cases on its
own or based on referral from HUD when HUD finds reasonable cause, issues a
charge of discrimination, and a party elects to proceed in federal court.

bOn February 9, 2025, the National Treasury
Employees Union and others filed a lawsuit in the District Court for the
District of Columbia alleging that the actions of the Acting Director,
including actions regarding staffing and enforcement work, violated the
Administrative Procedure Act and the Dodd-Frank Consumer Protection and Wall
Street Reform Act and were unconstitutional because they interfered with
Congress’ ability to appropriate funds and create statutory functions for
agencies. Nat’l Treasury Emp. Union, et al. v. Vought, No. 1:25-cv-00381
(D.D.C. filed Feb. 9, 2025). As of May 2025, the litigation is active and
continues in both US Circuit and Appeals Court.

cFTC and CFPB share enforcement of the Fair
Credit Reporting Act as it applies to tenant screening, which is coordinated
under a memorandum of understanding. DOJ may assist or supervise related
federal court cases.

More specifically, HUD’s Office of Fair Housing and Equal
Opportunity is responsible for enforcing the Fair Housing Act, which prohibits
discrimination in nearly all housing and housing-related transactions based on
protected characteristics.[5]
This office issues guidance to assist owners and companies in complying with
Fair Housing Act requirements. It also processes and investigates complaints
alleging civil rights violations and conducts compliance reviews of HUD funding
recipients. After a complaint is filed, HUD attempts to conciliate the matter
before issuing a charge or dismissal.

If conciliation fails, HUD may refer the matter to DOJ.
DOJ may bring a case in federal court if HUD investigated the complaint or
issued a charge and one of the parties elected to proceed in court. In fair
housing cases, DOJ can seek injunctive relief—such as training and policy
changes—monetary damages, and civil penalties in pattern or practice cases.[6]

FTC and CFPB enforce the Fair Credit Reporting Act (FCRA),
including oversight of consumer reporting agencies.[7] These agencies assemble or evaluate
consumer information—such as employment, criminal, rental, eviction, and credit
history—into consumer reports provided to third parties, including rental
property owners who use them to determine eligibility for rental housing. Both FTC
and CFBP can investigate and initiate enforcement action against consumer
reporting agencies that violate FCRA requirements, such as those for ensuring
accuracy.

In addition to enforcement, FTC and CFPB seek input from
and publish resources for the public.[8]
Both CFPB and FTC can bring actions for alleged FCRA violations.  FTC can seek
injunctive relieve on its own but must seek civil penalties through DOJ in
federal district court.  CFPB can bring its own cases alleging FCRA violations
for civil penalties and injunctive or other relief. 

FTC enforces Section 5 of the FTC Act, which prohibits
unfair methods of competition or deceptive acts or practices affecting
commerce.[9]
Under this authority, FTC may investigate and initiate enforcement actions if
it has reason to believe a violation occurred or is occurring. The FTC Act
allows the agency to seek cease-and-desist orders or injunctive relief, impose
civil penalties for violations of its orders or certain rules, and obtain
consumer redress in certain circumstances.

HUD Administration and Oversight of Subsidized Housing

In addition to enforcing the Fair Housing Act, HUD plays a
role in administering and overseeing subsidized housing programs.

Office of Public and Indian Housing. This office
administers the Public Housing and Housing Choice Voucher programs, two of HUD’s
largest subsidized housing programs.[10]
PHAs—typically municipal, county, or state agencies created under state
law—operate both programs. Under the Public Housing program, HUD provides
subsidies to PHAs, which own and operate rental housing designated for eligible
low-income households. Under the Housing Choice Voucher program, HUD provides
rental subsidies, through PHAs, that renters can use to obtain housing in the
private market.

Office of Policy Development and Research. This
office develops fair market rents, which are used to determine the maximum
allowable rent for Housing Choice Voucher recipients. Fair market rent
generally reflects the cost of renting a moderately priced unit in a local
housing market. HUD calculates and publishes these rents annually for thousands
of locations.

Owners and Renters Use
Proptech Tools for Listing, Searching, Screening, and Rent-Setting

Owners and renters may use one or more of the four types
of proptech tools we reviewed—advertising platforms, tenant screening tools,
rent-setting software, and facial recognition technology—for functions
including advertising, rent-setting, screening tenants, and controlling access.

Advertising platforms. These platforms allow owners
to list rental properties on websites for prospective renters to view (see fig.
1). Features may include targeted advertising, virtual touring, and rent
estimates. Algorithms and machine learning may be used to personalize search
results by tailoring recommendations based on user data. Other platform
features may include rental-management tools for landlords, such as application
intake, tenant screening, electronic lease signing, and rent collection.





Tenant screening tools. Property owners use tenant
screening tools to assess the suitability of prospective renters for a rental
unit (see fig. 2). The tools assemble and evaluate tenant background
information such as credit and criminal history, employment and income verification,
and rental payment or eviction history to generate a screening report. Owners
may use these reports to attempt to evaluate an applicant’s likelihood of
fulfilling lease obligations.[11]
Some tools use machine learning to analyze patterns in applicant data to
attempt to predict a prospective renter’s reliability based on historical
trends. These predictions may be presented as a score or recommendation that
owners consider when deciding whether to approve an applicant.





Rent-setting software. Also known as revenue
management software, these tools help property owners determine rents by
generating data-driven pricing recommendations (see fig. 3). They may use
proprietary or public data, such as occupancy rates, market trends, and
comparable unit prices. This tool may also apply artificial intelligence
techniques, including machine learning, to forecast demand and suggest optimal
rental levels. The software allows owners to adjust rents in response to
current market conditions and their own pricing objectives.





Facial recognition technology. Building owners use
facial recognition technology for security, including access control (see fig.
4). For example, cameras installed at building entrances use software to verify
a renter’s identity by comparing their face to a database of stored images. A
successful match allows entry. The software uses computer vision, a type of
artificial intelligence, to recognize faces in real time, verify identities
from images, and improve accuracy over time through learning.





Selected Proptech Tools
Can Offer Convenience and Safety but Also Pose Privacy, Bias, and Other Risks

Online Listing Platforms Can Benefit Owners and Renters but Carry Risks
Such as Misrepresentation

Online listing platforms can provide several benefits for
owners and renters, according to the industry associations and advertising
companies we interviewed, including the following:

Wider advertising reach. Owners can use third-party
advertising features to extend the visibility of their listings across multiple
platforms. These tools use algorithms to target potential renters more
efficiently than traditional print advertising. They also can save owners time
by creating one listing that can be displayed across various websites and
social medial platforms. Broader exposure can help fill vacancies more quickly
and maximize rental income.

Convenience and cost savings of virtual tours.
Listing platforms allow potential renters to take virtual tours—often in
three-dimensional formats—that show a unit’s floor layout. Virtual touring is
available at any time and may reduce travel and other costs. It also helps
individuals with mobility or accessibility limitations assess whether a unit
meets their needs without having to physically be present at the unit.

Cost savings and efficiencies from universal
applications. One company with which we spoke offers “universal” rental
applications for potential renters and property owners. The service is free for
owners who accept the application. Prospective renters submit a single form and
pay one fee to apply to multiple units over a 30-day period.

However, representatives of advocacy groups and officials
from federal agencies we interviewed noted that advertising tools posed
potential risks to renters, including potential misrepresentation and
discrimination.

Misrepresentation of listings or costs. Owners may
post fraudulent or misleading images, including altered or staged photographs.
For example, potential renters may rely on pictures and videos provided in a
virtual tour, but owners could manipulate these images to make rooms appear
larger or smaller than they are. Owners also may make unclear statements about
rent costs. For example, rent may be advertised online in a way that obscures
actual costs, or fees for amenities like gyms or conference rooms may be hidden
until move-in.

Discriminatory advertising. Platforms or owners may
limit who sees listings or include language that discourages renters on a
prohibited basis, such as by protected classes. For example, phrases such as “no
children” or “no wheelchairs” may violate the Fair Housing Act by
discriminating on the basis of familial status or disability. Unlike
traditional print advertising, where a listing is provided in writing and a
landlord subsequently assesses applicants, algorithms may analyze characteristics
such as income or location and steer certain users to or from specific listings
before they can apply. This practice may reduce housing access for minorities,
women, families with children, and individuals with disabilities.

Online platforms use several approaches to help mitigate
these risks, according to representatives from one of the three advertising
companies we interviewed. For example, this company said its platform includes
a rental cost and fee calculator that helps potential renters understand the
full cost of a unit. The tool lists associated fees and expenses, such as
monthly costs for utilities, parking, and pet fees, as well as one-time charges
like security deposits and application or administrative fees. This company
described a set of controls designed to prevent discriminatory advertising.
These include keyword detection logic to filter potentially discriminatory
language—for instance, blocking phrases that discourage families with children.
The platform also displays information about relevant federal, state, and local
fair housing laws to owners before they upload a listing. In addition, the
website allows users to report potential discriminatory content for manual
review by the company.

Screening Technology Can Help Owners Manage Risks, but Inaccurate
Information May Result Denials

According to industry associations, and representatives
from two companies we interviewed, owners benefit from tenant screening tools
in part because they help mitigate renter-based risks, including the following:

Failure to meet lease obligations. Screening tools
may help owners assess background information to reduce the risk a renter might
not fulfill lease terms. For example, reviewing a prospective renter’s rent
payment and credit history may help owners identify applicants with a lower
risk of nonpayment. 

Fraud risks. Screening algorithms assist owners in
verifying information that potential renters provide. This verification helps
reduce risks of identity fraud (false names, Social Security numbers, or birth
dates) and synthetic fraud (fabricated documents, such as pay stubs).

Potential risks and challenges related to using screening
reports include the following:

Inaccurate information. Tenant screening reports
often contain inaccurate information, which can lead to unwarranted rental
application denials. For example, of the approximately 26,700 screening-related
complaints submitted to CFPB from January 2019 to September 2022, approximately
17,200 were related to inaccurate information.[12]
Specifically, these complaints noted challenges obtaining housing—for instance,
due to information erroneously included in their report; outdated information
that legally should not have been included; and inaccurate arrest, criminal,
and eviction records.

Model transparency issues. As we previously
reported, models using algorithms can produce unreliable and invalid results.[13] Advocacy groups noted that
algorithms may present a recommendation to the owner without disclosing the
specific data used or how it was weighted. As a result, owners and renters may
be unable to identify or correct errors, and renters may be unable to submit
mitigating information to improve their chances of securing housing.

Disparate impact. Screening algorithms that rely on
criminal history and eviction records may have an adverse impact on minorities,
including Black and Hispanic applicants, according to advocacy group officials.[14] For example, algorithms that
recommend rejecting applicants with any criminal or eviction record may
disproportionately affect these groups due to their overrepresentation in the
criminal justice and housing court systems. This may contribute to reduced
housing access and increased housing instability. In addition, algorithms may
not differentiate between serious offenses and minor ones that are unlikely to
affect renter’s reliability.

Representatives of tenant screening companies we
interviewed reported taking several steps to help mitigate these risks. For
example, one company allows potential renters to review their background
information and submit comments to the owner before applying. Renters can flag
inaccuracies or provide a narrative to explain circumstances the owner might
otherwise overlook. In addition, three companies told us they review and
subsequently correct inaccurate information on a tenant screening report if
notified by a renter or owner.

Rent-Setting Algorithms Offer Owners Pricing Insights but May Lead to
Higher Rents

Representatives from two industry associations, HUD
officials, and representatives from one company offering rent-setting software
noted that owners and renters may benefit from the use of tools that provide
rent-setting algorithms in several ways, including the following:

Responsiveness to market changes. Rent-setting
algorithms may help owners adjust rents more quickly in response to changing
market conditions. This can help owners achieve their desired occupancy rates,
keep their properties at their desired level relative to the market, and
minimize revenue lost from unintended vacancies.

Improved accuracy in HUD fair market rents. HUD
incorporates data from private market sources that includes a rent-setting tool
to enhance the accuracy and representativeness of its fair market rent
calculations.[15]
According to HUD officials, incorporating private-market data helps address
gaps in public data sources, such as census data, which may be outdated or lack
coverage in certain markets. HUD officials stated that more accurate and timely
fair market rent calculations can help renters with Housing Choice Vouchers
find suitable, affordable housing.

However, representatives of advocacy groups also told us
that renters face several risks when owners use rent-setting algorithms,
including the following:

Reduced bargaining power. When owners rely on
rent-setting algorithms to standardize rental prices across a geographic
market, advertised rents may increase and renters may have less ability to
negotiate lower prices. Algorithms may recommend rents that owners treat as a
market benchmark, limiting flexibility to lower prices, even when individual
renters attempt to negotiate. Advocacy groups expressed concern that this
effect is especially pronounced in tight housing markets, where limited supply
further constrains renters’ leverage.

Potential increases in rental housing costs.
According to University of Pennsylvania researchers, owners using rent-setting
software adjusted rents more responsively to changing market conditions compared
to other property owners.[16]
This included increasing rents and reducing occupancy rates during periods of
economic growth.[17] Moreover,
this pattern was also found at the geographic level when, during periods of
economic growth, higher levels of rent-setting software were associated with
higher rent levels and lower occupancy rates. Advocacy groups we spoke to
reiterated these findings, noting that dependent on market conditions, the use
of a rent-setting algorithm can lead to higher rents for some renters. 

Facial Recognition Technology Can Enhance Security but Pose Privacy Risks

Representatives from three of the four industry
associations and all 10 of the PHAs we spoke to told us that the use of facial
recognition technology can enhance safety for both private and subsidized
rental housing. Owners and PHAs may install surveillance cameras equipped with
facial recognition technology to improve property security. Industry
association and PHA officials overseeing properties with such technology told
us that it can enhance safety by helping ensure that only renters and their
authorized guests can enter buildings. They noted that the technology may
reduce the risk of unauthorized individuals entering public housing facilities
and engaging in criminal activity.

However, representatives from advocacy organizations we
interviewed raised concerns about the use of facial recognition technology in
rental housing, citing risks related to accuracy, privacy, and informed
consent.

Error rates. Advocacy groups we interviewed
expressed concerns about facial recognition technology’s higher error rates for
identifying and verifying individuals from certain demographics—particularly
Black women. In the rental housing context, such inaccuracies could result in
frequent access denials for some individuals.[18]
Representatives of facial recognition companies cited several factors that may
contribute to these errors, including poor lighting, facial expressions, and
obscured facial features. They also noted that data quality—including outdated
or low-resolution images used for comparison—may also affect accuracy.[19]

Privacy and consent. Facial recognition technology
relies on the use of biometric information, which is unique to each person.
Representatives from advocacy groups we interviewed expressed concern that
surveillance data collected by owners could be used without renter consent. For
example, owners could share this information with law enforcement or use it for
action against a renter, such as an eviction or fine. Additionally,
representatives of 6 of the ten PHAs we interviewed expressed uncertainty about
what steps they should take to obtain consent when using facial recognition
technology as part of their housing operations.

Federal Agencies
Addressed Some Proptech Risks, but HUD Could Further Mitigate Risks

Federal agencies took several actions to address risks
related to selected proptech tools.[20]
However, HUD has opportunities to further mitigate risks related to facial
recognition technology in public housing.

Agencies Took Steps to Address Allegedly Misleading and Discriminatory
Advertising Practices

To address potential risks associated with online listing
platforms—specifically, misleading advertisements and discriminatory
advertising—FTC, HUD, and DOJ initiated legal actions and HUD issued guidance.

Misleading advertisements. In 2022, FTC initiated a
lawsuit against Roomster, a platform for rental housing and roommate listings.[21] FTC alleged that the company
participated in deceptive acts or practices in violation of Section 5 of the
FTC Act because its website contained inaccurate rental listings and misleading
or fake consumer reviews. A federal court issued a stipulated order requiring
the company to stop misrepresenting its listings, investigate any consumer
complaints, and pay a fine.

Discriminatory advertising practices. In March
2019, following a complaint and investigation, HUD issued a Charge of
Discrimination against Meta alleging that its advertising platform violated the
Fair Housing Act by allowing advertisers to target or exclude users based on
protected characteristics. For example, Meta’s platform included a toggle
feature that allowed advertisers to exclude men or women from viewing an
advertisement.[22]

The case was referred to DOJ, which filed suit in in 2022.
DOJ alleged that Meta’s advertising delivery system used algorithms that
considered traits such as familial status, race, religion, and sex.[23] As part of the settlement, Meta
agreed to stop using an advertising tool known as the Special Audience Tool,
develop a new system that would address racial and other disparities caused by
its use of personalization algorithms in its advertisement delivery system for
housing, and eliminate targeting options related to protected characteristics
under the Fair Housing Act. 

In April 2024, HUD’s Office of Fair Housing and Equal
Opportunity issued guidance outlining advertising practices that may violate
the Fair Housing Act when used to categorize or target online users.[24] These include advertisements that
discourage certain groups from applying, offer different prices or conditions
based on protected characteristics, or steer individuals toward specific
neighborhoods. The guidance advised housing providers to avoid targeting
options that directly or indirectly relate to protected characteristics. It
also encouraged platforms to test their systems to ensure advertisements are
not delivered in a discriminatory fashion.

Agencies Took Enforcement and Other Actions to Address Tenant Screening and
Reporting Issues

Federal agencies have taken steps to address issues
related to the accuracy and potential adverse impact of tenant screening tools.
These steps include issuing guidance, taking enforcement actions and filing a
statement of interest.

Guidance on Tenant Screening Tools. Also in April
2024, HUD issued guidance on applying the Fair Housing Act to screening of
rental housing applicants.[25]
HUD’s Office of Fair Housing and Equal Opportunity published the guidance on
its public website. The guidance also addressed the application of the Fair
Housing Act to tenant screening processes that use artificial intelligence and
machine learning. It included best practices for rental housing owners and
tenant screening companies to support compliance with fair housing laws and
mitigate artificial intelligence-related and other risks. To address inaccuracy
risks, the guidance recommended that owners give applicants an opportunity to
dispute the accuracy or relevance of negative information in a tenant screening
report. To improve transparency, it advised tenant screening companies to
include all relevant information behind a denial decision, particularly when
artificial intelligence tools are used and the outcome may be difficult to
explain. In February 2025, we observed that HUD had removed the tenant
screening guidance from its public website. HUD provided two explanations for
the removal: (1) In March 2025, HUD officials told us the guidance was taken
down as part of an agencywide review of its policies and guidance to ensure
consistency with an executive order; and (2) in March 2025, HUD told us the
agency was updating its website and the guidance remained in effect.[26]

AppFolio. In December 2020, DOJ and FTC, sued
AppFolio, Inc. a tenant screening company, for violating FCRA. The complaint
alleged that the company used incorrect and obsolete eviction and criminal
arrest records—some more than 7 years old—in its tenant screening reports.[27] The agencies also alleged the
company failed to use reasonable procedures for accuracy, relying on
third-party data without sufficient verification. As part of the settlement,
the company agreed to pay a fine and implement corrective measures to improve
its accuracy procedures.

TransUnion Rental Screening Solutions. In October
2023, FTC and CFPB jointly sued TransUnion Rental Screening Solutions and its
parent company for violating FCRA.[28]
The agencies alleged the company failed to follow reasonable procedures to
ensure maximum possible accuracy of report content, including reporting sealed
or incorrect eviction records, and did not disclose the sources of third-party
information when requested by consumers. The stipulated settlements filed with
the complaint in October 2023 required the company to enhance its procedures to
ensure maximum possible accuracy for verifying eviction data and to pay
consumer redress and a civil penalty.

Five background screening companies. In September
2023, FTC initiated an enforcement action against five background screening
companies that advertised tenant screening reports (among other things),
alleging violations of FCRA and Section 5 of the FTC Act.[29] FTC alleged that the companies
inaccurately reported nonexistent or irrelevant criminal records—such as
traffic violations—and allowed subscribers to edit their reports without
adequate verification. In October 2023, a federal court entered a stipulated
order that required the companies to cease certain practices unless they
implemented compliance measures aligned with FCRA.

Statement of interest. In January 2023, DOJ and HUD
submitted a statement of interest in the federal court case Louis et al. v.
SafeRent Solutions and Metropolitan Management Group.[30] In the case, the two plaintiffs were
Black rental applicants who used housing vouchers and alleged that SafeRent’s
algorithm-based tenant screening tool assigned them low “SafeRent” scores,
resulting in the denial of their rental applications. The plaintiffs also
alleged that the tool disproportionately affected Black and Hispanic applicants
by relying on factors such as credit history and non-housing-related debts,
while failing to account for of housing vouchers as a reliable source of
income. The agencies asserted that the Fair Housing Act applies to tenant
screening companies that provide data used by housing owners to make
suitability determinations. Accordingly, DOJ and HUD stated that such companies
are prohibited from engaging in practices that result in discrimination based
on protected characteristics.[31]
In December 2024, the court approved a settlement between SafeRent, the other
defendants, and the class of plaintiffs, and the case was subsequently
dismissed.

HUD Has Opportunities to Further Mitigate Risks Relating to Facial
Recognition

In September 2023, HUD’s Office of Public and Indian
Housing published a letter advising PHAs to balance security concerns with
their public housing residents’ privacy rights when using surveillance
technology.[32]
However, the letter does not provide specific direction on key operational
issues regarding facial recognition technology. For example, it does not
discuss how PHAs should manage privacy risks or share data with law
enforcement.

Representatives of all 10 PHAs we interviewed stated they
would benefit from additional information and direction from HUD on their use
of facial recognition technology, especially on the following topics:

·       Purpose
specification. Six of the 10 PHAs wanted HUD to clarify the permitted uses
of facial recognition technology. While they primarily use it to control
building access for tenants and their authorized guests, they expressed
interest in guidance on other potential uses, such as whether and how to
disclose data to third parties, including law enforcement.

·       Renter
consent. Six of the 10 PHAs wanted guidance on what constitutes adequate
renter consent. For example, they questioned whether posting signs about the
use of facial recognition systems was sufficient. They also sought clarity on
whether written consent is required before including tenant facial images in
system databases, and what steps to take if a renter declined to provide
consent.

·       Data
management. Five of the 10 PHAs wanted guidance on managing data collected
through facial recognition systems. For example, they sought clarity on how
long to retain images after a tenant moved out.

·       Accuracy.
One PHA wanted HUD to provide guidance on mitigating potential accuracy
concerns.

HUD officials stated the agency has no plans to revise the
September 2023 letter or issue additional written direction on facial
recognition technology, citing the need to preserve PHAs’ autonomy in
implementing it. However, as discussed earlier, representatives of six PHAs we
interviewed expressed uncertainty about what steps they should take to obtain
consent when using facial recognition technology as part of their housing
operations.

HUD officials also stated that developing new guidance
would require surveying about 3,300 PHAs to identify their information gaps,
straining limited resources. However, the risks of facial recognition
technology are well documented. For example, we have previously reported on
concerns related to accuracy and the use of the technology without an
individual’s consent.[33]
As a result, we believe HUD could develop written direction for PHAs without
the use of a survey.

According to federal internal control standards, program
managers should externally communicate the necessary quality information to
achieve the entity’s objectives.[34]
By providing additional direction on use of facial recognition technology, HUD
could help PHAs it oversees mitigate privacy and accuracy concerns and offer
clarity on key issues such as purpose, consent, and data management.

Conclusions

The selected proptech tools we reviewed offer benefits to
individuals searching for, living in, or owning or managing rental properties.
However, they also can pose risks—particularly related to the accuracy of
personal information and the potential for misrepresentation and
discrimination. Federal agencies took steps to address these risks, but HUD has
opportunities to further support PHAs it oversees. Providing additional written
direction on the appropriate use of facial recognition technology would give
PHAs greater clarity and help mitigate privacy and accuracy concerns.

Recommendation for
Executive Action

The Secretary of HUD should ensure that the Assistant
Secretary for Public and Indian Housing provides additional written direction
to public housing agencies on the use of facial recognition technology. For
example, this direction could specify permitted uses of the technology, define
what constitutes renter consent, and address data management and accuracy
concerns.

Agency Comments

We provided a draft of this report to HUD, CFPB, FTC, and
DOJ for review and comment. These agencies provided technical comments, which
we incorporated as appropriate. 

As agreed with your office, unless you publicly announce
the contents of this report earlier, we plan no further distribution until 30
days from the report date. At that time, we will send copies to the appropriate
congressional committees, the Secretary of Housing and Urban Development, Chair
of Federal Trade Commission, Acting Director of the Consumer Financial
Protection Bureau, Attorney General, and other interested parties. In addition,
the report will be available at no charge on the GAO website at https://www.gao.gov.

If you or your staff have any questions about this report,
please contact me at cackleya@gao.gov.
Contact points for our Offices of Congressional Relations and Public Affairs
may be found on the last page of this report. GAO staff who made key contributions
to this report are listed in appendix III.

Sincerely,



Alicia Puente Cackley
Director, Financial Markets and Community Investment



Appendix I: Objectives,
Scope, and Methodology



This report examines (1) selected property
technology (proptech) tools available in the rental housing market, how
they are used, and by whom; (2) the benefits and risks that
selected proptech tools may pose for owners and renters; and (3)
steps taken by federal agencies to oversee these selected proptech tools.

We identified thirty four proptech tools through a review
of reports from federal regulators, academics, industry groups, and advocacy
groups. We focused on tools that incorporate artificial intelligence and are
used by owners and renters in the rental housing process. We purposively
selected four types of proptech tools for examination: advertising platforms,
tenant screening tools, rent-setting software, and facial recognition
technology. These tools are not representative of all proptech tools used in the
private and subsidized rental housing markets.

To gather information on owner and renter use of the
selected proptech tools—and the associated benefits and risks—we interviewed
representatives of a purposeful, nongeneralizable sample of 12 companies that
provide such tools. These consisted of three advertising companies, three
tenant screening companies, four facial recognition companies, and two
rent-setting software companies. To identify these companies, we reviewed
research reports, and publicly available lists of firms offering the selected
tools to generate a list of companies. From that list, we selected companies
that offered one or more of the tools and were responsive to our outreach. The
information obtained from these interviews cannot be generalized to all
companies that offer proptech tools in the private and subsidized rental
housing markets.

In addition, we interviewed representatives from four
industry associations and five advocacy organizations, selected because they
published reports from 2019 to 2024 about the selected tools.[35] We also interviewed officials from
two organizations funded by the Fair Housing Initiative Program, a federal
program designed to assist people who believe they have been victims of housing
discrimination.[36]
We selected the two organizations because they work directly with renters, are
in the metropolitan area with the largest number of public housing units based
on Department of Housing and Urban Development (HUD) data and were recipients
of HUD Fair Housing Initiative Program grants within the past 2 years.

To understand the use of and benefits and risks of the
selected proptech tools in the subsidized housing market, we conducted
semi-structured interviews with representatives of a nongeneralizable sample of
10 public housing agencies (PHA). We selected these PHAs to achieve a diversity
in PHA size, Census region, and use of facial recognition technology. The
information we gathered from these interviews cannot be generalized to the
approximately 3300 PHAs.

To identify steps taken by federal agencies to oversee the
selected proptech tools, we reviewed agency guidance, final rulemakings,
federal court orders, agency enforcement actions, and relevant advisory
opinions issued from 2019 through 2024. We analyzed relevant HUD documentation
and interviewed HUD and selected PHA officials on HUD efforts to communicate to
PHAs on the use of surveillance technology in their operations. We also
compared a HUD communication to PHAs about using surveillance technology in their
operations against federal internal control standards.[37] We determined that the internal
control principle that program managers should externally communicate the
necessary quality information to achieve the entity’s objectives was
significant to this objective.

To address all three objectives, we also reviewed relevant
laws, including the Fair Housing Act, the Fair Credit Reporting Act, the
Federal Trade Commission Act, and the Sherman Antitrust Act. We also reviewed
relevant regulations, such as HUD’s regulation on discriminatory advertising. In
addition, we interviewed representatives from the following federal agencies:
Consumer Financial Protection Bureau, Department of Justice, Federal Trade
Commission, and HUD. Within HUD, we interviewed officials from the Office of
Public and Indian Housing, Office of Multifamily Housing, Office of Policy
Development and Research, and Office of Fair Housing and Equal Opportunity.[38]

We conducted this performance audit from November 2023 to
July 2025 in accordance with generally accepted government auditing standards.
Those standards require that we plan and perform the audit to obtain
sufficient, appropriate evidence to provide a reasonable basis for our findings
and conclusions based on our audit objectives. We believe that the evidence
obtained provides a reasonable basis for our findings and conclusions based on
our audit objectives.



Appendix II: Selected
Actions Federal Agencies Took Related to Property Technology Use in the Rental
Housing Context, 2019–2024



This appendix provides information on selected legal
actions or guidance initiated by federal agencies from 2019 through 2024
related to tenant screening, advertising platforms, or rent-setting software.




 
  
  DOJ
  
  
  Litigation
  
  
  Rent-setting software. In 2024, the Department
  of Justice (DOJ) and attorneys general from eight states initiated a civil
  antitrust lawsuit against RealPage, Inc, a company that develops and sells
  rent-setting software.a The complaint alleges that the company’s
  software enables rental housing owners to share confidential, competitively
  sensitive information—such as rental prices and lease terms—allowing them to
  align rents and reduce competition in violation of the Sherman Act. In
  December 2024, RealPage moved to dismiss, arguing that its software lacked
  market-wide influence and that DOJ failed to show anticompetitive effects.
  Additionally, in 2025, DOJ amended its complaint to include certain property
  owners.b 
  
 
 
  
  HUD-DOJ
  
  
  Amicus brief
  
  
  Tenant screening. In Connecticut Fair Housing
  Center v. CoreLogic Rental Property Solutions LLC, the Department of Housing
  and Urban Development (HUD) and DOJ filed a joint amicus brief in support of
  plaintiffs-appellants-cross-appellees. The brief asserted that tenant
  screening companies such as CoreLogic are subject to the Fair Housing
  Act—even if they are not the entities rejecting or accepting potential
  renters—because their reports to owners can be used to make housing
  unavailable. The agencies also underscored that blanket exclusions based on
  criminal history can disproportionately affect minority groups.c
  They cited HUD’s 2016 guidance warning that such practices may violate the
  Fair Housing Act if not based on a substantial, legitimate, nondiscriminatory
  interest.d 
  
 
 
  
  DOJ
  
  
  Statement of interest
  
  
  Rent-setting software. In September 2023,
  plaintiffs filed an amended complaint alleging that RealPage and several
  property-management companies engaged in a price-fixing conspiracy in
  violation of Section 1 of the Sherman Act. They claimed RealPage’s
  rent-setting software facilitated the conspiracy by aggregating and sharing
  nonpublic rental data among competing property owners and managers. In
  November 2023, DOJ submitted a statement of interest arguing that such
  software enables owners to share nonpublic, sensitive information,
  potentially leading to coordinated rental housing prices.e This
  coordination, DOJ asserted, harms competition among owners and keeps prices
  artificially high for renters. 
  
 
 
  
  FTC-DOJ
  
  
  Statement of interest
  
  
  Rent-setting software. In September 2023, the
  plaintiff filed a class action lawsuit in federal court against Yardi
  Systems, Inc. and multiple property management companies.f The
  lawsuit alleges that the defendants collaborated to fix rental housing prices
  using Yardi’s rent-setting software. In December 2023, the defendants moved
  to dismiss, contending that each company independently chose to use Yardi’s
  software and set rents on its own. In March 2024, DOJ and the Federal Trade
  Commission (FTC) issued a joint statement of interest. They noted that owners’
  collective reliance on Yardi’s rent-setting algorithm could facilitate
  price-fixing agreements in potential violation of the Sherman Act. The
  agencies also noted that even without direct communication among owners or
  strict adherence to algorithm’s recommendations, shared use of the technology
  might lead to coordinated pricing, harming consumers through reduced
  competition. In December 2024, the federal court denied the motion to
  dismiss.g
  
 


Source: GAO analysis.  |  GAO‑25‑107196

aUnited States v. RealPage, Inc., No.
1:24-cv-710 (M.D.N.C. filed Aug. 23, 2024).

bThe six owners are Greystar Real Estate
Partners, Cushman & Wakefield, Camden Property Trust, LivCor, Pinnacle
Property Management Services, and Willow Bridge Property Company. The January
2025 amended complaint initially included Cortland Management, but on the same
date the United States and Cortland reached a final judgement where Cortland
agree to certain changes and to refrain from certain actions. In February 2025,
following the amended complaint, RealPage filed another motion to dismiss the
amended complaint which contained similar arguments on these points. As of June
26, 2025, the parties are awaiting court decisions on a motion to dismiss and a
proposed final judgment.

cIn Connecticut Fair Housing Center v.
CoreLogic Rental Property Solutions, the plaintiff alleged that CoreLogic’s
tenant screening tool, CrimeSAFE, discriminated based on race, national origin,
and disability when the tool’s use resulted in the denial of a disabled Latino
man with no criminal conviction from moving in with his mother, alleging
violations of the Fair Housing Act, Fair Credit Reporting Act, and the
Connecticut Unfair Trade Practice Act. No. 3:18-cv-705 (D. Conn.). The United
States District Court for the District of Connecticut ruled in favor of the
plaintiff for the Fair Credit Reporting Act claim but ruled in CoreLogic’s
favor for the Fair Housing Act and Connecticut Unfair Trade Practice Act
claims. The plaintiffs appealed the district court’s denial of their Fair
Housing Act claims, among others, and CoreLogic then cross-appealed the court’s
decision relating to the Fair Credit Reporting Act claim. In November 2024, the
parties, and the United States as amicus curiae supporting the plaintiff,
presented arguments in the United States Court of Appeals for the Second
Circuit. As of June 26, 2025, the Second Circuit had not issued a decision.
Conn. Fair Hous. Ctr. v. CoreLogic Rental Prop. Sol., LLC, No. 23-1166 (2d Cir.
argued Nov. 20, 2024).

dDepartment of Housing and Urban Development,
Office of General Counsel, Guidance on Application of Fair Housing Standards to
the Use of Criminal Records by Providers of Housing and Real Estate-Related
Transactions (Washington D.C.: April 2016); and Guidance on Fair Housing Act
Protections for Tenant Screening Practices Involving Criminal History
(Washington D.C.: June 2022).

eIn re RealPage, No.
3:23-MD-3071 (M.D. Tenn). In February 2025, following the amended
complaint, RealPage filed another motion to dismiss the amended complaint,
which contained similar arguments on these points. As of June 2025, the court
had not held oral arguments for RealPage’s motion to dismiss. The other
defendant property management companies subsequently filed separate motions to
dismiss on their own behalf. Settlement agreements have been reached with
multiple defendants. As of June 2025, the case was ongoing. 

fDuffy v. Yardi Sys., Inc., No. 2:23-cv-01391
(W.D. Wash.).

gAs of June, 2025, the case was ongoing.




 
  
   
   Topic
   
   
   Status
   
   
   Summary of advisory opinion
   
  
 
 
  
  Name-only matching procedures
  
  
  Withdrawn
  
  
  In November 2021, the Consumer Financial Protection
  Bureau (CFPB) published an advisory opinion stating that name-only matching
  procedures—those based solely on the similarity of first and last names—do
  not meet the Fair Credit Reporting Act’s (FCRA) requirement to use reasonable
  procedures to ensure maximum possible accuracy when generating consumer
  reports.a The opinion notes that relying solely on a person’s
  first and last name could lead to inaccurate information being included in a
  consumer report. CFPB withdrew this guidance on May 12, 2025. 
  
 
 
  
  Facially false data
  
  
  Active
  
  
  In October 2022, CFPB published an advisory opinion
  stating that consumer reporting agencies must implement reasonable procedures
  to detect and prevent the inclusion of facially false (logically
  inconsistent) data when generating consumer reports.b Examples
  include reporting a delinquency that predates the account opening or an
  account closure date that predates the consumer’s listed date of birth. 
  
 
 
  
  Report accuracy
  
  
  Withdrawn
  
  
  In January 2024, CFPB issued an advisory opinion on
  background reports.c The opinion clarifies that consumer reporting
  agencies must implement reasonable procedures to ensure that reports do not
  include information that is duplicative, expunged, sealed, or otherwise
  restricted from public access. Each adverse item is also subject to its own
  reporting period. For example, a criminal charge that did not result in a
  conviction generally cannot be reported more than 7 years after the date of
  the charge. CFPB withdrew this guidance on May 12, 2025. 
  
 
 
  
  File disclosures
  
  
  Withdrawn
  
  
  In January 2024, CFPB issued an advisory opinion on
  file disclosures.d The opinion underscores that under FCRA,
  consumers have the right to request and obtain all information in their
  consumer file at the time of request. The opinion explains how consumer
  reporting agencies must fulfill this request even if the consumer does not
  explicitly ask for a “complete file” and also must disclose the sources used
  to generate the report. CFPB withdrew this guidance on May 12, 2025. 
  
 


Source: GAO analysis of CFPB advisory opinions.  |  GAO‑25‑107196

CFPB issues advisory opinions to publicly address
uncertainty about its existing regulations and provide guidance to regulated
entities.

aConsumer Financial Protection Bureau, Fair
Credit Reporting: Name-Only Matching Procedures (Washington, D.C.: Nov. 4,
2021).

bConsumer Financial Protection Bureau, Fair
Credit Reporting: Facially False Data (Washington D.C.: Oct. 20, 2022).

cConsumer Financial Protection Bureau, Fair
Credit Reporting; Background Screening (Washington D.C.: Jan. 11, 2024).

dConsumer Financial Protection Bureau, Fair
Credit Reporting: File Disclosures (Washington, D.C.: Jan. 11, 2024).



Appendix III: GAO
Contact and Staff Acknowledgments



GAO Contact

Alicia Puente Cackley, CackleyA@gao.gov.

Staff Acknowledgments

In addition to the contact named above, Cory Marzullo
(Assistant Director), Brandon Jones (Analyst in Charge), Daniel Horowitz, Lydie
Loth, Marc Molino, Barbara Roesmann, Jessica Sandler, Tristan Shaughnessy,
Norma-Jean Simon, and Sean Worobec made key contributions to this report.

GAO’s Mission

The Government Accountability
Office, the audit, evaluation, and investigative arm of Congress, exists to
support Congress in meeting its constitutional responsibilities and to help
improve the performance and accountability of the federal government for the
American people. GAO examines the use of public funds; evaluates federal
programs and policies; and provides analyses, recommendations, and other
assistance to help Congress make informed oversight, policy, and funding
decisions. GAO’s commitment to good government is reflected in its core values
of accountability, integrity, and reliability.

Obtaining Copies of GAO Reports and Testimony

The fastest and easiest way to obtain copies of GAO documents at no cost
is through our website. Each weekday afternoon, GAO posts on its website newly released reports, testimony, and
correspondence. You can also subscribe to GAO’s email
updates to receive notification of newly posted products.

Order by Phone

The price of each GAO publication reflects GAO’s actual
cost of production and distribution and depends on the number of pages in the
publication and whether the publication is printed in color or black and white.
Pricing and ordering information is posted on GAO’s website, https://www.gao.gov/ordering.htm. 

Place orders by calling (202) 512-6000, toll free (866) 801-7077,
or 
TDD (202) 512-2537.

Orders may be paid for using
American Express, Discover Card, MasterCard, Visa, check, or money order. Call
for additional information.

Connect with GAO

Connect with GAO on X,
LinkedIn, Instagram, and YouTube.
Subscribe to our Email Updates. Listen to our Podcasts.
Visit GAO on the web at https://www.gao.gov.

To Report Fraud, Waste, and Abuse in Federal
Programs

Contact FraudNet:

Website: https://www.gao.gov/about/what-gao-does/fraudnet

Automated answering system: (800) 424-5454

Media Relations

Sarah Kaczmarek, Managing Director, Media@gao.gov 

Congressional Relations

A. Nicole Clowers, Managing Director, CongRel@gao.gov

General Inquiries

https://www.gao.gov/about/contact-us





[1]Artificial intelligence, in
general, refers to computer systems that can solve problems and perform tasks
that have traditionally required human intelligence and that continually get
better at their assigned tasks. The White House, Office of Science and Technology
Policy, American Artificial Intelligence Initiative:
Year One Annual Report (Washington, D.C.: February 2020). Machine
learning, a type of artificial intelligence, uses algorithms to identify
patterns in information.

[2]We use “tenant screening”
throughout this report to refer to tools that seek to assess the suitability of
prospective renters for rental housing. In all other contexts, we use “renter”
rather than “tenant.” We use “owners” to refer to individuals who own rental
property, as well as those individuals or entities—such as property
managers—acting on their behalf. 

[3]Several resources we reviewed
rely on the disparate-impact theory of liability, mentioned below. Under
Executive Order 14281, executive departments and agencies are tasked with—among
other things—repealing or amending rules and regulations to the extent they
contemplate disparate-impact liability and evaluating and taking appropriate
action regarding pending investigations and proceedings and existing consent
judgments and permanent injunctions relying on theories of disparate-impact
liability. Executive Order 14281: Restoring Equality of Opportunity and
Meritocracy, 90 Fed. Reg. 17537 (Apr. 28, 2025). As of May 8, 2025, the rules,
regulations, and legal actions mentioned below have not been amended or
repealed, though they may be affected to the extent they rely on the
disparate-impact theory of liability. On February 9, 2025, the National
Treasury Employees Union and others filed a lawsuit in the District Court for
the District of Columbia alleging that the actions of the Acting Director of
CFPB including regarding staffing and enforcement work violated the
Administrative Procedure Act and were unconstitutional because they violated
the Congressional mandate in the Dodd-Frank Wall Street Reform and Consumer
Protection Act for CFPB to perform its statutory functions. Nat’l Treasury
Emp. Union, et al. v. Vought, 1:25-cv-00381 (D.D.C. filed Feb. 9,
2025). As of May 2025, the litigation is active and continues in both the
District Court for the District of Columbia and the DC Circuit Court of
Appeals. 

[4]GAO, Standards for Internal Control in the Federal Government,
GAO‑14‑704G
(Washington, D.C.: Sept. 10, 2014). 

[5]Fair Housing Act, Pub. L. No.
90-284, 82 Stat. 73, 81–89, §§ 801–819 (1968), as amended (codified at 42
U.S.C. §§ 3601–3619). Protected characteristics under the act include
race, color, religion, national origin, sex, disability, and familial status.
HUD also provides funding to Fair Housing Initiative Program organizations to
assist individuals who believe they have been victims of housing
discrimination. These organizations also conduct preliminary investigations of
discriminatory claims, including sending “testers” to properties suspected of
practicing housing discrimination. State and local governments may enforce
their own statutes and ordinances that are substantially equivalent to the Fair
Housing Act. 

[6]If an election is made, HUD
refers the case to DOJ, which then files a complaint in federal court. If no
election is made, HUD will litigate the case before its administrative law
judges. 

[7]15 U.S.C. §§ 1681-1681. CFPB
also has supervisory authority over covered persons, including certain consumer
reporting agencies, with respect to compliance with FCRA. 12 U.S.C §§
5511(c)(4); 5481(6), (14), (15)(A)(ix); 5514–5516.

[8]In 2023, FTC and CFPB jointly
issued a request for information seeking public input on the use of algorithms
in tenant screening. The request also asked about the potential for
discriminatory outcomes related to the use of criminal history and eviction records
in the rental housing process. Agency officials told us they issued the request
for general information-gathering purposes. 

[9]15 U.S.C. § 45. 

[10]As
of May, 2025, approximately 970,000 households were living in public housing.
The Housing Choice Voucher program subsidizes the rents of more than 2.3
million households. We did not include HUD’s Office of Multifamily Housing in
our review because it is not directly responsible for overseeing the use of the
selected proptech tools in subsidized properties, according to officials.

[11]The information is typically
obtained from sources such as potential renters, third-party vendors,
courthouses, and other consumer reporting agencies.

[12]Consumer Financial Protection
Bureau, Consumer Snapshot: Tenant Background Checks
(Washington D.C.: November 2022). CFPB’s reported data were the most current
data available as of May 2025. 

[13]GAO, Artificial Intelligence in Health Care: Benefits and
Challenges of Technologies to Augment Patient Care, GAO‑21‑7SP (Washington,
D.C.: Nov. 30, 2020).

[14]There are two principal
theories of liability for discrimination: disparate treatment and disparate impact.
Disparate impact can occur when a facially neutral policy or practice may be
unlawfully discriminatory because it has a disproportionately negative impact
on members of a protected class without a legitimate business need or where
that need could be achieved as well through less discriminatory means. Disparate
impact may occur when an algorithm uses a variable that does not directly refer
to a protected class but still leads to disparate outcomes for certain groups.
In 2015, the Supreme Court upheld the application of disparate impact under the
Fair Housing Act in Tex. Dep’t of Hous. & Cmty. Aff. v. The Inclusive
Cmties. Project, Inc., 576 U.S. 519 (2015). In April 2025, the President issued
an Executive Order establishing a policy to eliminate the use of disparate
impact liability in all contexts to the maximum degree possible. The Executive
Order directs all federal agencies to deprioritize enforcement of statutes and
regulations that include disparate impact liability. Executive Order 14281:
Restoring Equity of Opportunity and Meritocracy, 90 Fed. Reg. 17537 (Apr. 28,
2025).

[15]In 2024, HUD supplemented
public data with six private-sector data sources as part of its methodology to
determine fair market rents. These sources were: (1) Apartment List Rent
Estimate; (2) CoStar Group average effective rent; (3) CoreLogic’s
single-family, combined 3-bedroom median rent index; (4) Moody’s Analytics
average gross revenue per unit; (5) RealPage’s average effective rent per unit;
and (6) Zillow’s Observed Rent Index. 

[16]Calder-Wang,
Sophie and Gi Heung Kim, Algorithmic Pricing in
Multifamily Rentals: Efficiency Gains or Price Coordination? (Aug. 16,
2024). 

[17]The study also found that
owners that used rent-setting software decreased rents to increase occupancy
rates during economic downturns. 

[18]We previously reported that
accuracy in facial recognition has been found to be variable and
demographically biased. See GAO, Biometric
Identification Technologies: Considerations to Address Information Gaps and
Other Stakeholder Concerns, GAO‑24‑106293
(Washington, D.C: Apr. 22, 2024). 

[19]For additional information on
the accuracy of facial recognition technology across demographics, see GAO, Facial Recognition Technology: Privacy and Accuracy Issues
Related to Commercial Uses, GAO‑20‑522 (Washington,
D.C.: July 13, 2020). 

[20]We limited our discussion of
agency actions to finalized legal judgments and executed compliance agreements.
Appendix II includes information on currently active legal actions initiated by
HUD, DOJ, and FTC related to the proptech tools we reviewed. Executive Order
14281 requires agencies to evaluate existing consent judgments and permanent
injunctions that rely on theories of disparate impact liability and to take
appropriate actions with respect to matters consistent with the order. The
order eliminates the use of disparate impact liability in all contexts to the
maximum degree possible. The agencies have 90 days from April 23, 2025 to
complete the review. The agencies may make decisions that impact the cases
listed in this report. As of May 6, 2025, the agencies have not made public
notification of any changes to these closed cases based on Executive Order
14281. 

[21]Fed. Trade Comm’n et al. v.
Roomster, 1:22-cv-07389 (S.D.N.Y.). Separately, another defendant was ordered
to stop selling consumer reviews or endorsements and fined.

[22]Facebook, a social media
platform owned by Meta, displays advertisements to users, including those for
rental housing opportunities. 

[23]United States v. Meta
Platforms, Inc, f/k/a Facebook, Inc., No. 1:22-cv-05187 (S.D.N.Y). 

[24]Department of Housing and
Urban Development, Guidance on Application of the
Fair Housing Act to the Advertising of Housing, Credit, and Other Real
Estate-Related Transactions through Digital Platforms (Washington, D.C.:
April 2024). 

[25]Department of Housing and
Urban Development, Guidance on the Application of
the Fair Housing Act to the Screening of Applicants for Rental Housing
(Washington, D.C.: April 2024). HUD issued the guidance in response to
Executive Order 14110, which required HUD to issue guidance on the use of
artificial intelligence in housing decisions. Executive Order 14110 was
rescinded on January 20, 2025, by Executive Order 14148.

[26]The White House, Ending Radical and Wasteful Government DEI Programs and
Preferencing, Executive Order 14151 (Washington, D.C.: Jan. 20, 2025).

[27]United States v. AppFolio,
Inc., No. 1:20-cv-03563 (D.D.C.). 

[28]Fed. Trade Comm’n v.
TransUnion Rental Screening Sol., No. 1:23-cv-02659 (D. Colo.). See also, In re TransUnion, 2023-CFPB-0011 (Oct. 12, 2023).

[29]Fed. Trade Comm’n v. Instant
Checkmate, LLC et al., No. 3:23-cv-01674 (S.D. Cal.). The five affiliated
companies are Instant Checkmate, Truthfinder, Intelicare Direct, The Control
Group Media Company, and PubRec. FTC alleged in its complaint that all five
companies operated as a common enterprise while engaging in the alleged
unlawful acts and practices.   

[30]Louis
et al v. SafeRent Sol., LLC and Metro. Mgmt. Grp., No. 1:22-cv-10800 (D.
Mass.) DOJ is authorized under 28 U.S.C. §§ 516 and 517 to file statements of
interest in federal and state court cases between private parties in which the
United States has an interest.

[31]In July 2023, the court denied
the defendants’ motions to dismiss the Fair Housing Act claim and permitted the
case to proceed against SafeRent and the other defendants. The court dismissed
certain counts against SafeRent that were unrelated to the Fair Housing Act.

[32]Department of Housing and
Urban Development, Office of Public and Indian Housing, Letter to Public Housing Agencies (Sep. 22, 2023).
As stated previously, as of May 2025, approximately 970,000 households were
living in public housing. As administrator of the public housing program, HUD
is responsible for providing guidance to PHAs on operations, including use of
surveillance technology. 





[35]The industry associations are
the Consumer Data Industry Association, National Apartments Association,
National Multifamily Housing Council, and Security Industry Association. The
advocacy organizations are the National Fair Housing Alliance, National Low
Income Housing Coalition, American Civil Liberties Union, Center for Democracy
and Technology, and National Consumer Law Center. 

[36]The two Fair Housing
Initiative Program organizations are Brooklyn Legal Service and the Fair
Housing Justice Center. 

[37]GAO, Standards for Internal Control in the Federal Government,
GAO‑14‑704G
(Washington, D.C.: Sept. 10, 2014). 

[38]We did not include HUD’s
Office of Multifamily Housing in our review because it is not directly
responsible for overseeing the use of the selected proptech tools in subsidized
properties, according to officials. 

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Have to Feel It]]></title>
            <link>https://mitchellh.com/writing/feel-it</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075048</guid>
            <description><![CDATA[You see a series of checkboxes checked. Schedule met.
Documented requirements satisfied. Demo video delivered.
It's a good day. Good job, you, good job! A promotion is in sight.]]></description>
            <content:encoded><![CDATA[You see a series of checkboxes checked. Schedule met.
Documented requirements satisfied. Demo video delivered.
It's a good day. Good job, you, good job! A promotion is in sight.
But you didn't feel it. You didn't feel it.
We, as people, feel something with every interaction. Frustration, joy, relief,
confidence. A feeling. A person interacts with our work. Our work evokes
a feeling. The feeling matters. The feeling is part of the work. The
desired feeling is part of the requirements.
When you feel it, you know. The feature makes you smile when you use it.
It fits right in, like it was always meant to be there. You want to
use it again. You want to tell people about it.
This is the difference. This is what metrics, specifications, and demos
miss. They don't capture the feeling. For the people who will use and live
in the work, the feeling is part of their daily experience. Which means
you can't stop at checking the boxes on paper. You have to sit with it,
use it, live with it.
You have to feel it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Condor's Cuzco RISC-V Core at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074895</guid>
            <description><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive.]]></description>
            <content:encoded><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache missesIf you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI models need a virtual machine]]></title>
            <link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074467</guid>
        </item>
        <item>
            <title><![CDATA[Bcachefs Goes to "Externally Maintained"]]></title>
            <link>https://lwn.net/Articles/1035736/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074312</guid>
            <description><![CDATA[Linus Torvalds has quietly changed the maintainer status of bcachefs to 'externally maintained' [...]]]></description>
            <content:encoded><![CDATA[
That's exactly why I've resisted the push to put someone else in that role. If it's one of the other people working on bcachefs I don't want to risk them burning out and losing another engineer; I'd be ok with it if it was another long standing member of the kernel community with experience working with Linus, but for some strange reason no one I've talked to wants to take that on. 
And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.

Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.

So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.

      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 4:57 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [Link] (1 responses)
      
      
      
> And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.
>
> Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.
The distros are downstreams. If they want to package and ship your code, in whatever way they see fit, you've already given them permission to do so. And they are not shy about exercising that permission. I remember several years ago, jwz asked Debian to stop shipping XScreenSaver, because he disagreed with their practice of backporting fixes to old versions. Debian said no, and XScreenSaver is still in the repository today. As you might imagine, some rather harsh words were exchanged, but in the end, both sides went back to their respective corners of the internet and proceeded to mostly ignore each other.

Linus, however, is not in the business of playing that game. If there's nobody actively maintaining (his copy of) bcachefs, then I find it hard to believe it's going to be allowed to stick around indefinitely.

> So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.

My interpretation of events is that there are only three long-term paths that make sense here:

* You accept that you cannot control what appears in Linus's tree, but would prefer that some recent-ish version of bcachefs is there (as opposed to no bcachefs or a very old bcachefs). You designate somebody as I've described upthread, they upstream patches at whatever rate Linus is willing to take them, and everybody is more or less willing to live with the result.
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They continue to ship an old bcachefs for (at least) the rest of the current release cycle, but eventually it bitrots and they delete it. You might or might not choose to ship it out-of-tree like ZFS, and various distros might or might not package some version of it for you (whether you want them to or not).
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They fork bcachefs or mirror it from your out-of-tree version, and slightly-old or modified versions continue to appear in the kernel indefinitely. As I explained, I think this is less likely, but I don't want to entirely discount it.

I do not see any plausible outcome where you are allowed to control what appears in Linus's tree. He has very explicitly closed the door on that. For expository purposes, and because you are a functioning adult, I have assumed that you will accept this lack of control, but that does not actually matter - one of the above scenarios will inevitably play out, regardless of your opinion of it. The only choice you have at this point is whether it's the first bullet or one of the other two.

I do not say this to be cruel. Based on your words in this and other threads, I genuinely believe that this process has been very painful for you, and I doubt you enjoy being reminded that Linus's tree does, in fact, belong to Linus (I'm sure other developers have screamed that at you enough times by now). Unfortunately, this is not a matter of right or wrong. It is a matter of power. You are aggrieved about something that neither Linus, nor anybody else on LKML, is prepared to recognize as an injury to you. Regardless of whether that is the right way or the wrong way of looking at it, Linus is going to conduct the kernel's release cycle as he sees fit. The *healthy* way of looking at it is to accept that that is not within your power to change, and redirect your attention to the things you can change.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 16:30 UTC (Sat)
                               by ttuttle (subscriber, #51118)
                              [Link] 
      
      
      
Thank you for posting such a compassionate response. This thread could easily turn into an unkind discussion or an all-out flame war, but you went out of your way to be kind instead.


      
          
        
     

      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 7:57 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (6 responses)
      
      
      
> Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.
Debian was not even close to the topic at hand, and yet you felt the need to bring it up, just to attack someone, and with information that is misrepresenting the truth. This is something you've done before, and you were very recently called out in lkml for it. Stop.

To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. It was ultimately removed from unstable because noone was able to get through. Source: I am one of them.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 11:44 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (2 responses)
      
      
      
The specific, technical issue was the package maintainer switching out the Rust dependencies for the packaged versions from Debian. I explained that this was a bad idea at the outset, because it invalidated all the testing we do, and the Debian package wasn't replicating that testing; it was also wholly unnecessary because Rust dependencies are statically linked.
He did so anyways, and then swapped out bindgen for an old version that was explicitly unsupported according to the Cargo.toml, which broke the build, and he sat on it and Debian users stopped getting updates (I didn't even see a report until months later).

This resulted in users being unable to access their filesystems.

There was briefly a buggy version of bcachefs-tools that couldn't pass mount options correctly; users in every other distro got a fix quickly, but Debian users did not - and we found out about this when a lot of users weren't able to mount in degraded mode after having a drive die.

What you're doing is conflating technical criticism with personal, and then using that as an excuse to ramp up the drama. Technical criticism, including pointing out failures of processes, has to be ok for engineering to function, otherwise we don't learn from our mistakes. That can make for a harsh learning environment, but when you're shipping critical system components that have to work, that's what you signed up for; we have responsibilities.

The person in question was warned explicitly that what he was doing was a bad idea; he could have at any point said "this is too complicated an issue for me to handle; I'll let someone else take this one" (and there are mechanisms in Debian process for obtaining exceptions to process rules that could have avoided this, by simply skipping the Rust dependency unbundling with a clear explanation of why); he ignored advice and plowed ahead, and a lot of people were affected by those actions.

When we work on this kind of code, we have to be responsible for the work we do, including our mistakes.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 14:25 UTC (Sat)
                               by ma4ris8 (subscriber, #170509)
                              [Link] (1 responses)
      
      
      
My goal is to show the power of listening to the other. I'll try to listen first, and then answer.
I hope that you get my point of listening well in order to carefully heal the relationships.
Listen part: I'm trying to repeat roughly the same as you wrote above, to show that I listened you:

First you state that maintainer switched Rust dependencies for the packaged versions from Debian.
You explained that it was a bad idea, for multiple reasons: statically linked dependencies, and
invalidating all your active testing.

He changed Rust dependencies anyways, and then swapped out bindgen into older version,
which broke the build for Debian, and file system users stopped getting updates.

Important end question: Did I repeat (re-phrase in text) precisely what you wrote?

Answer part:
You wrote many items into one message. I answered only for the first one,
to keep the answer small enough. Some progress, but further messages
could increase coverage.

For me it sounds like there were some mistakes done by both you and others.
The unfortunate end result was, that Debian users had problems with the bug.
I didn't get from your message, the outcome of the relationships between persons:
whether personal relationships were worsened, stayed the same, or healed in the
end (each relation individually).

How to communicate (listen) effectively, to heal relationships?

This way of listening is mentioned in
https://www.verywellmind.com/what-is-active-listening-302...
"Paraphrasing and reflecting back what has been said"
( Those who know psychology, know these things ).

What I showed, is one way to restore human relationships, with Linus and others:
You could try to restore relationships with just listening others. Choose carefully
messaging cases, in which you think that you won't cause much backslash,
but you could have progress with healing the relationship by listening to the other.

If you get a backslash, you was just given an opportunity to listen the complaint.
Repeat in nearly the same words the whole complaint, 
so that the other one feels of being heard fully.
Try to at least have progress, thus please listen carefully the mentioned
complaint by repeating it. You can have pauses, like answering another day, to reduce the burden.
Please don't open up any new problems. If you do (I do mistakes sometimes),
and get a backslash as a heated answer, please listen and repeat it carefully,
to reduce the impact.

By doing this just very slightly to not burden others,
you could both improve your communication skills,
and perhaps others could learn from it too,
and perhaps then relations with other stakeholders, like maintainers,
and Linus, could be restored into a level that you can co-operate efficiently together again.

I've seen that sometimes this listening technique helps on-line, in addition of meeting face to face.
I'm trying to improve my communicating skills in the contexts of
change management for "OWASP top 10", and AI adoption.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:21 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] 
      
      
      
>  For me it sounds like there were some mistakes done by both you and others.
Why are you trying to bothsides this?

You seem to have the facts straight, but I'm not at all clear on what you think I did wrong.

All this was explained clearly, calmly and patiently to the Debian package maintainer when he started; he decided to do it his way, and when the breakage became apparent I asked if he was going to fix it and he just said "nope, too complicated" and walked off. So I got stuck with warning bcachefs users away from Debian, and he wrote a screed of a blog post about how impossible I am to work with.

Sorry, but from where I sit that just looks crazy.

I'm all about focusing on the human aspect, sitting down with people and having open and honest conversations. I do that regularly, and believe me I and others have tried ratcheting down the tensions, bringing the focus back to the technical and looking for ways to make this easier and take things in little steps.

The whole rest of the 6.16 merge cycle after the journal_rewind fiasco was just that, from myself and others; we've tried to bridge the gap, bring the focus back to the technical, look for ways to make things work - it doesn't seem to be getting us anywhere.


      
          
        
     



    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 12:24 UTC (Sat)
                               by muase (subscriber, #178466)
                              [Link] (2 responses)
      
      
      
Hm, for me it simply reads like OP is mentioning his frustration about release cycles, and is citing a pretty legitimate example: The fact that developers are overwhelmed with obsolete bug reports, because LTS distros are months or even years behind, is not something new and a real problem for some projects.
I know it's not the distros' fault; it's simply how LTS has to work in practice – however I can understand the frustration that arises if there seems to be an opportunity to finally update a package(set)... and then that opportunity is missed, and now the dev knows that they have to endure those obsolete bug reports for another n-year release cycle. It definitely didn't read as "just to attack someone".

> To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another.

Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case.  If the communication is public, do you have a link or something? :)


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:27 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (1 responses)
      
      
      
> Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case. If the communication is public, do you have a link or something? :)
Kent in https://lore.kernel.org/linux-bcachefs/wona7sjqodu7jgchtx... called part of a maintainer's job as "bullshit, make-work job", told Debian to "develop a better and more practical minded attitude" and to "stop wasting my time with this stupid bullshit and I can get back to real work". The issues we had spent a lot of our volunteer time to fix were very real issues, many of them upstream, and one in the Rust ecosystem. At the time this was sent, all issues were fixed, or were on the way to be fixed, and a recent bcachefs-tools package with all of the appropriate dependencies was a few weeks away from getting to Debian testing.

bcachefs-tools was orphaned by its maintainer a few weeks later; myself and another contributor (the two of us had done all recent advancements), stopped investing our time as well. The package has remained orphaned since, for about a year. Anyone can pick it up, but noone has, and that's not because of technical difficulties (as far as packages go, it's pretty trivial).

As an aside, the very existence of this thread was as a "PSA" to his users to avoid Debian and Fedora, telling them that "you'll want to be on a more modern distro". *Two weeks later*, he responded in https://lore.kernel.org/lkml/nxyp62x2ruommzyebdwincu26kmi... to Linus that he expects the "major distros" to pick up bcachefs soon. Whether he was dishonest or just naive, I'll leave that to your judgement.

The above was just a small sample. There were literally dozens of responses of this style at time, random offensive comments etc., across multiple mediums (mailing lists, IRC, Reddit, etc.). I am not keeping a file though, as I don't feel the need to convince anyone with hard evidence. You don't know me, and I understand that my opinion may not be of much value to you. I hope, though, that you and others may see this as one tiny part of a broader pattern of countless long-time contributors across multiple projects expressing that they have been alienated and driven away by Kent's conduct and sense of entitlement, and that they have good reasons for it.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 19:20 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] 
      
      
      
So yes, I could have been more diplomatic in my response.
But please do try to put yourself in my shoes; that was after getting a bunch of bug reports from Debian users, and there had been a _lot_ of fail at that point in how the Debian packaging was handled.

I do have to reiterate: the unbundling of Rust dependencies should not have happened for bcachefs-tools, there was no technical reason for that, all my explanations were met with "but that's our policy", and no amount of reasoning was getting anywhere; and the Debian packager breaking the build and sitting on it just should not have happened.

I do sincerely hope you can analyze how things went from the other end and ask yourself what could have been done better to avoid this, because from my end, this was an intensely frustrating issue, and it wasn't being taken seriously and it had very real effects.

Before you start focusing on language and diplomacy, you really need ask yourself if the technical decisionmaking leading up to that point was sound. When we get breakage as bad as what happened with the Debian package, you can expect the kind of frustration I was voicing there, and "bullshit, make-work projects" still seems to accurately describe what Debain's been doing with Rust dependency unbundling.

When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities.


      
          
        
     



      A few suggestions (which you don’t have to follow)
       Posted Aug 30, 2025 17:12 UTC (Sat)
                               by DemiMarie (subscriber, #164188)
                              [Link] 
      
      
      
My recommendation is to have an out-of-tree repository that has the latest changes and to send them to Linus at a pace Linus is okay with.  Users who need the very latest code can use the driver from your tree via DKMS or similar.  I also recommend having a way to implement new features that are not on hot paths (such as recovery) in userspace.  Finally, a FUSE version would be a good idea and (as you mentioned earlier IIRC) make kernel changes that are not corruption or crash fixes less urgent, because users could recover in userspace and even access their data (albeit more slowly) while that is happening.
For anything that has to happen before the filesystem can be accessed at all, it might make sense to have an option for the userspace mount helper to do the work.  In this case, the userspace helper has far fewer disadvantages I know of.

My dream would be for bcachefs to have SQLite’s level of testing and input validation, or (even better) formal verification.   Either would massively reduce the rate of bugs making it into a release, but neither is reasonable to ask for outside of a suitably-priced commercial engagement.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:53 UTC (Sat)
                               by ATLief (subscriber, #166135)
                              [Link] 
      
      
      
> I'd be ok with it if it was another long standing member of the kernel community with experience working with Linus, but for some strange reason no one I've talked to wants to take that on.
That’s entirely expected given the circumstances; if longstanding Linux developers collectively don’t want to work with you, then they wouldn’t want to individually work with you either.


      
          
        
     ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cognitive Load is what matters]]></title>
            <link>https://github.com/zakirullin/cognitive-load</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074248</guid>
            <description><![CDATA[🧠 Cognitive Load is what matters. Contribute to zakirullin/cognitive-load development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Cognitive Load is what matters
Readable version | Chinese translation | Korean translation | Turkish translation
It is a living document, last update: August 2025. Your contributions are welcome!
Introduction
There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.
Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.
Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.
Cognitive load

Cognitive load is how much a developer needs to think in order to complete a task.

When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.
Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.


We should reduce the cognitive load in our projects as much as possible.

  Cognitive load and interruptions
  

Types of cognitive load
Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.
Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.


Let's jump straight to the concrete practical examples of extraneous cognitive load.

We will refer to the level cognitive load as follows:
🧠: fresh working memory, zero cognitive load
🧠++: two facts in our working memory, cognitive load increased
🤯: cognitive overload, more than 4 facts

Our brain is much more complex and unexplored, but we can go with this simplistic model.

Complex conditionals
if val > someConstant // 🧠+
    && (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    && (condition4 && !condition5) { // 🤯, we are messed up by this point
    ...
}
Introduce intermediate variables with meaningful names:
isValid = val > someConstant
isAllowed = condition2 || condition3
isSecure = condition4 && !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid && isAllowed && isSecure {
    ...
}
Nested ifs
if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} 
Compare it with the early returns:
if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+
We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.
Inheritance nightmare
We are asked to change a few things for our admin users: 🧠
AdminController extends UserController extends GuestController extends BaseController
Ohh, part of the functionality is in BaseController, let's have a look: 🧠+
Basic role mechanics got introduced in GuestController: 🧠++
Things got partially altered in UserController: 🧠+++
Finally we are here, AdminController, let's code stuff! 🧠++++
Oh, wait, there's SuperuserController which extends AdminController. By modifying AdminController we can break things in the inherited class, so let's dive in SuperuserController first: 🤯
Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.
Too many small methods, classes or modules

Method, class and module are interchangeable in this context

Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.
Deep module - simple interface, complex functionality
Shallow module - interface is relatively complex to the small functionality it provides


Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.

Information hiding is paramount, and we don't hide as much complexity in shallow modules.

I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.
Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.

The best components are those that provide powerful functionality yet have a simple interface.
John K. Ousterhout

The interface of the UNIX I/O is very simple. It has only five basic calls:
open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)
A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.

This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.

P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.
Responsible for one thing
All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.
We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.

A module should be responsible to one, and only one, user or stakeholder.

This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.
But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.
Too many shallow microservices
This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.
I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. Diagnostic difficulty in integration space skyrocketed. Both time to market and cognitive load were unacceptably high. 🤯
Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.
The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.
A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.
Feature-rich languages
We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.
If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!
You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. 🤯
These statements are made by none other than Rob Pike.

Reduce cognitive load by limiting the number of choices.

Language features are OK, as long as they are orthogonal to each other.

  Thoughts from an engineer with 20 years of C++ experience ⭐️
  
  I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!
  I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.
  Like, can you imagine, the token || has a different meaning in requires ((!P<T> || !Q<T>)) and in requires (!(P<T> || Q<T>)). The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.
  You can't allocate space for a trivial type and just memcpy a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.
  Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. 🤯
  There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then... 🤯
  This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).
  I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.
  By no means I am trying to blame C++. I love the language. It's just that I am tired now.Thanks to 0xd34df00d for writing.

Business logic and HTTP status codes
On the backend we return:
401 for expired jwt token
403 for not enough access
418 for banned users
The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:
401 is for expired jwt token // 🧠+, ok just temporary remember it
403 is for not enough access // 🧠++
418 is for banned users // 🧠+++
Frontend developers would (hopefully) introduce some kind numeric status -> meaning dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.
Then QA engineers come into play:
"Hey, I got 403 status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.
Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:
{
    "code": "jwt_has_expired"
}
Cognitive load on the frontend side: 🧠 (fresh, no facts are held in mind)
Cognitive load on the QA side: 🧠
The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.

People spend time arguing between 401 and 403, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.

P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.
Abusing DRY principle
Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.
Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. 🤯
In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.
Rob Pike once said:

A little copying is better than a little dependency.

We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.
All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.
Tight coupling with a framework
There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.
Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. 🤯
By no means do we advocate to invent everything from scratch!
We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.

Why I Hate Frameworks

Layered architecture
There is a certain engineering excitement about all this stuff.
I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. 🤯
Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. 🤯
This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.

  Coding principles and experience
  
  @flaviocopes

If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.

With a sufficient number of users of an API,
it does not matter what you promise in the contract:
all observable behaviors of your system
will be depended on by somebody.

We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.
So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.
These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.
Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.
Layers of abstraction aren't free of charge, they are to be held in our limited working memory.


Domain-driven design
Domain-driven design has some great points, although it is often misinterpreted. People say "We write code in DDD", which is a bit strange, because DDD is about problem space, not about solution space.
Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.
Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. 🤯
Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.
Cognitive load in familiar projects

The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!
It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.
In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”
There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.
Thanks to Dan North for his comment.

If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.


The more mental models there are to learn, the longer it takes for a new developer to deliver value.
Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.
If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.
Examples

Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres
How Instagram scaled to 14 million users with only 3 engineers
The companies where we were like ”woah, these folks are smart as hell” for the most part failed
One function that wires up the entire system. If you want to know how the system works - go read it

These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.
Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.
Conclusion
Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. 🤯
Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.


We should reduce any cognitive load above and beyond what is intrinsic to the work we do.

LinkedIn, X, GitHub
Readable version

    Comments
    
    Rob PikeNice article.
    Andrej Karpathy (ChatGPT, Tesla)Nice post on software engineering. Probably the most true, least practiced viewpoint.
    Elon MuskTrue.
    Addy Osmani (Chrome, the most complex software system in the world)I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.
    The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."
    What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.
    And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.
    Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.
    One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.
    Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. 
    Sometimes the simplest solution is the best one, even in a complex system.
    antirez (Redis)Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.
    A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.
    Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).
    These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D
    A developer from the internetYou would not hire me... I sell myself on my track record of released enterprise projects.
    I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.
    I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.
    I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.
    Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.
    Comments on Hacker News

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FBI cyber cop: Salt Typhoon pwned 'nearly every American']]></title>
            <link>https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074157</guid>
            <description><![CDATA[: Plus millions of other people across 80+ countries]]></description>
            <content:encoded><![CDATA[
China's Salt Typhoon cyberspies hoovered up information belonging to millions of people in the United States over the course of the years-long intrusion into telecommunications networks, according to a top FBI cyber official.
"There's a good chance this espionage campaign has stolen information from nearly every American," Michael Machtinger, deputy assistant director for the FBI's cyber division, told The Register.
"There's a thought among the public that if you don't work in a sensitive area that the PRC might be interested in for its traditional espionage activities, then you are safe, they will not target you," he said, during a Thursday interview with The Register. "As we have seen from Salt Typhoon, this is no longer an assumption that anyone can afford to make."

    

The Beijing-backed spying campaign began at least in 2019 but wasn't uncovered by US authorities until last fall. On Wednesday, US law enforcement and intelligence agencies along with those from 12 other countries warned the ongoing espionage activity expanded far beyond nine American telcos and government networks. According to Machtinger, at least 80 countries were hit by the digital intrusions.

        


        

Around 200 American organizations were compromised by the espionage activity, Machtinger said, including the previously disclosed telecommunications firms such as Verizon and AT&T.
Yesterday's joint security alert also pointed the allies' collective finger at three China-based entities affiliated with Salt Typhoon: Sichuan Juxinhe Network Technology, Beijing Huanyu Tianqiong Information Technology, and Sichuan Zhixin Ruijie Network Technology. These companies, and likely others, provide cyber products and services to China's Ministry of State Security and People's Liberation Army, the governments said.


What the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space

"This is one of the most consequential cyber espionage breaches that we've ever seen in the United States," Machtinger said.
"What this really underscores is that what the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space," he added. "And that should really set off alarm bells for us — not only in the United States. The scale of indiscriminate targeting is unlike what we've seen in the past."

        

This indiscriminate targeting, as the FBI and White House security officials have previously noted, allowed Beijing’s snoops to geo-locate millions of mobile phone users, monitor their internet traffic, and, in some cases, record their phone calls. Victims reportedly included President Donald Trump and Vice President JD Vance.
Machtinger declined to confirm whether Trump and Vance were among those surveilled, but did say that victims included more than 100 current and former presidential administration officials.
"As we look at the impact on the different sets of victims," he said, Salt Typhoon collected "bulk information from millions of Americans."

        

For the more targeted group of individuals, "most of whom are very high-profile, current and former presidential administration officials, and campaign appointees from both major political parties," the data collection went much deeper, Machtinger added. "Down to intercepting actual content."


If you thought China's Salt Typhoon was booted off critical networks, think again

China's Salt Typhoon spies spotted on US govt networks before telcos, CISA boss says

This is the FBI, open up. China's Volt Typhoon is on your network

How does China keep stealing our stuff, wonders DoD group responsible for keeping foreign agents out

In addition to Salt Typhoon, the feds over the past year have issued warnings about other Chinese cyber operations. These include Volt Typhoon intruders, who infected hundreds of outdated routers to build a botnet and break into US critical infrastructure facilities. The Beijing-backed crew, we would later learn, was prepositioning itself and readying destructive cyberattacks.
Another China-linked crew, Silk Typhoon has spent more than a decade compromising IT and cloud providers to steal sensitive data from their government, technology, education, and legal and professional services customers.
China is not the only source of threats, Machtinger noted. Russia, Iran, North Korea, plus along with home-grown and international cybercriminals and ransomware crooks, assault computers and networks of both individuals and organizations, every day.
"These actors are going to continue their efforts, and they're going to get more sophisticated," Machtinger said. "We need to make sure that we, a nation, are taking cybersecurity seriously, updating systems, removing end-of-life devices, and making it as hard and costly as possible for the myriad of actors that are out there to successfully compromise." ®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Client Protocol]]></title>
            <link>https://agentclientprotocol.com/overview/introduction</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074147</guid>
            <description><![CDATA[Get started with the Agent Client Protocol (ACP)]]></description>
            <content:encoded><![CDATA[The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).
The protocol is still under development, but it should be complete enough to build interesting user experiences using it.Why ACP?
AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.
This creates several problems:
Integration overhead: Every new agent-editor combination requires custom work
Limited compatibility: Agents work with only a subset of available editors
Developer lock-in: Choosing an agent often means accepting their available interfaces

ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.
Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.
This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.Overview
ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.
Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.
The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.Supported Editors

Zed
neovim through the CodeCompanion plugin

Supported Agents

Gemini
… more coming soon ;)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[F-Stack – A network development kit with high performance based on DPDK]]></title>
            <link>https://www.f-stack.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074115</guid>
            <description><![CDATA[F-Stack Official Home Page]]></description>
            <content:encoded><![CDATA[
                    
                        
                                Introduction
                                With the rapid development of NIC, the poor performance of data packets processing with Linux kernel has become the bottleneck. However, the rapid development of the Internet needs high performance of network processing, kernel bypass has caught more and more attention. There are various similar technologies appear, such as DPDK, NETMAP and PF_RING. The main idea of kernel bypass is that Linux is only used to deal with control flow, all data streams are processed in user space. Therefore, kernel bypass can avoid performance bottlenecks caused by kernel packet copy, thread scheduling, system calls and interrupt. Furthermore, kernel bypass can achieve higher performance with multi optimizing methods. Within various techniques, DPDK has been widely used because of its more thorough isolation from kernel scheduling and active community support.
                                F-Stack is an open source network framework with high performance based on DPDK， include an user space TCP/IP stack(port FreeBSD 11.0 stable), Posix API(Socket, Epoll, Kqueue), Progamming SDK(Coroutine) and some apps(Nginx, Redis) interface.
                            
                        
                            
                        
                    
                    
                                F-Stack with follow characteristics
                                
                                    Ultra high network performance which can achieve network card under full load, 10 million concurrent connection, 5 million RPS, 1 million CPS.
                                    Transplant FreeBSD 11.01 user space stack, provides a complete stack function, cut a great amount of irrelevant features. Therefore greatly enhance the performance.
                                    Support Nginx, Redis and other mature applications, service can easily use F-Stack
                                    With Multi-process architecture, easy to extend
                                    Provide micro thread interface. Various applications with stateful app can easily use F-Stack to get high performance without processing complex asynchronous logic.
                                    Provide Epoll/Kqueue interface that allow many kinds of applications easily use F-Stack
                                
                                Currently, there are various products in Tencent Cloud has used the F-Stack, such as DKDNS(DNSPod's authorization DNS server), HttpDNS (D+), COS access module, CDN access module, etc..
                            
                ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nokia’s legendary font makes for a great user interface font]]></title>
            <link>https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074071</guid>
            <description><![CDATA[Home > OS News > It turns out Nokia’s legendary font makes for a great general user interface font]]></description>
            <content:encoded><![CDATA[ Home > OS News > It turns out Nokia’s legendary font makes for a great general user interface fontIf you’re of a certain age (and not American), there’s a specific corporate font you’re most likely aware of. You may not know its exact name, and you may not actively remember it, but once you see it, you know exactly what you’re looking at. The font’s called Nokia Sans (and Nokia Serif), and it was used by pretty much every single Nokia device between roughly 2002 and 2013 or so, when it was replaced by a very bland font made by Bruno Maag (with help from the person who designed Comic Sans) that they used after that.I can’t remember why, exactly, but I got majorly nostalgic for Nokia’s characteristic, recognisable font, and decided to see if it would work as a user interface font. Now, the font is still owned by Nokia and I couldn’t find a proper place to download it, but I eventually stumbled upon a site that had each individual variant listed for download. I downloaded each of them, installed them using KDE’s font installation method, and tried it out as my user interface font.You’ll quickly discover you shouldn’t use the regular variant, but should instead opt for the Nokia Sans Wide variant. Back in 2011, when Nokia originally announced it was replacing Nokia Sans, the creator of the font, Erik Spiekermann, responded to the announcement on his blog. Apparently, one of the major reasons for Nokia to change fonts was that they claimed Nokia Sans wouldn’t work as a user interface font, but Spiekermann obviously disagrees, pointing specifically to the Wide variant. In fact, Spiekermann does not pull any punches.After 10 years it was high time to look at Nokia’s typefaces as the dominant visual voice of the brand but whoever decided on a completely new direction was either not aware of what was available or was persuaded by Bruno Maag to start over. Bruno may not create the most memorable typefaces, but he certainly knows how to sell them. And technically, their fonts are excellent. Too bad they didn’t have the confidence to work with me on an update. Instead they’re throwing out ten years of brand recognition in favour of blandness.
↫ Erik SpiekermannI was pleasently surprised by just how nice the font looks when used as a general user interface font. It’s extremely legible at a variety of sizes, and has a ton of character without becoming gimmicky or overbearing. What originally started as mere curiosity has now become my UI font of choice on all my machines, finally displacing Inter after many years of uncontested service. Of course, all of this is deeply personal and 95% an issue of taste, but I wanted to write about it to see if I’m just entirely crazy, or if there’s some method to my madness.Do note that I’m using high DPI displays, and KDE on Wayland, and that all of this may look different on Windows or macOS, or on displays with lower DPI. One of Inter’s strengths is that it renders great on both high and lower DPI displays, but since I don’t have any lower DPI displays anymore, I can’t test it in such an environment. I’m also not entirely sure about the legal status of downloading fonts like this, but I am fairly sure you’re at least allowed to use non-free fonts for personal, non-commercial use, but please don’t quote me on that. Since downloading each variant of these Nokia fonts is annoying, I’d love to create and upload a zip file containing all of them, but I’m sure that’s illegal.I’m not a font connoisseur, so I may be committing a huge faux pas here? Not that I care, but reading about font nerds losing their minds over things I never even noticed is always highly entertaining.About The Author
Thom HolwerdaFollow me on Mastodon @[email protected]]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hardening Firefox – a checklist for improved browser privacy]]></title>
            <link>https://andrewmarder.net/firefox/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45073746</guid>
            <description><![CDATA[A checklist for configuring Mozilla Firefox for a more private browsing experience.]]></description>
            <content:encoded><![CDATA[  July 3, 2025 /  3 min read  
Updated:
August 29, 2025    This checklist will walk you (and me) through the settings and extensions I use to improve my privacy when using Firefox.
If you’re looking for a web browser that offers a high degree of privacy out of the box with minimal setup, Brave is a common choice. However, I prefer Firefox for several reasons:

Firefox is developed by the nonprofit organization Mozilla.
I value Mozilla’s commitment to open source software.
Firefox is not based on Chromium. Brave, like most browsers, is based on Chromium, which is developed primarily by Google.

While there are many web browsers to choose from, I’ve decided Firefox is best for me. This post is a checklist of how I’ve configured it to better protect my privacy while browsing the web.
1. Basic Privacy Settings
Access Firefox’s settings by clicking the menu button (three horizontal lines) in the top-right corner and selecting “Settings.”

 Change Default Search Engine: In the Search tab, change the “Default Search Engine” to a privacy-respecting option like DuckDuckGo.
 Enable HTTPS-Only Mode: In the Privacy & Security tab, scroll down to “HTTPS-Only Mode” and select “Enable HTTPS-Only Mode in all windows.”
 Disable Telemetry: Still in Privacy & Security, scroll to “Firefox Data Collection and Use” and uncheck all the boxes to stop Firefox from sending data back to Mozilla.
 Set Enhanced Tracking Protection to Strict: Under Privacy & Security, set “Enhanced Tracking Protection” to Strict. This offers stronger protection against trackers. If a site breaks, you can easily disable it for that specific site by clicking the shield icon in the address bar.

2. Recommended Extensions

 Install uBlock Origin: A comprehensive content blocker that stops ads and tracking scripts, which speeds up page loading and enhances privacy.
 Install ClearURLs: This extension automatically removes tracking elements from URLs, helping prevent another form of web tracking.
 Install Privacy Badger: From the Electronic Frontier Foundation, this extension automatically learns to block invisible trackers. Instead of relying on blocklists, it discovers trackers based on behavior.

3. Advanced Configuration (about:config)
To access this, type about:config into the address bar and accept the warning.
Warning: Changing advanced configuration preferences can impact Firefox performance or security. Proceed with caution.

 Isolate Cookies to the First-Party Domain:

Search for privacy.firstparty.isolate and set its value to true.
This prevents cookies from tracking you from one site to another, but it can break single sign-on on some websites.


Resist Fingerprinting:

I previously set privacy.resistFingerprinting to true to make my browser fingerprint less unique.
It caused minor display issues on some sites and broke image uploads to Bluesky, so I set it back to false.



By following this checklist, you can significantly improve your privacy while using Firefox. Please let me know if I’m missing anything in the comments.
     ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms]]></title>
            <link>https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45072160</guid>
            <description><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms
What is attention?
In any autoregressive model, the prediction of the future tokens is based on some preceding context …]]></description>
            <content:encoded><![CDATA[7 min read15 hours ago--Press enter or click to view image in full sizeWhat is attention?In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.“The animal didn’t cross the street because it was too tired”.In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.How is attention calculated?There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.Key Concepts in Attention MechanismsBefore diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vectorKV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.To understand how each of these vectors are scores are calculated you can refer to this blog.The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.Now, let's dive into each of these techniquesMulti-Head Attention (MHA)In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.Press enter or click to view image in full sizeImage credits: Illustrated TransformersIn multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.Models using MHA – Bert, RoBerta, T5, etc.Multi-Query Attention (MQA)A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHAModels using MQA – PaLM, FalconGrouped Query Attention (GQA)Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.Models using GQA – Llama2, Llama3, MistralMulti-Head Latent Attention (MHLA)While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.So during the inference:A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectorsThis is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectorsAdditionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embeddingAdditionally, the same process is done for attention Queries as well, which will reduce the activation memory during trainingPress enter or click to view image in full sizeMHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.Models using MHLA– Deepseek- V2, Deep seek V2ConclusionIn addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Theoretical Limitations of Embedding-Based Retrieval]]></title>
            <link>https://arxiv.org/abs/2508.21038</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068986</guid>
            <description><![CDATA[Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.
    

    
    
      
          Subjects:
          
            Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21038 [cs.IR]
        
        
           
          (or 
              arXiv:2508.21038v1 [cs.IR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.21038
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Orion Weller [view email]          [v1]
        Thu, 28 Aug 2025 17:43:53 UTC (195 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Do the simplest thing that could possibly work]]></title>
            <link>https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068091</guid>
            <description><![CDATA[When designing software systems, do the simplest thing that could possibly work. It’s surprising how far you can take this piece of advice. I genuinely think…]]></description>
            <content:encoded><![CDATA[When designing software systems, do the simplest thing that could possibly work.
It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.
A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.
Simple can be underwhelming
System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.
However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.
In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.
Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.
You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?
Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?
You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.
What’s wrong with doing the simplest thing?
Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.
Big balls of mud
To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.
But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.
It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.
What is simplicity?
Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:

Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them
Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces

Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.
What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.
When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.
Why wouldn’t you want to be scalable?
A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?
No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.
The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.
The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!
Final thoughts
The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.
There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.
edit: this article has gotten some comments on Hacker News.
One interesting comment thread says that simplicity of architecture doesn’t matter at scale, because the complexity of “state space exploration in implementation” (I think that means something like what I wrote about here) dominates any other complexity. I disagree - the more complex your feature interactions become, the more important a simple architecture becomes, because your “complexity budget” is almost exhausted.
I also want to credit Ward Cunningham and Kent Beck for inventing the expression - I genuinely thought I’d just come up with the wording myself, but I almost certainly just remembered it. Oops! Thanks to the HN user ternaryoperator for pointing this out.




It’s just Unix sockets and forked processes! I love Unicorn.
↩


Every tech company has some kind of edge proxy.
↩


I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).
↩


I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.
↩


Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.
↩


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[John Carmack's arguments against building a custom XR OS at Meta]]></title>
            <link>https://twitter.com/ID_AA_Carmack/status/1961172409920491849</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066395</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Wikipedia as a Graph]]></title>
            <link>https://wikigrapher.com/paths</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066060</guid>
        </item>
        <item>
            <title><![CDATA[Essential Coding Theory [pdf]]]></title>
            <link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065705</guid>
        </item>
        <item>
            <title><![CDATA[Deploying DeepSeek on 96 H100 GPUs]]></title>
            <link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45064329</guid>
            <description><![CDATA[<p>DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which us...]]></description>
            <content:encoded><![CDATA[DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.

Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs.
It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences.
To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale.
By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API.
Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x.
This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.
Highlight
✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.
✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.
✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.
✅ All experiments and code are fully open-sourced for community access and further development.
Outline

Parallelism Design
Prefill and Decode Disaggregation
Large-scale Expert Parallelism
Evaluation
Toolkits
Limitations and Future Work
Conclusion
Acknowledgment

Parallelism Design
Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.
Attention Layers
DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.
Dense FFNs
Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:

Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.
Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.
Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.

The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting --moe-dense-tp-size=1.

Sparse FFNs
In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.
The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.
LM Head
The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.
Prefill and Decode Disaggregation
LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.
Issues with Unified Scheduling
The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:

Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.
DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.
Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.

PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.
Implementation Details
The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:

Upon receiving an input request, the workflow proceeds as follows:

A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.
The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.
Once computed, the data transfers to the Decode Server, which handles iterative token generation.

This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:

Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.
RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.
Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.

More details can be found in our design document.
Large-scale Expert Parallelism
Expert Parallelism with DeepEP
DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.
DeepEP provides two specialized dispatch modes to address varying workload demands:

Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.
Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.

In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:



Mode
Long Input
Long Output
DP Attention
CUDA Graph




Normal
✅
❌
✅
❌


Low-Latency
❌
✅
✅
✅


Auto
✅
✅
❌
✅



PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.
DeepGEMM Integration
DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.

Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.
Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.

DeepGEMM integrates smoothly with the dispatch modes of DeepEP:

For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.
The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.

SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable SGL_ENABLE_JIT_DEEPGEMM to 1, offering even greater computational efficiency for non-MoE operations.
Two-batch Overlap
In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.
Implementation Challenges
Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.

Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.
Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.

Abstraction for Clean Implementation
To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:
operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)

Prefill Overlapping Implementation
We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:

The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.
An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.

To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).

Expert Parallelism Load Balancer
In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.
To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.
Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.
In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.

EPLB for Real-World Serving
For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:

Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).
Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.

Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.
Implementation of Rebalancing
SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:

System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.
Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.
Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.

This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.
Evaluation
End-to-end Performance
Experimental Setup
We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:

SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.
SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.
SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.
DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.

Performance Analysis of Prefill and Decode Phases
To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:

Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.
Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.


Profile Results
This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.
Overall Throughput
For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.
The results are presented below:




DeepSeek Blog (excl. cache hit)
DeepSeek Profile
SGLang (Default)
SGLang + Simulated Perfect EPLB




Batch Size
N/A
16,384
16,384
16,384


Input Length
N/A
4,096
4,096
4,096


Throughput (per node)
32,206
62,713
50,302
59,337



DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.
For decode, the results are shown below:




DeepSeek Blog
DeepSeek Profile
SGLang (Default)
SGLang + Simulated MTP (Slow Attention)




Batch Size
N/A
128
256
128


KV Cache Length
4,989
4,096
2,000
4,000


Number of Nodes
18
16
9
9


Throughput (per node)
14,800
18,598
22,282
17,373



Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.
Detail Breakdown
The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:


Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.
Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.
Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.

SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:

Key observations include:

Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.
MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.
Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.

While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.
Ablation Study: Two-batch Overlap
Impact of Batch Size and Attention Time
This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.

TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:

Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.
Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.

TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:

Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.
Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.

Detail Breakdown
We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:

TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.
Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.
Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.


For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:

TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.
TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.


Ablation Study: EPLB
This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.
Overall Results
The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.

Case Study: Workload Imbalance Versus Overall Throughput
To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:

The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.
Case Study: Expert Distribution Statistics
The following figure presents expert distribution statistics for prefill and decode sample data:

Key observations include:

Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.
Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.

These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.
Toolkits
Disposable Tensor
Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)

In this code, del hidden_state is intended to release the memory occupied by hidden_state after intermediate_state is computed. However, as hidden_state is still referenced outside the function, the del operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.
SGLang addresses this with the DisposableTensor class, a subclass of torch.Tensor which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)

By wrapping hidden_state in a DisposableTensor and calling dispose() when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.
Expert Workload Extraction and Simulation
SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:

Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.
Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.

This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.
Limitations and Future Work
While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:

Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.
Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.
Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.
EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.
Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.
Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.

Conclusion
By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.
Acknowledgment
We would like to express our heartfelt gratitude to the following teams and collaborators:

SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.
Atlas Cloud Team —  Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.
NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.
NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.
LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.
Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.
FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.
Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.

Thank you all for your invaluable support and collaboration.
Appendix
Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142
]]></content:encoded>
        </item>
    </channel>
</rss>