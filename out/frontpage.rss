<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Tue, 02 Sep 2025 15:51:10 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Americans Lose Faith That Hard Work Leads to Economic Gains, WSJ-NORC Poll Finds]]></title>
            <link>https://www.wsj.com/economy/wsj-norc-economic-poll-73bce003</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104082</guid>
        </item>
        <item>
            <title><![CDATA[You don't want to hire "the best engineers"]]></title>
            <link>https://www.otherbranch.com/shared/blog/no-you-dont-want-to-hire-the-best-engineers</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45103646</guid>
            <description><![CDATA[I think this might be the meanest thing I've ever written.]]></description>
            <content:encoded><![CDATA[“We only want to hire the best engineers”I hear this from almost every client I speak to. So does every other recruiter.Seriously - just say those eight words to any room full of recruiting people, and everyone will give a wry chuckle and roll their eyes. We've all heard it a million times.“We only want to hire the best engineers.”No. No, you do not.The best engineers make more than your entire payroll. They have opinions on tech debt and timelines. They have remote jobs, if they want them. They don’t go “oh, well, this is your third company, so I guess I’ll defer to you on all product decisions”. They care about comp, a trait you consider disqualifying. They can care about work-life balance, because they’re not desperate enough to feel the need not to. And however successful your company has been so far, they have other options they like better.You’re not stupid. If I asked you, point blank, “do you actually think the best engineers in the world would give your company a second thought,” I bet you could say “well, no, obviously not”.But you don’t act like it.You lock in the same set of criteria as every other startup. Experience at early stage. Highly independent. In-office in the Bay Area. Not too “salary motivated”. Don’t even apply if you want a 40h/week job - we work hard and play hard.Four months later, you haven’t found a good founding engineer. Do you know how long four months is in the life of a young startup? That’s an eternity, and you’ve spent it in stasis.Hiring is a negotiation, and you’re acting like you’re holding all the cards when you aren't. You’re looking for a highly competitive candidate pool, and you’re not being competitive: you’re just checking the same baselines as everybody else. You're acting like a replacement-level employer and expecting more than replacement-level candidates.Would you rather spend four months in stasis waiting for a senior candidate who hits the ground running on day one, or hire a skilled midlevel hacker who will be at full capacity in two weeks immediately?Would you rather spend four months in stasis waiting for a 50h/week candidate, or have a 40h/week candidate now?Would you rather be a green bar in this chart, or a red one?You don’t ask yourself these questions. You say “I want a candidate with these traits,” and sit on your hands until one materializes, until you run out of money, or - more likely - until someone manages to worm through your unrealistic expectations and convince you to compromise for them. If you had accepted compromise, you could have opened the floodgates on day one and had your pick of ten great-but-not-perfect candidates. Instead, you waited months and settled for one.When you accept that you need a great engineer, and not the best engineer, you can deal with the trade-offs consciously. What traits are actually important? How much are you willing to give up to get them? What’s the dollar value of a hire this month versus next month? “What actually matters today?” is the most important question a startup can ask, and you haven't applied it to one of the most important aspects of running a company!"Well, we're a little different from other companies, because we have really high standards."Does it sound like you're different?"We just raised a very exciting Series A!"So did literally a thousand other companies. There was $26B in early stage venture investment last quarter, and you can do the math as to how much of that your $10M raise occupies. The hires you need aren't looking at your company as the slam-dunk success that a founder necessarily needs to believe that it is. Maybe they will once they talk to you, but that's later - at the top of your funnel, you're just another face in the crowd, and you need to act like it.I’m not telling you to hire people who aren’t good. I’m not even telling you that the traits you want aren’t good things to look for. I’m not telling you to actually compromise on quality. I’m telling you that trying to hire the best engineers is the enemy of actually hiring great ones. You’re going to have to give up something (possibly time, possibly comp, possibly workplace policy) to make the hire you want.The longer you aren’t thinking about what to give up, the more you’re implicitly choosing to give up time, the thing startups treasure more than anything else. And you’re giving up time to - what, play it safe?The default outcome for a startup is always failure. You took a risk by even starting one. You ship things that might be broken all the time, because you know that speed is more important than perfection. You take moonshots, because you know that big wins matter more than small losses. And then you give up months of time because you refuse to apply the same philosophy to hiring!I run a recruiting company. It’s no skin off my back if you want to be irrational about hiring. Please, by all means, continue. You’re leaving a thousand great engineers to sit in my database instead of your ATS, and I would much rather you pay me 40 grand to find them than find them yourself.Or you can act like the scrappy realist you probably like to think you are, stop insisting on perfection, and move fast.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Imgur's Community Is in Full Revolt Against Its Owner]]></title>
            <link>https://www.404media.co/imgurs-community-is-in-full-revolt-against-its-owner/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45102905</guid>
            <description><![CDATA[The front page of the image hosting website is full of John Oliver giving the owner the middle finger.]]></description>
            <content:encoded><![CDATA[
        
          
              



The front page of Imgur, a popular image hosting and social media site, is full of pictures of John Oliver raising his middle finger and telling MediaLab AI, the site’s parent company, “fuck you.” Imgurians, as the site’s users call themselves, telling their business daddy to go to hell is the end result of a years-long degradation of the website. The Imgur story is one a classic case of enshitification,



Imgur began life in 2009 when Ohio University student Alan Schaaf got tired of how hard it was to upload and host images on the internet. He created Imgur as a simple one stop shop for image hosting and the service took off. It was a place where people could host images they wanted to share across multiple services and became ubiquitous on sites like Reddit.As the internet evolved, most of the rest of the internet got its act together and platforms built their own image sharing infrastructure and people used Imgur less. But the site still had a community of millions of people who shared images to the site every day. It was a social media  based around images and upvotes, with its own in-jokes, memes, and norms.In 2021, a media holding company called MediaLab AI acquired Imgur and Schaaf left. MediaLab AI also owns Genius and World Star and on its website, the company bills itself as a place where advertisers can “reach audiences at scale, on platforms that build community and influence culture.”The community and culture of Imgur, which MedialLab AI claims is 41 million strong, is pissed.For the last few days, the front page of Imgur (which cultivates the day’s “most viral posts”) has been full of anti MediaLab AI sentiment. Imgurian VoidForScreaming posted the first instance of the John Oliver meme several days ago, and it’s become a favorite of the community, but there are also calls to flood the servers and crash the site, and a list of grievances Imgurians broadly agree brought them to the place they’re in now.GhostTater, a longtime Imgurian, told me that the protest was about a confluence of things including a breakdown of the basic features of the site and the disappearance of human moderators. “The moderators on Imgur have always been active members of the community. Many were effectively public figures, and their sudden group absence was immediately noticed,” he said. “Several very well-known mods posted generic departure messages, smelling strongly of Legal Department approval. These mods had many friends and acquaintances on the site, and while some are still visiting the site as users, they have gone completely silent.”A former Imgur employee who spoke with 404 Media on the condition that we preserve their anonymity because they’re afraid of retaliation from MediaLab AI said that several people on the Imgur team were laid off without notice. Others were moved to MediaLab’s internal teams. “To the best of my knowledge, no employees are remaining solely focused on Imgur. Imgur's social media has been silent for a month,” the employee said. “As far as I am aware, the dedicated part-time moderation team was laid off sometime in the last 8 months, including the full-time moderation manager.”Imgurians are convinced that MediaLab AI has replaced those moderators with unreliable AI systems. The Community & Content Policy on MediaLab AI’s website says it employs human  moderators but also uses AI technologies. A common post in the past few days is Imgurians sharing the weird things they’ve been banned for, including one who made the comment “tell me more” under a post and others who’ve seen their John Olivers removed.“There were no humans responding to appeals or concerns,” GhostTater said. “Once the protest started, many users complained about posts being deleted and suspensions or bans being handed out when those posts were critical of MediaLab but not in violation of the written rules.”But this isn’t just about bad moderation. Multiple posts on Imgur also called out the breakdown of the site’s basic functionality. GhostTater told me he’d personally experienced the broken notification system and repeated failures of images to upload. “The big one (to me) is the fact that hosted video wouldn’t play for viewers who were not logged in to Imgur,” he said. “The site began as an image hosting site, a place to upload your images and get a link, so that one could share images.”MediaLab AI did not respond to 404 Media’s request for comment. “MediaLab’s presence has seemed to many users to fall somewhere between casual institutional indifference and ruthless mechanization. Many report, and resent, feeling explicitly harvested for profit,” GhostTater said.Like all companies, MediaLab AI is driven by profit. It makes money as a media holding company, scooping up popular websites and plastering them with ads. It also owns the lyrics sharing site Genius and the once-influential WorldStarHipHop. It’s also being sued by many of the people it bought these sites from, including Imgur’s founder. Schaaf and others have accused MediaLab AI of withholding payments owed to them as part of the sales deals they made.The John Olivers and other protest memes keep flowing. Some have set up alternative image sharing sites. “There is a movement rattling around in User Submitted calling for a boycott day, suggesting that all users stay off the site on September first,” GhostTater said. “It has some steam, but we will have to see if it gets enough buy-in to make an impact.”




                    
    
      About the author
      Matthew Gault is a writer covering weird tech, nuclear war, and video games. He’s worked for Reuters, Motherboard, and the New York Times.
      
    
        
      
  
          
        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Moribito – A TUI for LDAP Viewing/Queries]]></title>
            <link>https://github.com/ericschmar/moribito</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45102664</guid>
            <description><![CDATA[Contribute to ericschmar/moribito development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[森人 - Mori-bito (forest-person)

  

A terminal-based LDAP server explorer built with Go and BubbleTea, providing an interactive interface for browsing LDAP directory trees, viewing records, and executing custom queries.
Features

🌲 Interactive Tree Navigation: Browse LDAP directory structure with keyboard/mouse
📄 Record Viewer: View detailed LDAP entry attributes
📋 Clipboard Integration: Copy attribute values to system clipboard
🔍 Custom Query Interface: Execute custom LDAP queries with real-time results
📖 Paginated Results: Efficient pagination for large result sets with automatic loading
⚙️ Flexible Configuration: Support for config files and command-line options
🔐 Secure Authentication: Support for SSL/TLS and various authentication methods
🔄 Auto-Update Notifications: Optional checking for newer releases from GitHub
🎨 Modern TUI: Clean, intuitive interface built with BubbleTea
🔀 Multiple Connections: Save and switch between multiple LDAP server configurations

Screenshots
Main Interface

Initial startup screen with connection options
Adding Connections

_Interface for adding new LDAP Connections
Interactive Tree Navigation

Browse LDAP directory structure with keyboard/mouse navigation
Record Viewer

View detailed LDAP entry attributes with clipboard integration
Custom Query Interface

Execute custom LDAP queries with real-time results and formatting
Installation
Homebrew (Recommended for macOS/Linux)
From Custom Tap
brew install ericschmar/tap/moribito
From Formula URL (if tap not available)
brew install https://raw.githubusercontent.com/ericschmar/moribito/main/homebrew/moribito.rb
From GitHub Releases
Download the latest pre-built binary from GitHub Releases:
Option 1: Quick Install Scripts (Recommended)
Linux/Unix:
curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.sh | bash
macOS:
curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install-macos.sh | bash
Windows (PowerShell):
irm https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.ps1 | iex
The install scripts will:

Download the appropriate binary for your platform
Install it to the system PATH
Create OS-specific configuration directories
Generate sample configuration files

Option 2: Manual Download
# Linux x86_64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# Linux ARM64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Intel
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Apple Silicon
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/
For Windows, download moribito-windows-amd64.exe from the releases page.

Note: Homebrew is also available for Windows via WSL (Windows Subsystem for Linux). If you have WSL installed, you can use the Homebrew installation method above.

From Source
git clone https://github.com/ericschmar/moribito
cd moribito
go build -o moribito cmd/moribito/main.go
Usage
Command Line Options
# Connect with command line options
moribito -host ldap.example.com -base-dn "dc=example,dc=com" -user "cn=admin,dc=example,dc=com"

# Enable automatic update checking
moribito -check-updates -host ldap.example.com -base-dn "dc=example,dc=com"

# Use a configuration file
moribito -config /path/to/config.yaml

# Get help
moribito -help
Configuration File
Moribito will automatically look for configuration files in OS-specific locations:
Linux/Unix:

~/.config/moribito/config.yaml (XDG config directory)
~/.moribito/config.yaml (user directory)
~/.moribito.yaml (user home file)

macOS:

~/.moribito/config.yaml (user directory)
~/Library/Application Support/moribito/config.yaml (macOS standard)
~/.moribito.yaml (user home file)

Windows:

%APPDATA%\moribito\config.yaml (Windows standard)
%USERPROFILE%\.moribito.yaml (user home file)

All platforms also check:

./config.yaml (current directory)

Creating Configuration
Use the built-in command to create a configuration file:
moribito --create-config
Or manually create a configuration file:
ldap:
    host: "ldap.example.com"
    port: 389
    base_dn: "dc=example,dc=com"
    use_ssl: false
    use_tls: false
    bind_user: "cn=admin,dc=example,dc=com"
    bind_pass: "your-password"
pagination:
    page_size: 50 # Number of entries per page
retry:
    enabled: true # Connection retries (default: true)
    max_attempts: 3 # Retry attempts (default: 3)
    initial_delay_ms: 500 # Initial delay (default: 500)
    max_delay_ms: 5000 # Max delay cap (default: 5000)
Navigation
General Controls

Tab - Switch between views (Tree → Record → Query → Tree)
1/2/3 - Jump directly to Tree/Record/Query view
q - Quit application

Tree View

↑/↓ or k/j - Navigate up/down
Page Up/Down - Navigate by page
Home/End - Jump to top/bottom
→ or l - Expand node (load children)
← or h - Collapse node
Enter - View record details

Record View

↑/↓ or k/j - Scroll up/down
Page Up/Down - Scroll by page
Home/End - Jump to top/bottom
c - Copy current attribute value to clipboard

Query View

/ or Escape - Focus query input
Ctrl+Enter or Ctrl+J - Execute query
Ctrl+F - Format query with proper indentation
Escape - Clear query
Ctrl+V - Paste from clipboard
↑/↓ - Navigate results (when not in input mode)
Page Up/Down - Navigate by page (automatically loads more results)
Enter - View selected record


Note: The Query View uses automatic pagination to efficiently handle large result sets. When you scroll near the end of loaded results, the next page is automatically fetched from the LDAP server.

Query Formatting
The Ctrl+F key combination formats complex LDAP queries with proper indentation for better readability:
# Before formatting:
(&(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After formatting (Ctrl+F):
(&
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)

Authentication Methods
The tool supports various LDAP authentication methods:
Simple Bind
bind_user: "cn=admin,dc=example,dc=com"
bind_pass: "password"
OU-based Authentication
bind_user: "uid=john,ou=users,dc=example,dc=com"
bind_pass: "password"
Active Directory Style
bind_user: "john@example.com"
bind_pass: "password"
Anonymous Bind
# Leave bind_user and bind_pass empty or omit them
Security Options
SSL/LDAPS (Port 636)
ldap:
    host: "ldaps.example.com"
    port: 636
    use_ssl: true
StartTLS (Port 389)
ldap:
    host: "ldap.example.com"
    port: 389
    use_tls: true
Query Examples
In the Query view, you can execute custom LDAP filters:

(objectClass=*) - All objects
(objectClass=person) - All person objects
(cn=john*) - Objects with cn starting with "john"
(&(objectClass=person)(mail=*@example.com)) - People with example.com emails
(|(cn=admin)(uid=admin)) - Objects with cn=admin OR uid=admin

Complex Query Formatting
For complex nested queries, use Ctrl+F to automatically format them for better readability:
Simple queries remain unchanged:
(objectClass=person)

Complex queries are formatted with proper indentation:
# Original
(&(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After Ctrl+F
(&
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)

Performance & Pagination
LDAP CLI uses intelligent pagination to provide optimal performance when working with large directories:
Automatic Pagination

Default Page Size: 50 entries per page
Configurable: Adjust via config file or --page-size flag
On-Demand Loading: Next pages load automatically as you scroll
Memory Efficient: Only loaded entries are kept in memory

Configuration Examples
# Command line override
moribito --page-size 100 --host ldap.example.com

# Configuration file
pagination:
  page_size: 25  # Smaller pages for slower networks
Performance Tips

Smaller page sizes (10-25) for slower networks or limited LDAP servers
Larger page sizes (100-200) for fast networks and powerful LDAP servers
Use specific queries to reduce result sets instead of browsing all entries

Connection Reliability & Retries
LDAP CLI includes automatic retry functionality to handle connection failures gracefully:
Automatic Retries

Default: Enabled with 3 retry attempts
Exponential Backoff: Delay doubles between attempts (500ms → 1s → 2s → ...)
Connection Recovery: Automatically re-establishes broken connections
Smart Detection: Only retries connection-related errors, not authentication failures

Configuration Examples
# Default retry settings (automatically applied)
# No configuration needed - retries work out of the box
# Custom retry configuration
retry:
    enabled: true
    max_attempts: 5 # Maximum retry attempts (default: 3)
    initial_delay_ms: 1000 # Initial delay in milliseconds (default: 500)
    max_delay_ms: 10000 # Maximum delay cap (default: 5000)
# Disable retries if needed
retry:
    enabled: false
Retryable Conditions
The system automatically retries for:

Network timeouts and connection drops
Connection refused errors
Server unavailable responses
Connection reset by peer
LDAP server down errors

Authentication errors, invalid queries, and permission issues are not retried.
Development
Building
# Build for current platform
make build

# Build for all platforms
make build-all

# Clean build artifacts
make clean
Code Quality
# Format code
make fmt

# Run linter
make lint

# Run tests
make test

# Run all CI checks (format, lint, test, build)
make ci
Testing
go test ./...
Continuous Integration
This project uses GitHub Actions for CI/CD:


CI Workflow: Runs on every push and pull request to main and develop branches

Code formatting verification
Linting (with warnings)
Testing
Building for current platform
Multi-platform build artifacts (on main branch pushes)



Release Workflow: Triggered by version tags (e.g., v1.0.0)

Runs full CI checks
Builds for all platforms (Linux amd64/arm64, macOS amd64/arm64, Windows amd64)
Creates GitHub releases with binaries and checksums
Generates installation instructions



Dependencies

BubbleTea - TUI framework
Lipgloss - Styling
go-ldap - LDAP client
golang.org/x/term - Terminal utilities

Homebrew Distribution
This project includes full Homebrew support for easy installation on macOS and Linux. See the homebrew/ directory for:

Ready-to-use Homebrew formula
Formula generation and maintenance scripts
Documentation for creating custom taps
Instructions for submitting to homebrew-core

Versioning
This project follows Semantic Versioning. See docs/versioning.md for details on the release process.
Documentation
Comprehensive documentation is available using DocPress. To build and view the documentation:
# Build static documentation website
make docs

# Serve documentation locally with live reload
make docs-serve
The documentation covers:

Installation and setup
Usage guide with examples
Interface navigation
Development setup
Contributing guidelines
API reference and advanced features

Visit the generated documentation site for the complete guide.
Contributing

Fork the repository
Create your feature branch (git checkout -b feature/amazing-feature)
Commit your changes (git commit -m 'Add some amazing feature')
Push to the branch (git push origin feature/amazing-feature)
Open a Pull Request

License
This project is licensed under the MIT License - see the LICENSE file for details.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RubyMine is now free for non-commercial use]]></title>
            <link>https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45102186</guid>
            <description><![CDATA[RubyMine, a JetBrains IDE for Ruby and Rails, is now free for non-commercial use! Learn more in the blog post.]]></description>
            <content:encoded><![CDATA[
                                        RubyMine Is Now Free for Non-Commercial Use                    Read this post in other languages:
                    
Hold on to your helper methods – RubyMine is now FREE for non-commercial use! Whether you’re learning Ruby and Rails, pushing open-source forward, creating dev content, or building your passion project, we want to make sure you have the tools to enjoy what you do even more… for free.


    







Another chapter in the story



We recently introduced a new licensing model for WebStorm, RustRover, Rider, and CLion – making them free for non-commercial use. RubyMine is now joining the party! For commercial use, our existing licensing model still applies.



Why are we doing this?



We believe developers do their best work when the right tools are accessible. We’ve been listening closely to the Ruby and Rails community – their feedback, success, challenges, and passion for building with joy. Now, we’re making a change that reflects what we’ve heard.



By making RubyMine free for non-commercial use, we hope to lower the barrier to starting and help more people write clean, confident Ruby code from day one. It’s our way of supporting the unique Ruby community – from those who choose Ruby for their projects to maintainers of gems and frameworks who contribute to the Ruby ecosystem. Whether you’re debugging at midnight, crafting clever DSLs, or launching your first Rails app, RubyMine is here to help you build smarter (and crash less).



Commercial vs. non-commercial use



As defined in the Toolbox Subscription Agreement for Non-Commercial Use, commercial use means developing products and earning commercial benefits from your activities. However, certain categories are explicitly excluded from this definition. Common examples of non-commercial uses include learning and self-education, open-source contributions without earning commercial benefits, any form of content creation, and hobby development.



It’s important to note that, if you’re using a non-commercial license, you cannot opt out of the collection of anonymous usage statistics. We use this information to improve our products. The data we collect is exclusively that of anonymous feature usages of our IDEs. It is focused on what actions are performed and what types of functionality of the IDE are used. We do not collect any other data. This is similar to our Early Access Program (EAP) and is in compliance with our Privacy Policy.



FAQ



Below are answers to the most common questions. Check out the full FAQ for more information.



Licensing



What features are included under the free license?



With the new non-commercial license type, you can enjoy a full-featured IDE that is identical to its paid version. The only difference is in the Code With Me feature – you get Code With Me Community with your free license.



Which license should I choose if I want to use RubyMine for both non-commercial and commercial projects?



If you intend to use RubyMine for commercial development for which you will receive direct or indirect commercial advantage or monetary compensation within the meaning of the definitions provided in the Toolbox Subscription Agreement for Non-Commercial Use, you will need to purchase a commercial subscription (either individual or organizational). This license can then also be used for non-commercial development.



How do renewals and upgrades work now?



Non-commercial subscriptions are issued for one year and will automatically renew after that. However, for the renewal to happen, you must have used the assigned license at least once during the last 6 months of the subscription period. If it has been more than 6 months since you last used an IDE activated with this type of license and the renewal did not occur automatically, you can request a new non-commercial subscription again at any time.



Am I eligible for a refund if I’ve already bought a paid subscription but do non-commercial development?



If you’re unsure whether you qualify for a refund, you’ll find full details of our policy here. Please note that if you also work on projects that qualify as commercial usage, you can’t use the free license for them.



Anonymous data collection 



Does my IDE send any data to JetBrains?



The terms of the non-commercial agreement assume that the product may also electronically send JetBrains anonymized statistics (IDE telemetry) related to your usage of the product’s features. This information may include but is not limited to frameworks, file templates used in the product, actions invoked, and other interactions with the product’s features. This information does not contain personal data.



Is there a way to opt out of sending anonymized statistics?



We appreciate that this might not be convenient for everyone, but there is unfortunately no way to opt out of sending anonymized statistics to JetBrains under the terms of the Toolbox agreement for non-commercial use. The only way to opt out is by switching to either a paid subscription or one of the complimentary options mentioned here.



Getting a non-commercial subscription 



What should I do to apply for this subscription? 



It can be easily done right inside your IDE:




Install RubyMine and run it.



Upon startup, there will be a license dialog box where you can choose the Non-commercial use option.



Log in to your JetBrains account or create a new one. 



Accept the Toolbox Subscription Agreement for Non-Commercial Use.



Enjoy development in your IDE.




If you’ve already started a trial period or have activated your IDE using a paid license, you still can switch to a non-commercial subscription by following these steps:




Go to Help | Register.



In the window that opens, click on the Deactivate License button.



Choose Non-commercial use.



Log in to your JetBrains account or create a new one. 



Accept the Toolbox Subscription Agreement for Non-Commercial Use.



Enjoy development in your IDE.




I don’t see the Non-commercial use option in my IDE. What should I do? 



The most likely explanation for this is that you’re using an older version of RubyMine. Unfortunately, we don’t support obtaining the non-commercial license for any releases prior to RubyMine 2025.2.1.



That’s it for today! If you don’t find an answer to your question, feel free to leave a comment or contact us at sales@jetbrains.com.



The RubyMine team



JetBrains



Make it happen. With code.
                    
                                                                                                                                                                                                                            
                                
                                
                                
                                                                    
                            
                                                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Toronto’s underground labyrinth]]></title>
            <link>https://www.worksinprogress.news/p/torontos-underground-labyrinth</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45101779</guid>
            <description><![CDATA[How Canada's largest city developed a 30 kilometer network of pedestrian tunnels]]></description>
            <content:encoded><![CDATA[Toronto has one of the world’s great commercial downtowns. Two metro lines, eight suburban heavy railways, an extensive bus system, a highway, and North America’s greatest surviving tram network all converge on a tiny area by the shores of Lake Ontario. Hundreds of thousands of commuters pour into the downtown every day, filling the great towers that line its nineteenth-century streets.As with many downtowns, this causes congestion. Streets and pavements are thronged at peak times. Bicycles, pedestrians, cars, trams and buses compete for scarce space. As with all traditional radial transport systems, there is an enormous concentration of movement in a tiny area. Pedestrians make slow progress across the urban fabric, stopping every block for a minute or two to wait to cross the road.In the early twentieth century, Toronto’s businesses developed a novel response to this. They began to create pedestrian tunnels from their offices to the metro stations so that their employees could flow in smoothly, avoiding the congested streets (and, in winter, the cold). Shops quickly started to be added. After a few businesses had done this, a ‘network effect’ emerged whereby other businesses started to add their own tunnels to the system, benefiting from the existing tunnels while also making them more useful. It became routine for downtown developers to tie new office blocks into the network. Over many decades, a sort of ‘pedestrian metro’ emerged.Known as the Path, the network today stretches for more than 30 kilometers, linking nearly all central metro and railway stations with many of the major office buildings. Although the Path forms a unified network, it is not in unified ownership: it is divided into some 35 chunks, each of which is still owned and managed separately by descendants of whichever business originally contributed it. Many branches of the Path thus terminate in the lobbies of office buildings, with the curious result that these grand spaces function as metro entrances for the general public. The municipal authorities play only a limited regulatory role.The Path is unlike the gloomy and malodorous underpasses with which most of us are familiar. It is expensively decorated and feels like a high-end shopping mall, which in a way it is. It is extremely clean and closely policed by dozens of private security teams. Until recently, it was thronged with shoppers: this use suffered in the pandemic and has not wholly recovered, but the Path is still used for its original commuting purpose by hundreds of thousands of people every weekday.This is an underpass, but not quite as you know it. Image credit: Author’s collection.Urbanists are normally sceptical about pedestrian tunnels, fearing that they kill off street life. This is a reasonable concern and may often be a decisive reason to avoid them. But in a downtown like Toronto’s, human density is so enormous that this worry seems exaggerated. The pavements of Toronto are still busy, even on weekends, despite hundreds of thousands of pedestrians going underground. The carriageways are still congested with vehicles. It is facile to see this as a simple trade-off between walking and driving. The Path frees up space for bicycles, buses and trams as well as cars. It complements the metro and railway system by shaving off five minutes from the station-to-office walk. The Path may well be substantially net positive for a radial system of public and active transport.The Path is also interesting for what it tells us about transport economics. It is exceptionally unusual in forming an integrated network without having been developed by a single body. The equivalent would be a railway that was created piecemeal by uncoordinated landowners, with each adding small chunks until it stretched from one city terminus to another – a thing which, to my knowledge, has never happened.There are at least two reasons for the Path’s distinctive economics. First, pedestrian tunnels are clearly exceptionally valuable to individual landowners, such that they are prepared to bear the entire cost of providing transport infrastructure from which other landowners could subsequently benefit. It wasn’t necessary for the whole network to exist in order for the first landowners to get started: in downtown Toronto, pedestrian tunnels are so good that there was no first-mover problem. This is generally untrue of transport infrastructure: for example, any given stretch of a railway is virtually useless until the entire railway is finished, which is why public authorities usually have to plan entire railways from the start.Second, pedestrian tunnels have an extremely high upper limit on how many people they can take. As a famous 2003 advert illustrated, a human travelling in a vehicle requires space for their vehicle as well as themselves. A human travelling by foot does not, and is thus much more spatially efficient. This means that additional chunks can be added to the Path without normally creating ‘pedestrian jams’ in the existing stretches: there is nearly always space for more people. This is often untrue for roads and railways, where planners constantly worry that adding additional branches will cause congestion on central trunks.This means the Path’s model isn’t easily replicable for other transport modes: roads and railways really do need unified planning. But it isn’t clear why pedestrian metros should be impossible in other cities. In fact, to some extent, they do exist. Montreal has a similar system, while Tokyo, Osaka, Seoul, Hong Kong, Singapore and Houston have systems that resemble the Path in some respects. A few European cities also make considerable use of pedestrian tunnels, including Helsinki, Stockholm and Munich.Overall, though, the list of pedestrian metros is short. It would be interesting to investigate why this is. Why aren’t there pedestrian metros in Manhattan, Boston, Shanghai, Vancouver, Paris and the City of London? Is there some reason why the economics are different? Is the soil already too full of tunnels and utilities? Or are there regulatory constraints that might, in principle, be fixable?Downtowns were one of the great triumphs of nineteenth-century urbanism. Where they have not been destroyed by cars and modernism and civic mismanagement, they remain enormously valuable urban forms today. But they present, and have always presented, unique and interesting transport problems. Maybe pedestrian metros have a role to play in solving them.Samuel is an editor at Works in Progress. He focuses on urbanism and cities. He has previously written The beauty of concrete, Making architecture easy, Against the survival of the prettiest, and In praise of pastiche for Works in Progress.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What's New with Firefox 142]]></title>
            <link>https://www.mozilla.org/en-US/firefox/142.0.1/whatsnew/?oldversion=139.0.4&amp;utm_medium=firefox-desktop&amp;utm_source=update&amp;utm_campaign=142</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45101300</guid>
            <description><![CDATA[What’s New | Firefox 142]]></description>
            <content:encoded><![CDATA[
  
    
    
            What’s New | Firefox 142
            Keep your email address to yourself.
            Firefox Relay generates secure email masks when you sign up for new online accounts, so you can stay anonymous and get less spam in your inbox.
            
          



    
        Know before you click. Stay in control as you browse.
        Firefox helps you preview links and keep tabs tidy so nothing slows you down.

        
            Link Previews
            See what's behind the link before you click.
            Link Previews show a snapshot of a page before you open it, helping you decide what’s worth your time. Just long press any link to preview and reduce distractions.
            
          

        
            AI-Enhanced Tab Groups
            Your tabs, automatically grouped by topic.
            A local AI model identifies similar tabs, automatically organizes them into groups, and even suggests group names, helping you stay organized. Everything happens on your device to respect your privacy.
            
          

        
          See Full Release Notes
        
      



    
    
        Tailor Firefox to fit your flow.
        Stay organized, find what you need, and browse your way.
        
      

    
    
        Keep up with all things Firefox
        Get monthly how-tos, advice and news to make your Firefox experience work best for you.
        






  
    
      
    
    

    Your email address
    
      
    
    

    

    
    
    
    
      
        Please enter a valid email address
        We are sorry, but there was a problem with our system. Please try again later!
      
    
  
  


      

    
    
        What’s New on Firefox Mobile?
        New tools for focus, privacy, and smoother mobile browsing.
        
          
            PrivacyAndroid
            
            Private tabs that stay private
            Your private tabs lock automatically when you step away — and only unlock with your face, fingerprint, or PIN.
          
          
            LanguageAndroid
            
            Getting even more multilingual
            Now translate web pages into Japanese, Chinese, Korean and more, so you can browse in your preferred language.
          
          
            SecurityiOS
            
            Smarter passwords, fewer hassles
            Firefox suggests strong passwords when you’re creating a new account on any site, and keeps them secure, ready on any device when you sync.
          
          
            DesigniOS
            
            A cleaner look with sharper focus
            A streamlined UI and upgraded dark mode on Firefox for iOS bring clarity and calm to everything you browse.
          
        
      

    
    
                Take Firefox with you
                Scan the QR code to get Firefox Mobile and browse with calm, focus, and control — wherever you go.
              

  
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Run Erlang/Elixir on Microcontrollers and Embedded Linux]]></title>
            <link>https://www.grisp.org/software</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45100499</guid>
            <description><![CDATA[A modular embedded ecosystem—bare-metal hardware, software stacks, and a cloud platform for real-time IoT and distributed systems.]]></description>
            <content:encoded><![CDATA[GRiSP SoftwareRun Erlang/Elixir on Microcontrollers and Embedded LinuxDeploy Erlang and Elixir on embedded systems with three purpose-built software stacks. From microcontrollers to enterprise Linux builds, GRiSP provides deterministic, real-time runtime environments that boot directly into the BEAM. Manage your deployments at scale with GRiSP-io cloud platform.GRiSP Software StacksBring Erlang/Elixir all the way to the edge. Deterministic, fault-tolerant, and production-readyGRiSP MetalErlang/Elixir on RTEMS. Tiny BEAM for devices.GRiSP Metal, formerly just GRiSP, boots straight into the Erlang/Elixir VM on RTEMS for deterministic, real‑time behavior with a minimal footprint. It runs on microcontrollers, and we've made the full stack fit in 16 MB of RAM, ideal when every byte and millisecond matter.Boots directly to the BEAM (Erlang/Elixir) on RTEMSMCU-class footprint (fits in 16 MB RAM)Real-time scheduling with predictable I/ODirect, low-overhead access to hardware interfacesSupervision trees bring BEAM reliability to the edgeGRiSP AlloyErlang/Elixir on Linux RT. Buildroot edition.GRiSP Alloy boots directly into the Erlang/Elixir VM atop a lean, Buildroot-based real-time Linux. Run multiple Erlang/Elixir VMs with distinct priorities and/or pinned to different cores, connected via efficient, secure distributed Erlang links.Minimal Linux RT image (Buildroot) with BEAM-first boot pathMultiple BEAM instances with priority separation and core affinityLocal, secure node-to-node links via distributed ErlangFull access to Linux drivers, filesystems, and networkingFast boot, small attack surface, production-readyGRiSP ForgeErlang/Elixir on Linux RT.Yocto edition.GRiSP Forge applies the same architecture as GRiSP Alloy to Yocto, for teams requiring long-term, customizable Linux stacks and BSP integration. Boots directly into the Erlang/Elixir VM with the same multi-VM model and secure local links via distributed Erlang.Yocto-based builds with reproducible, customizable imagesMultiple Erlang/Elixir VMs by design (priorities and/or core pinning)Efficient, secure local links via distributed ErlangIndustrial Linux ecosystem compatibility and toolingBuilt for long lifecycles and enterprise requirementsGRiSP-io: Manage Embedded Systems at ScaleGRiSP-io is our cloud and edge platform for deploying, monitoring, and managing distributed embedded systems built with GRiSP stacks. From remote updates to real-time system insights, it helps you stay in control—whether you're testing a prototype or scaling a fleet.Deploy and manage GRiSP-based devices remotelyMonitor system performance and health in real-timePerform over-the-air updates with confidenceIntegrate cloud and edge control into your workflowsWhy Use GRiSP Software?GRiSP brings the power of Erlang/Elixir to embedded systems, making development efficient, scalable, and fault-tolerantFor DevelopersGRiSP enables developers to run Erlang and Elixir on bare metal or embedded Linux, reducing complexity with minimal overhead and real-time capabilities. With GRiSP stacks and GRiSP-io, they can build and deploy robust, distributed applications optimized for embedded environments.For IoT & Industrial SystemsFrom prototyping to production, GRiSP provides open-source tools that scale with project needs. Its real-time execution supports automation, robotics, and connected devices, while GRiSP-io enables remote management and monitoring of deployments.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kapa.ai (YC S23) is hiring research and software engineers]]></title>
            <link>https://www.ycombinator.com/companies/kapa-ai/jobs</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45099939</guid>
            <description><![CDATA[Jobs at kapa.ai]]></description>
            <content:encoded><![CDATA[The fastest way to build AI assistants on technical contentJobs at kapa.aiGB / DE / FR / NO / DK / SE / FI / PT / ES / BE / NL / IT / CH / AT / CZ / PL / EE / LV / LT / SK / HU / SI / HR / RU / UA / Remote (GB; DE; FR; NO; DK; SE; FI; PT; ES; BE; NL; IT; CH; AT; CZ; PL; EE; LV; LT; SK; HU; SI; HR; RU; UA)$100K - $150K0.10% - 0.30%3+ yearsGB / EG / RU / UA / TR / FR / IT / ES / PL / RO / KZ / NL / BE / SE / CZ / GR / PT / HU / AT / CH / BG / DK / FI / NO / SK / LT / EE / DE / Remote (GB; EG; RU; UA; TR; FR; IT; ES; PL; RO; KZ; NL; BE; SE; CZ; GR; PT; HU; AT; CH; BG; DK; FI; NO; SK; LT; EE; DE)$100K - $150K0.10% - 0.30%3+ yearsWhy you should join kapa.aiWe make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:

As chat interface on their public documentation to answer developer questions.
As first line of defense on their support forms to reduce tickets.
As internal assistant for their GTM teams to navigate their own complex product.

We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.
Founded:2023Batch:S23Team Size:19Status:ActiveFoundersFinn BauerFounderEmil SoerensenFounder]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Next.js is infuriating]]></title>
            <link>https://blog.meca.sh/3lxoty3shjc2z</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45099922</guid>
            <description><![CDATA[Hey, it's finally happened. I've decided to write a blog post. And if you're reading this, I've also finished one. I have wanted to do this for a long time, but could never find the motivation to start. But you know what they say: anger is the best motivator. They do say that, right?]]></description>
            <content:encoded><![CDATA[Hey, it's finally happened. I've decided to write a blog post. And if you're reading this, I've also finished one. I have wanted to do this for a long time, but could never find the motivation to start. But you know what they say: anger is the best motivator. They do say that, right?Some context that's in the backgroundWe're going on a journey, you and I. But first, we need to set the scene. Imagine we're working for $COMPANY and one of our Next.js services did an oopsie. This being Next.js, we of course have no idea what actually happened since the default logging is only enabled during development.Our quest is to go in and setup some production ready logging. It's not going to be easy, but then again, nothing ever is.Middleware? Middle of nowhere!The first step of our journey is the middleware. The documentation even states this:Middleware executes before routes are rendered. It's particularly useful for implementing custom server-side logic like authentication, logging, or handling redirects.Alright, looks simple enough. Time to pick a logging library. I went with pino since we have used it before. Anything is an upgrade over console.log anyways. We'll get this done before lunch.Let's set up a basic middleware:// middleware.ts
import { NextResponse, NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  return new NextResponse.next({
    request: request,
    headers: request.headers,
    // status: 200,
    // statusText: 'OK'
  });
}

export const config = {
  matcher: "/:path*",
};I think we already have a problem here. You can pass a grand total of 4 parameters from your middleware. The only thing that actually affects the invoked route is the headers. Let's not skip over the fact that you can't have multiple middlewares or chain them either. How do you fuck this up so bad? We've had middlewares since at least the early 2010s when Express came out.Anyways, we're smart and modern Node.js has some pretty nifty tools. Let's reach for AsyncLocalStorage.// app/logger.ts
import { AsyncLocalStorage } from "async_hooks";
import { Logger, pino } from "pino";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "trace",
});

export const LoggerStorage = new AsyncLocalStorage<Logger>();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// middleware.ts
export async function middleware(request: NextRequest) {
  LoggerStorage.enterWith(requestLogger());
  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next();
}Whew, hard work done. Let's test it out. Visit localhost:3000 and we see this:{ requestId: 'ec7718fa-b1a2-473e-b2e2-8f51188efa8f' } { url: 'http://localhost:3000/' } 'Started processing request!'
 GET / 200 in 71ms
{ requestId: '09b526b1-68f4-4e90-971f-b0bc52ad167c' } { url: 'http://localhost:3000/next.svg' } 'Started processing request!'
{ requestId: '481dd2ff-e900-4985-ae15-0b0a1eb5923f' } { url: 'http://localhost:3000/vercel.svg' } 'Started processing request!'
{ requestId: 'e7b29301-171c-4c91-af25-771471502ee4' } { url: 'http://localhost:3000/file.svg' } 'Started processing request!'
{ requestId: '13766de3-dd00-42ce-808a-ac072dcfd4c6' } { url: 'http://localhost:3000/window.svg' } 'Started processing request!'
{ requestId: '317e054c-1a9a-4dd8-ba21-4c0201fbeada' } { url: 'http://localhost:3000/globe.svg' } 'Started processing request!'I don't know if you've ever used pino before, but this is wrong. Can you figure out why?Unlike Next.js I won't keep you waiting in limbo. This is the browser output. Why? Well, it's because the default Next.js middleware runtime is edge. We can of course switch to the nodejs runtime which should work. Except, of course, it might not.I've tried it on a fresh Next.js project and it does work. But it didn't when I was trying it out on our actual project. I swear I'm not crazy. Anyways, this isn't the main issue. We're slowly getting there.Paging the local mental asylumLogging in the middleware is cool and all, but it's not where the bulk of the magic happens. For that, we need to log in pages and layouts. Let's try it out.// app/page.tsx
export default function Home() {
  logger()?.info("Logging from the page!");

  return <div>Real simple website!</div>
}Refresh the page and we get this:✓ Compiled / in 16ms
 GET / 200 in 142msThat's it? That's it. Nothing. Nada. Zilch.For posterity's sake, this is what it's supposed to look like:✓ Compiled / in 2.2s
[11:38:59.259] INFO (12599): Logging from the page!
    requestId: "2ddef9cf-6fee-4d1d-8b1e-6bb16a3e636b"
 GET / 200 in 2520msOk,this is getting a bit long, so I'll get to the point. The logger function returns null. Why? I'm not entirely sure, but it seems like rendering is not executed in the same async context as the middleware.What's the solution then? You're not going to believe this. Remember how the only value you can pass from the middleware is headers? Yeah. That's what we need to use.The following is for people with strong stomachs:// app/log/serverLogger.ts
import { pino } from "pino";

export const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

// app/log/middleware.ts
// Yes, we need to split up the loggers ...
// Mostly the same as before
import { loggerInstance } from "./serverLogger";

export function requestLogger(requestId: string): Logger {
  return loggerInstance.child({ requestId });
}

// app/log/server.ts
import { headers } from "next/headers";
import { loggerInstance } from "./serverLogger";
import { Logger } from "pino";
import { NextRequest } from "next/server";

const REQUEST_ID_HEADER = "dominik-request-id";

export function requestHeaders(
  request: NextRequest,
  requestId: string,
): Headers {
  const head = new Headers(request.headers);
  head.set(REQUEST_ID_HEADER, requestId);
  return head;
}

// Yeah, this has to be async ...
export async function logger(): Promise<Logger> {
  const hdrs = await headers();
  const requestId = hdrs.get(REQUEST_ID_HEADER);

  return loggerInstance.child({ requestId });
}

// middleware.ts
import { logger, LoggerStorage, requestLogger } from "./app/log/middleware";
import { requestHeaders } from "./app/log/server";

export async function middleware(request: NextRequest) {
  const requestId = crypto.randomUUID();
  LoggerStorage.enterWith(requestLogger(requestId));

  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next({ headers: requestHeaders(request, requestId) });
}

// app/page.tsx
export default async function Home() {
  (await logger())?.info("Logging from the page!");

  // ...
}Isn't it beautiful? I especially appreciate how it's now possible to import the middleware logging code from the server. Which of course won't work. Or import the server logging code from the middleware. Which also won't work. Better not mess up. Also, we haven't even touched upon logging in client components, which despite the name also run on the server. Yeah, that's a third split.Congratulations, you're being coddled. Please do not resist.Listen. I wanted to apologize, because I've led you into this trap. You see, I have already fallen into it several times before. A middleware system can be pretty useful when designed correctly and I wanted you to see what it looks like when it's not. The reason for writing this blog post actually started here.I think every one of us has reached a point in their lives where they've had enough. For me, it was right here. Fuck it, let's use a custom server.A custom Next.js server allows you to programmatically start a server for custom patterns. The majority of the time, you will not need this approach. However, it's available if you need to eject.Let's take a look at the example from the documentation:import { createServer } from 'http'
import { parse } from 'url'
import next from 'next'
 
const port = parseInt(process.env.PORT || '3000', 10)
const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()
 
app.prepare().then(() => {
  createServer((req, res) => {
    const parsedUrl = parse(req.url!, true)
    handle(req, res, parsedUrl)
  }).listen(port)
 
  console.log(
    `> Server listening at http://localhost:${port} as ${
      dev ? 'development' : process.env.NODE_ENV
    }`
  )
})Note that once again, handle doesn't really take any parameters. Only the request URL and the raw request and response.Anyways, we still have AsyncLocalStorage so this doesn't concern us too much. Let's modify the example a bit.// app/logger.ts
// Reverted back to our AsyncLocalStorage variaton
import { pino, Logger } from "pino";
import { AsyncLocalStorage } from "async_hooks";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

export const LoggerStorage = new AsyncLocalStorage<Logger>();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// server.ts
import { logger, LoggerStorage, requestLogger } from "./app/logger";

app.prepare().then(() => {
  createServer(async (req, res) => {
    // This is new
    LoggerStorage.enterWith(requestLogger());
    logger()?.info({}, "Logging from server!");

    const parsedUrl = parse(req.url!, true);
    await handle(req, res, parsedUrl);
  }).listen(port);
});

// middleware.ts
import { logger } from "./app/logger";

export async function middleware(request: NextRequest) {
  logger()?.info({}, "Logging from middleware!");
  return NextResponse.next();
}

// app/page.tsx
import { logger } from "./logger";

export default async function Home() {
  logger()?.info("Logging from the page!");
  
  // ...
}Ok, let's test it out. Refresh the browser and ...> Server listening at http://localhost:3000 as development
[12:29:52.183] INFO (19938): Logging from server!
    requestId: "2ffab9a2-7e15-4188-8959-a7822592108f"
 ✓ Compiled /middleware in 388ms (151 modules)
 ○ Compiling / ...
 ✓ Compiled / in 676ms (769 modules)That's it. Are you fucking kidding me right now? What the fuck?Now, you might be thinking that this is just not how AsyncLocalStorage works. And you might be right. But I would like to point out that headers() and cookies() use AsyncLocalStorage. This is a power that the Next.js devs have that we don't.As far as I can tell there are only two ways to pass information from a middleware to a page.HeadersNextResponse.redirect / NextResponse.rewrite to a route with extra params (eg. /[requestId]/page.tsx)As you might have noticed, neither of these are very pleasant to use in this case.You are being coddled. The Next.js devs have a vision and it's either their way or the highway. Note that if it was just the middleware, I wouldn't be sitting here, wasting away my weekend, ranting about a React framework. Believe it or not, I've got better things to do. It's constant pain you encounter daily when working with Next.js.Vercel can do betterWhat's infuriating about this example is that Vercel can very much do better. I don't want to sing too many praises at Svelte(Kit) because I have some misgivings about its recent direction, but it's so much better than Next.js. Let's look at their middleware docs:handle - This function runs every time the SvelteKit server receives a request [...] This allows you to modify response headers or bodies, or bypass SvelteKit entirely (for implementing routes programmatically, for example).Looking good so far.locals - To add custom data to the request, which is passed to handlers in +server.js and server load functions, populate the event.locals object, as shown below.I'm crying tears of joy right now. You can also stuff real objects/classes in there. Like a logger for instance.You can define multiple handle functions and execute them with sequence.This is what real engineering looks like. SvelteKit is a Vercel product. How is the flagship offering worse than what is essentially a side project. What the hell?Scientists discover a new super massive black hole at https://github.com/vercel/next.js/issuesI don't have anything else to add, but while I'm here I feel like I have to talk about the GitHub issue tracker. This is perhaps the crown jewel of the dumpster fire that is Next.js. It's a place where hopes and issues come to die. The mean response time for a bug report is never. I've made it a sport to search the issue tracker/discussion for problems I'm currently facing and bet on how many years it takes to even get a response from a Next.js dev.You think I'm joking? There are hundreds of issues with as many 👍 emojis with no official response for years. And when you finally get a response, it's to tell you that what you're doing is wrong and a solution to your real problems is on the way. Then they proceed to keep the "solution" in canary for years on end.I personally reported two issues a year ago. Keep in mind that to have a valid bug report, you need a reproduction.So, what do you get for taking the time to make a minimal reproduction? That's right. Complete silence.I would have reported about a dozen other issues I have encountered, but after this experience I gave up.Honestly, I don't even know if the issues are still valid. Have we learned anything?I don't know. For me, personally, I don't want to use Next.js anymore. You might think that this is just a singular issue and I'm overreacting. But there's bugs and edge cases around every corner. How did they manage to make TypeScript compile slower than Rust? Why make a distinction between code running on client and server and then not give me any tools to take advantage of that? Why? Why? Why?I don't think I quite have enough pull to move us out of Next.js land. But, I think I will voice my opinion if we end up writing another app. We'll see if the grass is any greener on the other side.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Collecting All Causal Knowledge]]></title>
            <link>https://causenet.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45099418</guid>
            <description><![CDATA[Collecting All Causal Knowledge]]></description>
            <content:encoded><![CDATA[
            Collecting All Causal Knowledge
            
                    CauseNet aims at creating a causal knowledge base that comprises all human causal knowledge and to separate it from mere causal beliefs, with the goal of enabling large-scale research into causal inference.
                
        

    

    Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.

    Download

    We provide three versions of our causality graph CauseNet:
    
      CauseNet-Full: The complete dataset
      CauseNet-Precision: A subset of CauseNet-Full with higher precision
      CauseNet-Sample: A small sample dataset for first explorations and experiments without provenance data
    

    Statistics

    
      
        
           
          Relations
          Concepts
          File Size
        
      
      
        
          CauseNet-Full
          11,609,890
          12,186,195
          1.8GB
        
        
          CauseNet-Precision
          199,806
          80,223
          135MB
        
        
          CauseNet-Sample
          264
          524
          54KB
        
      
    

    Data Model

    The core of CauseNet consists of causal concepts which are connected by causal relations. Each causal relation has comprehensive provenance data on where and how it was extracted.

    

    Examples of Causal Relations

    Causal relations are represented as shown in the following example. Provenance data is omitted.

    {
    "causal_relation": {
        "cause": {
            "concept": "disease"
        },
        "effect": {
            "concept": "death"
        }
    }
}


    For CauseNet-Full and CauseNet-Precision, we include comprehensive provenance data. In the following, we give one example per source.

    For relations extracted from natural language sentences we provide:
    
      surface: the surface form of the sentence, i.e., the original string
      path_pattern: the linguistic path pattern used for extraction
    

    ClueWeb12 Sentences

    
      clueweb12_page_id: page id as provided in the ClueWeb12 corpus
      clueweb12_page_reference: page reference as provided in the ClueWeb12 corpus
      clueweb12_page_timestamp: page access data as stated in the ClueWeb12 corpus
    

    {
    "causal_relation":{
        "cause":{
            "concept":"smoking"
        },
        "effect":{
            "concept":"disability"
        }
    },
    "sources":[
        {
            "type":"clueweb12_sentence",
            "payload":{
                "clueweb12_page_id":"urn:uuid:4cbae00e-8c7f-44b1-9f02-d797f53d448a",
                "clueweb12_page_reference":"http://atlas.nrcan.gc.ca/site/english/maps/health/healthbehaviors/smoking",
                "clueweb12_page_timestamp":"2012-02-23T21:10:45Z",
                "sentence": "In Canada, smoking is the most important cause of preventable illness, disability and premature death.",
                "path_pattern":"[[cause]]/N\t-nsubj\tcause/NN\t+nmod:of\t[[effect]]/N"
            }
        }
    ]
}


    Wikipedia Sentences

    
      wikipedia_page_id: the Wikipedia page id
      wikipedia_page_title: the Wikipedia page title
      wikipedia_revision_id: the Wikipedia revision id of the last edit
      wikipedia_revision_timestamp: the timestamp of the Wikipedia revision id of the last edit
      sentence_section_heading: the section heading where the sentence comes from
      sentence_section_level: the level where the section heading comes from
    

    {
    "causal_relation":{
        "cause":{
            "concept":"human_activity"
        },
        "effect":{
            "concept":"climate_change"
        }
    },
    "sources":[
        {
            "type":"wikipedia_sentence",
            "payload":{
                "wikipedia_page_id":"13109",
                "wikipedia_page_title":"Global warming controversy",
                "wikipedia_revision_id":"860220175",
                "wikipedia_revision_timestamp":"2018-09-19T04:52:18Z",
                "sentence_section_heading":"Global warming controversy",
                "sentence_section_level":"1",
                "sentence": "The controversy is, by now, political rather than scientific: there is a scientific consensus that climate change is happening and is caused by human activity.",
                "path_pattern":"[[cause]]/N\t-nmod:agent\tcaused/VBN\t+nsubjpass\t[[effect]]/N"
            }
        }
    ]
}


    Wikipedia Lists

    
      list_toc_parent_title: The heading of the parent section the list appears in
      list_toc_section_heading: The heading of the section the list appears in
      list_toc_section_level: The nesting level of the section within the table of content (toc)
    

    {
    "causal_relation":{
        "cause":{
            "concept":"separation_from_parents"
        },
        "effect":{
            "concept":"stress_in_early_childhood"
        }
    },
    "sources":[
        {
            "type":"wikipedia_list",
            "payload":{
                "wikipedia_page_id":"33096801",
                "wikipedia_page_title":"Stress in early childhood",
                "wikipedia_revision_id":"859225864",
                "wikipedia_revision_timestamp":"2018-09-12T16:22:05Z",
                "list_toc_parent_title":"Stress in early childhood",
                "list_toc_section_heading":"Causes",
                "list_toc_section_level":"2"
            }
        }
    ]
}


    Wikipedia Infoboxes

    
      infobox_template: The Wikipedia template of the infobox
      infobox_title: The title of the Wikipedia infobox
      infobox_argument: The argument of the infobox (the key of the key-value pair)
    

    {
    "causal_relation":{
        "cause":{
            "concept":"alcohol"
        },
        "effect":{
            "concept":"cirrhosis"
        }
    },
    "sources":[
        {
            "type":"wikipedia_infobox",
            "payload":{
                "wikipedia_page_id":"21365918",
                "wikipedia_page_title":"Cirrhosis",
                "wikipedia_revision_id":"861860835",
                "wikipedia_revision_timestamp":"2018-09-30T15:40:21Z",
                "infobox_template":"Infobox medical condition (new)",
                "infobox_title":"Cirrhosis",
                "infobox_argument":"causes"
            }
        }
    ]
}


    Loading CauseNet into Neo4j

    We provide sample code to load CauseNet into the graph database Neo4j.

    The following figure shows an excerpt of CauseNet within Neo4j (showing a coronavirus causing the disease SARS):

    

    Concept Spotting Datasets

    For the construction of CauseNet, we employ a causal concept spotter as a causal concept can be composed of multiple words (e.g., “global warming”, “human activity”, or “lack of exercise”). We determine the exact start and end of a causal
concept in a sentence with a sequence tagger. Our training and evaluation data is available as part of our concept spotting datasets: one for Wikipedia infoboxes, Wikipedia lists, and ClueWeb sentences. We split each dataset into 80% training, 10% development and 10% test set

    Paper

    CauseNet forms the basis for our CIKM 2020 paper CauseNet: Towards a Causality Graph Extracted from the Web. Please make sure to refer to it as follows:

    @inproceedings{heindorf2020causenet,
  author    = {Stefan Heindorf and
               Yan Scholten and
               Henning Wachsmuth and
               Axel-Cyrille Ngonga Ngomo and
               Martin Potthast},
  title     = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {{CIKM}},
  publisher = {{ACM}},
  year      = {2020}
}


    

    For questions and feedback please contact:

    Stefan Heindorf, Paderborn University
Yan Scholten, Technical University of Munich
Henning Wachsmuth, Paderborn University
Axel-Cyrille Ngonga Ngomo, Paderborn University
Martin Potthast, Leipzig University

    Licenses

    The code is licensed under a MIT license. The data is licensed under a Creative Commons Attribution 4.0 International license.
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Keyboards from my collection (2023)]]></title>
            <link>https://aresluna.org/50-keyboards-from-my-collection/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45099192</guid>
            <description><![CDATA[The most interesting keyboards I ever acquired]]></description>
            <content:encoded><![CDATA[
    
      
        Marcin Wichary
      
      
        12 February 2023 / 50 posts / 60 photos
      
    
    
    
    
      This is an archive of a Mastodon thread from 2023. You can still read the thread (and all the replies) at its original location, however the photo quality is much better on this page.
    
    
    
    
    



To celebrate the Kickstarter for Shift Happens going well, I thought I would show you 50 keyboards from my collection of really strange/esoteric/meaningful keyboards that I gathered over the years. (It might be the world’s strangest keyboard collection!)




This is technically a bit of a spoiler for the book, but a) a lot of them are not in the book, and b) the book comes out in half a year, and we’ll all forget by then! Let’s start!Shift Happens on Kickstarter


1.I have a SafeType, thanks to a friend who noticed one about to be thrown away. This is among the most notable and interesting “ergonomic” keyboards, complete with mirrors that help you orient yourself when you’re starting out.







2.The Comfort System keyboard is another “ergonomic“ device that is honestly pretty frightening to look at (explaining the challenge of making keyboards like these). You can reposition and reorient each of the three parts independently.







3.I love these DataDesk Little Fingers keyboards with smaller keys because you can see exactly when iMac was introduced and how the company tried to “redesign” the keyboard to fit the new style.





4.This is another Mac “alternate universe“ keyboard - an Adesso ergonomic keyboard that feels like “what if Apple Adjustable still existed when iMac came around”?







5.This strange “medical” keyboard is more mechanical than you’d expect! I wrote more about it here: A tale of three skeuomorphs. Cleaning required when flashing!







6.Once you’re done with your shift (no pun intended) at the hospital, how about some Pizza? This is i-Opener, one of the many shortlived internet appliances, this one with a gimmick that keeps on gimmicking.







7.Speaking of spacebar-adjacent gimmicks, I am mildly obsessed with how beautiful is this first NeXT keyboard from 1987, with a bunch of cool subtle things including a Command *bar* underneath the spacebar. As a matter of fact, I just finished writing an essay on it today!







8.This is Olivetti Praxis 48: perhaps one of the most beautiful among the most beautiful typewriters, and strangely similar in palette to the above NeXT keyboard. You could turn on this (electric) typewriter just by pressing any key. That’s pretty wild.





9.This Olympia Reporter typewriter is not beautiful, but it has a lot of POWER THIS and POWER THAT keys that celebrate its marriage with electricity? Why is X and some other keys red? Those are the ones that auto repeat! 







10.This is another typewriter, so proud of a functioning (erasing!) Backspace that it gives this a treatment I have never seen before or after.









12.This keypad… is so bad.







13.This was meant to be mounted atop Commodore 64 (which I don’t have), an interesting reversal from the early typewriters being nothing more than repurposed music keyboards.







14.These two are taking this idea even further – mount these overlays on regular keyboards to turn them into new kinds of interfaces.





15.There’s also professional gaming. It was cheaper for me to buy QSENN keyboards and replicate what professional StarCraft gamers were doing in the 1990s, than to find a good existing photo of one of these keyboards.







16.And speaking of gaming – we’re all used to the thumb style of typing from the first photo that it was fun to discover the short moment where the gaming keyboards looked like the one in the second photo.





17.And a bit earlier, some game consoles tried to reinvent themselves as home computers with keyboard accessories. This is among the strangest of them: a “keyboard” to add BASIC to the Atari 2600.







18.I commissioned this “joystick” from @benjedwards and I am so happy with how it turned out. It’s technically a joystick without a stick, but software turned it into a one-key keyboard. It’s F11, currently mapped to muting/unmuting in Zoom. It’s *incredibly* rewarding to press.





19.Speaking of strange keyboards, this is my “space cadet” keyboard – a mini keyboard that outputs only spaces, and instead of legends, each key *feels* different. Wrote about it more here: Stop me if you’ve seen this one before





20.And here is a keyboard I built and hid in my shoes, made for one very specific reason. Are you interested what it is? Check out the whole story here:  To walk among keyboard magicians







21.This is one of the most rare keyboards I have – the strange abKey Evolution imported through a friend from Singapore – a keyboard that tried to reinvent perhaps one thing too many. Wrote more about it here: The worst keyboard ever made







22. And this one from Commodore is not really that unique, except it has this fun property – it reverses the usual beige colour scheme making the keys inside darker. It’s kinda neat!







23.This is a really cheap Bulgarian keyboard with such a poor build quality it cannot be unseen! I wrote more about it here: The worst keyboard ever made







24.Oh, it gets worse. This calculator keyboard is so cheap it’s not a keyboard at all – just an exposed PCB with a pen to complete the circuit. More about it here: The worst keyboard ever made





25.And this is the opposite, an incredibly well-built IBM Model F banking typewriter with an enclosure made out of zinc. Hefty enough to stop a bank robbery? Perhaps. More here: To save a keyboard, pt. 2







Halfway through! I need a bit of a break. Is this interesting? Should I keep going!?




26.If your bank robbery goes poorly, you probably end up typing on this Swintec, transparent so that no contraband could be hidden inside. More about transparent tech for prisons in this Techmoan video: YouTube







27.This simple braille keyboard – Tellatouch – was gorgeous and important. Type a key on one side, and the right braille letter assembles itself on the other.







28.This is a more modern version of an adjacent idea. Connect this device to a phone line, and you can speak even if you cannot talk. (Also, I just love any time a keyboard lands itself next to a segmented display.)







29.The creators of this Seiko keyboard recognized a watch with a keyboard wouldn’t make sense – so you could dock your watch and type this way. (I don’t have the watch itself. Too expensive!)







30.Just kidding! Here’s a keyboard on another Seiko watch. It’s an index keyboard – you don’t touch the keys directly, just move the cursor left and right like on Apple TV – since the keys are smaller than 1mm.







31.This TI calculator for school use has tiny keys… in between other keys. What a strange thing.





32.This calculator went… a different way.







33.I love hybrid things and in-betweeners. This tiny Panasonic Toughbook asks a question: what if a BlackBerry keyboard, but twice the width?







34.This one, for TermiFlex, is a one-hand operation, inspired by phone keypads. There are three shifts under your long fingers!





35.Speaking of complex shortcuts, look at this Apple keyboard with Avid software keycaps. The icon on Z is my favourite. I don’t even wanna know what this function does.







36.One among many foldable keyboards – this one for Palm devices (RIP).







37.This Sony remote had a built-in keyboard for typing in MiniDisc titles.







38.And *this* Sony keyboard had two numeric keypads going in two different directions! One for typical calculator use, and one inspired by mobile phones to allow to chat as easily for people who got used to chatting that way.







39.Very happy (and also maybe also a little concerned) to report I am in possession of the entire ProHance lineup of the strange pointing device/keyboard hybrids!







40.But it’s amazing how rarely the graphical user interfaces and keyboards intersect. This here – an old AT&T terminal keyboard – is an exception, providing dedicated keys for window management.







41.I had to get this keyboard for a now-obscure Harris word processor, just because LOOK AT THE SHAPE OF THIS ENTER KEY.







42.I have seen so many keyboards, but only this one – from a strange titling device meant to be connected to your TV – treats uppercase and lowercase exactly like all the other shifted and unshifted symbols. (With the exception of keyboards for kids, I assume!)







43.Back in the day, keyboards were so expensive that you often started on a “training” keyboard that came without the machine connected to it. Here’s a training keyboard for a Linotype, which is itself a fascinating machine.







44.Here’s another one for the first popular line of desk calculators that predates a 10-key keypad.







45.(I also have the actual calculator, called a Comptometer. It’s beautiful, really fun to use, and honestly a work of art. A truly impressive machine from the bygone era. I bought it because I was so impressed reading what it can do.)







46.Here’s another practice keyboard, with a record to play to teach you how to type!







47.And here’s the most modern version of a practice keyboard I know of – itself a small computer. After that, the likes of Mavis Beacon took over teaching typing in software.







48.Speaking of the 1980s, keyboards from failed computers often found a second life as Radio Shack components you could reuse in your DIY projects. Here’s one from a home computer called Coleco Adam.







49.While we’re speaking about failed computers, this is One Laptop Per Child’s interesting-looking keyboard. (I think OLPC is considered a failure? I’m not 100% sure. This computer is not in the book, so I haven’t researched that carefully.)







50. And here is Canon Cat, maybe my favourite failed machine of all time. Look at these Leap keys! I’m somewhat in love with this machine.Adult-onset felinophilia






That’s it! I hope you liked this sneak peek of my collection– if you did, consider backing the book since this is the level of quality I’ve been aiming at for the visual side… there are a lot more photos like these, and of course a lot more great stories attached to them. Shift Happens on Kickstarter





    
    

  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WinBoat: Run Windows apps on Linux with seamless integration]]></title>
            <link>https://github.com/TibixDev/winboat</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45099124</guid>
            <description><![CDATA[Run Windows apps on 🐧 Linux with ✨ seamless integration - TibixDev/winboat]]></description>
            <content:encoded><![CDATA[
  
    
      
        
      
      
        WinBoat
        Windows for Penguins.
        Run Windows apps on 🐧 Linux with ✨ seamless integration
      
    
  

Screenshots

  
  

⚠️ Work in Progress ⚠️
WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.
Features

🎨 Elegant Interface: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience
📦 Automated Installs: Simple installation process through our interface - pick your preferences & specs and let us handle the rest
🚀 Run Any App: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment
🖥️ Full Windows Desktop: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow
📁 Filesystem Integration: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle
✨ And many more: Smartcard passthrough, resource monitoring, and more features being added regularly

Prerequisites
Before running WinBoat, ensure your system meets the following requirements:

RAM: At least 4 GB of RAM
CPU: At least 2 CPU threads
Storage: At least 32 GB free space in /var
Virtualization: KVM enabled in BIOS/UEFI

How to enable virtualization


Docker: Required for containerization

Installation Guide


Docker Compose v2: Required for compatibility with docker-compose.yml files

Installation Guide


Docker User Group: Add your user to the docker group

Setup Instructions


FreeRDP: Required for remote desktop connection (Please make sure you have Version 3.x.x with sound support included)

Installation Guide


Kernel Modules: iptables and iptable_nat modules must be loaded

Module loading instructions



Downloading
You can download the latest Linux builds under the Releases tab. We currently offer two variants:

AppImage: A popular & portable app format which should run fine on most distributions
Unpacked: The raw unpacked files, simply run the executable (linux-unpacked/winboat)

Known Issues About Container Runtimes

Podman is unsupported for now
Docker Desktop is unsupported for now
Distros that emulate Docker through a Podman socket are unsupported
Any rootless containerization solution is currently unsupported

Building WinBoat

For building you need to have NodeJS and Go installed on your system
Clone the repo (git clone https://github.com/TibixDev/WinBoat)
Install the dependencies (npm i)
Build the app and the guest server using npm run build:linux-gs
You can now find the built app under dist with an AppImage and an Unpacked variant

Running WinBoat in development mode

Make sure you meet the prerequisites
Additionally, for development you need to have NodeJS and Go installed on your system
Clone the repo (git clone https://github.com/TibixDev/WinBoat)
Install the dependencies (npm i)
Build the guest server (npm run build-guest-server)
Run the app (npm run dev)

Contributing
Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.
Please note: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀
Feel free to:

Report bugs and issues
Submit feature requests
Contribute code improvements
Help with documentation
Share feedback and suggestions

Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.
License
WinBoat is licensed under the MIT license
Inspiration / Alternatives
These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.
They're awesome and you should check them out:

WinApps
Cassowary
dockur/windows (🌟 Also used in WinBoat)

Socials & Contact

🌐 Website: winboat.app
🐦 Twitter/X: @winboat_app
🦋 Bluesky: winboat.app
🗨️ Discord: Join our community
📧 Email: staff@winboat.app

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FreeDroidWarn]]></title>
            <link>https://github.com/woheller69/FreeDroidWarn</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45098722</guid>
            <description><![CDATA[Contribute to woheller69/FreeDroidWarn development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    



  
  
  






      

          

              




  Navigation Menu

  

  
          
            


                
      

      
          

                
                    
  
      
      
          
            GitHub Copilot
          
        Write better code with AI
      

    


                    
  
      
      
          
            GitHub Spark
              
                New
              
          
        Build and deploy intelligent apps
      

    


                    
  
      
      
          
            GitHub Models
              
                New
              
          
        Manage and compare prompts
      

    


                    
  
      
      
          
            GitHub Advanced Security
          
        Find and fix vulnerabilities
      

    


                    
  
      
      
          
            Actions
          
        Automate any workflow
      

    


                    
                
              
          

                
                    
  
      
      
          
            Codespaces
          
        Instant dev environments
      

    


                    
  
      
      
          
            Issues
          
        Plan and track work
      

    


                    
  
      
      
          
            Code Review
          
        Manage code changes
      

    


                    
  
      
      
          
            Discussions
          
        Collaborate outside of code
      

    


                    
  
      
      
          
            Code Search
          
        Find more, search less
      

    


                
              
          

      



                
      

      



                
      

      
                    Explore
                    
  
      Learning Pathways

    


                    
  
      Events & Webinars

    


                    
  
      Ebooks & Whitepapers

    


                    
  
      Customer Stories

    


                    
  
      Partners

    


                    
  
      Executive Insights

    


                
              



                
      

      
              

                
                    
  
      
      
          
            GitHub Sponsors
          
        Fund open source developers
      

    


                
              
              

                
                    
  
      
      
          
            The ReadME Project
          
        GitHub community articles
      

    


                
              
              
          



                
      

      

                
                    
  
      
      
          
            Enterprise platform
          
        AI-powered developer platform
      

    


                
              



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  





    






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    2

  

  
        
            
          Star
          106

  



        

        


          

  
    


  

  




          



  
  
  Folders and filesNameNameLast commit messageLast commit dateLatest commitHistory12 Commitsgradle/wrappergradle/wrapperlibrarylibrary.gitignore.gitignoreLICENSELICENSEREADME.mdREADME.mdbuild.gradlebuild.gradlegradle.propertiesgradle.propertiesgradlewgradlewgradlew.batgradlew.batsettings.gradlesettings.gradleREADMEApache-2.0 licenseFreeDroidWarn
Overview
This library shows an alert dialog with a deprecation warning informing that Google will require developer verification for Android apps outside the Play Store from 2026/2027 which the developer is not going to provide.
Google has announced that, starting in 2026/2027, all apps on certified Android devices
will require the developer to submit personal identity details directly to Google.
Since the developers of this app do not agree to this requirement, this app will no longer 
work on certified Android devices after that time.

https://www.androidauthority.com/android-developer-verification-requirements-3590911/
https://developer.android.com/developer-verification
Installation
Add the JitPack repository to your root build.gradle at the end of repositories:
allprojects {
  repositories {
    ...
    maven { url 'https://jitpack.io' }
  }
}
Add the library dependency to your build.gradle file.
dependencies {
    implementation 'com.github.woheller69:FreeDroidWarn:V1.3'
}
Usage
In onCreate of your app just add:
     FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE);


License
This library is licensed under the Apache V2.0 license.




      




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kazeta: An operating system that brings the console gaming experience of 90s]]></title>
            <link>https://kazeta.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45098269</guid>
            <content:encoded><![CDATA[
    
        
            
                Download
                Discord
                Docs
                GitHub
            
        
    

    
        
		An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.
                Explore Kazeta
            
        ↓
    

    
            
                Pure Gaming
                Insert a game cart, press power, and you're gaming instantly. Relive that nostalgic golden age where nothing stood between you and the games you love.
                
Zero setup
Direct to gameplay
Maximum performance
Distraction-free gaming
Classic '90s console experience
                
            
            
            
        

    
            
                Create Collect Play
                Transform your digital library into something tangible and permanent. Create physical game carts from your DRM-free titles and build a collection that you can play forever.
                
                    Turn any DRM-free game into a physical cart
                    Use SD cards or other external media as carts
                    Play without internet, accounts, or restrictions
                    Preserve your games as permanent, playable artifacts
                
            
	        
            
        

    
            
                Gaming Tranquility
                Say goodbye to the complexities of modern gaming and just play.
                
                    No DRM
                    No online
                    No servers
                    No updates
                    No accounts
                    No launchers
                    No subscriptions
                    No microtransactions
                
            
            
            
        

    
            
                Save Management
                Save data is captured automatically, so you never lose progress. When no cart is inserted, boot into a retro console inspired BIOS menu to manage your saves.
                
                    Retro-style BIOS screen
                    Automatic save capture
                    Playtime tracking
                    View and delete saves
                    Backup saves to external media
                
            
            
            
        

    
            
                Play It All
                Play almost any DRM-free game from platforms past or present.
                
                    AAA and indie games
                    Modern hits and old classics
                    GOG and itch.io games
                    Linux and Windows games
                    Classic console games with emulators
                
            
            
            
        

    
            
                Gaming For Everyone
                Bring back the family-friendly simplicity of gaming's distant past. Perfect for kids, parents, and grandparents who just want to play.
                
                    For kids who need a safe, offline environment
                    For older family members intimidated by modern gaming
                    For anyone craving gaming's simpler days
                
            
            
            
        

    
                Ready to game like it's 1995?
                
                    Download Kazeta today and rediscover the joy of pure gaming.
                
                Download Now
            


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi 5 support (OpenBSD)]]></title>
            <link>https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;w=2</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45096585</guid>
            <content:encoded><![CDATA[
[prev in list] [next in list] [prev in thread] [next in thread] 

List:       openbsd-cvs
Subject:    CVS: cvs.openbsd.org: src
From:       Marcus Glocker <mglocker () cvs ! openbsd ! org>
Date:       2025-09-01 18:56:04
Message-ID: dd1203a530237b22 () cvs ! openbsd ! org
[Download RAW message or body]

CVSROOT:	/cvs
Module name:	src
Changes by:	mglocker@cvs.openbsd.org	2025/09/01 12:56:04

Modified files:
	distrib/arm64/iso: Makefile 
	distrib/arm64/ramdisk: Makefile install.md list 

Log message:
Add Raspberry Pi 5 Model B support for RAMDISK.

Known issues:
* Booting from PCIe storage HATs doesn't work because of missing U-Boot
support.
* WiFi on the Raspberry Pi 5 Model B "d0" boards doesn't work.
* The active cooler (fan) doesn't work because of missing pwm/clock
drivers (some work is in-progress).

ok kettenis@, deraadt@

[prev in list] [next in list] [prev in thread] [next in thread] 

  
    Configure | 

    About |
    News |
    Add a list |
    Sponsored by KoreLogic



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Patrick Winston: How to Speak (2018) [video]]]></title>
            <link>https://www.youtube.com/watch?v=Unzc731iCUY</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095849</guid>
        </item>
        <item>
            <title><![CDATA[Amazon has mostly sat out the AI talent war]]></title>
            <link>https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095603</guid>
            <description><![CDATA[Amazon has struggled to recruit top AI talent. An internal document and company insiders reveal the reasons.]]></description>
            <content:encoded><![CDATA[
            
            
            
            
                
                
                    
                      
                      
                    
                
                  
                        
                          
                          
                            
                              
                                    
                  
                          
                            Amazon CEO Andy Jassy
                            
                              
                                Fortune/Reuters Connect
                                        
                          
                        
                  
            
    
    
    
              
                
                
                
                  
                      2025-08-28T09:00:02Z
                    
                  
                
                
                        
      
            
      
              
              
              
              
                
                    Amazon struggles to attract AI talent due to its pay model and perception of falling behind others.
                    Amazon's compensation model has long caused complaints from employees.
                    Competitors like Meta and OpenAI offer more attractive packages for AI engineers.
                
              
      
            
            
            
            
            
            
                As the AI talent war sweeps across Silicon Valley, Amazon has largely sat on the sidelines. A confidential internal document, and accounts from people familiar with the matter, reveal why.The company has flagged its unique pay structure, lagging AI reputation, and rigid return-to-office rules as major hurdles. Now, the tech giant is being pushed to rethink its recruiting strategy as it scrambles to compete for top talent.The document, from late last year, was written by the HR team covering Amazon's non-retail businesses, including Amazon Web Services, advertising, devices, entertainment, and the newly formed artificial general intelligence team."GenAI hiring faces challenges like location, compensation, and Amazon's perceived lag in the space," the document noted. "Competitors often provide more comprehensive and aggressive packages." Business Insider obtained a copy of the document.Amazon's absence from recent splashy AI hires underscores those concerns. Meta has pulled in high-profile talent from ScaleAI, Apple, and OpenAI. Google and OpenAI continue to be top destinations for AI experts, while Microsoft has even drafted a wish list of Meta AI employees it hopes to recruit.Amazon's spokesperson initially told BI that the company continues to "adapt our approach to remain highly competitive, maintaining flexibility in both our compensation packages and work arrangements to attract and retain the best AI talent in this dynamic market."Hours later, the spokesperson updated the statement, saying the premise of the story was "wrong," without providing any specifics."We continue to attract and retain some of the best people in the world and they're building and deploying GenAI applications at a rapid clip. Our compensation is competitive, but we also want missionaries who are passionate about inventing things that will make a meaningful difference for customers — for those kinds of people, there's no better place in the world to build."Door desks and 'egalitarian' pay
              
              
              
              
                
            
              
              
              
            
              
              
                    
                      Amazon founder Jeff Bezos back in the 1990s
                      
              
              
              TNS/ABACA via Reuters Connect
              
              
                    
                  
              
            Amazon is famously frugal. One of its origin stories recounts how the company bought cheap doors from Home Depot and hacked them together as office desks. This became guiding symbol of Amazon's cautious spending, with founder Jeff Bezos still using door desks today.This penny-pinching culture has smashed straight into an AI hiring battle that's being fueled by unprecedented spending, putting Amazon in a tricky situation.The internal document described compensation as one of the "hotly debated topics" among Amazon recruiters, citing the company's strict use of fixed salary bands for each role. Amazon's "egalitarian philosophy" on pay leaves its offers "below par" compared with top rivals, it added."The lack of salary range increases for several key job families over the past few years does not position Amazon as an employer of choice for top tech talent," the document warned.For Amazon, missing out on top AI talent is a potential risk. ​​The pool of top-tier AI researchers and engineers is limited, and without experts with deep knowhow, it's hard to compete at the frontier of the field. Indeed, Amazon has yet to find a blockbuster AI product like OpenAI's ChatGPT or Anthropic's Claude, although its Bedrock AI cloud service has made progress.
              
              
              
            Amazon's pay structure has been a long-standing source of tension.Several people who spoke to Business Insider cited the 2020 departure of Amazon robotics VP Brad Porter as evidence of the company's frugal approach hampering talent recruitment and retention. Porter left in part after Amazon refused to raise his pay band.Amazon's stock vesting schedule is also heavily backloaded, a structure that can be less attractive to new hires. The policy extends even to top executives, who generally receive no cash bonuses.'Voting with their feet'
              
              
              
              
                
            
              
              
              
            
              
              
                    
                      Amazon CEO Andy Jassy
                      
              
              
              REUTERS/Brendan McDermid
              
              
                    
                  
              
            In addition to highlighting Amazon's "perceived lag in the AI space," the internal document said generative AI has further intensified the competition for specialized talent, particularly individuals with expertise in large language models.An August report from venture capital firm SignalFire shows Amazon is on the lower end of engineering retention, far below Meta, OpenAI, and Anthropic. Jarod Reyes, SignalFire's head of developer community, told Business Insider that Amazon rivals are making bigger strides in AI, across open models, foundational research, and developer tooling."Amazon hasn't clearly positioned itself as a leader in the generative AI wave," Reyes said. "Engineers are paying attention and they're voting with their feet."
              
              
              
              
                
            
              
              
              
            
              
              
                    
                      SignalFire chart on engineering talent retention
                      
              
              
              SignalFire
              
              
                    
                  
              
            Some investors share that view. On Amazon's earnings call last month, Morgan Stanley analyst Brian Nowak pressed CEO Andy Jassy on Wall Street's "narrative right now that AWS is falling behind" in AI and fears of losing market share to rivals. Jassy's response fell flat, sending Amazon's stock lower during the call.Amazon intends to tackle these concerns. According to the document, the company will refine its "compensation and location strategy" and host more events designed to highlight its generative AI capabilities. It also intends to set up dedicated recruiting teams for generative AI within business units like AWS to boost efficiency.'Hubs' constrain talent
              
              
              
              
                
            
              
              
              
            
              
              
                    
                      Hundreds of tech workers gathered outside Amazon's headquarters in Seattle.
                      
              
              
              REUTERS/Lindsey Wasson
              
              
                    
                  
              
            Another point of contention is Amazon's aggressive return-to-office mandate, which has already caused logistical issues.The company's new "hub" policy — which requires employees to relocate to a central office or risk termination — has further limited its access to "high-demand talent like those with GenAI skills," according to the internal document."Hubs constrain market availability," it stated.Amazon is exploring ways to allow for more "location-flexible" roles, the document added.Amazon's spokesperson told BI that the company is "always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations."Amazon hasn't been entirely on the sidelines. Last year, it brought on Adept CEO David Luan as part of a licensing deal with the AI startup. Luan now heads Amazon's AI agents lab. But the company has also seen departures, including senior AI leaders like chip designer Rami Sinno and VP Vasi Philomin, who worked on Bedrock.One Amazon recruiter told Business Insider that a growing number of job candidates started declining offers last year because of the company's RTO policy. Even if a competitor pays less, people are open to taking the job if they can stay remote, this person said."We are losing out on talent," this person added.Indeed, Bloomberg reported recently that Oracle has hired away more than 600 Amazon employees in the past two years because Amazon's strict RTO policy has made poaching easier.Staying the courseThe internal Amazon document dates to late last year, leaving open the possibility that the company has since adjusted its compensation approach to make exceptions for top AI talent.Still, multiple people familiar with the situation told Business Insider there haven't been any formal updates to internal pay guidelines. One current Amazon manager said it remains almost impossible for the company to enact sweeping changes, given its long track record of sticking to the existing system. The people who spoke with Business Insider asked not to be identified discussing sensitive matters."Based on how we run our business and what we have achieved, there are more risks than potential benefits from changing an approach that has been so successful for our shareholders over the past several decades," Amazon wrote this year about executive compensation in its annual proxy statement.Of course, the AI talent war may end up being an expensive and misguided strategy, stoked by hype and investor over-exuberance.Some of the high-profile recruits Meta recently lured have already departed.Have a tip? Contact this reporter via email at ekim@businessinsider.com or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.
            
            
            
            
            
            
            
              
            
    
    
    
    
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The future of 32-bit support in the kernel]]></title>
            <link>https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095475</guid>
            <description><![CDATA[Arnd Bergmann started his Open Source Summit Europe 2025 talk with a clear statement of positio [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



Arnd Bergmann started his Open
Source Summit Europe 2025 talk with a clear statement of position: 32-bit
systems are obsolete when it comes to use in any sort of new products.  The
only reason to work with them at this point is when there is existing
hardware and software to support.  Since Bergmann is the overall maintainer
for architecture support in the kernel, he is frequently asked whether
32-bit support can be removed.  So, he concluded, the time has come to talk
more about that possibility.


People naturally think about desktop machines first, he continued.  If you
were running Linux in the 1990s, you had a 32-bit, desktop system.  Unix
systems, though, moved to 64-bit platforms around 30 years ago, and
the Linux desktop made that move about 20 years ago.  Even phones and
related devices have been 64-bit for the last decade.  If those systems
were all that Linux had to support, 32-bit support would have long since
been removed from the kernel.  He summarized the situation with this slide,
showing how the non-embedded architectures have transitioned to either
64-bit or nonexistence over time:




The world is not all desktops — or servers — though; embedded Linux exists
as well.  About 90% of those systems are running on Arm processors.  The
kernel has accumulated a lot of devicetree files describing those systems
over the years; only in this last year has the number of devicetrees for
armv8 (64-bit) systems exceeded the number for armv7 (32-bit) systems.

For Arm processors with pre-armv7 architectures, there are only three for
which it is still possible to buy hardware, but a number are still
supported by the kernel community:






Many other pre-armv7 CPUs are out of production,
but the kernel still has support for them.  Of those, he said, there are
about ten that could be removed now.  It would be nice to be able to say
that support for the others will be removed after a fixed period, ten years
perhaps, but hardware support does not work that way.  Instead, one has to
think in terms of half lives; every so often, it becomes possible to remove
support for half of the platforms.  It all depends on whether there are
users for the processors in question.

The kernel is still adding support for some 32-bit boards, he said, but at
least ten new 64-bit boards gain support for each 32-bit one.

There are a number of non-Arm 32-bit architectures that still have support
in the kernel; these include arc, microblaze, nios2, openrisc, rv32,
sparc/leon, and xtensa.  All of them are being replaced by RISC-V
processors in new products.  RISC-V is what you use if you don't care about
Arm compatibility, he said.

Then, there is the dusty corner where nommu (processors without a
memory-management unit) live; these include armv7-m, m68k, superh, and
xtensa.  Nobody is building anything with this kind of hardware now, and
the only people who are working on them in any way are those who have to
support existing systems.  "Or to prove that it can be done."

There are still some people who need to run 32-bit applications that cannot
be updated; the solution he has been pushing people toward is to run a
32-bit user space on a 64-bit kernel.  This is a good solution for
memory-constrained systems; switching to 32-bit halves the memory usage of
the system.  Since, on most systems, almost all memory is used by user
space, running a 64-bit kernel has a relatively small cost.  Please, he
asked, do not run 32-bit kernels on 64-bit processors.




There are some definite pain points that come with maintaining 32-bit
support; most of the complaints, he said, come from developers in the
memory-management subsystem.  The biggest problem there is the need to
support high memory; it is complex, and requires support throughout the
kernel.  High memory is needed when the kernel lacks the address space to
map all of the installed physical memory; that tends to be at about 800MB
on 32-bit systems. (See this article for
more information about high memory).

Currently the kernel is able to support 32-bit systems with up to 16GB of
installed memory.  Such systems are exceedingly rare, though, and support
for them will be going away soon.  There are a few 4GB systems out there,
including some Chromebooks.  Systems with 2GB are a bit more common.  Even
these systems, he said, are "a bit silly" since the memory costs
more than the CPU does.  There are some use cases for such systems, though.
Most 32-bit systems now have less than 1GB of installed memory.  The
kernel, soon, will not try to support systems with more than 4GB.

There are some ideas out there for how to support the larger-memory 32-bit
systems without needing the high-memory abstraction.  Linus Walleij is
working on entirely separating the kernel and user-space address spaces,
giving each 4GB to work with; this is a variant on the "4G/4G" approach
that has occasionally been tried for many years.  It is difficult to make
such a system work efficiently, so this effort may never succeed, Bergmann
said.

Another approach is the proposed "densemem" memory model, which does some
fancy remapping to close holes in the physical address space.  Densemem can
support up to 2GB and is needed to replace the
SPARSEMEM memory model, the removal of which which will eventually be
necessary in any case.  This work has to be completed before high memory
can be removed; Bergmann said that he would be interested in hearing from
potential users of the densemem approach.

One other possibility is to drop high memory, but allow the extra physical
memory to be used as a zram swap
device.  That would not be as efficient as accessing the memory directly,
but it is relatively simple and would make it possible to drop the
complexity of high memory.

Then, there is the year-2038 problem, which
he spent several years working on.  The kernel-side work was finished in
2020; the musl C library was updated that same year, and the GNU C
Library followed the year after.  Some distributors have been faster than
others to incorporate this work; Debian and Ubuntu have only become
year-2038-safe this year.

The year-2038 problem is not yet completely solved, though; there are a lot
of packages that have unfixed bugs in this area.  Anything using futex(),
he said, has about a 50% chance of getting time handling right.  The legacy
32-bit system calls, which are not year-2038 safe, are still enabled in the
kernel, but they will go away at some point, exposing more bugs.  There are
languages, including Python and Rust, that have a lot of broken language
bindings.  Overall, he said, he does not expect any 32-bit desktop system to
survive the year-2038 apocalypse.

A related problem is big-endian support, which is also 32-bit only, and
also obsolete.  Its removal is blocked because IBM is still supporting
big-endian mainframe and PowerPC systems; as long as that support
continues, big-endian support will stay in the kernel.

A number of other types of support are under discussion.  There were once
32-bit systems with more than eight CPUs, but nobody is using those
machines anymore, so support could be removed.  Support for armv4
processors, such as the DEC StrongARM CPU,
should be removed.  Support for early armv6 CPUs, including the omap2 and
i.mx31, "complicates everything"; he would like to remove it, even
though there are still some Nokia
770 systems in the wild.  The time is coming for the removal of all
non-devicetree board files.  Removal of support for Cortex M CPUs,
which are nommu systems, is coming in a couple of years.  Developers are
eyeing i486 CPU support, but that will not come out yet.  Bergmann has sent
patches to remove support for KVM on 32-bit CPUs, but there is still
"one PowerPC user", so that support will be kept for now.

To summarize, he said, the kernel will have to retain support for armv7
systems for at least another ten years.  Boards are still being produced
with these CPUs, so even ten years may be optimistic for removal.
Everything else, he said, will probably fade away sooner than that.  The
removal of high-memory support has been penciled in for sometime around
2027, and nommu support around 2028.  There will, naturally, need to be
more discussion before these removals can happen.

An audience member asked how developers know whether a processor is still
in use or not; Bergmann acknowledged that it can be a hard question.  For
x86 support, he looked at a lot of old web pages to make a list of which
systems existed, then showed that each of those systems was already broken
in current kernels for other reasons; the lack of complaints showed that
there were no users.  For others, it is necessary to dig through the Git
history, see what kinds of changes are being made, and ask the developers
who have worked on the code; they are the ones who will know who is using
that support.

Another person asked about whether the kernel would support big-endian
RISC-V systems.  Bergmann answered that those systems are not supported
now, and he hoped that it would stay that way.  "With RISC-V, anybody
can do anything, so they do, but it is not always a good idea".  The
final question was about support for nommu esp32 CPUs; he answered that
patches for those CPUs exist, but have not been sent upstream.  Those
processors are "a cool toy", but he does not see any practical
application for them.

The slides
for this talk are available.  The curious may also want to look at Bergmann's 2020 take on this topic.


[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           KernelArchitectures
            ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Implementing a Foil Sticker Effect]]></title>
            <link>https://www.4rknova.com/blog/2025/08/30/foil-sticker</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095460</guid>
            <description><![CDATA[A breakdown of how to build a custom Three.js shader that recreates the iridescent, sparkling look of foil stickers using vertex deformation, angle-based color shifts, and procedural flakes.]]></description>
            <content:encoded><![CDATA[In this post, I’ll walk you through how to create a custom shader in Three.js that simulates the look of a foil sticker, complete with angle-dependent iridescence and sparkling metallic flakes. The goal is to capture that premium, holographic effect you see on collectible stickers, trading cards, and high-end packaging, but to render it in real time directly in the browser.IridescenceIf you’ve ever tilted a holographic sticker or watched sunlight catch on a soap bubble, you’ve seen iridescence in action. In the real world, this rainbow shimmer comes from thin-film interference. When light waves bounce between layers of a surface, some wavelengths are reinforced while others cancel out, causing colors to shift depending on your viewing angle.In real-time computer graphics, we don’t need to simulate the exact physics. Instead, we can approximate this by mapping view angle to hue, as the surface tilts relative to the camera, its color smoothly shifts through a spectrum. This gives that dynamic, “alive” quality you expect from foil stickers.Foil FlakesAlongside the shifting colors, there’s another key detail: foil flakes. Real metallic foils have tiny reflective particles embedded in them, creating hundreds of bright, sharp highlights that twinkle as you move. These aren’t smooth reflections but randomized sparkles, giving the surface its tactile, premium feel.To replicate this in a shader, we’ll introduce procedural noise to generate small random patches of brightness across the surface. When combined with lighting, they look like metallic specks catching the light. Together, angular hue shifts and flake sparkles create a convincing illusion of printed holographic foil without expensive rendering tricks.ImplementationThis implementation simulates a peeling, iridescent sticker with foil flakes using Three.js. While I will borrow concepts such as metalness, roughness, and Fresnel from Physically Based Rendering (PBR), this shader is not physically based. The goal is to create a visually plausible, artistic effect.Below is a live demo of the shader, where you can modify its parameters and experiment with different configurations. Use your mouse to rotate the sticker around and see how the material reacts to the lighting.Vertex ShaderThe vertex shader handles the peel geometry and passes useful information to the fragment shader.Uniform / VaryingTypePurposeuPeelAmountfloatOverall peel strength (0 = flat, 1 = fully peeled).uPeelAnglefloatPeel direction in degrees.vUvvec2UV coordinates for texture mapping.vWorldPosvec3Vertex position in world space.vNormalvec3Transformed normal for lighting.vAOIntensityfloatDistance moved by vertex, used to darken lifted areas.The shader goes through the following simple steps:Compute vector from hinge to current vertex.Calculate the peel factor and angle.Define the rotation axis and apply Rodrigues’ rotation formula to rotate the vertex around that axis.Apply the same rotation to the normal.Calculate a fake ambient occlusion term.Here’s the full vertex shader code:uniform float uPeelAmount;  // Strength of peel (0.0 → no peel, 1.0 → full peel)
uniform float uPeelAngle;   // Peel angle in degrees (converted to radians in shader)
varying vec2  vUv;          // UV coordinates
varying vec3  vWorldPos;    // Vertex position in world space
varying vec3  vNormal;      // Transformed vertex normal
varying float vAOIntensity; // Ambient occlusion or peel intensity factor

void main() {
    vUv = vec2(uv.x, 1.0 - uv.y);
    vec3 pos = position;

    // Define hinge point for peel
    vec3 hinge = vec3(0.0, 0.0, 0.0);

    // Vector from hinge to current vertex
    vec3 toVertex = pos - hinge;

    // Peel factor calculation
    // Interpolates peel strength diagonally
    // (bottom-left → top-right)
    float peelFactor = (uv.x + uv.y) * 0.5;

    // Convert peel angle to radians
    // Final angle is scaled by peelAmount
    // and per-vertex peelFactor
    float radAngle = radians(uPeelAngle);
    float angle = radAngle * uPeelAmount * peelFactor;

    // Define rotation axis for peel
    // Diagonal axis pointing from top-left 
    // to bottom-right
    vec3 axis = normalize(vec3(-1.0, 1.0, 0.0));
    float cosA = cos(angle);
    float sinA = sin(angle);

    // Apply Rodrigues' rotation formula
    // Rotates the vertex around the diagonal axis
    vec3 rotated = toVertex * cosA +
                   cross(axis, toVertex) * sinA +
                   axis * dot(axis, toVertex) * (1.0 - cosA);

    // Update vertex position after rotation
    pos = hinge + rotated;

    // Rotate vertex normal the same way to
    // ensure lighting matches the peeled
    // geometry
    vec3 rotatedNormal = normal * cosA +
                         cross(axis, normal) * sinA +
                         axis * dot(axis, normal) * (1.0 - cosA);

    // Transform normal into view space
    vNormal = normalize(normalMatrix * rotatedNormal);

    // Transform vertex to world space
    vec4 worldPos = modelMatrix * vec4(pos, 1.0);
    vWorldPos = worldPos.xyz;

    // Ambient Occlusion term based on distance moved
    // from original vertex position
    vAOIntensity = length(toVertex - rotated);

    // Final projection
    gl_Position = projectionMatrix * viewMatrix * worldPos;
}
Fragment ShaderThe fragment shader handles all lighting, reflections, iridescence, and foil flakes. It layers procedural effects to create a rich, dynamic look.UniformTypePurposemapsampler2DSticker albedo + alpha.envMap2Dsampler2DEnvironment map for reflections.uCameraPosvec3Camera position for view vector.uAlphaCutofffloatDiscard pixels below this alpha.uFlakesEnabledfloatToggle foil flakes.uFlakeSizefloatSize of flakes.uFlakeReductionfloatRandomness threshold for flakes.uFlakeThresholdfloatBrightness threshold to show flakes.uFlakeBrightnessfloatBase brightness of flakes.uMetalnessfloatPBR-like metal reflectivity control.uRoughnessfloatControls reflection sharpness.uEnvIntensityfloatScales environment contribution.uMetalmaskfloatMask controlling metallic regions.uIridescencefloatStrength of angle-dependent rainbow effect.uIriMin, uIriRangefloatRange for simulated film thickness.uPeelAmount, uPeelAnglefloatPeel geometry info for shading.This is how this works:Alpha cutoff to discard transparent pixels early.Back-face shading to render the rear surface as plain white or darkened, depending on peel.Foil flakes are computed using procedural noise. Normals are perturbed slightly to create sparkle variation. The environment map is sampled to get an iridescent tint.Iridescence (thin-film approximation) is calculated using sine-based waves to shift hue by view angle.Environment reflections are modulated by Fresnel.Final shading combines diffuse base, reflections, iridescence, and flakes.Here’s the full vertex shader code:precision highp float;

#define PI  3.14159265

varying vec2 vUv;
varying vec3 vNormal;
varying vec3 vWorldPos;
varying float vAOIntensity;

uniform sampler2D map;      // sticker albedo + alpha
uniform sampler2D envMap2D; // LDR equirectangular environment

uniform vec3  uCameraPos;
uniform float uAlphaCutoff;
uniform float uMaxMip;
uniform float uFlakesEnabled;
uniform float uFlakeSize;
uniform float uFlakeReduction;
uniform float uFlakeThreshold;
uniform float uFlakeBrightness;
uniform float uPeelAmount;
uniform float uPeelAngle;
uniform float uMetalness;
uniform float uRoughness;
uniform float uEnvIntensity;
uniform float uMetalmask;
uniform float uIridescence;
uniform float uIriMin;
uniform float uIriRange;

float hash(vec2 p) {
    return fract(sin(dot(p, vec2(127.1, 311.7))) * 43758.5453123);
}

// Map 3D dir to 2D equirect UV
vec2 dirToEquirectUv(vec3 dir) {
    dir = normalize(dir);
    float phi = atan(dir.z, dir.x);
    float theta = acos(clamp(dir.y, -1.0, 1.0));
    return vec2((phi + 3.14159265) / (2.0 * 3.14159265), theta / 3.14159265);
}

vec3 sampleEnvRough(vec3 R, float roughness) {
    vec2 uv = dirToEquirectUv(R);

    // Map roughness to LOD level
    float lod = roughness * uMaxMip;
    vec3 color = texture2DLodEXT(envMap2D, uv, lod).rgb;

    return color;
}

// Iridescence / thin-film color
vec3 iridescenceColor(float cosTheta) {
    float thickness = uIriMin + uIriRange * (1.0 - cosTheta);
    float phase = 6.28318 * thickness * 0.01; // scaled for visuals
    vec3 rainbow = 0.5 + 0.5 * vec3(sin(phase), sin(phase + 2.094), sin(phase + 4.188));
    return mix(vec3(1.0), rainbow, uIridescence);
}

// Convert RGB to perceived luminance (Rec.709)
float luminance(vec3 color) {
    return dot(color, vec3(0.2126, 0.7152, 0.0722));
}

void main() {

    vec4 base = texture2D(map, vUv);
    if(base.a < uAlphaCutoff)
        discard;

    if(!gl_FrontFacing) {
        float col = 1.0;
        if(uPeelAngle > 0.0) {
            col = mix(1.0, 0.2, vAOIntensity);
        }
        // Render back side as white
        gl_FragColor = vec4(vec3(col), base.a);
        return;
    }

    vec3 N = normalize(vNormal);
    vec3 V = normalize(uCameraPos - vWorldPos);
    vec3 R = reflect(-V, N);

    // Ambient occlusion / peel shadow
    float peelShadow = 0.0;

    if(uPeelAngle < 0.0) {
        peelShadow = smoothstep(0.0, 0.3, vAOIntensity);
        base.rgb *= mix(1.0, 0.3, peelShadow);
    }

    // Flakes
    float flakeIntensity = 0.0;
    vec3 flakeEnv = vec3(0.0);

    float brightness = luminance(base.rgb);

    if(uFlakesEnabled > 0.5) {
        // Procedural flake mask
        float flake = hash(floor(vUv * uFlakeSize));
        float flakeMask = smoothstep(uFlakeReduction, 1.0, flake);

        // Base brightness influence
        float flakeBoost = smoothstep(uFlakeThreshold, 1.0, brightness);

        // Perturbed flake normal
        float angleOffset = (hash(vec2(flake, flake + 3.0)) - 0.5) * 0.25;
        vec3 perturbedNormal = normalize(N + vec3(angleOffset, 0.0, angleOffset));

        // Reflection for sparkle
        vec3 PR = reflect(-V, perturbedNormal);

        // Dynamic flicker factor (only brightens, never darkens)
        float flakePhase = hash(floor(vUv * uFlakeSize) + floor(PR.xy * 15.0));
        float phaseMod = mix(1.0, 1.8, flakePhase);
        
        // Core sparkle factor (glimmer preserved)
        float flakeSpec = pow(clamp(dot(perturbedNormal, V) * 0.5 + 0.5, 0.0, 1.0), 8.0);
        flakeSpec = max(flakeSpec, 0.15); // always visible

        // Environment tint (never too dark, controlled by uniform)
        float flakeRough = clamp(uRoughness * 0.4, 0.0, 1.0);
        flakeEnv = sampleEnvRough(PR, flakeRough) * mix(0.9, 1.2, brightness);
        flakeEnv = max(flakeEnv, vec3(uFlakeBrightness));

        vec3 flakeIri = iridescenceColor(dot(perturbedNormal, V));
        flakeEnv *= mix(vec3(1.0), flakeIri, 0.9);

        // Final intensity
        flakeIntensity = flakeMask * flakeBoost * flakeSpec * phaseMod * 18.0;
        flakeIntensity = clamp(flakeIntensity, 0.0, 1.0);
    }

    // Final roughness modulation
    float finalRough = clamp(mix(uRoughness, 1.0, flakeIntensity), 0.0, 1.0);

    // Environment reflection
    vec3 env = sampleEnvRough(R, finalRough) * uEnvIntensity;

    // Blend in flake environment contribution
    env = mix(env, flakeEnv, clamp(flakeIntensity, 0.0, 1.0));

    // Fresnel term
    float cosTheta = clamp(dot(N, V), 0.0, 1.0);
    float F0 = mix(0.04, 1.0, uMetalness);
    float fres = F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);

    // Iridescence
    float metalicMask = mix(uMetalmask, 1.0, brightness);
    vec3 iriCol = iridescenceColor(cosTheta) * metalicMask;

    // Final color
    vec3 diffuse = base.rgb * (1.0 - uMetalness);
    vec3 spec = env * fres * iriCol * (1.0 - finalRough * 0.85);
    vec3 color = diffuse + spec;

    gl_FragColor = vec4(color, base.a);
}
LicensingThe code in this page is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). Feel free to share and adapt the code for non-commercial purposes with proper attribution. If you wish to use the code commercially, please contact me for a separate license agreement.]]></content:encoded>
        </item>
    </channel>
</rss>