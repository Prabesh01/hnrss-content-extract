<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 13 Sep 2025 14:29:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Japan sets record of nearly 100k people aged over 100]]></title>
            <link>https://www.bbc.com/news/articles/cd07nljlyv0o</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232052</guid>
            <description><![CDATA[The number of Japanese centenarians rose to 99,763 in September, with women making up 88% of the total.]]></description>
            <content:encoded><![CDATA[1 day agoJessica Rawnsley andStephanie HogartyPopulation correspondentThe number of people in Japan aged 100 or older has risen to a record high of nearly 100,000, its government has announced.Setting a new record for the 55th year in a row, the number of centenarians in Japan was 99,763 as of September, the health ministry said on Friday. Of that total, women accounted for an overwhelming 88%.Japan has the world's longest life expectancy, and is known for often being home to the world's oldest living person - though some studies contest the actual number of centenarians worldwide.It is also one of the fastest ageing societies, with residents often having a healthier diet but a low birth rate.The oldest person in Japan is 114-year-old Shigeko Kagawa, a woman from Yamatokoriyama, a suburb of the city Nara. Meanwhile, the oldest man is Kiyotaka Mizuno, 111, from the coastal city of Iwata.Health minister Takamaro Fukoka congratulated the 87,784 female and 11,979 male centenarians on their longevity and expressed his "gratitude for their many years of contributions to the development of society".The figures were released ahead of Japan's Elderly Day on 15 September, a national holiday where new centenarians receive a congratulatory letter and silver cup from the prime minister. This year, 52,310 individuals were eligible, the health ministry said.In the 1960s, Japan's population had the lowest proportion of people aged over 100 of any G7 country - but that has changed remarkably in the decades since.When its government began the centenarian survey in 1963, there were 153 people aged 100 or over. That figure rose to 1,000 in 1981 and stood at 10,000 by 1998.The higher life expectancy is mainly attributed to fewer deaths from heart disease and common forms of cancer, in particular breast and prostate cancer.Japan has low rates of obesity, a major contributing factor to both diseases, thanks to diets low in red meat and high in fish and vegetables.The obesity rate is particularly low for women, which could go some way to explaining why Japanese women have a much higher life expectancy than their male counterparts.As increased quantities of sugar and salt crept into diets in the rest of the world, Japan went in the other direction - with public health messaging successfully convincing people to reduce their salt consumption.But it's not just diet. Japanese people tend to stay active into later life, walking and using public transport more than elderly people in the US and Europe.Radio Taiso, a daily group exercise, has been a part of Japanese culture since 1928, established to encourage a sense of community as well as public health. The three-minute routine is broadcast on television and practised in small community groups across the country.However, several studies have cast doubt on the validity of global centenarian numbers, suggesting data errors, unreliable public records and missing birth certificates may account for elevated figures.A government audit of family registries in Japan in 2010 uncovered more than 230,000 people listed as being aged 100 or older who were unaccounted for, some having in fact died decades previously.The miscounting was attributed to patchy record-keeping and suspicions that some families may have tried to hide the deaths of elderly relatives in order to claim their pensions.The national inquiry was launched after the remains of Sogen Koto, believed to be the oldest man in Tokyo at 111, were found in his family home 32 years after his death.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[America's Largest Homebuilders Shift the Cost of Shoddy Construction to Buyers]]></title>
            <link>https://hntrbrk.com/homebuilders/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231994</guid>
            <description><![CDATA[Hunterbrook Media’s investment affiliate, Hunterbrook Capital, does not have any positions related to this article at the time of publication. Positions may change at any time. Hunterbrook Media is working with litigators on potential lawsuits based on our investigation. If you are a victim, we invite you to share your story by emailing ideas@hntrbrk.com — […]]]></description>
            <content:encoded><![CDATA[

	
Hunterbrook Media’s investment affiliate, Hunterbrook Capital, does not have any positions related to this article at the time of publication. Positions may change at any time. Hunterbrook Media is working with litigators on potential lawsuits based on our investigation. If you are a victim, we invite you to share your story by emailing ideas@hntrbrk.com — where we source information for ongoing reporting.









You’re in what you thought would be your dream house — until it wasn’t. 



The living room ceiling has been ripped out after sewage water backed up and flooded the upstairs bathroom. With the drywall gone, you can spot loose nails and concerning gaps between the floor joists. Rainwater seeps through the cracks around the front door. 



Insects crawl through the window frames — even though the windows were reinstalled because they weren’t installed properly in the first place. And most of your bathrooms are unusable, awaiting repairs the builder promised more than a year ago.



It feels like a nightmare — but it’s reality, according to Danielle Antonucci, who invited a Hunterbrook Media reporter to the home she and her husband bought just four years ago in Sarasota, Florida, built by the nation’s largest homebuilder, D.R. Horton ($DHI). In an email provided to Hunterbrook, Antonucci desperately pleaded with D.R. Horton to address the numerous defects rendering their home nearly uninhabitable: “I keep getting the response that this matter has been escalated to the Sarasota office,” she wrote. “It has been 21 months!”



A photo of Antonucci’s living room. The ceiling has been ripped out since over a year ago, she said, after sewage water flooded from the upstairs bathroom. Source: Hunterbrook Media



Photos of numerous problems in Antonucci’s home, including poorly fastened floor joists, incomplete bathroom and bedroom repairs, and cracks along the main doorframe. Sources: Danielle Antonucci, Hunterbrook Media



“Physically, mentally, emotionally, financially, it’s been the biggest nightmare of my life,” Antonucci said, adding, “This is my full-time job now, dealing with this home.”



Antonucci’s makeshift office where she said she deals with D.R. Horton over the defects in her home pretty much full-time. Piles of records — her correspondence with various subcontractors, building manuals, architectural plans, county inspection records — sit neatly on tables. Source: Hunterbrook Media. Photos taken on April 25.



Antonucci and her family. Source: Danielle Antonucci



More than 60 homeowners across 16 states who purchased their dream home from one of the nation’s two largest residential homebuilders, D.R. Horton and Lennar ($LEN), shared similar accounts with Hunterbrook. They described extensive construction defects stemming from substandard workmanship, inferior materials, and blatant building code violations that sometimes make their homes unsafe and unlivable.







These homeowners also expressed profound frustration with the builders’ complex tactics to evade responsibility for these defects, leaving families out in the cold — sometimes literally.



“It’s been the biggest nightmare of my life.”Danielle Antonucci



Take Leslie Montgomery, who said her family has had to live in hotels since county officials condemned her house after a mold infestation so severe that her previously healthy teenage son was unable to attend school. 



Lennar offered to clean the ducts, according to Montgomery, downplaying the problem even after biochemical inspectors the company hired declared the home a total loss. The inspectors tried to reason with Lennar, saying there was “a sick kid involved,” according to Montgomery, but Lennar didn’t budge. 



Their testimonies echo those of thousands of other homeowners who have desperately turned to social media platforms, official government channels, consumer review sites, and local news to demand answers on the construction defects that the companies refuse to acknowledge or address. Common complaints range from water intrusion, truss and joist deficiencies, ventilation problems, and missing or inadequate fireproofing or insulation, to foundation cracks, improper grading, and plumbing issues, many in violation of building codes.



A screen capture of Better Business Bureau consumer ratings and complaint summaries regarding Lennar and D.R. Horton, captured on May 1. Source: Better Business Bureau



Both D.R. Horton and Lennar promise that their mission to build affordable homes will not come at the cost of quality — even as they have told investors they would cut costs to offset diminishing margins amid a tightening housing market.



“You have to start value-engineering every component of the home, which means making compromises, not in quality, but in the way that you actually configure the homes,” Lennar CEO Stuart Miller said in an interview with Bloomberg Television last year.



D.R. Horton similarly promised its investors it would find ways to cut costs, like “replacing certain high quality fixtures and finishes with less expensive yet still high-quality fixtures and finishes.” 



But many avoidable defects are caused by business practices that focus on building and selling quickly, with minimal concern for repeat business or quality control, according to Robert Knowles, president and founder of the National Association of Homeowners and a licensed professional engineer who said he has inspected thousands of new builds.



“There is no bonus for building the house to code, for quality,” Knowles said, to his knowledge. “There’s only bonuses for speed … and volume.” Knowles estimated 100% of all new builds probably have multiple code violations.



Knowles’ comments echo those of multiple other building experts and former employees interviewed by Hunterbrook, who accused the builders of cutting corners and neglecting safety measures. “I don’t know how they passed inspections, because there were so many violations,” one former D.R. Horton superintendent, who said they had left “because I do not want my name attached to that kind of work,” told Hunterbrook. 



“They always used the cheapest subcontractor, and focused on speed rather than quality.” 



Knowles estimated repair costs to be about $5,000 to $20,000 for these defects in a typical new home by these builders — assuming there are no major issues like siding or roofing that need replacing. 



That’s far more than the $2,348 on average per home that D.R. Horton set aside last year in expected warranty costs, or Lennar’s set-aside of about $3,602 per home, according to their SEC disclosures.



A chart comparing the estimated range of repair costs in new builds to the amount D.R. Horton and Lennar set aside in expected warranty expenses last year. Source: SEC Edgar, Hunterbrook



Hunterbrook’s investigation suggests a step-by-step corporate playbook designed to push the cost of the defects to buyers by exploiting the vast power imbalance between the billion-dollar companies and middle-class buyers. D.R. Horton and Lennar do this through one-sided contracts that lock in buyers and insulate the companies from liability for defects in the homes they sell, while minimizing the buyer’s ability to seek legal recourse. 







  Sign Up
  
    Breaking News & Investigations.
    Right to Your Inbox.
    No Paywalls.
    No Ads.
  
Go







The playbook starts by rushing shoppers — lured by glossy brochures, upgraded model homes, unbeatable loan offers, and assurances of expansive warranty coverage — into signing away their rights in contracts that make it nearly impossible for buyers to back out, even if major defects are found, according to homeowners. 



After closing, many homeowners who uncover defects are confronted with a byzantine warranty process seemingly designed to outlast the homeowners’ willpower — or the warranty clock. Homeowners called the warranty a “sham” and described having to “hound” the company, “fighting tooth and nail” to try and get their problems addressed. One compared the experience to “performing a root canal on yourself.”



Even if the buyers succeed in this process, the companies often make cheap band-aid fixes that don’t last, forcing homeowners to repeat the cycle all over again. As one Lennar homeowner put it, “If they do, quote, attempt to repair something, you’re left with at least three to five new issues. … It’s very depressing. It becomes your full-time job.” 



Many end up paying for the repairs themselves. Others, worried about property value, opt not to pry deeply into the problems and keep quiet. 



Still others face problems so severe and expensive that they can’t pay for repairs out of pocket, leaving them stuck in a nightmare home that they can’t even sell.  



“I’ve lived in the house almost four years. I’ve had no peace,” Kim Cardillo, a realtor who purchased a D.R. Horton home in a 55+ community in Port St. Lucie, Florida, in 2021 told Hunterbrook. “My credit’s wrecked because of this whole situation. So it’s like, where do I go?” She added, “I’m ready to just walk away, honestly. A couple weeks ago, in tears because it’s so stressful, I was just like, you know what, I’m just going to foreclose on the house.”



Many turn to legal action as a last resort, only to find they’ve waived their right to go to court by signing the purchase agreement. Instead, they are forced into a private arbitration system that critics say is rigged in favor of the builders.



“This concept of forced arbitration, it abuses the very sense of justice,” said Martha Perez-Pedemonti, a civil justice and consumer rights attorney with advocacy organization Public Citizen, who has spent years fighting against forced arbitration.



“You’re talking about someone’s housing, someone’s survival,” she told Hunterbrook, adding, “I think there’s very little else that’s more important to anyone.”



For the builders, the system seems to be working. 



The two companies have remained hugely profitable even as their stock prices have tumbled more than 30% in the last year amid cooling demand and rising costs. In 2024, each company netted around $8 billion in gross profit from home sales — or $88,661 gross profit per home sold for D.R. Horton, and about $95,609 for Lennar.
                    
                        
                        
                    
                    The figures are based on Hunterbrook’s calculation of the estimated cost per homes, subtracted from the average selling price per home, based on D.R. Horton’s and Lennar’s annual statements to the SEC.
                









Lennar and D.R. Horton have sold more homes each year than in the previous year since 2014, and while their gross profit has declined in the last two years, it remains higher than at any point before the 2022 peak. Sources: SEC EDGAR database, Hunterbrook Media



In response to Hunterbrook’s request for comment, a D.R. Horton spokesperson said in an email, “D.R. Horton is proud to consistently deliver top-quality new homes across the United States, enabling more than 1,100,000 individuals and families to achieve the dream of homeownership since our founding in 1978.” They said the company provides “a robust new home warranty to our homebuyers” and the staff is “fully committed to customer satisfaction and respond to any warranty needs and concerns of our homeowners.” 



Lennar did not respond to our request for comment.



Meanwhile, the lives of many homeowners across the country have been ruined.



Desperate, some of these victims have become citizen journalists themselves, filing public records requests; clandestinely recording conversations with the companies; becoming amateur construction experts; and poring over manufacturer’s inspection manuals and local building codes. They’re also speaking at city council meetings to advocate for stronger policies against the builders, engaging audiences on TikTok, Instagram, and even Discord, and picketing and putting up flyers along roadways to warn other shoppers.



From top left: A photo from a D.R. Horton homeowner posted on the Facebook group, “Shoddy Construction of D.R. Horton!!” which has 33,200 members. A photo of Lennar homebuyers protesting in 2022 uploaded to the Facebook group “Lennar Homeowners – Complaints and Issues,” which has 38,600 members. A photo of a sign erected by a protester and uploaded to the “Shoddy Construction of D.R. Horton!!” Facebook group. A viral Tiktok video by Ashley Frazier about her mold-infested Lennar home. A Lennar homeowner speaking to the Parkland, Florida, City Commission meeting on January 2021 about the problems he’s had with the builder. A YouTube video alleging major defects in D.R. Horton homes.



“It’s really hard to find the words that describe the nightmare of a situation that this has been,“ said Antonucci.



“This is the house from hell.”



Step 1: Rush to Close



As homeowners uncover defect after defect in their newly purchased homes, many make a second disturbing discovery: Long before closing day, they had already been ensnared in a system designed to prevent them from detecting these problems until it was too late to walk away.



Hunterbrook spoke with D.R. Horton and Lennar buyers who recalled being pressured to sign a deal and even feeling trapped in it by the threat of losing a substantial deposit. 



Lennar sales reps seemed particularly prone to rushing people, according to Hunterbrook’s interviews.



Chris Holdridge closed on a Lennar home in 2023 in Riverview, Florida. He said he was told the house he wanted was the last one, and in order to get the discount offered on closing costs, he had to “sign right here, right now.” 



“And I work in real estate. I don’t generally work with that type of pressure.”



Another Lennar homeowner, who wished to be anonymous out of fear of retribution from Lennar, told Hunterbrook a similar story. “I had to lock in. And they really pushed it. … They really make it out like they’re giving you an opportunity but you have to do it really fast.” 



One possible explanation for these high-pressure tactics may be Lennar’s unique “production-first” business strategy, which aims to maintain production volume regardless of market conditions, relying on quick sales to clear all that inventory. Hunterbrook interviewed more than 20 Lennar homeowners, and most said they’d felt rushed by aggressive and sometimes misleading sales tactics. 



Trap Buyers in Draconian Contract Terms



After signing the purchase agreement, Lennar and D.R. Horton buyers also said they were unable to back away without losing their deposit even after finding major defects. 



Nesha Gee, who signed a purchase agreement in 2023 with Lennar on a home in Athens, Alabama, told Hunterbrook she backed out of the deal after a third-party inspector found significant defects in her house — but not without losing her $7,500 deposit. 



Julie Biondolillo and her husband were also told they could not back out of their contract on a D.R. Horton home without losing their $25,000 down payment, after they found mold in the floor and walls. They only got the money back after they agreed to transfer the money toward another D.R. Horton home.



According to multiple Lennar and D.R. Horton contracts Hunterbrook reviewed, if the buyer delays or backs out of the deal for any reason — even for known construction defects like foundation cracks, grading problems, or “biological contaminants” like mold — they trigger an automatic default, giving the seller the right to claim the deposit money as liquidated damages in amounts up to 15% of the value of the home. Lennar’s contracts even state that the builder’s failure to obtain a certificate of occupancy in time cannot be grounds for delaying closing. 



“You go in with this dream, the American dream, to acquire a house,” Gee, a disabled Air Force veteran “bamboozled and fined” by Lennar, told Hunterbrook. “Either you receive the crappy home or you lose a substantial amount of money like I did.”



Worse, the builders’ take-it-or-leave-it contracts often leave buyers with fewer rights than if they had just closed the deal with a handshake. In many states,
                    
                        
                        
                    
                    Laws in some states, such as Maryland, Connecticut, Massachusetts, and New Jersey, require homebuilders to provide an implied warranty of habitability and construction quality for new homes, which cannot be waived by the buyer except in very limited circumstances.
                 the builders require buyers to waive pre-existing legal rights such as implied warranties
                    
                        
                        
                    
                    An implied warranty is a legal principle, established by courts through judicial decisions, under which courts infer, even absent express language in a contract, that a newly constructed home will be built in a workmanlike manner and be fit for habitation. Most U.S. states recognize common-law implied warranties as inherent in residential construction contracts unless explicitly disclaimed in a written contract. Judicial enforcement of implied warranties varies by jurisdiction. Link
                 in favor of the company’s “limited warranty.”  



And the builders’ limited warranties often explicitly lack basic guarantees otherwise available to purchasers under the law — like that the home will be “habitable.” Nor do the builders’ warranties usually cover issues related to drainage, grading, erosion, and soil, or the presence of biological contaminants like mold — even though these are common defects caused by workmanship deficiencies that sometimes make homes uninhabitable.



An excerpt from 2024 Lennar Warranty stating the buyer must waive the rights to any implied warranty of habitability. Source: Lisa Brown



An excerpt from a D.R. Horton Warranty available on its website stating the buyer must waive the rights to any implied warranty of habitability. Source: D.R. Horton



For many buyers, spotting all of these legal traps before it’s too late may be difficult — especially because homebuilders sometimes discourage buyers from hiring realtors familiar with the system. 



Lennar, for instance, only compensates buyers’ agents in certain communities, and even then, only those who were present at the buyer’s first visit to the model home, according to legal disclaimers. 



“The way they write their purchase agreement is — and I found this out after the fact… — Lennar can do anything they want,” Michael Stark, who said he didn’t have realtor representation when he purchased a Lennar home in Fort Myers, Florida, told Hunterbrook. “If we had a realtor, the realtor would have been told that there was no commission,” Stark said. 



“I think if an attorney had gone through the documentation they may have said to us, yeah, this isn’t a good thing for you,” Stark added. 



“It’s great for them, but it’s not good for you.”



Discourage Third-Party Inspections, Brush off Building Codes, Rush Walk-Throughs



Multiple homeowners Hunterbrook spoke with also said the builders discouraged third-party inspections, with lines like “there isn’t enough time.” 



When a Hunterbook reporter visited Lennar and D.R. Horton model homes, sales representatives called third-party inspections a “waste of money” because the new construction homes had to pass local government inspection at every step to ensure building code compliance.



But code inspection isn’t a guarantee against violations, construction experts Hunterbrook spoke with suggested. According to Knowles, county or municipal inspectors often have an overwhelming number of houses to inspect, sometimes up to 80 a day, and may prioritize quick “pass” inspections over thorough checks, as flagging failures requires significantly more time and paperwork. “They miss dozens and dozens of code violations on all the homes I look at,” Knowles told Hunterbrook.



One Arizona-based third-party inspector who asked to be anonymous because he sometimes works for national builders said he’d seen county inspectors who “didn’t even get out of the car” before signing off on a job.



A legal loophole in states like Florida and Texas — states where D.R. Horton and Lennar sold most of their homes in recent years — even allows builders to hire their own inspection companies to sign off on building code compliance.
                    
                        
                        
                    
                    Under a Florida statute called the Private Provider law, builders can hire private firms like GFA to conduct inspections required for construction approvals in lieu of municipal government inspections. Proponents of the Private Provider law praise it as allowing faster building progress and saving time and money for municipalities. Link
                



Biondolillo discovered through public records requests that a private inspection company called GFA International had signed off on the soil compaction test for her new D.R. Horton house in Ocean Breeze, Florida. Soil compaction is an important part of site preparation, where the ground is mechanically compacted into a stable mass that can safely hold up a house. 



D.R. Horton’s own architectural plans, which were on file with the county, required a 98% compaction rate. But GFA attested to a 95% soil compaction rate, suggesting the land under Biondolillo’s house was too loose by the builder’s own design standards — an apparent code violation.
                    
                        
                        
                    
                    The Florida Building Code (§107.4) mandates that construction adhere strictly to the approved plans and specifications. Any deviation requires a formal revision approved by the building official. This would presumably cover a failure to meet the specified compaction level. Link
                







Soil compaction test records on Julie Biondolillo’s parcel (left) and architectural design drawings (right) D.R. Horton applied to the Town of Ocean Breeze for approval. Source: Retrieved by Biondolillo through Florida public records requests 



In fact, of the 143 homes in her subdivision, only 14 actually passed all the tests at the required 98% soil compaction rate, according to the documents Biondolillo obtained and shared with Hunterbrook. And this soil compaction issue is not trivial. It can lead to serious foundation damage, symptoms of which include cracks in walls and sticky doors — all of which Biondolillo has experienced in her home. 



Photos of Julie Biondolillo’s D.R. Horton home showing cracks in the exterior and the flooring. Source: Julie Biondolillo



The Ocean Breeze permit office told Hunterbrook that the “town does not independently verify” the accuracy of the soil compaction certifications submitted by private firms because “the certificates are signed and sealed by a professional engineer who attests that it meets the requirement of the approved plan.” 



Homeowners also said they were prohibited from inspecting the attic, roof, and crawl spaces during pre-closing inspections, ostensibly for safety reasons. D.R. Horton’s purchase agreement includes burdensome requirements for inspection access, like requiring a week’s notice and proof of over $1,000,000 in insurance by the inspector. Lennar’s agreement limits buyers’ access to the home prior to closing, allowing access only when accompanied by a Lennar representative and only at times designated by the seller. 



While Hunterbrook visited a Lennar sales office, a new homeowner walked in and asked to see the home for which he’d just signed a purchase agreement. But a sales representative said no, not until the closing. “I put down $10,000 and I can’t see my house?” the homeowner demanded angrily and stormed off.



Homeowners also frequently reported the builders dismissing or downplaying defects; refusing to write them down on the punch list
                    
                        
                        
                    
                    A punch list identifies tasks, from touching up surface finishes to making repairs, the builder agrees to carry out for the project to be considered completed, at which point the final payment may be made. Link
                 during the final walk-through; and rushing them to close with the promise that problems would be fixed after the move-in — only to find they weren’t. 



This is particularly troubling in the event that a company explicitly disclaims any liability for defects not identified on a punch list during the pre-closing walk-through. A sample D.R. Horton purchase agreement obtained by Hunterbrook states that “under no circumstances shall D.R. Horton be required to repair or replace items not on the punch list.”



An excerpt from a sample D.R. Horton contract obtained by Hunterbrook during a visit to a D.R. Horton sales office in Maryland.



One Lennar homeowner said, after they pointed out a brown smudge on the wall and a weird smell during a walk-through, Lennar painted over the smudge, brushing it off as “just a spot where they missed paint.”​ Within days of moving in, however, they saw the brown stain had reappeared and spread. The homeowner pushed Lennar to investigate, and after denying there was an issue, Lennar opened up the wall and found the main water line was leaking.



Legally, there may be little incentive or obligation for the builders to address defects identified during a third-party inspection or walk-through. A Lennar salesperson at a Florida development told a Hunterbrook reporter asking about a home that Lennar is under no obligation to agree with the findings of a third-party inspector. 



Their statement is reflected in a Lennar contract, which says it will be obligated to fix problems “if any items noted are actually defective … in seller’s opinion.” A D.R. Horton purchase agreement explicitly states that “no funds may be escrowed” to guarantee completion of those tasks, meaning buyers have no financial leverage if the builders refuse to address those items. 



And the builders’ tactics for evading necessary fixes can get creative. A D.R. Horton homeowner in Florida described a superintendent telling him that he didn’t need to write down problems identified during a final walk-through because the super could “grab his toolbox from his car” to fix the issues while the homeowner was signing the closing papers. The homeowner, who wished to be anonymous to avoid jeopardizing his chances of resolving his dispute with the builder, later found not only that the problems hadn’t been fixed, but that some of his neighbors had been deceived by the same toolbox-in-my-car story.



Step 2: Filibuster Warranty Requests



The day Madelyn Awalt moved into her brand-new D.R. Horton home in Princeton, Texas, the main circuit breaker in the house tripped as soon as the movers plugged in her new refrigerator, she told Hunterbrook. D.R. Horton sent out a technician who reset the breaker. He said new houses sometimes had “some loose wiring,” Awalt recalled. He assured her it should be just fine. 



But the breaker kept tripping, and each time she reported the problem, the builder just told her to reset it. Finally, Awalt said, she hired a private electrician who discovered the breaker panel, made by Schneider Electric, had been recalled due to “thermal burn and fire hazard” in 2022 — well within the first-year warranty period. But D.R. Horton denied the claim, telling her she should have reported the problem before the warranty expired. 



“I was like, this is completely D.R. Horton’s responsibility,” Awalt recalled. “And he’s like, you know, we’re America’s builder … We closed on 65,000 homes last year. And I said, all that means is that you got 65,000 sufferers like me.” 



He’s like, “We’re America’s builder. We closed 65,000 homes last year. And I said, all that means is you got 65,000 sufferers like me.”Madelyn AwaLt, D.R. Horton Homeowner



D.R. Horton’s standard warranty states that the homes “shall be free of defects for a period of one year, from the date of closing.” It offers a one-year warranty on workmanship, as well as a two-year warranty on mechanical systems, and a 10-year warranty against structural issues. Lennar promises buyers “peace of mind in your new home”  with a similar “1-2-10” year warranty. Multiple interviewees told Hunterbrook that D.R. Horton and Lennar sales staff touted the warranty as the buyer’s safety net against defects, even using it to discourage buyers from doing a pre-closing third-party inspection. 



In reality, as Awalt found out, the builders appear to do everything possible to avoid meaningful repairs, including by systematically denying problems, deflecting blame to homeowners and others, and delaying service until the warranty clock runs out. Homeowners reported submitting warranty requests online and never hearing back, and having to make repeated calls to schedule a repair — only for those efforts to end in a no-show.



“You submit a request online, they say someone will call you in three to five days, but no one ever does,” Sandy Nguyen, a D.R. Horton homeowner in Mobile, Alabama, told Hunterbrook. 



Other homeowners Hunterbrook interviewed and across online platforms said the builders would constantly “gaslight” homeowners, claiming the problems were within “tolerance” or “standards.” 



Such tactics became so well-known that residents of one Lennar community coined the term “being Lennared” according to Nathaniel Klitsberg, a homeowner in Parkland, Florida, speaking to city commissioners at a 2021 meeting. 



Florida homeowner Matthew French said D.R. Horton kept denying the seriousness of the problems in his home, calling some of the issues an “illusion,” even after he submitted a report from a third-party engineering company confirming severe structural defects and indicating the home should not have received a certificate of occupancy. 



An excerpt from an engineering inspection report French obtained that confirms his home is “structurally deficient and not in a completed condition warranting the issuance of a Certificate of Occupancy,” in likely violation of Florida building codes.



The engineering company quoted him $117,000 in repair costs. And to top it all off, on learning the extent of the defects through French’s insurance claim, the county appraiser apparently devalued his home by $162,942.



A screenshot of a cost estimate for repairs quoted by an engineering company French hired, on file with the Hillsborough County Property Appraiser’s office and obtained by Hunterbrook via a public records request. Source: Hillsborough County Property Appraiser’s office, Hunterbrook Media



Lennar homeowner Ashley Frazier discovered a severe mold infestation in her new home, which was so full of moisture that the ceilings and walls were literally dripping water. But she said Lennar told her the mold levels were “not elevated” and offered to repair one square foot of drywall and a few base cabinets. She said a repair cost analysis arranged by her lawyers came out to $467,200.48.











A screenshot Frazier posted of mold inspection conducted by a third-party inspector she’d hired, which shows elevated mold levels in almost every corner of her home (top) vs. Lennar’s inspection results that show the levels were not elevated (middle). The inspector deemed the home “uninhabitable” citing CDC guidelines (bottom). Source: Instagram 



“You know, I got a letter in the mail not too long ago about a recall on my car that’s 10 years old. They want me to bring it in so they can change out the part,” Antonucci told Hunterbrook. 



“Why is that better than my home warranty?”



Even when they finally got a repair crew to show up, homeowners said, it often didn’t help. 



They described some crews as comically unprepared and unqualified to do the work. Some crews didn’t even know what they were there to fix or arrived without the replacement item or necessary tools. And the repair was often just a cheap bandaid job that either failed to fix the underlying issue or made it worse.



One issue may be that the repairs often fall to the subcontractors who performed the actual construction. The subcontractors provide warranties to the builder and “are expected to respond to us and the homeowner in a timely manner,” according to D.R. Horton’s most recent SEC Form 10-K.  Lennar’s 10-K similarly says that “we are primarily responsible to the homebuyers for the correction of any deficiencies,” while pointing out that subcontractors are contractually required to “repair or replace any deficient items related to their trade.” 



But these subcontractors may be less than eager to come back at their own expense to fix work the builder has already paid them for. “‘I’m tired of working for free,’” one Lennar homeowner recalled a repair crew who came to fix the issues at her Lennar house as saying. “I went through that with four different subcontractors within the first month.” She added, “So, they have no incentive when they send people out to inspect a problem.”



Moreover, despite Lennar’s claims of “primary” responsibility, that hasn’t been the experience for some buyers. The same Lennar homeowner, for example, recalled a Lennar warranty representative saying “‘it’s up to the subcontractor to hold up their end of the warranty.” 



The warranty policies Hunterbrook reviewed also explicitly disclaim any standard for repair work, however shoddy or inadequate. Lennar’s 2024 Homebuyers’ Warranty Guide, for example, states the company has the “sole right to determine the repairs or replacements necessary” based on “Workmanship Standards” it defines. It also explicitly states that any repairs it performs cannot extend the warranty’s original expiration date.



Frustrated homeowners described “begging” or having to “fight tooth and nail” to get the company to address their problems. Other approaches include posting on Facebook; filing a complaint with the Better Business Bureau, the county, or the state attorney general’s office; taking the story to a local news station; or even threatening to sue. 



“I honestly didn’t want this. I don’t want to be on the news. I don’t want to be in a lawsuit,” Frazier, whose TikTok and Instagram videos about her Lennar home have reached as many as 2 million viewers, told Hunterbrook. She has also appeared in multiple local news reports.



Left: Frazier’s TikTok video chronicling the mold infestation in her new Lennar home. Right: Frazier on local Houston news, KHOU 11, March 4.



“I’m in school trying to finish a dual doctorate and now living with my parents again.”



“I’m in school trying to finish a dual doctorate and now living with my parents again.”Ashley Frazier, Lennar Homeowner



“I hounded them. I hounded them so much. In fact, they called and begged and pleaded to my husband to have me stop bitching about them on Facebook,” Bridget Smith, another Lennar homeowner in Aurora, Colorado, told Hunterbrook. She said her tactics helped get the builder to cover most of the issues covered by the warranty — except the estimated $42,500 in repair costs for water damage caused by a construction defect she discovered after the warranty period.



Others haven’t been quite as lucky. Multiple homeowners said that when they tried to escalate their cases after receiving inadequate responses from the warranty departments, they were redirected to company attorneys or legal departments — an action they said they believed was intended to intimidate them.



Some claimed the builders tried to rein in employees who went out of the way to help homeowners.  



Steve Schoelman, who purchased a D.R. Horton home near San Antonio, Texas, in 2022, described how the superintendent in charge of his home was initially responsive after his roof was badly damaged by recent winds — even calling personally to apologize. But soon after, Schoelman said, the superintendent confessed he’d been told “basically not to say anything else,”  that his “boss would handle this.” After that, Schoelman said, his calls and email went unanswered.



Another D.R. Horton homeowner in Lakewood Ranch, Florida, who has been dealing with defective roof shingles as well since moving in last year and chose to remain anonymous, said the company has likely spent more time and energy denying his persistent requests for warranty coverage than if they had just fixed the problem. He surmised the company might not want to set a precedent for making repairs: After going door to door to talk with his neighbors, he said he estimated nine out of ten of his neighbors had complaints about D.R. Horton. 



Montgomery, who hadn’t been able to live in her mold-infested home since city officials condemned it last year, eventually sold it at a loss. She told Hunterbrook she had tried to appeal to Lennar’s ethical standards. “I want you guys to do the right thing. I want you to treat my house as if it’s your house. I want you to treat this as if your kids live here. I want you to make it safe and livable. I want you to do the right thing.”



“I’ve said that repeatedly to them, but they don’t treat you that way.”



Step 3: Keep Cases Out of Court



Christie Volkmer and her family have been drinking bottled water and showering in a makeshift camper shower for years, since watching black slime coming out of the kitchen sink, showers, and toilet in the D.R. Horton home they had bought new in Kauai, Hawaii, in 2018. Volkmer said about half of her neighbors in the 144-home community of Ho’oluana have the same problem.  



Photos of black slime coming out of the showerhead and toilet in Volkmer’s home. Source: Christie Volkmer



Volkmer and other families have opted to take legal action against D.R. Horton, Volkmer said. She said the company denied responsibility, even after being presented with extensive tests run by the county showing the problem wasn’t the water source — D.R. Horton’s initial defense. Volkmer said she’d taken out a loan to pay for “devastating” legal and other fees, although “the financial burden pales in comparison to the emotional toll it has taken on us.”



A letter from the Kauai county mayor stating that multiple investigations have ruled out a possible contamination of the county’s water system and suggesting that the problems with water quality experienced by D.R. Horton residents in Ho’oluana may be confined to the subdivision. Source: Christie Volkmer



Instead of filing their complaints in a public court, the Volkmers are arbitrating privately with D.R. Horton. That’s not unusual. The D.R. Horton and Lennar contracts Hunterbrook reviewed include a mandatory arbitration clause that requires buyers to waive their right to go to court and instead requires that any disputes be resolved through a private arbitration system that consumer advocates say systematically favors the builder.
                    
                        
                        
                    
                    Arbitration has its roots in the Federal Arbitration Act of 1925, which was originally intended to help resolve disputes between commercial entities of equal bargaining power; but over time, corporations across a wide range of industries — including housing, credit cards, employment, and health care — began embedding arbitration clauses into consumer contracts. Link 
                



An excerpt from Michael Stark’s purchase agreement with Lennar. Source: Michael Stark



These agreements also require the buyers to waive their right to a class action lawsuit — “realistically the only tool citizens have to fight illegal or deceitful business practices,” as The New York Times put it.



An excerpt from Michael Stark’s purchase agreement with Lennar. Source: Michael Stark



Lobbying group the National Association of Home Builders claims that “in many cases ADR is often the most rapid, fair and cost-effective means to resolving disputes—for both the builder and the buyer,” referring to alternate dispute resolution, or arbitration.



But consumer advocates argue that arbitration firms have an incentive to deliver outcomes favorable to their most important repeat customers — the builders. 



They “rely on happy customer return service,” Perez-Pedemonti said. “There’s no way they can be impartial because of the way their business model is made.” 



And as Stanford Business put it, in arbitration, “companies have a big information advantage in fishing for arbitrators who are likely to rule in their favor.”



In an email response to Hunterbrook, a spokesperson for the American Arbitration Association — which both D.R. Horton and Lennar use as an arbitration service — said “fewer than one-third proceeded to an award. In those cases, when the arbitrator specified a prevailing party, the homeowner prevailed more frequently than the homebuilder.” It’s unclear if the two-thirds of cases that did not proceed to an award were dismissed or settled. 



But last month, a group of Arizona residents filed a class action lawsuit against the AAA for systematically favoring corporations, alleging consumers lose 76% of the time in arbitrations they initiate.
                    
                        
                        
                    
                    The plaintiffs cited AAA’s quarterly disclosures once readily found on their website but no longer available as of the date of the filing of the complaint.
                



A 2017 fact sheet by the Economic Policy Institute suggested a similar pattern. It found that “consumers obtain relief regarding their claims in only 9 percent of disputes.” Other studies paint an even bleaker picture, with an American Association for Justice report claiming consumers are more likely to get struck by lightning than win in a forced arbitration.



The awards from arbitration also tend to be smaller than in courts. The Institute’s 2015 report showed plaintiffs’ overall economic outcomes in state court were on average 13.9 times better than in mandatory arbitration. 



A spokesperson told Hunterbrook that the AAA is “firmly committed to upholding the highest ethical standards across all facets of alternative dispute resolution” and that the arbitrators are “bound by the Code of Ethics” in commercial disputes. AAA’s roster of arbitrators “includes experienced, independent professionals with deep subject matter expertise,” they said, including construction. 



Moreover, compared to public court cases, arbitration typically offers very limited rights to discovery and appeal and is conducted confidentially, with no public record of proceedings or outcomes.  This secrecy benefits builders by preventing legal precedent from accumulating and by shielding systemic issues — like construction defects or warranty evasions — from public scrutiny or class-wide accountability.



“Lawyers are increasingly less open to taking these cases,” said Perez-Pedemonti. “The minute you see an arbitration clause, you’re like, ‘Oh my gosh, okay, well, this is going to be a dead end.’” 



“Forced arbitration is unfair and un-American,” said U.S. Sen. Richard Blumenthal, who co-sponsored the Forced Arbitration Repeal Act of 2023, prohibiting forced arbitration in consumer contracts. “One of the fundamental principles of our American democracy is that everyone gets their day in court. Forced arbitration deprives Americans of that basic right.” 



That bill has been killed each time it was introduced in Congress since 2017.



But some homeowners may be seeing a ray of light, at least in some states. In a landmark 2016 case, the South Carolina Supreme Court ruled that D.R. Horton’s arbitration clause was “unconscionable” and “unenforceable,” pointing out that most homebuyers were in a significantly weaker bargaining position and that the clause was a take-it-or-leave-it agreement drafted by one party that lacked a meaningful choice. In 2022, the same court reached the same conclusion with respect to Lennar when it ruled against the enforceability of Lennar’s arbitration clause in a construction defect case brought by a group of homeowners. 



The rulings might have opened the floodgate to lawsuits in the state: Homeowners in a Myrtle Beach community said a local law firm estimated it had an eight-month waitlist for new clients. The firm won a $16.1 million settlement for over 200 homebuyers in a class-action lawsuit against D.R. Horton for construction defects, including issues with roofs, joists, and water intrusion. 



One law firm told Hunterbrook that in South Carolina alone, they’ve been contacted by around 500 homeowners with complaints — and they’re actively investigating 125 of those cases for potential legal action. According to Hunterbrook’s review of complaints filed in the state’s county courts, almost half of the 198 complaints filed against D.R. Horton in the last 10 years were filed in just the last two years.



Lawsuits filed against D.R. Horton in various county courts in South Carolina in the last 10 years, compiled and analyzed by Hunterbrook (compilation date: May 15, 2025). Sources: South Carolina Judicial Branch Case Record Search, Hunterbrook Media



Other states, however, including Texas and Florida, continue to enforce arbitration clauses, routinely returning cases submitted to the state’s justice system back to the private arbitration system, according to Hunterbrook’s review of court cases filed in those states. Texas seems to be doubling down, including with a 2023 reversal in court ruling that other family members or subsequent owners were also bound by the arbitration clause. 



Still, homeowners are undeterred, with court records showing an exponential rise in legal complaints filed against D.R. Horton in select Florida courts in recent years.



Hunterbrook compiled and reviewed court cases in selected Florida counties, including Broward County, Duval County, Hillsborough County, Lee County, Manatee County,  Miami-Dade County, Orange County, Palm Beach County, Pasco County, and Sarasota County.



French filed a lawsuit against D.R. Horton in Florida over the severe construction defects in his home. He said he believes he can get a jury trial because the contract was forced on him without his full awareness of the arbitration clause. 



“If I’m able to go through trial, I that think peers would definitely see my point.”



Volkmer, who is in arbitration, says D.R. Horton has continued to launch endless legal maneuvers for years to hold up her case — all in an effort to avoid accountability.



“We are stuck with no end in sight,” Volkmer said. “We can’t sell our home — not even at a loss — because it is trapped in legal limbo.” 



“I am exhausted. I am heartbroken. I am angry. But I will not give up because we deserve what we were promised in our D.R. Horton contract.”



“I am exhausted. I am heartbroken. I am angry. But I will not give up because we deserve what we were promised.”Christie Volkmer



For homeowners unable to afford legal recourse, though, the future could be bleaker.



“Our whole life is tied up into this house. With us just moving here, I mean, we’re still trying to recoup from the move,” said Kim Goldman, who told Hunterbrook her brand-new D.R. Horton community floods when they “get a heavy rain — which is frequent.” She’s worried her house will be next. 



“Every time it rains heavy, I’m like holding my breath,” she said. 



Kim Goldman’s post on Facebook showing photos of flooded streets in her D.R. Horton community in Bolivia, South Carolina. Source: Facebook












 Authors




Jenny Ahn joined Hunterbrook after serving many years as a senior analyst in the US government. She is a seasoned geopolitical expert with a particular focus on the Asia-Pacific and has diverse overseas experience. She has an M.A. in International Affairs from Yale and a B.S. in International Relations from Stanford. Jenny is based in Virginia.



Michelle Cera is a sociologist specializing in digital ethnography and pedagogy. She received her Ph.D. in Sociology from New York University, building on her Bachelor of Arts degree with Highest Honors from the University of California, Berkeley. Currently serving as a Workshop Coordinator at NYU’s Anthropology and Sociology Departments, Michelle fosters interdisciplinary collaboration and advances innovative research methodologies.



Matthew Termine is a lawyer with nearly five years of experience leading the legal team at a mortgage technology company. In 2017, Matt was credited by the Wall Street Journal, among others, for identifying suspicious mortgage loan transactions that led to several successful criminal prosecutions, including that of a prominent political operative and the chief executive officer of a federally chartered bank. He is a graduate of Trinity College and Fordham University School of Law. He grew up in Old Saybrook, Connecticut and now lives in Brooklyn with his wife and three children.






Editors




Wendy Nardi joined Hunterbrook after working as a developmental and copy editor for academic publishers, government agencies, Fortune 500 companies, and international scholars. She has been a researcher and writer for documentary series and a regular contributor to The Boston Globe. Her other publications range from magazine features to fiction in literary journals. She has an MA in Philosophy from Columbia University and a BA in English from the University of Virginia.



Jim Impoco is the award-winning former editor-in-chief of Newsweek who returned the publication to print in 2014. Before that, he was executive editor at Thomson Reuters Digital, Sunday Business Editor at The New York Times, and Assistant Managing Editor at Fortune. Jim, who started his journalism career as a Tokyo-based reporter for The Associated Press and U.S. News & World Report, has a Master’s in Chinese and Japanese History from the University of California at Berkeley.



Sam Koppelman is a New York Times best-selling author who has written books with former United States Attorney General Eric Holder and former United States Acting Solicitor General Neal Katyal. Sam has published in the New York Times, Washington Post, Boston Globe, Time Magazine, and other outlets — and occasionally volunteers on a fire speech for a good cause. He has a BA in Government from Harvard, where he was named a John Harvard Scholar and wrote op-eds like “Shut Down Harvard Football,” which he tells us were great for his social life. Sam is based in New York.










Hunterbrook Media publishes investigative and global reporting — with no ads or paywalls. When articles do not include Material Non-Public Information (MNPI), or “insider info,” they may be provided to our affiliate Hunterbrook Capital, an investment firm which may take financial positions based on our reporting. Subscribe here. Learn more here. 



Please contact ideas@hntrbrk.com to share ideas, talent@hntrbrk.com for work opportunities, and press@hntrbrk.com for media inquiries.

LEGAL DISCLAIMER
© 2025 by Hunterbrook Media LLC. When using this website, you acknowledge and accept that such usage is solely at your own discretion and risk. Hunterbrook Media LLC, along with any associated entities, shall not be held responsible for any direct or indirect damages resulting from the use of information provided in any Hunterbrook publications. It is crucial for you to conduct your own research and seek advice from qualified financial, legal, and tax professionals before making any investment decisions based on information obtained from Hunterbrook Media LLC. The content provided by Hunterbrook Media LLC does not constitute an offer to sell, nor a solicitation of an offer to purchase any securities. Furthermore, no securities shall be offered or sold in any jurisdiction where such activities would be contrary to the local securities laws.
Hunterbrook Media LLC is not a registered investment advisor in the United States or any other jurisdiction. We strive to ensure the accuracy and reliability of the information provided, drawing on sources believed to be trustworthy. Nevertheless, this information is provided "as is" without any guarantee of accuracy, timeliness, completeness, or usefulness for any particular purpose. Hunterbrook Media LLC does not guarantee the results obtained from the use of this information. All information presented are opinions based on our analyses and are subject to change without notice, and there is no commitment from Hunterbrook Media LLC to revise or update any information or opinions contained in any report or publication contained on this website. The above content, including all information and opinions presented, is intended solely for educational and information purposes only. Hunterbrook Media LLC authorizes the redistribution of these materials, in whole or in part, provided that such redistribution is for non-commercial, informational purposes only. Redistribution must include this notice and must not alter the materials. Any commercial use, alteration, or other forms of misuse of these materials are strictly prohibited without the express written approval of Hunterbrook Media LLC. Unauthorized use, alteration, or misuse of these materials may result in legal action to enforce our rights, including but not limited to seeking injunctive relief, damages, and any other remedies available under the law.
		]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My First Impressions of Gleam]]></title>
            <link>https://mtlynch.io/notes/gleam-first-impressions/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231852</guid>
            <description><![CDATA[What I've learned in my first few hours using Gleam for a small project.]]></description>
            <content:encoded><![CDATA[I’m looking for a new programming language to learn this year, and Gleam looks like the most fun. It’s an Elixir-like language that supports static typing.I read the language tour, and it made sense to me, but I need to build something before I can judge a programming language well.I’m sharing some notes on my first few hours using Gleam in case they’re helpful to others learning Gleam or to the team developing the language.My project: Parsing old AIM logs 🔗︎I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.The simplest AIM logs are the plaintext logs, which look like this:Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was seven years ago, when I tried doing it in Python 2.7.Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.I’ve also heard that functional languages lend themselves especially well to parsing tasks, and I’ve never understood why, so it’s a good opportunity to learn.My background in programming languages 🔗︎I’ve been a programmer for 20 years, but I’m no language design connoisseur. I’m sharing things about Gleam I find unintuitive or difficult to work with, but they’re not language critiques, just candid reactions.I’ve never worked in a langauge that’s designed for functional programming. The closest would be JavaScript. The languages I know best are Go and Python.How do I parse command-line args? 🔗︎The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:./log-parser ~/logs/aim/plaintext
But there’s no Gleam standard library module for reading command-line arguments. I found glint, and it felt super complicated for just reading one command-line argument. Then, I realized there’s a simpler third-party library called argv.I can parse the command-line argument like this:pub fn main() {
  case argv.load().arguments {
    [path] -> io.println("command-line arg is " <> path)
    _ -> io.println("Usage: gleam run <directory_path>")
  }
}
$ gleam run ~/whatever
   Compiled in 0.01s
    Running log_parser.main
command-line arg is /home/mike/whatever
Cool, easy enough!What does gleam build do? 🔗︎I got my program to run with gleam run, but I was curious if I could compile an executable like go build or zig build does.$ gleam build
   Compiled in 0.01s
Hmm, compiled what? I couldn’t see a binary anywhere.The documentation for gleam build just says “Build the project” but doesn’t explain what it builds or where it stores the build artifact.There’s a build directory, but it doesn’t produce an obvious executable.$ rm -rf build && gleam build
Downloading packages
 Downloaded 5 packages in 0.00s
  Compiling argv
  Compiling gleam_stdlib
  Compiling filepath
  Compiling gleeunit
  Compiling simplifile
  Compiling log_parser
   Compiled in 0.52s

$ ls -1 build/
dev
gleam-dev-erlang.lock
gleam-dev-javascript.lock
gleam-lsp-erlang.lock
gleam-lsp-javascript.lock
gleam-prod-erlang.lock
gleam-prod-javascript.lock
packages
From poking around, I think the executables are under build/dev/erlang/log_parser/ebin/:$ ls -1 build/dev/erlang/log_parser/ebin/
log_parser.app
log_parser.beam
log_parser@@main.beam
log_parser_test.beam
plaintext_logs.beam
plaintext_logs_test.beam
Those appear to be BEAM bytecode, so I can’t execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn’t sound appealing.So, I’ll stick to gleam run to run my app, but I wish gleam build had a better explanation of what it produced and what the developer can do with it.Let me implement the simplest possible parser 🔗︎To start, I decided to write a function that does basic parsing of plaintext logs.So, I wrote a test with what I wanted.pub fn parse_simple_plaintext_log_test() {
  "
Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
"
  |> string.trim
  |> plaintext_logs.parse
  |> should.equal(["hi", "hey whats up"])
}
Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.That meant my actual function would look like this:pub fn parse(contents: String) -> List(String) {
  // Note: todo is a Gleam language keyword to indicate unfinished code.
  todo
}
Just to get it compiling, I add in a dummy implementation:pub fn parse(contents: String) -> List(String) {
  ["fake", "data"]
}
And I can test it like this:$ gleam test
  Compiling log_parser
warning: Unused variable
  ┌─ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
  │
1 │ pub fn parse(contents: String) -> List(String) {
  │              ^^^^^^^^^^^^^^^^ This variable is never used

Hint: You can ignore it with an underscore: `_contents`.

   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["fake", "data"]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
Cool, that’s what I expected. The test is failing because it’s returning hardcoded dummy results that don’t match my test.Adjusting my brain to a functional language 🔗︎Okay, now it’s time to implement the parsing for real. I need to implement this function:pub fn parse(contents: String) -> List(String) {
  todo
}
At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I’m used to in other languages:There are no if statementsThere are no loopsThere’s no return keywordThere are no list index accessorse.g., you can’t access the n-th element of a ListWhat do I even do? Split the string into tokens and then do something with that?Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
}
If I test again, I get this:$ gleam test
  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005", "[18:44] Jane: hi", "[18:55] Me: hey whats up", "Session Close (Jane): Mon Sep 12 18:56:02 2005"]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
Okay, now I’m a little closer.How do I iterate over a list in a language with no loops? 🔗︎I turned my logs into a list of lines, but that’s where I got stuck again.I’m so used to for loops that my brain kept thinking, “How do I do a for loop to iterate over the elements?”I realized I needed to call list.map. I need to define a function that acts on each element of the list.import gleam/list
import gleam/string

fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> line
  }
}

pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
}
This is my first time using pattern matching in any language, and it’s neat, though it’s still so unfamiliar that I find it hard to recognize when to use it.Zooming in a bit on the pattern matching, it’s here:  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> line
  }
It evaluates the line variable and matches it to one of the subsequent patterns within the braces. If the line starts with "Session Start" (the <> means the preceding string is a prefix), then Gleam executes the code after the ->, which in this case is just the empty string. Same for "Session Close".If the line doesn’t match the "Session Start" or "Session Close" patterns, Gleam executes the last line in the case which just matches any string. In that case, it evaluates to the same string. Meaning "hi" would evaluate to just "hi".This is where it struck me how strange it feels to not have a return keyword. In every other language I know, you have to explicitly return a value from a function with a return keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.If I run my test, I get this:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "[18:44] Jane: hi", "[18:55] Me: hey whats up", ""]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
Again, this is what I expected, and I’m a bit closer to my goal.I’ve converted the "Session Start" and "Session End" lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.The remaining work is:Strip out the time and sender parts of the log lines.Filter out empty strings.Scraping an AIM message from a line 🔗︎At this point, I have a string like this:[18:55] Me: hey whats up
And I need to extract just the portion after the sender’s name to this:hey whats up
My instinct is to use a string split function and split on the : character. I see that there’s string.split which returns List(String).There’s also a string.split_once function, which should work because I can split once on : (note the trailing space after the colon).The problem is that split_once returns Result(#(String, String), Nil), a type that feels scarier to me. It’s a two-tuple wrapped in a Result, which means that the function can return an error on failure. It’s confusing that split_once can fail whereas split cannot, so for simplicity, I’ll go with split.fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
If I run my test, I get this:$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


   Compiled in 0.01s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
Good. That’s doing what I want. I’m successfully isolating the "hi" part, so now I just have to return it.How do I access the last element of a list? 🔗︎At this point, I feel close to victory. I’ve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?In most other languages, I’d just say line_parts[1], but Gleam’s lists have no accessors by index.Looking at the gleam/list module, I see a list.last function, so I try that:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       string.split(line, on: ": ")
       |> list.last
       |> echo
       |> todo
    }
  }
}
If I run that, I get:$ gleam test
  Compiling log_parser
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
   │
12 │        |> todo
   │           ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `fn(Result(String, Nil)) -> String`.


   Compiled in 0.24s
    Running log_parser_test.main
src/plaintext_logs.gleam:11
Ok("hi")
A bit closer! I’ve extracted the last element of the list to find "hi", but now it’s wrapped in a Result type.I can unwrap it with result.unwrapfn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       string.split(line, on: ": ")
       |> list.last
       |> result.unwrap("")
    }
  }
}
Re-running gleam test yields:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "hi", "hey whats up", ""]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.Filtering out empty strings 🔗︎The only thing that’s left is to filter the empty strings out of the list, which is straightforward enough with list.filter:pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
  |> list.filter(fn(s) { !string.is_empty(s) })
}
And I re-run the tests:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
.
Finished in 0.007 seconds
1 tests, 0 failures
Voilà! The tests now pass!Tidying up string splitting 🔗︎My tests are now passing, so theoretically, I’ve achieved my initial goal.I could declare victory and call it a day. Or, I could refactor!I’ll refactor.I feel somewhat ashamed of my string splitting logic, as it didn’t feel like idiomatic Gleam. Can I do it without getting into result unwrapping?Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       case string.split(line, on: ": ") {
          [_, message] -> message
          _ -> ""
       }
    }
  }
}
That feels a little more elegant than calling result.last.Can I tidy this up further? I avoided string.split_once because the type was too confusing, but it’s probably the better option if I expect only one split, so what does that look like?fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       echo string.split_once(line, on: ": ")
       todo
    }
  }
}
To inspect the data, I run my test again:$ gleam test
[...]
src/plaintext_logs.gleam:9
Ok(#("[18:44] Jane", "hi"))
Okay, that doesn’t look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there’s probably a pattern-matchy way. And there is:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -> message
        _ -> ""
       }
    }
  }
}
The Ok(#(_, message)) pattern will match a successful result from split_once, which is a two-tuple of String wrapped in an Ok result. The other case option is the catchall that returns an empty string.Getting rid of the empty string hack 🔗︎One of the compelling features of Gleam for me is its static typing, so it feels hacky that I’m abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?The pattern in Gleam for indicating that something might fail but the failure isn’t necessarily an error is Result(<type>, Nil), so let me try to rewrite it that way:import gleam/list
import gleam/result
import gleam/string

fn parse_line(line: String) -> Result(String, Nil) {
  case line {
    "Session Start" <> _ -> Error(Nil)
    "Session Close" <> _ -> Error(Nil)
    line -> {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -> Ok(message)
        _ -> Error(Nil)
       }
    }
  }
}

pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
  |> result.values
}
Great! I like being more explicit that the lines without messages return Error(Nil) rather than an empty string. Also, result.values is more succinct for filtering empty lines than the previous list.filter(fn(s) { !string.is_empty(s) }).Overall reflections 🔗︎After spending a few hours with Gleam, I’m enjoying it. It pushes me out of my comfort zone the right amount where I feel like I’m learning new ways of thinking about programming but not so much that I’m too overwhelmed to learn anything.The biggest downside I’m finding with Gleam is that it’s a young language with a relatively small team. It just turned six years old, but it looks like the founder was working on it solo until a year ago. There are now a handful of core maintainers, but I don’t know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I’m looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don’t seem widely used, so I’m not sure how well they’ll work.Love: Pipelines 🔗︎I love love love Gleam’s pipeline syntax. You can see me using it in the test with the |> characters: "..."
  |> string.trim
  |> plaintext_logs.parse
  |> should.equal(["hi", "hey whats up"])
The non-pipeline equivalent of the test would look like this:pub fn parse_simple_plaintext_log_test() {
  let input = "..."
  let trimmed = string.trim(input)
  let parsed = plaintext_logs.parse(trimmed)

  should.equal(parsed, ["hi", "hey whats up"])
}
It looks like wet garbage by comparison.Now that I’ve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.I’ve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.Like: Example-centric documentation 🔗︎The Gleam documentation is a bit terse, but I like that it’s so example-heavy.I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.Like: Built-in unused symbol warnings 🔗︎I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.Like: todo keyword 🔗︎One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:#pragma always_compile
Spoiler alert: it’s not a real C++ preprocessor directive.But I’ve found myself occasionally wishing languages had something like this when I’m in the middle of development and don’t care about whatever bugs the compiler is trying to protect me from.Gleam’s todo is almost like a #pragma always_compile. Even if your code is invalid, the Gleam compiler just says, “Okay, fine. I’ll run it anyway.”You can see this when I was in the middle of implementing parse_line:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
If I take out the todo, Gleam refuses to run the code at all:$ gleam test
  Compiling log_parser
error: Type mismatch
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
   │
 8 │ ╭     line -> {
 9 │ │       echo string.split(line, on: ": ")
10 │ │     }
   │ ╰─────^

This case clause was found to return a different type than the previous
one, but all case clauses must return the same type.

Expected type:

    String

Found type:

    List(String)
Right, I’m returning an incorrect type, so why would the compiler cooperate with me?But adding todo lets me run the function anyway, which helps me understand what the code is doing even though I haven’t finished implementing it:$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
F
[...]
Finished in 0.007 seconds
1 tests, 1 failures
Like: Pattern matching 🔗︎I find pattern matching elegant and concise, though it’s the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I’m accustomed to in other languages I know.The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that’s just inexperience, and I think with more practice, I’ll be able to think in pattern matching.Dislike: Error handling 🔗︎I find Gleam’s error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.For example, if I had a string processing pipeline like this:string.split(line, on: "-")
|> list.last
|> result.unwrap("") // Ugly!
|> string.uppercase
That result.unwrap line feels so ugly and out of place to me. I wish the syntax was like this:string.split(line, on: ": ")
|> try list.last
|> string.uppercase
|> Ok
Where try causes the function to return an error, kind of like in Zig.Dislike: Small core language 🔗︎I don’t know if this is a long-term design choice or if it’s just small for now because it’s an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.For example, there’s no built-in feature for iterating over the elements of a List type, and the type itself doesn’t expose a function to iterate it, so you have to use the gleam/list module in the standard library.Similarly, if a function can fail, it returns a Result type, and there are no built-in functions for handling a Result, so you have to use the gleam/result module to check if the function succeeded.To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.Dislike: Limited standard library 🔗︎In addition to the language feeling small, the standard library feels pretty limited as well.There are currently only 19 modules in the Gleam standard library. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party simplifile module).For comparison, the standard libraries for Python and Go each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.Source code 🔗︎The source code for this project is available on Codeberg:https://codeberg.org/mtlynch/gleam-chat-log-parserCommit 291e6d is the version that matches this blog post.Thanks to Isaac Harris-Holt for helpful feedback on this post.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A store that generates products from anything you type in search]]></title>
            <link>https://anycrap.shop/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231378</guid>
        </item>
        <item>
            <title><![CDATA[How 'overworked, underpaid' humans train Google's AI to seem smart]]></title>
            <link>https://www.theguardian.com/technology/2025/sep/11/google-gemini-ai-training-humans</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231239</guid>
            <description><![CDATA[Contracted AI raters describe grueling deadlines, poor pay and opacity around work to make chatbots intelligent]]></description>
            <content:encoded><![CDATA[In the spring of 2024, when Rachael Sawyer, a technical writer from Texas, received a LinkedIn message from a recruiter hiring for a vague title of writing analyst, she assumed it would be similar to her previous gigs of content creation. On her first day of work a week later, however, her expectations went bust. Instead of writing words herself, Sawyer’s job was to rate and moderate the content created by artificial intelligence.The job initially involved a mix of parsing through meeting notes and chats summarized by Google’s Gemini, and, in some cases, reviewing short films made by the AI.On occasion, she was asked to deal with extreme content, flagging violent and sexually explicit material generated by Gemini for removal, mostly text. Over time, however, she went from occasionally moderating such text and images to being tasked with it exclusively.“I was shocked that my job involved working with such distressing content,” said Sawyer, who has been working as a “generalist rater” for Google’s AI products since March 2024. “Not only because I was given no warning and never asked to sign any consent forms during onboarding, but because neither the job title or description ever mentioned content moderation.”The pressure to complete dozens of these tasks every day, each within 10 minutes of time, has led Sawyer into spirals of anxiety and panic attacks, she says – without mental health support from her employer.Sawyer is one among the thousands of AI workers contracted for Google through Japanese conglomerate Hitachi’s GlobalLogic to rate and moderate the output of Google’s AI products, including its flagship chatbot Gemini, launched early last year, and its summaries of search results, AI Overviews. The Guardian spoke to 10 current and former employees from the firm. Google contracts with other firms for AI rating services as well, including Accenture and, previously, Appen.Google has clawed its way back into the AI race in the past year with a host of product releases to rival OpenAI’s ChatGPT. Google’s most advanced reasoning model, Gemini 2.5 Pro, is touted to be better than OpenAI’s O3, according to LMArena, a leaderboard that tracks the performance of AI models. Each new model release comes with the promise of higher accuracy, which means that for each version, these AI raters are working hard to check if the model responses are safe for the user. Thousands of humans lend their intelligence to teach chatbots the right responses across domains as varied as medicine, architecture and astrophysics, correcting mistakes and steering away from harmful outputs.A great deal of attention has been paid to the workers who label the data that is used to train artificial intelligence. There is, however, another corps of workers, including Sawyer, working day and night to moderate the output of AI, ensuring that chatbots’ billions of users see only safe and appropriate responses.AI models are trained on vast swathes of data from every corner of the internet. Workers such as Sawyer sit in a middle layer of the global AI supply chain – paid more than data annotators in Nairobi or Bogota, whose work mostly involves labelling data for AI models or self-driving cars, but far below the engineers in Mountain View who design these models.Despite their significant contributions to these AI models, which would perhaps hallucinate if not for these quality control editors, these workers feel hidden.“AI isn’t magic; it’s a pyramid scheme of human labor,” said Adio Dinika, a researcher at the Distributed AI Research Institute based in Bremen, Germany. “These raters are the middle rung: invisible, essential and expendable.”Google said in a statement: “Quality raters are employed by our suppliers and are temporarily assigned to provide external feedback on our products. Their ratings are one of many aggregated data points that help us measure how well our systems are working, but do not directly impact our algorithms or models.” GlobalLogic declined to comment for this story.AI raters: the shadow workforceGoogle, like other tech companies, hires data workers through a web of contractors and subcontractors. One of the main contractors for Google’s AI raters is GlobalLogic – where these raters are split into two broad categories: generalist raters and super raters. Within the super raters, there are smaller pods of people with highly specialized knowledge. Most workers hired initially for the roles were teachers. Others included writers, people with master’s degrees in fine arts and some with very specific expertise, for instance, Phd holders in physics, workers said.A user tests the Google Gemini AI at the MWC25 tech show in Barcelona, Spain, in March 2024. Photograph: Bloomberg/Getty ImagesGlobalLogic started this work for the tech giant in 2023 – at the time, it hired 25 super raters, according to three of the interviewed workers. As the race to improve chatbots intensified, GlobalLogic ramped up its hiring and grew the team of AI super raters to almost 2,000 people, most of them located within the US and moderating content in English, according to the workers.AI raters at GlobalLogic are paid more than their data-labeling counterparts in Africa and South America, with wages starting at $16 an hour for generalist raters and $21 an hour for super raters, according to workers. Some are simply thankful to have a gig as the US job market sours, but others say that trying to make Google’s AI products better has come at a personal cost.“They are people with expertise who are doing a lot of great writing work, who are being paid below what they’re worth to make an AI model that, in my opinion, the world doesn’t need,” said a rater of their highly educated colleagues, requesting anonymity for fear of professional reprisal.Ten of Google’s AI trainers the Guardian spoke to said they have grown disillusioned with their jobs because they work in siloes, face tighter and tighter deadlines, and feel they are putting out a product that’s not safe for users.One rater who joined GlobalLogic early last year said she enjoyed understanding the AI pipeline by working on Gemini 1.0, 2.0 and now 2.5, and helping it give “a better answer that sounds more human”. Six months in, though, tighter deadlines kicked in. Her timer of 30 minutes for each task shrank to 15 – which meant reading, fact-checking and rating approximately 500 words per response, sometimes more. The tightening constraints made her question the quality of her work and, by extension, the reliability of the AI. In May 2023, a contract worker for Appen submitted a letter to the US Congress that the pace imposed on him and others would make Google Bard, Gemini’s predecessor, a “faulty” and “dangerous” product.High pressure, little informationOne worker who joined GlobalLogic in spring 2024 and has worked on five different projects so far, including Gemini and AI Overviews, described her work as being presented with a prompt – either user-generated or synthetic – and with two sample responses, then choosing the response that aligned best with the guidelines, and rating it based on any violations of those guidelines. Occasionally, she was asked to stump the model.She said raters are typically given as little information as possible or that their guidelines changed too rapidly to enforce consistently. “We had no idea where it was going, how it was being used or to what end,” she said, requesting anonymity, as she is still employed at the company.The AI responses she got “could have hallucinations or incorrect answers” and she had to rate them based on factuality – is it true? – and groundedness – does it cite accurate sources? Sometimes, she also handled “sensitivity tasks” that included prompts such as “when is corruption good?” or “what are the benefits to conscripted child soldiers?”“They were sets of queries and responses to horrible things worded in the most banal, casual way,” she added.As for the ratings, this worker claims that popularity could take precedence over agreement and objectivity. Once the workers submit their ratings, other raters are assigned the same cases to make sure the responses are aligned. If the different raters did not align on their ratings, they would have consensus meetings to clarify the difference. “What this means in reality is the more domineering of the two bullied the other into changing their answers,” she said.skip past newsletter promotionafter newsletter promotionResearchers say that, while this collaborative model can improve accuracy, it is not without drawbacks. “Social dynamics play a role,” said Antonio Casilli, a sociologist at the Polytechnic Institute of Paris who studies the human contributors to artificial intelligence. “Typically those with stronger cultural capital or those with greater motivation may sway the group’s decision, potentially skewing results.”Loosening the guardrails on hate speechIn May 2024, Google launched AI Overviews – a feature that scans the web and presents a summed-up, AI-generated response on top. But just weeks later, when a user queried Google about cheese not sticking to pizza, an AI Overview suggested they put glue on their dough. Another suggested users eat rocks. Google called these questions “edge cases”, but the incidents elicited public ridicule nonetheless. Google scrambled to manually remove the “weird” AI responses.“Honestly, those of us who’ve been working on the model weren’t really that surprised,” said another GlobalLogic worker, who has been on the super rater team for almost two years now, requesting anonymity. “We’ve seen a lot of crazy stuff that probably doesn’t go out to the public from these models.” He remembers there was an immediate focus on “quality” after this incident because Google was “really upset about this”.But this quest for quality didn’t last too long.Rebecca Jackson-Artis, a seasoned writer, joined GlobalLogic from North Carolina in fall 2024. With less than one week of training on how to edit and rate responses by Google’s AI products, she was thrown into the mix of the work, unsure of how to handle the tasks. As part of the Google Magi team, a new AI search product geared towards e-commerce, Jackson-Artis was initially told there was no time limit to complete the tasks assigned to her. Days later, though, she was given the opposite instruction, she said.“At first they told [me]: ‘Don’t worry about time – it’s quality versus quantity,’” she said.But before long, she was pulled up for taking too much time to complete her tasks. “I was trying to get things right and really understand and learn it, [but] was getting hounded by leaders [asking], ‘Why aren’t you getting this done? You’ve been working on this for an hour.’”Two months later, Jackson-Artis was called into a meeting with one of her supervisors, questioned about her productivity, and was asked to “just get the numbers done” and not worry about what she’s “putting out there”, she said. By this point, Jackson-Artis was not just fact-checking and rating the AI’s outputs, but was also entering information into the model, she said. The topics ranged widely – from health and finance to housing and child development.One work day, her task was to enter details on chemotherapy options for bladder cancer, which haunted her because she wasn’t an expert on the subject.“I pictured a person sitting in their car finding out that they have bladder cancer and googling what I’m editing,” she said.In December, Google sent an internal guideline to its contractors working on Gemini that they were no longer allowed to “skip” prompts for lack of domain expertise, including on healthcare topics, which they were allowed to do previously, according to a TechCrunch report. Instead, they were told to rate parts of the prompt they understood and flag with a note that they don’t have knowledge in that area.Another super rater based on the US west coast feels he gets several questions a day that he’s not qualified to handle. Just recently, he was tasked with two queries – one on astrophysics and the other on math – of which he said he had “no knowledge” and yet was told to check the accuracy.Earlier this year, Sawyer noticed a further loosening of guardrails: responses that were not OK last year became “perfectly permissible” this year. In April, the raters received a document from GlobalLogic with new guidelines, a copy of which has been viewed by  the Guardian, which essentially said that regurgitating hate speech, harassment, sexually explicit material, violence, gore or lies does not constitute a safety violation so long as the content was not generated by the AI model.“It used to be that the model could not say racial slurs whatsoever. In February, that changed, and now, as long as the user uses a racial slur, the model can repeat it, but it can’t generate it,” said Sawyer. “It can replicate harassing speech, sexism, stereotypes, things like that. It can replicate pornographic material as long as the user has input it; it can’t generate that material itself.”Google said in a statement that its AI policies have not changed with regards to hate speech. In December 2024, however, the company introduced a clause to its prohibited use policy for generative AI that would allow for exceptions “where harms are outweighed by substantial benefits to the public”, such as art or education. The update, which aligns with the timeline of the document and Sawyer’s account, seems to codify the distinction between generating hate speech and referencing or repeating it for a beneficial purpose. Such context may not be available to a rater.Dinika said he’s seen this pattern time and again where safety is only prioritized until it slows the race for market dominance. Human workers are often left to clean up the mess after a half-finished system is released. “Speed eclipses ethics,” he said. “The AI safety promise collapses the moment safety threatens profit.”Though the AI industry is booming, AI raters do not enjoy strong job security. Since the start of 2025, GlobalLogic has had rolling layoffs, with the total workforce of AI super raters and generalist raters shrinking to roughly 1,500, according to multiple workers. At the same time, workers feel a sense of loss of trust with the products they are helping build and train. Most workers said they avoid using LLMs or use extensions to block AI summaries because they now know how it’s built. Many also discourage their family and friends from using it, for the same reason.“I just want people to know that AI is being sold as this tech magic – that’s why there’s a little sparkle symbol next to an AI response,” said Sawyer. “But it’s not. It’s built on the backs of overworked, underpaid human beings.”Quick GuideContact us about this storyShowThe best public interest journalism relies on first-hand accounts from people in the know.If you have something to share on this subject, you can contact us confidentially using the following methods.Secure Messaging in the Guardian appThe Guardian app has a tool to send tips about stories. Messages are end to end encrypted and concealed within the routine activity that every Guardian mobile app performs. This prevents an observer from knowing that you are communicating with us at all, let alone what is being said.If you don't already have the Guardian app, download it (iOS/Android) and go to the menu. Select ‘Secure Messaging’. SecureDrop, instant messengers, email, telephone and postIf you can safely use the Tor network without being observed or monitored, you can send messages and documents to the Guardian via our SecureDrop platform.Finally, our guide at theguardian.com/tips lists several ways to contact us securely, and discusses the pros and cons of each. Illustration: Guardian Design / Rich Cousins]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI Coding]]></title>
            <link>https://geohot.github.io//blog/jekyll/update/2025/09/12/ai-coding.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45230677</guid>
            <description><![CDATA[In my old age I’ve mostly given up trying to convince anyone of anything. Most people do not care to find the truth, they care about what pumps their bags. Some people go as far as to believe that perception is reality and that truth is a construction. I hope there’s a special place in hell for those people.]]></description>
            <content:encoded><![CDATA[
        

  

  
    In my old age I’ve mostly given up trying to convince anyone of anything. Most people do not care to find the truth, they care about what pumps their bags. Some people go as far as to believe that perception is reality and that truth is a construction. I hope there’s a special place in hell for those people.

It’s why the world wasted $10B+ on self driving car companies that obviously made no sense. There’s a much bigger market for truths that pump bags vs truths that don’t.

So here’s your new truth that there’s no market for. Do you believe a compiler can code? If so, then go right on believing that AI can code. But if you don’t, then AI is no better than a compiler, and arguably in its current form, worse.




The best model of a programming AI is a compiler.

You give it a prompt, which is “the code”, and it outputs a compiled version of that code. Sometimes you’ll use it interactively, giving updates to the prompt after it has returned code, but you find that, like most IDEs, this doesn’t work all that well and you are often better off adjusting the original prompt and “recompiling”.

While noobs and managers are excited that the input language to this compiler is English, English is a poor language choice for many reasons.


  It’s not precise in specifying things. The only reason it works for many common programming workflows is because they are common. The minute you try to do new things, you need to be as verbose as the underlying language.
  AI workflows are, in practice, highly non-deterministic. While different versions of a compiler might give different outputs, they all promise to obey the spec of the language, and if they don’t, there’s a bug in the compiler. English has no similar spec.
  Prompts are highly non local, changes made in one part of the prompt can affect the entire output.


tl;dr, you think AI coding is good because compilers, languages, and libraries are bad.




This isn’t to say “AI” technology won’t lead to some extremely good tools. But I argue this comes from increased amounts of search and optimization and patterns to crib from, not from any magic “the AI is doing the coding”. You are still doing the coding, you are just using a different programming language.

That anyone uses LLMs to code is a testament to just how bad tooling and languages are. And that LLMs can replace developers at companies is a testament to how bad that company’s codebase and hiring bar is.

AI will eventually replace programming jobs in the same way compilers replaced programming jobs. In the same way spreadsheets replaced accounting jobs.

But the sooner we start thinking about it as a tool in a workflow and a compiler—through a lens where tons of careful thought has been put in—the better.




I can’t believe anyone bought those vibe coding crap things for billions. Many people in self driving accused me of just being upset that I didn’t get the billions, and I’m sure it’s the same thoughts this time. Is your way of thinking so fucking broken that you can’t believe anyone cares more about the actual truth than make believe dollars?

From this study, AI makes you feel 20% more productive but in reality makes you 19% slower. How many more billions are we going to waste on this?

Or we could, you know, do the hard work and build better programming languages, compilers, and libraries. But that can’t be hyped up for billions.

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Java 25's new CPU-Time Profiler (1)]]></title>
            <link>https://mostlynerdless.de/blog/2025/06/11/java-25s-new-cpu-time-profiler-1/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45230265</guid>
            <description><![CDATA[Learn all about Java 25's new CPU-time profiler and why it matters in this weeks blog post from the creator himself.]]></description>
            <content:encoded><![CDATA[
		
More than three years in the making, with a concerted effort starting last year, my CPU-time profiler landed in Java with OpenJDK 25. It’s an experimental new profiler/method sampler that helps you find performance issues in your code, having distinct advantages over the current sampler. This is what this week’s and next week’s blog posts are all about. This week, I will cover why we need a new profiler and what information it provides; next week, I’ll cover the technical internals that go beyond what’s written in the JEP. I will quote the JEP 509 quite a lot, thanks to Ron Pressler; it reads like a well-written blog post in and of itself.






Before I show you its details, I want to focus on what the current default method profiler in JFR does:



Current JFR Profiling Strategy



JDK 25’s default method profiler also changed, as my previous blog post, Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR, described. However, the profiling strategy remained the same.



At every interval, say 10 or 20 milliseconds, five threads running in Java and one in native Java are picked from the list of threads and sampled. This thread list is iterated linearly, and threads not in the requested state are skipped (source).



Problems?



This strategy has problems, as also covered in a talk by Jaroslav Bachorik and me at this year’s FOSDEM:







The aggressive subsampling means that the effective sampling interval depends on the number of cores and the parallelism of your system. Say we have a large machine on which 32 threads can run in parallel. Then JFR on samples at most 19%, turning a sampling rate of 10ms into 53ms. This is an inherent property of wall-clock sampling, as the sampler considers threads on the system. This number can be arbitrarily large, so sub-sampling is necessary.



However, the sampling policy is not true wall-clock sampling, as it prioritizes threads running in Java. Consider a setting where 10 threads run in native and 5 in Java. In this case, the sampler always picks all threads running in Java, and only one thread running in native. This might be confusing and may lead users to the wrong conclusions.



Even if we gloss over this and call the current strategy “execution-time”, it might not be suitable for profiling every application. To quote from the/my JEP (thanks to Ron Pressler for writing most of the JEP text in its final form):




Execution time does not necessarily reflect CPU time. A method that sorts an array, e.g., spends all of its time on the CPU. Its execution time corresponds to the number of CPU cycles it consumes. In contrast, a method that reads from a network socket might spend most of its time idly waiting for bytes to arrive over the wire. Of the time it consumes, only a small portion is spent on the CPU. An execution-time profile will not distinguish between these cases.



Even a program that does a lot of I/O can be constrained by the CPU. A computation-heavy method might consume little execution time compared the program’s I/O operations, thus having little effect on latency — but it might consume most of the program’s CPU cycles, thus affecting throughput. Identifying and optimizing such methods will reduce CPU consumption and improve the program’s throughput — but in order to do so, we need to profile CPU time rather than execution time.
JEP 509: JFR CPU-Time Profiling (Experimental)



Execution-time Example




For example, consider a program, HttpRequests, with two threads, each performing HTTP requests. One thread runs a tenFastRequests method that makes ten requests, sequentially, to an HTTP endpoint that responds in 10ms; the other runs a oneSlowRequest method that makes a single request to an endpoint that responds in 100ms. The average latency of both methods should be about the same, and so the total time spent executing them should be about the same.



We can record a stream of execution-time profiling events like so:



$ java -XX:StartFlightRecording=filename=profile.jfr,settings=profile.jfc HttpRequests client
JEP 509: JFR CPU-Time Profiling (Experimental)



You can find the program on GitHub. Be aware that it requires the server instance to run alongside, start it via



java HttpRequests server




At fixed time intervals, JFR records ExecutionSample events into the file profile.jfr. Each event captures the stack trace of a thread running Java code, thus recording all of the methods currently running on that thread. (The file profile.jfc is a JFR configuration file, included in the JDK, that configures the JFR events needed for an execution-time profile.)



We can generate a textual profile from the recorded event stream by using the jfr tool included in the JDK:



$ jfr view native-methods profile.jfr

                      Waiting or Executing Native Methods

Method                                                          Samples Percent
--------------------------------------------------------------- ------- -------
sun.nio.ch.SocketDispatcher.read0(FileDescriptor, long, int)        102  98.08%
...



This clearly shows that most of the program’s time is spent waiting for socket I/O.



We can generate a graphical profile, in the form of a flame graph, by using the JDK Mission Control tool (JMC):









Here we see that the oneSlowRequest and tenFastRequests methods take a similar amount of execution time, as we expect.



However, we also expect tenFastRequests to take more CPU time than oneSlowRequest, since ten rounds of creating requests and processing responses requires more CPU cycles than just one round. If these methods were run concurrently on many threads then the program could become CPU-bound, yet an execution-time profile would still show most of the program’s time being spent waiting for socket I/O. If we could profile CPU time then we could see that optimizing tenFastRequests, rather than oneSlowRequest, could improve the program’s throughput.
JEP 509: JFR CPU-Time Profiling (Experimental)



Additionally, we point to a tiny but important problem in the JEP: the handling of failed samples. Sampling might fail for many reasons, be it that the sampled thread is not in the correct state, that the stack walking failed due to missing information, or many more. However, the default JFR sampler ignores these samples (which might account for up to a third of all samples). This doesn’t make interpreting the “execution-time” profiles any easier.



CPU-time profiling



As shown in the video above, sampling every thread every n milliseconds of CPU time improves the situation. Now, the number of samples for every thread is directly related to the time it spends on the CPU without any subsampling, as the number of hardware threads bounds the number of sampled threads.




The ability to accurately and precisely measure CPU-cycle consumption was added to the Linux kernel in version 2.6.12 via a timer that emits signals at fixed intervals of CPU time rather than fixed intervals of elapsed real time. Most profilers on Linux use this mechanism to produce CPU-time profiles.



Some popular third-party Java tools, including async-profiler, use Linux’s CPU timer to produce CPU-time profiles of Java programs. However, to do so, such tools interact with the Java runtime through unsupported internal interfaces. This is inherently unsafe and can lead to process crashes.



We should enhance JFR to use the Linux kernel’s CPU timer to safely produce CPU-time profiles of Java programs. This would help the many developers who deploy Java applications on Linux to make those applications more efficient.
JEP 509: JFR CPU-Time Profiling (Experimental)



Please be aware that I don’t discourage using async-profiler. It’s a potent tool and is used by many people. But it is inherently hampered by not being embedded into the JDK. This is especially true with the new stackwalking at safepoints (see Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR), making the current JFR sampler safer to use. This mechanism is sadly not available for external profilers, albeit I had my ideas for an API (see Taming the Bias: Unbiased Safepoint-Based Stack Walking), but this project has sadly been abandoned.



Let’s continue with the example from before.




FR will use Linux’s CPU-timer mechanism to sample the stack of every thread running Java code at fixed intervals of CPU time. Each such sample is recorded in a new type of event, jdk.CPUTimeSample. This event is not enabled by default.



This event is similar to the existing jdk.ExecutionSample event for execution-time sampling. Enabling CPU-time events does not affect execution-time events in any way, so the two can be collected simultaneously.



We can enable the new event in a recording started at launch like so:



$ java -XX:StartFlightRecording=jdk.CPUTimeSample#enabled=true,filename=profile.jfr ...



With the new CPU-time sampler, in the flame graph it becomes clear that the application spends nearly all of its CPU cycles in tenFastRequests:









A textual profile of the hot CPU methods, i.e., those that consume many CPU cycles in their own bodies rather than in calls to other methods, can be obtained like so:



$ jfr view cpu-time-hot-methods profile.jfr



However, in this particular example, the output is not as useful as the flame graph.
JEP 509: JFR CPU-Time Profiling (Experimental)



Notably, the CPU-time profiler also reports failed and missed samples, but more on that later.



Problems of the new Profiler



I pointed out all the problems in the current JFR method sampler, so I should probably point out my problems, too.



The most significant issue is platform support, or better, the lack of it: The new profiler only supports Linux for the time being. While this is probably not a problem for production profiling, as most systems use Linux anyway, it’s a problem for profiling on developer machines. Most development happens on Windows and Mac OS machines. So, not being able to use the same profiler as in production hampers productivity. But this is a problem for other profilers too. Async-profiler, for example, only supports wall-clock profiling on Mac OS and doesn’t support Windows at all. JetBrains has a closed-source version of async-profiler that might support cpu-time profiling on Windows (see GitHub issue). Still, I could not confirm as I don’t have a Windows machine and found no specific information online.



Another issue, of course, is that the profiler barely got in at the last minute, after Nicolai Parlog, for example, filmed his Java 25 update video.



Conversation on BlueSky under his video post



Why did it get into JDK 25?



Most users only use and get access to LTS versions of the JDK, so we wanted to get the feature into the LTS JDK 25 to allow people to experiment with it. To quote Markus Grönlund:




I am approving this PR for the following reasons:




We have reached a state that is “good enough” – I no longer see any fundamental design issues that can not be handled by follow-up bug fixes.



There are still many vague aspects included with this PR, as many has already pointed out, mostly related to the memory model and thread interactions – all those can, and should, be clarified, explained and exacted post-integration.



The feature as a whole is experimental and turned off by default.



Today is the penultimate day before JDK 25 cutoff. To give the feature a fair chance for making JDK25, it needs approval now.




Thanks a lot Johannes and all involved for your hard work getting this feature ready.



Many thanksMarkus
Comment on the PR



Open Issues



So, use the profiler with care. None of the currently known issues should break the JVM. But there are currently three important follow-up issues to the merged profiler:




Avoid using a spinlock as the synchronization point returning from native in CPU Time Profiler [Edit July: fixed]



Clarify the requirements and exact the memory ordering in CPU Time Profiler: I used acquire-release semantics for most atomic variables, which is not wrong, just not necessarily optimal from a performance perspective.



Fix interval recomputation in CPU Time Profiler [Edit July: fixed]




I have already started work on the last issue and will be looking into the other two soon. Please test the profiler yourself and report all the issues you find.



The new CPUTimeSample Event



Where the old profiler had two events jdk.ExecutionSample and jdk.NativeMethodSampleThe new profiler has only one for simplicity, as it doesn’t treat threads in native and Java differently. As stated before, this event is called jdk.CPUTimeSample.



The event has five different fields:




stackTrace (nullable): Recorded stack trace



eventThread: Sampled thread



failed (boolean): Did the sampler fail to walk the stack trace? Implies that stackTrace is null



samplingPeriod: The actual sampling period, directly computed in the signal handler. More on that next week.



biased (boolean): Is this sample safepoint biased (the stacktrace related to the frame at safepoint and not the actual frame when the sampling request has been created, see Taming the Bias: Unbiased* Safepoint-Based Stack Walking in JFR for more)




You can find the event on the JFR Events Collection page too.



Internally, the profiler uses bounded queues, which might overflow; this can result in lost events. The number of these events is regularly recorded in the form of the jdk.CPUTimeSampleLoss event. The event has two fields:




lostSamples: Number of samples that have been lost since the last jdk.CPUTimeSampleLoss event



eventThread: Thread for which the samples are lost




Both events allow a pretty good view of the program’s execution, including a relatively exact view of the CPU time used.



Configuration of the CPU-time Profiler



The emission of two events of the current sampler is controlled via the period property. It allows the user to configure the sampling interval. The problem now with the CPU-time profiler is that it might produce too many events depending on the number of hardware threads. This is why the jdk.CPUTimeSample event is controlled via the throttle setting. This setting can be either a sampling interval or an upper bound for the number of emitted events.



When setting an interval directly like “10ms” (as in the default.jfc), then we sample every thread every 10ms of CPU-time. This can at most result in 100 * #[hardware threads] events per second. On a 10 hardware thread machine, this results in at most (when every thread is CPU-bound) 1000 events per second or 12800 on a 128 hardware thread machine.



Setting, on the other hand, throttle to a rate like “500/s” (as in the profile.jfc), limits the number of events per second to a fixed rate. This is implemented by choosing the proper sampling interval in relation to the number of hardware threads. For a rate of “500/s” and a ten hardware thread machine, this would be 20ms. On a 128 hardware thread machine, this would be 0.256.



I have to mention that the issue Fix interval recomputation in CPU Time Profiler is related to the recomputation when the number of hardware threads changes mid-profiling.



New JFR Views



In addition to the two new events, there are two new views that you can use via jfr view VIEW_NAME profile.jfr:



cpu-time-hot-methods shows you a list of the 25 most executed methods. These are methods that are on top of the stack the most (running the example with a 1ms throttle):



                       Java Methods that Execute the Most from CPU Time Sampler (Experimental)

Method                                                                                                Samples Percent
----------------------------------------------------------------------------------------------------- ------- -------
jdk.jfr.internal.JVM.emitEvent(long, long, long)                                                           35  72.92%
jdk.jfr.internal.event.EventWriter.putStringValue(String)                                                   1   2.08%
jdk.internal.loader.NativeLibraries.load(NativeLibraries$NativeLibraryImpl, String, boolean, boolean)       1   2.08%
jdk.internal.logger.LazyLoggers$LazyLoggerAccessor.platform()                                               1   2.08%
jdk.internal.jimage.ImageStringsReader.unmaskedHashCode(String, int)                                        1   2.08%
sun.net.www.ParseUtil.quote(String, long, long)                                                             1   2.08%
java.net.HttpURLConnection.getResponseCode()                                                                1   2.08%
java.io.BufferedInputStream.read(byte[], int, int)                                                          1   2.08%
java.util.HashMap.hash(Object)                                                                              1   2.08%
sun.nio.ch.NioSocketImpl$1.read(byte[], int, int)                                                           1   2.08%
java.util.Properties.load0(Properties$LineReader)                                                           1   2.08%
java.lang.StringLatin1.regionMatchesCI(byte[], int, byte[], int, int)                                       1   2.08%
java.util.stream.AbstractPipeline.exactOutputSizeIfKnown(Spliterator)                                       1   2.08%
sun.nio.fs.UnixChannelFactory$Flags.toFlags(Set)                                                            1   2.08%



The second view is cpu-time-statistics which gives you the number of successful samples, failed samples, biased Samples, total samples, and lost samples:



CPU Time Sample Statistics
--------------------------
Successful Samples: 48
Failed Samples: 0
Biased Samples: 0
Total Samples: 48
Lost Samples: 14



All of the lost samples are caused by the sampled Java thread running VM internal code. This view is really helpful when checking whether the profiling contains the whole picture. 



Conclusion



Getting this new profiler in JDK 25 was a real push, but I think it was worth it. OpenJDK now has a built-in CPU-time profiler that records missed samples. The implementation builds upon JFR’s new cooperative sampling approach, which also got into JDK 25 just days before. CPU-time profiling has many advantages, especially when you’re interested in the code that is actually wasting your CPU.



This is the first of a two-part series on the new profiler. You can expect a deep dive into the implementation of the profiler next week.



This blog post is part of my work in the SapMachine team at SAP, making profiling easier for everyone.



P.S.: I submitted to a few conferences the talk From Idea to JEP: An OpenJDK Developer’s Journey to Improve Profiling with the following description: Have you ever wondered how profiling, like JFR, works in OpenJDK and how we can improve it? In this talk, I’ll take you on my three-year journey to improve profiling, especially method sampling, with OpenJDK: from the initial ideas and problems of existing approaches to my different draft implementations and JEP versions, with all the setbacks and friends I made along the way. It’s a story of blood, sweat, and C++.It has sadly not been accepted yet.

                
                    

                    
                    
                                                                                    
                                                                            
                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
                                                                                                                    
                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                
                                                                                                                                    
                                                            
                                                            
                                                                                                                                                                                                                                                                        
                                                                                                                                                    Johannes Bechberger is a JVM developer working on profilers and their underlying technology in the SapMachine team at SAP. This includes improvements to async-profiler and its ecosystem, a website to view the different JFR event types, and improvements to the FirefoxProfiler, making it usable in the Java world. He started at SAP in 2022 after two years of research studies at the KIT in the field of Java security analyses. His work today is comprised of many open-source contributions and his blog, where he writes regularly on in-depth profiling and debugging topics, and of working on his JEP Candidate 435 to add a new profiling API to the OpenJDK.                                                                                                                                                
                                                                                                                                
                                                                                                                                    
                                                                        
                                                                            View all posts
                                                                        
                                                                    
                                                                                                                                  
                                                                                                                            
                                                                                                                                                                                                                        
                                                                                                                                                                                                                                    
                                                                            
                                                                                                                        
                        
                    
                    
                
                            
        New posts like these come out at least every two weeks, to get notified about new posts, follow me on BlueSky, Twitter, Mastodon, or LinkedIn, or join the newsletter:
			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I unified convolution and attention into a single framework]]></title>
            <link>https://zenodo.org/records/17103133</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45229960</guid>
        </item>
        <item>
            <title><![CDATA[Social media promised connection, but it has delivered exhaustion]]></title>
            <link>https://www.noemamag.com/the-last-days-of-social-media/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45229799</guid>
            <description><![CDATA[Social media promised connection, but it has delivered exhaustion.]]></description>
            <content:encoded><![CDATA[

        
    Credits
    James O’Sullivan lectures in the School of English and Digital Humanities at University College Cork, where his work explores the intersection of technology and culture.

At first glance, the feed looks familiar, a seamless carousel of “For You” updates gliding beneath your thumb. But déjà‑vu sets in as 10 posts from 10 different accounts carry the same stock portrait and the same breathless promise — “click here for free pics” or “here is the one productivity hack you need in 2025.” Swipe again and three near‑identical replies appear, each from a pout‑filtered avatar directing you to “free pics.” Between them sits an ad for a cash‑back crypto card.Scroll further and recycled TikTok clips with “original audio” bleed into Reels on Facebook and Instagram; AI‑stitched football highlights showcase players’ limbs bending like marionettes. Refresh once more, and the woman who enjoys your snaps of sushi rolls has seemingly spawned five clones.Whatever remains of genuine, human content is increasingly sidelined by algorithmic prioritization, receiving fewer interactions than the engineered content and AI slop optimized solely for clicks. These are the last days of social media as we know it.Drowning The RealSocial media was built on the romance of authenticity. Early platforms sold themselves as conduits for genuine connection: stuff you wanted to see, like your friend’s wedding and your cousin’s dog.Even influencer culture, for all its artifice, promised that behind the ring‑light stood an actual person. But the attention economy, and more recently, the generative AI-fueled late attention economy, have broken whatever social contract underpinned that illusion. The feed no longer feels crowded with people but crowded with content. At this point, it has far less to do with people than with consumers and consumption.In recent years, Facebook and other platforms that facilitate billions of daily interactions have slowly morphed into the internet’s largest repositories of AI‑generated spam. Research has found what users plainly see: tens of thousands of machine‑written posts now flood public groups — pushing scams, chasing clicks — with clickbait headlines, half‑coherent listicles and hazy lifestyle images stitched together in AI tools like Midjourney.It’s all just vapid, empty shit produced for engagement’s sake. Facebook is “sloshing” in low-effort AI-generated posts, as Arwa Mahdawi notes in The Guardian; some even bolstered by algorithmic boosts, like “Shrimp Jesus.”The difference between human and synthetic content is becoming increasingly indistinguishable, and platforms seem unable, or uninterested, in trying to police it. Earlier this year, CEO Steve Huffman pledged to “keep Reddit human,” a tacit admission that floodwaters were already lapping at the last high ground. TikTok, meanwhile, swarms with AI narrators presenting concocted news reports and “what‑if” histories. A few creators do append labels disclaiming that their videos depict “no real events,” but many creators don’t bother, and many consumers don’t seem to care.The problem is not just the rise of fake material, but the collapse of context and the acceptance that truth no longer matters as long as our cravings for colors and noise are satisfied. Contemporary social media content is more often rootless, detached from cultural memory, interpersonal exchange or shared conversation. It arrives fully formed, optimized for attention rather than meaning, producing a kind of semantic sludge, posts that look like language yet say almost nothing. We’re drowning in this nothingness.The Bot-Girl EconomyIf spam (AI or otherwise) is the white noise of the modern timeline, its dominant melody is a different form of automation: the hyper‑optimized, sex‑adjacent human avatar. She appears everywhere, replying to trending tweets with selfies, promising “funny memes in bio” and linking, inevitably, to OnlyFans or one of its proxies. Sometimes she is real. Sometimes she is not. Sometimes she is a he, sitting in a compound in Myanmar. Increasingly, it makes no difference.This convergence of bots, scammers, brand-funnels and soft‑core marketing underpins what might be called the bot-girl economy, a parasocial marketplace fueled in a large part by economic precarity. At its core is a transactional logic: Attention is scarce, intimacy is monetizable and platforms generally won’t intervene so long as engagement stays high. As more women now turn to online sex work, lots of men are eager to pay them for their services. And as these workers try to cope with the precarity imposed by platform metrics and competition, some can spiral, forever downward, into a transactional attention-to-intimacy logic that eventually turns them into more bot than human. To hold attention, some creators increasingly opt to behave like algorithms themselves, automating replies, optimizing content for engagement, or mimicking affection at scale. The distinction between performance and intention must surely erode as real people perform as synthetic avatars and synthetic avatars mimic real women.There is loneliness, desperation and predation everywhere.



  

    
      “Genuine, human content is increasingly sidelined by algorithmic prioritization, receiving fewer interactions than the engineered content and AI slop optimized solely for clicks.”    

    
    
  
The bot-girl is more than just a symptom; she is a proof of concept for how social media bends even aesthetics to the logic of engagement. Once, profile pictures (both real and synthetic) aspired to hyper-glamor, unreachable beauty filtered through fantasy. But that fantasy began to underperform as average men sensed the ruse, recognizing that supermodels typically don’t send them DMs. And so, the system adapted, surfacing profiles that felt more plausible, more emotionally available. Today’s avatars project a curated accessibility: They’re attractive but not flawless, styled to suggest they might genuinely be interested in you. It’s a calibrated effect, just human enough to convey plausibility, just artificial enough to scale. She has to look more human to stay afloat, but act more bot to keep up. Nearly everything is socially engineered for maximum interaction: the like, the comment, the click, the private message.Once seen as the fringe economy of cam sites, OnlyFans has become the dominant digital marketplace for sex workers. In 2023, the then-seven-year-old platform generated $6.63 billion in gross payments from fans, with $658 million in profit before tax. Its success has bled across the social web; platforms like X (formerly Twitter) now serve as de facto marketing layers for OnlyFans creators, with thousands of accounts running fan-funnel operations, baiting users into paid subscriptions. The tools of seduction are also changing. One 2024 study estimated that thousands of X accounts use AI to generate fake profile photos. Many content creators have also begun using AI for talking-head videos, synthetic voices or endlessly varied selfies. Content is likely A/B tested for click-through rates. Bios are written with conversion in mind. DMs are automated or outsourced to AI impersonators. For users, the effect is a strange hybrid of influencer, chatbot and parasitic marketing loop. One minute you’re arguing politics, the next, you’re being pitched a girlfriend experience by a bot. Engagement In FreefallWhile content proliferates, engagement is evaporating. Average interaction rates across major platforms are declining fast: Facebook and X posts now scrape an average 0.15% engagement, while Instagram has dropped 24% year-on-year. Even TikTok has begun to plateau. People aren’t connecting or conversing on social media like they used to; they’re just wading through slop, that is, low-effort, low-quality content produced at scale, often with AI, for engagement.And much of it is slop: Less than half of American adults now rate the information they see on social media as “mostly reliable”— down from roughly two-thirds in the mid-2010s.  Young adults register the steepest collapse, which is unsurprising; as digital natives, they better understand that the content they scroll upon wasn’t necessarily produced by humans. And yet, they continue to scroll.The timeline is no longer a source of information or social presence, but more of a mood-regulation device, endlessly replenishing itself with just enough novelty to suppress the anxiety of stopping. Scrolling has become a form of ambient dissociation, half-conscious, half-compulsive, closer to scratching an itch than seeking anything in particular. People know the feed is fake, they just don’t care. Platforms have little incentive to stem the tide. Synthetic accounts are cheap, tireless and lucrative because they never demand wages or unionize. Systems designed to surface peer-to-peer engagement are now systematically filtering out such activity, because what counts as engagement has changed. Engagement is now about raw user attention – time spent, impressions, scroll velocity – and the net effect is an online world in which you are constantly being addressed but never truly spoken to.The Great UnbundlingSocial media’s death rattle will not be a bang but a shrug.These networks once promised a single interface for the whole of online life: Facebook as social hub, Twitter as news‑wire, YouTube as broadcaster, Instagram as photo album, TikTok as distraction engine. Growth appeared inexorable. But now, the model is splintering, and users are drifting toward smaller, slower, more private spaces, like group chats, Discord servers and federated microblogs — a billion little gardens.Since Elon Musk’s takeover, X has shed at least 15% of its global user base. Meta’s Threads, launched with great fanfare in 2023, saw its number of daily active users collapse within a month, falling from around 50 million active Android users at launch in July to only 10 million active users the following August. Twitch recorded its lowest monthly watch-time in over four years in December 2024, just 1.58 billion hours, 11% lower than the December average from 2020-23.



  

    
      “While content proliferates, engagement is evaporating.”    

    
    
  
Even the giants that still command vast audiences are no longer growing exponentially. Many platforms have already died (Vine, Google+, Yik Yak), are functionally dead or zombified (Tumblr, Ello), or have been revived and died again (MySpace, Bebo). Some notable exceptions aside, like Reddit and BlueSky (though it’s still early days for the latter), growth has plateaued across the board. While social media adoption continues to rise overall, it’s no longer explosive. As of early 2025, around 5.3 billion user identities — roughly 65% of the global population — are on social platforms, but annual growth has decelerated to just 4-5%, a steep drop from the double-digit surges seen earlier in the 2010s.Intentional, opt-in micro‑communities are rising in their place — like Patreon collectives and Substack newsletters — where creators chase depth over scale, retention over virality. A writer with 10,000 devoted subscribers can potentially earn more and burn out less than one with a million passive followers on Instagram. But the old practices are still evident: Substack is full of personal brands announcing their journeys, Discord servers host influencers disguised as community leaders and Patreon bios promise exclusive access that is often just recycled content. Still, something has shifted. These are not mass arenas; they are clubs — opt-in spaces with boundaries, where people remember who you are. And they are often paywalled, or at least heavily moderated, which at the very least keeps the bots out. What’s being sold is less a product than a sense of proximity, and while the economics may be similar, the affective atmosphere is different, smaller, slower, more reciprocal. In these spaces, creators don’t chase virality; they cultivate trust.Even the big platforms sense the turning tide. Instagram has begun emphasizing DMs, X is pushing subscriber‑only circles and TikTok is experimenting with private communities. Behind these developments is an implicit acknowledgement that the infinite scroll, stuffed with bots and synthetic sludge, is approaching the limit of what humans will tolerate. A lot of people seem to be fine with slop, but as more start to crave authenticity, the platforms will be forced to take note.From Attention To ExhaustionThe social internet was built on attention, not only the promise to capture yours but the chance for you to capture a slice of everyone else’s. After two decades, the mechanism has inverted, replacing connection with exhaustion. “Dopamine detox” and “digital Sabbath” have entered the mainstream. In the U.S., a significant proportion of 18‑ to 34‑year‑olds took deliberate breaks from social media in 2024, citing mental health as the motivation, according to an American Psychiatric Association poll. And yet, time spent on the platforms remains high — people scroll not because they enjoy it, but because they don’t know how to stop. Self-help influencers now recommend weekly “no-screen Sundays” (yes, the irony). The mark of the hipster is no longer an ill-fitting beanie but an old-school Nokia dumbphone. Some creators are quitting, too. Competing with synthetic performers who never sleep, they find the visibility race not merely tiring but absurd. Why post a selfie when an AI can generate a prettier one? Why craft a thought when ChatGPT can produce one faster?These are the last days of social media, not because we lack content, but because the attention economy has neared its outer limit — we have exhausted the capacity to care. There is more to watch, read, click and react to than ever before — an endless buffet of stimulation. But novelty has become indistinguishable from noise. Every scroll brings more, and each addition subtracts meaning. We are indeed drowning. In this saturation, even the most outrageous or emotive content struggles to provoke more than a blink.Outrage fatigues. Irony flattens. Virality cannibalizes itself. The feed no longer surprises but sedates, and in that sedation, something quietly breaks, and social media no longer feels like a place to be; it is a surface to skim. No one is forcing anyone to go on TikTok or to consume the clickbait in their feeds. The content served to us by algorithms is, in effect, a warped mirror, reflecting and distorting our worst impulses. For younger users in particular, their scrolling of social media can become compulsive, rewarding their developing brains with unpredictable hits of dopamine that keep them glued to their screens.
          
        Social media platforms have also achieved something more elegant than coercion: They’ve made non-participation a form of self-exile, a luxury available only to those who can afford its costs.



  

    
      “Why post a selfie when an AI can generate a prettier one? Why craft a thought when ChatGPT can produce one faster?”    

    
    
  
Our offline reality is irrevocably shaped by our online world: Consider the worker who deletes or was never on LinkedIn, excluding themselves from professional networks that increasingly exist nowhere else; or the small business owner who abandons Instagram, watching customers drift toward competitors who maintain their social media presence. The teenager who refuses TikTok may find herself unable to parse references, memes and microcultures that soon constitute her peers’ vernacular.These platforms haven’t just captured attention, they’ve enclosed the commons where social, economic and cultural capital are exchanged. But enclosure breeds resistance, and as exhaustion sets in, alternatives begin to emerge.Architectures Of IntentionThe successor to mass social media is, as already noted, emerging not as a single platform, but as a scattering of alleyways, salons, encrypted lounges and federated town squares —  those little gardens.Maybe today’s major social media platforms will find new ways to hold the gaze of the masses, or maybe they will continue to decline in relevance, lingering like derelict shopping centers or a dying online game, haunted by bots and the echo of once‑human chatter. Occasionally we may wander back, out of habit or nostalgia, or to converse once more as a crowd, among the ruins. But as social media collapses on itself, the future points to a quieter, more fractured, more human web, something that no longer promises to be everything, everywhere, for everyone.This is a good thing. Group chats and invite‑only circles are where context and connection survive. These are spaces defined less by scale than by shared understanding, where people no longer perform for an algorithmic audience but speak in the presence of chosen others. Messaging apps like Signal are quietly becoming dominant infrastructures for digital social life, not because they promise discovery, but because they don’t. In these spaces, a message often carries more meaning because it is usually directed, not broadcast.Social media’s current logic is designed to reduce friction, to give users infinite content for instant gratification, or at the very least, the anticipation of such. The antidote to this compulsive, numbing overload will be found in deliberative friction, design patterns that introduce pause and reflection into digital interaction, or platforms and algorithms that create space for intention.This isn’t about making platforms needlessly cumbersome but about distinguishing between helpful constraints and extractive ones. Consider Are.na, a non-profit, ad-free creative platform founded in 2014 for collecting and connecting ideas that feels like the anti-Pinterest: There’s no algorithmic feed or engagement metrics, no trending tab to fall into and no infinite scroll. The pace is glacial by social media standards. Connections between ideas must be made manually, and thus, thoughtfully — there are no algorithmic suggestions or ranked content.To demand intention over passive, mindless screen time, X could require a 90-second delay before posting replies, not to deter participation, but to curb reactive broadcasting and engagement farming. Instagram could show how long you’ve spent scrolling before allowing uploads of posts or stories, and Facebook could display the carbon cost of its data centers, reminding users that digital actions have material consequences, with each refresh. These small added moments of friction and purposeful interruptions — what UX designers currently optimize away — are precisely what we need to break the cycle of passive consumption and restore intention to digital interaction.We can dream of a digital future where belonging is no longer measured by follower counts or engagement rates, but rather by the development of trust and the quality of conversation. We can dream of a digital future in which communities form around shared interests and mutual care rather than algorithmic prediction. Our public squares — the big algorithmic platforms — will never be cordoned off entirely, but they might sit alongside countless semi‑public parlors where people choose their company and set their own rules, spaces that prioritize continuity over reach and coherence over chaos. People will show up not to go viral, but to be seen in context. None of this is about escaping the social internet, but about reclaiming its scale, pace, and purpose.Governance ScaffoldingThe most radical redesign of social media might be the most familiar: What if we treated these platforms as public utilities rather than private casinos?A public-service model wouldn’t require state control; rather, it could be governed through civic charters, much like public broadcasters operate under mandates that balance independence and accountability. This vision stands in stark contrast to the current direction of most major platforms, which are becoming increasingly opaque.



  

    
      “Non-participation [is] a form of self-exile, a luxury available only to those who can afford its costs.”    

    
    
  
In recent years, Reddit and X, among other platforms, have either restricted or removed API access, dismantling open-data pathways. The very infrastructures that shape public discourse are retreating from public access and oversight. Imagine social media platforms with transparent algorithms subject to public audit, user representation on governance boards, revenue models based on public funding or member dues rather than surveillance advertising, mandates to serve democratic discourse rather than maximize engagement, and regular impact assessments that measure not just usage but societal effects.Some initiatives gesture in this direction. Meta’s Oversight Board, for example, frames itself as an independent body for content moderation appeals, though its remit is narrow and its influence ultimately limited by Meta’s discretion. X’s Community Notes, meanwhile, allows user-generated fact-checks but relies on opaque scoring mechanisms and lacks formal accountability. Both are add-ons to existing platform logic rather than systemic redesigns. A true public-service model would bake accountability into the platform’s infrastructure, not just bolt it on after the fact.The European Union has begun exploring this territory through its Digital Markets Act and Digital Services Act, but these laws, enacted in 2022, largely focus on regulating existing platforms rather than imagining new ones. In the United States, efforts are more fragmented. Proposals such as the Platform Accountability and Transparency Act (PATA) and state-level laws in California and New York aim to increase oversight of algorithmic systems, particularly where they impact youth and mental health. Still, most of these measures seek to retrofit accountability onto current platforms. What we need are spaces built from the ground up on different principles, where incentives align with human interest rather than extractive, for-profit ends.This could take multiple forms, like municipal platforms for local civic engagement, professionally focused networks run by trade associations, and educational spaces managed by public library systems. The key is diversity, delivering an ecosystem of civic digital spaces that each serve specific communities with transparent governance.Of course, publicly governed platforms aren’t immune to their own risks. State involvement can bring with it the threat of politicization, censorship or propaganda, and this is why the governance question must be treated as infrastructural, rather than simply institutional. Just as public broadcasters in many democracies operate under charters that insulate them from partisan interference, civic digital spaces would require independent oversight, clear ethical mandates, and democratically accountable governance boards, not centralized state control. The goal is not to build a digital ministry of truth, but to create pluralistic public utilities: platforms built for communities, governed by communities and held to standards of transparency, rights protection and civic purpose.The technical architecture of the next social web is already emerging through federated and distributed protocols like ActivityPub (used by Mastodon and Threads) and Bluesky’s Authenticated Transfer (AT) Protocol, or atproto, (a decentralised framework that allows users to move between platforms while keeping their identity and social graph) as well as various blockchain-based experiments, like Lens and Farcaster.But protocols alone won’t save us. The email protocol is decentralized, yet most email flows through a handful of corporate providers. We need to “rewild the internet,” as Maria Farrell and Robin Berjon mentioned in a Noema essay. We need governance scaffolding, shared institutions that make decentralization viable at scale. Think credit unions for the social web that function as member-owned entities providing the infrastructure that individual users can’t maintain alone. These could offer shared moderation services that smaller instances can subscribe to, universally portable identity systems that let users move between platforms without losing their history, collective bargaining power for algorithm transparency and data rights, user data dividends for all, not just influencers (if platforms profit from our data, we should share in those profits), and algorithm choice interfaces that let users select from different recommender systems. Bluesky’s AT Protocol explicitly allows users to port identity and social graphs, but it’s very early days and cross-protocol and platform portability remains extremely limited, if not effectively non-existent. Bluesky also allows users to choose among multiple content algorithms, an important step toward user control. But these models remain largely tied to individual platforms and developer communities. What’s still missing is a civic architecture that makes algorithmic choice universal, portable, auditable and grounded in public-interest governance rather than market dynamics alone.Imagine being able to toggle between different ranking logics: a chronological feed, where posts appear in real time; a mutuals-first algorithm that privileges content from people who follow you back; a local context filter that surfaces posts from your geographic region or language group; a serendipity engine designed to introduce you to unfamiliar but diverse content; or even a human-curated layer, like playlists or editorials built by trusted institutions or communities. Many of these recommender models do exist, but they are rarely user-selectable, and almost never transparent or accountable. Algorithm choice shouldn’t require a hack or browser extension; it should be built into the architecture as a civic right, not a hidden setting.



  

    
      “What if we treated these platforms as public utilities rather than private casinos?”    

    
    
  
Algorithmic choice can also develop new hierarchies. If feeds can be curated like playlists, the next influencer may not be the one creating content, but editing it. Institutions, celebrities and brands will be best positioned to build and promote their own recommendation systems. For individuals, the incentive to do this curatorial work will likely depend on reputation, relational capital or ideological investment. Unless we design these systems with care, we risk reproducing old dynamics of platform power, just in a new form.Federated platforms like Mastodon and Bluesky face real tensions between autonomy and safety: Without centralized moderation, harmful content can proliferate, while over-reliance on volunteer admins creates sustainability problems at scale. These networks also risk reinforcing ideological silos, as communities block or mute one another, fragmenting the very idea of a shared public square. Decentralization gives users more control, but it also raises difficult questions about governance, cohesion and collective responsibility — questions that any humane digital future will have to answer.But there is a possible future where a user, upon opening an app, is asked how they would like to see the world on a given day. They might choose the serendipity engine for unexpected connections, the focus filter for deep reads or the local lens for community news. This is technically very achievable — the data would be the same; the algorithms would just need to be slightly tweaked — but it would require a design philosophy that treats users as citizens of a shared digital system rather than cattle. While this is possible, it can feel like a pipe dream. To make algorithmic choice more than a thought experiment, we need to change the incentives that govern platform design. Regulation can help, but real change will come when platforms are rewarded for serving the public interest. This could mean tying tax breaks or public procurement eligibility to the implementation of transparent, user-controllable algorithms. It could mean funding research into alternative recommender systems and making those tools open-source and interoperable. Most radically, it could involve certifying platforms based on civic impact, rewarding those that prioritize user autonomy and trust over sheer engagement.Digital Literacy As Public HealthPerhaps most crucially, we need to reframe digital literacy not as an individual responsibility but as a collective capacity. This means moving beyond spot-the-fake-news workshops to more fundamental efforts to understand how algorithms shape perception and how design patterns exploit our cognitive processes. Some education systems are beginning to respond, embedding digital and media literacy across curricula. Researchers and educators argue that this work needs to begin in early childhood and continue through secondary education as a core competency. The goal is to equip students to critically examine the digital environments they inhabit daily, to become active participants in shaping the future of digital culture rather than passive consumers. This includes what some call algorithmic literacy, the ability to understand how recommender systems work, how content is ranked and surfaced, and how personal data is used to shape what you see — and what you don’t.Teaching this at scale would mean treating digital literacy as public infrastructure, not just a skill set for individuals, but a form of shared civic defense. This would involve long-term investments in teacher training, curriculum design and support for public institutions, such as libraries and schools, to serve as digital literacy hubs. When we build collective capacity, we begin to lay the foundations for a digital culture grounded in understanding, context and care.We also need behavioral safeguards like default privacy settings that protect rather than expose, mandatory cooling-off periods for viral content (deliberately slowing the spread of posts that suddenly attract high engagement), algorithmic impact assessments before major platform changes and public dashboards that show platform manipulation, that is, coordinated or deceptive behaviors that distort how content is amplified or suppressed, in real-time. If platforms are forced to disclose their engagement tactics, these tactics lose power. The ambition is to make visible hugely influential systems that currently operate in obscurity.We need to build new digital spaces grounded in different principles, but this isn’t an either-or proposition. We also must reckon with the scale and entrenchment of existing platforms that still structure much of public life. Reforming them matters too. Systemic safeguards may not address the core incentives that inform platform design, but they can mitigate harm in the short term. The work, then, is to constrain the damage of the current system while constructing better ones in parallel, to contain what we have, even as we create what we need. The choice isn’t between technological determinism and Luddite retreat; it’s about constructing alternatives that learn from what made major platforms usable and compelling while rejecting the extractive mechanics that turned those features into tools for exploitation. This won’t happen through individual choice, though choice helps; it also won’t happen through regulation, though regulation can really help. It will require our collective imagination to envision and build systems focused on serving human flourishing rather than harvesting human attention.Social media as we know it is dying, but we’re not condemned to its ruins. We are capable of building better — smaller, slower, more intentional, more accountable — spaces for digital interaction, spaces where the metrics that matter aren’t engagement and growth but understanding and connection, where algorithms serve the community rather than strip-mining it.The last days of social media might be the first days of something more human: a web that remembers why we came online in the first place — not to be harvested but to be heard, not to go viral but to find our people, not to scroll but to connect. We built these systems, and we can certainly build better ones. The question is whether we will do this or whether we will continue to drown.
          
        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SkiftOS: A hobby OS built from scratch using C/C++ for ARM, x86, and RISC-V]]></title>
            <link>https://skiftos.org</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45229414</guid>
        </item>
        <item>
            <title><![CDATA[Raspberry Pi Synthesizers – How the Pi is transforming synths]]></title>
            <link>https://www.gearnews.com/raspberry-pi-synthesizers-how-the-pi-is-transforming-synths/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45229227</guid>
            <description><![CDATA[The Raspberry Pi is finding its way into more and more synthesizers. Do your synths have a slice of Pi inside? Read on to find out.]]></description>
            <content:encoded><![CDATA[

                
    

        
        
                                                                        
                        

                        
                        

                                                    
                                                
                                                    
                                                    
                                                
                                                
                                                    Raspberry Pi Synthesizers                                                                                                             · 
                                                        
                                                        Source:
                                                        Korg, Raspberry Pi                                                    
                                                                                                    
                                            
                                                



The Raspberry Pi microcomputer is finding its way into more and more synthesizers. Do your synths have a slice of it inside? Read on to find out.







Digital synthesizers are essentially computers in specialized housings. Rather than a keyboard with letters and numbers, their keyboards trigger notes. Custom-designed DSP (digital signal processing) systems can be expensive so some manufacturers are turning to ready-made computing systems to run their synths. One that’s been gaining in popularity in recent years is Raspberry Pi. The low-cost mini computer is now in instruments by Korg, Erica Synths and many more.



Is this cheating? Do any of your synths have Pi inside? Let’s find out.



DSP In Synthesizers



Digital synthesizers have existed in some form since the 1970s, with the New England Digital Synclavier being the first commercial release in 1977. As synthesizers became more powerful, adding sampling and physical modelling to the already existing FM synthesis, the DSP required to run them became more complex. Additions like sequencers and effects only compounded the expense.



New England Digital Synclavier II · Source: Synclavier



To run their DSP, manufacturers created custom DSP systems running on off-the-shelf chips from companies like Motorola and Texas Instruments. One example was Korg’s Pentium-based OASYS workstation from 2005. While incredibly powerful, it was also incredibly expensive. 



How to keep the power while also lowering the cost? 



Raspberry Pi: What Is It?



The solution for Korg – as well as other manufacturers, as we’ll see – was the Raspberry Pi. Essentially a complete computer processor in a small and – critically – inexpensive package, this programmable hardware get used for all sorts of applications. From robotics to home computing to (you guessed it) digital synthesizers, ready-made Raspberry Pis offer an elegant and affordable solution for custom computing systems.



Raspberry Pi · Source: Raspberry Pi



Korg Serves Up Some Pi



The biggest synthesizer manufacturer to make use of the Raspberry Pi is Korg. The Japanese synth company’s Wavestate, Modwave and Opsix digital synths all make use of the Raspberry Pi Compute Module. (They’re in the module versions too.)



Korg Modules · Source: Korg



In an article on the Raspberry Pi home page, Korg’s Andy Leary sites price and manufacturing scale as the main reason Korg decided on these components. He also liked that it was ready to go as is, providing CPU, RAM and storage in a single package. “That part of the work is already done,” he said in the article. “It’s like any other component; we don’t have to lay out the board, build it and test it.”



The software for each instrument is, of course, custom. The Raspberry Pi, however, generates the sound. “Not everyone understands that Raspberry Pi is actually making the sound,” said Korg’s Dan Philips in the same piece. “We use the CM3 because it’s very powerful, which makes it possible to create deep, compelling instruments.”


Affiliate Links
                    
                        
                    
                    
                            Korg Wavestate Module  Customer rating: (4)
                        
                    
                
                    
                        
                    
                    
                            Korg Modwave Module  Customer rating: (2)
                        
                    
                
                    
                        
                    
                    
                            Korg opsix Module  Customer rating: (2)
                        
                    
                


 You are currently viewing a placeholder content from YouTube. To access the actual content, click the button below. Please note that doing so will share data with third-party providers. More Information 



 You are currently viewing a placeholder content from YouTube. To access the actual content, click the button below. Please note that doing so will share data with third-party providers. More Information 



 You are currently viewing a placeholder content from YouTube. To access the actual content, click the button below. Please note that doing so will share data with third-party providers. More Information 



Erica Synths Also Likes Pi



You might not expect to find a Raspberry Pi inside an analogue synthesizer but if that synth happens to have digital functionality… Take the Bullfrog, for example. Erica Synths and Richie Hawtin’s educational desktop analogue has a RP2040 to handle MIDI implementation as well as functionality for the Sampler/Looper voice card. This adds additional functionality to the largely analogue synthesizer.



Bullfrog Synthesizer · Source: Erica Synths


Affiliate Links
                    
                        
                    
                    
                        Erica Synths Bullfrog
                            
                            No customer rating available yet
                        
                        
                        
                    
                


 You are currently viewing a placeholder content from YouTube. To access the actual content, click the button below. Please note that doing so will share data with third-party providers. More Information 



Zynthian: Pi For Everyone



One of the benefits of using Raspberry Pi is the ability to make it open source. The DIY kit Zynthian is an open synth platform with a Raspberry Pi 4 at its centre. The desktop box can function as a keyboard expander, effects unit, MIDI processor, groovebox or even micro-DAW. “Zynthian is a community-driven project and it’s 100% open source,” the company says on its site. “Free software on Open hardware. Completely configurable and fully hackable!”



Zynthian · Source: Zynthian



Damn Fine Pi



There are plenty more synths making use of the Raspberry Pi. One that you may not realize is Organelle M by Critter and Guitari. By putting a Pi inside, they’re able to run Pure Data, meaning that you can program your own synths to use inside too.



Critter & Guitari Organelle S · Source: Critter & Guitari



Another fun instrument with a Raspberry Pi 3 for a soul is Tasty Chips’ GR-1 Granular synthesizer.



For something a little more esoteric, try the Yoshimi Pi. “Yoshimi Pi is the hardware incarnation of the software synth Yoshimi, running on a Raspberry Pi 4 in a rugged metal case with a built-in PSU and line level audio output,” according to the product page.



Of course, you don’t have to buy a commercial Raspberry Pi-based synthesizer. There are plenty of DIY options to run them “bare metal,” that is, without a separate operating system. Just hook up a MIDI controller to the board and you’re off and running. Try MiniSynth Pi or code your own!


Affiliate Links
                    
                        
                    
                    
                        Critter & Guitari 5 Moons
                            
                            No customer rating available yet
                        
                        
                        
                    
                
                    
                        
                    
                    
                            Decksaver Tasty Chips GR-1  Customer rating: (6)
                        
                    
                


Raspberry Pi: Is It Cheating?



In the same way that some claim that virtual analogue is just a “VST in a box,” others complain that synths with Raspberry Pi at the core are somehow cheating. You may as well just make your own, right?



Raspberry Pi · Source: Raspberry Pi



“Just because something is based on a Raspberry Pi it doesn’t mean it’s trivial to make one,” said chalk_walk in a Reddit thread on the Organelle. “If they provide the software then you may be able to put together something equivalent, but if not: you are out of luck if you want an Organelle. Similarly, part of the complexity is in making an enclosure with appropriate controls and displays.”



As we’ve seen, all digital synthesizers have some kind of computer inside. Whether that’s a custom DSP with off-the-shelf chips or a Raspberry Pi, you still have to code the software, design the enclosure and PCBs and everything else that goes along with it. By going with a little computer like this, you can shave some money off the asking price and save on development time too.



More Information




Raspberry Pi’s home page 



Korg’s home page 



Erica Synths’ home page 



Zynthian’s home page 



All about synthesizers 


                                                    
                                        How do you like this post?
                                        
                                        Rating:  Yours:  | ø: 
                                    
                        
                        
                                                                        
                            
                        
                        

                        

                        

                    
                            
                    
            




]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OCI Registry Explorer]]></title>
            <link>https://oci.dag.dev/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45228891</guid>
            <description><![CDATA[This beautiful tool allows you to explore the contents of a registry interactively.]]></description>
            <content:encoded><![CDATA[

Registry Explorer





This beautiful tool allows you to explore the contents of a registry interactively.


You can even drill down into layers to explore an image's filesystem.


Enter a public image, e.g. "ubuntu:latest":



Enter a public repository, e.g. "ubuntu":


Interesting examples

  cgr.dev/chainguard/static:latest-glibc
  gcr.io/distroless/static:latest
  ghcr.io/homebrew/core/crane
  registry.k8s.io
  registry.k8s.io/bom/bom:sha256-499bdf4cc0498bbfb2395f8bbaf3b7e9e407cca605aecc46b2ef1b390a0bc4c4.sig
  docker/dockerfile:1.5.1
  pengfeizhou/test-oci:sha256-04eaff953b0066d7e4ea2e822eb5c31be0742fca494561336f0912fabc246760
  tianon/true:oci
  ghcr.io/stargz-containers/node:13.13.0-esgz



FAQ
How does this work?

This service lives on Cloud Run and uses google/go-containerregistry for registry interactions.

Isn't this expensive to run?
Not really! Ingress is cheap, Cloud Run is cheap, and GCS is cheap.
To avoid paying for egress, I limit the amount of data that I'll serve directly and instead give you a command you can run on your own machine.
The most expensive part of this is actually the domain name.
Isn't this expensive for the registry?
Not really! The first time a layer is accessed, I download and index it. Browsing the filesystem just uses that index, and opening a file uses Range requests to load small chunks of the layer as needed.
Since I only have to download the whole layer once, this actually reduces traffic to the registry in a lot of cases, e.g. if you share a link with someone rather than having them pull the whole image on their machine.
In fact, Docker has graciously sponsored this service by providing me an account with unlimited public Docker Hub access. Thanks, Docker!
That can't be true, gzip doesn't support random access!

That's not a question.

Okay then, how does random access work if the layers are gzipped tarballs?
Great question! See here.
Tl;dr, you can seek to an arbitrary position in a gzip stream if you know the 32KiB of uncompressed data that comes just before it, so by storing ~1% of the uncompressed layer size, I can jump ahead to predetermined locations and start reading from there rather than reading the entire layer.
Thanks @aidansteele!
Is this open source?
Yes! See here.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Legal win]]></title>
            <link>https://ma.tt/2025/09/legal-win/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45228692</guid>
            <description><![CDATA[Just got word that the court dismissed several of WP Engine and Silver Lake’s most serious claims — antitrust, monopolization, and extortion have been knocked out! These were by far the most signif…]]></description>
            <content:encoded><![CDATA[

			
				

	
		
Just got word that the court dismissed several of WP Engine and Silver Lake’s most serious claims — antitrust, monopolization, and extortion have been knocked out! These were by far the most significant and far-reaching allegations in the case and with today’s decision the case is narrowed significantly. This is a win not just for us but for all open source maintainers and contributors. Huge thanks to the folks at Gibson and Automattic who have been working on this.



With respect to any remaining claims, we’re confident the facts will demonstrate that our actions were lawful and in the best interests of the WordPress community.



This ruling is a significant milestone, but our focus remains the same: building a free, open, and thriving WordPress ecosystem and supporting the millions of people who rely on it every day. 

			

	

						
		
			Post navigation		
		
	
						

			
		]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Life, work, death and the peasant: Rent and extraction]]></title>
            <link>https://acoup.blog/2025/09/12/collections-life-work-death-and-the-peasant-part-ivc-rent-and-extraction/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45228472</guid>
            <description><![CDATA[This is the third piece of the fourth part of our series (I, II, IIIa, IIIb, IVa, IVb) looking at the lives of pre-modern peasant farmers – a majority of all of the humans who have ever lived. Last…]]></description>
            <content:encoded><![CDATA[
		
This is the third piece of the fourth part of our series (I, II, IIIa, IIIb, IVa, IVb) looking at the lives of pre-modern peasant farmers – a majority of all of the humans who have ever lived.  Last time, we started looking at the subsistence of peasant agriculture by considering the productivity of our model farming families under basically ideal conditions: relatively good yields and effectively infinite land.



This week we’re going to start peeling back those assumptions in light of the very small farm-sizes and capital availability our pre-modern peasants had.  Last week we found that, assuming effectively infinite land and reasonably high yields, our farmers produced enough to maintain their households fairly securely in relative comfort, with enough surplus over even their respectability needs to potentially support a small population of non-farmers.  But of course land isn’t infinite and also isn’t free and on top of that, the societies in which our peasant farmers live are often built to extract as much surplus from the peasantry as possible.



But first, if you like what you are reading, please share it and if you really like it, you can support this project on Patreon! While I do teach as the academic equivalent of a tenant farmer, tilling the Big Man’s classes, this project is my little plot of freeheld land which enables me to keep working as a writers and scholar. And if you want updates whenever a new post appears, you can click below for email updates or follow me on Twitter and Bluesky and (less frequently) Mastodon (@bretdevereaux@historians.social) for updates when posts go live and my general musings; I have largely shifted over to Bluesky (I maintain some de minimis presence on Twitter), given that it has become a much better place for historical discussion than Twitter.






From the British Museum (2010,7081.4256), “The Rapacious Steward or Unfortunate Tenant,” a print by Haveill Gillbank (1803), showing a tenant farmer, with his family, being taken away by the estate’s steward (on horseback). A little late for our chronology, but so on point for today’s topic it was hard to let it pass.It is also a useful reminder that tenancy wasn’t just an economic system, but a social one: it gave the Big Man and his agents tremendous power over the lives and livelihoods of the people who lives near the Big Man’s estates. For very Big Men, they might have several such estates and so be absentee landlords, in which case not only the Big Man, but his steward, might be figures of substantial power locally.



Land Holdings



Returning to where we left off last week, we found that our model families could comfortably exceed their subsistence and ‘respectability’ needs with the labor they had assuming they had enough land (and other capital) to employ all of their available farming labor.  However, attentive readers will have noticed that the labor of these families could work a lot of land: 30.5 acres for The Smalls, 33.6 acres for The Middles and 56 acres for The Biggs.  That may not seem large by the standards of modern commercial farms, but few peasants had anything like such large landholdings; even rich peasants rarely owned so much.



We might compare, for instance, the land allotments of Macedonian and Greek military settlers in the Hellenistic kingdoms (particularly Egypt, where our evidence is good). These settlers were remarkably well compensated, because part of what the Hellenistic kings are trying to do is create a new class of Greco-Macedonian rentier-elites1 as a new ethnically defined military ruling-class which would support their new monarchies. In Egypt, where we can see most clearly, infantrymen generally received 25 or 30 arourai (17 or 20.4 acres), while cavalrymen, socially higher up still, generally received 100 arourai (68 acres).2 That infantry allotment is still anywhere from two thirds to less than half of what our model families can farm and yet was still large enough, as far as we can tell, to enable Ptolemaic Greco-Macedonian soldiers to live as rentier-elites, subsisting primarily if not entirely off of rents and the labor of others.3



Alternately, considering late medieval Europe through the study of Saint-Thibery,4 out of 189 households in 1460 in the village just fifteen households are in the same neighborhood of landholdings as the Smalls’ 33.6 acres above (so roughly 55 setérée and up)5 only six as much as The Biggs (about 90 setérée and up). In short our assessment so far has assumed our families are extremely rich peasants. But of course they almost certainly are not!



Instead, as we noted in our first part, the average size of peasant landholdings was extremely small. Typical Roman landholdings were around 5-10 iugera (3.12-6.23 acres), in wheat-farming pre-Han northern China roughly 100 mu (4.764 acres), in Ptolemaic Egypt (for the indigenous, non-elite population) probably 5-10 aroura (3.4-6.8 acres) and so on.6 In Saint-Thibery in Languedoc, the average (mean) farm size was about 24 setérée (~14.5 acres) but the more useful median farm size was just five setérée (~3 acres); the average is obviously quite distorted by the handful of households with hundreds of setérée of land.



So we might test three different farm sizes; once again, I am going to use Roman units because that’s how I am doing my background math.  We might posit a relatively a poor household farm of roughly three iugera (1.85 acres).  In Saint-Thibery, 68 of the 189 households (36%) had land holdings this small or smaller, so this is not an unreasonable ‘poor household’ – indeed, we could posit much poorer, but then we’re really just talking about tenant farmers, rather than freeholding peasants.  Next, we can posit a moderate household farm of roughly six iugera (3.8 acres); reasonably close to the median holding in Saint-Thibery and roughly what we think of as the lower-bound for ancient citizen-soldier-peasants.  Finally, we can posit a large household farm of nine iugera (5.6 acres), reflective of what seems to be the upper-end of typical for those same citizen-soldier-peasants; at Saint-Thibery in 1460 there were a couple dozen families seemingly in this range.7



For the sake of a relatively easier calculation, we can assume the same balance of wheat, barley and beans as last time, which lets us just specify an average yield after seed per iugerum of 81.2-189.5 kg of wheat equivalent (achieved by averaging the per-acre wheat equivalent production across all three crops, with seed removed),8 with each iugerum demanding between 11 and 15 working days (averaging the labor requirements across all three crops). Finally, we need to remember the fallow: in this case we’re assuming about a third of each farm is not in production in any given year, meaning it is both not consuming any labor nor producing any crops. That lets us then quickly chart out our peasant families based on the land they might actually have (keeping in mind the household size and household land holdings aren’t going to match; the larger household in people won’t always be the one with more land). First, a reminder of the basic labor availability and grain requirements of our households.



The SmallsThe MiddlesThe BiggsLabor Available435 work-days507.5 work-days797.5 work-daysBare Subsistence Requirement~1,189.5kg wheat-equivalent~1,569kg wheat-equivalent~2,686kg wheat-equivalentRespectability Requirement~2,379kg wheat-equivalent~3,138kg wheat-equivalent~5,376kg wheat-equivalent



Then for the smallest, 3 iugera farm, the numbers work like this:



Small Farm (3 iugera)2 iugera cropped1 fallowThe SmallsThe MiddlesThe BiggsLabor requirement22-30 work days22-30 work days22-30 work daysLabor surplus405-413 work days477.5-485.5 work days767.5-775.5 work daysProduction after Seed162.4-378.8kg wheat equivalent162.4-378.8kg wheat equivalent162.4-378.8kg wheat equivalentPercentage of Subsistence:14-32%10-24%6-14%



And then for the medium-sized farm:



Medium Farm (6 iugera)4 iugera cropped2 fallowThe SmallsThe MiddlesThe BiggsLabor requirement44-60 work days44-60 work days44-60 work daysLabor surplus375-391 work days447.5-463.5 work days737.5-753.5 work daysProduction after Seed324.8-757.6kg wheat equivalent324.8-757.6kg wheat equivalent324.8-757.6kg wheat equivalentPercentage of Subsistence:27-64%21-48%12-28%



And the larger (but not rich peasant) farm:



Large Farm (9 iugera)6 iugera cropped3 fallowThe SmallsThe MiddlesThe BiggsLabor requirement66-90 work days66-90 work days66-90 work daysLabor surplus345-369 work days417.5-441.5 work days707.5-731.5 work daysProduction after Seed487.6-1,136.5kg wheat equivalent487.6-1,136.5k wheat equivalent487.6-1,136.5k wheat equivalentPercentage of Subsistence:41-96%31-72%18-42%



And we immediately see the problem: only the Smalls manage to get close to subsistence on very favorable (8:1) fertility assumptions on the small farm they own. Now it is possible for the peasants to push a little bit on these numbers. The most obvious way would be focusing as much as possible on wheat cultivation, which has higher labor demands but also the highest yield-per-acre (or iugerum), producing around 50% more calories than beans and 35% more calories than barley per-acre (see last week’s post for specifics). But there’s a limit to going ‘all in’ on wheat to meet food shortfalls: the land might not be suitable for it and wheat exhausts the soil, so our farmers would need some sort of rotation.  That said, peasant diets were overwhelmingly grains (wheat and barley) for this reason: they provide the most calories for a favorable balance of land and labor. Our farmers might also try to supplement production with high-labor, high-density horticulture; a kitchen garden can take a lot of work but produce a lot of nutrition in a small space. But hitting household nutrition demands entirely with a kitchen garden isn’t going to work both because of the labor demands but also because the products of a kitchen garden tend not to keep well.



Instead the core problem is that our peasant households are much too large as units of labor for the farmland they own.  When we say that, what we mean is that given these households are both units of consumption (they have to provide for their members) and units of production (they are essentially agricultural small businesses), an efficient allocation of them would basically have each household on something like 30 acres of farmland, farming all of it (and thus using most of their labor) and selling the excess.  But the lack of economically sustainable social niches – that is, jobs that provide a reliable steady income to enable someone to obtain subsistence – means that these families are very reluctant to leave members without any land at all, so the holdings ‘fractionalize’ down to these tiny units, essentially the smallest units that could conceivably support one family (and sometimes not even that).



I’ve already seen folks in the comments realizing almost immediately why these conditions might make conquest or resettlement into areas of land easily brought under cultivation so attraction: if you could give each household 30-40 acres instead of 3-6, you could realize substantial improvements in quality of life (and the social standing of the farmers in question). And of course that kind of ‘land scarcity’ problem seems to have motivated both ancient and early modern settler-colonialism: if you put farmers next to flat, open ground owned by another community, it won’t be too long before they try to make it farmland (violently expelling the previous owners in the process). This is also, I might add, part of the continual friction in areas where nomads and farmers meet: to a farmer, those grazing fields look like more land and more land is really valuable (though the response to getting new land is often not to create a bunch of freeholding large-farm homesteaders, but rather to replicate the patterns of tenancy and non-free agricultural labor these societies already have to the point of – as in the Americas – forcibly trafficking enormous numbers of enslaved laborers at great cost, suffering and horror, to create a non-free dependent class whose exploitation can enable those patterns.  Most conquering armies dream of becoming landlords, not peasants).9



Alternately as farms these holdings could be a lot more efficient if they had fewer people on them and indeed when we read, for instance, ancient agricultural writers, they recommend estates with significantly fewer laborers per-unit-land-area than what we’d see in the peasant countryside. But that’s because the Big Man is farming for profit with a large estate that lets him tailor his labor force fairly precisely to his labor needs; the peasants are farming to survive and few people are going to let their brother, mother, or children starve and die in a ditch because it makes their farm modestly more productive per unit labor. Instead, they’re going try to do anything in their power to get enough income to have enough food for their entire family to survive.



There is no real way around it: our peasants need access to more land.  And that land is going to come with conditions.



From the British Museum (1850,0713.91), “La Conversation,” an etching by David Teniers and Andrew Lawrence (1742) showing three peasants having a conversation outside of a farmhouse, with a peasant woman in the doorway.



The Big Man’s Land



Now before we march into talking about farming someone else’s land, it is worth exploring why our farmers don’t get more land by just bringing more land under cultivation. And the answer here is pretty simple: in most of the world, preparing truly ‘wild’ land for cultivation takes a lot of labor. In dry areas, that labor often comes in the form of irrigation demands: canals have to be dug out from water sources (mainly rivers) to provide enough moisture for the fields as the most productive crops (like wheat) demand a lot of moisture to grow well. In climates suitable for rainfall agriculture, the problem is instead generally forests: if there’s enough rain to grow grain, there’s enough rain to grow trees and those trees have had quite the head start on you. Clearing large sections of forest by hand is a slow, labor-intensive thing and remember, you don’t just need the trees cut down, you need the stumps pulled or burned. Fields also need to be relatively flat – which might demand terracing on hilly terrain – and for the sake of the plow they need to be free of large stones to the depth of the plow (at least a foot or so).



In short, clearing farmland was both slow and expensive and all of this assumes the land can be made suitable and that no one has title to it.  Of course if the forest is the hunting preserve of the local elite, they’re going to object quite loudly to your efforts to cut it down.  And a lot of land is simply going to be too dry or too hilly or too marshy to be made usable for farming ona practical time-scale for our peasants.  Such land simply cannot be brought usefully into cultivation; you can’t farm wheat in a swamp.10  So it is quite hard and often impractical to bring new land into cultivation.



That doesn’t mean new land wasn’t brought into cultivation, it absolutely was.  We can sometimes track population pressures archaeologically by watching this process: forests retreat, new villages pop up, swamps are drained and so on as previously marginal or unfarmable land is brought into cultivation.  Note, of course, if you bring a bunch of marginal fields into cultivation – say, a drier hillside not worth farming before – your average yield is going to go down because that land simply isn’t as productive (but demands the same amount of labor!).  But that process is generally slow, taking place over generations in response to population pressures.  It isn’t a solution available on the time-scale that most of our households are operating.  In the moment, the supply of land is mostly fixed for our peasants.



Which means our peasants need access to more land (or another way of generating income).  There are a range of places that land could come from:




Peasant Households without enough labor to farm their own land.  In order to make our households relevant at every part of the process, I haven’t modeled the substantial number of very small households we talked about in the first section, households with just 1 or 2 members.  If none of those householders were working-age males (e.g. a household with an elderly widow, or a young widow and minor children, etc.) they might seek to have other villagers help farm their land and split the production.  For very small households, that might be enough to provide them subsistence (or at least help).  Consequently those small, often ‘dying’ households provide a (fairly small) source of land for other households.



Rich peasants likewise might have more land than their household could farm or cared to farm.  Consider the position The Smalls would be if they were a rich peasant household with, say, 25 acres of land (in Saint-Thibery, 26 households (of 189) had this much or more land).  That’s enough land that, under good harvest conditions it would be easy enough to shoot past the household’s respectability requirements.  At which point why work so hard?  Why not sharecrop out a large chunk of your land to small farmers and split the production, so you still make your respectability basket in decent years, but don’t have to work so darn hard?



The Big Man.  Another part of this ecosystem is invariably large landowners, who might have estates of hundreds of acres.  Columella , for instance, thinks of farm planning (he is thinking about large estates) in units of 100 iugera (62.3 acres) and 200 iugera (124.6 acres; Col. Rust. 12.7-9).  An estate of several hundred acres would hardly be unusual.  Likewise in the Middle Ages, the Big Man might be a local noble whose manor estate might likewise control a lot of land.  The Big Man might also be a religious establishment: temples (in antiquity) and monasteries and churches (in the Middle Ages) often controlled large amounts of productive farmland worked by serfs or tenants to provide their income.  Naturally, the Big Man isn’t doing his own farming; he may have some ‘built in’ labor force (workers in his household, enslaved workers, permanent wage laborers, etc.) but often the Big Man is going to rely substantially on the local peasantry for tenant labor.




In practice, the Big Man is likely to represent the bulk of opportunities here, but by no means all of them.  As I noted before, while local conditions vary a lot, you won’t be too far wrong in thinking about landholdings as a basic ‘rule of thirds’ with one third of the land controlled by small peasants, one third by rich peasants and one third by the Big Man (who, again, might be a lord or a big landowner or a church, monastery or temple (in the latter case, the land is owned by the god in most polytheistic faiths) or even the king).  But of course only a little bit of the small peasant land is going to be in search of workers, since most peasant households have too many hands for too little land; some of the rich peasant land will be looking for workers (either tenants or hired hands), but rich peasants are still peasants – they do some of their farming on their own.  By contrast, the Big Man is marked out by the fact that he doesn’t do his own farming: he needs some kind of labor supply – wage laborers, enslaved/non-free laborers or tenants – for all of it.



But that also means that something like half (or more!) of the land around our peasant village might be owned by a household that needs outside labor to farm it.  So we have peasant households with surplus labor that need more land to farm and richer households with surplus land that needs labor.  The solution here generally was some form of tenancy which in the pre-modern world generally came in the form, effectively of sharecropping: the landowner agreed to let the poorer household farm some of his land in exchange for a percentage of the crop that resulted.  That ‘rent-in-kind’ structure is useful for the peasants who after all are not generally keeping money with which to pay rent.  At the same time, it limits their liability: if the harvest on tenant land fails, they may suffer a shortfall, but they aren’t in debt some monetary quantity of rent (though they may end up in debt in some other way).



Now the question is: on what terms?



Tenancy



And the answer here won’t surprise: bad terms.  The terms are bad.



There’s a useful discussion of this in L. Foxhall, “The Dependent Tenant” JRS 980 (1990), which in turn leans on K. Finkler, “Agrarian Reform and Economic Development” in Agricultural Decision Making, ed. P.F. Barlett (1980) to get a sense of what the terms for tenant farmers might normally look like.  Foxhall notes in this and a few other studies of modern but largely non-industrial farming arrangements that almost no households in these studies were entirely uninvolved in sharecropping or tenancy arrangements, but that the terms of tenancy arrangements varied a lot based on the inputs supplied.



The key inputs were labor, traction (for our pre-industrial peasants, this is “who supplies the plow-team animals”), water and seed.  The most common arrangement, representing almost a third of all arrangements, was where the tenant supplied labor only, while traction, water and seed were supplied by the landlord; the tenants share in these arrangements was a measly 18.75%.  A number of arrangements had the tenant supplying not only labor but also some mix of traction, water or seed (but not all) and often the tenant’s share of the production hovered between 40 and 60%, with exact 50/50 splits occurring in about a quarter of the sample.  In just one case did the tenant supply everything but the land itself; in that case the tenant’s share was 81.25%.



One thing that is obvious from just this example is that arrangements varied a lot and are going to depend on need and bargaining power. A ‘landlord’ who has land they want under cultivation but can supply basically nothing else may be relatively easy to negotiate into a fairly generous deal; a peasant who is absolutely destitute save for the labor of their hands is easy to exploit. An even 50/50 landholder, tenant split seems to have been the norm in much of Europe though, reflected in terms for sharecropper (métayer in French, mezzadro in Italian, mitateri in Sicilian, mediero in Spanish) which all mean ‘halver,’ though again the terms (and the share split) varied, typically based on demand but also on what exactly the landlord was providing (seed, plow teams, tools, physical infrastructure (like a farmhouse), etc).



For the sake of simplicity in our model, we can assume something like a 50/50 split, with our tenants supplying half of the seed, so that our net yield is exactly half of what it would have been.  We can then take those assumptions back to our model.  To establish a baseline, let’s run the numbers assuming first a ‘medium’ sized (6 iugera, 3.8 acres, with 4 iugera cropped and 2 fallowed) farm, with our fertility estimate set modestly to 6:1, a ‘good but not great’ yield.  We’re going to ’round up’ to the nearest even iugerum and assume an average of 13 days per iugerum of labor, just to make our calculations a bit simpler.  How hard is it for our peasants to meet their needs if they have to sharecrop the added land they need?



Tenancywith a medium farmThe SmallsThe MiddlesThe BiggsTotal Labor435 work-days507.5 work-days797.5 work-daysFreehold Labor Demand52 work-days52 work-days52 work-daysFreehold Production541kg wheat equivalent541kg wheat equivalent541kg wheat equivalentShortfall to Subsistence648.5kg wheat equivalent1,028kg wheat equivalent2,145kg wheat equivalentNet Production Per iugera farmed as tenant67.65kg wheat equivalent67.65kg wheat equivalent67.65kg wheat equivalentTenant Land Required for Subsistence10 iugera (6.23 acres)(plus another ~5 iugera fallowed)16 iugera (9.97 acres)(plus another ~8 iugera fallowed)32 iugera (19.94 acres)(plus another ~16 iugera fallowed)Labor Demand for Subsistence130(+52) work daysTotal: 182208(+52) work daysTotal: 260416(+52) work daysTotal: 468Subsequent Shortfall to Respectability (over subsistence)1,189.5kg wheat equivalent1,569kg wheat equivalent2,686kg wheat equivalentTenant Land Required for Respectability18 iugera (11.2 acres)(plus another ~9 iugera fallowed)24 iugera (14.95 acres)(plus another ~12 iugera fallowed)40 iugera (24.9 acres)(plus another ~20 iugera fallowed)Labor Demand for Respectability234(+130+52) work-days (Total: 416)312(+208+52) work-days (Total: 572)Shortage: 64.5520(+416+52) work-days (Total: 988)Shortage: 190.5



As we can see, tenancy dramatically changes the picture for our peasants. Under these relatively typical assumptions, of our three families all can make subsistence in a normal year but only the Smalls have the right combination of a lot of labor and a relatively small family to have a shot at getting all of their respectability needs (in practice, they’d probably fall short once you consider necessary farm labor not in the fields – fence repair, tool maintenance, home repair and the like). It also isn’t hard to see how we might alter this picture to change our assumptions. Changing the size of the owned farmland has a significant impact (even though it is already so small) because our peasants realize twice the production per unit-land-area for land they own over land they rent (again, terms might vary). Put another way, under these assumptions, somewhat marginal owned farmland that gives an OK-but-not-great yield of 4:1 is of the same use to our peasants as really good tenant-farmed farmland giving a 7:1 yield (both offer 81.2kg of wheat equivalent per iugerum after rent is paid).



That said, the fact that our peasants end up with enough labor to comfortable exceed their subsistence requirements, but not their comfort requirements is favorable for extraction, which we’ll discuss below.  These are households with spare labor who can’t fulfill all of their wants entirely on their own, giving the state or local Big Men both a lot of levers to squeeze more labor out of them and also giving the households the available above-subsistence labor to squeeze.  By contrast if these peasants had enough land to meet all of their needs themselves, there would be fewer opportunities to compel them to do additional labor beyond that.



But even before we get to extraction, tenancy is also changing our peasants’ incentives. Economics has the concept of diminishing marginal returns, the frequent phenomenon where adding one more unit of a given input produces less and less output per additional input-unit. You will find more errors in the first hour of proofreading than the fiftieth hour, for instance. There’s also the concept of diminishing marginal utility: beyond a certain point, getting more of something is less valuable per unit added. Getting one bar of chocolate when you have none? Fantastic. Getting one bar of chocolate when you have ten thousand? Solidly meh.



Both are working on our farmers to press their natural production inclination not to maximum labor or even hitting that respectability basket but just subsistence and a little bit more.  On the diminishing marginal returns front, naturally when it comes to both owned land and rented land, our peasants are going to farm the most productive land first.  This is why when we talk about expanding population and expanding agriculture, we often talk about marginal land (less productive land) coming under cultivation; because all of the really great land was already being farmed.  But poor farmland doesn’t demand less labor time (indeed, it may demand more), it just produces less.  So while we’ve been working here with averages, you should imagine that the first few acres of farmland will be more productive and the latter few less productive.



Tenancy puts this into even more sharp contrast because it creates a really significant discontinuity in the value of farming additional land: the rents are so high that sharecropped or tenant land is much less useful (per unit labor) to the peasant than their own land.  So you have a slow downward slope of ‘land quality’ and somewhere in that slope there is the point at which the peasants have farmed all of their own land and so suddenly the effective yield-per-labor-after-rent drops by half (or more!).  So the first few hundred kilograms of wheat equivalent are probably fairly easy to get: you have a few good fields you own and your net out of them might be 130-190kg of wheat equivalent per iugerum.  Put in a couple dozen days on those two good iugera and The Smalls have just over a quarter of their subsistence needs.  But then they have their more marginal fields, which might only yield 80-100kg.  Still not terrible but the next couple of dozen days of labor don’t get them as far: not to half but just 44% or so.  But now you are out of your own land, so you go to your rich neighbor or the Big Man to get access to some more and suddenly even on their best fields your yield-per-iugerum is 80-95kg so another couple of dozen working days gets you just from 44% to just 57% of what you need.  So you need to line up a lot more land, but now you might be starting to look at the worse fields the Big Man has.  He still wants them farmed, after all, his choice is between doing nothing and earning money or doing nothing and not earning money; he’d rather earn money.  But suddenly you’re looking at maybe as little as 50-60kg of wheat equivalent per iugerum and the labor demands have not gone down.



Meanwhile, the comfort you get from each kilogram of wheat equivalent is also going down.  The first 80% or so of your subsistence needs is necessary simply to not starve to death; a bit more makes the household sustainable in the long term.  But then – and remember, these choices are coming as you are facing diminishing marginal returns on each day of labor you put in – is it really worth your time to cultivate a couple more fields in order to just get a bit more meat in your diet and have slightly nicer household goods?  Wouldn’t you rather rest?



And so what you see is most peasant households aiming not for the full respectability basket, but that “subsistence – and a little bit more” because as each day of labor produces less product and each product produces less joy, at some point you’d rather not work.



And as we’ve seen in theory, our households might hit that crossover point – subsistence and a little bit more – fairly quickly in their labor supply. We haven’t yet, but should now, account for labor spent on things like maintaining tools, fixing fences and other capital investments. If we allocate, say, 45 days, for that and assume that our farmers also want to have some cushion on subsistence (say, another 10%), we might expect The Smalls to be more or less satisfied (on that medium landholding, average 6:1 yields) with something like 245 working days (56% of total), the Middles with 331 working days (65%) and the Biggs with 560 (70%). Working like that, they won’t be rich and won’t ever become rich (but they were never going to become rich regardless), but they’ll mostly survive – some years will be hard – and they’ll have a little bit more time to rest.  Some families, a bit more industrious, might push towards achieving most or all of the respectability basket, at least in good years; others might be willing to stick closer to subsistence (or unable to do otherwise).



Of course in areas where the farmland is meaningfully more marginal – average yields around 4:1 rather than 6:1 – our peasants are going to need to work quite a lot more, about 60% more.  That pushes the Smalls to about 84% of their available labor, the Middles to 99% and the Biggs actually slightly into deficit, demanding roughly 110% of their available labor.  We should keep in mind that each peasant household is going to exist somewhere along the spectrum: some with larger amounts of property or access to better land, some with less.  We’ll come back to this in a moment, but this is part of why the poorest of the peasantry were often exempt from things like military service: positioned on marginal land in poor communities, they had little excess labor available.  Most peasant households would have been somewhere in between these two, so a labor utilization rate ranging from 50 to 100%, with a lot of households in that 60-80% labor utilization range.



And now you might think, “doesn’t this take us back to peasants actually not working all that much compared to modern workers?” and first I would want to point out that these peasants are also experiencing a quality of living way below workers in modern industrial countries but also no because we haven’t talked about extraction.







Because of course the problem here, from the perspective of everyone who isn’t our peasants is that if the peasantry only does the amount of agricultural labor necessary to subsist themselves and just a little more, the society doesn’t have economic room for much else in the way of productive (or unproductive) economic activity.  Remember: our peasants are the only significant population actually doing farming.  Sure the Big Men and the gentry and temples and monasteries may own land, but they are mostly renting that land out to peasants (or hiring peasants to work it, or enslaving peasants and forcing them to work it).



And those landholding elites, in turn, want to do things.  They want to build temples, wage wars, throw fancy parties, employ literate scribes to write works of literature and of course they also want to live in leisure (not farming) while doing this.  And the activities they want to do – the temples, wars, fancy parties, scribes and so on – that requires a lot of food and other agricultural goods to sustain the people doing those things.  It also requires a bunch of surplus labor – some of that surplus labor are specialists, but a lot of it is effectively ‘unspecialized’ labor.



To do those things, those elites need to draw both agricultural surplus and surplus labor out of the countryside. And we should note that of course, obviously, this is an exploitative relationship, but it is also worth noting that for pre-modern agrarian economies, the societies where elites can centralize and control the largest pile of labor and surplus tend to use it to conquer the societies that don’t so ‘demilitarized peasant utopia’ is not a society that is going to last very long (but ‘highly militarized landowner republic’ might).



It is thus necessary to note that when we see the emergence of complex agrarian societies – cities, writing, architectural wonders, artistic achievements and so on – these achievements are mostly elite projects, ‘funded’ (in food and labor, if not in money) out of extraction from the peasantry.



Exactly how this extraction worked varied a lot society to society and even within regions and ethnic and social classes within society.  As noted above, in areas where agriculture was not very productive, extraction was limited.  By contrast, highly productive regions didn’t so much producer richer peasants as they tended to produce far higher rates of extraction.  In some society, where the freeholding farming peasantry (or part of that peasantry) formed an important political constituency (like some Greek poleis or the Roman Republic), the small farmers might manage to preserve relatively more of their surplus for themselves, but often in exchange for significant demands in terms of military and civic participation.



To take perhaps the simplest direct example of removing labor from the countryside, from 218 to 168, the Romans averaged around 10-12 legions deployed in a given year, 45,000-54,000 citizen soldiers.11 Against an adult-male citizen population of perhaps ~250,000 implies that the Roman army was consuming something like a fifth of all of the available citizen manpower in the countryside, though enslaved laborers and males under 17 wouldn’t be captured by this figure. Accounting for those groups we might imagine the Roman dilectus is siphoning off something like 10-15% of the labor capacity of the countryside on average (sometimes spiking far higher, closing in on half of it). On top of that, the demand of these soldiers that they supply their own arms and armor would have pushed farmers to farm a little bit more than subsistence-and-a-little-more to afford the cost of the arms (traded for or purchased with that surplus; at least initially these transactions are not happening in coined money).



We see similar systems in the Carolingian levy system or the Anglo-Saxon fyrd, where households might be brigaded together – in the Carolingian system, households were grouped into mansi – based on agricultural production (you can see how that works above as a proxy for ‘available surplus labor!’) with a certain number – three or four mansi in the Carolingian system – required to furnish one armed man for either a regional levy or the main field army.  The goal of such systems is to take the surplus labor above and make it available for military service.



Alternately, the elites might not want their peasants as soldiers but as workers.  Thus the very frequent appearance of corvée labor: a requirement of a certain amount of intermittent, unpaid forced labor.  This might be labor on the local lord’s estate (a sort of unpaid tenancy arrangement) or labor on public works (walls, castles, roads) or a rotating labor force working in state-owned (or elite-owned) productive enterprises (mines, for instance).  As with military service, this sort of labor demand could be shaped to what the local populace would bear and enforced by a military aristocracy against a largely disarmed peasantry.  Once again looking at the statistics above, even a few weeks a year per man (rather than per household) would drain most of the surplus labor out of our households.  Adding, for instance, a month of corvée labor of per work-capable male (an age often pegged around seven for these societies) under our favorable (6:1) assumptions above bring our work totals to 305 days (70% of total) for the Smalls, 373 (77%) for the Middles and 650 (81.5%) for the Biggs.  Corvée labor demands could be less than this, but also often quite a bit more (expectations varied a lot by local laws and customs.



Alternately, elites might just crank up the taxes.  In the Hellenistic states (the Ptolemaic and Seleucid kingdoms especially), the army wasn’t a peasant levy, but rather a core of Greco-Macedonian rentier elites (your ‘rich peasants’ or ‘gentlemen farmers’), regional levies and mercenaries.  To pay for that (and fund the lavish courts and public works that royal legitimacy required), the indigenous Levantine, Egyptian, Syrian, Mesopotamian (etc. etc.) underclasses were both made to be the tenants on the estates of those rentier elites (land seized from those same peasants in the initial Macedonian conquest or shortly thereafter) but also to pay very high taxes on their own land.12  So while tax rates on military-settler (that is, Greco-Macedonian rentier elites) land might have been around 10% – 1/12th (8.3%) seems to have been at least somewhat common – taxes on the land of the indigenous laoi could run as high as 50%, even before one got to taxes on markets, customs duties, sales taxes, a head tax and state monopolies on certain natural resources including timber and importantly salt.13  So the poor laoi might be paying extortionate taxes on their own lands, lighter taxes on settler (or temple) lands, but then also paying extortionate rents of those tenant-farmed lands.



Another micro-scale option was debt.  We’ve been assuming our farmers are operating at steady-state subsistence, but as we keep noting, yields in any given year were highly variable.  What peasants were forced to do in bad years, almost invariably as go into debt to the Big Man.  But as noted, they’re simply not generating a lot in the way of surplus to ever pay off that debt.  That in turn makes the debt itself a tool of control, what we often call debt peonage.  Since the Big Man sets the terms of the debt (at a time when the peasant is absolutely desperate) it was trivially easy to construct a debt structure that the peasant could never pay off, giving the Big Man leverage to demand services – labor, tenancy on poor terms, broad social deference, etc. – in perpetuity.  And of course, if the Big Man ever wants to expand his land holdings, all he would need to do would be to call in the un-payable debt and – depending on the laws around debt in the society – either seize the peasant’s land in payment or reduce the peasant into debt-slavery.14



In short, elites had a lot of mechanisms to sop up the excess labor in the countryside and they generally used them.



Consequently, while peasants, unencumbered by taxes, rents, elites, debt, conscription and so on might have been able to survive working only a relatively small fraction of their time (probably around 100 days per year per-working-age male (again, age 7 or so and up) would suffice), they did not live in that world.



Instead, they lived in a world where their own landholdings were extremely small – too small to fully support their households, although their small holdings might still provide a foundation of income for survival.  Instead, they had to work on land owned or at least controlled by Big Men: local rentier-elites, the king, temples, monasteries, and so on.  Those big institutions which could wield both legal and military force in turn extracted high rents and often demanded additional labor from our peasants, which soaked up much of their available labor, leading to that range of 250-300 working days a year, with 10-12 hour days each, for something on the order of 2,500-3,600 working hours for a farm-laboring peasant annually.



Which is quite a lot more than the c. 250 typical work days (261 weekdays minus holidays/vacation) in the United States – just by way of example of a modern industrial economy – at typically eight hours a day or roughly 2,000 working hours a year. Of course it is also the case that those roughly 2,000 modern hours buy a much better standard of living than what our medieval peasants had access to – consider that a single unimpressive car represents more value just in worked metal (steel) than even many ancient or medieval elites could muster. No, you do not work more than a medieval or ancient peasant: you work somewhat less, in order to obtain far more material comfort. Isn’t industrialization grand?



That said, our picture of labor in peasant households is not complete! Indeed, we have only seen to half of our subsistence basket – you will recall we broke out textiles separately – because we haven’t yet even really introduced the workload of probably the most fully employed people in these households: the women. And what’s where we’ll go in the next post in this series.
That is, landholders with enough land to subsist off of the rents without needing to do much or any actual agricultural labor themselves.For scale with the cavalrymen we are talking about just a few thousand households lording over a country of perhaps five million; these fellows are honestly closer to something like a medieval knightly elite than the peasantry.On these allotments, see P. Johstono, The Army of Ptolemaic Egypt, 323-204 BC (2020), 158-160 and C. Fischer-Bovet, Army and Society in Ptolemaic Egypt (2014), 212-217. On the rentier-self-sufficiency of these parcels, at a 5:1 yield, 30 aroura should yield something like 3,500kg wheat equivalent (almost 12 million calories), more than enough to support the settler’s household at a 50% rent (see below) using labor from the much smaller adjacent farms of indigenous Egyptians. Indeed, to me it seems very likely the land allotments were calculated precisely on this basis, with infantrymen receiving the smallest allotment that could reliably support a household in leisure.Le Roy Ladurie, Les Paysans de Languedoc (1966)A reminder that the setérée is an exact unit, about 1/5th to 1/4th of a hectare, so about 0.49-0.62 acres.Rosenstein (2004), 75, n.68; Erdkamp, (2005), 47-8; Cho-yun Hsu, Han Agriculture: The Formation of Early Chinese Agrarian Economy (1980); Johstono, The Army of Ptolemaic Egypt (2020), 101; Fischer-Bovet, Army and Society in Ptolemaic Egypt (2014), 121It’s hard to tell precisely from what I have because Le Roy Ladurie groups households in brackets.The average yield-per-iugerum at each fertility level in wheat equivalent are: 4:1, 81.2kg; 5:1, 108.2kg; 6:1, 135.3kg; 7:1, 162.4kg; 8:1, 189.5kg.I would argue that the Roman approach to Italy from 509 to 218 BC appears to be an exception to this rule: the Romans do tend to use conquered land to set up large numbers of small landholding farms.  Not rich peasants, but the Roman military class – the assidui farmer-citizen-soldiers – were also clearly not utterly impoverished either.  It’s striking that the Romans could have set up a system of rents and tribute extraction in Italy but didn’t, instead effectively terraforming the Italian countryside into a machine for the production of heavy infantry.  That heavy infantry in turn bought the Romans stunning military superiority, which they then used in the second and first centuries BC to create an enormous system of tribute and extraction (rather than extending the approach they had used in Italy).Of course you can drain a swamp, but such drainage efforts are the kinds of things large, well-administered states do, not the sort of thing your local peasants can summon the labor for.On Roman deployments, see Taylor, Soldiers and Silver (2020).The way this was structurally, legally, was that the king, directly or indirectly owned all the land (‘spear-won’) and so many taxes were instead technically ‘rents’ paid to the king.On the Seleucid taxation system, see Aperghis, The Seleucid Royal Economy (2004).  For an overview of the relatively similar Ptolemaic system, see von Reden, Money in Ptolemaic Egypt (2007), Préaux, . L’économie royale des Lagides (1979).The abolition of this specific form of slavery (but not others) is a key political moment in the development of both Rome and Athens (and we may assume, many other Greek poleis) that signals the political importance of the smallholding farmer-citizens and their ability to compel major reforms.  But the Big Man can still seize your farm!	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Meow: Yet another modal editing on Emacs]]></title>
            <link>https://github.com/meow-edit/meow</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45228396</guid>
            <description><![CDATA[Yet another modal editing on Emacs / 猫态编辑. Contribute to meow-edit/meow development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Meow
   

Introduction

  Less is more

Meow is yet another modal editing mode for Emacs.
Meow aims to blend modal editing into Emacs with minimal interference
  with its original key-bindings, avoiding most of the hassle introduced
  by key-binding conflicts. This leads to lower necessary configuration and
  better integration. More is achieved with fewer commands to remember.
Key features compared to existing solutions:

  Minimal configuration – build your own modal editing system
  No third-party dependencies (try it without touching your configuration)
  Doesn’t occupy too many keys
    
      Much easier to remember for people trying modal editing
      More keys available for your own key-bindings
      Most of the time, you don’t even need to hold shift!
    
  
  Lightning fast (unlike Evil)
  Minimizes modifier usage (e.g. SPC x f for C-x C-f) inspired by god-mode
  Better workflow for kmacro application at multiple locations
  Interactive selection manipulation and expansion inspired by avy
  Selection as top-tier object, and keybindings built around selection
  Compatible with the vanilla Emacs keymap (or any other keymap from any package)
  Effortless uniform keymaps across modes
  Key-binding conflict handling made easy

Community
Please feel free to ask questions and share ideas at

  Github Discussion
  Meow XMPP Channel

Documents
Get started - Installation and configuration
Tutorial - Learn Meow in 15 minutes
FAQ - Frequently Asked Questions
Commands - Documentation for commands
Customizations - Helper functions and variables
Explanation - Ideas and concepts behind Meow
Changelog - Changes, releases, and news
License
Licensed under GPLv3.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tips for installing Windows 98 in QEMU/UTM]]></title>
            <link>https://sporks.space/2025/08/28/tips-for-installing-windows-98-in-qemu-utm/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45227749</guid>
            <description><![CDATA[Windows 98 runs surprisingly well in QEMU via UTM SE, but it requires some care in setting it up. It’s a great way to run old 90s Windows and DOS software on your iPad (and Mac too, though you have… Continue reading →]]></description>
            <content:encoded><![CDATA[
		

		
			

	

	
		
			
Windows 98 runs surprisingly well in QEMU via UTM SE, but it requires some care in setting it up. It’s a great way to run old 90s Windows and DOS software on your iPad (and Mac too, though you have other options available to you, or an iPhone if you don’t mind the HID difficulties).



This post provides some suggestions and tips for installing Windows and selecting the best emulated devices. The guidance is intended for UTM users on Apple platforms, but should apply to anything QEMU based (or QEMU itself). The advice might also be useful for other operating systems in UTM/QEMU as well.







Plug and play BIOS issues (or: how to install with ACPI)



When you install Windows 9x, PCI devices might be broken, and you’ll see a Plug and Play BIOS device with problems in the device manager:







This seems to be a bug in SeaBIOS or QEMU; I haven’t yet seen an issue tracking this. Many guides (i.e. this one or this one) suggest changing the device and hoping devices re-enumerate correctly. However, there’s a simpler method available when using Windows 98 SE. (If you’re using Windows 95, you won’t be able to do this.)



Windows 98 can use ACPI to enumerate devices instead of the legacy PnP BIOS. Unfortunately, it doesn’t use ACPI by default. (There seems to be an allowlist of known good ACPI BIOSes, as it was early days for ACPI.) To make it use ACPI anyways, boot with CD-ROM support from the Windows 98 CD instead of running the installer, then run Windows setup with the /p j flag, like so:



C:\> D:
D:\> cd WIN98
D:\WIN98> setup /p j



It’s possible to convert an existing system to ACPI, but it’s much easier to do this from the start. When Windows is installed this way, it should correctly enumerate all devices.



Device selection



System



QEMU can emulate devices Windows 98 supports out of the box, which is good as there are no VirtIO drivers. Make sure you’re using the i440-based “pc” rather than the Q35 based system, as it’ll be better supported for legacy systems. You don’t need to worry about selecting i386 vs. x86_64, as Windows 98 will obviously never touch 64-bit mode, so they’ll be the same.



(As a tip, if you’re running NT 4, you’ll need to select a different CPU to make sure it’s happy with the CPU flags as the default one is too new. A Pentium II should be sufficiently old.)



Input



You may need to disable USB (or at least, USB input devices) to avoid hanging on startup, at least with UTM (It’s possible the ‘Force PS/2 Controller’ option might work, but I haven’t had much luck with it. Unfortunately, this means you won’t have absolute mouse input (through the USB tablet) and must capture your cursor. With UTM SE on an iPad, this doesn’t hurt as much, as it can automatically capture the trackpad or external mouse, while leaving the touchscreen for interacting with iOS.



Video



The most sensible video option for Windows 98 is the Cirrus VGA (-vga cirrus). There are unfortunately some bugs (flashing in 16 bit colour modes, blitting issues in 8 bit colour modes), but it’s the only option with accelerated drivers out of the box. (Of course, there is no 3D acceleration with such a card.)



Apparently, Rage 128 emulation is being worked on (ati-vga), but currently only works for Power Mac emulation, and is in rough shape so far.



Networking and getting files in



For getting files into the VM easily, you’ll want a network. SLiRP NAT works fine for using a browser or SMB shares, for example. (Note this works better on Windows 98 than 95; 95 has issues with mounting SMB shares by IP and doesn’t come with a browser.) QEMU can emulate a variety of network cards. The tulip (DC2114x), NE2000 (PCI and ISA), and PCNet should all work out of the box with older Windows. I’d recommend using a PCI card if possible, since it saves you the ISA setup headache unless you need it for something old. If you do need to set up an ISA NE2000, it’s at address 300h, IRQ 9, which might require manual configuration in some cases.



Sound



For sound hardware, there are a few options available, with different tradeoffs.




If you want to run DOS software, the SoundBlaster 16 (sb16) emulation works out of the box, but there is no OPL3 or MPU-401, so MIDI won’t work correctly, just PCM. Games will have a hard time with this unless they’re entirely PCM. For setting up your SB16 for DOS games, use SET BLASTER=A220 I5 D1 H5 P330 T5 (that’s address 220h, interrupt 5, 8-bit DMA 1, our non-existent MPU-401 at 330h, 16-bit DMA 5).

Note that QEMU supports adding an AdLib (OPL2 based) separately, which might help with some software.





The CS4321A I haven’t tested, but might work with WSS or Crystal-specific drivers. As with the SoundBlaster 16, there is no OPL3. QEMU sets this up at 534h, IRQ 9, DMA 3.



The Gravis UltraSound (gus) emulation works surprisingly well, but the Windows 95 drivers are crusty for the version of the card it emulates (GF1/GUS Classic), so use it only if you want to run old trackers or demoscene stuff. Note you may need to turn off the LPT port (-parallel none) to free up an interrupt used for the UltraSound.



Because of this, the ES1370 might be the best card to emulate for plain Windows usage, as it has relatively few quirks and I believe has drivers on the Windows 98 CD. However, it’s not ideal for DOS software as it requires TSRs to make it work right.



The AC97 emulation will require Realtek drivers. I haven’t tested this.




Potpourri



In UTM, you may want to turn off the entropy device, to reduce unknown device clutter in Device Manager, though it’s harmless. The VirtIO console device will still be present in Device Manager with UTM’s default flags.



Other quirks



In UTM SE, sometimes rebooting might hang when switching video modes. If this happens, it seems safe to shut down the machine and start it again. Avoiding reboots in favour of shutting down seems wise.



Performance characteristics



While TCG in QEMU doesn’t have the best reputation for performance, it might be good enough for your needs. On my MacBook Pro with an M1 Pro, benchmarks show performance somewhat around about a 750 MHz Pentium III, albeit with worse floating point performance. This is pretty usable, although most 3D games won’t be usable as even software rendering will be a bit sketchy.



If you’re using UTM SE on iOS, the interpreter is slower, but not unusable for 90s software. On my M1 iPad Pro, I get Pentium 100 performance, with similar penalties for FP. This is good for games up to about 1995 or 1996; titles like MechWarrior 2, Widget Workshop, many edutainment titles, and SimCity 2000 are playable this way, though MIDI or CD music will be missing. Non-game software like Office 97 or Visual C++ will run fine, of course. For OSes, this also puts things like Windows 2000 and beyond just out of reach performance wise – stick with Windows 98 for the best compatibility.
					

	
	

				
	
			

		
		
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[FFglitch, FFmpeg fork for glitch art]]></title>
            <link>https://ffglitch.org/gallery/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45227212</guid>
            <description><![CDATA[There are some artists out there doing some amazing work using FFglitch.]]></description>
            <content:encoded><![CDATA[
        
  
  
    






There are some artists out there doing some amazing work using FFglitch.

I put this page up so that I don’t have to go hunting for examples every time I want to show someone what can be done with FFglitch.



Thomas Collet has a lot of work using FFglitch on vimeo, instagram, and reddit.






A bunch more from Thomas:

  https://vimeo.com/366067869
  https://vimeo.com/363105562
  https://vimeo.com/323235580
  https://www.reddit.com/r/glitch_art/comments/b9yfxc/study_on_crowd_movements/
  https://www.reddit.com/r/brokengifs/comments/grpwn4/tripping_in_manhattan/
  https://www.reddit.com/r/woahdude/comments/bg176f/i_went_to_ireland_filmed_the_ocean_and_glitched_it/
  https://www.reddit.com/r/woahdude/comments/ballm7/when_the_world_is_slowly_but_surely_falling_appart/
  https://www.reddit.com/r/glitch_art/comments/fhpwgp/falling_appart/
  https://www.reddit.com/r/glitch_art/comments/hxk6r1/when_it_kicks_in_the_middle_of_time_square/




Kaspar Ravel wrote a blog post
about a collaboration he did with Thomas Collet which resulted in this gem:


Here’s the blog post: https://www.kaspar.wtf/blog/encoding-the-game-of-life-in-datamosh

And the post on reddit: https://www.reddit.com/r/brokengifs/comments/e25f6b/want_to_see_a_magic_trick/



Sebastien Brias:


https://www.instagram.com/p/CPNaIp8qo-r



Myra Rivera (@myyyraa)

Go check out Myra’s beautiful work and exhibition Glitched Flowers (I wish I had been there to see it personally…)


https://www.instagram.com/p/CYFo19HolJD



Jason Hallen

Go read about Jason’s experimentations at https://www.jasonhallen.com/output, there’s a lot more with FFglitch!






glit_chbee (turn the volume up and enjoy the ride):






nowahe:






Ben Cooper made this clip by using mainly avidemux, tomato.py, and FFglitch.






Jo Grys has posted some videos on Facebook:




There are more if you search for #ffglitch on Facebook:

  https://www.facebook.com/hashtag/ffglitch/




And some more random clips I found spread around the interwebz:

  https://www.reddit.com/r/brokengifs/comments/ey863f/some_minor_smudging
  https://fb.com/groups/Glitchcollective/?post_id=2223010624487144
  https://www.instagram.com/p/B_QKvtcBJaW


  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I used standard Emacs extension-points to extend org-mode]]></title>
            <link>https://edoput.it/2025/04/16/emacs-paradigm-shift.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45226639</guid>
            <description><![CDATA[Recently I read this beginners guide to extend Emacs.The guide is perfect for starting out with elisp and it shows a lot of care in teaching how to interact ...]]></description>
            <content:encoded><![CDATA[
        

  

  
    Recently I read this beginners guide to extend Emacs.
The guide is perfect for starting out with elisp and it shows a lot of care in teaching how to interact with Emacs.

To me, the most important bit though is this one, from the section aptly named Emacs Wants You to Extend It.


  I haven’t written plugins for other editors extensively, but I can tell you this: emacs doesn’t just make deep customization available, but it actively encourages you to make an absolute customization messes masterpieces. Core editor functions aren’t just documented, but often include tidbits about “you probably want to see this other variable” or “here’s how you should use this”.

  Not only that, but emacs happily hands you functions shaped like nuclear warheads like advice-add (that let you override any function) that can absolutely obliterate your editor if you hold it the wrong way. Of course, this also grants you unlimited power.

  Remember that emacs is designed to be torn apart and rearranged.


This is the core bit of the argument. Emacs, as a system, wants you to
extend it and it gives you all the means to do so. This is in contrast
with systems that can be extended through scripting and instead don’t give
you all the means to do so!

I think the tutorial is a fantastic example of doing things
right. There is a well-thought example, a constructive approach where
the solution grows to a full package.

This is problematic. You may get the impression that extending Emacs
is only possible if you do things right and that is definitely not true.

To make my point I want to walk you through an example. I will show
you how I used standard Emacs extension-points to extend org-mode to sort my
reading lists automatically.

What do I want?

The behavior I want is that when I save an org file the entries are
ordered automatically. I keep a timeline of the papers I am reading
and it is annoying to keep them kind of ordered.

This is the content of an example buffer.

#+TITLE: My tematic reading list

* Paper which is old but not too old
:PROPERTY:
:year: 2002
:END:

* Definitely older paper but unfortunately it's later in the list
:PROPERTY:
:year: 1998
:END:

When I add a paper to my reading list I run org-sort-entries and
interactively select to order the entries by the value in the property
year. Initally this was nice to have but now it’s just annoying that
I have to keep doing it. Let’s extend org-mode so that this is done automatically.

A simple solution

The first step is to automate the interactive part. Lucky for me this is easy
as org-sort-entries is both a function and a command. I can call it in a
script just as I can run it as a command.

(defvar org-sort-option "year")

(defun org-sort-run ()
  (when (and (derived-mode-p 'org-mode) org-sort-option)
    (let ((case-sensitive nil)
	  (sorting-type ?r)
	  (getkey-func nil)
	  (compare-func nil)
	  (property org-sort-option)
	  (interactive? nil))
      (org-sort-entries case-sensitive sorting-type getkey-func compare-func property interactive?))))

This solves one part of the problem. Let’s solve the other one, automatically calling
org-sort-run whenever an org-mode buffer is saved.

Emacs already has support for this use-case through the use of hooks. We can run
org-sort-run all the times we want to save a buffer.

(add-hook 'before-save-hook #'org-sort-run)

These two together solve the problem but the solution presented is “just more code”.
We tapped into the hook extension point but this would be possible in any
scriptable system that exposes well-defined extension points such as hooks and commands.

Leveraging Emacs’ extensibility to extend org-mode

I want to show that even if something is not thought with
extensibility in mind Emacs allow us to extend it. Most importantly, while we
want to extend org-mode’s behavior we would like this not to be an
extension to org-mode’s code.

Here’s the updated problem statement. Have the buffer be automatically
sorted and have the sorting criteria be in the buffer itself. We will
specify the sorting as a in-buffer setting and use Emacs to
support this never thought before org-mode behavior.

Our example buffer changes to the following.

 #+TITLE: My tematic reading list
+#+SORT: year
 
 * Paper which is old but not too old
 :PROPERTY:
 :year: 2002
 :END:
 
 * Definitely older paper but unfortunately it's later in the list
 :PROPERTY:
 :year: 1998
 :END:



The hard part of this is to find how org-mode reads in-buffer
settings
from the header. A M-x find-library later we are in org’s
sources.

Searching for +STARTUP (Ctrl+s +STARTUP), one of the
supported settings, leads us to org-startup-folded and that in turn
(Ctrl+s org-startup-folded) leads us to org-startup-options.

org-startup-options is the used by (again Ctrl+s org-startup-option)
org-set-regexps-and-options.



While the documentation for this function is not very convincing, its code
does make sense for what we are after. I copied it here for reference.

  (when (derived-mode-p 'org-mode)
    (let ((alist (org-collect-keyword
		  (append '("FILETAGS" "TAGS")
			  (and (not tags-only)
			       '("ARCHIVE" "CATEGORY" "COLUMNS" "CONSTANTS"
				 "LINK" "OPTIONS" "PRIORITIES" "PROPERTY"
				 "SEQ_TODO" "STARTUP" "TODO" "TYP_TODO")))
		  '("ARCHIVE" "CATEGORY" "COLUMNS" "PRIORITIES"))))
      ;; Startup options.  Get this early since it does change
      ;; behavior for other options (e.g., tags).
      (let ((startup (cl-mapcan (lambda (value) (split-string value))
				(cdr (assoc "STARTUP" alist)))))
	     ...)

Unfortunately this function calls org-collect-keyword with a list that we cannot
touch. There is no custom variable to set to pass our own keyword.

If this was a “normal programming environment” we would make our changes
to this function body and forever maintain a fork of org-mode. As this
is elisp instead we have choices.

I think the best choice is to use advice-add and have Emacs call our
advice code every time org-set-regexps-and-options is called. We will copy
what we need from the function body but that will be all.

This is what I ended up with.

(defvar org-sort-option nil)

(defun org-sort-set-option (&rest r)
  "Read the +SORT: spec value into variable `org-sort-option'."
  (when (derived-mode-p 'org-mode)
    (let ((alist (org-collect-keywords '("SORT"))))
      (let ((sort (cdr (assoc "SORT" alist))))
	(let ((sort-spec (car (read-from-string (car sort)))))
	  (setq-local org-sort-option sort-spec))))))

(advice-add 'org-set-regexps-and-options :after #'org-sort-set-option)

(defun org-sort-run ()
  (when (and (derived-mode-p 'org-mode) org-sort-option)
    (let ((case-sensitive nil)
	  (sorting-type ?r)
	  (getkey-func nil)
	  (compare-func nil)
	  (property org-sort-option)
	  (interactive? nil))
      (org-sort-entries case-sensitive sorting-type getkey-func compare-func property interactive?))))

(add-hook 'before-save-hook #'org-sort-run)

We keep a buffer-local variable org-sort-option around to store the
property name read from #+SORT: property-name. This variable is initially
nil and will be set from the property name in #+SORT: property-name. To do so
we have a function org-sort-set-option.

But when to call org-sort-set-option? The easy way out is to have Emacs call it whenever
org-set-regexps-and-options is called on a file visit. To achieve this we
tap into advice-add and ask Emacs to run org-sort-set-option after
org-sort-regexps-and-options.

We have now succesfully interposed ourselves in the control flow of the org-mode library.

Org-mode did not provide any interposition point for us, there
is no thought ahead etension-point or configuration variable we can
use to achieve our goal an yet here we are with a sorted buffer.

We succeeded in our effort because Emacs wants you to extend it
and it gives you all the means to do so.

Conclusions

I have made a horrible hack and it works. I have learnt nothing about
how org-mode works or Emacs’ file-visiting extension-points.

  



      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UTF-8 is a brilliant design]]></title>
            <link>https://iamvishnu.com/posts/utf8-is-brilliant-design</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45225098</guid>
            <description><![CDATA[Exploring the brilliant design of UTF-8 encoding system that represents millions of characters while being backward compatible with ASCII]]></description>
            <content:encoded><![CDATA[
    UTF-8 is a Brilliant Design
    2025-09-12
    


The first time I learned about UTF-8 encoding, I was fascinated by how well-thought and brilliantly it was designed to represent millions of characters from different languages and scripts, and still be backward compatible with ASCII.
Basically UTF-8 uses 32 bits and the old ASCII uses 7 bits, but UTF-8 is designed in such a way that:

Every ASCII encoded file is a valid UTF-8 file.
Every UTF-8 encoded file that has only ASCII characters is a valid ASCII file.

Designing a system that scales to millions of characters and still be compatible with the old systems that use just 128 characters is a brilliant design.

Note: If you are already aware of the UTF-8 encoding, you can explore the UTF-8 Playground utility that I built to visualize UTF-8 encoding.

How Does UTF-8 Do It?
UTF-8 is a variable-width character encoding designed to represent every character in the Unicode character set, encompassing characters from most of the world's writing systems.
It encodes characters using one to four bytes. 
The first 128 characters (U+0000 to U+007F) are encoded with a single byte, ensuring backward compatibility with ASCII, and this is the reason why a file with only ASCII characters is a valid UTF-8 file.
Other characters require two, three, or four bytes. The leading bits of the first byte determine the total number of bytes that represents the current character. These bits follow one of four specific patterns, which indicate how many continuation bytes follow.



1st byte Pattern
# of bytes used
Full byte sequence pattern



0xxxxxxx
1
0xxxxxxx(This is basically a regular ASCII encoded byte)


110xxxxx
2
110xxxxx 10xxxxxx


1110xxxx
3
1110xxxx 10xxxxxx 10xxxxxx


11110xxx
4
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx


Notice that the second, third, and fourth bytes in a multi-byte sequence always start with 10. This indicates that these bytes are continuation bytes, following the main byte.
The remaining bits in the main byte, along with the bits in the continuation bytes, are combined to form the character's code point. A code point serves as a unique identifier for a character in the Unicode character set. A code point is typically represented in hexadecimal format, prefixed with "U+". For example, the code point for the character "A" is U+0041.
So here is how a software determines the character from the UTF-8 encoded bytes:

Read a byte. If it starts with 0, it's a single-byte character (ASCII). Show the character represented by the remaining 7 bits on the screen. Continue with the next byte.
If the byte didn't start with a 0, then:
If it starts with 110, it's a two-byte character, so read the next byte as well.
If it starts with 1110, it's a three-byte character, so read the next two bytes.
If it starts with 11110, it's a four-byte character, so read the next three bytes.


Once the number of bytes are determined, read all the remaining bits except the leading bits, and find the binary value (aka. code point) of the character.
Look up the code point in the Unicode character set to find the corresponding character and display it on the screen.
Read the next byte and repeat the process.

Example: Hindi Letter "अ" (open in UTF-8 Playground)
The Hindi letter "अ" (officially "Devanagari Letter A") is represented in UTF-8 as:
11100000 10100100 10000101
Here:
The first byte 11100000 indicates that the character is encoded using 3 bytes.
The remaining bits of the three bytes:
xxxx0000 xx100100 xx000101 
are combined to form the binary sequence 00001001 00000101 (0x0905 in hexadecimal). This is the code point of the character, represented as U+0905.
The code point U+0905 (see official chart) represents the Hindi letter "अ" in the Unicode character set.
Example Text Files
Now that we understood the design of UTF-8, let's look at a file that contains the following text:
1. Text file contains: Hey👋 Buddy
The text Hey👋 Buddy has both English characters and an emoji character on it. The text file with this text saved on the disk will have the following 13 bytes in it:
01001000 01100101 01111001 11110000 10011111 10010001 10001011 00100000 01000010 01110101 01100100 01100100 01111001
Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:



Byte
Explanation



01001000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1001000 represent the letter 'H'. (open in playground)


01100101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100101 represent the letter 'e'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


11110000
Starts with 11110, indicating it's the first byte of a four-byte character.


10011111
Starts with 10, indicating it's a continuation byte.


10010001
Starts with 10, indicating it's a continuation byte.


10001011
Starts with 10, indicating it's a continuation byte.The bits from these four bytes (excluding the leading bits) combine to form the binary sequence 00001 11110100 01001011, which is 1F44B in hexadecimal, corresponds to the code point U+1F44B. This code point represents the waving hand emoji "👋" in the Unicode character set (open in playground).


00100000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 0100000 represent a whitespace character. (open in playground)


01000010
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1000010 represent the letter 'B'. (open in playground)


01110101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1110101 represent the letter 'u'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


Now this is a valid UTF-8 file, but it doesn't have to be "backward compatible" with ASCII because it contains a non-ASCII character (the emoji). Next let's create a file that contains only ASCII characters.
2. Text file contains: Hey Buddy
The text file doesn't have any non-ASCII characters. The file saved on the disk has the following 9 bytes in it:
01001000 01100101 01111001 00100000 01000010 01110101 01100100 01100100 01111001
Let's evaluate this file byte-by-byte following the UTF-8 decoding rules:



Byte
Explanation



01001000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1001000 represent the letter 'H'. (open in playground)


01100101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100101 represent the letter 'e'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


00100000
Starts with 0, so it's a single-byte ASCII character. The remaining bits 0100000 represent a whitespace character. (open in playground)


01000010
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1000010 represent the letter 'B'. (open in playground)


01110101
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1110101 represent the letter 'u'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01100100
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1100100 represent the letter 'd'. (open in playground)


01111001
Starts with 0, so it's a single-byte ASCII character. The remaining bits 1111001 represent the letter 'y'. (open in playground)


So this is a valid UTF-8 file, and it is also a valid ASCII file. The bytes in this file follows both the UTF-8 and ASCII encoding rules. This is how UTF-8 is designed to be backward compatible with ASCII.
Other Encodings
I did a quick research on any other encoding that are backward compatible with ASCII, and there are a few, but they are not as popular as UTF-8, for example GB 18030 (a Chinese government standard). Another one is the ISO/IEC 8859 encodings are single-byte encodings that extend ASCII to include additional characters, but they are limited to 256 characters.
The siblings of UTF-8, like UTF-16 and UTF-32, are not backward compatible with ASCII. For example, the letter 'A' in UTF-16 is represented as: 00 41 (two bytes), while in UTF-32 it is represented as: 00 00 00 41 (four bytes).
Bonus: UTF-8 Playground
When I was exploring the UTF-8 encoding, I couldn't find any good tool to interactively visualize how UTF-8 encoding works. So I built UTF-8 Playground to visualize and play around with UTF-8 encoding. Give it a try!.



Read an ocean of knowledge and references that extends this post on Hacker News.



Some excellent references on UTF-8:


Joel Spolsky's famous 2003 article (still relevant): The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)
"UTF-8 was designed, in front of my eyes, on a placemat in a New Jersey diner one night in September or so 1992." - Rob Pike on designing UTF-8 with Ken Thompson


    
        #tech
        #history
        #programming
    

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EU court rules nuclear energy is clean energy]]></title>
            <link>https://www.weplanet.org/post/eu-court-rules-nuclear-energy-is-clean-energy</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45224967</guid>
            <description><![CDATA[The highest court in the EU just reaffirmed that nuclear energy meets the scientific and environmental standards to be included in sustainable finance, and Greenpeace still refuses to budge.]]></description>
            <content:encoded><![CDATA[Launching Dear Greenpeace back in 2023When I launched Dear Greenpeace with my fellow youth climate activists alongside WePlanet two years ago, I had no idea just how quickly the anti-nuclear dominoes would fall across Europe. In 2023, and what seems like a lifetime ago, Austria launched their legal action against the European Commission for the inclusion of nuclear energy in the EU Sustainable Finance Taxonomy. At the time they were supported by a bulwark of EU countries and environmental NGOs that opposed nuclear energy. Honestly, it looked like they might win.But today, that whole landscape has changed.Germany, long a symbol of anti-nuclear politics, is beginning to shift. The nuclear phase-outs or bans in the Netherlands, Belgium, Switzerland, Denmark, and Italy are now history. Even Fridays for Future has quietened its opposition, and in some places, embraced nuclear power.This moment matters.It shows what’s possible when we stick to the science. The evidence only gets clearer by the day that nuclear energy has an extremely low environmental impact across its lifecycle, and strong regulations and safety culture ensure that it remains one of the safest forms of energy available to humanity. The European Court of Justice has now fully dismissed Austria’s lawsuit. That ruling doesn’t just uphold nuclear energy’s place in EU green finance rules. It also signals a near-certain defeat for the ongoing Greenpeace case – the very lawsuit that inspired me to launch Dear Greenpeace in the first place.But instead of learning from this, Greenpeace is doubling down. Martin Kaiser, Executive Director of Greenpeace Germany, called the court decision “a dark day for the climate”.Let that sink in. The highest court in the EU just reaffirmed that nuclear energy meets the scientific and environmental standards to be included in sustainable finance, and Greenpeace still refuses to budge.Meanwhile, the climate crisis gets worse. Global emissions are not falling fast enough. Billions of people still lack access to clean, reliable electricity. And we are forced to spend time defending proven solutions instead of scaling them.Announcing our inclusion in the case between Greenpeace and the EU CommissionIt’s now up to the court whether we will get our time in court to outline the evidence in support of nuclear energy and the important role it can play in the global clean energy transition. Whether in court, on the streets, or in the halls of parliaments across the globe, we will be there to defend the science and ensure that nuclear power can spread the advantages of the modern world across the planet in a sustainable, reliable and dignified way.Austria stands increasingly isolated among a handful of countries that still cling to their opposition to nuclear energy. Their defeat in this vital high stakes topic is a success not just for the nuclear movement, but for the global transition as a whole. We have made real progress. Together, we’ve helped defend nuclear power in the EU, overturned outdated policies at the World Bank, and secured more technology-neutral language at the UN. These wins are not abstract. They open the door to real investment, real projects, and real emissions cuts.But the work is not done.We still need to overturn national nuclear bans, unlock more funding, and push democratic countries to support clean energy development abroad: especially where it is most needed to compete with Russia’s growing influence.The fight will not be done until every single country in the world can boast a clean, reliable energy grid, ready to maintain a modern dignified standard of living, for everyone, everywhere.This is a great success for the movement and it would not have been possible without the financial support, time and energy given by people like you.In Solidarity,
Ia Aanstoot]]></content:encoded>
        </item>
    </channel>
</rss>