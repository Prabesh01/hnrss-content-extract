<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 10:10:20 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Telli (YC F24) is hiring engineers, designers, and interns [on-site Berlin]]]></title>
            <link>https://hi.telli.com/join-us</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45090216</guid>
        </item>
        <item>
            <title><![CDATA[What Is Complexity in Chess?]]></title>
            <link>https://lichess.org/@/Toadofsky/blog/what-is-complexity/pKo1swFh</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45089256</guid>
            <description><![CDATA[If we all knew, we would all be masters.]]></description>
            <content:encoded><![CDATA[If we all knew, we would all be masters.May 2020 an interesting proposal was suggested.
I provided some constructive criticism on research paper A Metric of Chess Complexity by FM David Peng, as well as constructive criticism on the codebase used to validate this experiment. For many months I have refrained from further comment, and although code has not progressed, two things have:1. Public interest in "complexity" as determined by ACPL (yuck).2. Lichess has a blogging platform where I can properly address deficiencies in the research method and control the conversation which I start.
... so the time has come for me to share my remaining criticisms of this ambitious project. Previously I had shared some easier-to-address criticisms while privately I shared Peng's suggestion with the Lichess team.
The Golden Goose
"Such a feature has the potential to revolutionize chess and would be invaluable to any chess website. Some specific applications include generating non-tactical puzzles (imagine tactics trainer for positional chess puzzles), creating chess computers that play with human personalities, and identifying concepts that are key to improvement at any rating level."
Science is a window for us to learn more about the world around us. Marketing is about selling ideas to an audience. This statement, if true, would have already garnered interest by both scientists and business people, who by exerting a modicum of effort could easily develop and sell products based upon them. Further, if true, this could also inspire a black market of cheating software to help players identify the risk associated with cheating in particular positions. Peng's paper makes many similar promises to the above, so this raises the level of scrutiny I take to the rest of the paper.
Propositions
This paper specifies complexity in two different propositions:a) Complexity is a 1-dimensional, transferable (teachable to a neural network) metric based upon centipawn loss as determined by some version(s) of Stockfish with or without a neural network.b) By definition, complexity can be used in real time to determine how difficult a position is.While some people's intuitions may support the notion that these propositions support or complement each other, I am unconvinced; regardless, it takes more than intuition to create useful tools.
Logic
Even if the above axioms were true, how many of these conclusions are logically valid?1. Non-tactical puzzles could be generated by identifying challenging positions (as opposed to the current method which is based upon positions where the solution is superior in evaluation to other variations).2. The current model for puzzle ratings (based upon "elo") takes many attempts to establish an initial puzzle rating.3. Holistic opening preparation can be automated by software, making players understand openings rather than memorize them.4. Interesting positions for books are the same as difficult positions, which are the same as complex positions.5. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop training materials to help players understand common key concepts.6. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop a diagnostic chess exam which identifies a player's rating and identifies key concepts for improvement.7. Large databases contain additional tagged information, such as time control, which would produce significant insight into which positions can be played intuitively. Large databases also indicate player ratings and therefore somehow complexity can be used to identify unique strategies useful for playing at a rating advantage or disadvantage.8. Chess players have human personalities.9. Opening systems can be devised around an opponent's tendency to seek or to avoid complexity.10. Chess players are likely to make errors in difficult positions, unlike engines, and therefore a complexity metric would be an invaluable tool.11. Spectating (and honestly, post-game analysis) of top chess games could be enriched by displaying complexity information related to each position, informing spectators who otherwise look at engine evaluations and variations & jump to conclusions.12. Complexity varies by variant; for example blitz and correspondence have different complexities for identical positions.
In my opinion, conclusion#11 is valid and others require further research. Anyway, on to Peng's research...
Neural Networks
This paper nearly predates efforts by DeepMind, the Leela Chess Zero team, and the Stockfish team which resulted in development of Stockfish-NNUE. We could not have anticipated such rapid developments! Many chess players had opinions that AlphaZero and Leela played much more human-like moves than traditional engines, in much the same manner that decades prior world champion Kasparov was astounded that Deep Blue played human-like sacrifices. Whatever conclusions are drawn may need to be updated since both Stockfish evaluations without NNUE, and Stockfish-NNUE evaluations, have rapidly changed (complementing Stockfish search changes and search parameter changes).
Endgame Scaling
Stockfish evaluations in the middlegame are capped at 10 and in the endgame are capped at 100. As such, it seems unreasonable to deviate from prior research indicating the need for a sigmoid to normalize evaluations before classifying example input moves as blunders.
Board Representation
Doing original research allows liberties in methods and models, although considerations offered here differ from those announced and discussed in public interviews by DeepMind's CEO. While I don't fully agree with DeepMind's emphasis on asymmetry and castling rights, I do question the need for an extra bit for White/Black to move. For example, the positions after 1. c3 e5 2. c4 (Black to move) and after 1. e4 c5 (White to move) should have the same relative evaluation.
Evaluation Skewness
There is ample prior research about ranking moves. In fact, Peng's research here is predicated on a notion that traditional engines sometimes indicate to spectators that two moves are equally good, despite one resulting in a more difficult position than the other. We cannot be fully certain that players in fact played the best moves, as this is the very concept we are trying to figure out how to measure! Regardless, we have to start somewhere, and this seems like a good first attempt.
Summary
I could further nitpick... I criticize because I am impressed and I care about this subject. I am further impressed that results were split by "elo," leading to a discovery that some positions are difficult for all players, whereas some positions are more difficult for lower-rated players than for higher-rated players.
Other possible improvements could involve:* Obtain segmented Stockfish evaluations (material, pawn structure, etc.) and WDL statistics* Obtain Stockfish-NNUE evaluations and WDL predictions* Model checks in sample input* Model log(time remaining) in sample input* Maybe bootstrap models based upon known pawn concepts* Maybe include some human versus engine games. Some bots such as Boris-Trapsky and TurtleBot have personalities!
Thanks for the suggestion and someday I hope to see Lichess.org or some other site implement a complexity metric before cheaters do.

Photo credit: Pacto Visual
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nintendo Switch 2 Dock USB-C Compatibility]]></title>
            <link>https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087971</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Spotilyrics – See synchronized Spotify lyrics inside VS Code]]></title>
            <link>https://github.com/therepanic/spotilyrics</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087905</guid>
            <description><![CDATA[🎧 See synchronized Spotify lyrics inside VS Code while coding - therepanic/spotilyrics]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    



  
  
  






      

          

              




  Navigation Menu

  

  
          
            


                
      

      
          

                
                    
  
      
      
          
            GitHub Copilot
          
        Write better code with AI
      

    


                    
  
      
      
          
            GitHub Spark
              
                New
              
          
        Build and deploy intelligent apps
      

    


                    
  
      
      
          
            GitHub Models
              
                New
              
          
        Manage and compare prompts
      

    


                    
  
      
      
          
            GitHub Advanced Security
          
        Find and fix vulnerabilities
      

    


                    
  
      
      
          
            Actions
          
        Automate any workflow
      

    


                    
                
              
          

                
                    
  
      
      
          
            Codespaces
          
        Instant dev environments
      

    


                    
  
      
      
          
            Issues
          
        Plan and track work
      

    


                    
  
      
      
          
            Code Review
          
        Manage code changes
      

    


                    
  
      
      
          
            Discussions
          
        Collaborate outside of code
      

    


                    
  
      
      
          
            Code Search
          
        Find more, search less
      

    


                
              
          

      



                
      

      



                
      

      
                    Explore
                    
  
      Learning Pathways

    


                    
  
      Events & Webinars

    


                    
  
      Ebooks & Whitepapers

    


                    
  
      Customer Stories

    


                    
  
      Partners

    


                    
  
      Executive Insights

    


                
              



                
      

      
              

                
                    
  
      
      
          
            GitHub Sponsors
          
        Fund open source developers
      

    


                
              
              

                
                    
  
      
      
          
            The ReadME Project
          
        GitHub community articles
      

    


                
              
              
          



                
      

      

                
                    
  
      
      
          
            Enterprise platform
          
        AI-powered developer platform
      

    


                
              



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  





    






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    2

  

  
        
            
          Star
          32

  



        

        


          

  
    


  

  




          



  
  
  Folders and filesNameNameLast commit messageLast commit dateLatest commitHistory46 Commits.idea.idea.vscode.vscodemediamediasrcsrc.gitignore.gitignore.prettierrc.prettierrc.vscodeignore.vscodeignoreLICENSELICENSEREADME.mdREADME.mddemo.pngdemo.pngeslint.config.mjseslint.config.mjsicon.pngicon.pnglogo.pnglogo.pngpackage-lock.jsonpackage-lock.jsonpackage.jsonpackage.jsonspotilyrics.imlspotilyrics.imltsconfig.jsontsconfig.jsonREADMEUnlicense license
  
  See synchronized Spotify lyrics inside VS Code while coding.
  
    
    
    
    
  


✨ Features

📌 Live lyrics sync with your Spotify playback.
🎨 Lyrics colors auto-themed from album cover (via colorthief).
🖥️ Smooth side panel view – code on the left, lyrics on the right.
🔑 Simple one-time login using your own Spotify Client ID.
🚪 Quick logout command to reset session.


📸 Demo

⚡️ Installation


Open VS Code → Extensions → search spotilyrics or install from VS Code Marketplace.


Run the command:


Show Spotify Lyrics via Spotilyrics


🔑 Authentication (one-time setup)

Go to Spotify Developer Dashboard.
Create an app → copy Client ID.
Important: set the Redirect URI for your app to: http://127.0.0.1:8000/callback
Run the Show Spotify Lyrics via Spotilyrics command.
Paste your Client ID in the panel and log in.
Enjoy synced lyrics while coding! 🎶


ℹ️ Why? – To respect Spotify API rate limits, you need your own ID.


⌨️ Commands

Show Spotify Lyrics via Spotilyrics (spotilyrics.lyrics) – open synced lyrics panel.
Logout from Spotilyrics (spotilyrics.logout) – clear session and re-auth when needed.


⚙️ Tech stack

Spotify Web API
LRClib for lyrics with timing
colorthief for cover-based theme
TypeScript + VS Code WebView


📜 License
This project is licensed as Unlicensed.
Feel free to use, hack, and remix it – but no warranties 😉

Made with ❤️ by therepanic. Code hard, vibe harder 🎧





      




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lewis and Clark marked their trail with laxatives]]></title>
            <link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid>
            <description><![CDATA[AS LEWIS AND CLARK’S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren’t thinking much about their place in history. So they weren’t taking any particular pains to document their every movement.

There were, however, some particular pains they were experiencing, as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.

Luckily, they had something that helped with that — a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called “thunder-clappers,” which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.

And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in “thunder-clappers” was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground — which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in — and take samples of the dirt in them. 

If it comes up with an off-the-charts reading for mercury, well, that’s a Corps of Discovery pit toilet — and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
												   
(Astoria, Clatsop County; 1800s) --  #ofor #oregonHistory #ORhistory -- 26 Jan 2025 -- By Finn J.D. John]]></description>
            <content:encoded><![CDATA[
			
		
				
		
		
		
        
			ASTORIA, CLATSOP COUNTY; 1800s: 
			
     
    
			  
			  			  
				   Audio version is not yet available
				  
            


		              By Finn J.D. John
			                January 26, 2025
                            
                        
		              
		              AS LEWIS AND CLARK’S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren’t thinking much about their place in history. So they weren’t taking any particular pains to document their every movement.
            There were, however, some particular pains they were experiencing with every movement, so to speak ... as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.
            Luckily, they had something that helped with that — a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called “thunder-clappers,” which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.
            
               
                  The reproduction of Fort Clatsop, built at or near the site of the Corps of Expedition's original buildings. Dr. Rush's Bilious Pills have not been particularly helpful in locating the original Fort Clatsop, long since rotted away — either because it hasn’t been found yet, or because the site of the old pit latrine has been disturbed by farming or logging activities in the years since. (Image: National Parks Service)
                
              
            
            And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in “thunder-clappers” was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground — which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in — and take samples of the dirt in them. 
            If it comes up with an off-the-charts reading for mercury, well, that’s a Corps of Discovery pit toilet — and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
            
              THESE PILLS WERE the pride and joy of Dr. Benjamin Rush, one of the Founding Fathers and a signer of the Declaration of Independence. Rush was also the man President Thomas Jefferson considered the finest physician in the republic. 
            In that opinion, Jefferson was probably alone, or at least in a small minority. Dr. Rush’s style of “heroic medicine” had caused his star to fall quite a bit by this time — especially after the Philadelphia yellow fever epidemic of 1793, when his patients died at a noticeably higher rate than untreated sufferers. 
            At the time, of course, very little was known about how the human body worked. Physicians were basically theorists, who made educated guesses and did their best. 
            The problem was, the education on which those educated guesses were based varied pretty wildly depending on what school you came from. Homeopathic physicians theorized that giving patients a tiny amount of something that mimicked their symptoms would stimulate the body to cure itself. Eclectic physicians sought cures from herbs and folk remedies. Hydropathic physicians believed hot and cold water, applied externally or internally, was all that was needed. 
            Dr. Rush wasn’t from one of these schools. He was from the school of mainstream medicine — also known as allopathic medicine (although that term is a perjorative today).
            Allopathic medical theory, in the early 1800s, dated from the second century A.D., courtesy of a Roman doctor named Galen. 
            Galen theorized that the human body ran on four different fluids, which he called “humours”: Blood, phlegm, yellow bile, and black bile. All disease, he claimed, stemmed from an imbalance in these humours.
            Thus, too much blood caused inflammation and fever; the solution was to let a pint or two out. Too much bile caused problems like constipation; the solution was to administer a purgative and let the patient blow out some black bile into a handy chamber-pot, or vomit up some yellow bile — or both.
            These interventions sometimes helped, but most of the time they had little or no good effect. So by Rush’s time, a number of physicians were going on the theory that what was needed was a doubling-down on their theory — in a style of practice that they called “heroic medicine.”
            If a sensible dose of a purgative didn’t get a patient’s bile back in balance, a “heroic” dose might. If a cup or two of blood didn’t get the fever down, four or five surely would.          
          
             
            
              [EDITOR'S NOTE: In "reader view" some phone browsers truncate the story here, algorithmically "assuming" that the second column is advertising. (Most browsers do not recognize this page as mobile-device-friendly; it is designed to be browsed on any device without reflowing, by taking advantage of the "double-tap-to-zoom" function.) If the story ends here on your device, you may have to exit "reader view" (sometimes labeled "Make This Page Mobile Friendly Mode") to continue reading. We apologize for the inconvenience.]
            
            —(Jump to top of next column)—
    

        
           
            A sketch of Fort Clatsop as it would have appeared in 1805. (Image: Oregon Historical Society)
          
        
        
          You can imagine what the result of this philosophy was, when applied to an actual sick person.
        “Some people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,” says physician and historian David Peck. “I think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.”
        In lieu of a trained physician, the Corps of Discovery’s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing “heat,” opium products for relieving pain and inducing sleep — and purgatives.
        Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called “Dr. Rush’s Bilious Pills.” They contained about 10 grains of calomel and 10 to 15 grains of jalap.
        
           
              This recipe for a milder version of Rush's Bilious Pills comes from the National Formulary in 1945. This image appears in the Lewis and Clark Fort Mandan Foundation's Web site, at which there's a lot more information about the ingredients in this compound. Mercury was still being used as an internal medicine in the 1960s and as a topical antiseptic (chiefly as Mercurochrome) into the 1990s.
            
          
        
        Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power. 
        And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don’t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance. 
        Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis “sporting house” before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.
        When symptoms broke out, the patient would be dosed with “thunder clappers” and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself. 
        And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative “on the regular” (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.
        And this low-fiber diet had predictable results.
        It had another result, too, which was less predictable — although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given “bilious pill” gets blown out post-haste in the ensuing “purge.”
        Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.
        So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way — a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.        
        
          
            (Sources: Class lecture in History of American Medicine, October 2009, Univ. of Oregon, by Dr. James Mohr; Or Perish in the Attempt: Wilderness Medicine in the Lewis and Clark Expedition, a book by David J. Peck published in 2002 by Farcountry Press; “Following Lewis and Clark’s Trail of Mercurial Laxatives,” an article by Marisa Sloan published in the Jan. 29, 2022, issue of Discover Magazine.)
          TAGS: #Archaeology #HeroicMedicine #DavidPeck #Jalap #Syphilis #CorpsOfDiscovery #BenjaminRush #Humours #Medicine #FrontierDoctors #Galen #FortClatsop #Calomel #MercuryPoisoning #Thunderclappers #Constipation #DrJamesMohr #OregonTrail #DrRush's #BiliousPills #Bile #COLUMBIAgorge #CLATSOPcounty
        

		  
          

          
          

      

     
    
		
		    Background image is a postcard, a hand-tinted photograph of Crown Point and the Columbia Gorge Scenic Highway. Here is a link to the Offbeat Oregon article about it, from 2024.
		    Scroll sideways to move the article aside for a better view.
		    
		    Looking for more?
            On our Sortable Master Directory you can search by keywords, locations, or historical timeframes. Hover your mouse over the headlines to read the first few paragraphs (or a summary of the story) in a pop-up box.
            ... or ...		    
		    Home
		    
	      

    
    
  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Linux version of the Procmon Sysinternals tool]]></title>
            <link>https://github.com/microsoft/ProcMon-for-Linux</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid>
            <description><![CDATA[A Linux version of the Procmon Sysinternals tool. Contribute to microsoft/ProcMon-for-Linux development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Process Monitor for Linux (Preview) 
Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows.  Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.

Installation & Usage
Requirements

OS: Ubuntu 18.04 lts
cmake >= 3.14 (build-time only)
libsqlite3-dev >= 3.22 (build-time only)

Install Procmon
Please see installation instructions here.
Build Procmon
Please see build instructions here.
Usage
Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file
Examples
The following traces all processes and syscalls on the system:
sudo procmon
The following traces processes with process id 10 and 20:
sudo procmon -p 10,20
The following traces process 20 only syscalls read, write and open at:
sudo procmon -p 20 -e read,write,openat
The following traces process 35 and opens Procmon in headless mode to output all captured events to file procmon.db:
sudo procmon -p 35 -c procmon.db
The following opens a Procmon tracefile, procmon.db, within the Procmon TUI:
sudo procmon -f procmon.db
Feedback

Ask a question on Stack Overflow (tag with ProcmonForLinux)
Request a new feature on GitHub
Vote for popular feature requests
File a bug in GitHub Issues

Contributing
If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:

How to build and run from the source
The development workflow, including debugging and running tests
Coding Guidelines
Submitting pull requests

Please see also our Code of Conduct.
License
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We should have the ability to run any code we want on hardware we own]]></title>
            <link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid>
            <description><![CDATA[Refuting the common and flawed argument of]]></description>
            <content:encoded><![CDATA[
  
  Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: “I should be able to run whatever code I want on hardware I own”. I agree entirely with this point, but within the context of this discussion it’s moot.


  “I should be able to run whatever code I want on hardware I own”


When Google restricts your ability to install certain applications they aren’t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. It’s through this control of the operating system that Google is exerting control, not at the hardware layer. You often don’t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Apple’s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.

You shouldn’t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldn’t be of the restrictions in place in the operating systems they provide – rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sony’s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.


  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[New Ruby Curl bindings with Fiber native support]]></title>
            <link>https://github.com/taf2/curb/blob/master/ChangeLog.md</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086276</guid>
            <description><![CDATA[Ruby bindings for libcurl. Contribute to taf2/curb development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[
          
            


                
      

      
          

                
                    
  
      
      
          
            GitHub Copilot
          
        Write better code with AI
      

    


                    
  
      
      
          
            GitHub Spark
              
                New
              
          
        Build and deploy intelligent apps
      

    


                    
  
      
      
          
            GitHub Models
              
                New
              
          
        Manage and compare prompts
      

    


                    
  
      
      
          
            GitHub Advanced Security
          
        Find and fix vulnerabilities
      

    


                    
  
      
      
          
            Actions
          
        Automate any workflow
      

    


                    
                
              
          

                
                    
  
      
      
          
            Codespaces
          
        Instant dev environments
      

    


                    
  
      
      
          
            Issues
          
        Plan and track work
      

    


                    
  
      
      
          
            Code Review
          
        Manage code changes
      

    


                    
  
      
      
          
            Discussions
          
        Collaborate outside of code
      

    


                    
  
      
      
          
            Code Search
          
        Find more, search less
      

    


                
              
          

      



                
      

      



                
      

      
                    Explore
                    
  
      Learning Pathways

    


                    
  
      Events & Webinars

    


                    
  
      Ebooks & Whitepapers

    


                    
  
      Customer Stories

    


                    
  
      Partners

    


                    
  
      Executive Insights

    


                
              



                
      

      
              

                
                    
  
      
      
          
            GitHub Sponsors
          
        Fund open source developers
      

    


                
              
              

                
                    
  
      
      
          
            The ReadME Project
          
        GitHub community articles
      

    


                
              
              
          



                
      

      

                
                    
  
      
      
          
            Enterprise platform
          
        AI-powered developer platform
      

    


                
              



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What to do with C++ modules?]]></title>
            <link>https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086210</guid>
        </item>
        <item>
            <title><![CDATA[Eternal Struggle]]></title>
            <link>https://yoavg.github.io/eternal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid>
            <description><![CDATA[change background]]></description>
            <content:encoded><![CDATA[
  
  
  change background



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use One Big Server (2022)]]></title>
            <link>https://specbranch.com/posts/one-big-server/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45085029</guid>
            <description><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system …]]></description>
            <content:encoded><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system architecture is worth the developer time and
cost overheads.  By thinking about the real operational considerations of our systems, we can
get some insight into whether we actually need distributed systems for most things.
We have all gotten so familiar with virtualization and abstractions between our software
and the servers that run it.  These days, "serverless" computing is all the rage, and even
"bare metal" is a class of virtual machine.  However, every piece of software runs on a
server.  Since we now live in a world of virtualization, most of these servers are a lot
bigger and a lot cheaper than we actually think.
Meet Your Server



This is a picture of a server used by Microsoft Azure with AMD CPUs.  Starting from the left,
the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes
that the copper tubes are attached to are heat exchangers on each CPU.  The CPUs are AMD's
third generation server CPU, each of which has the following specifications:

64 cores
128 threads
~2-2.5 GHz clock
Cores capable of 4-6 instructions per clock cycle
256 MB of L3 cache

In total, this server has 128 cores with 256 simultaneous threads.  With all of the cores working
together, this server is capable of 4 TFLOPs of peak double precision computing performance. This
server would sit at the top of the top500 supercomputer list in early 2000. It would take until
2007 for this server to leave the top500 list.  Each CPU core is substantially more powerful than a
single core from 10 years ago, and boasts a much wider computation pipeline.
Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket.  The largest
capacity "cost effective" DIMMs today are 64 GB.  Populated cost-efficiently, this server can hold
1 TB of memory.  Populated with specialized high-capacity DIMMs (which are generally slower
than the smaller DIMMs), this server supports up to 8 TB of memory total.  At DDR4-3200, with
a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across
all of its cores.
In terms of I/O, each CPU offers 64 PCIe gen 4 lanes.  With 128 PCIe lanes total, this server is
capable of supporting 30 NVMe SSDs plus a network card.  Typical configurations you can buy will
offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is
in the top right, the network card.  This server is likely equipped with a 50-100 Gbps network
connection.
The Capabilities of One Server
One server today is capable of:

Serving video files at 400 Gbps (now 800 Gbps)
1 million IOPS on a NoSQL database
70k IOPS in PostgreSQL
500k requests per second to nginx
Compiling the linux kernel in 20 seconds
Rendering 4k video with x264 at 75 FPS

Among other things.  There are a lot of public benchmarks these days, and if you know how your
service behaves, you can probably find a similar benchmark.
The Cost of One Server
In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications
to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth
for $1,318/month.
Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores
and 128 GB of RAM for about €140.00/month.  This is a smaller server than the one from OVHCloud
(1/4 the size), but it gives you some idea of the price spread between hosting providers.
In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps
of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour
in the US East region.  This comes out to $6,055/month.  The cloud premium is real!
A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs,
SSDs, and support contracts), can be purchased from the Dell website for about $40,000.  However,
if you are going to spend this much on a server, you should probably chat with a salesperson to
make sure you are getting the best deal you can.  You will also need to pay to host this server
and connect it to a network, though.
In comparison, buying servers takes about 8 months to break even compared to using cloud servers,
and 30 months to break even compared to renting.  Of course, buying servers has a lot of drawbacks,
and so does renting, so going forward, we will think a little bit about the "cloud premium" and
whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the
cloud companies want you to pay").
Thinking about the Cloud
The "cloud era" began in earnest around 2010.  At the time, the state of the art CPU was an
8-core Intel Nehalem CPU.  Hyperthreading had just begun, so that 8-core CPU offered a
whopping 16 threads.  Hardware acceleration was about to arrive for AES encryption, and
vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a
whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to
offer a 3 TB hard drive.  Each core offered 4 FLOPs per cycle, meaning that your 8-core
server running at 2.5 GHz offered a blazing fast 80 GFLOPs.
The boom in distributed computing rode on this wave: if you wanted to do anything that
involved retrieval of data, you needed a lot of disks to get the storage throughput you want.
If you wanted to do large computations, you generally needed a lot of CPUs. This meant that
you needed to coordinate between a lot of CPUs to get most things done.
Since that time began, the size of servers has increased a lot, and SSDs have increased available
IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased
much, and we still use virtualized drives that perform more like hard drives than SSDs (although
this gap is closing).
One Server (Plus a Backup) is Usually Plenty
If you are doing anything short of video streaming, and you have under 10k QPS, one server
will generally be fine for most web services.  For really simple services, one server could
even make it to a million QPS or so.  Very few web services get this much traffic - if you
have one, you know about it.  Even if you're serving video, running only one server for your
control plane is very reasonable.  A benchmark can help you determine where you are.
Alternatively, you can use common benchmarks of similar applications, or
tables of common performance numbers to estimate how big of a
machine you might need.
Tall is Better than Wide
When you need a cluster of computers, if one server is not enough, using fewer larger servers
will often be better than using a large fleet of small machines.  There is non-zero overhead
to coordinate a cluster, and that overhead is frequently O(n) on each server.  To reduce this
overhead, you should generally prefer to use a few large servers than to use many small servers.
In the case of things like serverless computing, where you allocate tiny short-lived containers,
this overhead accounts for a large fraction of the cost of use.  On the other extreme end,
coordinating a cluster of one computer is trivial.
Big Servers and Availability
The big drawback of using a single big server is availability.  Your server is going to need
downtime, and it is going to break.  Running a primary and a backup server is usually enough,
keeping them in different datacenters.  A 2x2 configuration should appease the truly paranoid: two
servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will
give you a lot of redundancy.  If you want a third backup deployment, you can often make that
smaller than your primary and secondary.
However, you may still have to be concerned about correlated hardware failures.  Hard drives
(and now SSDs) have been known to occasionally have correlated failures: if you see one disk
fail, you are a lot more likely to see a second failure before getting back up if your disks
are from the same manufacturing batch.  Services like Backblaze overcome this by using many
different models of disks from multiple manufacturers.  Hacker news learned this the hard way
recently when the primary and backup server went down at the same time.
If you are using a hosting provider which rents pre-built servers, it is prudent to rent two
different types of servers in each of your primary and backup datacenters.  This should avoid
almost every failure mode present in modern systems.
Use the Cloud, but don't be too Cloudy
A combination of availability and ease of use is one of the big reasons why I (and most other
engineers) like cloud computers.  Yes, you pay a significant premium to rent the machines, but
your cloud provider has so much experience building servers that you don't even see most failures,
and for the other failures, you can get back up and running really quickly by renting a new
machine in their nearly-limitless pool of compute.  It is their job to make sure that you don't
experience downtime, and while they don't always do it perfectly, they are pretty good at it.
Hosting providers who are willing to rent you a server are a cheaper alternative to cloud
providers, but these providers can sometimes have poor quality and some of them don't understand
things like network provisioning and correlated hardware failures. Also, moving from one rented
server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a
price premium for a good reason.
However, when you deal with clouds, your salespeople will generally push you towards
"cloud-native" architecture.  These are things like microservices in auto-scaling VM groups with
legions of load balancers between them, and vendor-lock-in-enhancing products like serverless
computing and managed high-availability databases.  There is a good reason that cloud
salespeople are the ones pushing "cloud architecture" - it's better for them!
The conventional wisdom is that using cloud architecture is good because it lets you scale up
effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people
is not one of them: most services can serve millions of people at a time with one server, and
will never give you a surprise five-figure bill.
Why Should I Pay for Peak Load?
One common criticism of the "one big server" approach is that you now have to pay for your peak
usage instead of paying as you go for what you use.  Thus, serverless computing or fleets of
microservice VMs more closely align your costs with your profit.
Unfortunately, since all of your services run on servers (whether you like it or not), someone
in that supply chain is charging you based on their peak load.  Part of the "cloud premium" for
load balancers, serverless computing, and small VMs is based on how much extra capacity your
cloud provider needs to build in order to handle their peak load.  You're paying for someone's
peak load anyway!
This means that if your workload is exceptionally bursty - like a simulation that needs
to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if
your workload is not so bursty, you will often have a cheaper system (and an easier time building
it) if you go for few large servers.  If your cloud provider's usage is more bursty than yours,
you are going to pay that premium for no benefit.
This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM
24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with
a salesperson if you are big enough.
Generally, the burstier your workload is, the more cloudy your architecture should be.
How Much Does it Cost to be Cloudy?
Being cloudy is expensive.  Generally, I would anticipate a 5-30x price premium depending on what
you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and
30.
Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM.  I
am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above.
Large ARM servers and serverless ARM compute are both cheaper.
Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:


1k QPS is 60k queries per minute, or 3.6M queries per hour


Each query here gets 0.768 GB-seconds of RAM (amortized)


Replacing this server would cost about $46/hour using serverless computing


The price premium for serverless computing over the instance is a factor of 5.5.  If you can keep
that server over 20% utilization, using the server will be cheaper than using serverless computing.
This is before any form of savings plan you can apply to that server - if you can rent those big
servers from the spot market or if you compare to the price you can get with a 1-year contract,
the price premium is even higher.
If you compare to the OVHCloud rental price for the same server, the price premium of buying your
compute through AWS lambda is a factor of 25
If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you
should prefer the hosting provider if you can keep the server operating at 5% capacity!
Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k
QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x
(or 25x) premium. Of course, you should scale the size of the server to fit your application.
Common Objections to One Big Server
If you propose using the one big server approach, you will often get pushback from people who are
more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns.  Use your
judgment when you think about it, but most people vastly underestimate how much "cloud
architecture" actually costs compared to the underlying compute.  Here are some common objections.
But if I use Cloud Architecture, I Don't Have to Hire Sysadmins
Yes you do.  They are just now called "Cloud Ops" and are under a different manager. Also, their
ability to read the arcane documentation that comes from cloud companies and keep up  with the
corresponding torrents of updates and deprecations makes them 5x more expensive than system
administrators.
But if I use Cloud Architecture, I Don't Have to Do Security Updates
Yes you do.  You may have to do fewer of them, but the ones you don't have to do are the easy ones
to automate.  You are still going to share in the pain of auditing libraries you use, and making
sure that all of your configurations are secure.
But if I use Cloud Architecture, I Don't Have to Worry About it Going Down
The "high availability" architectures you get from using cloudy constructs and microservices just
about make up for the fragility they add due to complexity.  At this point, if you use two
different cloud regions or two cloud providers, you can generally assume that is good enough to
avoid your service going down.  However, cloud providers have often had global outages in the past,
and there is no reason to assume that cloud datacenters will be down any less often than your
individual servers.
Remember that we are trying to prevent correlated failures.  Cloud datacenters have a lot of
parts that can fail in correlated ways.  Hosting providers have many fewer of these parts.
Similarly, complex cloud services, like managed databases, have more failure modes than simple
ones (VMs).
But I can Develop More Quickly if I use Cloud Architecture
Then do it, and just keep an eye on the bill and think about when it's worth it to switch.  This
is probably the strongest argument in favor of using cloudy constructs.  However, if you don't
think about it as you grow, you will likely end up burning a lot of money on your cloudy
architecture long past the time to switch to something more boring.
My Workload is Really Bursty
Cloud away.  That is a great reason to use things like serverless computing.  One of the big
benefits of cloud architecture constructs is that the scale down really well.  If your workload
goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud
architecture probably works really well for you.
What about CDNs?
It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings,
with one big server.  This is also true of other systems that need to be distributed, like backups.
Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of
thing to buy rather than build.
A Note On Microservices and Monoliths
Thinking about "one big server" naturally lines up with thinking about monolithic architectures.
However, you don't need to use a monolith to use one server.  You can run many containers on one
big server, with one microservice per container.  However, microservice architectures in general
add a lot of overhead to a system for dubious gain when you are running on one big server.
Conclusions
When you experience growing pains, and get close to the limits of your current servers, today's
conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture
that gives you horizontal scaling "for free."  It is often easier and more efficient to scale
vertically instead.  Using one big server is comparatively cheap, keeps your overheads at a
minimum, and actually has a pretty good availability story if you are careful to prevent correlated
hardware failures.  It's not glamorous and it won't help your resume, but one big server will serve
you well.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[When the sun will literally set on what's left of the British Empire]]></title>
            <link>https://oikofuge.com/sun-sets-on-british-empire/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084913</guid>
            <description><![CDATA[A while ago I treated you to a dissertation entitled “Does The Sun Set On The British Empire?”, and concluded that it doesn’t. The UK’s widely scattered overseas territories, sparse tho…]]></description>
            <content:encoded><![CDATA[
		
Click to enlarge


A while ago I treated you to a dissertation entitled “Does The Sun Set On The British Empire?”, and concluded that it doesn’t. The UK’s widely scattered overseas territories, sparse though they are, mean that the sun is still always shining, somewhere in the world, over British territory.



The most important territories in maintaining this late-empire sunlight are the Pitcairn Islands, in the Pacific, and the British Indian Ocean Territory, in the Indian Ocean. To illustrate that, I offered the sunlight chart below, showing how Pitcairn and BIOT catch the sunlight when it’s dark in the UK.



Click to enlarge


In fact, as my map at the head of this post shows, BIOT is pivotal. There, I’ve plotted the distribution of light and darkness, across the globe, at 02:15 Greenwich Mean Time, during the June solstice of 2024.*



And here’s the situation at the December solstice:



Click to enlarge


Just after the sun sets in Pitcairn, it’s dark over every British territory except BIOT.



I’m revisiting the situation because the UK government has announced plans to hand over sovereignty of the Chagos Archipelago, which houses BIOT, to Mauritius. The announcement was made in October 2024, but the original agreement has now been contested by a new government in Mauritius. And the situation is further complicated by the fact that BIOT houses a large US military base on the island of Diego Garcia, so the new Trump administration also has a say in the process. (Meanwhile, the unfortunate Chagossians, evicted from their homeland in 1968 to make way for the military base, have so far been given no voice in the negotiations.)



The current proposal suggests that the military base would be maintained under a long-term lease agreement, in which case British sovereignty would be lost, and BIOT would cease to exist. At that point, the role of easternmost British territory would fall to the Sovereign Base Areas (SBAs), in Cyprus.



The SBAs are worth a few paragraphs, both because they’re relatively obscure, and because their existence, as sovereign military territories, perhaps has some slight relevance to how the situation on Diego Garcia might play out, should the Trump administration raise strong objections to the current plan.



The SBAs came into existence when Cyprus gained its independence from the UK in 1960. Under the Treaty of Establishment, the UK retained sovereignty over about 250 square kilometres of the island, in two separate areas—the Western Sovereign Base Area of Akrotiri, and the Eastern Sovereign Base Area of Dhekelia. These have extremely complicated boundaries, designed to avoid Cypriot settlements while including British military establishments. The Eastern SBA contains three Cypriot enclaves—the towns of Ormideia and Xylotymbou, and the area surrounding the Dhekelia power station (which is crossed by a British road). It also features a long northward extension along the road to the village of Ayios Nikolaos, which now houses a signals intelligence unit.



And the whole border situation became even more complicated after the Turkish invasion of Cyprus in 1974, which has left the island traversed by a UN buffer zone. British territory, including the Ayios Nikolaos road, forms part of the buffer zone. Elsewhere, the Turkish-controlled town of Kokkina has its very own buffer zone. Here’s an overview map, followed by some detail of the SBAs:







(Interestingly, the British military settlements within the SBAs are referred to as cantonments, a military term which, to me at least, has something of a colonial ring to it, given its association with British rule in India.)



The relevance, here, to the current situation of Diego Garcia, is because the UK government made plans to hand the SBAs back to Cyprus in 1974, but were persuaded to retain sovereignty by the USA, which valued access to signals intelligence in the Eastern Mediterranean, as well as a convenient location from which to fly, among other things, U2 spy planes. The difference, of course, is that the Cypriot government appears to have been compliant with that arrangement, whereas it seems unlikely, at time of writing, that the Mauritians would agree to such a deal.



We’ll see how it goes. Meanwhile, I’ve plotted another sunrise/sunset graph, showing how sunlight is handed off between the two key players in the absence of BIOT: 



Click to enlarge


(For my sunlight calculation, I’ve plugged in the latitude and longitude of the easternmost part of the Eastern SBA—Ayios Nikolaos.)



It’s close—in June there’s less than an hour when it’s dark in both Pitcairn and the SBAs. But, if BIOT goes, when the sun sets on Pitcairn, it will also set on (what’s left of) the British Empire.







* I haven’t plotted British Antarctic Territory, because territorial claims in Antarctica are in abeyance under the Antarctic Treaty.





	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: VibeFlow (YC S25) – Web app generator with visual, editable workflows]]></title>
            <link>https://news.ycombinator.com/item?id=45084759</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084759</guid>
            <description><![CDATA[I want to like this and dig into it as someone who has recently used Lovable and Base44 (and been using Bubble for a while), but the YouTube ‘demo’ video is really weak.]]></description>
            <content:encoded><![CDATA[
I want to like this and dig into it as someone who has recently used Lovable and Base44 (and been using Bubble for a while), but the YouTube ‘demo’ video is really weak.The pace is too fast and you spend barely any time showing off your visual workflow feature, which according to your description is your differentiator.I would strongly recommend using some of your YC money to have a professional recreate that demo and show off what makes you unique. Even if it goes longer than two minutes - if I’m interested I’ll keep watching.I’ll still try it out because I’m a sucker for trying out new vibecoding tools, but you’re not doing yourself any favors with that video…
Thanks a lot for the feedback. The video was meant as a very spontaneous ‘as it is’ showcase, but we’ll definitely make new demos that go deeper into the editor!
> recently used Lovable and Base44Are you happy with either product? I tried them earlier in the year, and it was also really slow to make changes. I felt like they got stuck after a bit, too.It's a neat concept, but I feel like they're expensive templates. I'd honestly prefer a template gallery with a smooth and fast editing UI.
In general to me it makes a lot of sense to lean much more into "templates" (I'm sure lovable etc already do it, because it's also a nice way to save money). And it's much easier to at least guarantee some basic security when it comes to auth, payments, db setup etc. Of course you can shoot yourself in the foot right after that.
Totally agree, security is a big point. It’s hard to trust LLMs on security, which is why we aim to make ‘white box’ backends
Quite like the positioning of "this is the backend to your lovable ui", probably how chef (the vibe coding tool from the makers of convex) should have positioned it. (and kind of do).
I like this. Any chance you'll be bringing similar tactility (is that a word) to the frontend? Granular changes to components via prompts leaves a lot to be desired.
Yes, more or less. I've been tinkering with what this might look like this summer, but haven't found a good solution. Seeing your "exploded view" of the backend resonated on that front.But instead of the nodes representing steps in a workflow, you'd have screens/views as nodes. Now, how the user would handle layout and components at that level, I'm not sure, but that's directionally what I'm asking about.I hope that makes (a little bit of) sense.
Check out WeWeb, it gives you an AI-assisted editor that lets you do exactly what you're looking for.
This looks great! Can I export my end code / app and host it elsewhere easily? Where else would easily be able to host it?
They say they use Convex for the backend, which means you could in principle run it on your own account or go through the hoops of self hosting convex infra
I've been playing around with vibeflow for a while, it's impressive how fast you can go from a prompt to a working full stack app. The visual workflow editor is a game changer.
I built a small url shortener and also experimented with a map‑based mood tracker. what stood out to me is how quickly I could go from a prompt to a working frontend + backend without boilerplate.For me, the most useful next nodes would be:
1) auth 
2) stripe
3) file upload
4) convex action nodes (for more complex workflows)
I think the evolution of vibe coding tool is definitely the editor. Having a black box with no way to maintain it is an absolute liability.That's why I think app generators must be a good editor before being able to generate anything. It seems you went this way with the cool node interface.I'm doing the same thing with https://luna-park.app, but for fullstack apps.
Well I am/was building something that looks a lot like this, a shame I never applied to YC, wondering now if I should apply to other funds now so I can continue working on it, the prototype is ready so I have the main part figured out.
A question, perhaps, could you give some tips to pitch this specifically, just for incubators, based on your experience?
Well all incubators ask the same thing (and search for the same profile). Just blast it to every incubator you find. Take some time write a nice YC application. There are tools like acceleratorfiller.xyz to send them to multiple accelerators.
BTW (It's my company)
that what everyone always says. But i think pitcing to a incubator is actually a good way to focus your idea. Anyways what is 2 hours in the grand scheme of things
Will have to try this later, the YT video looks promising. Found tools similar to this promising to create early mockups or other pre-prototypes when developing products.
Glad to hear that. We want to make it as logical and white box as possible. Have you tried adding custom behavior after the first generation?
Congrats! Doesn't replit have an integrated database as well? Lovable has supabase, and I'm pretty sure Base44 as well, plus other agent integrations.
Thanks! Yes, Replit has KV store and managed Postgres, Lovable uses Supabase (requires manual setup). Base44 doesn't have a manual setup but has a black box backend.
In VibeFlow:
- no manual setup required
- low code backend editor n8n style - no black box anymore
- everything you do in the backend is code that you ownIt's not just about databases, think about all the users currently using n8n with Lovable separately, without even owning the full stack
I tried this but kept getting errors.
I asked it to build a TODO list that searches the internet to "augment" my todo list with advice
Great question! We chose Convex for multiple reasons:– We spin up isolated projects for each user. Convex handles this seamlessly with zero manual setup, while Supabase/Firebase have limitations and manual configuration needed
– We abstract backend logic as visual nodes, so Convex's modularity makes it logical to find the right granularity for workflow representation.
– Everything is reactive, so UIs and workflows stay in sync without bolting on listeners
– Everything is end-to-end TypeScript with transactions by default, so generated code is predictable and maintainable
congrats on the launch, lots of competition in this space. (leap.new, replit etc). Even convex has their own app-builder.
thank you! There’s definitely a lot happening in this space, our focus is on making backends secure, robust, and understandable rather than just black-box codegen
Don't get me wrong, I wish your start-up all the best, but this particular application seems so stereotypical by current standards. It's at least four buzzwords combined into one "idea". As someone who has never tried to apply, I wonder how difficult it was to get through Y Combinator's selection process.
I worry that almost all the 2025 startups I've seen are AI app builders. Where are the novel new applications? I get that codegen is currently one area where AI does well, but it also feels like we're struggling with other use cases.
I’ve spent an enormous amount of time with practically every AI model out there, from coding to image-gen to video-genThe tech is still simply too hard to use effectively for the vast majority of lay people, especially for anything beyond a cool product demoSome of it is due to quality of the models, some of it due t quality of the toolingPrompt engineering is still a skill and that’s beyond what a casual user can figure out
My optimism says the good new stuff is coming slowly because people who care about their craft and taking things slowly aren’t in any rush to get to market.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Jujutsu for everyone]]></title>
            <link>https://jj-for-everyone.github.io/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45083952</guid>
            <description><![CDATA[A Jujutsu tutorial that requires no previous experience with Git or other version control systems.]]></description>
            <content:encoded><![CDATA[
    
            Keyboard shortcuts
            
                Press ← or → to navigate between chapters
                Press S or / to search in the book
                Press ? to show this help
                Press Esc to hide this help
            
        
    
        
        

        
        

        

        
        

        
            
            
            
            
        

        

            
                    
                        Introduction
This is a tutorial for the Jujutsu version control system.
It requires no previous experience with Git or any other version control system.
At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu.
This tutorial is my attempt to fill the void of beginner learning material for Jujutsu.
If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.
This tutorial requires you to work in the terminal.
Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet.
The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac.
If you're on Windows (and can't switch to Linux), consider using WSL.
How to read this tutorial
The tutorial is split into levels, which are the top-level chapters in the sidebar.
The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned.
Once you're comfortable with those skills, come back for the next level.
There is one exception to this:
If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.
Here's an overview of the planned levels:
LevelDescription
1The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.
2The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.
3Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.
4History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.
5Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.
6Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.


Only a few levels are complete right now, the rest are on the way.
Reset your progress
Throughout the tutorial, you will build an example repository.
Later chapters depend on the state of previous ones.
Losing the state of the example repo can therefore block you from making smooth progress.
This might happen for several reasons:

You use the example repo for practice and experimentation.
You switch to a different computer or reinstall the OS.
You intentionally delete it to clean up your home directory.
The tutorial is updated significantly while you're taking a break.

To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter.
To identify the chapter you want to continue with, the script expects a keyword as an argument.
Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.



Always be careful when executing scripts from the internet!




The script is not complicated, you can verify that it's not doing anything malicious.
Basically, it's just the list of commands I tell you to run manually.
For convenience, it's included in the expandable text box below.
You can also download the script here and then execute it locally once you have inspected it.





Source of reset script




#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "✅✅✅ Reset script completed successfully! ✅✅✅"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj > /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" > README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" >> README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" > hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" > README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" >> README.md

echo "*.tar.gz" > .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i < 10; i = i + 1):
    print('Hello, world!')" > hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" > hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" > hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &> /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1



Stay up to date
Both this tutorial and Jujutsu are still evolving.
In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo.
You will be notified of important changes:

A new level becomes available.
An existing level is changed significantly.

I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content.
I consider this tutorial up-to-date with the latest version of Jujutsu (0.32) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.
You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".

Help make this tutorial better
If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner.
If you have general suggestions for improvement, please open an issue.
I am also very interested in experience reports, for example:

Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?
Was there a section that wasn't explained clearly?
(If you didn't understand something, it's probably the tutorial's fault, not yours!)
Did you complete a level but didn't feel like you had the skills that were promised in the level overview?
Is there something missing that's not being taught but should?
Do you feel like the content could be structured better?

Thank you for helping me improve this tutorial!
What is version control and why should you use it?
I will assume you're using version control for software development, but it can be used for other things as well.
For example, authoring professionally formatted documents with tools like Typst.
The source of this tutorial is stored in version control too!
What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time.
You don't want to lose any of it and you want to be able to go back to previous states of your work.
Often, several people need to work on the project at the same time.
A general-purpose backup solution can keep a few copies of your files around.
A graphical document editor can allow multiple people to edit the text simultaneously.
But sometimes, you need a sharper knife.
Jujutsu is the sharpest knife available.
Why Jujutsu instead of Git?
Git is by far the most commonly used VCS in the software development industry.
So why not use that?
Using the most popular thing has undeniable benefits.
There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc.
Why make life harder on yourself by using a lesser-known alternative?
Here's my elevator pitch:


Jujutsu is compatible with Git.
You're not actually losing anything by using Jujutsu.
You can work with it on any existing project that uses Git for version control without issues.
Tools that integrate with Git mostly work just as well with Jujutsu.


Jujutsu is easier to learn than Git.
(That is, assuming I did a decent job writing this tutorial.)
Git is known for its complicated, unintuitive user interface.
Jujutsu gives you all the functionality of Git with a lot less complexity.
Experienced users of Git usually don't care about this, because they've paid the price of learning Git already.
(I was one of these people once.)
But you care!


Jujutsu is more powerful than Git.
Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust.
Don't worry, you don't have to use that power right away.
But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back.
This is not a watered-down "we have Git at home" for slow learners!


Learning Jujutsu instead of Git as your first VCS does have some downsides:


When talking about version control with peers, they will likely use Git-centric vocabulary.
Jujutsu shares a lot of Git's concepts, but there are also differences.
Translating between the two in conversation can add some mental overhead.
(solution: convince your peers to use Jujutsu 😉)


Jujutsu is relatively new and doesn't cover 100% of the features of Git yet.
When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly.
Still, having to use two tools instead of one is slightly annoying.
I plan to teach such Git features in this tutorial in later levels.
The tutorial should be a one-stop-shop for all Jujutsu users.


The command line interface of Jujutsu is not yet stable.
That means in future versions of Jujutsu, some commands might work a little differently or be renamed.
I personally don't think this should scare you away.
Many people including me have used Jujutsu as a daily driver for a long time.
Whenever something did change, my reaction was usually:
"Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!"
Consider subscribing to GitHub releases of this tutorial.
You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.


Despite some downsides, I think the benefits are well worth it.

                    

                    
                        

                            
                                
                            

                        
                    
                

            

                    
                        
                    
            

        




        


        
        
        

        
        
        

        


    
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Survey: a third of senior developers say over half their code is AI-generated]]></title>
            <link>https://www.fastly.com/blog/senior-developers-ship-more-ai-code</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45083635</guid>
            <description><![CDATA[Fastly’s survey shows senior developers trust gen AI tools enough to ship 2.5x more AI code, while juniors stick to traditional coding and caution.]]></description>
            <content:encoded><![CDATA[Fastly’s July 2025 survey of 791 developers found a notable difference in how much AI-generated code is making it into production. About a third of senior developers (10+ years of experience) say over half their shipped code is AI-generated — nearly two and a half times the rate reported by junior developers (0–2 years of experience), at 13%.“AI will bench test code and find errors much faster than a human, repairing them seamlessly. This has been the case many times,” one senior developer said. A junior respondent noted the trade-offs: “It’s always hard when AI assumes what I’m doing and that’s not the case, so I have to go back and redo it myself.”Senior developers were also more likely to say they invest time fixing AI-generated code. Just under 30% of seniors reported editing AI output enough to offset most of the time savings, compared to 17% of juniors. Even so, 59% of seniors say AI tools help them ship faster overall, compared to 49% of juniors.Senior Developers Are More Optimistic About AI Saving TimeJust over 50% of junior developers say AI makes them moderately faster. By contrast, only 39% of more senior developers say the same. But senior devs are more likely to report significant speed gains: 26% say AI makes them a lot faster, double the 13% of junior devs who agree.One reason for this gap may be that senior developers are simply better equipped to catch and correct AI’s mistakes. They have the experience to recognize when code “looks right” but isn’t. That makes them more confident at using AI tools efficiently, even for high-stakes or business-critical code. By contrast, junior developers may not fully trust their ability to spot errors, which can make them more cautious about relying on AI, or more likely to avoid using it in production at all.That tracks with how much AI-generated code actually makes it into production. Among junior devs, just 13% say over half of their shipped code is AI-generated. By contrast, 32% of senior developers say the same, suggesting that more experienced engineers are not only using AI more aggressively, but are also trusting it more in production environments. This is surprising given growing concerns about “vibe coding” introducing vulnerabilities into applications. Perception vs. RealityNearly 1 in 3 developers (28%) say they frequently have to fix or edit AI-generated code enough that it offsets most of the time savings. Only 14% say they rarely need to make changes. And yet, over half of developers still feel faster with AI tools like Copilot, Gemini, or Claude.Fastly’s survey isn’t alone in calling AI productivity gains into question. A recent randomized controlled trial (RCT) of experienced open-source developers found something even more striking: when developers used AI tools, they took 19% longer to complete their tasks.This disconnect may come down to psychology. AI coding often feels smooth: code autocompletes with a few keystrokes. This gives the impression of momentum, but the early speed gains are often followed by cycles of editing, testing, and reworking that eat into any gains. This pattern is echoed both in conversations we've had with Fastly developers and in many of the comments we received in our survey.One respondent put it this way: “An AI coding tool like GitHub Copilot greatly helps my workflow by suggesting code snippets and even entire functions. However, it once generated a complex algorithm that seemed correct but contained a subtle bug, leading to several hours of debugging.”Another noted: “The AI tool saves time by using boilerplate code, but it also needs manual fixes for inefficiencies, which keep productivity in check.”Yet, AI still seems to improve developer job satisfaction. Nearly 80% of developers say AI tools make coding more enjoyable. For some, it’s about skipping grunt work. For others, it might be the dopamine rush of code on demand.“It helps me complete a task that I’m stuck with. It allows me to find the answers necessary to finish the task,” one survey respondent says.Enjoyment doesn’t equal efficiency, but in a profession wrestling with burnout and backlogs, that morale boost might still count for something.The Hidden Cost of AI CodingFastly’s survey also explored developer awareness of green coding—the practice of writing energy-efficient software— and the energy cost behind AI coding tools. The practice of green coding goes up sharply with experience. Just over 56% of junior developers say they actively consider energy use in their work, while nearly 80% among mid- and senior-level engineers consider this when coding. Developers are very aware of the environmental cost of AI tools: roughly two-thirds of developers across all experience levels said they know that these tools can carry a significant carbon footprint. Only a small minority (under 8% even at the most junior levels) were completely unaware. Altogether, the data suggests that sustainability is increasingly embedded in developer culture.MethodologyThis survey was conducted by Fastly from July 10 to July 14, 2025, with 791 professional developers. All respondents confirm that writing or reviewing code is a core part of their job. The survey is distributed in the US and quality-controlled for accuracy, though, as with all self-reported data, some bias is possible. ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[“This telegram must be closely paraphrased before being communicated to anyone”]]></title>
            <link>https://history.stackexchange.com/questions/79371/this-telegram-must-be-closely-paraphrased-before-being-communicated-to-anyone</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45082731</guid>
            <description><![CDATA[Some historical documents from WWII have a notice on them stating

This telegram must be closely paraphrased before being communicated to anyone.

The documents I've found were received by the United]]></description>
            <content:encoded><![CDATA[
It appears that it was US military communications doctrine to not send the exact same message twice using different encryption ("none" counting as one type of encryption), and the term of art for changing a message to avoid that was indeed "paraphrase".
I managed to dig up a US Army document on Cryptology from roughly that era that appears to discuss paraphrasing. The document in question is Department of the Army Technical Manual TM 32-220(pdf), dated 1950, titled "BASIC CRYPTOGRAPHY". It apparently supersedes previous documents TM-484 from March 1945 and TM 11-485 (alternative)(pdf) from June 1944. It would probably be more ideal to look at them, since they are closer to the time you are interested in, unfortunately I was not able to find TM-484 online.
Here's what this declassified manual had to say about "paraphrasing", from Chapter 7, in the section Fundamental Rules of Cryptographic Security, section 84, subsection b, rule 3 (titled "Text of messages")

(a) Never repeat in the clear the identical text of a message once
sent in cryptographic form, or repeat in cryptographic form  the text
of a message once sent in the clear. Anything which  will enable an
alert enemy to compare a given piece of plain  text with a cryptogram
that supposedly contains this plain  text is highly dangerous to the
safety of the cryptographic  system. Where information must be given
out for publicity,  or where information is handled by many persons,
the plain text version should be very carefully paraphrased before
distribution, to minimize the data an enemy might obtain  from an
accurate comparison of the cryptographic text with  the equivalent,
original plain text. To paraphrase a message  means to rewrite it so
as to change its original wording as  much as possible without
changing the meaning of the message. This is done by altering the
positions of sentences in  the message, by altering the positions of
subject, predicate,  and modifying phrases or clauses in the sentence,
and by  altering as much as possible the diction by the use of
synonyms and synonymous expressions. In this process, deletion  rather
than expansion of the wording of the message is  preferable, because
if an ordinary message is paraphrased  simply by expanding it along
its original lines, an expert can  easily reduce the paraphrased
message to its lowest terms, and  the resultant wording will be
practically the original message.  It is very important to eliminate
repeated words or proper  names, if at all possible, by the use of
carefully selected  pronouns; by the use of the words "former,"
"latter," "first-mentioned," "second-mentioned"; or by other means.
After  carefully paraphrasing, the message can be sent in the other
key or code.
(b) Never send the literal plain text or a paraphrased
version of  the plain text of a message which has been or will be
transmitted in cryptographed form except as specifically provided  in
appropriate regulations

(emphasis mine)
In fact the allies would have have known intimately about how this was possible, because this is one of the ways they ended up decrypting the stronger German Enigma cipher. Captured machines using simpler ciphers were used to break those simpler ciphers. The fact that the Germans were encrypting the exact same messages in both ciphers meant the allies could know (for those messages) what both the unencrypted and encrypted messages were, which allowed them to decrypt the stronger cyphers as well, or quickly figure out what today's code was.

Though Enigma had some cryptographic weaknesses, in practice it was
German procedural flaws, operator mistakes, failure to systematically
introduce changes in encipherment procedures, and Allied capture of
key tables and hardware that, during the war, enabled Allied
cryptologists to succeed.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why haven't quantum computers factored 21 yet?]]></title>
            <link>https://algassert.com/post/2500</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45082587</guid>
            <description><![CDATA[Craig Gidney's computer science blog]]></description>
            <content:encoded><![CDATA[

In 2001, quantum computers factored the number 15.
It’s now 2025, and quantum computers haven’t yet factored the number 21.
It’s sometimes claimed this is proof there’s been no progress in quantum computers.
But there’s actually a much more surprising reason 21 hasn’t been factored yet, which jumps out at you when contrasting the operations used to factor 15 and to factor 21.

The circuit (the series of quantum logic gates) that was run to factor 15 can be seen in Figure 1b of “Experimental realization of Shor’s quantum factoring algorithm using nuclear magnetic resonance”:


  


The important cost here is the number of entangling gates.
This factoring-15 circuit has 6 two-qubit entangling gates (a mix of CNOT and CPHASE gates).
It also has 2 Toffoli gates, which each decompose into 6 two-qubit entangling gates.
So there’s a total of 21 entangling gates in this circuit.

Now, for comparison, here is a circuit for factoring 21.
Sorry for rotating it, but I couldn’t get it to fit otherwise.
Try counting the Toffolis:



(Here’s an OPENQASM2 version of the circuit, so you can test it produces the right distribution if you’re inclined to do so.)

In case you lost count: this circuit has 191 cnot gates and 369 Toffoli gates, implying a total of 2405 entangling gates.
That’s 115x more entangling gates than the factoring-15 circuit.
The factoring-21 circuit is more than one hundred times more expensive than the factoring-15 circuit.

When I ask people to guess how many times larger the factoring-21 circuit is, compared to the factoring-15 circuit, there’s a tendency for them to assume it’s 25% larger.
Or maybe twice as large.
The fact that it’s two orders of magnitude more expensive is shocking.
So I’ll try to explain why it happens.

(Quick aside: the amount of optimization that has gone into this factoring-21 circuit is probably unrepresentative of what would be possible when factoring big numbers.
I think a more plausible amount of optimization would produce a circuit with 500x the cost of the factoring-15 circuit… but a 100x overhead is sufficient to make my point.
Regardless, special thanks to Noah Shutty for running expensive computer searches to find the conditional-multiplication-by-4-mod-21 subroutine used by this circuit.)

Where does the 100x come from?

A key background fact you need to understand is that the dominant cost of a quantum factoring circuit comes from doing a series of conditional modular multiplications under superposition.
To factor an $n$-bit number $N$, Shor’s algorithm will conditionally multiply an accumulator by $m_k = g^{2^k} \pmod{N}$ for each $k < 2n$ (where $g$ is a randomly chosen value coprime to $N$).
Sometimes people also worry about the frequency basis measurement at the end of the algorithm, which is crucial to the algorithm’s function, but from a cost perspective it’s irrelevant.
(It’s negligible due by an optimization called “qubit recycling”, which I also could have used to reduce the qubit count of the factoring-21 circuit, but in this post I’m just counting gates so meh).

There are three effects that conspire to make the factoring-15 multiplications substantially cheaper than the factoring-21 multiplications:


  All but two of the factoring-15 multiplications end up multiplying by 1.
  The first multiplication is always ~free, because its input is known to be 1.
  The one remaining factoring-15 multiplication can be implemented with only two CSWAPs.


Let’s consider the case where $g=2$.
In that case, when factoring 15, the constants to conditionally multiply by would be:

>>> print([pow(2, 2**k, 15) for k in range(8)])
[2, 4, 1, 1, 1, 1, 1, 1]


First, notice that the last six constants are 1.
Multiplications by 1 can be implemented by doing nothing.
So the factoring-15 circuit is only paying for 2 of the expected 8 multiplications.

Second, notice that the first conditional multiplication (by 2) will either leave the accumulator storing 1 (when its control is off) or storing 2 (when its control is on).
This can be achieved much more cheaply by performing a controlled xor of $1 \oplus 2 = 3$ into the accumulator.

Third, notice that the only remaining multiplication is a multiplication by 4.
Because 15 is one less than a power of 2, multiplying by 2 modulo 15 can be implemented using a circular shift.
A multiplication by 4 is just two multiplications by 2, so it can also be implemented by a circular shift.
This is a very rare property for a modular multiplication to have, and here it reduces what should be an expensive operation into a pair of conditional swaps.
(If you go back and look at the factoring-15 circuit at the top of the post, the 2 three-qubit gates are being used to implement these two conditional swaps.)

You may worry that these savings are specific to the choice of $g=2$ and $N=15$.
And they are in fact specific to $N=15$.
But they aren’t specific to $g=2$.
They occur for all possible choices of $g$ when factoring 15.

For contrast, let’s now consider what happens when factoring 21.
Using $g=2$, the multiplication constants would be:

>>>  print([pow(2, 2**k, 21) for k in range(10)])
[2, 4, 16, 4, 16, 4, 16, 4, 16, 4]


This is going to be a lot more expensive.

First, there’s no multiplications by 1, so the circuit has to pay for every multiplication instead of only a quarter.
That’s a ~4x relative cost blowup vs factoring 15.
Second, although the first-one’s-free trick does still apply, proportionally speaking it’s not as good.
It cheapens 10% of the multiplications rather than 50%.
That’s an extra ~1.8x cost blowup vs factoring 15.
Third, the multiplication by 4 and 16 can’t be implemented with two CSWAPs.
The best conditionally-multiply-by-4-mod-21 circuit that I know is the one being used in the diagram above, and it uses 41 Toffolis.
These more expensive multiplications add a final bonus ~20x cost blowup vs factoring 15.

(Aside: multiplication by 16 mod 21 is the inverse of multiplying by 4 mod 21, and the circuits are reversible, so multiplying by 16 uses the same number of Toffolis as multiplying by 4.)

These three factors (multiplying-by-one, first-one’s-free, and multiplying-by-swapping) explain the 100x blowup in cost of factoring 21, compared to factoring 15.
And this 100x increase in cost explains why no one has factored 21 with a quantum computer yet.



Another contributor to the huge time gap between factoring 15 and factoring 21 is that the 2001 factoring of 15 was done with an NMR quantum computer.
These computers were known to have inherent scaling issues, and in fact it’s debated whether NMR computers were even properly “quantum”.
If the 2001 NMR experiment doesn’t count, I think the actually-did-the-multiplications runner-up is a 2015 experiment done with an ion trap quantum computer (discussed by Scott Aaronson at the time).

Yet another contributor is the overhead of quantum error correction.
Performing 100x more gates requires 100x lower error, and the most plausible way of achieving that is error corection.
Error correction requires redundancy, and could easily add a 100x overhead on qubit count.
Accounting for this, I could argue that factoring 21 will be ten thousand times more expensive than factoring 15, rather than “merely” a hundred times more expensive.

There are papers that claim to have factored 21 with a quantum computer.
For example, here’s one from 2021.
But, as far as I know, all such experiments are guilty of using optimizations that imply the code generating the circuit had access to information equivalent to knowing the factors (as explained in “Pretending to factor large numbers on a quantum computer” by Smolin et al).
Basically: they don’t do the multiplications, because the multiplications are hard, but the multiplications are what make it factoring instead of simpler forms of period finding.
So I don’t count them.

There is unfortunately a trickle of bullshit results that claim to be quantum factoring demonstrations.
For example, I have a joke paper in this year’s sigbovik proceedings that cheats in a particularly silly way.
More seriously, I enjoyed “Replication of Quantum Factorisation Records with an 8-bit Home Computer, an Abacus, and a Dog” making fun of some recent egregious papers.
I also recommend Scott Aaronson’s post “Quantum computing motte-and-baileys”, which complains about papers that benchmark “variational” factoring techniques while ignoring the lack of any reason to expect them to scale.

Because of the large cost of quantum factoring numbers (that aren’t 15), factoring isn’t yet a good benchmark for tracking the progress of quantum computers.
If you want to stay abreast of progress in quantum computing, you should be paying attention to the arrival quantum error correction (such as surface codes getting more reliable as their size is increased) and to architectures solving core scaling challenges (such as lost neutral atoms being continuously replaced).


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Crack in the Cosmos]]></title>
            <link>https://drb.ie/articles/a-crack-in-the-cosmos/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080313</guid>
            <description><![CDATA[The clash of science and belief in Ancient Greece]]></description>
            <content:encoded><![CDATA[
Some time around the year 466 BCE – in the second year  of the 78th Olympiad, the Roman naturalist Pliny the Elder tells us – a massive meteor blazed across the sky in broad daylight, crashing to the earth with an enormous explosion near the small Greek town of Aegospotami, or ‘Goat Rivers’, on the European side of the Hellespont in northeastern Greece. Pliny’s younger contemporary, the Greek biographer Plutarch, wrote that the locals still worshipped the scorched brownish metallic boulder, the size of a wagon-load, that was left after the explosion; it remained on display in Pliny and Plutarch’s time, five centuries later.
Both writers connect the meteorite with the Greek scientist Anaxagoras, who had a widely-known theory that heavenly bodies are made of the same sort of matter found on earth. The amazed Greeks took the stone as spectacular confirmation of this crazy idea, and Anaxagoras’s name would be linked to it forever afterward.
To get a better idea of this meteorite’s figurative impact, consider a parallel from closer to our own time. Albert Einstein published his general theory of relativity in 1915, a decade after the narrower special theory of relativity had established a secure scientific reputation for him, along with other important papers he published in 1905, his annus mirabilis. Few physicists could follow the ins and outs of general relativity, and its immediate influence was slight. But one of those few was Arthur Eddington, whose widely publicised 1919 observations of a solar eclipse appeared to confirm Einstein’s revolutionary prediction that gravity bends light. ‘LIGHTS ALL ASKEW IN THE HEAVENS’ ran the delightful headline in The New York Times. ‘Men of Science More or Less Agog Over Results of Eclipse Observations. EINSTEIN THEORY TRIUMPHS.’ Overnight, Einstein’s name became synonymous with genius; two years later he won a Nobel Prize.
If we took that level of agog and multiplied it hundredfold, we might begin to approach the shockwave of the Aegospotami meteorite – and its effect on Anaxagoras’s reputation. For one thing, revolutionary as it was, Einstein’s improvement on Newton was subtle, not only in being hard to grasp but also in the sense that the effects Einstein predicted will never be perceptible to normal people under normal circumstances. Anaxagoras’s ‘general theory’ of the heavens completely shattered normal circumstances. The sky would never be the same.
For a modern person, grasping Anaxagoras’s audacity takes some doing, but it will help to recall that the chief organ of sight is not the eye but the brain. In other words, it’s our brains, not our eyes, that tell us what we’re looking at. When people in the ancient world looked up at the night sky, like us they saw lights. But that’s where the shared experience stops. Our brains tell us most of those lights are distant suns, and a handful of others are floating spheres in orbit around our sun, like the earth. Their brains told them all those lights were gods or mythical creatures. The idea that they might be objects floating in space didn’t exist yet. We look at the sun and see a giant ball of hydrogen converting itself into helium through a process of nuclear fusion; an ancient Greek saw the god Helios, driving a blazing chariot. We look at the moon and see a cold, airless, dusty ball of rock partly lit by the sun; an ancient Greek saw the goddess Selene, aglow with her own soft luminosity. The cosmos was alive, brimming with gods and goddesses, and their movements were omens. A basically well-ordered place – the Greek word cosmos originally meant “order” – its stability was nonetheless always under threat by the unruly. It quivered with agency and import, and it was all connected in an organic whole, from great to small alike.
The first to split this seamless cosmos was Thales, who lived perhaps just over a century before Anaxagoras, in the region of coastal Anatolia (modern Turkey) known to the Greeks as Ionia. Ionia’s principal city was the fabulous trading port of Miletus, where Thales was from, so he is known to history as Thales of Miletus. If the Greek world were a quiet pond and we threw a stone representing ‘the tradition of free rational inquiry’ into it, Miletus would be the site of the splash. From there, the waves spread outward within a few generations. Thales had students, his students had students, those students had their own students, and so on. Thales and his successors recognised that there’s a real world out there, that it’s governed by orderly operations of its own, and that we don’t need gods or spirits to explain how those operations work. They cracked the world in two parts, natural and supernatural, and in doing so they pushed the supernatural off to one side. Together, they are the first scientists. And, yes, they made waves.
The achievements of these early scientists have long been dismissed by scholars, largely because, with the benefit of hindsight, they seem so basic. And it’s true that compared with the rapid progress Greek science made starting a bit later, in Plato and Aristotle’s time, those contributions might well appear trifling. Not to mention compared with today. Yet science had to start somewhere. A more accurate assessment might judge them not against what came after, but against what came before. From this perspective, these earliest insights were astonishing and unparalleled, as Daniel Graham, a classicist and historian of science at Brigham Young University, has now demonstrated in his authoritative 2013 book Science Before Socrates.
Graham paints a new picture of early science that gives central roles to Anaxagoras and his perhaps slightly older contemporary, Parmenides. Both could trace their intellectual ancestry back to Thales through teacher-student relationships. While invoking natural causes for things, Thales had stuck to the conventional view that the earth is a flat plate supported by something. As far as we know, the intuitive idea of a flat earth was common to all cultures the world over, though the supporting arrangement runs the gamut from turtles for the Maya to pillars for the Hebrews to elephants for the Indians. Thales’s student Anaximander was the first to propose the earth as an object floating in space. According to him, it was shaped something like a bongo drum, with a flat surface at either end. The sky was generally seen as a dome or vault above the flat earth, across which the sun, moon, planets and stars moved or were pulled in regular patterns. One of Anaximander’s students, Xenophanes, envisaged a sort of grid universe in which the earth is an infinite plane with numerous suns and moons crossing it in straight lines. Another thinker, Heraclitus (of not-being-able-to-step-in-the-same-river-twice fame), claimed the moon is a bowl of fire. He, too, thought we get a new sun every day – the size of a human foot. It’s understandable that modern scholars might dismiss such notions as wildly speculative and ungrounded in empirical observation.
Not so fast, says Daniel Graham. Even Heraclitus had worked out his fiery bowl theory of the moon so that it agreed with the observable lunar phases. What makes his theory feel out of touch to us is not so much the bowl part (after all, why not?) as the part where the moon is giving off its own light. That was the universal belief at the time, however, and not senseless at all, but common sense.
This is where Parmenides comes in, a student of Xenophanes from the Greek colony of Elea in southern Italy. Long understood only as a feverish theoriser caught up with abstruse questions of being and nothingness (which, admittedly, he was), Parmenides is also revealed by Graham’s careful research to have been a hard-headed empirical astronomer. For it was he who figured out that the moon gets its light from the sun, an insight now so pervasive as to be entirely taken for granted by little children. Yet there it is for the first time in the few surviving scraps of Parmenides’s poem On Nature, in which he strikingly describes the moon as ‘a lamp by night, wandering around the earth with borrowed light’ and ‘ever gazing toward the rays of the sun’. That second part suggests how Parmenides worked: by methodically observing lunar phases, and noticing what Heraclitus had not – that the bright part of the moon always faces the sun. Before Parmenides, other theorists, like Heraclitus, all portray the moon as generating its own light; after him, they all understand that moonlight is actually reflected sunlight. Heliophotism, this idea has been dubbed recently, in the way scientists still like to use Greek. Or, in English, ‘sunlightism’. It is arguably the first genuine scientific discovery, and quite possibly the most productive. Much else would be illuminated by it.
Right away, Parmenides’s explanation shone a bright light on the sphere, sphaira in Greek. Heliophotism entails that the moon is a sphere – it doesn’t make sense any other way. And if the moon is a sphere, why not the earth? Parmenides’s other great contribution follows naturally: he was the first thinker to suggest the sphericity of the earth, building on the earlier conception of the earth as an object floating in space. Soon to follow: orbits, and, eventually, rotation.
All well and good, but straightway heliophotism also put a spotlight on the mother of all omens: eclipses.
Cut to Athens in the year 478 BCE, on the morning of February 17th. The Aegospotami meteorite’s landfall is twelve years in the future. It’s an extraordinary time for the Athenians, cresting a wave after the decisive defeat of invading Persian forces over the previous couple of years. The city has been reduced to rubble – but no matter. The Athenians had deliberately abandoned it to fight at sea, under the leadership of the brilliant Themistocles. The strategy paid off at nearby Salamis, where Greek ships under Athenian command routed the numerically superior Persian navy under the stunned gaze of Xerxes himself, the Persian king. Now the Persians are on the run, chased back across the Aegean Sea. Not far from Aegospotami, Sestus, the main Persian base on the Hellespont, is about to fall to Athenian-led forces after a long siege that began the previous summer.
A few minutes before noon, however, Athens literally turns gloomy. The sky darkens, and the sun dims to a dusky disc encircled by a ring of fire. Disaster looming, despite the recent run of luck? Nemesis rearing its ugly head over the hubristic Athenians? Perhaps not. Or at least not yet. Eclipses had always – always – been taken as portents of catastrophe, but Daniel Graham argues that this one is different. This solar eclipse, he suggests, is the first one in history to be understood at the time it was happening – for one Ionian theorist, at least, the first disenchanted eclipse, with no evil spirit, no malicious demon eating the sun god, just the shadow of a sphere moving across the earth. Before Anaxagoras, philosophers floated lots of different ideas about eclipses, ideas that may seem ridiculous now, but notice that none of them involve the gods. A ring around the sun closes and opens. Clouds block the sunlight. The sun runs out of fuel temporarily. Heraclitus, we are not terribly surprised to hear, said the sun is a fiery bowl and its light is blocked now and then by the rim. Like Anaxagoras, Thales, too, was credited by some later writers with figuring out eclipses, but, again, Graham emphasises the timing. After Anaxagoras, everyone seems to understand that in a solar eclipse the moon casts a shadow on the earth, and that in a lunar eclipse the earth casts a shadow on the moon.
Eclipses, then, another point of contact with Einstein and relativity, were Anaxagoras’s ‘special theory’, establishing a secure but modest reputation for him with other thinkers. He could easily have learned from sailors’ eyewitness reports what modern computer models show: the path of this solar eclipse had passed directly over the Peloponnesus, the large roughly circular landmass that makes up the southern Greek mainland, and of which Sparta was the chief city. The eclipse’s umbra was about the same size, putting nearly the entire Peloponnesus neatly in the shade. Later reports said Anaxagoras claimed the moon was about the size of the Peloponnesus, and the sun somewhat larger, which is of course partly true, technically.
That sort of investigation was in keeping with the Ionian tradition in which Anaxagoras had been trained. It was called historia, which, owing to its popularisation a scant generation later by Herodotus, has come down to us as ‘history’. But its basic meaning is inquiry, and it still applied to the natural world. Only later, after Herodotus, would it take on the meaning of inquiry into the human past. At this time, Herodotus was a boy, growing up just south of Ionia. He knew it well and may have lived there for a time. When he came to write his groundbreaking history of the wars with Persia, he too would interview witnesses, asking questions and most likely buying drinks.
Science and history both got their start in this Ionian tradition, but Ionia had been a war zone for decades by 460 BCE, around the time Anaxagoras probably came to Athens, where he is said to have stayed thirty years, as the Athenians told stories about him as an old man. Many have the feel of tales told to burnish an image of eccentric and cosmopolitan genius. When asked if he was interested in his home country, Anaxagoras gestured to the sky and replied that, yes, he was very interested in his home country. Plutarch says Anaxagoras let his house fall to ruin ‘while he pursued his lofty thoughts and his passion for speculation’, which couldn’t have pleased his poor neighbours much. One of his lofty thoughts was that the universe is governed by a principle he called ‘Mind’, so the salty Athenians’ nickname for him, naturally, was ‘Mind’. Not so lofty, perhaps, was his insistence, against Parmenides, that the earth was flat – in another parallel with Einstein, also a refugee scientist who balked at some of the implications of his own discoveries.
His ‘prediction’ of the Aegospotami meteorite had secured Anaxagoras’s reputation as a full-on genius. He must have spent the years between the eclipse and the meteorite pondering the implications of heliophotism, and of his own theory of eclipses, which rested on it. Among his conclusions, as Graham sees it, are that the moon is a solid body, like the earth. It blocks sunlight completely, rather than letting some of it shine through, the way clouds do. And if the moon is a solid body, there could be other solid bodies up there too. Again, this seems basic to us, but remember – no one at the time had the foggiest notion what was up there. Or rather, a foggy notion was exactly what they had. A common theory about the moon held that it was cloud-like in its makeup, so Anaxagoras really was going out on a limb (so to speak). In fact, the prevailing view among his predecessors was that celestial and atmospheric phenomena were all of a piece. Clouds, stars, sun, moon – pretty much the same light, airy stuff.
This is why the Aegospotami meteorite hit so hard, and why it was taken as Anaxagoras’s prediction. He didn’t predict the time or place, but he did predict the possibility. That prognostication went not only against common sense, but against the best science as well. There are rocks in the sky! Lights all askew in the heavens! Chicken Little, look to your laurels. If there had been a New York Times, it would have read ANAXAGORAS THEORY TRIUMPHS. A voice in the wilderness was suddenly and dramatically vindicated. Men of science were completely agog.
And possibly women too. One of Anaxagoras’s fellow Ionians in Athens was Aspasia, a brilliant and highly cultivated refugee from Miletus. Prominent in Athenian society, she’s also one of the few women we know anything about in ancient Greece, which was highly patriarchal. But Plutarch says she taught rhetoric in her home, and that Socrates was among her students. Pericles, the Athenian democratic leader who lent his name to the age, famously divorced his wife to pursue a relationship with Aspasia. As Pericles himself was also very close to Anaxagoras, it would have been easy for the highly curious Aspasia to keep herself up on the latest scientific developments. In fact, some sources paired her with Anaxagoras as well, though that might just be historians’ gossip. Later writers portray her as a courtesan, but that too may reflect male discomfort.
As for Pericles, Plutarch says that Anaxagoras, about five years the politician’s senior, was the ‘one man more closely associated with Pericles than any other’. Expounding on the favourable influence of Anaxagoras and science generally, he writes that Pericles ‘learned from his teaching to rise above the superstitious terror which springs from an ignorant wonder at the common phenomena of the heavens. It affects those who know nothing of the causes of such things, who fear the gods to the point of madness and are easily confused through their lack of experience. A knowledge of natural causes, on the other hand, banishes those fears and replaces morbid superstition with a piety which rests on a secure foundation supported by rational hopes.’
Or not, as the case may be. Plutarch’s rationalistic idea of piety resembled that of those eminent Victorians who embraced science and the established church with equal nonchalance. But if Plutarch had lived in Victorian times, he would certainly have known Newton’s third law of motion: ‘For every action, there is an equal and opposite reaction.”’
The backlash came sooner rather than later, though it certainly came late too. Scientific explanation wasn’t the only Greek novelty. Plutarch himself tells the story, which the Irish classicist ER Dodds revisited in his iconoclastic 1951 book The Greeks and the Irrational. At the height of classical Athens, an era future generations would hold up as a shining triumph of reason and enlightenment, ‘most of the leaders of progressive thought’ were prosecuted for offences against religion, Dodds wrote. Evoking Newton, perhaps, he described this campaign in a chapter titled ‘Rationalism and Reaction in the Classical Age’.
And it started with the trial and banishment of Anaxagoras. The charges against Anaxagoras, Plutarch says, were brought by a diviner named Diopeithes, one of those whose livelihoods were based on the interpretation of eclipses and other omens; his decree proposed ‘that anybody who did not believe in the gods or taught theories about celestial phenomena should be liable to prosecution’. This is history’s first anti-intellectualism, and the first science-bashing by the pious, though hardly the last of either. It’s also the first recorded attempt at state-sponsored thought control, a good eight centuries before the Christian church staked out that territory with the backing of Roman imperial power, and indeed (as far as I can tell) the first explicit mention of religious belief at all, as opposed to practice. Plutarch’s account suggests Anaxagoras delayed publishing his only book, perhaps for several decades, out of concern for precisely such a backlash – in much the way Charles Darwin would during the Victorian era, another age of scientific progress and religious reaction (respectable nonchalance notwithstanding). ‘Pericles,’ Plutarch says, ‘was so alarmed for Anaxagoras’s safety that he smuggled him out of the city.’ Banished from Athens, the old philosopher settled in Lampsacus, across the Hellespont from Aegospotami, where he lived out his days a stone’s throw from the meteorite that had brought him fame. The people there, we’re told, held him in great esteem; after his death he was given a solemn funeral and a fine memorial.
In Athens they’d had enough astronomy for the moment. ‘Public opinion was instinctively hostile to natural philosophers …’ Plutarch writes in explaining the spree that began with Anaxagoras’s trial, ‘since it was generally believed they belittled the power of the gods…’ It was this instinctive suspicion of ‘natural causes’ – ho physikos logos in Plutarch’s Greek – that the opportunistic, disgruntled diviner Diopeithes took advantage of to attack Anaxagoras, Athens’s poster-boy for scientific genius.
But why would the Athenians in particular turn against science in this way? Starting in 431 BCE, the city  became embroiled in a war against Sparta that Pericles, an Athenian expansionist, had pushed for. Pericles died in the war’s second year, a victim of the great plague that killed perhaps one in three Athenians as they were penned into the city by Spartan forces. Known as the Peloponnesian War, this long, destructive conflict lasted nearly thirty years. As Dodds suggested, and as the Romanian classicist Alexander Rubel demonstrates in his book Fear and Loathing in Ancient Athens (first published in German in 2000 but translated into English only in 2014), the war exacerbated the suspicions many Athenians already harboured against philosophers and scientists, giving traction to religious fears, misgivings, and resentments that might have lain dormant in less anxious times. Rubel and Richard Janko, a British classicist currently at the University of Michigan, have proposed the year 430 BCE, not too long after the outbreak of war and plague – and right after another eclipse that was followed by a disastrous military expedition under Pericles’ command – for Anaxagoras’s exile. (Anaxagoras’s dates are notoriously difficult, and the sources are contradictory. Richard Janko, who accepts Graham’s main points about the scientific picture, offers a different timeline for Anaxagoras’s movements, which I have followed in this essay.) Prosecutions for impiety continued. One Diagoras of Melos, popularly known as ‘the Atheist’, was convicted of impiety and exiled around the middle of the war. So was another associate of Pericles, the sophist Protagoras, a suspected espouser of moral relativity and secular inquiry, which – then as now – carried a whiff of subversion to the nostrils of true believers. The progressive playwright Euripides, whose razor-sharp dramas incorporated the latest ideas in philosophy and science (making him the Tom Stoppard of classical Athens), is said to have been acquitted of similar charges.
Influential Oxford scholars like Hugh Lloyd-Jones and Kenneth Dover – Dodds’s own student – rejected Dodds’s interpretation, questioning the historicity of these incidents, including Plutarch’s account of the Diopeithes decree, which is the only place it’s mentioned. Yet Dodds has enjoyed a bit of a comeback lately. A conference at Corpus Christi College, Oxford in 2014 produced Rediscovering E. R. Dodds: Scholarship, Education, Poetry, and the Paranormal (2019), a somewhat backhanded appreciation that leaves an impression of Dodds as a formidable scholar and subtle thinker who unfortunately happened to be wrong about everything. More recently, Daisy Dunn’s Not Far from Brideshead: Oxford Between the Wars (2022) offers an engaging and perceptive account of Dodds’s socially fraught career at Oxford, from his arrival in 1912 as an Irish boy with ‘dilapidated shoes’ to his controversial appointment as Regius Professor of Greek and eventual recognition as ‘one of the most vibrant intellectuals of the twentieth century’. And other classicists have defended Dodds more forcefully than the Corpus group, including Alexander Rubel and Richard Janko: the latter, writing in The Journal of Hellenic Studies in 2020, grounds ‘the Athenians’ war on science’ in the sources and sees ‘a failure of imagination’ in Dodds’s critics. As both Rubel and Janko observe, it’s true Plutarch can be unreliable, but it’s also true he had sources we do not. It feels tendentious to disqualify him, along with many other sources, so seemingly arbitrarily. Dodds might yet have the last word.
Because, finally, there was Socrates. In 399 BCE, at age seventy, the Athenian philosopher was tried, convicted, and executed for ‘impiety and corrupting the youth of Athens’ with his teaching. It was just five years after the Spartan victory, during the war’s violent and oppressive aftermath (and six years after its last major battle, the crushing defeat of the Athenian navy at none other than Aegospotami). The story of Socrates’s trial and death is familiar enough in its outlines. Hidden deep beneath this popular understanding, however, lies the curious possibility that in condemning Socrates, the Athenians were, in effect, really avenging themselves on Anaxagoras, decades after his escape to Lampsacus.
Socrates himself thought so, at least as Plato portrays him in the Apology. Accused – falsely, he maintains – of not believing in the gods, he says to the contrary that ‘just like the rest of humanity’ he believes the sun and moon are indeed gods. If his students ever heard otherwise, it was only because he was relating the theories of Anaxagoras, whose book was readily available (if overpriced) in the market. And everyone knows that they were Anaxagoras’s theories. It would have been ridiculous to attempt to pass them off as his own opinions, he argues, so don’t tar him with that brush. ‘Do you think you are accusing Anaxagoras?’
Elsewhere, Plato has Socrates say that he did study Anaxagoras’s theories and those of the other scientists as a young man. At first, Socrates tells us in the Phaedo, he ‘was tremendously eager for the kind of wisdom which they call investigation [historia] of nature.’ He was especially interested in knowing the true causes of things, but found esoteric discussions of being and nothingness and change quite confusing. ‘Then one day I heard a man reading from a book, as he said, by Anaxagoras, that it is Mind that arranges and causes all things.’ This felt right to him, as it answered his deep desire for a power that ‘arranges everything and establishes each thing as it is best for it to be’. So he read every scroll of Anaxagoras’s book, determined to learn ‘about the sun and the moon and the other stars, their relative speed, their revolutions, and their other changes, and why the passive or active condition of each of them is for the best’.
That ‘glorious hope’ was quickly dashed, however. In Anaxagoras’s account, it seemed to Socrates, Mind had no agency other than initially setting things in motion, and no morality. Things weren’t arranged for the best, they were merely arranged. In what strikes me as the first articulation of the concept we know as supernatural power, Socrates says he’s looking for a supreme being with ‘divine force’ (daimonia ischys in Plato’s Greek), but Anaxagoras’s Mind was a dud. For this reason, Socrates tells us plainly, he completely lost interest in the heavens, in science, and in physical reality (ta onta, ‘the things that are’).
Plato’s Socrates, then, was clearly disenchanted with disenchantment, though it goes against his modern image as an enlightened secular thinker. Instead, as I have worked on a book about the historical and psychological origins of religious faith, I’ve reluctantly come to see Socrates as a herald of woo. The very same distaste for natural causes expressed by Socrates and observed by Plutarch ultimately gave rise to a tsunami of triumphalist supernaturalism, which ER Dodds chronicled in fascinating detail in a later book, Pagan and Christian in an Age of Anxiety (1965). We might call the seismic impulse behind it science shock. After all, you can’t have a concept of ‘supernatural’ unless you already have a concept of ‘natural’ – but once you do, it follows right away. The stronger the bonds of nature are perceived to be, the stronger must be the ‘divine force’ that bends or breaks them; the more concrete the boundary, the bigger the thrill of transgression. As science grew more established in Hellenistic and then Graeco-Roman culture, slowly neutering the old gods of nature, this psychological effect set the stage for the new prominence of miracles in the crisis-ridden centuries of the early Christian era. In the same way, it also ratcheted up the power of the new Christian god, who stood above nature, and whose totalising authority made not just Zeus but even YHWH look rather anaemic by comparison.
St Paul offered a panacea for the trauma of disenchantment, a lifeboat that bobbed easily on the rising tide of supernaturalism. Writing in the time of Pliny and Plutarch, Paul extolled pistis, ‘faith’, defining it in a way Socrates surely would have recognised: ‘the assurance of things hoped for, the conviction of things not seen’. He even used the quintessentially Socratic word elenchus, here translated as ‘conviction’, which is not the only verbal echo of Plato in this short formulation. His truly ingenious move was to exalt not just one supernatural figure, like other mystery cults, but the very concept of supernatural power itself – and to promise believers a share in that power for themselves. If Jesus was McDonald, a fellow with a great burger shack, Paul was Ray Kroc, establishing the franchise in the name of the founder and issuing standardising directives to Romans, Corinthians, Ephesians, Thessalonians. He erected a warm and personal supernatural system to stand against the cold, impersonal natural system of science. His constituency was not just any old gentiles, but precisely those gentiles first crushed under the heavy hand of natural laws: the Greeks. And so (as I’ve argued in more detail elsewhere) the first global franchise was set up on an anti-science basis. We associate the ‘war on science’ with modern faith, or with historical figures such as Galileo or Giordano Bruno. But  faith itself grew out of the war on science.
Science shock and triumphalist supernaturalism are as old as science itself – but no older – and they’re still with us today, concealed beneath layers of love and fellowship and suicide vests and assassinated abortion doctors. Gorgeous or gross, such items are little more than tribal window dressing. Faith can do without them and they without it. But faith can’t do without affirming the triumph of the supernatural. This common denominator is what Diopeithes (and even Socrates) share with the preachers flogging Covid-19 snake oil in the form of ‘supernatural healings’ a few years ago to their megachurch flocks, who were enjoined to touch their TV screens and be healed through the mighty power of Jesus. Humans, it seems, have an instinctive affinity for supernatural thinking that science offends. Wherever science has been, there also has been science shock, which has expressed itself not only in religious faith, but also in a long list of pseudosciences and occult fads from astrology to theosophy to today’s New Age Goopies and anti-vaxxers. It can lie dormant in good times, when it might evoke little more than a plaintive protest resembling that of the immortal Peggy Lee, who (so enchantingly) asked, ‘Is that all there is?’ At other times – well, let’s just say it can flare up with a vengeance when things go south, if that rings a bell for those following recent events in, say, Washington, DC’s Department of Health and Human Services. Or anywhere else on this floating sphere of rock and water, for that matter, currently burning, boiling, plastic-strangle and addled by chaotic electronic impulses into something resembling a sort of global epilepsy. Age of Anxiet anyone? Paging Mr ER Dodds …
If Socrates rejected Anaxagoras and his ideas so strongly, we might well wonder, how did he end up in the dock for him? Somehow, despite the differences between them, Socrates had inherited from Anaxagoras the role of Athens’s resident intellectual. Partly, no doubt, it was because Socrates himself played up to it, with his relentless questions and disputations and definitions and symposiums. Perhaps this was why the comic playwright Aristophanes, in his hilarious send-up of rational inquiry, The Clouds, had chosen to name his mad-scientist villain, who was clearly inspired by Anaxagoras, ‘Socrates’. From his school, the Think Tank, ‘Socrates’ puts ‘the clouds’ in place of the gods, complete with a secular explanation taken straight from Anaxagoras, that clouds are bags of air, and thunder and lightning – Zeus’s special ordnance, no less! – are simply what happen when they bump into each other. Plato’s Socrates mentions this play, written years earlier in the middle of the war, as something that helped turn the public against him. At the end, the Think Tank is burned down, and ‘Socrates’ is beaten and chased off the stage to cries of ‘Revenge! Revenge for the injured gods!’
As Dodds observed, the revenge taken against the real Socrates was misplaced. When a truer revenge came, ironically, it would take the form of something very much like the higher power that Plato’s Socrates vainly sought in the godless scrolls of Anaxagoras. Today, Anaxagoras’s successors detect echoes of the Big Bang in the background heat of his cosmos. In much the same way, we’re also still living with the reverberations of that long-ago impact at Aegospotami.
1/7/2025
Colin Wells writes about history, culture, and religion. His books include Sailing from  Byzantium: How a Lost Empire Shaped the World and A Brief History of History: Great Historians and the Epic Quest to Explain the Past. After graduating magna cum laude from UCLA with a double major in English and History, Wells read Greats (Greek and Latin language and literature) for three years at Corpus Christi College, Oxford, taking an Upper Second in his exams. His articles have been published in Arion, The Hedgehog Review and elsewhere. Links are available at his website, colinwellsauthor.com. He is currently working on two books, one about the alphabet and another about the origins of religious faith. He follows Bartleby on social media.






            
            
            
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Qweremin]]></title>
            <link>https://www.linusakesson.net/qweremin/index.php</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45061702</guid>
            <description><![CDATA[I invented a new instrument: The Qweremin is a qwerty theremin.]]></description>
            <content:encoded><![CDATA[


I invented a new instrument: The Qweremin is a qwerty theremin.





Background

In the summer of 2022 I built a C64-based theremin, described and explained here. The
theremin, of course, is one of the oldest electronic instruments. Its main
drawback—and strength—is that it's incredibly hard to master. The performer has
ultimate control over volume and pitch, but it takes months of practice before
you can play even a simple scale.

Meanwhile, I've also developed a line of instruments featuring qwerty
keyboards, where the keys are laid out as on a Type B chromatic button
accordion. The earliest example is the Sixtyforgan, presented in spring 2021, but
there's also Qwertuoso, the Commodordion, the Paulimba, and the C=TAR. For these instruments, the
main challenge has been how to incorporate phrasing and other forms of musical
expression; to move beyond the rudimentary note-on, note-off of an organ.

The Qweremin represents the unification of these two worlds. On the one hand
(pun intended) you have ultimate control over volume and expression, and on the
other you can play quick melodies with large jumps, and even chords. It is my
most expressive 8-bit instrument yet.

The volume control

As mentioned in the presentation video at the top of this page, the SID chip
only offers a crude 4-bit master volume control. For greater precision you have
to work with the envelope generators, but those always produce ADSR curves.
What I did in the original theremin project was to hack together a feedback
loop that would continuously monitor the current output level from one of the
envelope generators: When the present level was lower than the desired volume,
I would trigger a new note to enter the attack phase, and when the level was
too high, I would release the note. In this way I could make the envelope
generator hover around any desired volume.

To make this work, I had to set the attack and release rates carefully. Too
fast, and the volume would audibly oscillate back and forth between two levels.
Too slow, and the volume changes would lag far behind the hand movements. I
thought I had found a sweet spot, but it turned out I was being naïve.

In 2023 I ran into Bass Cadet, a
thereminist with a C64 background, at the Revision demo party. She had seen my
video and was enthusiastic about some day being able to get her hands on (or
near, technically speaking) a C64 theremin. It turned out that we were both
planning to attend another party, X, later that year. And since I was going to
bring my C64 anyway, it was a simple matter to throw the spoon and clamp into
my luggage as well.

Now, I always knew that my homemade C64 theremin could never compete with
the precision and responsiveness of a professional-grade instrument, but it was
nevertheless very instructive to get feedback from a real thereminist. I
learned that the biggest problem was the slow response of the volume control
antenna; it wouldn't react to sudden changes, which made it impossible to
control the phrasing or “shape” of individual notes.

Falling back on just using the master volume register made the instrument
playable, but as expected you could hear the distinct levels. This is
especially problematic towards the low end of the range: For instance, going
from level 1 to level 2 doubles the amplitude of the sound, and
that can be quite jarring in the middle of a pianissimo part.

So once I had the idea to make a Qweremin, I knew I had to pay more
attention to the snappiness of the volume control. And since I was already
relying on external components (the 555 timers), it wasn't a big deal to
add an external DAC. Then I ran into the noise issue described in the
presentation video, and added a second DAC chip to compensate for it. I'm aware
that there are dual-channel DAC chips on the market; I used two separate chips
because that's what I happened to have at home.

The C64 is still very much involved in the operation of the volume antenna.
It needs to decode the pulses from the 555, convert that to a desired
volume level, and pass it to the DAC chips over the user port.




A touch of celebrity

By sheer coincidence, in the same year that I brought my C64 theremin to X,
the organizers had invited legendary game music composer Rob Hubbard as a guest
of honour. I was waiting in the long line of people who wanted to exchange a
few words with him when Mahoney, who happened to be standing nearby, had the
excellent idea that I should ask Rob to sign the clamp of my C64
theremin—something he gladly did!

Posted Friday 29-Aug-2025 06:46

Discuss this pageThere are no comments here yet.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ford and the Birth of the Model T]]></title>
            <link>https://www.construction-physics.com/p/ford-and-the-birth-of-the-model-t</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058349</guid>
            <description><![CDATA[Ford’s status as a large-volume car producer began with the predecessor to the Model T: the Model N, a four-cylinder, two-seater car initially priced at $500. At the time, the average car in the US cost more than $2,000, and it seemed nearly unimaginable that a car with the capabilities of the Model N could cost so littl]]></description>
            <content:encoded><![CDATA[This is an excerpt from my forthcoming book, The Origins of Efficiency, out September 23rd.Ford’s status as a large-volume car producer began with the predecessor to the Model T: the Model N, a four-cylinder, two-seater car initially priced at $500. At the time, the average car in the US cost more than $2,000, and it seemed nearly unimaginable that a car with the capabilities of the Model N could cost so little. In 1906, the year the Model N was introduced, Ford sold 8,500 of them, making the automaker bigger than the next two biggest car producers, Cadillac and Rambler, combined.To produce such a huge volume of cars, Ford began to use many of the production methods it would develop more fully with the Model T. Many of the Model N’s parts were made of vanadium steel, a strong, lightweight, durable steel alloy. Vanadium steel allowed for a lighter car (the Model N weighed only 1,050 pounds), and was “machined readily.” This was important because Ford also made increasing use of advanced machine tools that allowed it to produce highly accurate interchangeable parts. In 1906, Ford advertised that it was “making 40,000 cylinders, 10,000 engines, 40,000 wheels, 20,000 axles, 10,000 bodies, 10,000 of every part that goes into the car…all exactly alike.” Only by producing interchangeable parts, Ford determined, could the company achieve high production volumes and low prices. Furthermore, Ford’s machine tools were arranged in order of assembly operations rather than by type, allowing parts to move from machine to machine with minimal handling and travel distance. It also made extensive use of production aids such as jigs, fixtures, and templates. These “farmer tools”—so called because they supposedly made it possible for unskilled farmers to do machining work—greatly simplified Ford’s machining operations.The Model N was so popular that demand exceeded capacity, which allowed Ford to plan production far in advance. This meant Ford could purchase parts and materials in large quantities at better prices and schedule regular deliveries, ensuring a steady, reliable delivery of material, which allowed it to maintain just a 10-day supply of parts on hand.But even as the Model N became Ford’s bestseller, the company was designing the car that would supersede it: the Model T. In addition to improving upon many aspects of the Model N— the Model T would be a five-seater, include a new transmission that made it easier to shift gears, and have a three-point suspension that made it better able to navigate America’s low-quality roads—the Model T’s design also involved significant process improvements. For one, it pushed the machining precision of the Model N even further. In his history of Ford, Douglas Brinkley notes that “in an industry previously unfettered by anything like exacting measurements, the typical tolerance on the Model T was 1/64th of an inch.” Indeed, outside the auto industry, some types of manufacturing were done without even the aid of dimensioned drawings. This precision machining was “the rock upon which mass production of the Model T was based.” Not only did a high level of precision facilitate manufacturing, it also made for a better product—one that was much more reliable than any other car on the market. Because parts were interchangeable, repairs were simpler. And the precision of the machining meant that unlike most other automakers, Ford didn’t need to test the engine before it was attached to the chassis, since “if parts were made correctly and put together correctly, the end product would be correct.” The Model T would ultimately cost around $100 a year to maintain, at a time when the maintenance of other cars cost $1,500 per year.At the time, most four-cylinder engines were cast either as four separate cylinders or two groups of two cylinders and then attached together, which required extra time and material. The Model T’s engine block, on the other hand, was cast as a single piece. Several components—the rear axle housing, transmission housing, and crankcase—were not made of the more customary cast steel but rather stamped steel, a then-novel technology for automobiles that was cheaper than casting. Like any new technology, these production methods required time and effort to implement—it took 11 months of development to figure out how to produce the drawn steel crankcase—but the manufacturing cost savings were worth the effort. Over time, more and more Model T parts would be made from pressed steel, though the transmission housing itself was later changed to cast aluminum.The Model T was not the cheapest car on the road when it was introduced—at $850, it cost several hundred dollars more than the Model N. But even when Ford later briefly raised the price to over $900, no other car offered so many features for the same price. Between October 1908, when the new model was announced, and September of the following year, Ford sold 10,607 Model Ts. By March, Ford had temporarily stopped allowing orders because it had filled its factory capacity until August.As production for the Model T began, Ford was already busy reworking and improving the production system. Originally, cars were transported by rail to Ford dealerships all over the country. But, realizing this wasted train space, Ford soon began to create local assembly plants. Model T parts would be shipped to these plants and then assembled into cars, dramatically lowering shipping costs. In his history of the company, Allan Nevins notes that “by shipping parts in a knocked-down state, [Ford] was able to load the components of twenty-six Model Ts into an ordinary freight car instead of the three or four complete cars that could otherwise be sent.” And while the Model T had originally come in several different colors, in 1912 Ford announced that the Model T would now come in a single color: black.The first Model Ts were assembled in Ford’s Piquette Avenue plant in Detroit, which was built in 1904. But in 1910 it moved production to the new, larger Highland Park factory, also in Michigan, which was considered to be the best designed factory in the world. At a time when electricity was still somewhat uncommon in manufacturing, electric motors drove mechanical belting and overhead cranes were used to move material. At the Piquette Avenue plant, material came in on the bottom floor and final assembly was done on the top floor. But at Highland Park, material came in on the top floor and gradually moved down to assembly on the ground floor. To facilitate the movement of material, thousands of holes were cut in the floor, which allowed parts to move down through the factory through chutes, conveyors, and tubes.At Piquette Avenue, machine tool use had been extensive, but the machinery was largely general-purpose. With the decision to focus on a single model and the subsequent enormous increase in production volume, Ford began to buy or create dozens of special-purpose machine tools designed specifically for the Model T, such as a machine for automatically painting wheels and another for drilling holes in a cylinder block. As with the farmer tools first introduced on the Model N, these special-purpose tools not only produced parts more cheaply but could also be operated by less-skilled machinists, reducing labor costs.It was only the enormous production volumes of the Model T that enabled Ford to make such extensive use of special-purpose machinery. Similarly, it was only by virtue of its large volumes that Ford could afford to purchase dedicated steel-stamping presses to churn out pressed-steel crankcases, which were cheaper and used less material than the cast iron employed by other manufacturers.Ford experimented with machinery continuously, and the factory was in a constant state of rearrangement as new machinery was brought online and old machinery was scrapped. In some cases, machines that were just a month old were replaced with newer, better ones. By 1914, Highland Park had 15,000 machine tools. As at other automakers, machinery was packed close together in the order that operations were performed. But Ford took this concept much further, sandwiching drilling machines and even carbonizing furnaces between heavy millers and press punches. Not only did this machine placement for material flow keep handling to a minimum, but the tight packing of machinery also prevented inventory from building up in the factory.Ford’s process improvements weren’t limited to new and better machine tools. The company constantly examined its operations to figure out how they could be done in fewer steps. In one case, a part being machined on a lathe required four thumbscrews to attach it to the lathe, each of which had to be twisted into place to position the part and untwisted to remove it. By designing a special spindle with an automatic clamp for the part, Ford reduced the time to perform the operation by 90 percent.The Model T itself was continuously redesigned to reduce costs. When the stamped-steel axle housing proved complex to manufacture, it was redesigned to be simpler. Brass carburetors and lamps were replaced by cheaper ones made of iron and steel. A water pump that was found to be extraneous was removed. As a result of these constant improvements, Ford was able to continuously drop the price. By 1911, the cost of a Touring model had fallen to $780; by 1913, it had dipped to $600. And as costs fell, sales rose. In 1911, Ford sold 78,000 Model Ts. In 1912, it sold 168,000. And in 1913, it sold 248,000.Then, in 1913, Ford began to install the system that would become synonymous with mass production: the assembly line. Though gravity slides and conveyors had existed prior to 1913, Ford hadn’t yet developed a systematic method for continuously moving the work to the worker during assembly. The first assembly line was installed in the flywheel magneto department. Previously, workers had stood at individual workbenches, each assembling an entire flywheel magneto. But on April 1, 1913, Ford replaced the workbenches with a single steel frame with sliding surfaces on top. Workers were instructed to stand in a designated spot and, rather than assemble an entire magneto, perform one small action, then slide the work down to the next worker, repeating the process over and over.The results spoke for themselves. Prior to the assembly line, it took a single worker an average of 20 minutes to assemble a flywheel magneto. With the assembly line, it took just over 13 minutes.Ford quickly found even more ways to improve the process. To prevent workers from having to bend over, the height of the line was raised several inches. Moving the work in a continuous chain allowed it to be synchronized, which sped up the slow workers and slowed down the fast ones to an optimal pace. Within a year, the assembly time for flywheel magnetos had fallen to five minutes.This experiment in magneto assembly was quickly replicated in other departments. In June 1913, Ford installed a transmission assembly line, bringing assembly time down from 18 minutes to nine. In November, the company installed a line for the entire engine, slashing assembly time from 594 minutes to 226 minutes. As with the flywheel magneto, further adjustments and refinements to the lines yielded even greater productivity gains. In August, Ford began to create an assembly line for the entire chassis. Its first attempt, using a rope and hand crank to pull along the car frames, dropped assembly time from 12.5 hours to just under six. By October, the line had been lengthened and assembly time had fallen to three hours. By April 1914, after months of experimentation, car assembly time had been cut down to 93 minutes.In addition to reducing assembly times, the assembly line decreased inventories. By moving the work continuously along the line, there was no opportunity for parts to accumulate in piles near workstations. The Highland Park facility kept enough parts on hand to produce 3,000 to 5,000 cars—just six to 10 days’ worth of production. This was only possible through careful control of material deliveries and precise timing of the different assembly lines.As the assembly lines were installed, Ford continued to make other process improvements. Operations were constantly redesigned to require fewer production steps. One new machine reduced the number of operations required to install the steering arm to the stub axle from three to one. An analysis of a piston rod assembly found that workers spent almost 50 percent of their time walking back and forth. When the operation was redesigned to reduce time spent moving about, productivity increased 50 percent. A redesigned foundry that used molds mounted to a continuously moving conveyor belt not only increased assembly speed but also allowed the use of less-skilled labor in its operation.Meanwhile, Ford continued to tweak the design of the Model T. The body was redesigned to be simpler and less expensive to produce. Costly forged parts were eliminated by combining them with other components. Fastener counts were reduced. By 1913, comparable cars to the Model T cost nearly twice as much as the Model T did. And the price continued to fall. By 1916, the cost had dropped to just $360—a two-thirds reduction in just six years.With the Model T, Ford didn’t just create a cheap, practical car. It built an efficiency engine. With high-precision machining, Ford was able to manufacture highly accurate parts that resulted in a better, more reliable car, required less work to assemble, and used less-skilled labor. This made the car inexpensive, which, along with its excellent design, resulted in sky-high demand. High demand and high production volume enabled Ford to make additional process improvements. It designed and deployed special-purpose machine tools—large fixed costs that were only practical at huge production volumes—which increased production rates and decreased labor costs. It set up dedicated assembly plants (also large fixed costs), which enabled substantial reductions in transportation costs. It built a new factory (another large fixed cost) specially designed to optimize the flow of production. It placed massive material orders, resulting in lower prices, lower inventory costs, and smoother material delivery, reducing variability and making maximum use of production facilities. And, as all these improvements drove down the cost of the car, demand for the Model T continued to rise, enabling Ford to improve its processes even more.More generally, the high production volumes of the Model T made any process improvement incredibly lucrative. Even a small change had a big impact when multiplied over hundreds of thousands of Model Ts. Consider, for instance, the effect of one minor improvement among many, the removal of a forged bracket:Presume that it took just one minute to install the forged brackets on each chassis. Ford produced about 200,000 cars in 1914. It would have taken 200,000 minutes, or better than 3,300 hours for the installation of these forgings. Each of these brackets was held in place with three screws, three nuts, and three cotter pins; that’s six screws and nuts per car—1,200,000 of each! This saving does not take into account the cotter keys nor the brackets themselves. Each bracket had four holes which had to be drilled— 1,600,000 holes—which took some time as well. If the screws alone were as cheap as ten for a penny, the savings on screws alone would have been $1,200!Since any production step would be repeated millions of times, it was worth carefully studying even the smallest step for possible improvements. This resulted in an environment of continuous improvement, where processes were constantly experimented on, tweaked, ripped out, and replaced with better ones. Ford could, and often did, experiment with and create designs for its own machine tools, only later having a tool builder supply them. And if new machines didn’t work properly, Ford could afford to abandon the experiment. In 1916, a custom-designed piston-making machine that cost $180,000 to produce—$5 million in 2023 dollars—was “thrown into the yard” after repeated failures and replaced with simple lathes.The ultimate example of this environment of constant tinkering is the assembly line, which took years of experimentation to fully work out and required restructuring almost all of Ford’s operations. By breaking down operations into a series of carefully sequenced steps and mechanically moving material through them, Ford was able to eliminate extraneous operations, reduce inventories, and increase production rates, enabling even lower costs and greater scale. This entire chain of improvements was itself made possible by the development of precision-machined parts.The Model T would change the world, both by making the car a ubiquitous feature of American life and, more subtly but no less significantly, showing what could be achieved with large-volume production and a cascading chain of improvements.The Origins of Efficiency is available for preorder at Amazon, Stripe Press, Barnes and Noble, and Bookshop. It will be out September 23rd.]]></content:encoded>
        </item>
    </channel>
</rss>