<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 04 Sep 2025 22:32:28 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[What Is the Fourier Transform?]]></title>
            <link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132810</guid>
            <description><![CDATA[Amid the chaos of revolutionary France, one man’s mathematical obsession gave way to a calculation that now underpins much of mathematics and physics. The calculation, called the Fourier transform, decomposes any function into its parts.]]></description>
            <content:encoded><![CDATA[
    As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.
It took mathematicians until the 19th century to master this same calculation.
In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and you’ll get your original function. The technique, today called the Fourier transform, allowed the mathematician — previously an ardent proponent of the French revolution — to spur a mathematical revolution as well.
Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.
“It’s hard to overestimate the influence of Fourier analysis in math,” said Leslie Greengard of New York University and the Flatiron Institute. “It touches almost every field of math and physics and chemistry and everything else.”
Flames of Passion 
Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.

Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 — shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British — Fourier could think of nothing else.
If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly — meaning they have more energy — decay first, followed eventually by the lower frequencies. It’s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.
The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work “nothing short of impossible.”
What troubled his peers most were strange cases where the heat distribution might be sharply irregular — like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.
Today, we know that Fourier was broadly right.
“You can represent anything as a sum of these very, very simple oscillations,” said Charles Fefferman, a mathematician at Princeton University. “It’s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethoven’s Ninth Symphony.” The process only fails for the most bizarre functions, like those that oscillate wildly no matter how much you zoom in on them.
So how does the Fourier transform work?
A Well-Trained Ear
Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.
Mathematically, the Fourier transform is a function. It takes a given function — which can look complicated — as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, you’ll get the original function.

        
            
            Samuel Velasco/Quanta Magazine
        
    

To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Let’s look at a simple example.
Consider the following function:

        
    

The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Here’s what happens if we multiply the original by a sine wave with a frequency of 3:

        
    

There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.
Now let’s test if the frequency 5 is present. Here’s what you get when you multiply the original function by a sine wave with the frequency 5:

        
    

There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.
The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)
In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.
If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and — despite mathematicians’ early hesitation to accept such a thing — it is now an essential tool in the analysis of functions.

        
    

Encore
The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns — and simple combinations of them that resemble checkerboards — can be added together to re-create any image.
Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.

        
    

In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly — aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. “It’s now a part of everyday life,” Greengard said.
It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics — the physics of the very small — it even provides the mathematical foundation for the uncertainty principle, which says that it’s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particle’s possible positions; the Fourier transform of that function will describe the particle’s possible momenta. When your function can tell you where a particle will be located with high probability — represented by a sharp peak in the graph of the function — the Fourier transform will be very spread out. It will be impossible to determine what the particle’s momentum should be. The opposite is also true.
        
        
The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis — which studies the Fourier transform, as well as how to reverse it to rebuild the original function — is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. They’ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.
“If people didn’t know about the Fourier transform, I don’t know what percent of math would then disappear,” Fefferman said. “But it would be a big percent.”
Editor’s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between Quanta Magazine and the Simons Foundation is available here. 
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite?]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132498</guid>
            <content:encoded><![CDATA[





Small. Fast. Reliable.Choose any three.











Introduction

Suppose the
OpenDocument file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:

Smaller documents
Faster File/Save times
Faster startup times
Less memory used
Document versioning
A better user experience



Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

About OpenDocument And OpenDocument Presentation


The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
NeoOffice on Mac, or
LibreOffice on Linux and Windows.


An OpenDocument Presentation or "ODP" file is a
ZIP archive containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
SouthEast LinuxFest
conference:

Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
... 58 other pictures omitted...
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files



The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

application/vnd.oasis.opendocument.presentation


The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

Limitations Of The OpenDocument Presentation Format


The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:


Incremental update is hard.

It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

Startup is slow.

In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

More memory is required.

Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.


One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentations) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.


Crash recovery is difficult.

The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

Content is inaccessible.

One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

<?xml version="1.0" encoding="UTF-8"?>


The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.


First Improvement:  Replace ZIP with SQLite


Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);



For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.




Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
SQLAR utility:

-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp



The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.


The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
SQLite databases are atomic.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

Second Improvement:  Split content into smaller pieces


A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
of all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.


So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional


The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

SELECT slideContent FROM slide WHERE pageNumber=1;


This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order to render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facilitate rapid transitions to the next slide.


Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just a
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.


Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.


One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

Third Improvement:  Versioning


Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);



In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.


On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;



Or perhaps the application would rather use the
most recent checkinTime:

SELECT manifest, versionId, max(checkinTime) FROM version;



Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

Or, multiple presentations could be stored within the same document.

With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.


There are details to fill in here.
Perhaps a screen can be provided that displays all historical changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

And So Forth...


In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:

 Store an automated undo/redo stack in a database table so that
     Undo could go back into prior edit sessions.
 Add full text search capabilities to the slide deck, or across
     multiple slide decks.
 Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
 Break out the "Presenter Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
 Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.



An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.


Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
I/O performance is better than using separate
files.


Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into
Third Normal Form (3NF)
and store only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

Review Of The Benefits Of Using SQLite


In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:



An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.


The atomic update capabilities
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.


Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.


The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.


The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.



These are just a few of the benefits of using SQLite as an application file
format — the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the Application File Format
document for additional ideas.


Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.
This page last modified on  2025-05-12 11:56:41 UTC 

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rocketships and Slingshots]]></title>
            <link>https://postround.substack.com/p/rocketships-and-slingshots</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132183</guid>
        </item>
        <item>
            <title><![CDATA[Amazon RTO policy is costing it top tech talent, according to internal document]]></title>
            <link>https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132093</guid>
            <description><![CDATA[Amazon's strict return-to-office policy and relocation demands are hindering recruitment, affecting its ability to attract top tech talent.]]></description>
            <content:encoded><![CDATA[
            
            
            
            
                
                
                    
                      
                      
                    
                
                  
                        
                          
                          
                            
                              
                                    
                  
                          
                            Amazon's Seattle headquarters.
                            
                              
                                David Ryder/Getty Images
                                        
                          
                        
                  
            
    
    
    
              
                
                
                
                  
                      2025-09-04T09:00:01Z
                    
                  
                
                
                        
      
            
      
              
              
              
              
                
                    Amazon's strict return-to-office policy is limiting its recruitment efforts.
                    The policy requires employees to work in an office 5 days a week and relocate to hubs.
                    Amazon's AI reputation and pay structure also challenge its ability to attract talent.
                
              
      
            
            
            
            
            
            
                Amazon's 5-day return-to-office policy may be restoring discipline, but it's costing the company in the war for tech talent.An internal document and accounts from Amazon insiders show that the company's aggressive in-office work policy and requirement to move near designated "hub" offices are hampering recruiting efforts.The hub strategy is listed as one of the "hotly debated topics" for Amazon's recruiters, as it is limiting the ability to hire "high-demand talent, like those with GenAI skills," according to the internal document from late last year, obtained by Business Insider.Some Amazon recruiters told Business Insider that, starting last year, they saw an increase in candidates declining job offers specifically because of RTO. Those people were open to lower pay from other companies in exchange for the flexibility to work remotely.One of the recruiters said the company is losing out on tech talent due to this. The people who spoke with Business Insider asked not to be identified discussing sensitive topics.
                  
                In addition to the strict RTO rules, Amazon has flagged its unusual pay structure and lagging AI reputation as obstacles to recruitment, BI previously reported.The stakes are high. Amazon is racing to stay ahead in the highly competitive generative AI space, but without attracting top talent, the company risks falling behind.Oracle, for example, has hired away more than 600 Amazon employees in the past 2 years because Amazon's strict RTO policy has made poaching easier, Bloomberg reported recently.
              
              
              
            In an email to BI, Amazon's spokesperson said that the premise of this story was wrong, adding that the company continues to attract and retain some of the best people in the world.""We are always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations," the spokesperson said. Many firms are tightening RTO, but Amazon stands out. It demands 5 days in-office and ties compliance to promotions and performance reviews. Those who refuse to relocate to "hubs" are considered by Amazon to have voluntarily resigned."We continue to believe that teams produce the best results when they're collaborating and inventing in person, and we've observed that to be true now that we've had most people back in the office each day for some time," the Amazon spokesperson said. Wall Street is already noticing. A recent report by venture capital firm SignalFire found Amazon on the lower end of engineer retention, behind Meta, OpenAI, and Anthropic.Have a tip? Contact this reporter via email at ekim@businessinsider.com or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.
            
            
            
            
            
            
            
              
            
    
    
    
    
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ICPC 2025 World Finals Results]]></title>
            <link>https://worldfinals.icpc.global/scoreboard/2025/index.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131921</guid>
            <description><![CDATA[PC^2 Homepage
                
                					CSS by Tomas Cerny and Ray Holder
				
				Created by CSUS PC^2 version 
				9.11build 20250826 build 7926
            
            				Last updated
				Thu Sep 04 15:49:37 AZT 2025]]></description>
            <content:encoded><![CDATA[
            
                
                    RankNameSolvedTimeABCDEFGHIJKLTotal att/solv
                
                
                    191 St. Petersburg State University1114782/2552/530/--1/373/1501/203/2982/1951/422/813/1281/1921/11
                
                
                    299 The University of Tokyo1011161/2061/2710/--1/251/1841/340/--1/951/662/721/1261/1711/10
                
                
                    313 Beijing Jiaotong University1014252/2252/2041/--1/572/2702/520/--2/1322/842/1071/1481/618/10
                
                
                    4100 Tsinghua University98653/1390/--1/--1/331/1742/271/--1/711/872/841/1571/1315/9
                
                
                    576 Peking University98871/1800/--0/--1/142/1681/500/--1/1131/571/761/1831/2610/9
                
                
                    635 Harvard University99951/1240/--0/--1/351/2384/780/--2/1901/621/1011/691/1813/9
                
                
                    7140 University of Zagreb910752/1760/--0/--1/151/2422/770/--3/1752/641/641/1182/2415/9
                
                
                    860 Massachusetts Institute of Technology911231/2332/--0/--1/141/1131/360/--6/2041/504/1211/1452/2720/9
                
                
                    9131 University of Science and Technology of China911282/2240/--0/--1/411/2861/310/--2/2291/551/851/1231/1411/9
                
                
                    1083 Seoul National University911331/2013/2820/--1/320/--3/580/--1/1601/461/792/1611/1414/9
                
                
                    11129 University of Novi Sad911752/2442/--0/--1/392/2591/260/--1/1581/491/963/2081/1615/9
                
                
                    1282 Saratov State University911911/2033/--0/--2/372/2383/740/--1/1462/1041/1111/1641/1417/9
                
                
                    1357 Karlsruhe Institute of Technology911992/1331/--0/--1/452/2921/590/--2/2071/232/921/2341/3414/9
                
                
                    14127 University of Maryland912391/1064/--0/--1/171/2331/650/--3/2581/432/2251/1642/4817/9
                
                
                    1569 National Taiwan University912561/1900/--0/--2/452/2781/1160/--2/1821/1181/771/1731/1712/9
                
                
                    1687 Sharif University of Technology913291/734/--0/--1/231/2441/440/--4/2573/1364/1581/2131/2121/9
                
                
                    178 Arizona State University913311/2542/2980/--1/400/--1/940/--1/1692/692/1251/1991/2312/9
                
                
                    1838 HSE University86531/1680/--0/--1/382/--2/320/--2/1441/561/721/911/1212/8
                
                
                    1920 Carnegie Mellon University87661/2170/--0/--1/371/--1/510/--2/1791/611/741/1181/910/8
                
                
                    20126 University of Illinois Urbana-Champaign87742/2520/--0/--2/360/--1/1716/--1/1041/871/601/1621/1626/8
                
                
                    21144 Zhongshan (Sun Yat-sen) University88002/2760/--0/--1/363/--1/240/--2/1041/461/922/1511/1114/8
                
                
                    2255 KAIST88292/2591/1590/--1/250/--1/681/--3/--1/421/811/1431/3213/8
                
                
                    23143 Zhejiang University88711/1860/--0/--1/291/--1/450/--2/1442/1042/1151/1751/1312/8
                
                
                    2459 Kyoto University88801/2091/--0/--1/362/--2/820/--2/1531/592/1201/1461/1514/8
                
                
                    2571 National University of Singapore89231/1190/--0/--1/160/--1/291/2706/--1/852/452/2991/2016/8
                
                
                    2634 Harbin Institute of Technology89482/2380/--0/--1/700/--1/580/--1/1471/781/891/2311/179/8
                
                
                    2748 Institute of Science Tokyo89752/2212/--0/--1/672/--1/330/--1/2111/753/1471/1441/1715/8
                
                
                    2886 Shanghai University810091/26311/--0/--1/740/--2/570/--1/1541/992/662/2221/1422/8
                
                
                    2936 Hasso Plattner Institute810341/2601/--0/--1/390/--1/660/--1/1511/753/1341/2023/2713/8
                
                
                    3085 Shanghai Jiao Tong University810591/2410/--0/--1/200/--1/350/--2/1732/631/1145/2791/1414/8
                
                
                    3112 Beihang University810601/1940/--0/--1/394/--1/670/--2/2131/572/1133/2841/1316/8
                
                
                    32132 University of Science, VNU-HCM810901/2200/--1/--1/381/--1/260/--4/2381/902/1311/2501/1714/8
                
                
                    33121 University of California, Berkeley810921/2670/--0/--1/350/--1/540/--1/2031/941/1132/2871/199/8
                
                
                    3422 Central South University811391/2960/--0/--1/410/--1/610/--1/1943/1061/1421/2421/1710/8
                
                
                    3516 BINUS University811591/2940/--0/--1/410/--1/610/--3/2711/822/1321/2031/1511/8
                
                
                    3631 ETH Zürich812361/22113/--0/--2/890/--2/1210/--4/2462/403/1411/1891/2929/8
                
                
                    3758 Korea University812584/2921/--0/--1/990/--3/590/--2/2841/322/1121/1593/4118/8
                
                
                    3863 Moscow State University812611/2790/--0/--1/532/--2/630/--2/2181/925/1493/2291/1818/8
                
                
                    39117 Università di Pisa813435/2570/--0/--1/890/--2/550/--5/2941/992/1911/743/4420/8
                
                
                    4026 Delft University of Technology76033/--1/--0/--1/220/--1/590/--1/1091/1332/881/1591/1312/7
                
                
                    4178 Pohang University of Science and Technology76921/2560/--0/--1/210/--1/550/--3/--1/862/442/1641/2612/7
                
                
                    4253 Jagiellonian University in Krakow77184/--1/--0/--1/404/1351/500/--5/--1/571/1031/2332/2021/7
                
                
                    43137 University of Wroclaw77222/2680/--0/--1/350/--1/530/--1/1311/861/641/--2/4510/7
                
                
                    4495 The University of British Columbia77743/--0/--0/--1/260/--1/420/--3/2191/692/861/2501/2213/7
                
                
                    45125 University of Hong Kong77830/--0/--0/--1/440/--1/590/--1/1611/812/1081/2612/299/7
                
                
                    46141 UNSW Sydney77871/1450/--0/--1/304/--3/618/--0/--3/862/1162/1732/3626/7
                
                
                    4796 The University of Chicago78041/--4/2610/--1/280/--3/840/--0/--1/811/352/1461/4914/7
                
                
                    4825 De La Salle University78050/--0/--0/--1/480/--1/250/--2/2173/781/1171/2321/2810/7
                
                
                    4933 Georgia Institute of Technology78264/--0/--0/--1/192/--1/650/--3/2741/991/901/2121/2715/7
                
                
                    50123 University of Cambridge78271/--0/--0/--1/370/--2/770/--1/2691/1211/1362/922/3511/7
                
                
                    5194 Texas A&M University78283/--0/--0/--1/200/--2/590/--2/1583/1141/713/2282/3817/7
                
                
                    52124 University of Central Florida78822/2730/--0/--1/200/--1/420/--2/2043/1321/1090/--1/2211/7
                
                
                    5361 Moscow Aviation Institute79140/--0/--0/--1/460/--1/750/--2/2821/643/1941/1432/3011/7
                
                
                    5430 Ecole Polytechnique Fédérale de Lausanne79213/--0/--0/--1/440/--2/990/--1/2241/764/1681/1752/3515/7
                
                
                    55101 Union University - Faculty of Computer Science79250/--1/--0/--1/843/1421/420/--5/2611/752/1530/--1/2815/7
                
                
                    5690 St. Petersburg ITMO University79290/--0/--0/--1/164/--1/430/--1/1921/1084/1821/2971/3114/7
                
                
                    5710 Astana IT University79533/--0/--0/--1/500/--3/860/--3/1871/582/1981/2591/1515/7
                
                
                    58136 University of Warsaw79671/--0/--0/--1/430/--1/650/--7/1931/692/2261/2121/1915/7
                
                
                    5980 Rutgers University79870/--0/--0/--3/750/--1/560/--1/2481/1233/1651/2271/1311/7
                
                
                    6023 Chennai Mathematical Institute79920/--0/--0/--1/432/2601/660/--0/--1/1185/1552/1882/2214/7
                
                
                    6152 International IT University710050/--0/--0/--2/690/--1/890/--1/2201/1212/1691/2771/209/7
                
                
                    6215 Belarusian State University710110/--0/--0/--1/475/2891/790/--0/--1/1031/1242/1913/3814/7
                
                
                    6318 Brigham Young University711960/--2/2470/--1/650/--2/850/--0/--1/1334/2281/2982/2013/7
                
                
                    6465 Nanjing University of Science and Technology713403/2780/--0/--2/800/--6/1070/--0/--3/1142/1475/2931/2122/7
                
                
                    65122 University of California, Los Angeles64364/--4/--0/--1/280/--2/430/--2/1191/782/870/--1/2117/6
                
                
                    66115 Universidade Federal de Minas Gerais64750/--0/--0/--1/370/--1/490/--1/1801/861/1050/--1/186/6
                
                
                    67135 University of Toronto65440/--0/--0/--1/470/--2/580/--15/--1/722/1101/1851/3223/6
                
                
                    6866 National Economics University65600/--0/--0/--1/200/--1/500/--2/--2/991/1061/2381/279/6
                
                
                    6998 The University of Texas at Dallas65906/--0/--0/--1/580/--2/400/--3/1971/1002/740/--2/2117/6
                
                
                    7014 Beijing University of Posts and Telecommunications65960/--0/--0/--1/670/--1/480/--0/--1/691/2211/1701/216/6
                
                
                    7164 Nanjing University of Aeronautics and Astronautics65990/--0/--0/--1/710/--2/420/--11/--2/541/901/2901/1219/6
                
                
                    7241 Indian Institute of Technology - Indore66110/--0/--0/--2/810/--1/560/--0/--1/1061/1401/1911/177/6
                
                
                    7351 International Institute of Information Technology, Hyderabad66392/--0/--0/--1/170/--2/770/--1/2021/1211/1710/--1/319/6
                
                
                    7472 National Yang Ming Chiao Tung University66580/--0/--0/--2/400/--1/590/--1/2642/1111/1240/--1/208/6
                
                
                    75133 University of Tartu67250/--0/--0/--1/711/2661/900/--2/--1/1101/1500/--1/388/6
                
                
                    76103 Universidad de Buenos Aires - FCEN67250/--0/--0/--1/450/--1/670/--5/2811/1251/1043/--1/2313/6
                
                
                    7728 Ecole Polytechnique67265/--0/--0/--1/770/--1/520/--4/--2/871/1196/2571/1421/6
                
                
                    78120 University of Belgrade67820/--0/--1/--1/590/--4/1120/--0/--2/971/1752/1991/4012/6
                
                
                    7973 Neapolis University Pafos67910/--2/--0/--1/760/--2/390/--3/2891/1003/1436/--2/2420/6
                
                
                    8068 National Institute of Technology, Tokuyama College68030/--0/--0/--1/650/--1/350/--9/2471/901/1275/--1/7919/6
                
                
                    8181 Saarland University68080/--0/--0/--1/620/--1/430/--0/--1/1153/2241/2901/348/6
                
                
                    8289 Southern University of Science and Technology68260/--0/--0/--1/360/--2/580/--2/2404/2772/785/--1/1717/6
                
                
                    83134 University of Tehran68470/--0/--0/--1/580/--1/1250/--2/2891/1711/943/--3/5012/6
                
                
                    84142 Ural Federal University68720/--0/--0/--1/730/--1/980/--1/--1/1121/2511/2662/528/6
                
                
                    85118 Universiteit Utrecht69081/--0/--0/--1/590/--5/930/--9/--3/2152/1321/2491/2023/6
                
                
                    86109 Universidad Nacional de Rosario69960/--0/--0/--1/850/--2/990/--3/--3/2051/1562/2972/5414/6
                
                
                    8777 Petrozavodsk State University610040/--0/--0/--1/601/--3/1530/--4/2962/1212/2030/--1/3114/6
                
                
                    88128 University of Melbourne6104710/--0/--0/--3/1190/--4/1950/--2/2802/893/1652/--1/1927/6
                
                
                    8962 Moscow Institute of Physics and Technology53070/--0/--0/--1/691/--1/820/--4/--1/662/460/--1/2411/5
                
                
                    90130 University of Science and Technology - The University of Danang53780/--0/--0/--1/350/--2/630/--5/--1/1042/1161/--1/2013/5
                
                
                    9142 Indian Institute of Technology - Kanpur54150/--0/--0/--1/510/--1/790/--0/--1/1362/1070/--1/226/5
                
                
                    92114 Universidade Federal de Campina Grande54290/--0/--0/--1/340/--2/1020/--0/--1/591/1650/--2/297/5
                
                
                    934 Arab Academy for Science, Technology and Maritime Transport - Cairo54520/--0/--0/--1/720/--1/480/--0/--2/962/1420/--2/348/5
                
                
                    9470 National University of Science and Technology MISIS54590/--0/--0/--1/750/--1/230/--0/--2/1301/1470/--1/646/5
                
                
                    9527 Duke University54720/--0/--0/--1/490/--4/580/--0/--2/1131/1553/--1/1712/5
                
                
                    9629 Ecole Polytechnique de Tunisie54860/--0/--0/--1/350/--4/810/--0/--1/1161/1523/--2/2212/5
                
                
                    9740 Indian Institute of Technology - Delhi54861/--0/--0/--1/570/--1/830/--0/--1/993/1771/--1/309/5
                
                
                    9844 Indian Institute of Technology - Roorkee55283/--0/--0/--1/950/--3/680/--0/--2/1182/1470/--1/2012/5
                
                
                    99108 Universidad Nacional de Ingeniería - FC55440/--0/--0/--2/1020/--1/440/--0/--1/1202/2145/--1/2412/5
                
                
                    10049 Instituto Militar de Engenharia55540/--0/--0/--1/1030/--2/1050/--16/--3/1082/1460/--1/1225/5
                
                
                    1011 ADA University55570/--0/--0/--1/540/--2/870/--3/--1/792/2730/--1/2410/5
                
                
                    102111 Universidade de São Paulo55581/1180/--0/--1/340/--1/460/--0/--4/2595/--4/--2/2118/5
                
                
                    10339 Indian Institute of Technology - Bombay55990/--0/--0/--1/680/--1/1680/--0/--1/1982/1321/--1/137/5
                
                
                    1049 Assiut University56130/--0/--0/--1/530/--1/1020/--1/--2/1792/1940/--2/259/5
                
                
                    10554 Jordan University of Science and Technology56380/--0/--0/--1/572/--2/1030/--0/--1/1583/2370/--1/2310/5
                
                
                    10693 Syrian Virtual University56460/--0/--0/--1/700/--1/630/--0/--2/1513/2760/--1/268/5
                
                
                    10717 BRAC University56480/--0/--0/--1/540/--4/1400/--0/--1/2051/1520/--1/378/5
                
                
                    10874 Nizhny Novgorod State University56480/--0/--0/--1/980/--5/1050/--7/--1/592/2430/--1/4317/5
                
                
                    10943 Indian Institute of Technology - Kharagpur56600/--0/--0/--2/770/--1/1030/--0/--2/1472/2510/--1/228/5
                
                
                    11032 Universidad Nacional de La Plata56761/--1/--0/--1/951/--1/600/--1/--2/1214/2091/--2/9115/5
                
                
                    111107 Universidad Nacional de Colombia - Bogotá56850/--0/--0/--1/330/--1/830/--0/--1/1325/2820/--2/5510/5
                
                
                    112116 Universidade Federal de Pernambuco57070/--0/--0/--1/1370/--3/340/--0/--2/2302/1820/--1/449/5
                
                
                    11346 Innopolis University57100/--0/--0/--2/750/--1/890/--5/--3/1362/2170/--3/7316/5
                
                
                    11447 Institut National des Sciences Appliquées et de Technologie57110/--0/--0/--1/870/--1/1500/--7/--1/1852/2440/--1/2513/5
                
                
                    11588 Sohag University57250/--0/--0/--2/760/--5/1150/--0/--1/1354/2160/--1/2313/5
                
                
                    11621 Carnegie Mellon University in Qatar57290/--0/--0/--1/893/--2/850/--0/--2/1435/2680/--1/2414/5
                
                
                    11745 Indian Institute of Technology - Varanasi57380/--0/--0/--3/1390/--3/870/--0/--3/1931/1690/--1/3011/5
                
                
                    118104 Universidad de Chile57580/--0/--0/--1/490/--3/1111/--9/--2/1435/2920/--1/2322/5
                
                
                    119106 Universidad Mayor de San Simón57602/--0/--0/--1/780/--3/1720/--0/--1/1111/2942/--2/4512/5
                
                
                    12084 Shahjalal University of Science and Technology57730/--0/--0/--1/640/--4/1380/--0/--2/1155/2470/--1/4913/5
                
                
                    12179 Purdue University57840/--0/--0/--2/690/--4/2240/--6/--1/736/1870/--2/3121/5
                
                
                    12224 Damascus University57940/--0/--0/--1/760/--1/1080/--1/--3/1532/2010/--5/11613/5
                
                
                    1232 Ain Shams University - Faculty of Computer and Information Sciences58090/--0/--0/--1/830/--3/1190/--0/--1/1863/2710/--1/709/5
                
                
                    12450 Instituto Tecnológico de Costa Rica campus Alajuela58320/--0/--0/--1/1410/--2/740/--1/--1/1656/2810/--2/3113/5
                
                
                    12567 National Institute of Technology Tiruchirappalli58550/--0/--0/--1/840/--1/1290/--0/--1/1804/2980/--3/6410/5
                
                
                    1263 Arab Academy for Science, Technology and Maritime Transport - Alameen58980/--0/--0/--1/840/--4/1540/--0/--2/1974/2980/--1/2512/5
                
                
                    12711 Baku Higher Oil School59440/--0/--0/--1/490/--3/1150/--0/--6/2944/2630/--1/2315/5
                
                
                    128112 Universidade Estadual de Campinas510380/--0/--0/--2/530/--5/1880/--0/--3/2873/2910/--2/1915/5
                
                
                    1295 Arab Academy for Science, Technology and Maritime Transport - Smart Village43830/--0/--0/--1/520/--8/--0/--0/--1/1311/1740/--1/2612/4
                
                
                    13019 Cairo University - Faculty of Computers and Artificial Intelligence44530/--0/--0/--1/230/--4/2140/--0/--1/1162/--0/--1/409/4
                
                
                    131102 Universidad Autónoma de Yucatán44740/--0/--0/--1/1110/--3/1360/--0/--1/742/--0/--4/5311/4
                
                
                    132105 Universidad de La Habana45100/--0/--0/--2/1080/--1/990/--0/--4/1930/--0/--1/308/4
                
                
                    13337 Homs University45820/--0/--0/--3/990/--4/2020/--0/--1/1374/--0/--1/4413/4
                
                
                    134113 Universidade Federal de Alagoas47050/--0/--0/--1/910/--7/1860/--0/--4/2123/--0/--1/3616/4
                
                
                    13575 NU-FAST Karachi33030/--0/--0/--2/1010/--2/1280/--0/--0/--1/--0/--1/346/3
                
                
                    136119 University of Balamand33420/--0/--0/--1/1530/--3/1110/--0/--0/--3/--0/--1/388/3
                
                
                    13797 The University of Jordan38290/--0/--0/--8/2930/--7/--0/--0/--3/2671/--0/--3/4922/3
                
                
                    138110 Universidad Panamericana Campus Bonaterra21750/--0/--0/--1/1120/--2/--0/--0/--2/--0/--0/--1/636/2
                
                
                    13956 Kardan University000/--0/--0/--1/--0/--0/--0/--0/--0/--0/--0/--4/--5/0
                
                
                    Submitted/1st Yes/Total Yes130/73/4572/53/84/--/0170/14/13874/113/20273/17/13531/270/2278/71/66211/23/135287/35/128154/69/72193/6/1381877/887
                
            
        PC^2 Homepage
                
                					CSS by Tomas Cerny and Ray Holder
				
				Created by CSUS PC^2 version 
				9.11build 20250826 build 7926
            
            				Last updated
				Thu Sep 04 15:49:37 AZT 2025
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Classic 8×8-pixel B&W Mac patterns]]></title>
            <link>https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131538</guid>
            <description><![CDATA[TL;DR: I made a website for the original classic Mac patterns I was working on something and thought it would be fun to use one of the classic Mac black-and-white patterns in the project. I'm talking about the original 8×8-pixel ones that were in the...]]></description>
            <content:encoded><![CDATA[
        
TL;DR: I made a website for the original classic Mac patterns
I was working on something and thought it would be fun to use one of the
classic Mac black-and-white patterns in the project. I'm talking about the
original 8×8-pixel ones that were in the original Control Panel for setting the
desktop background and in MacPaint as fill patterns.


Screenshots via to Marcin's awesome interactive
history
I figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere
on the web. And perhaps there are, but after poking around a bit, I ran out of
energy for that, but by then had a head of steam for extracting the patterns en
masse from the original source, somehow. Then I could produce whatever format I
needed for them.
There are 38 patterns, introduced in the original System 1.0 in the 1984 debut
of the Macintosh. They were unchanged in later versions, so I decided to get
them from a System 6 disk, since that's a little easier with access to utility
programs.
Preparation

Download Mini vMac.
Acquire "old world" Mac ROMs.
Download a System 6 startup disk image.
Download ExportFl disk image.
Download sitPack disk image.
Install "The Unarchiver" (brew install --cask the-unarchiver)
Install the Xcode command-line tools.

Extraction process
Start System 6 (drag the ROM onto the Mini vMac icon, then drag the System 6
disk onto the window when you see the flashing floppy disk). Mount the ExportFl
and sitPack disks by dragging their files and dropping on the classic Mac
desktop.
In emulation
Double-click sitPack to launch the program. Command-O to open, then navigate to
the startup disk by clicking "Drive". Scroll to find "System Folder" and
double-click on it. Scroll to the bottom, select "System" and click "Open". Save
the output file as "System.sit" in the top-level of the startup disk. Quit
sitPack back to the Finder.
Start the ExportFl program. Command-O or pick "Open" from the "File" menu. Find
the "System.sit" created in the last step and click "Open". A regular file save
dialog will appear on the modern Mac, pick a location and save the file.
On the modern Mac
Drag the "System.sit" file onto The Unarchiver, or open the file from within it.
This will produce a file called "System" (with no extension).
Run DeRez (part of the Xcode developer command-line tools) on the System file.
I first added /Library/Developer/CommandLineTools/usr/bin to my $PATH, then
ran:
$ DeRez -only PAT\# System > patterns.r


This produces a text representation of the PAT# resource in the System file.
It's a series of bytes that comprise 38 8×8 patterns meant for QuickDraw
commands. There's a leading big-endian unsigned 16-bit number (0026) to indicate the number of 8-byte patterns to follow.
data 'PAT#' (0, purgeable) {
	$"0026 FFFF FFFF FFFF FFFF DDFF 77FF DDFF"
	$"77FF DD77 DD77 DD77 DD77 AA55 AA55 AA55"
	$"AA55 55FF 55FF 55FF 55FF AAAA AAAA AAAA"
	$"AAAA EEDD BB77 EEDD BB77 8888 8888 8888"
	$"8888 B130 031B D8C0 0C8D 8010 0220 0108"
	$"4004 FF88 8888 FF88 8888 FF80 8080 FF08"
	$"0808 8000 0000 0000 0000 8040 2000 0204"
	$"0800 8244 3944 8201 0101 F874 2247 8F17"
	$"2271 55A0 4040 550A 0404 2050 8888 8888"
	$"0502 BF00 BFBF B0B0 B0B0 0000 0000 0000"
	$"0000 8000 0800 8000 0800 8800 2200 8800"
	$"2200 8822 8822 8822 8822 AA00 AA00 AA00"
	$"AA00 FF00 FF00 FF00 FF00 1122 4488 1122"
	$"4488 FF00 0000 FF00 0000 0102 0408 1020"
	$"4080 AA00 8000 8800 8000 FF80 8080 8080"
	$"8080 081C 22C1 8001 0204 8814 2241 8800"
	$"AA00 40A0 0000 040A 0000 0384 4830 0C02"
	$"0101 8080 413E 0808 14E3 1020 54AA FF02"
	$"0408 7789 8F8F 7798 F8F8 0008 142A 552A"
	$"1408"
};

It would have been simple enough to
parse this text, but I had Claude quickly make a Python
program
to do so and output them in .pbm format, which is part of the Netpbm image
format class. This is a simple image format that is text-based, a '1' or a
'0' indicating a black or white pixel in a row and column.
For example, this subway tile pattern  is represented like
this in .pbm:
P1
8 8
1 1 1 1 1 1 1 1
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0

From here, I can generate image files for the patterns in any format and
resolution I want, using ImageMagick or similar. It's important when scaling the
patterns to use -filter point, so that ImageMagick doesn't try to interpolate
the pixels it needs to fill in, which would lead to blurry results.

Why do all this?
It's nostalgic, I have a fondness for these old patterns and the original B&W
Mac aesthetic, it reminds me of playing games like Dark Castle and Glider,
messing around with HyperCard, and using Tex-Edit and hoarding early shareware
programs.
The whole point of the above is to get a copy of the System file out with the
resource fork intact, that's
where the desktop patterns live.
According to old classic Mac
manuals,
the patterns were QuickDraw bit-pattern resources, a simple bitmap of 8 bits
per row packed into 8 bytes (columns). It was fast for QuickDraw to copy them
over an area of the screen. For example the following pattern was used for the
default gray desktop pattern on black-and-white Mac screens.

I could have extracted all 38 patterns other ways: I could have screenshotted
each one, I could have looked at each one and hand-written .pbm files, both of
which would have been tedious and error-prone.
Ultimately, I wanted to extract the exact original data from the source (or
close enough copy thereof) and have the patterns in a format I considered
archival for this limited purpose (.pbm files are trivial to parse and
manipulate).
Head over to my pattern site to get the patterns for yourself.
(Credit for replica Geneva 9pt and Chicago 12pt fonts)


        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Action was the best 8-bit programming language]]></title>
            <link>https://www.goto10retro.com/p/action-was-the-best-8-bit-programming</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131243</guid>
            <description><![CDATA[There were many programming languages available for 8-bit computers, the most common being BASIC and Assembly Language, but Action! beats them both!]]></description>
            <content:encoded><![CDATA[There were many programming languages available for 8-bit computers, the most common being BASIC and Assembly Language, but there were also other lesser-used languages such as Logo, Forth, and Pilot. The languages that would go on to dominate 16-bit computing, C and Pascal, were also available but were usually severely limited. An 8-bit computer generally did not have enough horsepower to run those more complex language compilers1.By 1983 Optimized Systems Software (OSS) was renown in the Atari world for its great updated versions of DOS (DOS XL), BASIC (BASIC XL/XE) and assembler (MAC/65), so it was no surprise that they were the ones to introduce a new language, Action!, into the Atari market.Created by Clinton Parker, Action! was an all-new compiled language that was designed and optimized for the 8-bit 6502 CPU. It was a 16K cartridge2 and had everything you need integrated into one package: the monitor, compiler, text editor and debugger3. In some ways, Action! was the first IDE (integrated development environment) for an 8-bit computer.Back in the 80s I never used Action! and instead mostly used BASIC and OSS BASIC XE for my programming. I did like reading Action! program listings in magazines, though. But I now have the Action! cartridge and just recently acquired an Action! user manual, so I felt it was time to take a closer look at this amazing software development tool.The version of Action! that I now have is the classic orange cartridge, paired with a small 3-ring yellow binder containing the documentation. Action! was also available in the yellow label cartridge and its manual was also in a larger binder and then later, perfect bound (like the BASIC XL and BASIC XE manuals I have). Having the manual in a binder would have certainly been more useful in the 80s when you had to refer to it frequently.Action! retailed for $99 in 1983 (about $320 in 2025) and was only available for the Atari 8-bit computers. Early advertisements indicated there would be forthcoming versions for the Apple II and Commodore 64, but those never materialized.The manual is just under 220 pages and is concise, but reasonable well-written. There is not a ton of sample code and it doesn’t try to teach too many concepts. To get the most of it, you really already need to know how to program.I had been looking for an actual Action! manual for years, but the ones I’d seen on eBay had always been prohibitively expensive. Luckily I found one last month for just $30 and snagged it.You don’t need a physical manual, of course. An updated manual is available online in several places, and here’s the PDF.The editor really was a wonder for its time. It is a full-screen text editor that can scroll to the right as the line of text you type becomes longer than the 40 characters of an Atari screen. That was an unusual feature for the time, but was necessary because it allowed the indentation, encouraged by Action!’s structured programming style, to remain easy to read.Not much to see here, but it’s an empty editor screen.The editor can copy and paste text, another somewhat new feature for Atari text editors in 1983, has the ability to tag lines to jump to them rapidly and it also has a split screen mode that let you show two files (or two parts of the same file) on the screen at once. At first this might seem silly considering the small size of the screen, but this was revolutionary for the time. Normally to look at another file, you’d have to open it, losing the file you were working on, and then reload the original file. It was tedious and was a reason why you would print your programs back then.Even looking at different parts of a file could be a pain because you’d just be scrolling all over the place, which was not always fast or easy in many text editors. This was even worse with something like BASIC, which required you to LIST line ranges to see parts of your program.To exit the editor, you press Control+Shift+M which takes you to the monitor.Today this would be called the shell, but it is essentially the command line interface for the entire system. From the monitor, you can switch to the editor, compile, trace code, look at memory and more.Action! is a structured, procedural programming language. It is similar to both C and Pascal, although not quite as advanced as either of them.It has the usual commands for looping, if-then-else, but it does not have anything like a switch or Case statement. There are also only three data types: BYTE, CARD and INT. Strings were essentially just BYTE arrays.I found it endearing that to end an IF block you used FI (IF spelled backwards) and to end a DO block you used OD. That is some interesting symmetry although I’m not really sure it helps readability.An Action! “Hello World” program would be this:PROC hello()
; This is a comment.
  DO
    PrintE("Goto 10")
  OD
RETURNThe Action! language may not have been as advanced as C or Pascal, but because it was designed with the 6502 CPU in mind, compiling the language was astonishingly fast.The original Atari Pascal system from APX needed multiple disk drives and could take several minutes to compile a small program. The only C package available in 1983 (Deep Blue C) was at least as limited as Action!, but also not an integrated package and compiled slowly. Draper Pascal only compiled to pseudo-code.Action! compiled your program to machine code in memory and in seconds. Typing C (to compile) and then R (to run) was hardly slower than just typing RUN in BASIC.It really is stupidly fast. Here’s the output from the above program:If there is a compile error, it is shown on the screen and will be highlighted when you switch back to the editor by typing “e”.Action! was not perfect and it had several limitations. In my opinion, the two biggest limitations were that that Action! cartridge was required to run Action! programs (because they depended on the library that was included the cartridge ROM) and that there was no floating point data type.Both of these did get solved, to some extent, with the purchase of an additional add-ons: Action! RunTime and Action! Toolkit.The RunTime package provided the ability to create stand-alone Action! programs that you could distribute to others to run without the cartridge.The RunTime included the library as source files that you could include at the beginning of your own programs so that everything that was needed to run would get compiled into a single executable program. From what I can tell, Action! had no concept of linking which is how something like C would have handled this.I don’t have an official Action! RunTime disk, but the image is readily available online.The ToolKit is essentially an enhanced Library with additional functions and features. Two notable things it adds are player/missile graphics support and some support for floating-point numbers via several “Real” functions.Unfortunately this floating point support is somewhat limited and it doesn’t look all that useful to me. For example, I’ve used the Archimedes Spiral program in some articles here on Goto 10 to demonstrate drawing a fun graphic on the screen. It is interesting to see how long it can take to do the drawing on an 8-bit computer. I’d love to port it to Action!, and I was hopeful I’d be able to do so with the Action! ToolKit. Alas, even though it does add some commands to do some floating-point math, it does not add any trigonometry functions. The lack of Sin and Cos make it impractical to port Archimedes Spiral4.I don’t have an official Action! ToolKit disk, but the image is readily available online.It seems that Action! was mostly use by hobbyists, public domain and magazine software. The only two known commercial product made with Action! were the HomePak5 productivity package by Russ Wetmore and the Games Computers Play online service.For the above screen shots, I was using Action! with my 130XE.I plan to dig into actually using Action! itself more in the coming months. It really looks like a fun language. Unfortunately, since Action! is a cartridge, I can’t use it directly with my Side3 cart. There are disk-based versions of Action! available at AtariWiki, so I may have to switch to one of those, or see if I can get one of the SuperCart images to work with Side3. Otherwise, I may try it old-school with SpartaDOS, my trusty 1050 disk drive and a RAM disk.AtariWiki has a create page with lots of links to Action!-related materials.Action! Archive is a great reference for Action! programming.If you want to learn more about how to program in Action!, be sure to check out David Arlington’s YouTube channel, which has a 25-part series on Action! programming.1Kyan Pascal was released in 1986 and worked pretty well, but really wanted a couple disk drives. LightSpeed C was also a decent version of C that debuted later in the 80s.2Actually an OSS SuperCartridge, which had 16K of ROM but only used 8K of address space in the computer.3Calling it a debugger might be a bit of stretch compared to modern tools.4Sure, I could probably implement my own version of those, but I don’t really want to.5HomePak was an integrated productivity package with a highly regarded terminal program, a slick word processor with not much free RAM for text (perhaps 5 pages) and an usual database. HomePak warrants its own article.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LLM Visualization]]></title>
            <link>https://bbycroft.net/llm</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45130260</guid>
            <description><![CDATA[A 3D animated visualization of an LLM with a walkthrough.]]></description>
            <content:encoded><![CDATA[LLM VisualizationHome]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Saquon Barkley is playing for equity]]></title>
            <link>https://www.readtheprofile.com/p/saquon-barkley-investment-portfolio</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129523</guid>
            <description><![CDATA[The NFL star running back is building a portfolio of startups and redefining what it means to be a modern athlete.]]></description>
            <content:encoded><![CDATA[Saquon Barkley calls me, but he’s distracted. In the background, two little voices shout “Bye, friends!” as Barkley wrangles his kids, Jada, 7, and Saquon Jr., 3, into the car. He apologizes, then explains they’re headed to an Old Spice photo shoot tied to his latest endorsement — a Saquon-branded shampoo and conditioner called “Saquon Soar.”It’s a cinematic image: one of the NFL’s biggest stars juggling dad duty and the demands of a sponsorship. But Barkley isn’t content to just cash checks and smile for the camera. He has always treated these deals as building blocks for the empire he’s determined to build before football ends. And he knows it could all end suddenly.From the moment he entered the NFL in 2018, Barkley carried a kind of financial paranoia that most athletes don’t confront until retirement. He parked his entire $31.2 million rookie contract into long-term investments like the S&P 500, vowing to live only off endorsements. In 2021, he went further, backing payments app Strike as his first startup investment and pledging to take all marketing income in Bitcoin through the platform. At the time, Bitcoin’s price was approximately $32,000. Today, it’s hovering around $111,000, turning a $10 million income stream into a $35 million asset.That move rattled more than a few people. “Good old American dollars should be the standard,” superagent Leigh Steinberg warned, cautioning athletes against chasing crypto volatility.Barkley heard the critics, but he doubled down. Sitting on cash, he argued, isn’t enough — not when most athletes earn the bulk of their money in their 20s and face inflation, poor financial literacy, and limited access to tools. “A sad yet common reality is many enter bankruptcy later on,” he wrote on X. “We can do better.”His drive isn’t just financial. Barkley has the career most players dream about, but those closest to him say that his humility is born out of a gnawing sense that it could all slip away.For Barkley, equity is a safeguard — a way to wrest back control from the injuries, draft picks, or franchise decisions that can derail a career overnight. Football is fragile, while investing gives him a different relationship to risk. It may be volatile, but it’s a risk he can shape. Guided by business manager Ken Katz, he has avoided the typical athlete playbook of podcasts and clothing lines, instead building a portfolio that looks more like an elite venture fund than a celebrity brand.After reading Peter Thiel’s Zero to One, Barkley was most inspired by the idea of getting ahead by betting on businesses that create something so original that competition becomes irrelevant.Barkley at the 2025 Met Gala (Photo Credit: Sharif Fennell Jr.)The Profile has exclusively learned that Barkley has invested a portion of his earnings so far — a mix of his rookie contract and endorsement income — across more than 10 private startups, typically writing checks between $250,000 and $500,000.The high-growth startups in his portfolio include Anthropic (currently valued at $183 billion), Anduril ($30.5 billion), Ramp ($22.5 billion), Cognition ($9.8 billion), Neuralink ($9 billion), Strike (~$1 billion), and Polymarket (~$1 billion). He’s also a limited partner in funds including Founders Fund, Thrive Capital, Silver Point Capital, and Multicoin Capital.Some of those bets have already appreciated dramatically: Strike, for example, has delivered a 10x return in value since his investment. Barkley has also allocated a portion of his wealth into steadier assets like index funds and real estate, hedging the volatility that comes with venture and crypto.Unlike peers LeBron James and Serena Williams, who command headlines with splashy business ventures, Barkley’s approach is leaner and more surgical. He is making selective bets on technology companies that he believes are creating lasting value for their users.The question is whether his bets will hold. Venture investing is inherently risky — even late-stage startups with multibillion-dollar valuations can falter or see their valuations slashed when markets turn. To date, none of Barkley’s investments have flamed out or depreciated, largely because he prefers to come in at later stages of a company’s growth. In the end, Barkley must confront the question: Is he building something that lasts, or simply trading one kind of risk for another?For more longform profiles of extraordinary people, make sure to sign up for The Profile here:A few days before Thanksgiving last year, Ramp CEO Eric Glyman glanced at his phone and did a double take. His colleague Sam Buck had texted that Saquon Barkley wanted to invest in the expense management startup. Glyman was stunned. Most athletes ask for cash in exchange for an endorsement. Barkley wanted equity.“The best player in the NFL wants to work with us,” Glyman remembers thinking. “Like, what? What’s happening?”Barkley, who came in when Ramp was valued at $7 billion, didn’t just write a check. Convinced by Ramp’s mission to help businesses cut costs and perform more efficiently, he tied his own fortunes to the company’s — and then went to work to make sure it paid off.During the 2025 Super Bowl, Barkley starred in a Ramp commercial: a 15-second spot of him, in full pads, drowning in paperwork before Ramp’s automation saves the day.The ad aired as the Eagles defeated the Kansas City Chiefs, and Barkley set a new single-season rushing yards record for regular season and playoffs combined. For Ramp, it became their biggest traffic day ever. For Barkley, it was one of the rare times an endorsement literally boosted his own net worth.That was by design. Barkley’s process usually begins with his business manager, Ken Katz, reaching out to a company. Their strategy is to target technology companies and source deals through trusted word-of-mouth referrals in Katz’s investor network. The founder then gets invited to dinner with Barkley, where the running back plays interrogator. “It’s about asking them what they stand for, what their mission is, why they think they’ll be successful,” Barkley tells me. “They have to be confident, but arrogance is a turn-off.” If he’s interested, he often gives a verbal commitment on the spot.Sam Buck, Ramp’s head of financial institutions, insists Barkley was never treated like a celebrity mascot. “It was like, ‘Hey, you’re on the cap table now,’” he says. “We’re going to hold you accountable — just like we do with Founders Fund or General Catalyst — to help us grow.”And Barkley delivered. He FaceTimed Ramp partners to help close deals. He joined customer meet-and-greets. He talked about the company any chance he got.Fresh off the Super Bowl, he appeared on the TODAY show, still buzzing from the win. He fielded questions about being a champion, about his family, about life off the field. Then TODAY co-host Savannah Guthrie pivoted. “You have your own Super Bowl commercial, by the way, for Ramp, where you’re an investor,” she said. “So tell me how all this came about.”Barkley didn’t miss a beat. He turned the moment into a plug for his investment: “I fell in love with the team of Ramp … You get a lot of athletes who get involved with brands, and you get a certain dollar to show up and do things. But … as this company grows, I get to grow with it — it takes it to a whole new level for me.”The strategy is working. Barkley invested $500,000 into Ramp at a $7 billion valuation. The company’s valuation has more than tripled since then, which means his stake is now worth roughly $1.5 million. Earlier this year, Ramp announced $700 million in annualized revenue. “We’re growing faster this year than last year,” Glyman says, “and I don’t think it’s unrelated that Saquon has been involved.”But Barkley’s commitment also shows up in less public settings. In May, he flew to a Founders Fund symposium in Montana, an event filled with billionaires, CEOs, and top investors. He spent the day listening and networking, before catching a red-eye back to Philadelphia for a morning speaking obligation. Within hours, he was on a return flight to Montana. After another full day at the symposium, he boarded a midnight flight to make it to his daughter’s soccer game the next morning.Longtime venture investor Brian Singerman noticed his dedication to building meaningful relationships. “That stuff is impossible to fake,” he says.Even so, Barkley isn’t blind to the trade-offs. For someone intent on building generational wealth, venture capital represents both the biggest swing and the biggest vulnerability.“Honestly, putting your money in the S&P is going to beat 90% of venture capital,” says Singerman, who is one of Barkley’s advisers. “But that last 10% is going to crush the S&P.”This version of Barkley — the investor, the owner — didn’t come naturally. He didn’t grow up with stock tips or startup dinners. His financial philosophy was shaped by something starker: watching what happens when the money runs out.Barkley with his father, Alibay (Photo Credit: Sharif Fennell Jr.)When Barkley tells me about growing up in the Bronx, he chooses his words carefully. He had a loving family, he says, and doesn’t remember feeling the financial instability, at least not consciously.But to understand his obsession with money, risk, and generational wealth, you have to start with his parents. Tonya and Alibay were both born in the Bronx and raised five children together.In 2001, when Saquon was four, the family left for Pennsylvania. Over the years, Tonya developed a motto she repeated often: “All things are possible.” She believed it because she had lived it, navigating crises that could have broken the family but didn’t.One of those crises came when Barkley was in elementary school. The family was evicted and spent eight months without a home. The shelter system wouldn’t take fathers, so they were split up. Barkley and his sister went to live with a family friend while his parents scrambled to rebuild.“Financially, we obviously struggled, but my mom and dad never made us feel that,” he says. “It wasn’t like we were on the streets. We had family and friends who took us in, but [the experience] did give me the perspective I have on life right now.”His father carries his own formative scar. A gifted boxer, Alibay had made it to the semifinals of the 1992 New York State Golden Gloves before his shoulder gave out. He couldn’t afford surgery. At 21, he quit — not by choice but by circumstance.“I never had the money to get it fixed,” he says. The missed chance haunted him, and it became the lesson he drilled into his children: never quit on your passion. “If you quit this, it will be easy to quit jobs, quit relationships, quit on your kids,” he says. “That’s probably why Saquon is so adamant when he gets hurt about still being in there.”That “never quit” mantra became Barkley’s compass. At 13, frustrated in school and doubting football, he thought about walking away. His father told him the boxing story again. It stuck.From then on, Barkley zeroed in. He would play football, and he would become great.“Growing up, he had flashes of being great, but there was always someone in front of him,” says childhood friend Nick Shafnisky. “He was never going to let someone say he wasn’t the best.”At Penn State, Barkley fulfilled that prophecy, boasting 3,843 rushing yards and 43 touchdowns in three seasons. Head coach James Franklin drilled players with one message: “Use football to build generational wealth. Don’t let football use you.”Barkley took it to heart. One night, Franklin overheard his wife on a call talking about real estate. He showered, brushed his teeth, and got into bed, and she was still on the phone. When she finally hung up, he asked who it was. “It’s Saquon,” she told him.He was working on his first real estate investment and wanted to understand how to structure the deal and spot potential red flags. “I think curiosity is a really important trait in successful people, and Saquon is very curious,” Franklin tells me.His questions about money and wealth went from theoretical to real when the New York Giants drafted Barkley second overall in the 2018 NFL Draft and handed him a $31 million rookie contract. He delivered immediately: 1,300 rushing yards, 91 receptions, Rookie of the Year, Pro Bowl. The dream was intact — until it wasn’t.In Week 2 of the 2020 season, against the Chicago Bears, Barkley was tackled awkwardly near the sideline. He clutched his knee and was carted off the field. The diagnosis? A torn ACL, partially torn meniscus, and an MCL strain. It was a devastating cocktail of injuries. “I definitely had some dark moments during that time,” Barkley says. “I had those moments of, ‘Man, why me?’”Whether Barkley realized it or not, it was his father’s nightmare revisited: one injury, one twist, and everything could vanish. At Katz’s suggestion, Barkley watched Peter Thiel’s “Competition Is for Losers” lecture at Stanford and read his book Zero to One, both of which left an impression. Thiel’s argument — that the biggest returns come from a few breakthrough bets — resonated. Football suddenly felt fragile, and investing began to feel urgent.His return to the NFL was shaky. In 2021, after months of rehab, he sprained his ankle early in the season and stumbled to a career-low 593 rushing yards. Friends say he wasn’t himself. His mom reminded him: “No matter what, you can always get back up.” He did. By 2022, Barkley was back at 1,300 yards and another Pro Bowl.But it was no longer possible to ignore the reality that his body and career were vulnerable. The Giants reminded him of that in 2023 when they slapped him with the “franchise tag.” It was a one-year deal — not the long-term contract he had earned. To Barkley, it was a gut punch. Even after proving himself, he was treated as expendable.“It was a very contentious situation,” says former NFL defensive great J.J. Watt. “[Barkley] was the star of [the Giants], and they basically kicked him out the door.”Publicly, Barkley stayed composed. Privately, the rejection lit a fire. Running backs have the shortest shelf life in the league, but his bigger worry was what comes after. Approximately 78% of NFL players face financial distress within two years of retiring, and he was determined not to be one of them.By then, he had already built a portfolio and a mindset beyond football. In 2024, Barkley left the Giants for a three-year, $37.75 million deal with the Philadelphia Eagles.It was a declaration that he knew his value, and he wasn’t going to sign a deal that didn’t reflect it.Barkley as a Philadelphia Eagle (Photo Credit: Sharif Fennell Jr.)In February 2025, confetti rained down on the Philadelphia Eagles after they defeated the Kansas City Chiefs in the Super Bowl. Barkley had delivered — a standout game, a title, and the biggest win of his career on his 28th birthday. “I don’t think anything is going to top this,” he said on the field moments after the final whistle.That night, Barkley threw an afterparty at St. Pizza in New Orleans. The guest list was surreal, including Leonardo DiCaprio, Pete Davidson, Zac Efron, and Chance the Rapper. But scattered among the celebrities was a quieter, more unexpected group: the founders of Barkley’s portfolio companies.Jack Mallers, CEO of Strike, was one of them. “I felt bad for him,” Mallers says. “Even though he had just won the Super Bowl and it was his birthday, it felt like he was still working. He took a photo with everyone. Answered every question. Asked the founders how business was going. It was unbelievable.”Mallers first met Barkley through an email from business manager Ken Katz with the subject line: Saquon Barkley loves Bitcoin. “It felt like a scam,” Mallers laughs. But after dinner with Barkley, he realized the interest was real and gave him a spot in Strike’s $100 million round. Barkley invested $100,000, and today the company is reportedly valued at over $1 billion, marking a 10x return on his stake.That deal forged a deep relationship. Since they met, Barkley and Mallers have spent hours debating Bitcoin as a store of value. In 2021, Barkley went all in on the digital currency, announcing he would convert 100% of his endorsement income — Nike, Pepsi, Visa, Dunkin’ — into Bitcoin using Strike.The move was yet another example of Barkley using his platform to amplify a company in which he has equity. “The average NFL career is 3 years and inflation is real,” he tweeted. “Saving and preserving money over time is hard… Bitcoin is a proven, safe, global, and open system that allows anyone to save money.”Crypto executives hailed his entrance into the world of Bitcoin. “He’s setting a blueprint for athletes to take charge of their financial future,” says Coinbase president Emilie Choi. But traditional advisers recoiled. “The value of Bitcoin is much more volatile than the value of the U.S. dollar,” one financial planner warned in an op-ed. “How would you feel if your hypothetical Bitcoin paycheck was worth 10% less the day after it was deposited into your account?”For a while, Barkley looked prescient as his holdings appreciated in value. Mallers even joked, “My goal was to help him earn more through Bitcoin and Strike than he would in the NFL. And I think he will.”But then came the crash. Famed crypto exchange FTX collapsed near the end of 2022, its founder was accused of fraud, and Bitcoin’s price plunged below $16,000 — more than 50% below the level at which Barkley had invested.As he scrolled through the taunts on social media, Barkley picked up the phone and called Katz, asking the question on everyone’s mind: “What is going on?”Katz told him to hold, and Barkley did. When I ask about embracing risk in that moment, he likens it to football: one week you’re celebrated, the next you’re vilified. Volatility, he says, is temporary.That discipline is rooted in his partnership with Katz. The two met when Barkley was still at Penn State and Katz was 24, hustling to break into sports management. A decade later, Katz is still his consigliere who pushes him to think like a contrarian.Katz has consistently advised Barkley to use his influence to win deals. “My thesis has always been: Use the fame to get equity in the companies of the best founders in the world,” Katz says.The result is an unusually hands-on approach. Barkley keeps his circle small: just Katz, his financial adviser, and the founders he backs.Compare that to NBA star Kevin Durant, whose investment firm 35V operates with more institutional rigor and a tilt toward safer asset classes like private equity and real estate. “You’re still getting ownership, but it’s just not as risky as venture,” says co-founder Rich Kleiman.While Durant’s portfolio spans more than 100 companies, Barkley approaches investing with the focus of a rogue founder. He doesn’t have a fund structure, and he doesn’t neatly follow a playbook. He makes direct bets on companies that excite him. The strategy is lean, contrarian, and far more exposed.That ethos resonates with Russell Okung, the first NFL player to take part of his salary in Bitcoin.His advice for Barkley? “You’re not an athlete-investor, you’re an investor who allocates your time playing sports. The old model extracted value from athletes. The new model lets athletes extract value from everything else.” He adds, “Why be talent when you can be management? The cap table is where real wealth lives. It's where upside multiplies.”It’s a mindset Barkley has adopted fully, though it comes with a healthy dose of risk. Where most of us see volatility as a warning sign, he recognizes it as something familiar.Growing up without a safety net taught him that uncertainty can be an opening. “I’m not going to just jump into a pool of sharks,” he says, “but I do have a little bit of: ‘I don’t give a you-know-what.’”Barkley celebrating with his daughter Jada (Photo Credit: Sharif Fennell Jr.)Two days before the NFL draft, Barkley became a father. His daughter, Jada, arrived before the cameras and before the multimillion-dollar contract.He remembers looking into her eyes and seeing the future flash before his. In that moment, he realized he wasn’t just building for himself anymore. He was building a world she could inherit.“I was 21 when she was born,” he says, as Jada chatters in the car beside him. “I have a different dynamic with my kids than my parents had with me because I want to teach them how to be more financially literate. I’m still trying my best to learn, but as I learn, they’ll learn too.”To Barkley, investing is about one thing: control. He is trying to manufacture permanence while playing a game defined by uncertainty. His ultimate driver, he says, is his family: his fiancée, Anna, and his kids. “If you had asked me this question two years ago, then my answer would’ve been, ‘I want to be the best football player,’ but it’s not about just that anymore.”That shift is born out of urgency. Football is the most clock-driven sport, and his position is the most precarious. Does he feel the same countdown in his own career?“It’s funny you ask me that,” Barkley says. “Because that’s something that was on my mind this morning. I was just thinking about how I can only play for so long, so I really gotta take advantage, keep investing, and create wealth for me and my family.”He knows the day is coming when his body will tell him the game is over. But when his children ask what he built, he doesn’t only want to point to highlight reels or rushing titles. He wants to point to ownership — proof that he wrestled fragility into permanence.That is the paradox of Saquon Barkley: a man willing to gamble with risk in his life so that his kids never have to feel the sharp edge of uncertainty in theirs.-Written by Polina Pompliano | Edited by Laura Entis / Photos by Sharif Fennell Jr. / Video by Matt MarlinskiRyan Serhant Won’t Stop Until He’s No. 1·May 7In the greenroom at the TODAY show, Ryan Serhant, dressed in a tailored sage suit and sporting his signature gray hair, flips through notes as producers buzz around him.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Artie (YC S23) Is Hiring Engineers, AES, and Senior PMM]]></title>
            <link>https://www.ycombinator.com/companies/artie/jobs</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129442</guid>
            <description><![CDATA[Jobs at Artie]]></description>
            <content:encoded><![CDATA[Software that streams data from databases to warehouses in real-timeJobs at ArtieSan Francisco, CA, US$110K - $130K0.10%3+ yearsSan Francisco, CA, US$145K - $185K0.20% - 0.40%3+ yearsSan Francisco, CA, US$150K - $215K0.50% - 1.00%3+ yearsSan Francisco, CA, US$150K - $215K0.50% - 1.00%3+ yearsWhy you should join ArtieWe are building Artie, a real-time data streaming solution focused on databases and data warehouses. Typical ETL solutions leverage batched processes or schedulers (DAGs, Airflow), which cannot achieve real time data syncs. We leverage change data capture (CDC) and stream processing to perform data transfers in a more efficient way, which enables sub-minute latency.Founded:2023Batch:S23Team Size:8Status:ActiveLocation:San FranciscoFounders]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Wal3: A Write-Ahead Log for Chroma, Built on Object Storage]]></title>
            <link>https://trychroma.com/engineering/wal3</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129369</guid>
        </item>
        <item>
            <title><![CDATA[A PM's Guide to AI Agent Architecture]]></title>
            <link>https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129237</guid>
            <description><![CDATA[A complete guide to agent architecture, orchestration patterns, trust strategies, and adoption plans for PMs building AI agents.]]></description>
            <content:encoded><![CDATA[Last week, I was talking to a PM who'd in the recent months shipped their AI agent. The metrics looked great: 89% accuracy, sub-second respond times, positive user feedback in surveys. But users were abandoning the agent after their first real problem, like a user with both a billing dispute and a locked account."Our agent could handle routine requests perfectly, but when faced with complex issues, users would try once, get frustrated, and immediately ask for a human."This pattern is observed across every product team that focuses on making their agents "smarter" when the real challenge is making architectural decisions that shape how users experience and begin to trust the agent. In this post, I'm going to walk you through the different layers of AI agent architecture. How your product decisions determine whether users trust your agent or abandon it. By the end of this, you'll understand why some agents feel "magical" while others feel "frustrating" and more importantly, how PMs should architect for the magical experience.We'll use a concrete customer support agent example throughout, so you can see exactly how each architectural choice plays out in practice. We’ll also see why the counterintuitive approach to trust (hint: it's not about being right more often) actually works better for user adoption.You're the PM building an agent that helps users with account issues - password resets, billing questions, plan changes. Seems straightforward, right?But when a user says "I can't access my account and my subscription seems wrong" what should happen?Scenario A: Your agent immediately starts checking systems. It looks up the account, identifies that the password was reset yesterday but the email never arrived, discovers a billing issue that downgraded the plan, explains exactly what happened, and offers to fix both issues with one click.Scenario B: Your agent asks clarifying questions. "When did you last successfully log in? What error message do you see? Can you tell me more about the subscription issue?" After gathering info, it says "Let me escalate you to a human who can check your account and billing."Same user request. Same underlying systems. Completely different products.Think of agent architecture like a stack where each layer represents a product decision you have to make.The Decision: How much should your agent remember, and for how long?This isn't just technical storage - it's about creating the illusion of understanding. Your agent's memory determines whether it feels like talking to a robot or a knowledgeable colleague.For our support agent: Do you store just the current conversation, or the customer's entire support history? Their product usage patterns? Previous complaints? Types of memory to consider:Session memory: Current conversation ("You mentioned billing issues earlier...")Customer memory: Past interactions across sessions ("Last month you had a similar issue with...")Behavioral memory: Usage patterns ("I notice you typically use our mobile app...")Contextual memory: Current account state, active subscriptions, recent activityThe more your agent remembers, the more it can anticipate needs rather than just react to questions. Each layer of memory makes responses more intelligent but increases complexity and cost.The Decision: Which systems should your agent connect to, and what level of access should it have?The deeper your agent connects to user workflows and existing systems, the harder it becomes for users to switch. This layer determines whether you're a tool or a platform.For our support agent: Should it integrate with just your Stripe’s billing system, or also your Salesforce CRM, ZenDesk ticketing system , user database, and audit logs? Each integration makes the agent more useful but also creates more potential failure points - think API rate limits, authentication challenges, and system downtime.Here's what's interesting - Most of us get stuck trying to integrate with everything at once. But the most successful agents started with just 2-3 key integrations and added more based on what users actually asked for.The Decision: Which specific capabilities should your agent have, and how deep should they go?Your skills layer is where you win or lose against competitors. It's not about having the most features - it's about having the right capabilities that create user dependency.For our support agent: Should it only read account information, or should it also modify billing, reset passwords, and change plan settings? Each additional skill increases user value but also increases complexity and risk.Implementation note: Tools like MCP (Model Context Protocol) are making it much easier to build and share skills across different agents, rather than rebuilding capabilities from scratch. The Decision: How do you measure success and communicate agent limitations to users?This layer determines whether users develop confidence in your agent or abandon it after the first mistake. It's not just about being accurate - it's about being trustworthy.For our support agent: Do you show confidence scores ("I'm 85% confident this will fix your issue")? Do you explain your reasoning ("I checked three systems and found...")? Do you always confirm before taking actions ("Should I reset your password now?")? Each choice affects how users perceive reliability.Trust strategies to consider:Confidence indicators: "I'm confident about your account status, but let me double-check the billing details"Reasoning transparency: "I found two failed login attempts and an expired payment method"Graceful boundaries: "This looks like a complex billing issue - let me connect you with our billing specialist who has access to more tools"Confirmation patterns: When to ask permission vs. when to act and explainThe counterintuitive insight: users trust agents more when they admit uncertainty than when they confidently make mistakes.Okay, so you understand the layers. Now comes the practical question that every PM asks: "How do I actually implement this? How does the agent talk to the skills? How do skills access data? How does evaluation happen while users are waiting?"Your orchestration choice determines everything about your development experience, your debugging process, and your ability to iterate quickly.Lets walk through the main approaches, and I'll be honest about when each one works and when it becomes a nightmare.Everything happens in one agent's context. For our support agent: When the user says "I can't access my account," one agent handles it all - checking account status, identifying billing issues, explaining what happened, offering solutions. Why this works: Simple to build, easy to debug, predictable costs. You know exactly what your agent can and can't do.Why it doesn't: Can get expensive with complex requests since you're loading full context every time. Hard to optimize specific parts.Most teams start here, and honestly, many never need to move beyond it. If you're debating between this and something more complex, start here.You have a router that figures out what the user needs, then hands off to specialized skills.For our support agent: Router realizes this is an account access issue and routes to the `LoginSkill`. If the LoginSkill discovers it's actually a billing problem, it hands off to `BillingSkill`.Real example flow:User: "I can't log in"Router → LoginSkillLoginSkill checks: Account exists ✓, Password correct ✗, Billing status... wait, subscription expiredLoginSkill → BillingSkill: "Handle expired subscription for user123"BillingSkill handles renewal processWhy this works: More efficient - you can use cheaper models for simple skills, expensive models for complex reasoning. Each skill can be optimized independently.Why it doesn't: Coordination between skills gets tricky fast. Who decides when to hand off? How do skills share context?Here's where MCP really helps - it standardizes how skills expose their capabilities, so your router knows what each skill can do without manually maintaining that mapping.You predefine step-by-step processes for common scenarios. Think LangGraph, CrewAI, AutoGen, N8N, etc.For our support agent: "Account access problem" triggers a workflow:Check account statusIf locked, check failed login attempts  If too many failures, check billing statusIf billing issue, route to payment recoveryIf not billing, route to password resetWhy this works: Everything is predictable and auditable. Perfect for compliance-heavy industries. Easy to optimize each step.Why it doesn't: When users have weird edge cases that don't fit your predefined workflows, you're stuck. Feels rigid to users.Multiple specialized agents work together using A2A (agent-to-agent) protocols. The vision: Your agent discovers that another company's agent can help with issues, automatically establishes a secure connection, and collaborates to solve the customer's problem. Think a booking.com agent interacting with an American Airlines agent! For our support agent: `AuthenticationAgent` handles login issues, `BillingAgent` handles payment problems, `CommunicationAgent` manages user interaction. They coordinate through standardized protocols to solve complex problems.Reality check: This sounds amazing but introduces complexity around security, billing, trust, and reliability that most companies aren't ready for. We're still figuring out the standards.This can produce amazing results for sophisticated scenarios, but debugging multi-agent conversations is genuinely hard. When something goes wrong, figuring out which agent made the mistake and why is like detective work.Here's the thing: start simple. Single-agent architecture handles way more use cases than you think. Add complexity only when you hit real limitations, not imaginary ones.But here's what's interesting - even with the perfect architecture, your agent can still fail if users don't trust it. That brings us to the most counterintuitive lesson about building agents.Here's something counterintuitive: Users don't trust agents that are right all the time. They trust agents that are honest about when they might be wrong.Think about it from the user's perspective. Your support agent confidently says "I've reset your password and updated your billing address." User thinks "great!" Then they try to log in and... it doesn't work. Now they don't just have a technical problem - they have a trust problem.Compare that to an agent that says "I think I found the issue with your account. I'm 80% confident this will fix it. I'm going to reset your password and update your billing address. If this doesn't work, I'll immediately escalate to a human who can dive deeper."Same technical capability. Completely different user experience.Building trusted agents requires focus on three things:Confidence calibration: When your agent says it's 60% confident, it should be right about 60% of the time. Not 90%, not 30%. Actual 60%.Reasoning transparency: Users want to see the agent's work. "I checked your account status (active), billing history (payment failed yesterday), and login attempts (locked after 3 failed attempts). The issue seems to be..."Graceful escalation: When your agent hits its limits, how does it hand off? A smooth transition to a human with full context is much better than "I can't help with that."A lot of times we obsess over making agents more accurate, when what users actually want was more transparency about the agent's limitations.In Part 2, I'll dive deeper into the autonomy decisions that keep most PMs up at night. How much independence should you give your agent? When should it ask for permission vs forgiveness? How do you balance automation with user control?We'll also walk through the governance concerns that actually matter in practice - not just theoretical security issues, but the real implementation challenges that can make or break your launch timeline.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Age Simulation Suit]]></title>
            <link>https://www.age-simulation-suit.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129190</guid>
            <description><![CDATA[The GERontologic Test suit GERT creates the experience old age. GERT is the age simulation suit for universities, seminaries, institutes and companies.]]></description>
            <content:encoded><![CDATA[



GERonTologic simulator GERT
The age simulation suit GERT offers the opportunity to experience the impairments of older persons even for younger people.

The age-related impairments are:
■  opacity of the eye lens
■  narrowing of the visual field
■  high-frequency hearing loss
■  head mobility restrictions
■  joint stiffness
■  loss of strength
■  reduced grip ability
■  reduced coordination skills
GERT for only  � 1390,‑ / � 1250,-
complete as pictured, plus shipping and VAT if applicable
New: now with 2 pairs of glasses instead of the model shown






Due to the significant increase in the time and effort required to process orders, in particular as a result of incomplete or incorrect information provided with orders, and the fact that we increasingly have to send reminders for invoices for smaller amounts, we can only accept orders with a value of at least 300 euros or pounds.

Customer reviews:
The quality is great and it works how it is supposed to. I�m happy with my purchase.
Great way to teach about elderly behavior. I�ve been using this suit for a while now and it�s very durable and easy to use. Thanks!!














For many years ourage simulation suitGERT has been byfar the most popularproduct worldwide.The EuropeanCompetence Centrefor Accessibility hascertified our agesimulation suit GERT.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How we built an interpreter for Swift]]></title>
            <link>https://www.bitrig.app/blog/swift-interpreter</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129160</guid>
            <description><![CDATA[Build apps for your phone, on your phone.]]></description>
            <content:encoded><![CDATA[Bitrig dynamically generates and runs Swift apps on your phone. Normally this would require compiling and signing with Xcode, and you can’t do that on an iPhone.
To make it possible to instantly run your app, we built a Swift interpreter. But it’s an unusual interpreter, since it interprets from Swift… to Swift. One of the top questions we’ve gotten is how it’s implemented, so we wanted to share how it works. To make this more accessible and interesting, we simplified some of the more esoteric details. But we hope you’ll come away with a high-level picture of how the interpreter works.
The Swift project helpfully provides a way to reuse all of the parsing logic from the compiler: SwiftSyntax. This made our job a lot easier. We can easily take some Swift code and get a parsed tree out of it, which we can use to evaluate and call into to get dynamic runtime values. Let’s dig deeper.
We can start with generating the simplest kind of runtime values. For any literals (strings, floating point numbers, integers, and booleans), we can create corresponding Swift instances (String, Double, Int, Bool) to represent them. Since we’re not compiling this, we don’t know ahead of time what all the types will be, so we need to type erase all instances. Let’s make an enum to represent these runtime interpreter values (since we'll have multiple kinds soon):
enum InterpreterValue {
    case nativeValue(Any)
}

Next, we'll expand our interpreter runtime values to be able to represent developer-defined types, too. Let’s say we have a struct with two fields: a string and an integer. We’ll store it as a type that has a dictionary mapping from the property name to the runtime value. When an initializer gets called, we simply need to map the arguments to the property names and populate the dictionary.
enum InterpreterValue {
    case nativeValue(Any)
    case customInstance(CustomInstance)
}
struct CustomInstance {
    var type: InterpreterType
    var values: [String: InterpreterValue]
}

But, what happens when we want to call an API that comes from a framework, like SwiftUI? For example, let’s say we have a call to Text("Hello World"). We don’t want to rewrite all of the APIs, the whole benefit of making a native app is being able to call into those implementations! Well, those APIs are available for us to call into since the interpreter is also written in Swift (naturally!). We just need to change from a dynamic call to a compiled one. We can do that by pre-compiling a call to the Text initializer that can take dynamic arguments. Something like this:
func evaluateTextInitializer(arguments: [Argument]) -> Text {
  Text(arguments.first?.value.stringValue ?? "")
}

But of course, we need more than just the Text initializer, so we'll generalize this to any initializer we might be called with:
func evaluateInitializer(type: String, arguments: [Argument]) -> Any? {
  if type == "Text" {
    return Text(arguments.first?.value.stringValue ?? "")
  } else if type == "Image" {
    return Image(arguments.first?.value.stringValue ?? "")
  ...
}

We can follow this same pattern for all other API types: function calls, properties, subscripts, etc. The difficult part is that there are a lot of APIs. It’s not practical to hand-write code to call into all of them, but fortunately there is a structured list of all of them: the .swiftinterface file for each framework. We can parse those files to get a list of all of the APIs we need and then generate the necessary code to call into them.
One interesting thing about taking this approach to its most extreme is that even very basic operations that you might expect any interpreter to implement, like basic numeric operations, can still call into their framework implementations. So this kind of interpreter doesn’t know how to calculate or evaluate anything itself, and is really more of a glorified foreign function interface, but from dynamic Swift to compiled Swift.
One last important challenge is how to make custom types conform to framework protocols. For example, how do we make a custom SwiftUI View? Well, at runtime, we need a concrete type that conforms to the desired protocol. To do this, we can make stub types that conform to the protocol, but instead of having any logic of their own, simply call out to the interpreter to implement any requirements. Let’s look at Shape, a simple example:
struct ShapeStub: Shape {
    var interpreter: Interpreter
    var instance: CustomInstance

    var layoutDirectionBehavior: LayoutDirectionBehavior {
        instance.instanceMemberProperty("layoutDirectionBehavior", interpreter: interpreter).layoutDirectionBehaviorValue
    }
    func path(in rect: CGRect) -> Path {
        let arguments = [Argument(label: "in", value: rect)]
        return instance.instanceFunction("path", arguments: arguments, interpreter: interpreter).pathValue
    }
}

Coming back to View, this works the same way, with a little extra complexity because of the associated type that we have to type erase:
struct ViewStub: View {
    var interpreter: Interpreter
    var instance: CustomInstance

    var body: AnyView {
        instance.instanceMemberProperty("body", interpreter: interpreter).viewValue
            .map { AnyView($0) }
  }
}

And now we can make views that have dynamic implementations!
That’s a broad survey of how the interpreter is implemented. If you want to try it out in practice, download Bitrig!
If there’s more you want to know about the interpreter, or Bitrig as a whole, let us know!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Stripe Launches L1 Blockchain: Tempo]]></title>
            <link>https://tempo.xyz</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129085</guid>
            <description><![CDATA[A neutral, permissionless blockchain optimized for payment processing, enabling efficient transactions with predictable low fees and stablecoin interoperability.]]></description>
            <content:encoded><![CDATA[Why create anew blockchain?Stablecoins enable instant, borderless, programmable transactions, but current blockchain infrastructure isn’t designed for them: existing systems are either fully general or trading-focused. Tempo is a blockchain designed and built for real-world payments.Optimized forreal-world flowsTempo was started by Stripe and Paradigm, with design input from Anthropic, Coupang, Deutsche Bank, DoorDash, Lead Bank, Mercury, Nubank, OpenAI, Revolut, Shopify, Standard Chartered, Visa, and more.If you’re a company with large, real-world economic flows and would like to help shape the future of Tempo, get in touch.Partner with usTransform how your business  moves money01 :: Purpose-built payments capabilitiesOptimize your financial flows with embedded payment features, including memo fields and batch transfers.02 :: Speed and reliabilityProcess over 100,000 transactions per second (TPS) with sub-second finality, enabling real-time payments at a global scale.03 :: Predictable low feesTransform your cost structure with near-zero transaction fees that are highly predictable and can be paid in any stablecoin.04 :: Built-in privacy measuresProtect your users by keeping important transaction details private while maintaining compliance standards.Performant and scalable for any payments use case01 :: RemittancesSend money across borders instantly, securely, and at a fraction of traditional costs.02 :: Global payoutsPay anyone, anywhere, in any currency—without banking delays or fees.03 :: Embedded financeBuild compliant, programmable payments—in any stablecoin—directly into your products.04 :: MicrotransactionsEnable sub-cent payments for digital goods and on-demand services.05 :: Agentic commerceFacilitate low-cost, instant payments for agents to autonomously execute transactions.06 :: Tokenized depositsMove customer funds onchain for instant settlement and efficient interbank transfers.Technicalfeatures01 :: Fee flexibilityPay transaction fees in any stablecoin.02 :: Dedicated payments laneTransfer funds cheaply and reliably in blockspace that’s isolated from other activity.03 :: Stablecoin interoperabilitySwap stablecoins, including custom-issued ones, natively with low fees.04 :: Batch transfersSend multiple transactions onchain at once with native account abstraction.05 :: Blocklists / allowlistsMeet compliance standards by setting user-level permissions for transactions.06 :: Memo fieldsSpeed up reconciliation with offchain transactions by adding context that’s compatible with ISO 20022 standards.Frequentlyasked questions01 :: How is Tempo different from other blockchains?Tempo is an EVM-compatible L1 blockchain, purpose-built for payments. It doesn’t displace other general-purpose blockchains; rather, it incorporates design choices that meet the needs of high-volume payment use cases. These include predictable low fees in a dedicated payments lane, stablecoin neutrality, a built-in stablecoin exchange, high throughput, low latency, private transactions, payment memos compatible with standards like ISO 20022, compliance hooks, and more.02 :: Who can build on Tempo?Tempo is a neutral, permissionless blockchain open for anyone to build on. We’re currently collaborating with global partners to test various use cases, including cross-border payouts, B2B payments, remittances, and ecommerce. Interested in working with Tempo? Request access to our private testnet here.03 :: When will Tempo launch?We’re providing select partners with priority access to our testnet now. Contact us here if you’re interested.04 :: Who will run validator nodes?A diverse group of independent entities, including some of Tempo’s design partners, will run validator nodes initially before we transition to a permissionless model.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Slashy (YC S25) – AI that connects to apps and does tasks]]></title>
            <link>https://news.ycombinator.com/item?id=45129031</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129031</guid>
            <description><![CDATA[Hi HN! – We’re Pranjali, Dhruv and Harsha, building Slashy (https://www.slashy.ai). We’re building a general agent that connects to apps and can read data across them and perform actions via custom tools, semantic search, and personalized memory. Here’s a demo: https://www.youtube.com/watch?v=OeApHMHhccA.]]></description>
            <content:encoded><![CDATA[Launch HN: Slashy (YC S25) – AI that connects to apps and does tasks49 points by hgaddipa001 6 hours ago  | hide | past | favorite | 76 commentsHi HN! – We’re Pranjali, Dhruv and Harsha, building Slashy (https://www.slashy.ai). We’re building a general agent that connects to apps and can read data across them and perform actions via custom tools, semantic search, and personalized memory. Here’s a demo: https://www.youtube.com/watch?v=OeApHMHhccA.While working on a previous startup, we realized we were spending more time doing busywork in apps than actually building product. We lost hundreds of hours scraping LinkedIn profiles, updating spreadsheets, updating investor reports, and communicating across multiple Slack channels. Our breaking point happened after I checked my screen time and realized I spent 4 hours a day in Gmail. We decided that we could create more value solving this than by working on the original startup (a code generation agent similar to Lovable).Slashy is an AI agent that uses direct tool calls to services such as Gmail, Calendar, Notion, Sheets and more. We built all of our tools in-house since we found that most MCPs are low quality and add an unnecessary layer of abstraction. Through these tools, the agent is able to semantically search across your apps, get relevant information, and perform actions (e.g. send emails, create calendar events, etc). This solves the problem of context-switching and copy-pasting information from an app back and forth into ChatGPT.Slashy integrates to 15 different services so far (G-Suite, Slack, Notion, Dropbox, Airtable, Outlook, Phone, Linear, Hubspot, and more). We use a single agent architecture (as we found this reduces hallucinations), and use our own custom tools—doing so allows the model to have higher quality as we can design them to work in a general agent structure, for example we use markdown for Slack/Notion instead of their native text structure.So what makes Slashy different from the 100 other general agents?- It Actually Takes Action: Unlike ChatGPT or Claude that just give you information, Slashy researches companies, creates Google Docs with findings, adds contacts to your CRM, schedules follow-ups, and sends personalized emails – all in one workflow.- Cross-Tool Context: Most automation tools work in silos (one of the biggest problems with MCP). Slashy understands your data across platforms. It can read your previous Slack conversations about a prospect, check your calendar for availability, research their company online, and draft a personalized email. What powers this is our own semantic search functionality.- User Action Graphs: Our agent over time has memory not just of past conversations, but also forms user actions graphs to know what actions are expected based on previous user conversations.- No Technical Setup Required: While Zapier requires building complex flows and fails silently, Slashy works through natural language. Just describe what you want automated.- Custom UI: For our tool calls we design custom UI for each of them to make the UX more natural.Here are some examples of workflows people use us for:▪ "Every day look at my calendar and send me a notion doc with in-depth backgrounds on everyone I’m meeting"▪ "Find the emails of everyone who reacted to my latest LinkedIn post and send personalized outreach"▪ "Can you make me an investor pitch deck with market research, competitive analysis, and financial projections"▪ "Doing a full Nvidia Discounted Cash Flow (DCF) analysis"Slashy.ai is live with a free tier (100 daily credits) along with 500 credits for any new account. You can immediately try out workflows like the ones above and we have a special code for HN (HACKERNEWS at checkout).Hope you all enjoy Slashy as much as we do :)]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Farewell to Meshnet]]></title>
            <link>https://nordvpn.com/blog/meshnet-shutdown/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45128299</guid>
        </item>
        <item>
            <title><![CDATA[WiFi signals can measure heart rate]]></title>
            <link>https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45127983</guid>
            <description><![CDATA[Engineers prove their technique is effective even with the lowest-cost WiFi devices]]></description>
            <content:encoded><![CDATA[











Key takeaways




The Pulse-Fi system is highly accurate, achieving clinical-level heart rate monitoring with ultra low-cost WiFi devices, making it useful for low resource settings. 



The system works with the person in a variety of different positions and from up to 10 feet away.





Heart rate is one of the most basic and important indicators of health, providing a snapshot into a person’s physical activity, stress and anxiety, hydration level, and more.



Traditionally, measuring heart rate requires some sort of wearable device, whether that be a smart watch or hospital-grade machinery. But new research from engineers at the University of California, Santa Cruz, shows how the signal from a household WiFi device can be used for this crucial health monitoring with state-of-the-art accuracy—without the need for a wearable.



Their proof of concept work demonstrates that one day, anyone could take advantage of this non-intrusive WiFi-based health monitoring technology in their homes. The team proved their technique works with low-cost WiFi devices, demonstrating its usefulness for low resource settings.



A study demonstrating the technology, which the researchers have coined “Pulse-Fi,” was published in the proceedings of the 2025 IEEE International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT).



Measuring with WiFi



Professor of Computer Science and Engineering Katia Obraczka and Ph.D. student Nayan Bhatia in the lab.



A team of researchers at UC Santa Cruz’s Baskin School of Engineering that included Professor of Computer Science and Engineering Katia Obraczka, Ph.D. student Nayan Bhatia, and high school student and visiting researcher Pranay Kocheta designed a system for accurately measuring heart rate that combines low-cost WiFi devices with a machine learning algorithm.



WiFi devices push out radio frequency waves into physical space around them and toward a receiving device, typically a computer or phone. As the waves pass through objects in space, some of the wave is absorbed into those objects, causing mathematically detectable changes in the wave.



Pulse-Fi uses a WiFi transmitter and receiver, which runs Pulse-Fi’s signal processing and machine learning algorithm. They trained the algorithm to distinguish even the faintest variations in signal caused by a human heart beat by filtering out all other changes to the signal in the environment or caused by activity like movement.



“The signal is very sensitive to the environment, so we have to select the right filters to remove all the unnecessary noise,” Bhatia said.




High school student Pranay Kocheta joined the Pulse-Fi project as a researcher through UC Santa Cruz’s Science Internship Program.




 Dynamic results



The team ran experiments with 118 participants and found that after only five seconds of signal processing, they could measure heart rate with clinical-level accuracy. At five seconds of monitoring, they saw only half a beat-per-minute of error, with longer periods of monitoring time increasing the accuracy.



The team found that the Pulse-Fi system worked regardless of the position of the equipment in the room or the person whose heart rate was being measured—no matter if they were sitting, standing, lying down, or walking, the system still performed. For each of the 118 participants, they tested 17 different body positions with accurate results



These results were found using ultra-low-cost ESP32 chips, which retail between $5 and $10 and Raspberry Pi chips, which cost closer to $30. Results from the Raspberry Pi experiments show even better performance. More expensive WiFi devices like those found in commercial routers would likely further improve the accuracy of their system.



They also found that their system had accurate performance with a person three meters, or nearly 10 feet, away from the hardware. Further testing beyond what is published in the current study shows promising results for longer distances.



“What we found was that because of the machine learning model, that distance apart basically had no effect on performance, which was a very big struggle for past models,” Kocheta said. “The other thing was position—all the different things you encounter in day to day life, we wanted to make sure we were robust to however a person is living.”



Creating the dataset




The researchers proved their heart rate monitoring technique works with ultra-low-cost, WiFi-emitting ESP32 chips, which retail between $5 and $10.




To make their heart rate detection system work, the researchers needed to train their machine learning algorithm to distinguish the faint detections in WiFi signals caused by a human heartbeat. They found that there was no existing data for these patterns using an ESP32 device, so they set out to create their own dataset.



In the UC Santa Cruz Science and Engineering library, they set up their ESP32 system along with a standard oximeter to gather “ground truth” data. By combining the data from the Pulse-Fi setup with the ground truth data, they could teach a neural network which changes in signals corresponded with heart rate.



In addition to the ESP32 dataset they collected, they also tested Pulse-Fi using a dataset produced by a team of researchers in Brazil using a Raspberry Pi device, which created the most extensive existing dataset on WiFi for heart monitoring, as far as the researchers are aware.



Beyond heart rate



Now, the researchers are working on further research to extend their technique to detect breathing rate in addition to heart rate, which can be useful for the detection of conditions like sleep apnea. Unpublished results show high promise for accurate breathing rate and apnea detection.



Those interested in commercial use of this technology can contact Assistant Director of Innovation Transfer Marc Oettinger: marc.oettinger@ucsc.edu.










]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Almost anything you give sustained attention to will begin to loop on itself]]></title>
            <link>https://www.henrikkarlsson.xyz/p/attention</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45126503</guid>
            <description><![CDATA[When people talk about the value of paying attention and slowing down, they often make it sound prudish and monk-like. But we shouldn’t forget how interesting and overpoweringly pleasurable sustained attention can be.]]></description>
            <content:encoded><![CDATA[Brioches and Knife, Eliot Hodgkin, 08/1961When people talk about the value of paying attention and slowing down, they often make it sound prudish and monk-like. Attention is something we “have to protect.” And we have to “pay”1 attention—like a tribute.But we shouldn’t forget how interesting and overpoweringly pleasurable sustained attention can be. Slowing down makes reality vivid, strange, and hot.Let me start with the most obvious example.As anyone who has had good sex knows, sustained attention and delayed satisfaction are a big part of it. When you resist the urge to go ahead and get what you want and instead stay in the moment, you open up a space for seduction and fantasy. Desire begins to loop on itself and intensify.I’m not sure what is going on here, but my rough understanding is that the expectation of pleasure activates the dopaminergic system in the brain. Dopamine is often portrayed as a pleasure chemical, but it isn’t really about pleasure so much as the expectation that pleasure will occur soon. So when we are being seduced and sense that something pleasurable is coming—but it keeps being delayed, and delayed skillfully—the phasic bursts of dopamine ramp up the levels higher and higher, pulling more receptors to the surface of the cells, making us more and more sensitized to the surely-soon-to-come pleasure. We become hyperattuned to the sensations in our genitals, lips, and skin.And it is not only dopamine ramping up that makes seduction warp our attentional field, infusing reality with intensity and strangeness. There are a myriad of systems that come together to shape our feeling of the present: there are glands and hormones and multiple areas of the brain involved. These are complex physical processes: hormones need to be secreted and absorbed; working memory needs to be cleared and reloaded, and so on. The reason deep attention can’t happen the moment you notice something is that these things take time.What’s more, each of these subsystems update what they are reacting to at a different rate. Your visual cortex can cohere in less than half a second. A stress hormone like cortisol, on the other hand, has a half-life of 60–90 minutes and so can take up to 6 hours to fully clear out after the onset of an acute stressor. This means that if we switch what we pay attention to more often than, say, every 30 minutes, our system will be more or less decohered—different parts will be “attending to” different aspects of reality.2 There will be “attention residue” floating around in our system—leftovers from earlier things we paid attention to (thoughts looping, feelings circling below consciousness, etc.), which crowd out the thing we have in front of us right now, making it less vivid.Inversely, the longer we are able to sustain the attention without resolving it and without losing interest, the more time the different systems of the body have to synchronize with each other, and the deeper the experience gets.Locked in on the same thing, the subsystems begin to reinforce each other: the dopamine makes us aware of our skin, and sensations on the skin ramp up dopamine release, making us even more aware of our skin. A finger touches our belly, and we start to fantasize about where that finger might be going; and so now our fantasies are locked in, too, releasing even more dopamine and making us even more aware of our skin. The more the subsystems lock in, the more intense the feedback loops get. After twenty minutes, our sense of self has evaporated, and we’re in a realm where we do, feel, and think things that would seem surreal in other contexts.Similar things happen when we are able to sustain our attention to things other than sex, too. The exact mechanics differ, I presume, but the basic pattern is that when we let our attention linger on something, our bodily systems synchronize and feed each other stimuli in an escalatory loop that restructures our attentional field.Almost anything that we are able to direct sustained attention at will begin to loop on itself and bloom.To take a dark example, if you focus on your anxiety, the anxiety can begin to loop on itself until you hyperventilate and get tunnel vision and become filled with nightmarish thoughts and feelings—a panic attack.And you do the same thing with joy. If you learn to pay sustained attention to your happiness, the pleasant sensation will loop on itself until it explodes and pulls you into a series of almost hallucinogenic states, ending in cessation, where your consciousness lets go and you disappear for a while. This takes practice. The practice is called jhanas, and it is sometimes described as the inverse of a panic attack. I have only ever entered the first jhana, once while spending an hour putting our four-year-old to sleep and meditating on how wonderful it is to lie there next to her. It was really weird and beautiful. If you want to know more about these sorts of mental states, I recommend José Luis Ricón Fernández de la Puente’s recent write-up of his experiences, Nadia Asparouhova on her experiences, and her how-to guide.Here is José, whose blog is normally detailed reflections on cell biology and longevity and metascience, describing the second evening of a jhana retreat:So I went down to the beach. “Kinda nice”, I thought. The sky had a particularly vibrant blue color, the waves had ‘the right size’, their roar was pleasant. I started to walk around trying to continue meditating. I focused my awareness on an arising sensation of open heartedness and then I noticed my eyes tearing up (“Huh? I thought”). I looked again at the ocean and then I saw it. It was fucking amazing. So much color and detail: waves within waves, the fractal structure of the foamy crests as they disintegrate back into the ocean. The feeling of the sun on my skin. I felt overwhelmed. As tears ran down my face and lowkey insane grin settled on my face I found myself mumbling “It’s... always been like this!!!!” “What the fuck??!” followed by “This is too much!! Too much!!!”. The experience seemed to be demanding from me to feel more joy and awe than I was born to feel or something like that. In that precise moment I felt what “painfully beautiful” means for the first time in my life.The fact that we can enter fundamentally different, and often exhilarating, states of mind by learning how to sustain our attention is fascinating. It makes you wonder what other states are waiting out there. What will happen if you properly pay attention to an octopus?3 What about your sense of loneliness?4 A mathematical idea?5 The weights of a neural net?6 The footnotes here take you to examples of people who have done that. There are so many things to pay attention to and experience.One of my favorite things to sustain attention toward is art.There was a period in my twenties when I didn’t get art. I thought artists were trying to say something, but I felt superior because I thought there had to be better ways of getting their ideas across (and also, better ideas). But then I realized that good art—at least the art I am spontaneously drawn to—has little to do with communication. Instead, it is about crafting patterns of information that, if you feed them sustained attention, will begin to structure your attentional field in interesting ways. Art is guided meditation. The point isn’t the words, but what happens to your mind when you attend to those words (or images, or sounds). There is nothing there to understand; it is just something to experience, like sex. But the experiences can be very deep and, sometimes, transformative.In 2019, for example, I saw a performance of Jean Sibelius’s 5th Symphony at the University Hall in Uppsala.Before the concert began, I spent a few minutes with my eyes closed, doing a body scan, to be fully present when the music began. As the horns at the opening of the piece called out, I decided to keep my eyes closed, so I wouldn’t be distracted by looking at the hands of the musicians. Then… a sort of daydream started up. The mood suggested to me the image of a cottage overlooking a sloping meadow and a thick wood of pines, a few hours from Helsinki. It was a pretty obvious image, since I knew that Sibelius wrote the piece at Aniola, which is 38 km north of Helsinki. But then I saw an old man walking up the meadow and into the house. The camera cut. Through an open door, I saw the man, alone, working at a desk. I saw it as clearly as if it had been projected on a screen before me: the camera moved slowly toward the back of the man.Through the window above his desk, I could see a light in the distance. Perhaps it was Helsinki? No, it felt alive, like a being—something alive and growing, something that was headed here. But then again, if you were to see a city from space, watching it sped up by 100,000x, it would look like a being moving through the landscape, spreading, getting closer. The old man sat there for a hundred years, watching the light. There was a sinking feeling in my body.One spring, birds fell dead from the sky. They littered the fields, whole droves of them filled the ditches—blue birds, red birds, and black. The man carried them into his woodshed and placed them in waist-high piles.The film kept going, and the emotional intensity and complexity gradually ramped up. For the thirty minutes that it took the orchestra to play the three movements of the symphony, I experienced what felt like two or three feature films, all interconnected by some strange emotional logic. In the third movement, a group of hunter-gatherers was living in a cave that reminded me of the entrance to a nuclear waste facility. A girl hiding behind a tree saw men with cars arrive…The structure of the music was such that it gave me enough predictability and enough surprise to allow my attention to deeply cohere. The melody lines and harmonies dredged up memories and images from my subconscious, weaving them into a rich cinematic web of stories. Guided by the music, my mind could tunnel into an attentional state where I was able to see things I had never seen before and where I could work through some deep emotional pain that seemed to resolve itself through the images.When the music stopped, I barely knew where I was.I opened my eyes and remembered that my brother was sitting next to me.“What did you think?” I said.“I don’t know,” he said. “I felt kind of restless.”Like always, the research for this essay was funded by the contribution of paying subscribers. Thank you! We wouldn’t have been able to do this without you. If you enjoy the essays and want to support Escaping Flatland, we are not yet fully funded:A special thanks to Johanna Karlsson, Nadia Asparouhova, Packy McCormick, and Esha Rana, who all read and commented on drafts of this essay. The image of the University Hall is by Ann-Sofi Cullhed.If you liked this essay, you might also like:Becoming perceptive·September 10, 2024This is the second part of an essay series that began with “Everything that turned out well in my life followed the same design process.” There is also a third part. It can be read on its own.1In Spanish, you “lend” attention. In Swedish, you “are” attention.2It is not like 30 minutes is some ideal. Attention can, under the right conditions, keep getting deeper and more coherent for much longer, as attested by people who meditate for weeks. Inversely, you can, if you have a well developed dorsal attention network and low cortisol level etc, cohere to a high degree in a few minutes. (Though if you have a lot of stress hormones, thirty minutes will not be nearly enough to get out of a flighty mode of attention.) In other words, I don’t think you can put a precise number at it.Time to coherence depends on your starting place (mood, hormones, chemical make up in the brain), your skill, and the level of coherence you want to pursue. There is a famous study saying it takes people 23 minutes to get to full productivity after an interruption, which seems like it is correlated to the time it takes them to deeply cohere their attentional field. On the other hand, there is also an upper limit at how long you can cohere, which also depends on a bunch of factors. If I’m working on an essay, I notice that the quality of my thinking drops after about 20 minutes of sustained attention and I need to pause for a few minutes and walk around to get back up to full focus. So in my case, my deepest thinking seem to decohere before I even reach that infamous 23 minute mark! And after 3-4 hours, the quality of my attention goes down so much that everything I write ends up being deleted the day after. For more relaxed attention, like meditation, I haven’t reached the limit for how long I can deepen my coherence—after an hour, which is the longest I’ve gone, I’m still shifting deeper into attention.3Charles Darwin:[During our stay in Porto Praya,] I was much interested, on several occasions, by watching the habits of an Octopus, or cuttle-fish. Although common in the pools of water left by the retiring tide, these animals were not easily caught. By means of their long arms and suckers, they could drag their bodies into very narrow crevices; and when thus fixed, it required great force to remove them. At other times they darted tail first, with the rapidity of an arrow, from one side of the pool to the other, at the same instant discolouring the water with a dark chestnut-brown ink. These animals also escape detection by a very extraordinary, chameleon-like power of changing their colour. They appear to vary their tints according to the nature of the ground over which they pass: when in deep water, their general shade was brownish purple, but when placed on the land, or in shallow water, this dark tint changed into one of a yellowish green.The colour, examined more carefully, was a French grey, with numerous minute spots of bright yellow: the former of these varied in intensity; the latter entirely disappeared and appeared again by turns. These changes were effected in such a manner, that clouds, varying in tint between a hyacinth red and a chestnut-brown, were continually passing over the body. Any part, being subjected to a slight shock of galvanism, became almost black: a similar effect, but in a less degree, was produced by scratching the skin with a needle. These clouds, or blushes as they may be called, are said to be produced by the alternate expansion and contraction of minute vesicles containing variously coloured fluids.This cuttle-fish displayed its chameleon-like power both during the act of swimming and whilst remaining stationary at the bottom. I was much amused by the various arts to escape detection used by one individual, which seemed fully aware that I was watching it. Remaining for a time motionless, it would then stealthily advance an inch or two, like a cat after a mouse; sometimes changing its colour: it thus proceeded, till having gained a deeper part, it darted away, leaving a dusky train of ink to hide the hole into which it had crawled.While looking for marine animals, with my head about two feet above the rocky shore, I was more than once saluted by a jet of water, accompanied by a slight grating noise. At first I could not think what it was, but afterwards I found out that it was this cuttle-fish, which, though concealed in a hole, thus often led me to its discovery. That it possesses the power of ejecting water there is no doubt, and it appeared to me that it could certainly take good aim by directing the tube or siphon on the under side of its body. From the difficulty which these animals have in carrying their heads, they cannot crawl with ease when placed on the ground. I observed that one which I kept in the cabin was slightly phosphorescent in the dark.from:  https://www.gutenberg.org/ebooks/9444Sasha Chapin writes: In late winter 2024, I noticed that I wasn’t living up to my stated policy of trying to accept every emotion passing through my system. There were certain shades of existential loneliness that I was pushing away. This was causing some friction. Solitude is simply part of my current life chapter, since Cate is more independent than any of my previous partners, and Berkeley is a place where I don’t feel at home socially.As a response, I made feelings of solitude the central focus of my practice. I tried to become like a sommelier, going out of my way to appreciate all the shades of loneliness that colored my afternoons, trying to zoom in on every micro-pixel and embrace rather than reject.Again—normal. This is what, for me, long-term practice often consists of: noticing when my reactions don’t line up with my principles, and seeing if I can bring myself into deeper alignment.However, I noticed something odd. Dropping the resistance to loneliness allowed me to slip into deeper sensations of flow. It was almost as if the emotional resistance had been preventing the emergence of a more intuitive part of my will. There were a few memorable walks I took where the feeling of solitude felt like a portal into an exquisitely smooth parallel world. When I allowed my emotions to pierce me more deeply, I fell into a different degree of cooperation with reality. Every step felt precise and necessary, like a choreographed dance.5Michael Nielsen writes about this in an essay where he describes the experience of pushing himself to go deeper than usual in understanding a mathematical proof:I gradually internalize the mathematical objects I’m dealing with [using spaced repetition]. It becomes easier and easier to conduct (most of) my work in my head. [. . .] Furthermore, as my understanding of the objects change – as I learn more about their nature, and correct my own misconceptions – my sense of what I can do with the objects changes as well. It’s as though they sprout new affordances, in the language of user interface design, and I get much practice in learning to fluidly apply those affordances in multiple ways. [. . .]After going through the [time-consuming process of deeply understanding a proof,] I had a rather curious experience. I went for a multi-hour walk along the San Francisco Embarcadero. I found that my mind simply and naturally began discovering other facts related to the result. In particular, I found a handful (perhaps half a dozen) of different proofs of the basic theorem, as well as noticing many related ideas. This wasn’t done especially consciously – rather, my mind simply wanted to find these proofs.6Chris Olah writes: Research intimacy is different from theoretical knowledge. It involves internalizing information that hasn’t become part of the “scientific cannon” yet. Observations we don’t (yet) see as important, or haven’t (yet) digested. The ideas are raw.(A personal example: I’ve memorized hundreds of neurons in InceptionV1. I know how they behave, and I know how that behavior is built from earlier neurons. These seem like obscure facts, but they give me powerful, concrete examples to test ideas against.)Research intimacy is also different from research taste. But it does feed into it, and I suspect it’s one of the key ingredients in beating the “research taste market.”As your intimacy with a research topic grows, your random thoughts about it become more interesting. Your thoughts in the shower or on a hike bounce against richer context. Your unconscious has more to work with. Your intuition deepens.I suspect that a lot of “brilliant insights” are natural next steps from someone who has deep intimacy with a research topic. And that actually seems more profound.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Atlassian is acquiring The Browser Company]]></title>
            <link>https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45126358</guid>
            <description><![CDATA[OpenAI and Perplexity both reportedly looked at acquiring the startup.]]></description>
            <content:encoded><![CDATA[Mike Cannon-Brookes, co-founder and CEO of Atlassian, speaks at the National Electrical Vehicle Summit in Canberra, Australia, on Aug. 19, 2022. Cannon-Brookes is urging Australia to show more ambition on climate action, even as the new government legislates plans to strengthen the country's carbon emissions cuts.Hilary Wardhaugh | Bloomberg | Getty ImagesAtlassian said it has agreed to acquire The Browser Co., a startup that offers a web browser with artificial intelligence features, for $610 million in cash.The companies aim to close the deal in Atlassian's fiscal second quarter, which ends in December.Established in 2019, The Browser Co. has gone up against some of the world's largest companies, including Google, with Chrome, and Apple, which includes Safari on its computers running MacOS.The startup debuted Arc, a customizable browser with a built-in whiteboard and the ability to share groups of tabs, in 2022. The Dia browser, a simpler option that allows people to chat with an AI assistant about multiple browser tabs at once, became available in beta in June.Atlassian co-founder and CEO Mike Cannon-Brookes said he sees shortcomings in the most popular browsers for those who do much of their work on computers."Whatever it is that you're actually doing in your browser is not particularly well served by a browser that was built in the name to browse," he said in an interview. "It's not built to work, it's not built to act, it's not built to do."Cannon-Brookes said Arc has helped him feel like he can manage his work, with its ability to organize tabs and automatically archive old ones.But only a small percentage of people who used The Browser Co.'s Arc adopted the program's special features."Our metrics were more like a highly specialized professional tool (like a video editor) than a mass-market consumer product, which we aspired to be closer to," Josh Miller, The Browser Co.'s co-founder and CEO, said in a newsletter update. The startup stopped building new features for Arc, leading to questions of whether it would release the browser under an open-source license.Read more CNBC tech newsHuawei launches second trifold smartphone at $2,500 as it looks to cement comebackC3 AI reports declining revenue, announces new CEO to replace SiebelOpenAI boosts size of secondary share sale to $10.3 billionApple has survived Trump's tariffs so far. It might raise iPhone prices anywayAI search startup Perplexity, which offered Google $34.5 billion for Chrome, talked with The Browser Co. about a possible acquisition in December, The Information reported. OpenAI also held deal talks with The Browser Co., according to the report.Cannon-Brookes wouldn't specify whether Atlassian considered buying Google's browser. Last year, the U.S. Justice Department proposed a divestiture after a federal judge ruled that the company enjoyed an internet search monopoly."I'm not even sure if there is a bidding competition for Chrome," Cannon-Brookes said. "I didn't see Google putting up an auction just yet. Look, I think we focus on actually getting acquisitions done and actually making those products a part of a coherent whole and delivering value for our customers. I'm not sure that stunt PR acquisition offers are really our thing, but we'll leave that for them to do."Perplexity has been providing early access to its own AI browser, which is named Comet. Google, for its part, will be able to keep Chrome but will have to share some data with rivals, a U.S. district court judge ruled on Tuesday.The Browser Co. was valued at $550 million last year. Investors include Atlassian Ventures, Salesforce Ventures, Figma co-founder Dylan Field and LinkedIn co-founder Reid Hoffman.The browser is central for those using Atlassian products, such as the Jira project management software, which shows existing support requests on the web. But the plan isn't simply to make it nicer to work with Atlassian products online."It's really about taking Arc's SaaS application experience and power user features, and Dia's AI and elegance and speed and sort of svelte nature, and Atlassian's enterprise know-how, and working out how to put all that together into Dia, or into the AI part of the browser," Cannon-Brookes said.watch now]]></content:encoded>
        </item>
    </channel>
</rss>