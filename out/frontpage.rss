<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 29 Aug 2025 16:41:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Wikipedia as a Graph]]></title>
            <link>https://wikigrapher.com/paths</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066060</guid>
        </item>
        <item>
            <title><![CDATA[German Economist fined €16,100 for sarcastic X posts]]></title>
            <link>https://reclaimthenet.org/german-economist-fined-for-sarcastic-posts-on-politicians-journalist</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065977</guid>
        </item>
        <item>
            <title><![CDATA[Amtrak's New Acela Trains Are Here. They're Moving Slower Than the Old Ones]]></title>
            <link>https://www.wsj.com/us-news/amtraks-new-acela-trains-are-here-theyre-moving-slower-than-the-old-ones-f0794127</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065717</guid>
        </item>
        <item>
            <title><![CDATA[Essential Coding Theory [pdf]]]></title>
            <link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065705</guid>
        </item>
        <item>
            <title><![CDATA[Taco Bell rethinks AI drive-through after man orders 18,000 waters]]></title>
            <link>https://www.bbc.com/news/articles/ckgyk2p55g8o</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065391</guid>
            <description><![CDATA[The fast food chain is reassessing its use of the tech after a number of errors were shared widely online.]]></description>
            <content:encoded><![CDATA[6 hours agoShiona McCallumSenior Tech Reporter Getty ImagesTaco Bell is rethinking its use of artificial intelligence (AI) to power drive-through restaurants in the US after comical videos of the tech making mistakes were viewed millions of times. In one clip, a customer seemingly crashed the system by ordering 18,000 water cups, while in another a person got increasingly angry as the AI repeatedly asked him to add more drinks to his order.Since 2023, the fast-food chain has introduced the technology at over 500 locations in the US, with the aim of reducing mistakes and speeding up orders. But the AI seems to have served up the complete opposite. Taco Bell's Chief Digital and Technology Officer Dane Mathews told The Wall Street Journal that deploying the voice AI has had its challenges. "Sometimes it lets me down, but sometimes it really surprises me," he said.He said the firm was "learning a lot" - but he would now think carefully about where to use AI going forwards, including not using it at drive-throughs.In particular, Mr Matthews said, there are times when humans are better placed to take orders, especially when the restaurants get busy. "We'll help coach teams on when to use voice AI and when it's better to monitor or step in," he said. The issues have been building online as disgruntled customers take to social media to complain about the service - with many pointing out glitches and issues.One clip on Instagram, which has been viewed over 21.5 million times, shows a man ordering "a large Mountain Dew" and the AI voice continually replying "and what will you drink with that?".It isn't the first time there has been issues with AI not getting it right when it comes to processing food and drink orders.  Last year McDonald's withdrew AI from its own drive-throughs as the tech misinterpreted customer orders - resulting in one person getting bacon added to their ice cream in error, and another having hundreds of dollars worth of chicken nuggets mistakenly added to their order.But despite some of the viral glitches facing Taco Bell, it says two million orders have been successfully processed using the voice AI since its introduction.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why AI Isn't Ready to Be a Real Coder]]></title>
            <link>https://spectrum.ieee.org/ai-for-coding</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065343</guid>
            <description><![CDATA[Can AI truly collaborate with human coders? Researchers highlight the hurdles and potential solutions in AI-driven software engineering.]]></description>
            <content:encoded><![CDATA[Artificial intelligence (AI) has transformed the coding sphere, with AI coding tools completing source code, correcting syntax errors, creating inline documentation, and understanding and answering questions about a codebase. As the technology advances beyond automating programming tasks, the idea of full autonomy looms large. Is AI ready to be a real coder?A new paper says not yet—and maps out exactly why. Researchers from Cornell University, MIT CSAIL, Stanford University, and UC Berkeley highlight key challenges that today’s AI models face and outline promising research directions to tackle them. They presented their work at the 2025 International Conference on Machine Learning.The study offers a clear-eyed reality check amid all the hype. “At some level, the technology is powerful and useful already, and it has gotten to the point where programming without these tools just feels primitive,” says Armando Solar-Lezama, a co-author of the paper and an associate director at MIT CSAIL, where he leads the computer-aided programming group. He argues, however, that AI-powered software development has yet to reach “the point where you can really collaborate with these tools the way you can with a human programmer.”Challenges With AI Coding ToolsAccording to the study, AI still struggles with several crucial facets of coding: sweeping scopes involving huge codebases, the extended context lengths of millions of lines of code, higher levels of logical complexity, and long-horizon or long-term planning about the structure and design of code to maintain code quality.Koushik Sen, a professor of computer science at UC Berkeley and also a co-author of the paper, cites fixing a memory safety bug as an example. (Such bugs can cause crashes, corrupt data, and open security vulnerabilities.) Software engineers might approach debugging by first determining where the error originates, “which might be far away from where it’s crashing, especially in a large codebase,” Sen explains. They’ll also have to understand the semantics of the code and how it works, and make changes based on that understanding. “You might have to not only fix that bug but change the entire memory management,” he adds.These kinds of complex tasks can be difficult for AI development tools to navigate, resulting in hallucinations about where the bug is or its root cause, as well as irrelevant suggestions or code fixes with subtle problems. “There are many failure points, and I don’t think the current LLMs [large language models] are good at handling that,” says Sen.Among the various paths suggested by the researchers toward solving these AI coding challenges—such as training code LLMs to better collaborate with humans and ensuring human oversight for machine-generated code—the human element endures.“A big part of software development is building a shared vocabulary and a shared understanding of what the problem is and how we want to describe these features. It’s about coming up with the right metaphor for the architecture of our system,” Solar-Lezama says. “It’s something that can be difficult to replicate by a machine. Our interfaces with these tools are still quite narrow compared to all the things that we can do when interacting with real colleagues.”Enhancing AI-Human Collaboration in CodingCreating better interfaces, which today are driven by prompt engineering, is integral for developer productivity in the long run. “If it takes longer to explain to the system all the things you want to do and all the details of what you want to do, then all you have is just programming by another name,” says Solar-Lezama.Shreya Kumar, a software engineer and an associate teaching professor in computer science at the University of Notre Dame who was not involved in the research, echoes the sentiment. “The reason we have a programming language is because we need to be unambiguous. But right now, we’re trying to adjust the prompt [in a way] that the tool will be able to understand,” she says. “We’re adapting to the tool, so instead of the tool serving us, we’re serving the tool. And it is sometimes more work than just writing the code.”As the study notes, one way to address the dilemma of human-AI interaction is for AI systems to learn to quantify uncertainty and communicate proactively, asking for clarification or more information when faced with vague instructions or unclear scenarios. Sen adds that AI models might also be “missing context that I have in my mind as a developer—hidden concepts that are embedded in the code but hard to decipher from it. And if I give any hint to the LLM about what is happening, it might actually make better progress.”For Abhik Roychoudhury, a professor of computer science at the National University of Singapore who was also not involved in the research, a crucial aspect missing from the paper and from most AI-backed software development tools entails capturing user intent.“A software engineer is doing a lot of thinking in understanding the intent of the code. This intent inference—what the program is trying to do, what the program is supposed to do, and the deviation between the two—is what helps in a lot of software engineering tasks. If this outlook can be brought in future AI offerings for software engineering, then it will get closer to what the software engineer does.”Where Does AI Coding Go From Here? Roychoudhury also assumes that many of the challenges identified in the paper are either being worked on now or “would be solved relatively quickly” due to the rapid pace of progress in AI for software engineering. Additionally, he believes that an agentic AI approach can help, viewing significant promise in AI agents for processing requirements specifications and ensuring they can be enforced at the code level.“I feel the automation of software engineering via agents is probably irreversible. I would dare say that it is going to happen,” Roychoudhury says.Sen is of the same view but looks beyond agentic AI initiatives. He pinpoints ideas such as evolutionary algorithms to enhance AI coding skills and projects like AlphaEvolve that employ genetic algorithms “to shuffle the solutions, pick the best ones, and then continue improving those solutions. We need to adopt a similar technology for coding agents, where the code is continuously improving in the background.”However, Roychoudhury cautions that the bigger question lies in “whether you can trust the agent, and this issue of trust will be further exacerbated as more and more of the coding gets automated.”That’s why human supervision remains vital. “There should be a check and verify process. If you want a trustworthy system, you do need to have humans in the loop,” says Notre Dame’s Kumar.Solar-Lezama agrees. “I think it’s always going to be the case that we’re ultimately going to want to build software for people, and that means we have to figure out what it is we want to write,” he says. “In some ways, achieving full automation really means that we get to now work at a different level of abstraction.”So while AI may become a “real coder” in the near future, Roychoudhury acknowledges that it probably won’t gain software developers’ complete trust as a team member, and thus might not be allowed to do its tasks fully autonomously. “That team dynamics—when an AI agent can become a member of the team, what kind of tasks will it be doing, and how the rest of the team will be interacting with the agent—is essentially where the human-AI boundary lies,” he says.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Seedbox Lite: A lightweight torrent streaming app with instant playback]]></title>
            <link>https://github.com/hotheadhacker/seedbox-lite</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065278</guid>
            <description><![CDATA[A light-weight torrent media center at one place. Contribute to hotheadhacker/seedbox-lite development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[🎬 SeedBox Lite
Stream Torrents Instantly

🚀 Overview
SeedBox Lite is a cutting-edge torrent streaming platform that allows you to watch movies and TV shows instantly without waiting for complete downloads. Built with modern web technologies, it provides a Netflix-like experience with powerful torrent capabilities.
✨ Key Highlights

🎯 Instant Streaming - Start watching immediately as the torrent downloads
🔐 Password Protection - Secure access with authentication
📱 Mobile Optimized - Perfect responsive design for all devices
🎥 Smart Video Player - Advanced player with subtitles and fullscreen support
⚡ Fast Setup - Deploy in minutes with Docker or PM2
🌐 Cross-Platform - Works on Windows, macOS, and Linux
🎨 Modern UI - Clean, intuitive interface inspired by popular streaming services

🎯 Features
Core Streaming Features

Torrent to Stream - Convert any movie/TV torrent to instant streaming
Progress Tracking - Real-time download progress and cache management
Smart Caching - Intelligent caching system with configurable limits
Multiple Formats - Support for MP4, MKV, AVI, and more video formats
Subtitle Support - Automatic subtitle detection and loading

User Experience

Netflix-Style Interface - Familiar and intuitive design
Mobile-First Design - Optimized for smartphones and tablets
Native Fullscreen - True fullscreen experience on mobile devices
Gesture Controls - Double-tap to fullscreen, intuitive video controls
Responsive Layout - Adapts perfectly to any screen size

Technical Features

Password Authentication - Secure access control
CORS Enabled - Cross-origin resource sharing for flexible deployment
Health Monitoring - Built-in health checks and monitoring
Production Ready - Optimized for production deployments
Docker Support - Easy containerized deployment
PM2 Integration - Process management for Node.js applications

Mobile Optimizations

iOS Safari Support - Native fullscreen using WebKit APIs
Android Chrome - Optimized for Android mobile browsers
Range Requests - HTTP range support for smooth video seeking
Mobile Viewport - Proper viewport handling for app-like experience
Touch Optimized - Gesture-friendly video controls

📸 Screenshots
View all screenshots
🚀 Quick Start
Using Docker (Recommended)
# Clone the repository
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Start with Docker Compose
docker-compose up -d

# Access the application
open http://localhost:5174
Using PM2
# Clone and install dependencies
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server && npm install

# Install frontend dependencies  
cd ../client && npm install

# Build frontend
npm run build

# Start with PM2
pm2 start ecosystem.config.js
📋 Prerequisites
System Requirements

Node.js 18+
npm 8+
Docker 20+ (for Docker deployment)
PM2 (for PM2 deployment)

Operating System Support

✅ Windows 10/11
✅ macOS 10.15+
✅ Ubuntu 18.04+
✅ Debian 10+
✅ CentOS 7+

Browser Support

✅ Chrome 90+
✅ Firefox 88+
✅ Safari 14+
✅ Edge 90+
✅ Mobile browsers (iOS Safari, Android Chrome)

🛠 Installation
Method 1: Docker Deployment (Recommended)
Step 1: Clone Repository
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite
Step 2: Configure Environment
# Copy and edit environment variables
cp .env.example .env
nano .env
Key Environment Variables:
# Server Configuration
NODE_ENV=production
SERVER_PORT=3001
ACCESS_PASSWORD=your_secure_password

# Frontend Configuration  
FRONTEND_URL=http://localhost:5174
VITE_API_BASE_URL=http://localhost:3001

# Docker Ports
BACKEND_PORT=3001
FRONTEND_PORT=5174
Step 3: Deploy
# Start all services
docker-compose up -d

# Check status
docker-compose ps

# View logs
docker-compose logs -f
Step 4: Access Application

Frontend: http://localhost:5174
Backend API: http://localhost:3001
Default Login: Password set in ACCESS_PASSWORD

Method 2: PM2 Deployment
Step 1: System Setup
# Install Node.js 18+
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# Install PM2 globally
npm install -g pm2
Step 2: Application Setup
# Clone repository
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server
npm install
cd ..

# Install and build frontend
cd client
npm install
npm run build
cd ..
Step 3: Configure Environment
# Backend environment
cd server
cp .env.example .env
nano .env
Backend .env Configuration:
NODE_ENV=production
SERVER_PORT=3001
SERVER_HOST=0.0.0.0
ACCESS_PASSWORD=your_secure_password
FRONTEND_URL=http://localhost:5174
Step 4: Start Services
# Start backend with PM2
cd server
pm2 start ecosystem.config.js

# Serve frontend with nginx or serve
cd ../client/dist
npx serve -s . -l 5174

# Or use PM2 for frontend
pm2 start "npx serve -s . -l 5174" --name "seedbox-frontend"
Step 5: PM2 Management
# View running processes
pm2 list

# View logs
pm2 logs

# Restart services
pm2 restart all

# Save PM2 configuration
pm2 save
pm2 startup
Method 3: Development Setup
Step 1: Clone and Install
git clone https://github.com/hotheadhacker/seedbox-lite.git
cd seedbox-lite

# Install backend dependencies
cd server
npm install

# Install frontend dependencies
cd ../client  
npm install
Step 2: Configure Development Environment
# Backend environment
cd server
cp .env.example .env
Development .env:
NODE_ENV=development
SERVER_PORT=3000
SERVER_HOST=localhost
ACCESS_PASSWORD=seedbox123
FRONTEND_URL=http://localhost:5173
Step 3: Start Development Servers
# Terminal 1: Start backend
cd server
npm run dev

# Terminal 2: Start frontend  
cd client
npm run dev
🧪 Testing
Docker Testing
# Health check
curl http://localhost:3001/api/health
curl http://localhost:5174/health

# API endpoints
curl -X POST http://localhost:3001/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"password":"your_password"}'

# Cache stats
curl http://localhost:3001/api/cache/stats
PM2 Testing
# Check PM2 status
pm2 list
pm2 logs seedbox-backend
pm2 logs seedbox-frontend

# Test API endpoints
curl http://localhost:3001/api/health
curl http://localhost:5174
Frontend Testing
cd client
npm test

# Run Cypress e2e tests
npm run test:e2e

# Accessibility testing
npm run test:a11y
Backend Testing
cd server
npm test

# API integration tests
npm run test:integration

# Load testing
npm run test:load
📚 Configuration
Environment Variables Reference
Backend Variables



Variable
Default
Description




NODE_ENV
production
Application environment


SERVER_PORT
3001
Backend server port


SERVER_HOST
0.0.0.0
Backend server host


ACCESS_PASSWORD
seedbox123
Authentication password


MAX_CACHE_SIZE
5GB
Maximum cache size


CLEANUP_INTERVAL
1h
Cache cleanup interval



Frontend Variables



Variable
Default
Description




VITE_API_BASE_URL
http://localhost:3001
Backend API URL


FRONTEND_URL
http://localhost:5174
Frontend URL



Docker Variables



Variable
Default
Description




BACKEND_PORT
3001
Docker backend port mapping


FRONTEND_PORT
5174
Docker frontend port mapping



Advanced Configuration
Nginx Configuration (Production)
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://localhost:5174;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location /api/ {
        proxy_pass http://localhost:3001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
SSL/HTTPS Setup
# Install Certbot
sudo apt install certbot python3-certbot-nginx

# Get SSL certificate
sudo certbot --nginx -d your-domain.com

# Auto-renewal
sudo crontab -e
# Add: 0 12 * * * /usr/bin/certbot renew --quiet
🔧 Troubleshooting
Common Issues
Port Conflicts
# Check if ports are in use
lsof -i :3001
lsof -i :5174

# Kill processes using ports
sudo kill -9 $(lsof -ti:3001)
sudo kill -9 $(lsof -ti:5174)
Docker Issues
# Rebuild containers
docker-compose down
docker-compose up --build

# Clear Docker cache
docker system prune -a

# Check container logs
docker-compose logs seedbox-backend
docker-compose logs seedbox-frontend
PM2 Issues
# Reset PM2
pm2 kill
pm2 start ecosystem.config.js

# Check PM2 logs
pm2 logs --lines 50

# Monitor PM2 processes
pm2 monit
Permission Issues
# Fix file permissions
sudo chown -R $USER:$USER .
chmod +x deploy.sh

# Docker permission issues
sudo usermod -aG docker $USER
newgrp docker
Mobile Video Issues

Ensure CORS is enabled in backend
Check video format compatibility
Verify range request support
Test with different browsers

📖 API Documentation
Authentication Endpoints
POST /api/auth/login
{
  "password": "your_password"
}
Torrent Endpoints
GET /api/torrents/search?q=movie+name
POST /api/torrents/add
{
  "magnetLink": "magnet:..."
}
Streaming Endpoints
GET /api/stream/:torrentId/:fileIndex
Range requests supported for video seeking
Cache Management
GET /api/cache/stats
POST /api/cache/clear
🛡 Security
Best Practices

Change default password immediately
Use HTTPS in production
Keep dependencies updated
Enable firewall rules
Regular security audits

Security Headers
The application includes security headers:

X-Frame-Options: SAMEORIGIN
X-Content-Type-Options: nosniff
X-XSS-Protection: 1; mode=block
Referrer-Policy: no-referrer-when-downgrade

🚀 Deployment
Production Deployment Checklist

 Change default passwords
 Configure HTTPS/SSL
 Set up monitoring
 Configure backups
 Set up log rotation
 Configure firewall
 Test mobile compatibility
 Verify video streaming
 Test authentication
 Monitor performance

Scaling
For high-traffic deployments:

Use load balancer (nginx/HAProxy)
Scale backend horizontally
Implement Redis for session storage
Use CDN for static assets
Monitor resource usage

📞 Support
Getting Help

📖 Documentation
🐛 Issue Tracker
💬 Discussions

Contributing

Fork the repository
Create feature branch
Make changes
Add tests
Submit pull request

⚠️ Legal Disclaimer
IMPORTANT: Please read this disclaimer carefully before using SeedBox Lite.
SeedBox Lite is an open-source project provided for educational and personal use only. We do not endorse, promote, or facilitate copyright infringement, illegal streaming, or piracy in any form. This software is designed to be used with legal content only.

We do not host, store, or distribute any content. All torrents and media are accessed through your own connections.
This application is intended for use with content that you have the legal right to access and stream.
Users are solely responsible for how they use this software and for ensuring compliance with all applicable laws in their jurisdiction.
The creators and contributors of SeedBox Lite take no responsibility for how this software is used.
Using torrents to download or share copyrighted materials without permission may be illegal in your country.

By using SeedBox Lite, you acknowledge that you understand these terms and agree to use the software responsibly and legally.
📄 License
This project is licensed under the Custom Non-Commercial License - see the LICENSE file for details.
Important License Restrictions:

This software is provided for personal, educational, and non-commercial use only
Commercial use is strictly prohibited without explicit written permission
Redistribution must include this license and copyright notice
No warranty or liability is provided with this software

🙏 Acknowledgments

WebTorrent for torrent streaming capabilities
React team for the amazing framework
Docker community for containerization
All contributors and users



Made with ❤️ by hotheadhacker
⭐ Star this repo if you find it useful!

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deploying DeepSeek on 96 H100 GPUs]]></title>
            <link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45064329</guid>
            <description><![CDATA[<p>DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which us...]]></description>
            <content:encoded><![CDATA[DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.

Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs.
It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences.
To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale.
By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API.
Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x.
This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.
Highlight
✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.
✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.
✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.
✅ All experiments and code are fully open-sourced for community access and further development.
Outline

Parallelism Design
Prefill and Decode Disaggregation
Large-scale Expert Parallelism
Evaluation
Toolkits
Limitations and Future Work
Conclusion
Acknowledgment

Parallelism Design
Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.
Attention Layers
DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.
Dense FFNs
Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:

Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.
Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.
Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.

The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting --moe-dense-tp-size=1.

Sparse FFNs
In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.
The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.
LM Head
The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.
Prefill and Decode Disaggregation
LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.
Issues with Unified Scheduling
The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:

Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.
DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.
Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.

PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.
Implementation Details
The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:

Upon receiving an input request, the workflow proceeds as follows:

A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.
The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.
Once computed, the data transfers to the Decode Server, which handles iterative token generation.

This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:

Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.
RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.
Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.

More details can be found in our design document.
Large-scale Expert Parallelism
Expert Parallelism with DeepEP
DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.
DeepEP provides two specialized dispatch modes to address varying workload demands:

Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.
Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.

In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:



Mode
Long Input
Long Output
DP Attention
CUDA Graph




Normal
✅
❌
✅
❌


Low-Latency
❌
✅
✅
✅


Auto
✅
✅
❌
✅



PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.
DeepGEMM Integration
DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.

Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.
Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.

DeepGEMM integrates smoothly with the dispatch modes of DeepEP:

For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.
The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.

SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable SGL_ENABLE_JIT_DEEPGEMM to 1, offering even greater computational efficiency for non-MoE operations.
Two-batch Overlap
In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.
Implementation Challenges
Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.

Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.
Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.

Abstraction for Clean Implementation
To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:
operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)

Prefill Overlapping Implementation
We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:

The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.
An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.

To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).

Expert Parallelism Load Balancer
In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.
To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.
Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.
In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.

EPLB for Real-World Serving
For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:

Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).
Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.

Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.
Implementation of Rebalancing
SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:

System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.
Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.
Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.

This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.
Evaluation
End-to-end Performance
Experimental Setup
We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:

SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.
SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.
SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.
DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.

Performance Analysis of Prefill and Decode Phases
To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:

Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.
Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.


Profile Results
This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.
Overall Throughput
For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.
The results are presented below:




DeepSeek Blog (excl. cache hit)
DeepSeek Profile
SGLang (Default)
SGLang + Simulated Perfect EPLB




Batch Size
N/A
16,384
16,384
16,384


Input Length
N/A
4,096
4,096
4,096


Throughput (per node)
32,206
62,713
50,302
59,337



DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.
For decode, the results are shown below:




DeepSeek Blog
DeepSeek Profile
SGLang (Default)
SGLang + Simulated MTP (Slow Attention)




Batch Size
N/A
128
256
128


KV Cache Length
4,989
4,096
2,000
4,000


Number of Nodes
18
16
9
9


Throughput (per node)
14,800
18,598
22,282
17,373



Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.
Detail Breakdown
The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:


Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.
Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.
Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.

SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:

Key observations include:

Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.
MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.
Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.

While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.
Ablation Study: Two-batch Overlap
Impact of Batch Size and Attention Time
This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.

TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:

Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.
Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.

TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:

Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.
Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.

Detail Breakdown
We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:

TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.
Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.
Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.


For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:

TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.
TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.


Ablation Study: EPLB
This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.
Overall Results
The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.

Case Study: Workload Imbalance Versus Overall Throughput
To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:

The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.
Case Study: Expert Distribution Statistics
The following figure presents expert distribution statistics for prefill and decode sample data:

Key observations include:

Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.
Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.

These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.
Toolkits
Disposable Tensor
Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)

In this code, del hidden_state is intended to release the memory occupied by hidden_state after intermediate_state is computed. However, as hidden_state is still referenced outside the function, the del operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.
SGLang addresses this with the DisposableTensor class, a subclass of torch.Tensor which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)

By wrapping hidden_state in a DisposableTensor and calling dispose() when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.
Expert Workload Extraction and Simulation
SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:

Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.
Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.

This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.
Limitations and Future Work
While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:

Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.
Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.
Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.
EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.
Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.
Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.

Conclusion
By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.
Acknowledgment
We would like to express our heartfelt gratitude to the following teams and collaborators:

SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.
Atlas Cloud Team —  Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.
NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.
NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.
LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.
Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.
FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.
Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.

Thank you all for your invaluable support and collaboration.
Appendix
Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Find Hidden Gems on HN]]></title>
            <link>https://pj4533.com/hn-overlooked/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45064210</guid>
            <description><![CDATA[This tool helps you discover recent hidden gems on Hacker News – high-effort posts that haven't gotten much attention.]]></description>
            <content:encoded><![CDATA[
                This tool helps you discover recent hidden gems on Hacker News – high-effort posts that haven't gotten much attention.
                
                Why "Recent"?
                We search the HN API's Ask, Show, and New story feeds, which typically contain posts from the last 3-7 days. This ensures fresh content while keeping the search fast.
                
                Passion Score
                Posts are ranked by their Passion Score, which identifies high-effort, low-engagement content:
                
                
                    Passion Score = (Text Length Score) / (Engagement + 1)
                    
                    Where:
                    • Text Length Score = min(text_length / 500, 10)
                    • Engagement = votes + (comments × 2)
                
                
                Higher scores indicate more "overlooked" posts – substantial writing with minimal recognition. Perfect for finding thoughtful contributions that the community may have missed.
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Private Equity Snaps Up Disability Services, Challenging Regulators]]></title>
            <link>https://www.governing.com/management-and-administration/private-equity-snaps-up-disability-services-challenging-regulators</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063972</guid>
            <description><![CDATA[Private equity firms have acquired more than 1,000 disability and elder care providers in recent years. Some have been accused of patient harm.]]></description>
            <content:encoded><![CDATA[
                                
                                    Private equity companies have gobbled up group homes and other services for people with disabilities, attracting the attention of state and federal regulators across the nation and alarming advocates.People with intellectual or developmental disabilities have suffered abuse, neglect and even death while under the care of private equity-owned providers, according to a recent report from watchdog group Private Equity Stakeholder Project.“Private equity firms are, more than many other types of investors, laser-focused on maximizing their cash flow, often trying to double or triple their investment over a relatively short period of time, usually just a handful of years,” said Eileen O’Grady, the report’s author. “The way that private equity firms will often do that is to cut costs.”For companies that provide essential services for people with disabilities, she said, “those cuts can have really harmful impacts on people’s lives.”In late 2023, Florida moved to revoke the license of NeuroRestorative, one branch of the private equity-owned health services company Sevita, which provides services for people with disabilities. State regulators cited repeat violations by NeuroRestorative and a failure to “protect the rights of its clients to be free from physical abuse.” Ultimately the state opted not to revoke the license and fined the company $13,000 in a settlement.But in recent years regulators have documented instances of patient harm at Sevita’s affiliates in multiple other states, including Colorado, Indiana, Iowa, Massachusetts and Utah. In 2019, a U.S. Senate committee conducted a probe into the company’s operations in Iowa and Oregon following multiple reports of patient abuse and neglect.“Any entity that receives taxpayer dollars, but especially those charged with caring for our fellow Americans who may have an intellectual disability, ought to be doing everything under the sun to ensure quality care and continually improve,” U.S. Sen. Chuck Grassley, an Iowa Republican, said in a statement in 2020 following his investigation.In a statement to Stateline, Sevita did not address the sanctions directly, but avowed its commitment to providing services and supports to give people greater independence, regardless of their intellectual or physical challenges.“Since 2019, when new ownership acquired the company, there has been significant capital investment to improve and expand our services, enhance facilities, implement robust training and new technologies, and strengthen our workforce — all with the goal of better serving our individuals and communities,” the statement said.The disability care industry has proven increasingly attractive to private equity.In recent years, a handful of large private equity-owned companies such as Sevita have snapped up hundreds of smaller providers of disability services — often community nonprofits, mom-and-pop businesses and religious organizations — and rolled them into larger corporations.From 2013 to 2023, private equity firms acquired more than 1,000 disability and elder care providers, according to the report by the Private Equity Stakeholder Project. That’s likely an undercount because they’re generally not required to disclose acquisitions, the report said.Cash CowPrivate equity firms use pooled investments from pension funds, sovereign wealth funds, endowments and wealthy individuals to buy a controlling stake in a company. They seek to maximize its value — often by cutting costs — and then sell it at a profit.Most of Sevita’s revenue comes from providing disability services. It operates companies in 40 states under various brands, including Mentor Network, NeuroRestorative and REM.Sevita is currently owned by private equity firms Centerbridge Partners and Vistria Group, which also own Help at Home, a home health company with more than 200 locations across about a dozen states.Nearly all of Sevita’s revenue comes from Medicaid, according to a February 2025 report from S&P Global.Through Medicaid and Medicare, the government pays for most services for people with intellectual or developmental disabilities. The two programs cover services such as group homes, adult day programs, in-home care, and physical and occupational therapy.“Sevita has been owned by private equity firms for over a decade now, and has been under investigation and scrutiny at the federal and state level for basically that entire time,” O’Grady said.In 2022, Iowa fined a NeuroRestorative group home $10,500 after a resident was left unattended in a liquor store and drank three-quarters of a bottle of vodka. The same year, Massachusetts temporarily removed Sevita’s license to operate group homes after regulators reported inadequate staff training and supervision, and a “myriad of issues that were uncovered onsite,” according to a Massachusetts Department of Developmental Services report.The federal Centers for Medicare & Medicaid Services has fined a NeuroRestorative facility in Utah four times since 2022. A February 2024 inspection report by the agency found the facility “failed to prevent abuse, neglect … and exploitation” of residents.Last year, Florida fined another Sevita brand, Florida Mentor, for improper use of restraints. More issues have been documented in Sevita-owned locations in Arkansas, California, Colorado, Illinois, Indiana, New Hampshire and Nevada.Meanwhile, Sevita’s owners, Centerbridge and Vistria, have collected nearly half a billion dollars since 2019 by loading Sevita and Help at Home with debt in order to pay dividends to investors, according to Moody’s, a financial services company.Similar financial maneuvering contributed to the recent collapse of Steward Health Care, a private equity-owned hospital system that once had more than 30 hospitals nationwide. Steward has become a cautionary tale about the harm that profit-driven private equity firms can do to a state’s health system.“Before Steward Health Care ultimately collapsed, executives spent years hiding their financial information from state regulators, putting patients and our health care system at risk,” Massachusetts Democratic House Speaker Ron Mariano said in a statement earlier this year announcing a new state law that beefs up reporting and financial requirements for private investors.“That’s why ensuring that our institutions are equipped to monitor the health care landscape, and to guard against trends and transactions that drive up costs without improving patient outcomes, is so important.”David vs. GoliathAfter two residents of a New Jersey group home died from choking on food in 2017, attorney Cory Bernstein became interested in private equity’s involvement in disability services. The residents had been living in homes operated by AdvoServ, a company then owned by the private equity firm Wellspring Capital Management. The state had cited AdvoServ more times than any other operator in New Jersey for abuse, neglect and unsafe conditions.AdvoServ later ceased operations in 2019 after multiple state agencies, including in New Jersey, Florida and Maryland, launched investigations.But even when state regulators are doing all they can to protect people with disabilities from substandard care, they’re limited in how much they can hold a company accountable, Bernstein told Stateline.“It’s state-level oversight on a national entity with not much [help] coming from the federal side,” said Bernstein, who is now a staff attorney at the National Disability Rights Network, a membership organization of federally mandated state disability advocacy programs.“States just don’t really have the resources or tools to do what needs to be done.”A regulatory agency in Georgia might shut down all the group homes owned by a certain company, for example, but those regulators can’t do anything about the company’s abuses in, say, Montana. With branches in multiple states, a company is better able to withstand sanctions or even a loss of license in one state, he said.“[States] are not set up to go up against a national operator with billions of dollars in resources in a regulatory or oversight battle,” Bernstein said.Further complicating things for state regulators and for consumers is that a large services company such as Sevita might operate under multiple brand names, even in one state. It can be hard to parse out who owns a sanctioned business. Multiple brand names can also obscure a company’s monopoly on a particular regional market.When Florida regulators reached a settlement agreement with Sevita’s NeuroRestorative last year, the state dismissed its proposed license revocation. O’Grady believes one reason the state chose to settle is the difficulty of finding alternative facilities to relocate the residents who would have been displaced from the 13 locations the company operated around the state.“Because of that dearth of alternatives and the impotence of the state to act more fully, this company will continue to be allowed to operate,” she said.Further complicating oversight: Large companies often operate various services that are overseen by different agencies. Group homes might be regulated under the state’s Medicaid program, while facilities that provide more intensive care might come under federal Medicare oversight.There could be “two completely different oversight systems for facilities serving the same population in the same state with the same name,” Bernstein said.State SolutionsSome states have moved to address problems with private equity involvement in health care by passing tighter restrictions on mergers and acquisitions of health care companies.In Rhode Island, where private equity companies’ mismanagement of health care providers threatened the future of local hospitals, a robust oversight law allowed the state attorney general to impose conditions to protect the hospitals’ finances.More states are following suit. In 2023 alone, 24 states enacted laws related to health system consolidation and competition, while this year at least half a dozen have considered legislation to check private equity-fueled health care mergers.This article was published by Stateline. Read the original here.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Sosumi.ai – Convert Apple Developer docs to AI-readable Markdown]]></title>
            <link>https://sosumi.ai/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063874</guid>
            <description><![CDATA[sosumi.ai provides Apple Developer documentation in an AI-readable format by converting JavaScript-rendered pages into Markdown.]]></description>
            <content:encoded><![CDATA[
            
                Disclaimer: This is an unofficial, independent project and is not affiliated with or
                endorsed by Apple Inc. “Apple”, “Xcode”, and related marks are trademarks of Apple Inc.
            
            
                This service is an accessibility-first, on‑demand renderer. It converts a single Apple Developer page to
                Markdown only when requested by a user. It does not crawl, spider, or bulk download; it does not attempt
                to bypass authentication or security; and it implements rate limiting to avoid imposing unreasonable
                load.
            
            
                Content is fetched transiently and may be cached briefly to improve performance (approximately 30
                minutes). No permanent archives are maintained. All copyrights and other rights in the underlying
                content remain with Apple Inc. Each page links back to the original source.
            
            
                Your use of this service must comply with Apple’s Terms of Use and applicable law. You are solely
                responsible for how you access and use Apple’s content through this tool. Do not use this service to
                circumvent technical measures or for redistribution.
            
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: A minimal TS library that generates prompt injection attacks]]></title>
            <link>https://prompt-injector.blueprintlab.io/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063547</guid>
            <description><![CDATA[Enterprise-grade prompt injection testing for AI security professionals]]></description>
            <content:encoded><![CDATA[🛡️  TypeScript Library for AI Security Testing   Lightweight AI Security Testing Library A minimal TypeScript library with 25+ curated prompt injection patterns from leading security research. 
						Easy to integrate, comprehensive coverage, production-ready. npm install prompt-injector import { PromptInjector } from 'prompt-injector'  🎯 0 Attack Patterns 🔬 4 Attack Categories 📚 SOTA Research Based 📝 0 Generated Prompts Attack Categories 🎭 Jailbreaking (5 patterns) Role-play and persona-based attacks that attempt to bypass AI safety guidelines through character roleplay and fictional scenarios. 🔀 Instruction Hijacking (6 patterns) Direct attempts to override system prompts and inject new instructions that change AI behavior and responses. 🔐 Encoding Attacks (7 patterns) Obfuscation techniques using Base64, ROT13, Unicode, and other encodings to bypass content filters and detection systems. 🧠 Logic Traps (6 patterns) Sophisticated reasoning exploits using hypothetical scenarios, false urgency, and academic authority to manipulate responses. Quick Start Example import { PromptInjector } from 'prompt-injector';

// Initialize with your preferred configuration
const injector = new PromptInjector({
  severity: 'intermediate',
  categories: ['jailbreak', 'instruction-hijack'],
  maxAttempts: 50
});

// Generate test cases
const testCases = injector.generateTests('customer-service-bot');

// Test your AI system
const results = await injector.runTests(yourAISystem);
const report = injector.generateReport(results);

console.log(`Risk Score: ${report.summary.riskScore}`);]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gun Maker Sig Sauer Citing National Security to Keep Documents from Public]]></title>
            <link>https://practicalshootinginsights.com/eighth-circuit-fmeca-update/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063431</guid>
            <description><![CDATA[The secrecy battle over the Army’s Failure Modes, Effects, and Criticality Analysis (FMECA) for Sig Sauer’s P320 has followed Glasscock v. Sig Sauer to the Eighth Circuit. A media intervenor is now asking the appellate court to keep key records open—and their brief places Practical Shooting Insights (this site) squarely in the middle of the story.]]></description>
            <content:encoded><![CDATA[
      
        
          
          

  
    
      
      
        
        
        August 28, 2025
      
    

    

    
      
      

      
        
        
          4 minute read
        
      
    
  


        
      

      
        
        The secrecy battle over the Army’s Failure Modes, Effects, and Criticality Analysis (FMECA) for Sig Sauer’s P320 has followed Glasscock v. Sig Sauer to the Eighth Circuit. A media intervenor is now asking the appellate court to keep key records open—and their brief places Practical Shooting Insights (this site) squarely in the middle of the story.

What’s new


  The Trace intervenes in the appeal. The newsroom moved to intervene for the limited purpose of opposing sealed filings tied to class certification and the FMECA, arguing the public’s right of access and noting the district court cited the FMECA nine times when it certified the class.





  Sig Sauer says “national security” and asks for deference to the Army. In opposing intervention, Sig Sauer urges the court to leave FMECA-related material sealed and to give the Army time to weigh in, framing the dispute in terms of protecting “military secrets.”







  A second FMECA document emerges. Sig Sauer’s opposition confirms there are two FMECA records in the class-certification exhibits: a FMECA Spreadsheet and a FMECA Memorandum—the latter not previously described in public filings—raising fresh questions about what the memo contains and who authored it.





  PSI’s reporting is part of the record. The Trace’s filing tells the court the unredacted FMECA was found on CourtListener, de‑obscured, and published on Practical Shooting Insights, where it “remains available”—and it recounts Sig Sauer’s own executive discussing it on a podcast while pointing viewers to this website.






The FMECA document was previously published here.



The Trace’s pitch: This isn’t secret anymore

The Trace walks the appellate court through how the FMECA left the bottle: it was posted on this website and then widely republished; a YouTube explainer discussing it surpassed 100,000 views. The filing quotes Sig Sauer’s VP of Consumer Affairs Phil Strader being asked on the Behind the Lens podcast why the FMECA shouldn’t be public and responding, “No, there’s not” (nothing to hide), while directing viewers to this website to see the document and describing its contents.





The reporting regarding Phil Strader’s interview was previously published here

How many times has the unredacted FMECA been “shared”? The filings don’t give a precise share count. What they do document is widespread republication and discussion, including the 100k‑plus video and multiple re‑posts mirroring the PSI copy. In other words, the genie is out of the bottle.

The Trace also points to DoD Instruction 5230.24, the policy Sig Sauer invokes, noting it does not authorize withholding unclassified information about evaluations of performance and reliability of military equipment—and that the PSI‑hosted FMECA bears no DoD distribution markings.





Sig Sauer’s response: Let the Army decide—and keep the lid on

Sig Sauer tells the Eighth Circuit The Trace lacks standing and that parallel briefing is already underway in the district court. Substantively, Sig Sauer leans on military‑secrets concerns, requests time for the Army to opine on release, and characterizes the FMECA as controlled technical information created under the MHS contract. (The company also recounts how the spreadsheet briefly became public in another case before being pulled down.)



Two details in Sig Sauer’s papers matter going forward:

1) The “FMECA Memorandum.” Sig Sauer identifies the memo alongside the previously published spreadsheet. If the memo is Sig Sauer‑authored, it could reveal how the company framed the Army analysis internally—an issue directly relevant to notice, risk mitigation, and marketing claims.

2) Ongoing Army communications. Sig Sauer’s litigation counsel filed a declaration stating he asked the Army about the FMECA’s distribution status and that key Army decision‑makers were unavailable the week of the deadline; Sig Sauer says the Army may submit information and seeks additional time.





The transparency question, distilled


  
    Is the FMECA “national‑security” material? The Trace says no—and points to DoDI 5230.24’s carve‑out: it does not provide authority to withhold unclassified evaluations of performance and reliability—exactly what a FMECA is. It also underscores the lack of any DoD marking on the PSI copy.
  
  
    Is secrecy even possible at this point? The record shows the unredacted spreadsheet is online on this website, has been reposted broadly, and has been discussed by Sig Sauer’s own executive on air—who told listeners where to find it. One video discussing it has 100,000+ views.
  




Why this matters to the class action—and to owners

The district court relied on the FMECA repeatedly when certifying the Missouri class, including on notice and risk‑mitigation questions—the very issues consumers care about. Keeping the heart of that analysis under seal on appeal would blunt the public’s ability to scrutinize a product‑safety fight with real‑world consequences.



My role, plainly

Practical Shooting Insights is an independent site covering the shooting‑sports and firearms industry. The Trace’s filing names PSI as the first publisher of the unredacted spreadsheet and quotes Strader pointing viewers here. I will continue to publish filings and analysis so readers can compare the arguments to the documents themselves.



What to watch next

1) Whether the Eighth Circuit permits intervention and applies the strong presumption of public access to class‑certification records.
2) If the Army weighs in—and on what basis—regarding the FMECA’s distribution status.
3) Disclosure of the FMECA Memorandum. If it’s Sig Sauer-authored, it could illuminate internal framing of hazards and fixes—material at the core of consumer‑protection claims.

        
      

      

      


      
  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Deepnote (YC S19) is hiring engineers to build a better Jupyter notebook]]></title>
            <link>https://deepnote.com/join-us</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062914</guid>
            <description><![CDATA[We need powerful data science tools that help us explore, reason, and collaborate. These tools do not exist yet. Help us invent them.]]></description>
            <content:encoded><![CDATA[We build tools for explorersWe are here to revolutionize how data teams work together.We started Deepnote to help data teams solve the hardest problems. We don’t just need better algorithms, bigger data sets, and more computing power. We need tools that help us explore, collaborate, and share. These tools don’t exist yet. We need to invent them first.Data work is as much a scientific and creative process as it is an engineering one. It involves working together, failing, learning, and going back to the drawing board. Data professionals are explorers. To make projects successful, we need tools that are both powerful and easy to use. Tools that help us collaborate and share our work in an engaging way. Tools that make working with data fun again.That’s why we’re building the new standard in data tooling: a notebook that brings teams together to code, query, visualize, organize, and share — all in one place.We are building tools for explorers. Join us.Build the future with usWe’re building a collaborative notebook that beautifully integrates analytics and data science into every workflow and decision. But it’s not just about designing, shipping, and selling. It’s about the people who power it — and that means you.Get ready to do your best workTransforming how people work with data isn't easy. But we built a culture that allows us to do precisely that.We move with urgencyWe are a small, passionate team revolutionizing how data teams work. We give everyone the tools they need and enable them to take action.We keep learningWe are knowledge-seekers. We invest in continuous learning across every role and encourage a culture of proactive feedback.We take ownershipWe are makers. We expect everyone to be a decision-maker — no politics or walls to get in the way.We collaborateWe are partners. We work in a fully transparent environment and put open, effective communication above all else.Backed by the best in the businessWe’re backed by industry leaders — and they’re as excited about reimagining the future as we are.Y CombinatorIndex VenturesAccelGreg BrockmanCTO at OpenAIElad GilAngel InvestorNaval RavikantAngel InvestorElena VernaAngel InvestorExplore open positionsThousands of data professionals already use Deepnote — but we’re only scratching the surface of what’s possible. We’re building out our core team, and we want kind, curious explorers to join and grow with us.Senior Business Development Executive (B2B SaaS)Remote→Go to jobHead of Data / Solutions EngineerPrague→Go to job]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Meta might be secretly scanning your phone's camera roll]]></title>
            <link>https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062910</guid>
            <description><![CDATA[Some Facebook users have noticed new settings that let Meta analyze and retain your phone's photos. Yes, you read that right.]]></description>
            <content:encoded><![CDATA[   Why is Facebook cloud-processing my device's camera roll?   Meta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.      Where is this feature being tested?   Meta has confirmed the feature is a test, saying, "We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll."   The test is currently available in the US and Canada, but it's not available in Illinois or Texas due to those states' privacy laws.      Did Facebook ask for my consent before turning this on?   Meta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.      Elyse Betters Picaro / ZDNET Can I remove my photos once they've been uploaded? ZDNET's sister site, CNET, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by downloading your Facebook account data.   Why is this a potential privacy issue?   It expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.   Also: What Zuckerberg's 'personal superintelligence' sales pitch leaves outEven if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.   How many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?   Editor's note: This article was updated on Aug. 24, 2025 to clarify that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[If you have a Claude account, they're going to train on your data moving forward]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062738</guid>
        </item>
        <item>
            <title><![CDATA[Updates to Consumer Terms and Privacy Policy]]></title>
            <link>https://www.anthropic.com/news/updates-to-our-consumer-terms</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062683</guid>
            <description><![CDATA[Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.]]></description>
            <content:encoded><![CDATA[FAQWhat’s changing?We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until September 28, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.These updates do not apply to services under our Commercial Terms, including:Claude for Work, which includes our Team and Enterprise plansOur API, Amazon Bedrock, or Google Cloud’s Vertex APIClaude Gov and Claude for EducationWhy are you making this change?All large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.It’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.Why are you extending the data retention period?AI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.The extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.If you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.What action do I need to take?Current users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select "not now" and decide later. You have until September 28, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once September 28 arrives, you'll need to select your preference to continue using Claude.If you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.What happens if I allow my data to be used for model training and then change my mind?You can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tesla said it didn't have key data in a fatal crash, then a hacker found it]]></title>
            <link>https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062614</guid>
            <description><![CDATA[The key evidence was presented last month to a jury, which found the company partially liable for the 2019 crash in Key Largo, Florida.]]></description>
            <content:encoded><![CDATA[Years after a Tesla driver using Autopilot plowed into a young Florida couple in 2019, crucial electronic data detailing how the fatal wreck unfolded was missing. The information was key for a wrongful death case the survivor and the victim’s family were building against Tesla, but the company said it didn’t have the data.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Probability of typing a wrong Bitcoin address]]></title>
            <link>https://www.johndcook.com/blog/2025/08/28/wrong-address/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45061980</guid>
            <description><![CDATA[How likely is it that a mistyped Bitcoin address is a valid address? How likely is it that some address is a plausible typo of another?]]></description>
            <content:encoded><![CDATA[
		I heard someone say that Bitcoin is dangerous because you could easily make a typo when entering an address, sending money to the wrong person, and have no recourse. There are dangers associated with Bitcoin, such as losing a private key, but address typos are not a major concern.
Checksums
There are several kinds of Bitcoin addresses. Each is at least 20 bytes (160 bits) long, with at least 4 bytes (32 bits) of checksum. The chances of a typo resulting in a valid checksum are about 1 in 232.
Used addresses
Let’s ignore the checksum for this section.
Because addresses are formed by cryptographic hash functions, we can assume the values are essentially randomly distributed in the space of possible addresses. The addresses are deterministic, but for modeling purposes, random is as random does.
This means a typo of an actual address is no more or less likely to be another actual address than an address typed at random. This is unlike, say, English words: a mistyped English word is more likely to be another English word than random keystrokes would be.
There have been on the order of a billion Bitcoin addresses used, in a space of 2160 possibilities. (Actually more since some addresses have more than 160 bits.) There’s about a 1 in 1039 chance that a random 160-bit sequence corresponds to an address somewhere on the Bitcoin blockchain.
Addresses close in edit distance
Someone with the Caesarean handle Veni Vidi Vici on X asked
What about the odds that out of those 1B addresses, two of them are one character swap away from each other?
That’s an interesting question. Let’s assume the addresses are Base58-encoded strings of length 26. Addresses could be longer, but assuming the minimum length increases the probability of addresses being close.
How many addresses are within one or two character swaps of another? I addressed a similar question here a couple weeks ago. If all the characters were unique, the number of strings within k swaps of each other would be
|S1(26, 26 − k)|
where S1 denotes Stirling numbers of the first kind. For k = 1 this would be 325 and for k = 2 this would be 50,050. This assumes all the characters are unique; I haven’t thought through the case where characters are repeated.
For round numbers, let’s say there are a billion addresses, and for each address there are a million other addresses that are close in some sense, plausible typos of the address. That would be 1012 addresses and typos, spread out in a space of ≈1045 (i.e. 5826) possible addresses.
Now there’s an implicit Birthday Problem here. No particular address is likely to collide with another, even when you allow typos, but what about the likelihood that some address collides?
Say we partition our space of 1045 addresses into N = 1029 addresses with a million possible typos for each address. Then as a rule of thumb, you’d need around √N random draws before you have a 50-50 chance of seeing a collision. Since 109 is a lot less than 1014.5, it’s unlikely that any two addresses collide, even when you consider each address along with a million associated typos.
			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Strange CW Keys]]></title>
            <link>https://sites.google.com/site/oh6dccw/strangecwkeys</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060161</guid>
            <description><![CDATA[Made by OH6DC
You can also use the Text-only index page (divided into useful categories).]]></description>
            <content:encoded><![CDATA[HomeStrange CW KeysMade by OH6DCYou can also use the Text-only index page (divided into useful categories).Lever arch file CW key Lambic pedals Valentine's day lollipop CW paddle Rubber stamp CW key Letter scale CW key Clamp cootie Code book Pepper mill CW key Lightsaber CW key Nutcracker CW key Straight(ener) key Smoke alarm CW key Teletubbygraph key Soap dispenser CW key Vinyl record player CW key Moomin triangle CW key Antiperspirant roll-on CW key Dual banana CW paddle Power twister CW key Power twister CW key Handsaw CW key Hole punch CW key Watering can CW key Toilet brush CW key CW glove Remote control CW key Tea bag CW key Eyebrow-raising CW key with optical transmitter Back scratcher CW key Whisk CW key Pliers CW key Liver casserole CW key Licorice pipe CW key Chocolate CW key Ski-W key Power drill CW keyer Six megapixel CW key Suspenders CW key Spirit bottle cap CW key Speed skate CW key Flower CW key Knee pad sideswiper CW key for portable operation QRP transmitter powered by a CW key Alarm clock CW key Hammer CW key CW gun Nail clipper CW key Ballpoint pen CW key Rotary dial CW key Hammock CW key Joystick CW key Rowing boat CW key Guitar CW key Wallet CW key Radio controlled CW key Amaryllis telegraphiensis Multi-function knife with CW key Toilet paper roll CW key Table ice hockey CW key Big toe CW key Waffle iron CW key Lego straight key Lego bug Pogo stick CW key Crutch CW key Smoke signal CW key CCW key Necktie CW key Toothbrush CW key Bench press CW key Handshake CW key Chopsticks CW key Trailer hitch CW key Typewriter CW keyboard Refrigerator CW key Mobile phone CW key Paper cup iambic paddles Morsetrap CW key Fingertips CW key Vacuum cleaner semi-automatic CW key Banana CW key Rolling pin CW key Toaster CW key Cheese slicer CW key Rocking chair CW key QLF pedal for left foot CW Cross-country ski shoe CW key CW insoles QRQ paddles Onion chopper CW key Beer can CW key Egg slicer CW key Stapler CW key Bicycle pump CW key Iron bar CW key Homebrew semi-automatic bug Hacksaw blade sideswiper CW key Plywood CW key Home  |  Homebrew QRP  Page updated Google SitesReport abuse]]></content:encoded>
        </item>
    </channel>
</rss>