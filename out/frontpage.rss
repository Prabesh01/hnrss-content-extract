<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 05 Sep 2025 00:40:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Updating restrictions of sales to unsupported regions]]></title>
            <link>https://www.anthropic.com/news/updating-restrictions-of-sales-to-unsupported-regions</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133344</guid>
            <description><![CDATA[Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.]]></description>
            <content:encoded><![CDATA[Anthropic's Terms of Service prohibit use of our services in certain regions due to legal, regulatory, and security risks. However, companies from these restricted regions—including adversarial nations like China—continue accessing our services in various ways, such as through subsidiaries incorporated in other countries.Companies subject to control from authoritarian regions like China face legal requirements that can compel them to share data, cooperate with intelligence services, or take other actions that create national security risks. These requirements make it difficult for companies to resist these pressures regardless of where they operate or of the personal preferences of the individuals at those companies. When these entities access our services through subsidiaries, they could use our capabilities to develop applications and services that ultimately serve adversarial military and intelligence services and broader authoritarian objectives. They could also potentially use our models to advance their own AI development through techniques like distillation and to compete globally with trusted technology companies headquartered in the United States and allied countries.To account for this reality and better align with our commitment to ensuring that transformative AI capabilities advance democratic interests, we are strengthening our regional restrictions. This update prohibits companies or organizations whose ownership structures subject them to control from jurisdictions where our products are not permitted, like China, regardless of where they operate. This includes entities that are more than 50% owned, directly or indirectly, by companies headquartered in unsupported regions. This change ensures our Terms of Service reflect real-world risks and are true to the spirit of our policies.Consistent with this concern, we continue to advocate for policies like strong export controls to prevent authoritarian nations from developing frontier AI capabilities that could threaten national security, accelerating energy projects on US soil to build out large-scale infrastructure for AI scaling, and rigorously evaluating AI models for national security relevant capabilities, including those that could be exploited by US adversaries.The safety and security of AI development requires collective commitment to preventing its misuse by authoritarian adversaries. Responsible AI companies can and should take decisive action to ensure that transformative technologies serve US and allied strategic interests and support our democratic values.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unix Conspiracy (1991)]]></title>
            <link>http://www.catb.org/~esr/jargon/html/U/Unix-conspiracy.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133289</guid>
            <description><![CDATA[[ITS] According to a conspiracy theory long popular among
   ITS and TOPS-20 fans, Unix's
   growth is the result of a plot, hatched during the 1970s at Bell Labs,
   whose intent was to hobble AT&T's competitors by making them dependent
   upon a system whose future evolution was to be under AT&T's control.
   This would be accomplished by disseminating an operating system that is
   apparently inexpensive and easily portable, but also relatively unreliable
   and insecure (so as to require continuing upgrades from AT&T).  This
   theory was lent a substantial impetus in 1984 by the paper referenced in
   the back door entry.]]></description>
            <content:encoded><![CDATA[ [ITS] According to a conspiracy theory long popular among
   ITS and TOPS-20 fans, Unix's
   growth is the result of a plot, hatched during the 1970s at Bell Labs,
   whose intent was to hobble AT&T's competitors by making them dependent
   upon a system whose future evolution was to be under AT&T's control.
   This would be accomplished by disseminating an operating system that is
   apparently inexpensive and easily portable, but also relatively unreliable
   and insecure (so as to require continuing upgrades from AT&T).  This
   theory was lent a substantial impetus in 1984 by the paper referenced in
   the back door entry.In this view, Unix was designed to be one of the first computer
   viruses (see virus) — but a virus spread to
   computers indirectly by people and market forces, rather than directly
   through disks and networks.  Adherents of this ‘Unix virus’
   theory like to cite the fact that the well-known quotation “Unix is
   snake oil” was uttered by DEC president
   Kenneth Olsen shortly before DEC began actively promoting its own family of
   Unix workstations.  (Olsen now claims to have been misquoted.)If there was ever such a conspiracy, it got thoroughly out of the
   plotters' control after 1990.  AT&T sold its Unix operation to Novell
   around the same time Linux and other free-Unix
   distributions were beginning to make noise.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Is the decline of reading making politics dumber?]]></title>
            <link>https://www.economist.com/culture/2025/09/04/is-the-decline-of-reading-making-politics-dumber</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133222</guid>
        </item>
        <item>
            <title><![CDATA[I ditched Spotify and set up my own music stack]]></title>
            <link>https://leshicodes.github.io/blog/spotify-migration/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133109</guid>
            <description><![CDATA[{/ truncate /}]]></description>
            <content:encoded><![CDATA[

Why I Ditched Spotify, and How I Set Up My Own Music Stack​
For years, I relied on Spotify like millions of others. The convenience was undeniable stream anything, anywhere, discover new music through algorithms, and share playlists with friends. But over time, several issues became impossible to ignore: artists getting paid fractions of pennies per stream, fake Artists and ghost Tracks, AI music and impersonation, creepy age verification complicity and the fact that despite paying monthly, I never actually owned anything.
So I decided to take back control of my music experience. Here's how I built my own self-hosted music streaming setup that gives me everything Spotify offered and more.
tipThere are components of this post which may be improved if you zoom in with your device. The mermaid diagram and code blocks in particular may be hard to read on smaller screens.
High Level Overview​

The Components​
Music Player: Navidrome​
At the core of my setup is Navidrome, an open-source music server that handles streaming your personal music collection.
To access my music from anywhere, I expose Navidrome via a CloudFlare Tunnel, which provides secure access without exposing my home IP address or dealing with port forwarding.
For client apps, I use:

Browser: Navidrome's built-in web player works perfectly
iOS: Play:Sub connects seamlessly
Android: Symfonium offers excellent playback quality and features
Desktop: Feishin provides a native app experience with synced lyrics

DefinitionScrobbling: (Internet slang) To publish one's music-listening habits to the Internet via software, in order to track when and how often certain songs are played.
Every track I play through this setup automatically scrobbles to my Last.fm account, which becomes important for music discovery later.
Music Collection Management: Lidarr​
Lidarr helps manage my music collection by tracking artists and albums I own or purchase. It can monitor for new releases from favorite artists and helps organize my library.
warningLidarr is just a tool. Like any tool, it can be misused.
Yes, people could point it at less-than-legal sources. No, I'm not telling you to do that.
If you want to support artists, buy their work. If you don't, don't pretend Spotify streams are "support."
Important Note: Always ensure you're obtaining music through legal channels such as:

Digital purchases (Bandcamp, iTunes, Amazon, etc.)
Ripping CDs you've purchased
Free legal downloads offered by artists
Music available under Creative Commons licenses

My setup uses sabnzbd integrated with Lidarr for handling downloads of content I've purchased. Both services run in Docker containers and are NOT exposed to the internet for security.
Synced Lyrics: lrcget-kasm​
A feature I missed from Spotify was synced lyrics. lrcget-kasm fills this gap by mass-downloading LRC synced lyrics files for my music library.
Since lrcget is GUI-only (no CLI version yet), I'm using a containerized version of it via Kasm and access it through my browser. It's a bit resource-intensive, so I only run it when adding new music or updating lyrics.
I've opened a feature request for a CLI version, which would make this process more automation-friendly.
Music Discovery: Lidify​
One of Spotify's strongest features was music discovery. For this, I use Lidify, which connects to my Lidarr library and Last.fm account to generate recommendations.
I've also connected my Last.fm scrobbles to ListenBrainz, which promises to build weekly discovery playlists similar to Spotify's in the future.
The Results​
After several months with this setup, I'm extremely satisfied with the results:
How My Solution Compares to Spotify​
FeatureSpotifyMy Self-Hosted StackMusic QualityUp to 320kbpsUnlimited (FLAC/lossless)Monthly Cost$9.99-$14.99One-time server setup + storageArtist Payment~$0.003-0.005 per streamDirect support via purchasesMusic OwnershipRental onlyFull ownership foreverOffline AccessLimited downloadsComplete library availablePrivacyData collection & trackingComplete privacyContent PermanenceCan disappear anytimeNever removed unless I choose
The initial setup took a weekend, but maintenance is minimal. When I want new music, Lidarr handles it automatically. If I need to manually add something, I just drop the files in the right folder and Navidrome indexes them immediately.
Is it for everyone? No. But if you care about music, value ownership, and have basic technical skills, building your own music streaming solution is both achievable and rewarding. The freedom from corporate streaming platforms is worth the effort.
Supporting Artists​
Moving away from Spotify doesn't mean abandoning artists. In fact, I now support musicians more directly by:

Purchasing music directly from platforms like Bandcamp where artists receive 82-90% of sales
Buying physical media from official stores
Supporting Patreon/subscription services for favorite artists
Attending concerts and buying merchandise

Buying a $10 album on Bandcamp puts about $8.20-$9.00 in the artist's pocket. To match that on Spotify, you're talking roughly 1.6k-3k streams of that album per listener. If the artist has a label taking a cut on Spotify, the stream counts needed go up further.
My self-hosted setup is about controlling my listening experience and owning what I pay for, not avoiding fair compensation to artists.
What's Next?​
I'm continually refining this setup. Future improvements include automating the lyrics process and exploring more discovery tools. The beauty of a self-hosted solution is that it can evolve with my needs, rather than changing at the whim of a company's business model.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What Is the Fourier Transform?]]></title>
            <link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132810</guid>
            <description><![CDATA[Amid the chaos of revolutionary France, one man’s mathematical obsession gave way to a calculation that now underpins much of mathematics and physics. The calculation, called the Fourier transform, decomposes any function into its parts.]]></description>
            <content:encoded><![CDATA[
    As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.
It took mathematicians until the 19th century to master this same calculation.
In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and you’ll get your original function. The technique, today called the Fourier transform, allowed the mathematician — previously an ardent proponent of the French revolution — to spur a mathematical revolution as well.
Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.
“It’s hard to overestimate the influence of Fourier analysis in math,” said Leslie Greengard of New York University and the Flatiron Institute. “It touches almost every field of math and physics and chemistry and everything else.”
Flames of Passion 
Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.

Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 — shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British — Fourier could think of nothing else.
If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly — meaning they have more energy — decay first, followed eventually by the lower frequencies. It’s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.
The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work “nothing short of impossible.”
What troubled his peers most were strange cases where the heat distribution might be sharply irregular — like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.
Today, we know that Fourier was broadly right.
“You can represent anything as a sum of these very, very simple oscillations,” said Charles Fefferman, a mathematician at Princeton University. “It’s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethoven’s Ninth Symphony.” The process only fails for the most bizarre functions, like those that oscillate wildly no matter how much you zoom in on them.
So how does the Fourier transform work?
A Well-Trained Ear
Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.
Mathematically, the Fourier transform is a function. It takes a given function — which can look complicated — as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, you’ll get the original function.

        
            
            Samuel Velasco/Quanta Magazine
        
    

To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Let’s look at a simple example.
Consider the following function:

        
    

The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Here’s what happens if we multiply the original by a sine wave with a frequency of 3:

        
    

There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.
Now let’s test if the frequency 5 is present. Here’s what you get when you multiply the original function by a sine wave with the frequency 5:

        
    

There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.
The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)
In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.
If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and — despite mathematicians’ early hesitation to accept such a thing — it is now an essential tool in the analysis of functions.

        
    

Encore
The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns — and simple combinations of them that resemble checkerboards — can be added together to re-create any image.
Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.

        
    

In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly — aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. “It’s now a part of everyday life,” Greengard said.
It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics — the physics of the very small — it even provides the mathematical foundation for the uncertainty principle, which says that it’s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particle’s possible positions; the Fourier transform of that function will describe the particle’s possible momenta. When your function can tell you where a particle will be located with high probability — represented by a sharp peak in the graph of the function — the Fourier transform will be very spread out. It will be impossible to determine what the particle’s momentum should be. The opposite is also true.
        
        
The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis — which studies the Fourier transform, as well as how to reverse it to rebuild the original function — is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. They’ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.
“If people didn’t know about the Fourier transform, I don’t know what percent of math would then disappear,” Fefferman said. “But it would be a big percent.”
Editor’s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between Quanta Magazine and the Simons Foundation is available here. 
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Memory is slow, Disk is fast – Part 2]]></title>
            <link>https://www.bitflux.ai/blog/memory-is-slow-part2/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132710</guid>
            <description><![CDATA[Sourcing data directly from disk IS faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.]]></description>
            <content:encoded><![CDATA[
                TL;DR
Sourcing data directly from disk IS faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.
Introduction
In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades.  I then asserted, without proof, that sourcing data from disk can be faster than from memory.  What follows is the proof.
Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast.  Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.
Experimental set up
We need data and something straight forward to do with the data.  I used my free will or the illusion thereof to create a benchmark I cleverly call "counting 10s".  I write some pseudo random integers between 0 and 20 to a buffer and then count how many of the integers are 10.  I want to make sure we are doing all the counting in a single thread to simulate an Amdahl's Law situation.
So how fast can we expect this to run?  The upper limit would be the memory bandwidth.
My testing rig is a server with an old AMD EPYC 7551P 32-Core Processor on a Supermicro H11SSL-i and 96GB of DDR4 2133 MHz and a couple of 1.92TB Samsung PM983a PCIe 3.0 SSDs I pieced together from EBay parts.  Given the way this server is configured, the upper limit for memory bandwidth can be calculated as 3 channels * 2133MT/s * 8B/T / 4 numa domains = ~13GB/s for a single thread.  It's kind of an odd system but that just makes it more fun to optimize for!
The disks are rated at 3.1GB/s read BW each for an upper limit of 6.2GB/s.  I made a raid0 volume with 4KB stripe size, formatted the the raid as ext4 with no journaling, and made sure it fully finished initializing the metadata before running the tests.
sudo mdadm --create /dev/md0 --level=0 --raid-devices=2 --chunk=4K /dev/nvme1n1 /dev/nvme2n1
sudo mkfs.ext4 -F -L data -O ^has_journal -E lazy_itable_init=0 /dev/md0
sudo mount -o noatime /dev/md0 mnt

We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.
Simple Loop
The simple and cleanest way to do this in C would look like this.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }

    printf("Found %ld 10s\n", count);
}

Just mmap() the file which will give us a buffer that we can read from.  Then we just loop and count the 10s.
Because the point is to benchmark we will integrate some timing mechanisms before we move on.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <sys/time.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    long start = get_time_us();
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

For the first run we're going to be reading from the disk. The disk/filesystem read is going to limit the performance before the memory bandwidth can.
❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 0.61 GB/s

As expected, it's not anywhere near memory speeds because as everyone knows, disk is slow.  We can look at the system and confirm that the first run cached the data to memory.

Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.
❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 3.71 GB/s


It is faster, but clearly that’s slower than the memory can feed it to the processor.  What bottleneck might we be hitting?  This speed does look possibly correlated to the instructions per second limit for this generation of CPU (between 2GHz * 1.5 IPC = 3G and 3GHz boost * 1.5 IPC = 4.5G instructions per second).
We can use perf to see if the CPU is using vector instructions, if not then the actual compute is the bottleneck.
Percent│      test     %rbp,%rbp
       │    ↓ je       84
       │      lea      (%rbx,%rbp,4),%rcx
       │      mov      %rbx,%rax
       │      xor      %ebp,%ebp
       │      nop
       │70:   xor      %edx,%edx
  1.31 │      cmpl     $0xa,(%rax)
 42.38 │      sete     %dl
 45.72 │      add      $0x4,%rax
  0.01 │      add      %rdx,%rbp
 10.42 │      cmp      %rax,%rcx
  0.16 │    ↑ jne      70
       │84:   xor      %eax,%eax
       │      shr      $0x14,%r12
       │    → call     get_time_us
       │      pxor     %xmm0,%xmm0
       │      pxor     %xmm1,%xmm1

Confirmed. We're running non-vectorized instructions, with a single thread counting that's as fast as it can go with a 2GHz CPU.  Well crap.  We’ve hit our first non-exponential limit.  Even a brand new CPU running this machine code would probably struggle to do much better than a 50% improvement, still well below the memory bandwidth limit.
Unrolling the loop
Good news is this code can definitely be vectorized if we help the compiler.  Unroll the loop!
We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <stdint.h>
#include <sys/time.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_unrolled
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    void* buffer = mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    // Get the compiler to align the buffer
    const int * __restrict data = (const int * __restrict)__builtin_assume_aligned(buffer, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    long start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(data[i+ 0] == 10);
        c1  += (unsigned)(data[i+ 1] == 10);
        c2  += (unsigned)(data[i+ 2] == 10);
        c3  += (unsigned)(data[i+ 3] == 10);
        c4  += (unsigned)(data[i+ 4] == 10);
        c5  += (unsigned)(data[i+ 5] == 10);
        c6  += (unsigned)(data[i+ 6] == 10);
        c7  += (unsigned)(data[i+ 7] == 10);
        c8  += (unsigned)(data[i+ 8] == 10);
        c9  += (unsigned)(data[i+ 9] == 10);
        c10 += (unsigned)(data[i+10] == 10);
        c11 += (unsigned)(data[i+11] == 10);
        c12 += (unsigned)(data[i+12] == 10);
        c13 += (unsigned)(data[i+13] == 10);
        c14 += (unsigned)(data[i+14] == 10);
        c15 += (unsigned)(data[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    long elapsed = get_time_us() - start;

    printf("unrolled loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

Check if we now have vectorized instructions with perf.
Percent│       movq      %xmm0,%rcx
       │       movdqa    %xmm7,%xmm14
       │       pxor      %xmm0,%xmm0
       │       nop
       │ e8:   movdqa    %xmm6,%xmm4
  0.30 │       movdqa    %xmm6,%xmm3
  0.12 │       movdqa    %xmm6,%xmm2
  0.35 │       add       $0x1,%rdx
  1.54 │       pcmpeqd   (%rax),%xmm4
 54.64 │       pcmpeqd   0x10(%rax),%xmm3
  1.62 │       movdqa    %xmm6,%xmm1
  0.99 │       add       $0x40,%rax
  0.12 │       pcmpeqd   -0x20(%rax),%xmm2
  3.03 │       pcmpeqd   -0x10(%rax),%xmm1
  1.32 │       pand      %xmm5,%xmm4
  1.25 │       pand      %xmm5,%xmm3
  1.55 │       movdqa    %xmm4,%xmm15
  0.24 │       punpckhdq %xmm0,%xmm4


Confirmed. We're using 128bit vector instructions, this should be up to 4x faster than the original.

NOTE: These are 128-bit vector instructions, but I expected 256-bit.  I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions.  I forced the compiler to use 256-bit instructions and found it was actually slower.  Looks like the compiler was smart enough to know that here.

Let's benchmark this unrolled version with the data as page cache in memory.
❯ sudo  ./count_10_unrolled ./mnt/datafile.bin 53687091200
unrolled loop found 167802249 10s processed at 5.51 GB/s


We're still nowhere close to hitting the memory bus speed limit of 13GB/s but 50% faster than the original is a win.  There must be some other bottleneck.
Can the SSDs beat that?
5.51GB/s?  On paper the SSDs can read at 6.2GB/s, but the first run from disk only did 0.61GB/s.  How can I meet or beat this performance sourcing the data directly from disk?
Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk.  When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk.  Unfortunately these legacy mechanisms just aren't set up for serious high performance IO.  Note that at 610MB/s it's faster than what a disk SATA can do.  On the other hand, it only managed 10% of our disk's potential.  Clearly we're going to have to do something else.
SSDs don't just automatically read data at multigigabyte speeds.  You need to put some real effort into an IO pipeline to get serious performance.
I made a io_uring based IO engine, a kind of userspace driver, that can hit these speeds.  The main thread will request data, the IO engine will handle the IO, then the main thread will do the counting when the data is in a buffer.  We will use a set of queues to manage the IO requests, responses, and buffers.  The IO engine will start 6 workers, target a queue depth of 8192, and have a buffer size of 16KB.
I wish I had tighter code here, but A) I didn’t have time to clean it up B) some of the complexity is intractable.  The IO engine code was a lot to scroll through so I moved it to github link
#include "io_engine.h"
#include <sys/mman.h>
#include <getopt.h>
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <stdint.h>
#include <sys/time.h>

#define DEFAULT_WORKERS 6
#define DEFAULT_BLOCK_SIZE 16384
#define DEFAULT_QUEUE_DEPTH 8192

// Count the number of "10" (int format) in the buffer
static inline size_t count_tens_unrolled(void* data, size_t size_bytes) {
    const size_t total = size_bytes / sizeof(int);
    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned(data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    return (t0 + t1) + (t2 + t3);
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);

    // Set up the io engine
    ioengine_t* na = ioengine_alloc(filename, size_bytes, DEFAULT_QUEUE_DEPTH, DEFAULT_BLOCK_SIZE, DEFAULT_WORKERS);

    sleep(1);

    // Use the background workers to read file directly
    size_t total_blocks = na->file_size / na->block_size;
    uint64_t uid = 1;
    size_t count = 0;

    long start = get_time_us();

    // Read all blocks
    size_t blocks_queued = 0;
    size_t blocks_read = 0;
    int buffer_queued = 0;
    while (blocks_read < total_blocks) {
        //// Queue IO phase //////
        //     Do we have more blocks to queue up?
        if (buffer_queued < na->num_io_buffers/2 && blocks_queued <= total_blocks) {
            // Calculate how many blocks on average we want our workers to queue up
            size_t free_buffers = (size_t)(na->num_io_buffers - buffer_queued - 4); // hold back a few buffers
            size_t blocks_remaining = total_blocks - blocks_queued;  // how many blocks have we not queued
            size_t blocks_to_queue = free_buffers > blocks_remaining ? blocks_remaining : free_buffers;
            int blocks_to_queue_per_worker = (int) (blocks_to_queue + na->num_workers - 1) / na->num_workers;
            // Iterate through workers and assign work
            for (int i = 0; i < na->num_workers; i++) {
                worker_thread_data_t* worker = &na->workers[i];
                // Try to queue N blocks to this worker
                for (int j = 0; j < blocks_to_queue_per_worker; j++) {
                    if (blocks_queued == total_blocks) break;
                    int bgio_tail = worker->bgio_tail;
                    int bgio_head = worker->bgio_head;
                    int bgio_next = (bgio_tail + 1) % worker->num_max_bgio;
                    int next_bhead = (worker->buffer_head + 1) % worker->num_max_bgio;
                    if (bgio_next == bgio_head) break;  // queue for send requests is full
                    if (next_bhead == worker->buffer_tail) break; // queue for recieving completed IO is full
                    // Queue this block with the worker.  We have to track which buffer it's going to.
                    int buffer_idx = worker->buffer_start_idx + worker->buffer_head;
                    na->buffer_state[buffer_idx] = BUFFER_PREFETCHING;
                    worker->bgio_uids[bgio_tail] = (uid++)<<16; // unique id helps track IOs in io_uring, we encode 4 bytes later
                    worker->bgio_buffer_idx[bgio_tail] = buffer_idx;
                    worker->bgio_block_idx[bgio_tail] = blocks_queued++;  // block sized index into file
                    worker->bgio_queued[bgio_tail] = -1;  // Requested but not yet queued
                    int next_tail = (bgio_tail + 1) % worker->num_max_bgio;
                    worker->bgio_tail = next_tail;
                    // Log the buffer in an ordered queue for us to read
                    worker->complete_ring[worker->buffer_head] = buffer_idx;
                    worker->buffer_head = next_bhead;
                    buffer_queued++;
                }
                // Tell the worker to submit IOs as a group
                worker->bgio_submit++;
            }
        }

        //// Completion Phase //////
        //     Iterate through worker and check if they have complete IOs
        for (int i = 0; i < na->num_workers; i++) {
            worker_thread_data_t* worker = &na->workers[i];
            int current = worker->buffer_tail;
            // We know what IO's we're waiting on, but we have to poll
            //  to see if they are done.
            for (int scan = 0; scan < worker->num_max_bgio; scan++) {
                // Scan until we get to the end of the list
                if (current == worker->buffer_head) break;
                int buffer_idx = worker->complete_ring[current];
                int state = na->buffer_state[buffer_idx];
                if (state == BUFFER_PREFETCHED) {
                    // This buffer is completed - Process this buffer.
                    count += count_tens_unrolled(na->io_buffers[buffer_idx], na->block_size);
                    na->buffer_state[buffer_idx] = BUFFER_UNUSED;
                    blocks_read++;
                    buffer_queued--;
                }
                current = (current + 1) % worker->num_max_bgio;
            }
            // IO's might have been completed out of order, advance the tail when we can
            current = worker->buffer_tail;
            while (current != worker->buffer_head) {
                int buffer_idx = worker->complete_ring[current];
                int state = na->buffer_state[buffer_idx];
                if (state != BUFFER_UNUSED) break;
                current = (current + 1) % worker->num_max_bgio;
            }
            worker->buffer_tail = current;
            worker->bgio_submit++;  // probably unnecessary
        }
    }
    long elapsed = get_time_us() - start;
    printf("diskbased found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));

    // Cleanup I/O system
    ioengine_free(na);

    return 0;
}

I hope all this extra code makes it faster.
❯ sudo ./diskbased/benchmark ./mnt/datafile.bin 53687091200
diskbased found 167802249 10s processed at 5.81 GB/s


Boom!  Disk is faster than memory!  It takes several hundred lines of code but now we can source the data from my SSDs faster than the copy from the page cache in memory.
So what's going on here?
Of course my 6GB/s disk stripe isn’t actually faster than the memory bus, even on this weird hack of a system.  So what is happening?  Where is the bottleneck?  It's got to be the way the data is being read from the page cache in memory.
What if we replace the mmap() with a read() from disk into a preallocated buffer.  That way we can measure the counting with the data in-memory without any page cache related overhead mmap() can introduce.
#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdint.h>
#include <string.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY|O_DIRECT);
    void *buf;
    posix_memalign(&buf, 4096, size_bytes);
    int *data = buf;

    size_t off = 0;
    while (off < size_bytes) {
        ssize_t n = read(fd, (char*)data + off, size_bytes - off);
        off += (size_t)n;   // YOLO: assume n > 0 until done
    }

    long start = get_time_us();
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));


    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned((void*)data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    elapsed = get_time_us() - start;

    printf("unrolled loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

If we keep the dataset smaller than a numa domain and we bind this to a single numa node to prevent numa overheads we see that the theoretical memory bandwidth we projected seems to be the primary bottleneck for the unrolled loop as we hoped to see at the outset.
❯  sudo numactl --cpunodebind=0   ./in_ram mnt/datafile.bin 2147483648
simple loop 6709835 10s processed at 4.76 GB/s
unrolled loop 6709835 10s processed at 13.04 GB/s

But this isn't useful to compare the with the other runs with the 50GB dataset.  However if we do the full 50GB dataset the performance suffers.  We have to get much of the data across numa domains which is going to be higher cost.
❯ sudo ./in_ram ./mnt/datafile.bin 53687091200
simple loop 167802249 10s processed at 3.76 GB/s
unrolled loop 167802249 10s processed at 7.90 GB/s


Comparing the results of "fully in-memory (50GB)" which is pre-loaded in memory before measuring against the "unrolled loop" that is only cached in memory we see 40% overhead.  That's 2.75 seconds out of 9 seconds that was spent waiting on the caching system instead of counting.  Why so much?
mmap()
The mmap() call presents the process with a buffer that is a blank slate even when the data is already in memory.  The buffer is populated page by page as it's accessed from the page cache.  This isn't a copy, it's just the operating system mapping the cached memory into the process.  This costs more than it might seem.  The worst case with mmap() the counting has to pause at every 4KB page boundary while the kernel processes a fault, tracks down the page of data in the page cache, then updates the page table of the process to insert the memory into the process.  Fundamentally this is a process that is limited by the memory latency, not the CPU speed or memory bandwidth.  With the potential for TLB walks and searching lists that track the page cache, we’re taking potentially dozens of CPU cache misses and several microseconds of waiting on memory for every 4KB page.
direct IO
Using our direct from disk approach uses pipelines and streams which avoids the kind of memory latency dominated bottleneck that mmap() has.  In our case we're limited by the bandwidth of our disks yet because of the pipelining, the larger latency of the IOs doesn't get in the critical path of the counting very much.  Allowing for higher throughput.
Scaling
Consider the implications of these experiments as we scale.  The well vetted solution to get data from memory to a process is slower than using the disk directly.  This isn't because the memory is slower than the disk.  The memory has higher bandwidth than the disk, not by an order of magnitude, but a decent margin.  But the latency of the memory is orders of magnitude lower than the disk.  Nevertheless the way the data in memory is accessed is the culprit.  Its a synchronous approach that assumes memory operations are cheap and low latency.  These accesses add up and it ends up waiting on memory latencies.  The disk method on the other hand is as a streaming approach built to leverage bandwidth and hide latencies.
extending the existing rig
If I got a few more of these disks I could push the IO bandwidth to be greater than the 13GB/s per thread memory bandwidth limit.  IO is DMA'ed to buffers that are pretty small compared to the total dataset. These buffers scale with the throughput capabilities of the CPU and the disks, not the dataset size. The buffers can be located in a single numa domain allowing us to avoid the overhead of accessing the buffers between NUMA domains.  Add more disks to this system I might be able to create a disk based solution to count at the full 13GB/s rather than be limited to the 7.90GB/s we see with the in memory example at the full 50GB dataset.  With such a system our throughput would not be affected by the dataset size, unlike the in-memory case, which has numa overhead and eventually runs out of memory to scale.
faster than memory is possible
On a proper modern server the CPUs will let you do IO directly to the L3 cache, bypassing memory altogether.  Because PCIe bandwidth is higher than memory bandwidth, on paper we could even get more max bandwidth than we can get from memory if we carefully pin the buffers into the CPU cache.  I haven't confirm this works in practice, however, it could be made to work and is the sort of thing that CPU designs will be forced to lean into to push performance forward.
memory is changing too
This isn't just about disks vs memory.  Similar techniques and principles apply to memory.  Memory bandwidth is still scaling even if the latency is not.  This means to take advantage of memory performance you have to actually treat it more like a disk and less like Random Access Memory.  To scale performance with generational updates you have to make sure to stream data from memory into the CPU caches in blocks, similar to how data is streamed from disk to memory.  If not you end up with 90s level memory throughput.  A custom mechanism to cache data in memory could easily avoid the memory latency problems seen with the default mmap() solution with much less code than the io_uring solution.
Is this worth it?
I'm not going to say that going to the effort of implementing something like this is always worth it.  The mmap() method is sure elegant from a coding perspective, especially when compared to all the code I had to write to get the io_uring setup working.  Sometimes the simple way is the way to go.
Is using 6 cores of IO for 1 core of compute is always the right answer?  Probably not.  This was an extreme situation to prove a point.  In realworld situations you'll need to look at the tradeoffs and decide what's best for your use case.  Correctly understanding the strengths and weaknesses of the hardware can open up a number of possibilities where you can get a lot more performance for a lot less money.
The kind of overhead demonstrated with mmap() isn’t going to go away, new hardware isn't going to fix it.  At the same time disk bandwidth and the number of cores are scaling each generation.  But doing things that scale performance with new technology is going to take extra code and effort.
But don't just blow this stuff off.  Sure you can dedicate a server with 3TB of memory to serve 10K client connections. Memory in the cloud is like ~$5/GB/month, if you can afford it, then you do you.  However it is worth considering that humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app.  I figure either the karmic debt to the planet, or a vengeful AI demigod hungry for silicon and electricity will come for those that don't heed this warning, eventually.  Either way my conscience is clear.
Recap

Memory is slow - when you use it oldschool.
Disk is fast - when you are clever with it.
Test the dogma - compounded exponentials are flipping somethings from true to false.

Bad news is that this cleverness requires extra code and effort.
Good news is we now have AI to write and test the extra code this cleverness requires.
Better news is that, for those that are willing to learn, AI's don't do this unless you know how to ask them.
Lean into things that scale, avoid things that don’t.
Next Time
What will be revealed in the next episode?

Is O(√n) actually faster than O(log n)?  Will the foundations of Computer Science survive this unveiling?
Will traditional code be consumed into the latent space of our AI overlords?
Is AI hiding these performance gains from me?  Is AI even capable of writing optimized code?


Jared Hulbert

A few notes for the "um actually" haters commenting on Hacker News:

This is not and does not claim to be an academic paper.
I do not intend to prove that NAND is a drop in replacement for DRAM.
Tis but a humble and hopefully fun exercise in exploring the limits and trends of modern hardware and the tradeoffs needed to maximize performance.
As I stated before I have no problem with your choice to ignore this and write lazy code that will perform just as fast on new hardware in 15 years as it does on todays hardware.  In fact I applaud your choice.  Jeff Bezos has an orbital yacht to build, someone has to pay for it, why not you?
I am not an AI.  I am a human with a computer that don't write perfect.



source code can be found here.


            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite?]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132498</guid>
            <content:encoded><![CDATA[





Small. Fast. Reliable.Choose any three.











Introduction

Suppose the
OpenDocument file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:

Smaller documents
Faster File/Save times
Faster startup times
Less memory used
Document versioning
A better user experience



Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

About OpenDocument And OpenDocument Presentation


The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
NeoOffice on Mac, or
LibreOffice on Linux and Windows.


An OpenDocument Presentation or "ODP" file is a
ZIP archive containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
SouthEast LinuxFest
conference:

Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
... 58 other pictures omitted...
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files



The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

application/vnd.oasis.opendocument.presentation


The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

Limitations Of The OpenDocument Presentation Format


The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:


Incremental update is hard.

It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

Startup is slow.

In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

More memory is required.

Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.


One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentations) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.


Crash recovery is difficult.

The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

Content is inaccessible.

One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

<?xml version="1.0" encoding="UTF-8"?>


The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.


First Improvement:  Replace ZIP with SQLite


Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);



For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.




Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
SQLAR utility:

-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp



The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.


The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
SQLite databases are atomic.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

Second Improvement:  Split content into smaller pieces


A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
of all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.


So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional


The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

SELECT slideContent FROM slide WHERE pageNumber=1;


This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order to render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facilitate rapid transitions to the next slide.


Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just a
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.


Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.


One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

Third Improvement:  Versioning


Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);



In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.


On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;



Or perhaps the application would rather use the
most recent checkinTime:

SELECT manifest, versionId, max(checkinTime) FROM version;



Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

Or, multiple presentations could be stored within the same document.

With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.


There are details to fill in here.
Perhaps a screen can be provided that displays all historical changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

And So Forth...


In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:

 Store an automated undo/redo stack in a database table so that
     Undo could go back into prior edit sessions.
 Add full text search capabilities to the slide deck, or across
     multiple slide decks.
 Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
 Break out the "Presenter Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
 Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.



An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.


Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
I/O performance is better than using separate
files.


Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into
Third Normal Form (3NF)
and store only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

Review Of The Benefits Of Using SQLite


In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:



An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.


The atomic update capabilities
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.


Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.


The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.


The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.



These are just a few of the benefits of using SQLite as an application file
format — the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the Application File Format
document for additional ideas.


Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.
This page last modified on  2025-05-12 11:56:41 UTC 

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rocketships and Slingshots]]></title>
            <link>https://postround.substack.com/p/rocketships-and-slingshots</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132183</guid>
        </item>
        <item>
            <title><![CDATA[ICPC 2025 World Finals Results]]></title>
            <link>https://worldfinals.icpc.global/scoreboard/2025/index.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131921</guid>
            <description><![CDATA[PC^2 Homepage
                
                					CSS by Tomas Cerny and Ray Holder
				
				Created by CSUS PC^2 version 
				9.11build 20250826 build 7926
            
            				Last updated
				Thu Sep 04 15:49:37 AZT 2025]]></description>
            <content:encoded><![CDATA[
            
                
                    RankNameSolvedTimeABCDEFGHIJKLTotal att/solv
                
                
                    191 St. Petersburg State University1114782/2552/530/--1/373/1501/203/2982/1951/422/813/1281/1921/11
                
                
                    299 The University of Tokyo1011161/2061/2710/--1/251/1841/340/--1/951/662/721/1261/1711/10
                
                
                    313 Beijing Jiaotong University1014252/2252/2041/--1/572/2702/520/--2/1322/842/1071/1481/618/10
                
                
                    4100 Tsinghua University98653/1390/--1/--1/331/1742/271/--1/711/872/841/1571/1315/9
                
                
                    576 Peking University98871/1800/--0/--1/142/1681/500/--1/1131/571/761/1831/2610/9
                
                
                    635 Harvard University99951/1240/--0/--1/351/2384/780/--2/1901/621/1011/691/1813/9
                
                
                    7140 University of Zagreb910752/1760/--0/--1/151/2422/770/--3/1752/641/641/1182/2415/9
                
                
                    860 Massachusetts Institute of Technology911231/2332/--0/--1/141/1131/360/--6/2041/504/1211/1452/2720/9
                
                
                    9131 University of Science and Technology of China911282/2240/--0/--1/411/2861/310/--2/2291/551/851/1231/1411/9
                
                
                    1083 Seoul National University911331/2013/2820/--1/320/--3/580/--1/1601/461/792/1611/1414/9
                
                
                    11129 University of Novi Sad911752/2442/--0/--1/392/2591/260/--1/1581/491/963/2081/1615/9
                
                
                    1282 Saratov State University911911/2033/--0/--2/372/2383/740/--1/1462/1041/1111/1641/1417/9
                
                
                    1357 Karlsruhe Institute of Technology911992/1331/--0/--1/452/2921/590/--2/2071/232/921/2341/3414/9
                
                
                    14127 University of Maryland912391/1064/--0/--1/171/2331/650/--3/2581/432/2251/1642/4817/9
                
                
                    1569 National Taiwan University912561/1900/--0/--2/452/2781/1160/--2/1821/1181/771/1731/1712/9
                
                
                    1687 Sharif University of Technology913291/734/--0/--1/231/2441/440/--4/2573/1364/1581/2131/2121/9
                
                
                    178 Arizona State University913311/2542/2980/--1/400/--1/940/--1/1692/692/1251/1991/2312/9
                
                
                    1838 HSE University86531/1680/--0/--1/382/--2/320/--2/1441/561/721/911/1212/8
                
                
                    1920 Carnegie Mellon University87661/2170/--0/--1/371/--1/510/--2/1791/611/741/1181/910/8
                
                
                    20126 University of Illinois Urbana-Champaign87742/2520/--0/--2/360/--1/1716/--1/1041/871/601/1621/1626/8
                
                
                    21144 Zhongshan (Sun Yat-sen) University88002/2760/--0/--1/363/--1/240/--2/1041/461/922/1511/1114/8
                
                
                    2255 KAIST88292/2591/1590/--1/250/--1/681/--3/--1/421/811/1431/3213/8
                
                
                    23143 Zhejiang University88711/1860/--0/--1/291/--1/450/--2/1442/1042/1151/1751/1312/8
                
                
                    2459 Kyoto University88801/2091/--0/--1/362/--2/820/--2/1531/592/1201/1461/1514/8
                
                
                    2571 National University of Singapore89231/1190/--0/--1/160/--1/291/2706/--1/852/452/2991/2016/8
                
                
                    2634 Harbin Institute of Technology89482/2380/--0/--1/700/--1/580/--1/1471/781/891/2311/179/8
                
                
                    2748 Institute of Science Tokyo89752/2212/--0/--1/672/--1/330/--1/2111/753/1471/1441/1715/8
                
                
                    2886 Shanghai University810091/26311/--0/--1/740/--2/570/--1/1541/992/662/2221/1422/8
                
                
                    2936 Hasso Plattner Institute810341/2601/--0/--1/390/--1/660/--1/1511/753/1341/2023/2713/8
                
                
                    3085 Shanghai Jiao Tong University810591/2410/--0/--1/200/--1/350/--2/1732/631/1145/2791/1414/8
                
                
                    3112 Beihang University810601/1940/--0/--1/394/--1/670/--2/2131/572/1133/2841/1316/8
                
                
                    32132 University of Science, VNU-HCM810901/2200/--1/--1/381/--1/260/--4/2381/902/1311/2501/1714/8
                
                
                    33121 University of California, Berkeley810921/2670/--0/--1/350/--1/540/--1/2031/941/1132/2871/199/8
                
                
                    3422 Central South University811391/2960/--0/--1/410/--1/610/--1/1943/1061/1421/2421/1710/8
                
                
                    3516 BINUS University811591/2940/--0/--1/410/--1/610/--3/2711/822/1321/2031/1511/8
                
                
                    3631 ETH Zürich812361/22113/--0/--2/890/--2/1210/--4/2462/403/1411/1891/2929/8
                
                
                    3758 Korea University812584/2921/--0/--1/990/--3/590/--2/2841/322/1121/1593/4118/8
                
                
                    3863 Moscow State University812611/2790/--0/--1/532/--2/630/--2/2181/925/1493/2291/1818/8
                
                
                    39117 Università di Pisa813435/2570/--0/--1/890/--2/550/--5/2941/992/1911/743/4420/8
                
                
                    4026 Delft University of Technology76033/--1/--0/--1/220/--1/590/--1/1091/1332/881/1591/1312/7
                
                
                    4178 Pohang University of Science and Technology76921/2560/--0/--1/210/--1/550/--3/--1/862/442/1641/2612/7
                
                
                    4253 Jagiellonian University in Krakow77184/--1/--0/--1/404/1351/500/--5/--1/571/1031/2332/2021/7
                
                
                    43137 University of Wroclaw77222/2680/--0/--1/350/--1/530/--1/1311/861/641/--2/4510/7
                
                
                    4495 The University of British Columbia77743/--0/--0/--1/260/--1/420/--3/2191/692/861/2501/2213/7
                
                
                    45125 University of Hong Kong77830/--0/--0/--1/440/--1/590/--1/1611/812/1081/2612/299/7
                
                
                    46141 UNSW Sydney77871/1450/--0/--1/304/--3/618/--0/--3/862/1162/1732/3626/7
                
                
                    4796 The University of Chicago78041/--4/2610/--1/280/--3/840/--0/--1/811/352/1461/4914/7
                
                
                    4825 De La Salle University78050/--0/--0/--1/480/--1/250/--2/2173/781/1171/2321/2810/7
                
                
                    4933 Georgia Institute of Technology78264/--0/--0/--1/192/--1/650/--3/2741/991/901/2121/2715/7
                
                
                    50123 University of Cambridge78271/--0/--0/--1/370/--2/770/--1/2691/1211/1362/922/3511/7
                
                
                    5194 Texas A&M University78283/--0/--0/--1/200/--2/590/--2/1583/1141/713/2282/3817/7
                
                
                    52124 University of Central Florida78822/2730/--0/--1/200/--1/420/--2/2043/1321/1090/--1/2211/7
                
                
                    5361 Moscow Aviation Institute79140/--0/--0/--1/460/--1/750/--2/2821/643/1941/1432/3011/7
                
                
                    5430 Ecole Polytechnique Fédérale de Lausanne79213/--0/--0/--1/440/--2/990/--1/2241/764/1681/1752/3515/7
                
                
                    55101 Union University - Faculty of Computer Science79250/--1/--0/--1/843/1421/420/--5/2611/752/1530/--1/2815/7
                
                
                    5690 St. Petersburg ITMO University79290/--0/--0/--1/164/--1/430/--1/1921/1084/1821/2971/3114/7
                
                
                    5710 Astana IT University79533/--0/--0/--1/500/--3/860/--3/1871/582/1981/2591/1515/7
                
                
                    58136 University of Warsaw79671/--0/--0/--1/430/--1/650/--7/1931/692/2261/2121/1915/7
                
                
                    5980 Rutgers University79870/--0/--0/--3/750/--1/560/--1/2481/1233/1651/2271/1311/7
                
                
                    6023 Chennai Mathematical Institute79920/--0/--0/--1/432/2601/660/--0/--1/1185/1552/1882/2214/7
                
                
                    6152 International IT University710050/--0/--0/--2/690/--1/890/--1/2201/1212/1691/2771/209/7
                
                
                    6215 Belarusian State University710110/--0/--0/--1/475/2891/790/--0/--1/1031/1242/1913/3814/7
                
                
                    6318 Brigham Young University711960/--2/2470/--1/650/--2/850/--0/--1/1334/2281/2982/2013/7
                
                
                    6465 Nanjing University of Science and Technology713403/2780/--0/--2/800/--6/1070/--0/--3/1142/1475/2931/2122/7
                
                
                    65122 University of California, Los Angeles64364/--4/--0/--1/280/--2/430/--2/1191/782/870/--1/2117/6
                
                
                    66115 Universidade Federal de Minas Gerais64750/--0/--0/--1/370/--1/490/--1/1801/861/1050/--1/186/6
                
                
                    67135 University of Toronto65440/--0/--0/--1/470/--2/580/--15/--1/722/1101/1851/3223/6
                
                
                    6866 National Economics University65600/--0/--0/--1/200/--1/500/--2/--2/991/1061/2381/279/6
                
                
                    6998 The University of Texas at Dallas65906/--0/--0/--1/580/--2/400/--3/1971/1002/740/--2/2117/6
                
                
                    7014 Beijing University of Posts and Telecommunications65960/--0/--0/--1/670/--1/480/--0/--1/691/2211/1701/216/6
                
                
                    7164 Nanjing University of Aeronautics and Astronautics65990/--0/--0/--1/710/--2/420/--11/--2/541/901/2901/1219/6
                
                
                    7241 Indian Institute of Technology - Indore66110/--0/--0/--2/810/--1/560/--0/--1/1061/1401/1911/177/6
                
                
                    7351 International Institute of Information Technology, Hyderabad66392/--0/--0/--1/170/--2/770/--1/2021/1211/1710/--1/319/6
                
                
                    7472 National Yang Ming Chiao Tung University66580/--0/--0/--2/400/--1/590/--1/2642/1111/1240/--1/208/6
                
                
                    75133 University of Tartu67250/--0/--0/--1/711/2661/900/--2/--1/1101/1500/--1/388/6
                
                
                    76103 Universidad de Buenos Aires - FCEN67250/--0/--0/--1/450/--1/670/--5/2811/1251/1043/--1/2313/6
                
                
                    7728 Ecole Polytechnique67265/--0/--0/--1/770/--1/520/--4/--2/871/1196/2571/1421/6
                
                
                    78120 University of Belgrade67820/--0/--1/--1/590/--4/1120/--0/--2/971/1752/1991/4012/6
                
                
                    7973 Neapolis University Pafos67910/--2/--0/--1/760/--2/390/--3/2891/1003/1436/--2/2420/6
                
                
                    8068 National Institute of Technology, Tokuyama College68030/--0/--0/--1/650/--1/350/--9/2471/901/1275/--1/7919/6
                
                
                    8181 Saarland University68080/--0/--0/--1/620/--1/430/--0/--1/1153/2241/2901/348/6
                
                
                    8289 Southern University of Science and Technology68260/--0/--0/--1/360/--2/580/--2/2404/2772/785/--1/1717/6
                
                
                    83134 University of Tehran68470/--0/--0/--1/580/--1/1250/--2/2891/1711/943/--3/5012/6
                
                
                    84142 Ural Federal University68720/--0/--0/--1/730/--1/980/--1/--1/1121/2511/2662/528/6
                
                
                    85118 Universiteit Utrecht69081/--0/--0/--1/590/--5/930/--9/--3/2152/1321/2491/2023/6
                
                
                    86109 Universidad Nacional de Rosario69960/--0/--0/--1/850/--2/990/--3/--3/2051/1562/2972/5414/6
                
                
                    8777 Petrozavodsk State University610040/--0/--0/--1/601/--3/1530/--4/2962/1212/2030/--1/3114/6
                
                
                    88128 University of Melbourne6104710/--0/--0/--3/1190/--4/1950/--2/2802/893/1652/--1/1927/6
                
                
                    8962 Moscow Institute of Physics and Technology53070/--0/--0/--1/691/--1/820/--4/--1/662/460/--1/2411/5
                
                
                    90130 University of Science and Technology - The University of Danang53780/--0/--0/--1/350/--2/630/--5/--1/1042/1161/--1/2013/5
                
                
                    9142 Indian Institute of Technology - Kanpur54150/--0/--0/--1/510/--1/790/--0/--1/1362/1070/--1/226/5
                
                
                    92114 Universidade Federal de Campina Grande54290/--0/--0/--1/340/--2/1020/--0/--1/591/1650/--2/297/5
                
                
                    934 Arab Academy for Science, Technology and Maritime Transport - Cairo54520/--0/--0/--1/720/--1/480/--0/--2/962/1420/--2/348/5
                
                
                    9470 National University of Science and Technology MISIS54590/--0/--0/--1/750/--1/230/--0/--2/1301/1470/--1/646/5
                
                
                    9527 Duke University54720/--0/--0/--1/490/--4/580/--0/--2/1131/1553/--1/1712/5
                
                
                    9629 Ecole Polytechnique de Tunisie54860/--0/--0/--1/350/--4/810/--0/--1/1161/1523/--2/2212/5
                
                
                    9740 Indian Institute of Technology - Delhi54861/--0/--0/--1/570/--1/830/--0/--1/993/1771/--1/309/5
                
                
                    9844 Indian Institute of Technology - Roorkee55283/--0/--0/--1/950/--3/680/--0/--2/1182/1470/--1/2012/5
                
                
                    99108 Universidad Nacional de Ingeniería - FC55440/--0/--0/--2/1020/--1/440/--0/--1/1202/2145/--1/2412/5
                
                
                    10049 Instituto Militar de Engenharia55540/--0/--0/--1/1030/--2/1050/--16/--3/1082/1460/--1/1225/5
                
                
                    1011 ADA University55570/--0/--0/--1/540/--2/870/--3/--1/792/2730/--1/2410/5
                
                
                    102111 Universidade de São Paulo55581/1180/--0/--1/340/--1/460/--0/--4/2595/--4/--2/2118/5
                
                
                    10339 Indian Institute of Technology - Bombay55990/--0/--0/--1/680/--1/1680/--0/--1/1982/1321/--1/137/5
                
                
                    1049 Assiut University56130/--0/--0/--1/530/--1/1020/--1/--2/1792/1940/--2/259/5
                
                
                    10554 Jordan University of Science and Technology56380/--0/--0/--1/572/--2/1030/--0/--1/1583/2370/--1/2310/5
                
                
                    10693 Syrian Virtual University56460/--0/--0/--1/700/--1/630/--0/--2/1513/2760/--1/268/5
                
                
                    10717 BRAC University56480/--0/--0/--1/540/--4/1400/--0/--1/2051/1520/--1/378/5
                
                
                    10874 Nizhny Novgorod State University56480/--0/--0/--1/980/--5/1050/--7/--1/592/2430/--1/4317/5
                
                
                    10943 Indian Institute of Technology - Kharagpur56600/--0/--0/--2/770/--1/1030/--0/--2/1472/2510/--1/228/5
                
                
                    11032 Universidad Nacional de La Plata56761/--1/--0/--1/951/--1/600/--1/--2/1214/2091/--2/9115/5
                
                
                    111107 Universidad Nacional de Colombia - Bogotá56850/--0/--0/--1/330/--1/830/--0/--1/1325/2820/--2/5510/5
                
                
                    112116 Universidade Federal de Pernambuco57070/--0/--0/--1/1370/--3/340/--0/--2/2302/1820/--1/449/5
                
                
                    11346 Innopolis University57100/--0/--0/--2/750/--1/890/--5/--3/1362/2170/--3/7316/5
                
                
                    11447 Institut National des Sciences Appliquées et de Technologie57110/--0/--0/--1/870/--1/1500/--7/--1/1852/2440/--1/2513/5
                
                
                    11588 Sohag University57250/--0/--0/--2/760/--5/1150/--0/--1/1354/2160/--1/2313/5
                
                
                    11621 Carnegie Mellon University in Qatar57290/--0/--0/--1/893/--2/850/--0/--2/1435/2680/--1/2414/5
                
                
                    11745 Indian Institute of Technology - Varanasi57380/--0/--0/--3/1390/--3/870/--0/--3/1931/1690/--1/3011/5
                
                
                    118104 Universidad de Chile57580/--0/--0/--1/490/--3/1111/--9/--2/1435/2920/--1/2322/5
                
                
                    119106 Universidad Mayor de San Simón57602/--0/--0/--1/780/--3/1720/--0/--1/1111/2942/--2/4512/5
                
                
                    12084 Shahjalal University of Science and Technology57730/--0/--0/--1/640/--4/1380/--0/--2/1155/2470/--1/4913/5
                
                
                    12179 Purdue University57840/--0/--0/--2/690/--4/2240/--6/--1/736/1870/--2/3121/5
                
                
                    12224 Damascus University57940/--0/--0/--1/760/--1/1080/--1/--3/1532/2010/--5/11613/5
                
                
                    1232 Ain Shams University - Faculty of Computer and Information Sciences58090/--0/--0/--1/830/--3/1190/--0/--1/1863/2710/--1/709/5
                
                
                    12450 Instituto Tecnológico de Costa Rica campus Alajuela58320/--0/--0/--1/1410/--2/740/--1/--1/1656/2810/--2/3113/5
                
                
                    12567 National Institute of Technology Tiruchirappalli58550/--0/--0/--1/840/--1/1290/--0/--1/1804/2980/--3/6410/5
                
                
                    1263 Arab Academy for Science, Technology and Maritime Transport - Alameen58980/--0/--0/--1/840/--4/1540/--0/--2/1974/2980/--1/2512/5
                
                
                    12711 Baku Higher Oil School59440/--0/--0/--1/490/--3/1150/--0/--6/2944/2630/--1/2315/5
                
                
                    128112 Universidade Estadual de Campinas510380/--0/--0/--2/530/--5/1880/--0/--3/2873/2910/--2/1915/5
                
                
                    1295 Arab Academy for Science, Technology and Maritime Transport - Smart Village43830/--0/--0/--1/520/--8/--0/--0/--1/1311/1740/--1/2612/4
                
                
                    13019 Cairo University - Faculty of Computers and Artificial Intelligence44530/--0/--0/--1/230/--4/2140/--0/--1/1162/--0/--1/409/4
                
                
                    131102 Universidad Autónoma de Yucatán44740/--0/--0/--1/1110/--3/1360/--0/--1/742/--0/--4/5311/4
                
                
                    132105 Universidad de La Habana45100/--0/--0/--2/1080/--1/990/--0/--4/1930/--0/--1/308/4
                
                
                    13337 Homs University45820/--0/--0/--3/990/--4/2020/--0/--1/1374/--0/--1/4413/4
                
                
                    134113 Universidade Federal de Alagoas47050/--0/--0/--1/910/--7/1860/--0/--4/2123/--0/--1/3616/4
                
                
                    13575 NU-FAST Karachi33030/--0/--0/--2/1010/--2/1280/--0/--0/--1/--0/--1/346/3
                
                
                    136119 University of Balamand33420/--0/--0/--1/1530/--3/1110/--0/--0/--3/--0/--1/388/3
                
                
                    13797 The University of Jordan38290/--0/--0/--8/2930/--7/--0/--0/--3/2671/--0/--3/4922/3
                
                
                    138110 Universidad Panamericana Campus Bonaterra21750/--0/--0/--1/1120/--2/--0/--0/--2/--0/--0/--1/636/2
                
                
                    13956 Kardan University000/--0/--0/--1/--0/--0/--0/--0/--0/--0/--0/--4/--5/0
                
                
                    Submitted/1st Yes/Total Yes130/73/4572/53/84/--/0170/14/13874/113/20273/17/13531/270/2278/71/66211/23/135287/35/128154/69/72193/6/1381877/887
                
            
        PC^2 Homepage
                
                					CSS by Tomas Cerny and Ray Holder
				
				Created by CSUS PC^2 version 
				9.11build 20250826 build 7926
            
            				Last updated
				Thu Sep 04 15:49:37 AZT 2025
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Classic 8×8-pixel B&W Mac patterns]]></title>
            <link>https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131538</guid>
            <description><![CDATA[TL;DR: I made a website for the original classic Mac patterns I was working on something and thought it would be fun to use one of the classic Mac black-and-white patterns in the project. I'm talking about the original 8×8-pixel ones that were in the...]]></description>
            <content:encoded><![CDATA[
        
TL;DR: I made a website for the original classic Mac patterns
I was working on something and thought it would be fun to use one of the
classic Mac black-and-white patterns in the project. I'm talking about the
original 8×8-pixel ones that were in the original Control Panel for setting the
desktop background and in MacPaint as fill patterns.


Screenshots via to Marcin's awesome interactive
history
I figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere
on the web. And perhaps there are, but after poking around a bit, I ran out of
energy for that, but by then had a head of steam for extracting the patterns en
masse from the original source, somehow. Then I could produce whatever format I
needed for them.
There are 38 patterns, introduced in the original System 1.0 in the 1984 debut
of the Macintosh. They were unchanged in later versions, so I decided to get
them from a System 6 disk, since that's a little easier with access to utility
programs.
Preparation

Download Mini vMac.
Acquire "old world" Mac ROMs.
Download a System 6 startup disk image.
Download ExportFl disk image.
Download sitPack disk image.
Install "The Unarchiver" (brew install --cask the-unarchiver)
Install the Xcode command-line tools.

Extraction process
Start System 6 (drag the ROM onto the Mini vMac icon, then drag the System 6
disk onto the window when you see the flashing floppy disk). Mount the ExportFl
and sitPack disks by dragging their files and dropping on the classic Mac
desktop.
In emulation
Double-click sitPack to launch the program. Command-O to open, then navigate to
the startup disk by clicking "Drive". Scroll to find "System Folder" and
double-click on it. Scroll to the bottom, select "System" and click "Open". Save
the output file as "System.sit" in the top-level of the startup disk. Quit
sitPack back to the Finder.
Start the ExportFl program. Command-O or pick "Open" from the "File" menu. Find
the "System.sit" created in the last step and click "Open". A regular file save
dialog will appear on the modern Mac, pick a location and save the file.
On the modern Mac
Drag the "System.sit" file onto The Unarchiver, or open the file from within it.
This will produce a file called "System" (with no extension).
Run DeRez (part of the Xcode developer command-line tools) on the System file.
I first added /Library/Developer/CommandLineTools/usr/bin to my $PATH, then
ran:
$ DeRez -only PAT\# System > patterns.r


This produces a text representation of the PAT# resource in the System file.
It's a series of bytes that comprise 38 8×8 patterns meant for QuickDraw
commands. There's a leading big-endian unsigned 16-bit number (0026) to indicate the number of 8-byte patterns to follow.
data 'PAT#' (0, purgeable) {
	$"0026 FFFF FFFF FFFF FFFF DDFF 77FF DDFF"
	$"77FF DD77 DD77 DD77 DD77 AA55 AA55 AA55"
	$"AA55 55FF 55FF 55FF 55FF AAAA AAAA AAAA"
	$"AAAA EEDD BB77 EEDD BB77 8888 8888 8888"
	$"8888 B130 031B D8C0 0C8D 8010 0220 0108"
	$"4004 FF88 8888 FF88 8888 FF80 8080 FF08"
	$"0808 8000 0000 0000 0000 8040 2000 0204"
	$"0800 8244 3944 8201 0101 F874 2247 8F17"
	$"2271 55A0 4040 550A 0404 2050 8888 8888"
	$"0502 BF00 BFBF B0B0 B0B0 0000 0000 0000"
	$"0000 8000 0800 8000 0800 8800 2200 8800"
	$"2200 8822 8822 8822 8822 AA00 AA00 AA00"
	$"AA00 FF00 FF00 FF00 FF00 1122 4488 1122"
	$"4488 FF00 0000 FF00 0000 0102 0408 1020"
	$"4080 AA00 8000 8800 8000 FF80 8080 8080"
	$"8080 081C 22C1 8001 0204 8814 2241 8800"
	$"AA00 40A0 0000 040A 0000 0384 4830 0C02"
	$"0101 8080 413E 0808 14E3 1020 54AA FF02"
	$"0408 7789 8F8F 7798 F8F8 0008 142A 552A"
	$"1408"
};

It would have been simple enough to
parse this text, but I had Claude quickly make a Python
program
to do so and output them in .pbm format, which is part of the Netpbm image
format class. This is a simple image format that is text-based, a '1' or a
'0' indicating a black or white pixel in a row and column.
For example, this subway tile pattern  is represented like
this in .pbm:
P1
8 8
1 1 1 1 1 1 1 1
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0

From here, I can generate image files for the patterns in any format and
resolution I want, using ImageMagick or similar. It's important when scaling the
patterns to use -filter point, so that ImageMagick doesn't try to interpolate
the pixels it needs to fill in, which would lead to blurry results.

Why do all this?
It's nostalgic, I have a fondness for these old patterns and the original B&W
Mac aesthetic, it reminds me of playing games like Dark Castle and Glider,
messing around with HyperCard, and using Tex-Edit and hoarding early shareware
programs.
The whole point of the above is to get a copy of the System file out with the
resource fork intact, that's
where the desktop patterns live.
According to old classic Mac
manuals,
the patterns were QuickDraw bit-pattern resources, a simple bitmap of 8 bits
per row packed into 8 bytes (columns). It was fast for QuickDraw to copy them
over an area of the screen. For example the following pattern was used for the
default gray desktop pattern on black-and-white Mac screens.

I could have extracted all 38 patterns other ways: I could have screenshotted
each one, I could have looked at each one and hand-written .pbm files, both of
which would have been tedious and error-prone.
Ultimately, I wanted to extract the exact original data from the source (or
close enough copy thereof) and have the patterns in a format I considered
archival for this limited purpose (.pbm files are trivial to parse and
manipulate).
Head over to my pattern site to get the patterns for yourself.
(Credit for replica Geneva 9pt and Chicago 12pt fonts)


        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Action was the best 8-bit programming language]]></title>
            <link>https://www.goto10retro.com/p/action-was-the-best-8-bit-programming</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45131243</guid>
            <description><![CDATA[There were many programming languages available for 8-bit computers, the most common being BASIC and Assembly Language, but Action! beats them both!]]></description>
            <content:encoded><![CDATA[There were many programming languages available for 8-bit computers, the most common being BASIC and Assembly Language, but there were also other lesser-used languages such as Logo, Forth, and Pilot. The languages that would go on to dominate 16-bit computing, C and Pascal, were also available but were usually severely limited. An 8-bit computer generally did not have enough horsepower to run those more complex language compilers1.By 1983 Optimized Systems Software (OSS) was renown in the Atari world for its great updated versions of DOS (DOS XL), BASIC (BASIC XL/XE) and assembler (MAC/65), so it was no surprise that they were the ones to introduce a new language, Action!, into the Atari market.Created by Clinton Parker, Action! was an all-new compiled language that was designed and optimized for the 8-bit 6502 CPU. It was a 16K cartridge2 and had everything you need integrated into one package: the monitor, compiler, text editor and debugger3. In some ways, Action! was the first IDE (integrated development environment) for an 8-bit computer.Back in the 80s I never used Action! and instead mostly used BASIC and OSS BASIC XE for my programming. I did like reading Action! program listings in magazines, though. But I now have the Action! cartridge and just recently acquired an Action! user manual, so I felt it was time to take a closer look at this amazing software development tool.The version of Action! that I now have is the classic orange cartridge, paired with a small 3-ring yellow binder containing the documentation. Action! was also available in the yellow label cartridge and its manual was also in a larger binder and then later, perfect bound (like the BASIC XL and BASIC XE manuals I have). Having the manual in a binder would have certainly been more useful in the 80s when you had to refer to it frequently.Action! retailed for $99 in 1983 (about $320 in 2025) and was only available for the Atari 8-bit computers. Early advertisements indicated there would be forthcoming versions for the Apple II and Commodore 64, but those never materialized.The manual is just under 220 pages and is concise, but reasonable well-written. There is not a ton of sample code and it doesn’t try to teach too many concepts. To get the most of it, you really already need to know how to program.I had been looking for an actual Action! manual for years, but the ones I’d seen on eBay had always been prohibitively expensive. Luckily I found one last month for just $30 and snagged it.You don’t need a physical manual, of course. An updated manual is available online in several places, and here’s the PDF.The editor really was a wonder for its time. It is a full-screen text editor that can scroll to the right as the line of text you type becomes longer than the 40 characters of an Atari screen. That was an unusual feature for the time, but was necessary because it allowed the indentation, encouraged by Action!’s structured programming style, to remain easy to read.Not much to see here, but it’s an empty editor screen.The editor can copy and paste text, another somewhat new feature for Atari text editors in 1983, has the ability to tag lines to jump to them rapidly and it also has a split screen mode that let you show two files (or two parts of the same file) on the screen at once. At first this might seem silly considering the small size of the screen, but this was revolutionary for the time. Normally to look at another file, you’d have to open it, losing the file you were working on, and then reload the original file. It was tedious and was a reason why you would print your programs back then.Even looking at different parts of a file could be a pain because you’d just be scrolling all over the place, which was not always fast or easy in many text editors. This was even worse with something like BASIC, which required you to LIST line ranges to see parts of your program.To exit the editor, you press Control+Shift+M which takes you to the monitor.Today this would be called the shell, but it is essentially the command line interface for the entire system. From the monitor, you can switch to the editor, compile, trace code, look at memory and more.Action! is a structured, procedural programming language. It is similar to both C and Pascal, although not quite as advanced as either of them.It has the usual commands for looping, if-then-else, but it does not have anything like a switch or Case statement. There are also only three data types: BYTE, CARD and INT. Strings were essentially just BYTE arrays.I found it endearing that to end an IF block you used FI (IF spelled backwards) and to end a DO block you used OD. That is some interesting symmetry although I’m not really sure it helps readability.An Action! “Hello World” program would be this:PROC hello()
; This is a comment.
  DO
    PrintE("Goto 10")
  OD
RETURNThe Action! language may not have been as advanced as C or Pascal, but because it was designed with the 6502 CPU in mind, compiling the language was astonishingly fast.The original Atari Pascal system from APX needed multiple disk drives and could take several minutes to compile a small program. The only C package available in 1983 (Deep Blue C) was at least as limited as Action!, but also not an integrated package and compiled slowly. Draper Pascal only compiled to pseudo-code.Action! compiled your program to machine code in memory and in seconds. Typing C (to compile) and then R (to run) was hardly slower than just typing RUN in BASIC.It really is stupidly fast. Here’s the output from the above program:If there is a compile error, it is shown on the screen and will be highlighted when you switch back to the editor by typing “e”.Action! was not perfect and it had several limitations. In my opinion, the two biggest limitations were that that Action! cartridge was required to run Action! programs (because they depended on the library that was included the cartridge ROM) and that there was no floating point data type.Both of these did get solved, to some extent, with the purchase of an additional add-ons: Action! RunTime and Action! Toolkit.The RunTime package provided the ability to create stand-alone Action! programs that you could distribute to others to run without the cartridge.The RunTime included the library as source files that you could include at the beginning of your own programs so that everything that was needed to run would get compiled into a single executable program. From what I can tell, Action! had no concept of linking which is how something like C would have handled this.I don’t have an official Action! RunTime disk, but the image is readily available online.The ToolKit is essentially an enhanced Library with additional functions and features. Two notable things it adds are player/missile graphics support and some support for floating-point numbers via several “Real” functions.Unfortunately this floating point support is somewhat limited and it doesn’t look all that useful to me. For example, I’ve used the Archimedes Spiral program in some articles here on Goto 10 to demonstrate drawing a fun graphic on the screen. It is interesting to see how long it can take to do the drawing on an 8-bit computer. I’d love to port it to Action!, and I was hopeful I’d be able to do so with the Action! ToolKit. Alas, even though it does add some commands to do some floating-point math, it does not add any trigonometry functions. The lack of Sin and Cos make it impractical to port Archimedes Spiral4.I don’t have an official Action! ToolKit disk, but the image is readily available online.It seems that Action! was mostly use by hobbyists, public domain and magazine software. The only two known commercial product made with Action! were the HomePak5 productivity package by Russ Wetmore and the Games Computers Play online service.For the above screen shots, I was using Action! with my 130XE.I plan to dig into actually using Action! itself more in the coming months. It really looks like a fun language. Unfortunately, since Action! is a cartridge, I can’t use it directly with my Side3 cart. There are disk-based versions of Action! available at AtariWiki, so I may have to switch to one of those, or see if I can get one of the SuperCart images to work with Side3. Otherwise, I may try it old-school with SpartaDOS, my trusty 1050 disk drive and a RAM disk.AtariWiki has a create page with lots of links to Action!-related materials.Action! Archive is a great reference for Action! programming.If you want to learn more about how to program in Action!, be sure to check out David Arlington’s YouTube channel, which has a 25-part series on Action! programming.1Kyan Pascal was released in 1986 and worked pretty well, but really wanted a couple disk drives. LightSpeed C was also a decent version of C that debuted later in the 80s.2Actually an OSS SuperCartridge, which had 16K of ROM but only used 8K of address space in the computer.3Calling it a debugger might be a bit of stretch compared to modern tools.4Sure, I could probably implement my own version of those, but I don’t really want to.5HomePak was an integrated productivity package with a highly regarded terminal program, a slick word processor with not much free RAM for text (perhaps 5 pages) and an usual database. HomePak warrants its own article.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LLM Visualization]]></title>
            <link>https://bbycroft.net/llm</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45130260</guid>
            <description><![CDATA[A 3D animated visualization of an LLM with a walkthrough.]]></description>
            <content:encoded><![CDATA[LLM VisualizationHome]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Saquon Barkley is playing for equity]]></title>
            <link>https://www.readtheprofile.com/p/saquon-barkley-investment-portfolio</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129523</guid>
            <description><![CDATA[The NFL star running back is building a portfolio of startups and redefining what it means to be a modern athlete.]]></description>
            <content:encoded><![CDATA[Saquon Barkley calls me, but he’s distracted. In the background, two little voices shout “Bye, friends!” as Barkley wrangles his kids, Jada, 7, and Saquon Jr., 3, into the car. He apologizes, then explains they’re headed to an Old Spice photo shoot tied to his latest endorsement — a Saquon-branded shampoo and conditioner called “Saquon Soar.”It’s a cinematic image: one of the NFL’s biggest stars juggling dad duty and the demands of a sponsorship. But Barkley isn’t content to just cash checks and smile for the camera. He has always treated these deals as building blocks for the empire he’s determined to build before football ends. And he knows it could all end suddenly.From the moment he entered the NFL in 2018, Barkley carried a kind of financial paranoia that most athletes don’t confront until retirement. He parked his entire $31.2 million rookie contract into long-term investments like the S&P 500, vowing to live only off endorsements. In 2021, he went further, backing payments app Strike as his first startup investment and pledging to take all marketing income in Bitcoin through the platform. At the time, Bitcoin’s price was approximately $32,000. Today, it’s hovering around $111,000, turning a $10 million income stream into a $35 million asset.That move rattled more than a few people. “Good old American dollars should be the standard,” superagent Leigh Steinberg warned, cautioning athletes against chasing crypto volatility.Barkley heard the critics, but he doubled down. Sitting on cash, he argued, isn’t enough — not when most athletes earn the bulk of their money in their 20s and face inflation, poor financial literacy, and limited access to tools. “A sad yet common reality is many enter bankruptcy later on,” he wrote on X. “We can do better.”His drive isn’t just financial. Barkley has the career most players dream about, but those closest to him say that his humility is born out of a gnawing sense that it could all slip away.For Barkley, equity is a safeguard — a way to wrest back control from the injuries, draft picks, or franchise decisions that can derail a career overnight. Football is fragile, while investing gives him a different relationship to risk. It may be volatile, but it’s a risk he can shape. Guided by business manager Ken Katz, he has avoided the typical athlete playbook of podcasts and clothing lines, instead building a portfolio that looks more like an elite venture fund than a celebrity brand.After reading Peter Thiel’s Zero to One, Barkley was most inspired by the idea of getting ahead by betting on businesses that create something so original that competition becomes irrelevant.Barkley at the 2025 Met Gala (Photo Credit: Sharif Fennell Jr.)The Profile has exclusively learned that Barkley has invested a portion of his earnings so far — a mix of his rookie contract and endorsement income — across more than 10 private startups, typically writing checks between $250,000 and $500,000.The high-growth startups in his portfolio include Anthropic (currently valued at $183 billion), Anduril ($30.5 billion), Ramp ($22.5 billion), Cognition ($9.8 billion), Neuralink ($9 billion), Strike (~$1 billion), and Polymarket (~$1 billion). He’s also a limited partner in funds including Founders Fund, Thrive Capital, Silver Point Capital, and Multicoin Capital.Some of those bets have already appreciated dramatically: Strike, for example, has delivered a 10x return in value since his investment. Barkley has also allocated a portion of his wealth into steadier assets like index funds and real estate, hedging the volatility that comes with venture and crypto.Unlike peers LeBron James and Serena Williams, who command headlines with splashy business ventures, Barkley’s approach is leaner and more surgical. He is making selective bets on technology companies that he believes are creating lasting value for their users.The question is whether his bets will hold. Venture investing is inherently risky — even late-stage startups with multibillion-dollar valuations can falter or see their valuations slashed when markets turn. To date, none of Barkley’s investments have flamed out or depreciated, largely because he prefers to come in at later stages of a company’s growth. In the end, Barkley must confront the question: Is he building something that lasts, or simply trading one kind of risk for another?For more longform profiles of extraordinary people, make sure to sign up for The Profile here:A few days before Thanksgiving last year, Ramp CEO Eric Glyman glanced at his phone and did a double take. His colleague Sam Buck had texted that Saquon Barkley wanted to invest in the expense management startup. Glyman was stunned. Most athletes ask for cash in exchange for an endorsement. Barkley wanted equity.“The best player in the NFL wants to work with us,” Glyman remembers thinking. “Like, what? What’s happening?”Barkley, who came in when Ramp was valued at $7 billion, didn’t just write a check. Convinced by Ramp’s mission to help businesses cut costs and perform more efficiently, he tied his own fortunes to the company’s — and then went to work to make sure it paid off.During the 2025 Super Bowl, Barkley starred in a Ramp commercial: a 15-second spot of him, in full pads, drowning in paperwork before Ramp’s automation saves the day.The ad aired as the Eagles defeated the Kansas City Chiefs, and Barkley set a new single-season rushing yards record for regular season and playoffs combined. For Ramp, it became their biggest traffic day ever. For Barkley, it was one of the rare times an endorsement literally boosted his own net worth.That was by design. Barkley’s process usually begins with his business manager, Ken Katz, reaching out to a company. Their strategy is to target technology companies and source deals through trusted word-of-mouth referrals in Katz’s investor network. The founder then gets invited to dinner with Barkley, where the running back plays interrogator. “It’s about asking them what they stand for, what their mission is, why they think they’ll be successful,” Barkley tells me. “They have to be confident, but arrogance is a turn-off.” If he’s interested, he often gives a verbal commitment on the spot.Sam Buck, Ramp’s head of financial institutions, insists Barkley was never treated like a celebrity mascot. “It was like, ‘Hey, you’re on the cap table now,’” he says. “We’re going to hold you accountable — just like we do with Founders Fund or General Catalyst — to help us grow.”And Barkley delivered. He FaceTimed Ramp partners to help close deals. He joined customer meet-and-greets. He talked about the company any chance he got.Fresh off the Super Bowl, he appeared on the TODAY show, still buzzing from the win. He fielded questions about being a champion, about his family, about life off the field. Then TODAY co-host Savannah Guthrie pivoted. “You have your own Super Bowl commercial, by the way, for Ramp, where you’re an investor,” she said. “So tell me how all this came about.”Barkley didn’t miss a beat. He turned the moment into a plug for his investment: “I fell in love with the team of Ramp … You get a lot of athletes who get involved with brands, and you get a certain dollar to show up and do things. But … as this company grows, I get to grow with it — it takes it to a whole new level for me.”The strategy is working. Barkley invested $500,000 into Ramp at a $7 billion valuation. The company’s valuation has more than tripled since then, which means his stake is now worth roughly $1.5 million. Earlier this year, Ramp announced $700 million in annualized revenue. “We’re growing faster this year than last year,” Glyman says, “and I don’t think it’s unrelated that Saquon has been involved.”But Barkley’s commitment also shows up in less public settings. In May, he flew to a Founders Fund symposium in Montana, an event filled with billionaires, CEOs, and top investors. He spent the day listening and networking, before catching a red-eye back to Philadelphia for a morning speaking obligation. Within hours, he was on a return flight to Montana. After another full day at the symposium, he boarded a midnight flight to make it to his daughter’s soccer game the next morning.Longtime venture investor Brian Singerman noticed his dedication to building meaningful relationships. “That stuff is impossible to fake,” he says.Even so, Barkley isn’t blind to the trade-offs. For someone intent on building generational wealth, venture capital represents both the biggest swing and the biggest vulnerability.“Honestly, putting your money in the S&P is going to beat 90% of venture capital,” says Singerman, who is one of Barkley’s advisers. “But that last 10% is going to crush the S&P.”This version of Barkley — the investor, the owner — didn’t come naturally. He didn’t grow up with stock tips or startup dinners. His financial philosophy was shaped by something starker: watching what happens when the money runs out.Barkley with his father, Alibay (Photo Credit: Sharif Fennell Jr.)When Barkley tells me about growing up in the Bronx, he chooses his words carefully. He had a loving family, he says, and doesn’t remember feeling the financial instability, at least not consciously.But to understand his obsession with money, risk, and generational wealth, you have to start with his parents. Tonya and Alibay were both born in the Bronx and raised five children together.In 2001, when Saquon was four, the family left for Pennsylvania. Over the years, Tonya developed a motto she repeated often: “All things are possible.” She believed it because she had lived it, navigating crises that could have broken the family but didn’t.One of those crises came when Barkley was in elementary school. The family was evicted and spent eight months without a home. The shelter system wouldn’t take fathers, so they were split up. Barkley and his sister went to live with a family friend while his parents scrambled to rebuild.“Financially, we obviously struggled, but my mom and dad never made us feel that,” he says. “It wasn’t like we were on the streets. We had family and friends who took us in, but [the experience] did give me the perspective I have on life right now.”His father carries his own formative scar. A gifted boxer, Alibay had made it to the semifinals of the 1992 New York State Golden Gloves before his shoulder gave out. He couldn’t afford surgery. At 21, he quit — not by choice but by circumstance.“I never had the money to get it fixed,” he says. The missed chance haunted him, and it became the lesson he drilled into his children: never quit on your passion. “If you quit this, it will be easy to quit jobs, quit relationships, quit on your kids,” he says. “That’s probably why Saquon is so adamant when he gets hurt about still being in there.”That “never quit” mantra became Barkley’s compass. At 13, frustrated in school and doubting football, he thought about walking away. His father told him the boxing story again. It stuck.From then on, Barkley zeroed in. He would play football, and he would become great.“Growing up, he had flashes of being great, but there was always someone in front of him,” says childhood friend Nick Shafnisky. “He was never going to let someone say he wasn’t the best.”At Penn State, Barkley fulfilled that prophecy, boasting 3,843 rushing yards and 43 touchdowns in three seasons. Head coach James Franklin drilled players with one message: “Use football to build generational wealth. Don’t let football use you.”Barkley took it to heart. One night, Franklin overheard his wife on a call talking about real estate. He showered, brushed his teeth, and got into bed, and she was still on the phone. When she finally hung up, he asked who it was. “It’s Saquon,” she told him.He was working on his first real estate investment and wanted to understand how to structure the deal and spot potential red flags. “I think curiosity is a really important trait in successful people, and Saquon is very curious,” Franklin tells me.His questions about money and wealth went from theoretical to real when the New York Giants drafted Barkley second overall in the 2018 NFL Draft and handed him a $31 million rookie contract. He delivered immediately: 1,300 rushing yards, 91 receptions, Rookie of the Year, Pro Bowl. The dream was intact — until it wasn’t.In Week 2 of the 2020 season, against the Chicago Bears, Barkley was tackled awkwardly near the sideline. He clutched his knee and was carted off the field. The diagnosis? A torn ACL, partially torn meniscus, and an MCL strain. It was a devastating cocktail of injuries. “I definitely had some dark moments during that time,” Barkley says. “I had those moments of, ‘Man, why me?’”Whether Barkley realized it or not, it was his father’s nightmare revisited: one injury, one twist, and everything could vanish. At Katz’s suggestion, Barkley watched Peter Thiel’s “Competition Is for Losers” lecture at Stanford and read his book Zero to One, both of which left an impression. Thiel’s argument — that the biggest returns come from a few breakthrough bets — resonated. Football suddenly felt fragile, and investing began to feel urgent.His return to the NFL was shaky. In 2021, after months of rehab, he sprained his ankle early in the season and stumbled to a career-low 593 rushing yards. Friends say he wasn’t himself. His mom reminded him: “No matter what, you can always get back up.” He did. By 2022, Barkley was back at 1,300 yards and another Pro Bowl.But it was no longer possible to ignore the reality that his body and career were vulnerable. The Giants reminded him of that in 2023 when they slapped him with the “franchise tag.” It was a one-year deal — not the long-term contract he had earned. To Barkley, it was a gut punch. Even after proving himself, he was treated as expendable.“It was a very contentious situation,” says former NFL defensive great J.J. Watt. “[Barkley] was the star of [the Giants], and they basically kicked him out the door.”Publicly, Barkley stayed composed. Privately, the rejection lit a fire. Running backs have the shortest shelf life in the league, but his bigger worry was what comes after. Approximately 78% of NFL players face financial distress within two years of retiring, and he was determined not to be one of them.By then, he had already built a portfolio and a mindset beyond football. In 2024, Barkley left the Giants for a three-year, $37.75 million deal with the Philadelphia Eagles.It was a declaration that he knew his value, and he wasn’t going to sign a deal that didn’t reflect it.Barkley as a Philadelphia Eagle (Photo Credit: Sharif Fennell Jr.)In February 2025, confetti rained down on the Philadelphia Eagles after they defeated the Kansas City Chiefs in the Super Bowl. Barkley had delivered — a standout game, a title, and the biggest win of his career on his 28th birthday. “I don’t think anything is going to top this,” he said on the field moments after the final whistle.That night, Barkley threw an afterparty at St. Pizza in New Orleans. The guest list was surreal, including Leonardo DiCaprio, Pete Davidson, Zac Efron, and Chance the Rapper. But scattered among the celebrities was a quieter, more unexpected group: the founders of Barkley’s portfolio companies.Jack Mallers, CEO of Strike, was one of them. “I felt bad for him,” Mallers says. “Even though he had just won the Super Bowl and it was his birthday, it felt like he was still working. He took a photo with everyone. Answered every question. Asked the founders how business was going. It was unbelievable.”Mallers first met Barkley through an email from business manager Ken Katz with the subject line: Saquon Barkley loves Bitcoin. “It felt like a scam,” Mallers laughs. But after dinner with Barkley, he realized the interest was real and gave him a spot in Strike’s $100 million round. Barkley invested $100,000, and today the company is reportedly valued at over $1 billion, marking a 10x return on his stake.That deal forged a deep relationship. Since they met, Barkley and Mallers have spent hours debating Bitcoin as a store of value. In 2021, Barkley went all in on the digital currency, announcing he would convert 100% of his endorsement income — Nike, Pepsi, Visa, Dunkin’ — into Bitcoin using Strike.The move was yet another example of Barkley using his platform to amplify a company in which he has equity. “The average NFL career is 3 years and inflation is real,” he tweeted. “Saving and preserving money over time is hard… Bitcoin is a proven, safe, global, and open system that allows anyone to save money.”Crypto executives hailed his entrance into the world of Bitcoin. “He’s setting a blueprint for athletes to take charge of their financial future,” says Coinbase president Emilie Choi. But traditional advisers recoiled. “The value of Bitcoin is much more volatile than the value of the U.S. dollar,” one financial planner warned in an op-ed. “How would you feel if your hypothetical Bitcoin paycheck was worth 10% less the day after it was deposited into your account?”For a while, Barkley looked prescient as his holdings appreciated in value. Mallers even joked, “My goal was to help him earn more through Bitcoin and Strike than he would in the NFL. And I think he will.”But then came the crash. Famed crypto exchange FTX collapsed near the end of 2022, its founder was accused of fraud, and Bitcoin’s price plunged below $16,000 — more than 50% below the level at which Barkley had invested.As he scrolled through the taunts on social media, Barkley picked up the phone and called Katz, asking the question on everyone’s mind: “What is going on?”Katz told him to hold, and Barkley did. When I ask about embracing risk in that moment, he likens it to football: one week you’re celebrated, the next you’re vilified. Volatility, he says, is temporary.That discipline is rooted in his partnership with Katz. The two met when Barkley was still at Penn State and Katz was 24, hustling to break into sports management. A decade later, Katz is still his consigliere who pushes him to think like a contrarian.Katz has consistently advised Barkley to use his influence to win deals. “My thesis has always been: Use the fame to get equity in the companies of the best founders in the world,” Katz says.The result is an unusually hands-on approach. Barkley keeps his circle small: just Katz, his financial adviser, and the founders he backs.Compare that to NBA star Kevin Durant, whose investment firm 35V operates with more institutional rigor and a tilt toward safer asset classes like private equity and real estate. “You’re still getting ownership, but it’s just not as risky as venture,” says co-founder Rich Kleiman.While Durant’s portfolio spans more than 100 companies, Barkley approaches investing with the focus of a rogue founder. He doesn’t have a fund structure, and he doesn’t neatly follow a playbook. He makes direct bets on companies that excite him. The strategy is lean, contrarian, and far more exposed.That ethos resonates with Russell Okung, the first NFL player to take part of his salary in Bitcoin.His advice for Barkley? “You’re not an athlete-investor, you’re an investor who allocates your time playing sports. The old model extracted value from athletes. The new model lets athletes extract value from everything else.” He adds, “Why be talent when you can be management? The cap table is where real wealth lives. It's where upside multiplies.”It’s a mindset Barkley has adopted fully, though it comes with a healthy dose of risk. Where most of us see volatility as a warning sign, he recognizes it as something familiar.Growing up without a safety net taught him that uncertainty can be an opening. “I’m not going to just jump into a pool of sharks,” he says, “but I do have a little bit of: ‘I don’t give a you-know-what.’”Barkley celebrating with his daughter Jada (Photo Credit: Sharif Fennell Jr.)Two days before the NFL draft, Barkley became a father. His daughter, Jada, arrived before the cameras and before the multimillion-dollar contract.He remembers looking into her eyes and seeing the future flash before his. In that moment, he realized he wasn’t just building for himself anymore. He was building a world she could inherit.“I was 21 when she was born,” he says, as Jada chatters in the car beside him. “I have a different dynamic with my kids than my parents had with me because I want to teach them how to be more financially literate. I’m still trying my best to learn, but as I learn, they’ll learn too.”To Barkley, investing is about one thing: control. He is trying to manufacture permanence while playing a game defined by uncertainty. His ultimate driver, he says, is his family: his fiancée, Anna, and his kids. “If you had asked me this question two years ago, then my answer would’ve been, ‘I want to be the best football player,’ but it’s not about just that anymore.”That shift is born out of urgency. Football is the most clock-driven sport, and his position is the most precarious. Does he feel the same countdown in his own career?“It’s funny you ask me that,” Barkley says. “Because that’s something that was on my mind this morning. I was just thinking about how I can only play for so long, so I really gotta take advantage, keep investing, and create wealth for me and my family.”He knows the day is coming when his body will tell him the game is over. But when his children ask what he built, he doesn’t only want to point to highlight reels or rushing titles. He wants to point to ownership — proof that he wrestled fragility into permanence.That is the paradox of Saquon Barkley: a man willing to gamble with risk in his life so that his kids never have to feel the sharp edge of uncertainty in theirs.-Written by Polina Pompliano | Edited by Laura Entis / Photos by Sharif Fennell Jr. / Video by Matt MarlinskiRyan Serhant Won’t Stop Until He’s No. 1·May 7In the greenroom at the TODAY show, Ryan Serhant, dressed in a tailored sage suit and sporting his signature gray hair, flips through notes as producers buzz around him.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Artie (YC S23) Is Hiring Engineers, AES, and Senior PMM]]></title>
            <link>https://www.ycombinator.com/companies/artie/jobs</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129442</guid>
            <description><![CDATA[Jobs at Artie]]></description>
            <content:encoded><![CDATA[Software that streams data from databases to warehouses in real-timeJobs at ArtieSan Francisco, CA, US$110K - $130K0.10%3+ yearsSan Francisco, CA, US$145K - $185K0.20% - 0.40%3+ yearsSan Francisco, CA, US$150K - $215K0.50% - 1.00%3+ yearsSan Francisco, CA, US$150K - $215K0.50% - 1.00%3+ yearsWhy you should join ArtieWe are building Artie, a real-time data streaming solution focused on databases and data warehouses. Typical ETL solutions leverage batched processes or schedulers (DAGs, Airflow), which cannot achieve real time data syncs. We leverage change data capture (CDC) and stream processing to perform data transfers in a more efficient way, which enables sub-minute latency.Founded:2023Batch:S23Team Size:8Status:ActiveLocation:San FranciscoFounders]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Wal3: A Write-Ahead Log for Chroma, Built on Object Storage]]></title>
            <link>https://trychroma.com/engineering/wal3</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129369</guid>
        </item>
        <item>
            <title><![CDATA[A PM's Guide to AI Agent Architecture]]></title>
            <link>https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129237</guid>
            <description><![CDATA[A complete guide to agent architecture, orchestration patterns, trust strategies, and adoption plans for PMs building AI agents.]]></description>
            <content:encoded><![CDATA[Last week, I was talking to a PM who'd in the recent months shipped their AI agent. The metrics looked great: 89% accuracy, sub-second respond times, positive user feedback in surveys. But users were abandoning the agent after their first real problem, like a user with both a billing dispute and a locked account."Our agent could handle routine requests perfectly, but when faced with complex issues, users would try once, get frustrated, and immediately ask for a human."This pattern is observed across every product team that focuses on making their agents "smarter" when the real challenge is making architectural decisions that shape how users experience and begin to trust the agent. In this post, I'm going to walk you through the different layers of AI agent architecture. How your product decisions determine whether users trust your agent or abandon it. By the end of this, you'll understand why some agents feel "magical" while others feel "frustrating" and more importantly, how PMs should architect for the magical experience.We'll use a concrete customer support agent example throughout, so you can see exactly how each architectural choice plays out in practice. We’ll also see why the counterintuitive approach to trust (hint: it's not about being right more often) actually works better for user adoption.You're the PM building an agent that helps users with account issues - password resets, billing questions, plan changes. Seems straightforward, right?But when a user says "I can't access my account and my subscription seems wrong" what should happen?Scenario A: Your agent immediately starts checking systems. It looks up the account, identifies that the password was reset yesterday but the email never arrived, discovers a billing issue that downgraded the plan, explains exactly what happened, and offers to fix both issues with one click.Scenario B: Your agent asks clarifying questions. "When did you last successfully log in? What error message do you see? Can you tell me more about the subscription issue?" After gathering info, it says "Let me escalate you to a human who can check your account and billing."Same user request. Same underlying systems. Completely different products.Think of agent architecture like a stack where each layer represents a product decision you have to make.The Decision: How much should your agent remember, and for how long?This isn't just technical storage - it's about creating the illusion of understanding. Your agent's memory determines whether it feels like talking to a robot or a knowledgeable colleague.For our support agent: Do you store just the current conversation, or the customer's entire support history? Their product usage patterns? Previous complaints? Types of memory to consider:Session memory: Current conversation ("You mentioned billing issues earlier...")Customer memory: Past interactions across sessions ("Last month you had a similar issue with...")Behavioral memory: Usage patterns ("I notice you typically use our mobile app...")Contextual memory: Current account state, active subscriptions, recent activityThe more your agent remembers, the more it can anticipate needs rather than just react to questions. Each layer of memory makes responses more intelligent but increases complexity and cost.The Decision: Which systems should your agent connect to, and what level of access should it have?The deeper your agent connects to user workflows and existing systems, the harder it becomes for users to switch. This layer determines whether you're a tool or a platform.For our support agent: Should it integrate with just your Stripe’s billing system, or also your Salesforce CRM, ZenDesk ticketing system , user database, and audit logs? Each integration makes the agent more useful but also creates more potential failure points - think API rate limits, authentication challenges, and system downtime.Here's what's interesting - Most of us get stuck trying to integrate with everything at once. But the most successful agents started with just 2-3 key integrations and added more based on what users actually asked for.The Decision: Which specific capabilities should your agent have, and how deep should they go?Your skills layer is where you win or lose against competitors. It's not about having the most features - it's about having the right capabilities that create user dependency.For our support agent: Should it only read account information, or should it also modify billing, reset passwords, and change plan settings? Each additional skill increases user value but also increases complexity and risk.Implementation note: Tools like MCP (Model Context Protocol) are making it much easier to build and share skills across different agents, rather than rebuilding capabilities from scratch. The Decision: How do you measure success and communicate agent limitations to users?This layer determines whether users develop confidence in your agent or abandon it after the first mistake. It's not just about being accurate - it's about being trustworthy.For our support agent: Do you show confidence scores ("I'm 85% confident this will fix your issue")? Do you explain your reasoning ("I checked three systems and found...")? Do you always confirm before taking actions ("Should I reset your password now?")? Each choice affects how users perceive reliability.Trust strategies to consider:Confidence indicators: "I'm confident about your account status, but let me double-check the billing details"Reasoning transparency: "I found two failed login attempts and an expired payment method"Graceful boundaries: "This looks like a complex billing issue - let me connect you with our billing specialist who has access to more tools"Confirmation patterns: When to ask permission vs. when to act and explainThe counterintuitive insight: users trust agents more when they admit uncertainty than when they confidently make mistakes.Okay, so you understand the layers. Now comes the practical question that every PM asks: "How do I actually implement this? How does the agent talk to the skills? How do skills access data? How does evaluation happen while users are waiting?"Your orchestration choice determines everything about your development experience, your debugging process, and your ability to iterate quickly.Lets walk through the main approaches, and I'll be honest about when each one works and when it becomes a nightmare.Everything happens in one agent's context. For our support agent: When the user says "I can't access my account," one agent handles it all - checking account status, identifying billing issues, explaining what happened, offering solutions. Why this works: Simple to build, easy to debug, predictable costs. You know exactly what your agent can and can't do.Why it doesn't: Can get expensive with complex requests since you're loading full context every time. Hard to optimize specific parts.Most teams start here, and honestly, many never need to move beyond it. If you're debating between this and something more complex, start here.You have a router that figures out what the user needs, then hands off to specialized skills.For our support agent: Router realizes this is an account access issue and routes to the `LoginSkill`. If the LoginSkill discovers it's actually a billing problem, it hands off to `BillingSkill`.Real example flow:User: "I can't log in"Router → LoginSkillLoginSkill checks: Account exists ✓, Password correct ✗, Billing status... wait, subscription expiredLoginSkill → BillingSkill: "Handle expired subscription for user123"BillingSkill handles renewal processWhy this works: More efficient - you can use cheaper models for simple skills, expensive models for complex reasoning. Each skill can be optimized independently.Why it doesn't: Coordination between skills gets tricky fast. Who decides when to hand off? How do skills share context?Here's where MCP really helps - it standardizes how skills expose their capabilities, so your router knows what each skill can do without manually maintaining that mapping.You predefine step-by-step processes for common scenarios. Think LangGraph, CrewAI, AutoGen, N8N, etc.For our support agent: "Account access problem" triggers a workflow:Check account statusIf locked, check failed login attempts  If too many failures, check billing statusIf billing issue, route to payment recoveryIf not billing, route to password resetWhy this works: Everything is predictable and auditable. Perfect for compliance-heavy industries. Easy to optimize each step.Why it doesn't: When users have weird edge cases that don't fit your predefined workflows, you're stuck. Feels rigid to users.Multiple specialized agents work together using A2A (agent-to-agent) protocols. The vision: Your agent discovers that another company's agent can help with issues, automatically establishes a secure connection, and collaborates to solve the customer's problem. Think a booking.com agent interacting with an American Airlines agent! For our support agent: `AuthenticationAgent` handles login issues, `BillingAgent` handles payment problems, `CommunicationAgent` manages user interaction. They coordinate through standardized protocols to solve complex problems.Reality check: This sounds amazing but introduces complexity around security, billing, trust, and reliability that most companies aren't ready for. We're still figuring out the standards.This can produce amazing results for sophisticated scenarios, but debugging multi-agent conversations is genuinely hard. When something goes wrong, figuring out which agent made the mistake and why is like detective work.Here's the thing: start simple. Single-agent architecture handles way more use cases than you think. Add complexity only when you hit real limitations, not imaginary ones.But here's what's interesting - even with the perfect architecture, your agent can still fail if users don't trust it. That brings us to the most counterintuitive lesson about building agents.Here's something counterintuitive: Users don't trust agents that are right all the time. They trust agents that are honest about when they might be wrong.Think about it from the user's perspective. Your support agent confidently says "I've reset your password and updated your billing address." User thinks "great!" Then they try to log in and... it doesn't work. Now they don't just have a technical problem - they have a trust problem.Compare that to an agent that says "I think I found the issue with your account. I'm 80% confident this will fix it. I'm going to reset your password and update your billing address. If this doesn't work, I'll immediately escalate to a human who can dive deeper."Same technical capability. Completely different user experience.Building trusted agents requires focus on three things:Confidence calibration: When your agent says it's 60% confident, it should be right about 60% of the time. Not 90%, not 30%. Actual 60%.Reasoning transparency: Users want to see the agent's work. "I checked your account status (active), billing history (payment failed yesterday), and login attempts (locked after 3 failed attempts). The issue seems to be..."Graceful escalation: When your agent hits its limits, how does it hand off? A smooth transition to a human with full context is much better than "I can't help with that."A lot of times we obsess over making agents more accurate, when what users actually want was more transparency about the agent's limitations.In Part 2, I'll dive deeper into the autonomy decisions that keep most PMs up at night. How much independence should you give your agent? When should it ask for permission vs forgiveness? How do you balance automation with user control?We'll also walk through the governance concerns that actually matter in practice - not just theoretical security issues, but the real implementation challenges that can make or break your launch timeline.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Age Simulation Suit]]></title>
            <link>https://www.age-simulation-suit.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129190</guid>
            <description><![CDATA[The GERontologic Test suit GERT creates the experience old age. GERT is the age simulation suit for universities, seminaries, institutes and companies.]]></description>
            <content:encoded><![CDATA[



GERonTologic simulator GERT
The age simulation suit GERT offers the opportunity to experience the impairments of older persons even for younger people.

The age-related impairments are:
■  opacity of the eye lens
■  narrowing of the visual field
■  high-frequency hearing loss
■  head mobility restrictions
■  joint stiffness
■  loss of strength
■  reduced grip ability
■  reduced coordination skills
GERT for only  � 1390,‑ / � 1250,-
complete as pictured, plus shipping and VAT if applicable
New: now with 2 pairs of glasses instead of the model shown






Due to the significant increase in the time and effort required to process orders, in particular as a result of incomplete or incorrect information provided with orders, and the fact that we increasingly have to send reminders for invoices for smaller amounts, we can only accept orders with a value of at least 300 euros or pounds.

Customer reviews:
The quality is great and it works how it is supposed to. I�m happy with my purchase.
Great way to teach about elderly behavior. I�ve been using this suit for a while now and it�s very durable and easy to use. Thanks!!














For many years ourage simulation suitGERT has been byfar the most popularproduct worldwide.The EuropeanCompetence Centrefor Accessibility hascertified our agesimulation suit GERT.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How we built an interpreter for Swift]]></title>
            <link>https://www.bitrig.app/blog/swift-interpreter</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129160</guid>
            <description><![CDATA[Build apps for your phone, on your phone.]]></description>
            <content:encoded><![CDATA[Bitrig dynamically generates and runs Swift apps on your phone. Normally this would require compiling and signing with Xcode, and you can’t do that on an iPhone.
To make it possible to instantly run your app, we built a Swift interpreter. But it’s an unusual interpreter, since it interprets from Swift… to Swift. One of the top questions we’ve gotten is how it’s implemented, so we wanted to share how it works. To make this more accessible and interesting, we simplified some of the more esoteric details. But we hope you’ll come away with a high-level picture of how the interpreter works.
The Swift project helpfully provides a way to reuse all of the parsing logic from the compiler: SwiftSyntax. This made our job a lot easier. We can easily take some Swift code and get a parsed tree out of it, which we can use to evaluate and call into to get dynamic runtime values. Let’s dig deeper.
We can start with generating the simplest kind of runtime values. For any literals (strings, floating point numbers, integers, and booleans), we can create corresponding Swift instances (String, Double, Int, Bool) to represent them. Since we’re not compiling this, we don’t know ahead of time what all the types will be, so we need to type erase all instances. Let’s make an enum to represent these runtime interpreter values (since we'll have multiple kinds soon):
enum InterpreterValue {
    case nativeValue(Any)
}

Next, we'll expand our interpreter runtime values to be able to represent developer-defined types, too. Let’s say we have a struct with two fields: a string and an integer. We’ll store it as a type that has a dictionary mapping from the property name to the runtime value. When an initializer gets called, we simply need to map the arguments to the property names and populate the dictionary.
enum InterpreterValue {
    case nativeValue(Any)
    case customInstance(CustomInstance)
}
struct CustomInstance {
    var type: InterpreterType
    var values: [String: InterpreterValue]
}

But, what happens when we want to call an API that comes from a framework, like SwiftUI? For example, let’s say we have a call to Text("Hello World"). We don’t want to rewrite all of the APIs, the whole benefit of making a native app is being able to call into those implementations! Well, those APIs are available for us to call into since the interpreter is also written in Swift (naturally!). We just need to change from a dynamic call to a compiled one. We can do that by pre-compiling a call to the Text initializer that can take dynamic arguments. Something like this:
func evaluateTextInitializer(arguments: [Argument]) -> Text {
  Text(arguments.first?.value.stringValue ?? "")
}

But of course, we need more than just the Text initializer, so we'll generalize this to any initializer we might be called with:
func evaluateInitializer(type: String, arguments: [Argument]) -> Any? {
  if type == "Text" {
    return Text(arguments.first?.value.stringValue ?? "")
  } else if type == "Image" {
    return Image(arguments.first?.value.stringValue ?? "")
  ...
}

We can follow this same pattern for all other API types: function calls, properties, subscripts, etc. The difficult part is that there are a lot of APIs. It’s not practical to hand-write code to call into all of them, but fortunately there is a structured list of all of them: the .swiftinterface file for each framework. We can parse those files to get a list of all of the APIs we need and then generate the necessary code to call into them.
One interesting thing about taking this approach to its most extreme is that even very basic operations that you might expect any interpreter to implement, like basic numeric operations, can still call into their framework implementations. So this kind of interpreter doesn’t know how to calculate or evaluate anything itself, and is really more of a glorified foreign function interface, but from dynamic Swift to compiled Swift.
One last important challenge is how to make custom types conform to framework protocols. For example, how do we make a custom SwiftUI View? Well, at runtime, we need a concrete type that conforms to the desired protocol. To do this, we can make stub types that conform to the protocol, but instead of having any logic of their own, simply call out to the interpreter to implement any requirements. Let’s look at Shape, a simple example:
struct ShapeStub: Shape {
    var interpreter: Interpreter
    var instance: CustomInstance

    var layoutDirectionBehavior: LayoutDirectionBehavior {
        instance.instanceMemberProperty("layoutDirectionBehavior", interpreter: interpreter).layoutDirectionBehaviorValue
    }
    func path(in rect: CGRect) -> Path {
        let arguments = [Argument(label: "in", value: rect)]
        return instance.instanceFunction("path", arguments: arguments, interpreter: interpreter).pathValue
    }
}

Coming back to View, this works the same way, with a little extra complexity because of the associated type that we have to type erase:
struct ViewStub: View {
    var interpreter: Interpreter
    var instance: CustomInstance

    var body: AnyView {
        instance.instanceMemberProperty("body", interpreter: interpreter).viewValue
            .map { AnyView($0) }
  }
}

And now we can make views that have dynamic implementations!
That’s a broad survey of how the interpreter is implemented. If you want to try it out in practice, download Bitrig!
If there’s more you want to know about the interpreter, or Bitrig as a whole, let us know!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Stripe Launches L1 Blockchain: Tempo]]></title>
            <link>https://tempo.xyz</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129085</guid>
            <description><![CDATA[A neutral, permissionless blockchain optimized for payment processing, enabling efficient transactions with predictable low fees and stablecoin interoperability.]]></description>
            <content:encoded><![CDATA[Why create anew blockchain?Stablecoins enable instant, borderless, programmable transactions, but current blockchain infrastructure isn’t designed for them: existing systems are either fully general or trading-focused. Tempo is a blockchain designed and built for real-world payments.Optimized forreal-world flowsTempo was started by Stripe and Paradigm, with design input from Anthropic, Coupang, Deutsche Bank, DoorDash, Lead Bank, Mercury, Nubank, OpenAI, Revolut, Shopify, Standard Chartered, Visa, and more.If you’re a company with large, real-world economic flows and would like to help shape the future of Tempo, get in touch.Partner with usTransform how your business  moves money01 :: Purpose-built payments capabilitiesOptimize your financial flows with embedded payment features, including memo fields and batch transfers.02 :: Speed and reliabilityProcess over 100,000 transactions per second (TPS) with sub-second finality, enabling real-time payments at a global scale.03 :: Predictable low feesTransform your cost structure with near-zero transaction fees that are highly predictable and can be paid in any stablecoin.04 :: Built-in privacy measuresProtect your users by keeping important transaction details private while maintaining compliance standards.Performant and scalable for any payments use case01 :: RemittancesSend money across borders instantly, securely, and at a fraction of traditional costs.02 :: Global payoutsPay anyone, anywhere, in any currency—without banking delays or fees.03 :: Embedded financeBuild compliant, programmable payments—in any stablecoin—directly into your products.04 :: MicrotransactionsEnable sub-cent payments for digital goods and on-demand services.05 :: Agentic commerceFacilitate low-cost, instant payments for agents to autonomously execute transactions.06 :: Tokenized depositsMove customer funds onchain for instant settlement and efficient interbank transfers.Technicalfeatures01 :: Fee flexibilityPay transaction fees in any stablecoin.02 :: Dedicated payments laneTransfer funds cheaply and reliably in blockspace that’s isolated from other activity.03 :: Stablecoin interoperabilitySwap stablecoins, including custom-issued ones, natively with low fees.04 :: Batch transfersSend multiple transactions onchain at once with native account abstraction.05 :: Blocklists / allowlistsMeet compliance standards by setting user-level permissions for transactions.06 :: Memo fieldsSpeed up reconciliation with offchain transactions by adding context that’s compatible with ISO 20022 standards.Frequentlyasked questions01 :: How is Tempo different from other blockchains?Tempo is an EVM-compatible L1 blockchain, purpose-built for payments. It doesn’t displace other general-purpose blockchains; rather, it incorporates design choices that meet the needs of high-volume payment use cases. These include predictable low fees in a dedicated payments lane, stablecoin neutrality, a built-in stablecoin exchange, high throughput, low latency, private transactions, payment memos compatible with standards like ISO 20022, compliance hooks, and more.02 :: Who can build on Tempo?Tempo is a neutral, permissionless blockchain open for anyone to build on. We’re currently collaborating with global partners to test various use cases, including cross-border payouts, B2B payments, remittances, and ecommerce. Interested in working with Tempo? Request access to our private testnet here.03 :: When will Tempo launch?We’re providing select partners with priority access to our testnet now. Contact us here if you’re interested.04 :: Who will run validator nodes?A diverse group of independent entities, including some of Tempo’s design partners, will run validator nodes initially before we transition to a permissionless model.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Slashy (YC S25) – AI that connects to apps and does tasks]]></title>
            <link>https://news.ycombinator.com/item?id=45129031</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45129031</guid>
            <description><![CDATA[Hi HN! – We’re Pranjali, Dhruv and Harsha, building Slashy (https://www.slashy.ai). We’re building a general agent that connects to apps and can read data across them and perform actions via custom tools, semantic search, and personalized memory. Here’s a demo: https://www.youtube.com/watch?v=OeApHMHhccA.]]></description>
            <content:encoded><![CDATA[Launch HN: Slashy (YC S25) – AI that connects to apps and does tasks54 points by hgaddipa001 8 hours ago  | hide | past | favorite | 86 commentsHi HN! – We’re Pranjali, Dhruv and Harsha, building Slashy (https://www.slashy.ai). We’re building a general agent that connects to apps and can read data across them and perform actions via custom tools, semantic search, and personalized memory. Here’s a demo: https://www.youtube.com/watch?v=OeApHMHhccA.While working on a previous startup, we realized we were spending more time doing busywork in apps than actually building product. We lost hundreds of hours scraping LinkedIn profiles, updating spreadsheets, updating investor reports, and communicating across multiple Slack channels. Our breaking point happened after I checked my screen time and realized I spent 4 hours a day in Gmail. We decided that we could create more value solving this than by working on the original startup (a code generation agent similar to Lovable).Slashy is an AI agent that uses direct tool calls to services such as Gmail, Calendar, Notion, Sheets and more. We built all of our tools in-house since we found that most MCPs are low quality and add an unnecessary layer of abstraction. Through these tools, the agent is able to semantically search across your apps, get relevant information, and perform actions (e.g. send emails, create calendar events, etc). This solves the problem of context-switching and copy-pasting information from an app back and forth into ChatGPT.Slashy integrates to 15 different services so far (G-Suite, Slack, Notion, Dropbox, Airtable, Outlook, Phone, Linear, Hubspot, and more). We use a single agent architecture (as we found this reduces hallucinations), and use our own custom tools—doing so allows the model to have higher quality as we can design them to work in a general agent structure, for example we use markdown for Slack/Notion instead of their native text structure.So what makes Slashy different from the 100 other general agents?- It Actually Takes Action: Unlike ChatGPT or Claude that just give you information, Slashy researches companies, creates Google Docs with findings, adds contacts to your CRM, schedules follow-ups, and sends personalized emails – all in one workflow.- Cross-Tool Context: Most automation tools work in silos (one of the biggest problems with MCP). Slashy understands your data across platforms. It can read your previous Slack conversations about a prospect, check your calendar for availability, research their company online, and draft a personalized email. What powers this is our own semantic search functionality.- User Action Graphs: Our agent over time has memory not just of past conversations, but also forms user actions graphs to know what actions are expected based on previous user conversations.- No Technical Setup Required: While Zapier requires building complex flows and fails silently, Slashy works through natural language. Just describe what you want automated.- Custom UI: For our tool calls we design custom UI for each of them to make the UX more natural.Here are some examples of workflows people use us for:▪ "Every day look at my calendar and send me a notion doc with in-depth backgrounds on everyone I’m meeting"▪ "Find the emails of everyone who reacted to my latest LinkedIn post and send personalized outreach"▪ "Can you make me an investor pitch deck with market research, competitive analysis, and financial projections"▪ "Doing a full Nvidia Discounted Cash Flow (DCF) analysis"Slashy.ai is live with a free tier (100 daily credits) along with 500 credits for any new account. You can immediately try out workflows like the ones above and we have a special code for HN (HACKERNEWS at checkout).Hope you all enjoy Slashy as much as we do :)]]></content:encoded>
        </item>
    </channel>
</rss>