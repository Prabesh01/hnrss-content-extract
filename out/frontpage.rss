<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 05:10:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[What Is Complexity in Chess?]]></title>
            <link>https://lichess.org/@/Toadofsky/blog/what-is-complexity/pKo1swFh</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45089256</guid>
            <description><![CDATA[If we all knew, we would all be masters.]]></description>
            <content:encoded><![CDATA[If we all knew, we would all be masters.May 2020 an interesting proposal was suggested.
I provided some constructive criticism on research paper A Metric of Chess Complexity by FM David Peng, as well as constructive criticism on the codebase used to validate this experiment. For many months I have refrained from further comment, and although code has not progressed, two things have:1. Public interest in "complexity" as determined by ACPL (yuck).2. Lichess has a blogging platform where I can properly address deficiencies in the research method and control the conversation which I start.
... so the time has come for me to share my remaining criticisms of this ambitious project. Previously I had shared some easier-to-address criticisms while privately I shared Peng's suggestion with the Lichess team.
The Golden Goose
"Such a feature has the potential to revolutionize chess and would be invaluable to any chess website. Some specific applications include generating non-tactical puzzles (imagine tactics trainer for positional chess puzzles), creating chess computers that play with human personalities, and identifying concepts that are key to improvement at any rating level."
Science is a window for us to learn more about the world around us. Marketing is about selling ideas to an audience. This statement, if true, would have already garnered interest by both scientists and business people, who by exerting a modicum of effort could easily develop and sell products based upon them. Further, if true, this could also inspire a black market of cheating software to help players identify the risk associated with cheating in particular positions. Peng's paper makes many similar promises to the above, so this raises the level of scrutiny I take to the rest of the paper.
Propositions
This paper specifies complexity in two different propositions:a) Complexity is a 1-dimensional, transferable (teachable to a neural network) metric based upon centipawn loss as determined by some version(s) of Stockfish with or without a neural network.b) By definition, complexity can be used in real time to determine how difficult a position is.While some people's intuitions may support the notion that these propositions support or complement each other, I am unconvinced; regardless, it takes more than intuition to create useful tools.
Logic
Even if the above axioms were true, how many of these conclusions are logically valid?1. Non-tactical puzzles could be generated by identifying challenging positions (as opposed to the current method which is based upon positions where the solution is superior in evaluation to other variations).2. The current model for puzzle ratings (based upon "elo") takes many attempts to establish an initial puzzle rating.3. Holistic opening preparation can be automated by software, making players understand openings rather than memorize them.4. Interesting positions for books are the same as difficult positions, which are the same as complex positions.5. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop training materials to help players understand common key concepts.6. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop a diagnostic chess exam which identifies a player's rating and identifies key concepts for improvement.7. Large databases contain additional tagged information, such as time control, which would produce significant insight into which positions can be played intuitively. Large databases also indicate player ratings and therefore somehow complexity can be used to identify unique strategies useful for playing at a rating advantage or disadvantage.8. Chess players have human personalities.9. Opening systems can be devised around an opponent's tendency to seek or to avoid complexity.10. Chess players are likely to make errors in difficult positions, unlike engines, and therefore a complexity metric would be an invaluable tool.11. Spectating (and honestly, post-game analysis) of top chess games could be enriched by displaying complexity information related to each position, informing spectators who otherwise look at engine evaluations and variations & jump to conclusions.12. Complexity varies by variant; for example blitz and correspondence have different complexities for identical positions.
In my opinion, conclusion#11 is valid and others require further research. Anyway, on to Peng's research...
Neural Networks
This paper nearly predates efforts by DeepMind, the Leela Chess Zero team, and the Stockfish team which resulted in development of Stockfish-NNUE. We could not have anticipated such rapid developments! Many chess players had opinions that AlphaZero and Leela played much more human-like moves than traditional engines, in much the same manner that decades prior world champion Kasparov was astounded that Deep Blue played human-like sacrifices. Whatever conclusions are drawn may need to be updated since both Stockfish evaluations without NNUE, and Stockfish-NNUE evaluations, have rapidly changed (complementing Stockfish search changes and search parameter changes).
Endgame Scaling
Stockfish evaluations in the middlegame are capped at 10 and in the endgame are capped at 100. As such, it seems unreasonable to deviate from prior research indicating the need for a sigmoid to normalize evaluations before classifying example input moves as blunders.
Board Representation
Doing original research allows liberties in methods and models, although considerations offered here differ from those announced and discussed in public interviews by DeepMind's CEO. While I don't fully agree with DeepMind's emphasis on asymmetry and castling rights, I do question the need for an extra bit for White/Black to move. For example, the positions after 1. c3 e5 2. c4 (Black to move) and after 1. e4 c5 (White to move) should have the same relative evaluation.
Evaluation Skewness
There is ample prior research about ranking moves. In fact, Peng's research here is predicated on a notion that traditional engines sometimes indicate to spectators that two moves are equally good, despite one resulting in a more difficult position than the other. We cannot be fully certain that players in fact played the best moves, as this is the very concept we are trying to figure out how to measure! Regardless, we have to start somewhere, and this seems like a good first attempt.
Summary
I could further nitpick... I criticize because I am impressed and I care about this subject. I am further impressed that results were split by "elo," leading to a discovery that some positions are difficult for all players, whereas some positions are more difficult for lower-rated players than for higher-rated players.
Other possible improvements could involve:* Obtain segmented Stockfish evaluations (material, pawn structure, etc.) and WDL statistics* Obtain Stockfish-NNUE evaluations and WDL predictions* Model checks in sample input* Model log(time remaining) in sample input* Maybe bootstrap models based upon known pawn concepts* Maybe include some human versus engine games. Some bots such as Boris-Trapsky and TurtleBot have personalities!
Thanks for the suggestion and someday I hope to see Lichess.org or some other site implement a complexity metric before cheaters do.

Photo credit: Pacto Visual
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nintendo Switch 2 Dock USB-C Compatibility]]></title>
            <link>https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087971</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Spotilyrics ‚Äì See synchronized Spotify lyrics inside VS Code]]></title>
            <link>https://github.com/therepanic/spotilyrics</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087905</guid>
            <description><![CDATA[üéß See synchronized Spotify lyrics inside VS Code while coding - therepanic/spotilyrics]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    



  
  
  






      

          

              




  Navigation Menu

  

  
          
            


                
      

      
          

                
                    
  
      
      
          
            GitHub Copilot
          
        Write better code with AI
      

    


                    
  
      
      
          
            GitHub Spark
              
                New
              
          
        Build and deploy intelligent apps
      

    


                    
  
      
      
          
            GitHub Models
              
                New
              
          
        Manage and compare prompts
      

    


                    
  
      
      
          
            GitHub Advanced Security
          
        Find and fix vulnerabilities
      

    


                    
  
      
      
          
            Actions
          
        Automate any workflow
      

    


                    
                
              
          

                
                    
  
      
      
          
            Codespaces
          
        Instant dev environments
      

    


                    
  
      
      
          
            Issues
          
        Plan and track work
      

    


                    
  
      
      
          
            Code Review
          
        Manage code changes
      

    


                    
  
      
      
          
            Discussions
          
        Collaborate outside of code
      

    


                    
  
      
      
          
            Code Search
          
        Find more, search less
      

    


                
              
          

      



                
      

      



                
      

      
                    Explore
                    
  
      Learning Pathways

    


                    
  
      Events & Webinars

    


                    
  
      Ebooks & Whitepapers

    


                    
  
      Customer Stories

    


                    
  
      Partners

    


                    
  
      Executive Insights

    


                
              



                
      

      
              

                
                    
  
      
      
          
            GitHub Sponsors
          
        Fund open source developers
      

    


                
              
              

                
                    
  
      
      
          
            The ReadME Project
          
        GitHub community articles
      

    


                
              
              
          



                
      

      

                
                    
  
      
      
          
            Enterprise platform
          
        AI-powered developer platform
      

    


                
              



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  





    






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    2

  

  
        
            
          Star
          3

  



        

        


          

  
    


  

  




          



  
  
  Folders and filesNameNameLast commit messageLast commit dateLatest commitHistory46 Commits.idea.idea.vscode.vscodemediamediasrcsrc.gitignore.gitignore.prettierrc.prettierrc.vscodeignore.vscodeignoreLICENSELICENSEREADME.mdREADME.mddemo.pngdemo.pngeslint.config.mjseslint.config.mjsicon.pngicon.pnglogo.pnglogo.pngpackage-lock.jsonpackage-lock.jsonpackage.jsonpackage.jsonspotilyrics.imlspotilyrics.imltsconfig.jsontsconfig.jsonREADMEUnlicense license
  
  See synchronized Spotify lyrics inside VS Code while coding.
  
    
    
    
    
  


‚ú® Features

üìå Live lyrics sync with your Spotify playback.
üé® Lyrics colors auto-themed from album cover (via colorthief).
üñ•Ô∏è Smooth side panel view ‚Äì code on the left, lyrics on the right.
üîë Simple one-time login using your own Spotify Client ID.
üö™ Quick logout command to reset session.


üì∏ Demo

‚ö°Ô∏è Installation


Open VS Code ‚Üí Extensions ‚Üí search spotilyrics or install from VS Code Marketplace.


Run the command:


Show Spotify Lyrics via Spotilyrics


üîë Authentication (one-time setup)

Go to Spotify Developer Dashboard.
Create an app ‚Üí copy Client ID.
Important: set the Redirect URI for your app to: http://127.0.0.1:8000/callback
Run the Show Spotify Lyrics via Spotilyrics command.
Paste your Client ID in the panel and log in.
Enjoy synced lyrics while coding! üé∂


‚ÑπÔ∏è Why? ‚Äì To respect Spotify API rate limits, you need your own ID.


‚å®Ô∏è Commands

Show Spotify Lyrics via Spotilyrics (spotilyrics.lyrics) ‚Äì open synced lyrics panel.
Logout from Spotilyrics (spotilyrics.logout) ‚Äì clear session and re-auth when needed.


‚öôÔ∏è Tech stack

Spotify Web API
LRClib for lyrics with timing
colorthief for cover-based theme
TypeScript + VS Code WebView


üìú License
This project is licensed as Unlicensed.
Feel free to use, hack, and remix it ‚Äì but no warranties üòâ

Made with ‚ù§Ô∏è by therepanic. Code hard, vibe harder üéß





      




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lewis and Clark marked their trail with laxatives]]></title>
            <link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid>
            <description><![CDATA[AS LEWIS AND CLARK‚ÄôS Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren‚Äôt thinking much about their place in history. So they weren‚Äôt taking any particular pains to document their every movement.

There were, however, some particular pains they were experiencing, as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.

Luckily, they had something that helped with that ‚Äî a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called ‚Äúthunder-clappers,‚Äù which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.

And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in ‚Äúthunder-clappers‚Äù was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground ‚Äî which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in ‚Äî and take samples of the dirt in them. 

If it comes up with an off-the-charts reading for mercury, well, that‚Äôs a Corps of Discovery pit toilet ‚Äî and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
												   
(Astoria, Clatsop County; 1800s) --  #ofor #oregonHistory #ORhistory -- 26 Jan 2025 -- By Finn J.D. John]]></description>
            <content:encoded><![CDATA[
			
		
				
		
		
		
        
			ASTORIA, CLATSOP COUNTY; 1800s: 
			
     
    
			  
			  			  
				   Audio version is not yet available
				  
            


		              By Finn J.D. John
			                January 26, 2025
                            
                        
		              
		              AS LEWIS AND CLARK‚ÄôS Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably weren‚Äôt thinking much about their place in history. So they weren‚Äôt taking any particular pains to document their every movement.
            There were, however, some particular pains they were experiencing with every movement, so to speak ... as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.
            Luckily, they had something that helped with that ‚Äî a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called ‚Äúthunder-clappers,‚Äù which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.
            
               
                  The reproduction of Fort Clatsop, built at or near the site of the Corps of Expedition's original buildings. Dr. Rush's Bilious Pills have not been particularly helpful in locating the original Fort Clatsop, long since rotted away ‚Äî either because it hasn‚Äôt been found yet, or because the site of the old pit latrine has been disturbed by farming or logging activities in the years since. (Image: National Parks Service)
                
              
            
            And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in ‚Äúthunder-clappers‚Äù was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground ‚Äî which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in ‚Äî and take samples of the dirt in them. 
            If it comes up with an off-the-charts reading for mercury, well, that‚Äôs a Corps of Discovery pit toilet ‚Äî and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
            
              THESE PILLS WERE the pride and joy of Dr. Benjamin Rush, one of the Founding Fathers and a signer of the Declaration of Independence. Rush was also the man President Thomas Jefferson considered the finest physician in the republic. 
            In that opinion, Jefferson was probably alone, or at least in a small minority. Dr. Rush‚Äôs style of ‚Äúheroic medicine‚Äù had caused his star to fall quite a bit by this time ‚Äî especially after the Philadelphia yellow fever epidemic of 1793, when his patients died at a noticeably higher rate than untreated sufferers. 
            At the time, of course, very little was known about how the human body worked. Physicians were basically theorists, who made educated guesses and did their best. 
            The problem was, the education on which those educated guesses were based varied pretty wildly depending on what school you came from. Homeopathic physicians theorized that giving patients a tiny amount of something that mimicked their symptoms would stimulate the body to cure itself. Eclectic physicians sought cures from herbs and folk remedies. Hydropathic physicians believed hot and cold water, applied externally or internally, was all that was needed. 
            Dr. Rush wasn‚Äôt from one of these schools. He was from the school of mainstream medicine ‚Äî also known as allopathic medicine (although that term is a perjorative today).
            Allopathic medical theory, in the early 1800s, dated from the second century A.D., courtesy of a Roman doctor named Galen. 
            Galen theorized that the human body ran on four different fluids, which he called ‚Äúhumours‚Äù: Blood, phlegm, yellow bile, and black bile. All disease, he claimed, stemmed from an imbalance in these humours.
            Thus, too much blood caused inflammation and fever; the solution was to let a pint or two out. Too much bile caused problems like constipation; the solution was to administer a purgative and let the patient blow out some black bile into a handy chamber-pot, or vomit up some yellow bile ‚Äî or both.
            These interventions sometimes helped, but most of the time they had little or no good effect. So by Rush‚Äôs time, a number of physicians were going on the theory that what was needed was a doubling-down on their theory ‚Äî in a style of practice that they called ‚Äúheroic medicine.‚Äù
            If a sensible dose of a purgative didn‚Äôt get a patient‚Äôs bile back in balance, a ‚Äúheroic‚Äù dose might. If a cup or two of blood didn‚Äôt get the fever down, four or five surely would.          
          
            ¬†
            
              [EDITOR'S NOTE: In "reader view" some phone browsers truncate the story here, algorithmically "assuming" that the second column is advertising. (Most browsers do not recognize this page as mobile-device-friendly; it is designed to be browsed on any device without reflowing, by taking advantage of the "double-tap-to-zoom" function.) If the story ends here on your device, you may have to exit "reader view" (sometimes labeled "Make This Page Mobile Friendly Mode") to continue reading. We apologize for the inconvenience.]
            
            ‚Äî(Jump to top of next column)‚Äî
    

        
           
            A sketch of Fort Clatsop as it would have appeared in 1805. (Image: Oregon Historical Society)
          
        
        
          You can imagine what the result of this philosophy was, when applied to an actual sick person.
        ‚ÄúSome people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,‚Äù says physician and historian David Peck. ‚ÄúI think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.‚Äù
        In lieu of a trained physician, the Corps of Discovery‚Äôs leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing ‚Äúheat,‚Äù opium products for relieving pain and inducing sleep ‚Äî and purgatives.
        Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called ‚ÄúDr. Rush‚Äôs Bilious Pills.‚Äù They contained about 10 grains of calomel and 10 to 15 grains of jalap.
        
           
              This recipe for a milder version of Rush's Bilious Pills comes from the National Formulary in 1945. This image appears in the Lewis and Clark Fort Mandan Foundation's Web site, at which there's a lot more information about the ingredients in this compound. Mercury was still being used as an internal medicine in the 1960s and as a topical antiseptic (chiefly as Mercurochrome) into the 1990s.
            
          
        
        Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power. 
        And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don‚Äôt get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance. 
        Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis ‚Äúsporting house‚Äù before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.
        When symptoms broke out, the patient would be dosed with ‚Äúthunder clappers‚Äù and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself. 
        And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative ‚Äúon the regular‚Äù (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.
        And this low-fiber diet had predictable results.
        It had another result, too, which was less predictable ‚Äî although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given ‚Äúbilious pill‚Äù gets blown out post-haste in the ensuing ‚Äúpurge.‚Äù
        Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.
        So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way ‚Äî a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.        
        
          
            (Sources: Class lecture in History of American Medicine, October 2009, Univ. of Oregon, by Dr. James Mohr; Or Perish in the Attempt: Wilderness Medicine in the Lewis and Clark Expedition, a book by David J. Peck published in 2002 by Farcountry Press; ‚ÄúFollowing Lewis and Clark‚Äôs Trail of Mercurial Laxatives,‚Äù an article by Marisa Sloan published in the Jan. 29, 2022, issue of Discover Magazine.)
          TAGS: #Archaeology #HeroicMedicine #DavidPeck #Jalap #Syphilis #CorpsOfDiscovery #BenjaminRush #Humours #Medicine #FrontierDoctors #Galen #FortClatsop #Calomel #MercuryPoisoning #Thunderclappers #Constipation #DrJamesMohr #OregonTrail #DrRush's #BiliousPills #Bile #COLUMBIAgorge #CLATSOPcounty
        

		  
          

          
          

      

     
    
		
		    Background image is a postcard, a hand-tinted photograph of Crown Point and the Columbia Gorge Scenic Highway. Here is a link to the Offbeat Oregon article about it, from 2024.
		    Scroll sideways to move the article aside for a better view.
		    
		    Looking for more?
            On our Sortable Master Directory you can search by keywords, locations, or historical timeframes. Hover your mouse over the headlines to read the first few paragraphs (or a summary of the story) in a pop-up box.
            ... or ...		    
		    Home
		    
	      

    
    
  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Linux version of the Procmon Sysinternals tool]]></title>
            <link>https://github.com/microsoft/ProcMon-for-Linux</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid>
            <description><![CDATA[A Linux version of the Procmon Sysinternals tool. Contribute to microsoft/ProcMon-for-Linux development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Process Monitor for Linux (Preview) 
Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows.  Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.

Installation & Usage
Requirements

OS: Ubuntu 18.04 lts
cmake >= 3.14 (build-time only)
libsqlite3-dev >= 3.22 (build-time only)

Install Procmon
Please see installation instructions here.
Build Procmon
Please see build instructions here.
Usage
Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file
Examples
The following traces all processes and syscalls on the system:
sudo procmon
The following traces processes with process id 10 and 20:
sudo procmon -p 10,20
The following traces process 20 only syscalls read, write and open at:
sudo procmon -p 20 -e read,write,openat
The following traces process 35 and opens Procmon in headless mode to output all captured events to file procmon.db:
sudo procmon -p 35 -c procmon.db
The following opens a Procmon tracefile, procmon.db, within the Procmon TUI:
sudo procmon -f procmon.db
Feedback

Ask a question on Stack Overflow (tag with ProcmonForLinux)
Request a new feature on GitHub
Vote for popular feature requests
File a bug in GitHub Issues

Contributing
If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:

How to build and run from the source
The development workflow, including debugging and running tests
Coding Guidelines
Submitting pull requests

Please see also our Code of Conduct.
License
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We should have the ability to run any code we want on hardware we own]]></title>
            <link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid>
            <description><![CDATA[Refuting the common and flawed argument of]]></description>
            <content:encoded><![CDATA[
  
  Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: ‚ÄúI should be able to run whatever code I want on hardware I own‚Äù. I agree entirely with this point, but within the context of this discussion it‚Äôs moot.


  ‚ÄúI should be able to run whatever code I want on hardware I own‚Äù


When Google restricts your ability to install certain applications they aren‚Äôt constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. It‚Äôs through this control of the operating system that Google is exerting control, not at the hardware layer. You often don‚Äôt have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Apple‚Äôs success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.

You shouldn‚Äôt take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldn‚Äôt be of the restrictions in place in the operating systems they provide ‚Äì rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sony‚Äôs restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.


  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Best practices for dealing with human waste in the great outdoors]]></title>
            <link>https://theconversation.com/how-to-poop-outdoors-in-a-way-that-wont-harm-the-environment-and-other-hikers-262426</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086440</guid>
            <description><![CDATA[Most people want to properly dispose of their waste, but they‚Äôre typically not prepared. Land managers can help users meet the moment.]]></description>
            <content:encoded><![CDATA[
    If you‚Äôre one of the 63 million Americans who went hiking last year, chances are you‚Äôve found yourself needing to go, with no toilet in sight.

Aside from personal inconvenience, why is this such a big deal? 

Human fecal contamination is a public health concern in natural areas. Pathogens in human poop can remain active for a long time ‚Äì over a year in outdoor environments ‚Äì meaning that waste left behind today can cause severe gastrointestinal disease and other sicknesses for future visitors. Fecal waste can enter waterways after storms or snowmelt, harming water quality. Finally, it can be upsetting ‚Äì or at the least, unpleasant ‚Äì to encounter someone else‚Äôs poop and used toilet paper in nature.


            
            
              Toilet paper waste on Mount Elbert in the San Isabel National Forest in Colorado.
              Shari Edelson, CC BY-ND
            
          

As a researcher and a Ph.D. candidate who study human impacts on parks and protected areas, we have been thinking quite a lot about poop and ways people can tread more lightly on the landscape. Our focus is on Leave No Trace, an environmental education framework ‚Äì created by an organization with the same name ‚Äì that helps people implement minimal-impact practices in the outdoors.

Poop is causing problems in parks and protected areas

From the Appalachian Trail and Mount Everest ‚Äì known as Sagarmatha in Nepali ‚Äì to national parks in Norway and Aotearoa ‚Äì known as New Zealand to English speakers,  researchers have documented the negative impacts our bodily wastes are causing in the sensitive environments where we seek recreation and restoration. 

In Colorado, the problem has gotten so bad that land managers have decided to take action. In the Eagle-Holy Cross District of the White River National Forest, for example, the U.S. Forest Service now requires visitors to take their human waste out with them.


            
            
              A footbridge on the Chimney Tops Trail in the Great Smoky Mountains near the Appalachian Trail.
              Shari Edelson, CC BY-ND
            
          

Best practices for dealing with your poo in the great outdoors

One of us ‚Äì Derrick Taff ‚Äì works as a science adviser to Leave No Trace, an organization that has educated outdoor recreationists on this issue for more than 30 years and has provided concrete guidance based on scientific research. 

The first rule of thumb is to avoid the possibility of contamination entirely by not leaving waste in natural areas to begin with. Toilet facilities are regarded as the most effective method to reduce human waste in the backcountry. If there‚Äôs a toilet at the trailhead, use it before you head out.

Current research we‚Äôre doing in Grand Teton National Park in Wyoming and San Isabel National Forest in Colorado confirms that hikers prefer to use trailhead toilets when they‚Äôre available. 

But as anyone who‚Äôs been out in the woods is aware, remote wilderness areas do not necessarily offer such infrastructure. Access for maintenance and waste removal costs are major barriers for land management agencies considering installing backcountry toilets. 

And then there‚Äôs the very real likelihood that even when trailhead facilities do exist, you may be far away when nature calls. In our own research, pending publication, we surveyed hikers on Colorado‚Äôs Mount Elbert. Up to 70% of those needing to poop ended up doing so in the backcountry despite the presence of a trailhead toilet.

Issues develop because hikers aren‚Äôt prepared

This issue may persist because people aren‚Äôt aware of the current rules. In our soon-to-be-published study of Grand Teton hikers, 66% of backcountry trail visitors reported that they had not received any information on how to dispose of human waste in the park. 


            
            
              The view from String Lake in Grand Teton National Park.
              Shari Edelson, CC BY-ND
            
          

Other reasons why people may not follow the rules are because they may consider them onerous or unimportant. 

Research shows that clear, actionable messaging including relevant environmental and moral appeals does make a difference in shifting people‚Äôs behaviors in the outdoors. Although individual choices may seem inconsequential, they add up to big impacts in the aggregate.

How to poop in the backcountry

So what to do when there really is no potty? Leave No Trace advises us of two main options.

The first is to dig a little pit, commonly called a cat hole, and deposit your poop in there. Can‚Äôt aim? No worries ‚Äì Just poop next to the hole and scoop it in afterward.

The use of cat holes is recommended in areas where it‚Äôs possible to dig roughly the length of your hand deep in the soil, where moist ground indicates that material buried there will decompose, and where digging is not likely to disturb fragile environments. Make sure you‚Äôre about 70 steps away from any water source, trail or campsite to avoid water contamination and reduce the likelihood that someone else will accidentally come upon your waste.

You can typically leave toilet paper in a cat hole, but check local regulations and carry it out in a sealed bag if not. Never leave wet wipes behind. They don‚Äôt biodegrade.

Outdoor companies are now making lightweight trowels designed for digging cat holes in the backcountry. But there are also places where it‚Äôs difficult if not impossible to dig a cat hole because of snow, frozen ground, shallow soil or exposed bedrock, or where leaving human waste in the outdoors is not recommended due to environmental conditions. These typically include high-mountain zones above tree line, alpine environments inhabited by delicate and slow-growing flora, and deserts and other arid places characterized by low soil moisture. 

In places like this, it‚Äôs best to remove all poop and toilet paper and dispose of it in a proper location such as a trash can at the trailhead or even back at your home. Before you recoil in horror, remember that dog owners do this with their pets‚Äô waste when on a walk.

Wag bags ‚Äì short for waste aggregation and gelling ‚Äì are used to pack out poop. Wag bag kits typically include an inner and an outer bag as well as a drying agent to prevent odor and leakage. Our current research, as well as a recent study of Norwegian park users, has demonstrated that people are willing to use them.


            
            
              A kiosk offers free wag bags at the beginning of the Mount Elbert summit trail near Leadville, Colo. Wag bags are commonly used by hikers as self-contained receptacles for feces.
              Shari Edelson, CC BY-ND
            
          

Our study found that among people who defecated while on a hike to the summit of Mount Elbert, 30% used a wag bag to carry their waste off the mountain, and 87% expressed willingness to use one on future trips. 

These results suggest that people are willing to do the right thing when given the proper tools and information, and that it‚Äôs possible to effectively teach people how to care for our wild spaces.
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Installing UEFI Firmware on ARM SBCs]]></title>
            <link>https://interfacinglinux.com/2025/08/25/edk2-uefi-for-the-rock-5-itx/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086346</guid>
            <description><![CDATA[I'm going to neuralyze the SPI flash and install some Kelvin Timeline firmware that will allow me to boot and install generic ARM Linux images on a ROCK 5 ITX+.]]></description>
            <content:encoded><![CDATA[I am a huge fan of my Rock 5 ITX+. It wraps an ATX power connector, a 4-pin Molex, PoE support, 32 GB of eMMC, front-panel USB 2.0, and two Gen 3√ó2 M.2 slots around a Rockchip 3588 SoC that can slot into any Mini-ITX case. Thing is, I never put it in a case because the microSD slot lives on the side of the board, and pulling the case out and removing the side panel to install a new OS got old with a quickness.I originally wanted to rackmount the critter, but adding a deracking difficulty multiplier to the microSD slot minigame seemed a bit souls-like for my taste. So what am I going to do? Grab a microSD extender and hang that out the back? Nay! I‚Äôm going to neuralyze the SPI flash and install some Kelvin Timeline firmware that will allow me to boot and install generic ARM Linux images from USB.At least, that‚Äôs the plan üôÇEDK2 RK3588EDK2-RK3588 is a UEFI firmware implementation based on EDK2 for various RK3588 boards. It delivers a PC-like, standardised boot experience, supporting multiple operating systems, such as Windows, Linux, BSD, and VMware ESXi. At least, that‚Äôs what it says on the tin.In a perfect world, I could write the EDK2 firmware to a microSD and try before you buy. Welp, that‚Äôs not going to work on the Rock 5 ITX+ because it‚Äôs a wee bit, what‚Äôs a polite way to put this, special. Not only does the microSD trick not work, it also refuses to let me boot the EDK2 firmware from eMMC.Time to break out the neuralyzer!ARMBIANTo pull off this roundabout feat of engineering, I first need to boot the Rock 5 ITX+ to a desktop, and there is exactly one correct answer to that problem: Armbian 25.2.2 Noble Gnome with kernel 6.1.Once booted to the desktop, it‚Äôs time to crack open a copy of Chromium, head over to the EDK2-RK3588 GitHub page, and download the latest rock-5-itx UEFI release.Remember the ‚Äúspecialness‚Äù I talked about earlier? Yeah, the Rock 5 ITX+ exposes the SPI flash as a standard 17 MB block device, meaning I can flash the UEFI image with GNOME Disks.Granted, I‚Äôm still flashing the SPI, so I had to remind myself not to freak out when it seized up at the start, then proceeded to write the image at the speed of smell. The whole process ended up  taking about a minute.All that‚Äôs left to do at this point is power down the system, pop out the microSD, and reapply the electrons. Saying a prayer to an elder deity is optional but highly recommended.UEFI BOOTAfter a mild panic attack, I realised the EDK2 boot screen only shows up on when connected to the first HDMI output. I was finally greeted by a Radxa splash screen, and tapping Escape got me into the setup menu.There‚Äôs a gang of settings to play around with, but for the time being I‚Äôm only focused on one, ACPI / Device Tree.It‚Äôs located in Device Manager > Rockchip Platform Configuration > ACPI / Device Tree. Here I could tell the Rock 5 ITX+ to boot using ACPI, Device Tree, or both.I decided to keep the Config Table Mode set to both, with Device Tree Configuration set to Mainline, and DTB override and firmware fixups enabled.GENERIC ARM LINUXThe whole point of this experiment was to be able to download generic ARM images for any distribution and install them as I would on my x86-64 desktop PC. The thing is, there‚Äôs a miniboss hiding in plain sight on the EDK2-RK3588 GitHub page.Basically, if I want a hardware-accelerated desktop (or anything else), I‚Äôm going to need a distribution shipping with kernel 6.15+, and while that might seem comical to anyone reading from the future, Debian 13 was released this week and it‚Äôs rocking 6.12.The kernel 6.15+ requirement rules out the current releases of Armbian, openSUSE, Ubuntu, and Fedora. Thankfully, both Ubuntu (Daily) and Fedora (Rawhide) have bleeding-edge builds, so I‚Äôm crossing my pinky toes, hoping that one of them works.FEDORA RAWHIDEIf you haven‚Äôt heard of Rawhide, it‚Äôs bleeding-edge Fedora, aimed at advanced users, testers, and anyone trying to shove Fedora onto an ARM SBC running EDK2 firmware in the glorious year of his noodly appendage, 2025.Out of the gate, Fedora was being fussy, requiring me to switch the Config Table to Device Tree just to get a display. Even then, it took a reboot (or two) before finally gracing me with display output.Once I defeated GRUB, it was smooth sailing. Pick a language, select the installation drive, agree to murderate some databits, and exit to the desktop. All that‚Äôs left to do is power off the system, remove the flash drive, and reapply the electrons.After the reboot, I was greeted by a non-functioning GNOME desktop. Well, it wasn‚Äôt completely broken, as I could wiggle the mouse pointer, and that is how things remained until Mr üôÅ showed up.One of the superpowers you develop while working with ARM SBCs is the ability to get up and walk away in case it‚Äôs having an extended think, knowing the solution is simply to wait it out.After however long it took me to make a cup of tea, I came back to a task failed successfully and was finally able to complete the setup.A quick poke around the GNOME Control Center revealed that networking, power, and display options were intact and operational.Then I clicked on sound. I‚Äôm going to assume this is solvable by enabling some third-party repository, but that‚Äôs for someone else to figure out.UBUNTU 25.10I‚Äôve talked an egregious amount of smack about Ubuntu over the decades, 73.6% of it deserved, but credit where it‚Äôs due: they‚Äôve put serious time and energy into making their desktop work on both ARM and RISC-V.So I wasn‚Äôt terribly surprised that GRUB appeared after I reset the EDK2 firmware to its defaults, followed by the installer‚Äôs  splash screen.If you have installed Ubuntu before, there is nothing of note here. Like Fedora, it offered the option to install the system on NVMe or eMMC, along with settings to enable third-party repositories.Another rummage in the GNOME Control Center revealed that networking, power, and display options were intact and operational. Unlike Fedora, Ubuntu dispelled the myth that sound does not work on Linux. At least on ARM, out of the box‚Ä¶ on a bleeding edge distro.I also want to point out that Vulkan worked on both Ubuntu 25.10 and Fedora Rawhide. The open-source drivers still have some catching up to do, but they are most certainly getting there and are more than enough to provide a smooth desktop experience.NETBSD?I also grabbed a copy of NetBSD daily, and it booted right up. I didn‚Äôt bother trying to install it, but it picked up the ethernoodles, so you could probably get up to, something?VERDICTThis all started because I wanted to put the Rock 5 ITX+ into a rackmount case and have a way to swap out the OS without pulling it from the rack and fiddling with the SD card. And yes, while writing this, all I can think about is how long it will take for someone to point out the existence of SD card extenders in the YouTube comments.I won‚Äôt fault them for doing so. A simple SD extender plus a little arts and crafts with a Dremel would have been my solution if I needed a working fix yesterday.But I really wanted to see where EDK2 RK3588 stands, since Collabora is hard at work upstreaming support for the RK3588 to the Linux kernel, and that‚Äôs going to be the SoC where everything JustWorks‚Ñ¢ in a year or so.Right now, EDK2 + RK3588 isn‚Äôt there yet, but it‚Äôs not that far off either. It‚Äôs the worst of all states, kinda works if you squint a little. The big hangup is kernel 6.15, but that only matters if you need HDMI output with hardware acceleration. Building a headless fileserver? You‚Äôre already good to go.It looks like the Rock 5 ITX+ has escaped the rack, for now.PRODUCT LINKSAll the links in this article go directly to the sources they reference. There are no affiliate links and no backlinks to unrelated articles because I absolutely loathe how common that has become. If you want to support Interfacing Linux, you can use the affiliate links listed below or join the Patreon.Radxa ROCK 5 ITX+: https://s.click.aliexpress.com/e/_olkG93p IN WIN PC Case: https://amzn.to/3JrTKk5Some posts contain affiliate links. If you click on an affiliate link and later make a purchase, I may receive a small commission.Have questions about your Linux setup? Ask in the¬†forums.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lunar soil machine developed to build bricks using sunlight]]></title>
            <link>https://www.moondaily.com/reports/Lunar_soil_machine_developed_to_build_bricks_using_sunlight_999.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086238</guid>
            <description><![CDATA[Tokyo, Japan (SPX) Jul 30, 2025 - 
A Chinese research team has created a prototype machine that transforms moon soil into durable construction bricks using solar energy, marking a critical step toward building lunar structures from l]]></description>
            <content:encoded><![CDATA[



Lunar soil machine developed to build bricks using sunlight
by Riko Seibo
Tokyo, Japan (SPX) Jul 30, 2025






A Chinese research team has created a prototype machine that transforms moon soil into durable construction bricks using solar energy, marking a critical step toward building lunar structures from local materials.

Developed by the Deep Space Exploration Laboratory (DSEL) in Hefei, Anhui province, the system functions as a 3D printing device powered by concentrated solar heat. It employs a parabolic reflector to gather solar radiation, which is then funneled through fiber optic bundles. At the focus point, light intensity exceeds 3,000 times the standard level, reaching temperatures over 1,300 C to melt lunar regolith.

According to senior engineer Yang Honglun, the machine uses no additives - relying entirely on lunar soil. The bricks produced are dense and robust, suitable not only for shelter construction but also for roads and platforms on the lunar surface.

The project spanned two years of research and development. Key challenges included transporting and melting variable lunar soil compositions and achieving efficient solar energy transmission. To address this, the team created multiple types of simulated lunar soil for extensive trials.

While the technology is a milestone, Yang emphasized that lunar soil bricks alone cannot sustain pressure in the moon's vacuum and low-gravity environment. Instead, the bricks will act as protective layers over pressure-retaining habitat modules made of rigid and inflatable structures.

He outlined a broader vision for lunar construction involving brick manufacturing, modular component integration, and structural validation under true lunar surface conditions. These efforts aim to enable full-scale surface construction supported by automated robots and the brick-making device.

This development aligns with China's plans for the International Lunar Research Station (ILRS), a joint venture with 17 countries and over 50 research bodies. Scheduled in two phases, ILRS will establish a base in the lunar south pole region by 2035, with expansions in the 2040s.

In preparation, Chinese astronauts aboard the nation's space station will expose simulated lunar bricks - delivered by the Tianzhou 8 cargo spacecraft in November 2024 - to space conditions. These experiments will assess thermal durability, mechanical integrity, and radiation shielding to inform future lunar base construction.




Related Links

Deep Space Exploration Laboratory
Mars News and Information at MarsDaily.comLunar Dreams and more





The content herein, unless otherwise known to be public domain, are Copyright 1995-2024 - Space Media Network. All websites are published in Australia and are solely subject to Australian law and governed by Fair Use principals for news reporting and research purposes. AFP, UPI and IANS news wire stories are copyright Agence France-Presse, United Press International and Indo-Asia News Service. ESA news reports are copyright European Space Agency. All NASA sourced material is public domain. Additional copyrights may apply in whole or part to other bona fide parties. All articles labeled "by Staff Writers" include reports supplied to Space Media Network by industry news wires, PR agencies, corporate press officers and the like. Such articles are individually curated and edited by Space Media Network staff on the basis of the report's information value to our industry and professional readership. Advertising does not imply endorsement, agreement or approval of any opinions, statements or information provided by Space Media Network on any Web page published or hosted by Space Media Network. General Data Protection Regulation (GDPR) Statement Our advertisers use various cookies and the like to deliver the best ad banner available at one time. All network advertising suppliers have GDPR policies (Legitimate Interest) that conform with EU regulations for data collection. By using our websites you consent to cookie based advertising. If you do not agree with this then you must stop using the websites from May 25, 2018. Privacy Statement. Additional information can be found here at About Us.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What to do with C++ modules?]]></title>
            <link>https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086210</guid>
        </item>
        <item>
            <title><![CDATA[Eternal Struggle]]></title>
            <link>https://yoavg.github.io/eternal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid>
            <description><![CDATA[change background]]></description>
            <content:encoded><![CDATA[
  
  
  change background



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How is Ultrassembler so fast?]]></title>
            <link>https://jghuff.com/articles/ultrassembler-so-fast/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45085156</guid>
            <description><![CDATA[How is Ultrassembler so fast?]]></description>
            <content:encoded><![CDATA[
            

  How is Ultrassembler so fast?

Published 2025-08-30
Ultrassembler is a superfast and complete RISC-V assembler library that I'm writing as a component of the bigger Chata signal processing project. 
Assemblers take in a platform-dependent assembly language and output that platform's native machine code which runs directly on the processor.

"Why would you want to do this?" you might ask. First, existing RISC-V assemblers that conform the the entirety of the specification, as and llvm-mc, ship as binaries that you run as standalone programs. This is normally not an issue. However, in Chata's case, it needs to access a RISC-V assembler from within its C++ code. The alternative is to use some ugly C function like system() to run external software as if it were a human or script running a command in a terminal. 
Here's an example of what I'm talking about:
#include <iostream>
#include <string>
#include <stdlib.h>

int main() {
    std::string command = "riscv64-linux-gnu-as code.s -o code.bin";

    int res = std::system(command.data());

    if (res != 0) {
        std::cerr << "Error executing command: " << command << std::endl;
    }
    return res;
}

It gets even worse once you realize you need temporary files and possibly have to artisanally craft the command beforehand. Additionally, invoking the assembler in this manner incurs a significant performance overhead on embedded systems which lack significant processing power. There must be a better way. 
Enter Ultrassembler.
With these two goals of speed and standard conformance in mind, I wrote Ultrassembler as a completely standalone library with GNU as as the speed and standard conformity benchmark. 
The results are nothing short of staggering. 
After months of peformance optimization, Ultrassembler can assemble a test file with about 16 thousand RISC-V instructions over 10 times faster than as, and around 20 times faster than llvm-mc. To put it another way, it only takes about 1000 CPU instructions (+-50% depending on platform) to assemble one RISC-V instruction, while it takes 10,000 for as and 20,000 for llvm-mc. This happens with plain old C++ code only and no platform-specific assembly code, although integrating assembly could crank up the speed even further.
Such performance ensures a good user experience on the platforms where Chata runs, but also as a consequence of this lack of overhead, you could also combine Ultrassembler with fantastic libraries like libriscv to implement low-to-no-cost RISC-V scripting in things like games, or maybe even in your JIT programming language!
Let's look at some of the ways I made Ultrassembler this fast so that you can reap the benefits too.
WARNING! ¬† The code you're about to see here is only current as of this article's publishing. The actual code Ultrassembler uses could be different by the time you read this in the future!
Exceptions
Exceptions, C++'s first way of handling errors, are slow. Super duper slow. Mega slow. So slow, in fact, that many Programming Furus¬©Ô∏è¬ÆÔ∏è‚Ñ¢Ô∏è say you should never ever use them. They'll infect your code with their slowness and transform you into a slow old hunchback in no time. 
Or so you would think.
C++ exceptions, despite being so derided, are in fact zero-overhead. Huh? Didn't I just say they were super duper slow? Let me explain.
It's not clear when exactly exceptions are slow. I had to do some research here. As it turns out, GCC's libstdc++ uses a so-called "zero-overhead" exception system, meaning that in the ideal normal case where the C++ code calls zero exceptions, there is zero performance penalty. But when it does call an exception, it could become very slow depending on how the code is laid out. Most programmers, not knowing this, frequently use exceptions in their normal cases, and as a result, their programs are slow. Such mysterious behavior caught the attention of Programming Furus¬©Ô∏è¬ÆÔ∏è‚Ñ¢Ô∏è and has made exceptions appear somewhat of a curse.
This tragic curse turns out to be a heavenly blessing for Ultrassembler. In the normal case, there are zero errors to report as a result of proper usage of RISC-V instructions. But if there's some error somewhere, say somebody put in the wrong register, then Ultrassembler sounds the alarm. Since such mistakes only occur as a result of human error (ex bugs in codegen and Ultrassembler itself) the timeframe in which to report the error can expand to that of a human. As a result, even if an exception triggered by a mistake took a full 1 second (about a million times slower than it does in reality), it doesn't matter because the person percepting the error message can only do so in approximately that second timeframe.
"But hold on!" you exclaim. "What about std::expected?" In response to some programs which frequently need to handle errors not seen by humans, C++ added a system to reduce the overhead of calling errors, std::expected. I tried this in Ultrassembler and the results weren't pretty. It trades off exception speed for normal case speed. Since the normal case is the norm in Ultrassembler, std::expected incurred at least a 10% performance loss due to the way the std::expected object wraps two values (the payload and the error code) together. See this C++ standard document for the juicy details.
The end result of the use of exceptions is that there is zero performance penalty to optimize out.
Fast data structures
Between all of the RISC-V instruction set extensions, there are 2000+ individual "instructions" (many instructions are identical to one another with a slight numerical change). There are also hundreds of CSRs and just under a hundred registers. This requires data structures large enough to store the properties of thousands of entries. How do you do that? It's tricky. So, how about I just show you what Ultrassembler uses as of this writing:
struct rvregister {
    RegisterType type; //1B
    RegisterID id; //1B
    uint8_t encoding;
    uint8_t padding;
};

const std::array<rvregister, 96> registers;

struct rvinstruction {
    RVInstructionID id; //2B
    RVInstructionFormat type; //1B
    uint8_t opcode;
    uint16_t funct;
    RVInSetMinReqs setreqs; //1B
    rreq regreqs = reg_reqs::any_regs; //1B
    special_snowflake_args ssargs = special_snowflake_args(); //2B
};

// We use a strong typedef to define both rreq and ssflag, but the underlying is a uint8_t in both cases

namespace ssarg {

constexpr ssflag get_imm_for_rs = ssflag(0b00000001);
constexpr ssflag use_frm_for_funct3 = ssflag(0b00000010);
constexpr ssflag special_handling = ssflag(0b00000100);
constexpr ssflag swap_rs1_rs2 = ssflag(0b00001000);
constexpr ssflag use_funct_for_imm = ssflag(0b00010000);
constexpr ssflag no_rs1 = ssflag(0b00100000);
constexpr ssflag has_custom_reg_val = ssflag(0b01000000);

}

struct special_snowflake_args {
    uint8_t custom_reg_val = 0;
    ssflag flags; //1B
};

const std::array<rvinstruction, 2034> instructions;

Let's go over what each struct does.
rvregister
rvregister is how Ultrassembler stores the data for all the RISC-V registers. What describes a register? You have its friendly name (like x0 or v20), an alias (like zero or fa1), what kind of register it is (integer, float, or vector?), and what raw encoding it looks like in instructions. You can get away with single bytes to represent the type and encoding. And, that's what we use here to keep data access simple. You could squeeze everything into one or two bytes through clever bitmasking, but after doing so, I couldn't find much of a speedup. This could be situational and so you should not dismiss such a trick.
Why not store the name and alias strings? Ultrassembler does not actually reference the name nor the alias anywhere in its code. Why? Strings are very expensive. This fact is not obvious if you have not made software at the level of Ultrassembler, where string comparison and manipulation grind computation to a crawl. So we just don't use strings anywhere. In spite of this, the initializers of const std::array<rvregister, 96> registers do contain both the name and alias, but the constructors silently discard these data. Such inclusion enables external scripts to look at the array and generate code around it. We'll look at that in the next section. But for now, know that we hate strings.
rvinstruction
rvinstruction follows a similar idea, with the biggest differences being that it's much bigger, 2000+ entries versus 96, and that there is more information to store per entry. This necessitates some extra memory saving magic because there are so many different instructions. We first need an ID for each instruction to do specific checks if needed. We have almost more than 2048 instructions (subject to future expansion) but less than 4196, so we'll need 2 bytes. There are fewer than 256 "types" of instructions (R, I, S, B, U, J, etc.), so 1 byte is good. Same idea with all the other fields. Similarly to rvregister, it would be possible to use bitmasking to compress everything, but this might not result in a significant speedup.
special_snowflake_args
In RISC-V, many instructions require special attention because they have a special encoding, do something special, or are otherwise different from the rest of the herd. To avoid hardcoding behavior handling as much as possible, special_snowflake_args encodes specific properties that many of these special instructions share, such as getting an immediate value instead of a register, swapping the rs1 and rs2 registers (or vs1 and vs2), or omitting a register entirely. We can encode all these properties in a binary way so we use a custom bitmask system to save all the properties in a single byte. custom_reg_val, however, is a separate 1-byte field because registers use 5 bits, and only exists in tandem with has_custom_reg_val.
All together, we are able to use only 20kB of memory to save all the instructions, not withstanding future entries. This fits nicely into many CPU data caches.
Preallocated memory pools
In C++, by default, containers that dynamically allocate memory do so through the heap. The underlying OS provides the heap through assignment of a certain section of its virtual memory to the program requesting the heap memory. Heap allocation happens transparently most of the time. Unfortunately for us, it matters where exactly that memory is. Memory located far away from everything else (often the case with heap memory) unnecessarily clogs up the CPU's memory cache. Additionally, in C++, requesting that heap memory also requires a syscall every time the container geometrically changes size (roughly speaking, 1B -> 2B -> 4B -> 8B -> ... -> 1MB). Syscalls drastically slow down code execution (more so than yo mama is big) because the OS needs to save all the registers, swap in the kernel's, and run the kernel code, all while clogging up the CPU cache again. Therefore, we need a way to allocate memory close to our variables with zero syscalls. 
The solution? 
Preallocated memory pools.
C++ offers a totally neato way to use the containers you know and love with a custom crafted memory allocator of your choice. 
Here's how Ultrassembler does it.
constexpr size_t memory_pool_size = 33554432;

template <class T>
class MemoryBank;

typedef std::basic_string<char, std::char_traits<char>, MemoryBank<char>> ultrastring;

template <typename T>
using ultravector = std::vector<T, MemoryBank<T>>;

class GlobalMemoryBank {
    inline static std::array<std::byte, memory_pool_size> pool;
    inline static size_t used = 0; 
    inline static long pagesize = sysconf(_SC_PAGE_SIZE); // This only happens once :)

public:
    void* grab_some_memory(size_t requested);

    void reset();
};

extern GlobalMemoryBank memory_bank;

template <class T>
class MemoryBank {
public:
    using value_type = T;

    MemoryBank() = default;

    [[nodiscard]] T* allocate(size_t requested) {
        std::size_t bytes = requested * sizeof(T);
        return reinterpret_cast<T*>(memory_bank.grab_some_memory(bytes));
    }

    void deallocate(T* ptr, size_t requested) { return; }

    bool operator==(const MemoryBank&) const { return true; }
};

// In another file...

void* GlobalMemoryBank::grab_some_memory(size_t requested) {
    if (requested + used > pool.size()) {
        throw UASError(OutOfMemory, "Out of memory!");
    }
    void* ptr = reinterpret_cast<void*>(pool.data() + used);
    used += requested;
    return ptr;
}

void GlobalMemoryBank::reset() {
    used = 0;
}

Let's go through this section by section.
constexpr size_t memory_pool_size = 33554432;

template <class T>
class MemoryBank;

typedef std::basic_string<char, std::char_traits<char>, MemoryBank<char>> ultrastring;

template <typename T>
using ultravector = std::vector<T, MemoryBank<T>>;

This is boilerplate defining how big our memory pool is (in bytes), declaring the regular memory pool class (annoying!), what our special memory pool string is an alias of (a standard string but with the regular memory pool allocator), and the same creation of a vector using the regular memory pool.
class GlobalMemoryBank {
    inline static std::array<std::byte, memory_pool_size> pool;
    inline static size_t used = 0;
    inline static long pagesize = sysconf(_SC_PAGE_SIZE);

public:
    void* grab_some_memory(size_t requested);

    void reset();
};

extern GlobalMemoryBank memory_bank;

This class defines the memory pool wrapper that the actual allocator uses. Why? This has to do with how C++ uses custom allocators. When you use a container with a custom allocator, each declaration of that container creates a separate instance of that container and the allocator class. Therefore, if you added the memory pool array as a member of this custom allocator class, each declaration of the container would result in separate instantiations of the underlying memory pool object. This is UNACCEPTABLE for Ultrassembler. Therefore, we instead use a helper class that the allocators call to. As a consequence, it allows us to add memory pool functionality controlled independently of the containers through calls to the helper GlobalMemoryBank class in the future.
template <class T>
class MemoryBank {
public:
    using value_type = T;

    MemoryBank() = default;

    [[nodiscard]] T* allocate(size_t requested) {
        std::size_t bytes = requested * sizeof(T);
        return reinterpret_cast<T*>(memory_bank.grab_some_memory(bytes));
    }

    void deallocate(T* ptr, size_t requested) { return; }

    bool operator==(const MemoryBank&) const { return true; }
};

This is the actual custom allocator object that we pass to C++ containers. The definition of a custom allocator in C++ is simply a class that provides the allocate and deallocate functions publicly. That's literally it. There are in fact more potential functions that you could add to handle specific uses, but allocate and deallocate are all we need for Ultrassembler. We define this class as a template because the return value of the allocate function must match the underlying type of the container using the allocator class. We furthermore define the == operator because C++ requires that two objects using allocators match their allocators. You'll normally never notice this because the default allocator for all C++ containers, std::allocator, provides all the allocator functions and operator comparison functions, and as a result, handles all comparisons transparently. Ultrassembler only uses equality. Finally, we provide a default constructor MemoryBank() = default; as this is what the C++ standard expects too from allocator classes.
void* GlobalMemoryBank::grab_some_memory(size_t requested) {
    if (requested + used > pool.size()) {
        throw UASError(OutOfMemory, "Out of memory!");
    }
    void* ptr = reinterpret_cast<void*>(pool.data() + used);
    used += requested;
    return ptr;
}

void GlobalMemoryBank::reset() {
    used = 0;
}

These functions implement allocating the memory and resetting the memory bank. Allocating should be obvious. However, resetting might not. As it stands, the memory pool simply gives up if it runs out of memory to allocate. We don't deallocate because such an operation would add extra overhead and subjects us to the issue of memory fragementation. Memory fragmentation is when you deallocate a small object from a large area of allocated memory, leaving a small area of unallocated memory laying in the otherwise allocated area. If you want to allocate a new object, tough luck, you probably can't fit it in this small area. You need to wait for the other objects to deallocate first. This cycle continues until your memory usage looks like Swiss cheese and doesn't support allocating any more objects, leading to a system crash. Normally, the OS kernel handles this problem transparently. Linux for example uses a "buddy allocator" to help deal with it. Memory fragmentation is also less of an issue with huge swaths of memory on modern systems. Our memory pool unfortunately lacks those luxuries of large memory and processing power for buddy allocators. Therefore, we provide the reset function to start everything over if the software using Ultrassembler receives an OutOfMemory exception.
Our memory pool trick lets Ultrassembler enjoy optimal memory locality and predefined memory usage while also completely eliminating syscalls (almost) and memory leaks, notwithstanding occasional memory bank resets.
Value speculation
A while ago, I read this fascinating article on something called L1 value speculation. The basic idea is to free the branch predictor by giving it extra work to do guessing the next value in the linked list. If it's right (usually it is) then you get a free speedup.
Ultrassembler does something similar. Instead of a linked list, we iterate through an array checking for specific combinations of characters that define the end of a sequence to copy. 
auto ch = [&]() {
    return data[i];
};

volatile char preview;
while (i < data.size() && not_at_end(ch()) && !is_whitespace(ch())) {
    c.inst.push_back(ch());
    i++;
    preview = ch();
}

As built-in strings in C++ are super duper mega slow even with custom allocators, we spend a lot of time on c.inst.push_back(ch());. There's fortunately a workaround. If the CPU knows that we'll be accessing the next character in the target string, why not queue it up first? This is exactly what volatile char preview; and preview = ch(); accomplish. We already have an opportunity for speculation with the i++; and i < data.size();. Although I'm not 100% sure, my hypothesis on why preview provides a speedup is that the branch predictor can only handle i < data.size() and not additionally the character loading of ch(). Therefore, we should preemptively load ch() during c.inst.push_back(ch());. 
Eagle eyed readers will notice how there is an opportunity for memory overflow if we are at the end of a string and i++; then preview = ch(); loads a character past the string data. However, Ultrassembler accounts for this by preemptively adding an extra null character to the input string data earlier in the code, ensuring that such illegal memory accesses are impossible by definition. 
This optimization sped up parsing of the instruction names enough that the overall Ultrassembler performance increased by about 10%.
(Super) smart searches
Here's one weird trick I haven't seen anywhere else.
Imagine I provided you these words: apple, apricot, avocado, and banana. 
Now, what if I told you a mystery word I was looking for among the ones I provided was 7 letters long. You would immediately discard "apple" and "banana" because they're not 7 letters long. Now, I tell you that it starts with "a." You wouldn't discard any at this point because both "apricot" and "avocado" start with the letter a. Finally, I tell you that the second letter is "v." Immediately we know "avocado" is the mystery word because no other word remaining starts with "av."
This is the basic idea behind the instruction, register, CSR, and pseudoinstruction lookup systems in Ultrassembler. There's a rub, though. The code for these lookups looks something like this:
const uint16_t fast_instr_search(const ultrastring& inst) {
    const auto size = inst.size();

    if (size == 2) {
        if (inst[0] == 's') {
            if (inst[1] == 'd') return 44;
            if (inst[1] == 'w') return 17;
            if (inst[1] == 'b') return 15;
            if (inst[1] == 'h') return 16;
        }
        if (inst[0] == 'o') {
            if (inst[1] == 'r') return 35;
        }
        if (inst[0] == 'l') {
            if (inst[1] == 'd') return 43;
            if (inst[1] == 'w') return 12;
            if (inst[1] == 'b') return 10;
            if (inst[1] == 'h') return 11;
        }
    }

    if (size == 3) {
        etc...

Clearly, there's a lot of work to do if you've got thousands of entries like the instructions array does. There's a fix for that though! 
Enter codegen. 
Ultrassembler uses artisan-crafted Python scripts to traverse through the listings and extract the string names for each instruction, register, CSR, and pseudoinstruction. Then, these scripts generate C++ code which performs these precomputed lookups. 
Here's what the instruction search script looks like. WARNING! If this script looks ugly, it's because Python is one of the worst programming languages out there for anything more than mere supportive, throwaway software like this.
input = "src/instructions.cpp"
output = "src/generated/instruction_search.cpp"

import re

content = ""
with open(input, "r") as file:
    content = file.read()

regex = "(?<={)\"([\w.]+)\""

instructions = re.findall(regex, content)

for i in range(len(instructions)):
    instructions[i] = (instructions[i], i, len(instructions[i]))

instructions.sort()

print(instructions)

min_len = min([i[2] for i in instructions])

max_len = max([i[2] for i in instructions])

depth = 0

current_instr = ""

code = "// SPDX-License-Identifier: MPL-2.0\n"
code += "// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!\n"
code += "#include \"../instructions.hpp\"\n"
code += "#include \"../ultrassembler.hpp\"\n\n"
code += "namespace ultrassembler_internal {\n\n"
code += "const uint16_t fast_instr_search(const ultrastring& inst) {\n"
code += "    const auto size = inst.size();\n\n"

def ind():
    return "    " * (depth + 2)

def instr_exists(instr, length):
    for i in instructions:
        if i[0] == instr and i[2] == length:
            return True
    return False
    
def prefix_exists(prefix, length):
    for i in instructions:
        if i[0].startswith(prefix) and i[2] == length:
            return True
    return False

potentialchars = ""

for instr in instructions:
    for char in instr[0]:
        if char not in potentialchars:
            potentialchars += char

def process_depth(current_len):
    global code, current_instr, depth
    for letter in potentialchars:
        if instr_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') return {instructions[[i[0] for i in instructions].index(current_instr + letter)][1]};\n"
        elif prefix_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') {{\n"
            current_instr += letter
            depth += 1
            process_depth(current_len)
            depth -= 1
            current_instr = current_instr[:-1]
            code += ind() + "}\n"

for i in range(min_len, max_len + 1):
    code += f"    if (size == {i}) {{\n"
    process_depth(i)
    code += "    }\n\n"

code += "    return instr_search_failed;\n"
code += "}\n\n"
code += "} // namespace ultrassembler_internal"

print(code)

with open(output, "w") as file:
    file.write(code)

Let's go through it section by section.
input = "src/instructions.cpp"
output = "src/generated/instruction_search.cpp"

import re

content = ""
with open(input, "r") as file:
    content = file.read()

This simply tells the script what file to read and where to generate the code, imports the regex package, and reads the input file.
regex = "(?<={)\"([\w.]+)\""

instructions = re.findall(regex, content)

for i in range(len(instructions)):
    instructions[i] = (instructions[i], i, len(instructions[i]))

instructions.sort()

print(instructions)

This regex searches for all instances of quotes in the instruction C++ code. That code looks like this:
const std::array<rvinstruction, 2034> instructions = {
        {{"lui", LUI, U, op_LUI, 0b000, RVI, int_reg},
         {"auipc", AUIPC, U, op_AUIPC, 0b000, RVI, int_reg},
         {"jal", JAL, J, op_JAL, 0b000, RVI, int_reg}, etc...

Then, it creates a new array with the instruction name, what position it is in the array, and its length. This might seem redundant at first, but it's helpful later. We then sort all the insructions alphabetically (also important!) and show all of them for debugging/status purposes.
min_len = min([i[2] for i in instructions])

max_len = max([i[2] for i in instructions])

depth = 0

current_instr = ""

code = "// SPDX-License-Identifier: MPL-2.0\n"
code += "// The generate_instruction_search.py script automatically generated this code. DO NOT MODIFY!\n"
code += "#include \"../instructions.hpp\"\n"
code += "#include \"../ultrassembler.hpp\"\n\n"
code += "namespace ultrassembler_internal {\n\n"
code += "const uint16_t fast_instr_search(const ultrastring& inst) {\n"
code += "    const auto size = inst.size();\n\n"

def ind():
    return "    " * (depth + 2)

def instr_exists(instr, length):
    for i in instructions:
        if i[0] == instr and i[2] == length:
            return True
    return False
    
def prefix_exists(prefix, length):
    for i in instructions:
        if i[0].startswith(prefix) and i[2] == length:
            return True
    return False

potentialchars = ""

for instr in instructions:
    for char in instr[0]:
        if char not in potentialchars:
            potentialchars += char

This is a lot of boilerplate for the algorithm later to come. We find the shortest and longest instructions. We add the first parts of the generated file. We define an indentation helper for nice formatting. We define additional helper functions to check if a whole instruction exists with a given name and length or if there is an instruction with the provided prefix and length. Finally, we assemble an array with all the characters to search for that the instructions use to avoid unnecessary computation later.
def process_depth(current_len):
    global code, current_instr, depth
    for letter in potentialchars:
        if instr_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') return {instructions[[i[0] for i in instructions].index(current_instr + letter)][1]};\n"
        elif prefix_exists(current_instr + letter, current_len):
            code += ind() + f"if (inst[{depth}] == '{letter}') {{\n"
            current_instr += letter
            depth += 1
            process_depth(current_len)
            depth -= 1
            current_instr = current_instr[:-1]
            code += ind() + "}\n"

for i in range(min_len, max_len + 1):
    code += f"    if (size == {i}) {{\n"
    process_depth(i)
    code += "    }\n\n"

Here's where the magic happens. We process one instruction length depth at a time. Like the algorithm we talked about at the beginning of this section, we start with the shortest possible "words" and work our way to the longest. Each depth step works through a search of all the possible characters and first checks if we have already found an instruction. If there is such an instruction, we add it to the code. Alternatively, if there is no such instruction but there is in fact an instruction that starts with the current sequence, we go down a depth level because we know that eventually, we will find an instruction with an exact match. Once we've gone through all of the possible instructions and depths, we exit the for loop.
code += "    return instr_search_failed;\n"
code += "}\n\n"
code += "} // namespace ultrassembler_internal"

print(code)

with open(output, "w") as file:
    file.write(code)

This completes the generated search function, shows it all for debugging/status purposes, and finally writes the generated code to the output file path.
There are no other instances of this kind of codegen that I know of. That's surprising, because codegen allows us to perform lookup of thousands of instructions with near-zero overhead. I estimate each instruction lookup takes on the order of 10 instructions to complete.
Here's what the resulting compiled assembly looks like on my x86 PC:
0000000000029340 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE>:
   29340:	f3 0f 1e fa          	endbr64 
   29344:	48 8b 47 08          	mov    0x8(%rdi),%rax
   29348:	48 83 f8 02          	cmp    $0x2,%rax
   2934c:	0f 84 c6 00 00 00    	je     29418 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0xd8>
   29352:	48 83 f8 03          	cmp    $0x3,%rax
   29356:	75 28                	jne    29380 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x40>
   29358:	48 8b 17             	mov    (%rdi),%rdx
   2935b:	0f b6 0a             	movzbl (%rdx),%ecx
   2935e:	80 f9 61             	cmp    $0x61,%cl
   29361:	0f 84 79 2b 00 00    	je     2bee0 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2ba0>
   29367:	80 f9 64             	cmp    $0x64,%cl
   2936a:	0f 85 58 10 00 00    	jne    2a3c8 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x1088>
   29370:	80 7a 01 69          	cmpb   $0x69,0x1(%rdx)
   29374:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
   29379:	0f 84 09 2f 00 00    	je     2c288 <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x2f48>
   2937f:	c3                   	ret
   # There are thousands more lines of this!

And RISC-V:
000000000007c33c <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE>:
   7c33c:	7179                	addi	sp,sp,-48
   7c33e:	f406                	sd	ra,40(sp)
   7c340:	e42a                	sd	a0,8(sp)
   7c342:	6522                	ld	a0,8(sp)
   7c344:	00089317          	auipc	t1,0x89
   7c348:	afc33303          	ld	t1,-1284(t1) # 104e40 <_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEE4sizeEv@@Base+0xad9c4>
   7c34c:	9302                	jalr	t1
   7c34e:	ec2a                	sd	a0,24(sp)
   7c350:	6762                	ld	a4,24(sp)
   7c352:	4789                	li	a5,2
   7c354:	22f71c63          	bne	a4,a5,7c58c <_ZN22ultrassembler_internal17fast_instr_searchERKNSt7__cxx1112basic_stringIcSt11char_traitsIcENS_10MemoryBankIcEEEE+0x250>
   7c358:	4581                	li	a1,0
   7c35a:	6522                	ld	a0,8(sp)
   7c35c:	00089317          	auipc	t1,0x89
   7c360:	c6433303          	ld	t1,-924(t1) # 104fc0 <_ZNKSt7__cxx1112basic_stringIcSt11char_traitsIcEN22ultrassembler_internal10MemoryBankIcEEEixEm@@Base+0xaaef8>
   7c364:	9302                	jalr	t1
   # Also thousands more lines of this!

Compile-time templates
This is similar to script codegen but with native C++ only.
One of the verification steps in Ultrassembler involves checking that the immediate value of an instruction (for example, addi t0, t1, 100) fits within some known range. C++ allows us to both cleanly invoke this check for an arbitrary range and do so with little to no runtime overhead to calculate that range.
Here's how it works.
template <auto bits>
void verify_imm(const auto& imm) {
    using T = decltype(bits);
    if constexpr (std::is_signed_v<T>) {
        if (imm < -(1 << (bits - 1)) || imm >= (1 << (bits - 1))) {
            throw UASError(ImmOutOfRange, "Immediate " + to_ultrastring(imm) + " is out of range [" + to_ultrastring(-(1 << (bits - 1))) + ", " + to_ultrastring((1 << (bits - 1))) + ")", 0, 0);
        }
    } else if constexpr (std::is_unsigned_v<T>) {
        if (imm < 0 || imm >= (1u << bits)) {
            throw UASError(ImmOutOfRange, "Immediate " + to_ultrastring(imm) + " is out of range [0, " + to_ultrastring((1u << bits)) + ")", 0, 0);
        }
    }
}

Each invocation looks something like verify_imm<5u>(imm). We supply a numeric literal and the immediate variable to check. C++'s template facilities then check whether we've supplied a signed or unsigned numeric literal, as RISC-V instruction can vary whether they expect signed or unsigned numbers only. We then calculate the lowest possible number (-(1 << (bits - 1)) for signed and 0 for unsigned) and the highest possible number ((1 << (bits - 1)) for signed and (1u << bits) for unsigned) and check the input against that. We then throw an error if it doesn't fit these calculated constraints or return silently if it does.
The if constexpr tells the compiler to generate each signed or unsigned execution path at compile time depending on what numeric literal we've provided, allowing us to make each function call as pretty and fast as possible.
Fast string comparisons
For the times where we can't or don't want to use a precomputed string search, Ultrassembler uses an optimized string comparison function to minimize overhead.
bool fast_eq(const auto& first, const std::string_view& second) {
    if (first.size() != second.size()) { 
        return false;
    }
    for (size_t i = 0; i < first.size(); i++) {
        if (first[i] != second[i]) {
            [[likely]] return false;
        } else {
            [[unlikely]] continue;
        }
    }
    return true;
}

How does this work? First, we check to make sure the input strings are the same length. It's impossible by definition for them to be the same if they have different lengths. Then, we compare them character by character. Here, we use C++20's [[likely]] and [[unlikely]] tags to help the compiler optimize the positioning of each comparison. It's statistically more likely to have a comparison failure than a success because we are usually comparing one input string against many possible options but it can only match with up to one.
Reference bigger-than-fundamental objects in function arguments
This one surprised me.
When you call a C++ function, you can choose to pass your arguments by value, or by reference. By default, C++ uses by value, which means the code internally makes a copy of the argument and provides that copy to the function. If you add a & to make it a reference instead (there are other ways to do this too) then the code generates a pointer to that original object and passes that pointer to the function. However, unlike pointers, references handle referencing and dereferencing transparently. As an aside, this also means Ultrassembler technically doesn't use pointers... anywhere! Pointers are horrible.
One of the most common pieces of C++ optimization advice is to use references whenever possible to avoid the copy overhead incurred by value references. It might surprise you, then, to find out that the following code is vastly faster due to the use of a value argument:
size_t parse_this_line(size_t i, const ultrastring& data, assembly_context& c) {
    // code that does "i++;" a lot
}

// later, in a different function:
for (size_t i = 0; i < data.size();) {
    i = parse_this_line(i, data, c);
    // etc...
}

If we had applied the Programming Furus¬©Ô∏è¬ÆÔ∏è‚Ñ¢Ô∏è's advice to pass i by reference, it would have looked like:
void parse_this_line(size_t& i, const ultrastring& data, assembly_context& c) {
    // code that does "i++;" a lot
}

// later, in a different function:
for (size_t i = 0; i < data.size();) {
    parse_this_line(i, data, c);
    // etc...
}

So why is the first one faster? Here's why.
Under the hood of all programming languages, you have assembly code which translates to the CPU's machine code. There are also no variables. Instead, you've got registers which hold raw data and raw memory. In most application processors today, the registers are 64 bits wide, and maybe wider for special vector operations which don't matter here. 64 bits happens to match the maximum width of so-called fundamental types in C and C++ which are integers and most common floats. Therefore, we can fit at least one fundamental type into each register.
Quick refresher of the registers in RISC-V:

Assembly also has little concept of a function call. Internally, all function calls do is clear out the current registers, load them with the function parameters, then jump to the function's address. This means all function calls involve at least one copy per argument, whether it's a fundamental type or a pointer to a fundamental type or a pointer to something else.
# Here's what this looks like in RISC-V assembly.
# Say we have a number in register t0, like 69.

addi t0, x0, 69

# We also have a function foobar that takes a single integer argument (like "void foobar(size_t arg)" in C/C++)
# We can copy that register (and therefore its value) to argument register a0 before calling foobar

addi a0, t0, 0

jal foobar

# The copying of this value only took one step!

You can see where we're going. If our goal is to minimize copying, it would be better to copy a fundamental type once than to generate a pointer, copy that, then dereference that pointer to get the underlying value. That is the crux of this subtle optimization trick. The cost to copy one register is less than the cost to copy a register holding a pointer. 
Note how I've only talked about fundamental types. Any type which does not fit in a single register, AKA many structs, containers, or anything else that isn't a fundamental type, costs more to copy by value in multiple registers than it does to copy a single register holding a pointer. I don't know of any Programming Furu¬©Ô∏è¬ÆÔ∏è‚Ñ¢Ô∏è that makes this distinction clear.
Don't do insertions or deletions
One of the steps to assemble a jump operation in RISC-V assembly is to calculate the offset of bytes to the jump target. However, this is often impossible unless all other instructions are already assembled. Ultrassembler does its best to avoid insertions or deletions through a clever trick to assemble jump instructions with a placeholder jump offset and then insert the correct offset in-place at the end.
Here's how it works:
void solve_label_offsets(assembly_context& c) {
    using enum RVInstructionFormat;
    for (size_t i = 0; i < c.label_locs.size(); i++) {
        if (!c.label_locs.at(i).is_dest) {
            for (size_t j = 0; j < c.label_locs.size(); j++) {
                if (c.label_locs.at(j).is_dest && c.label_locs.at(j).id == c.label_locs.at(i).id) {
                    uint32_t inst = 0;

                    if (c.label_locs.at(i).i_bytes == 2) {
                        inst = reinterpret_cast<uint16_t&>(c.machine_code.at(c.label_locs.at(i).loc));
                    } else if (c.label_locs.at(i).i_bytes == 4) {
                        inst = reinterpret_cast<uint32_t&>(c.machine_code.at(c.label_locs.at(i).loc));
                    }

                    int32_t offset = c.label_locs.at(j).loc - c.label_locs.at(i).loc;

                    if (c.label_locs.at(i).format == Branch) {
                        inst &= 0b00000001111111111111000001111111;
                        inst |= ((offset >> 11) & 0b1) << 7;      // Add imm[11]
                        inst |= ((offset >> 1) & 0b1111) << 8;    // Add imm[4:1]
                        inst |= ((offset >> 5) & 0b111111) << 25; // Add imm[10:5]
                        inst |= ((offset >> 12) & 0b1) << 31;     // Add imm[12]
                    } else if (c.label_locs.at(i).format == J) {
                        inst &= 0b00000000000000000000111111111111;
                        inst |= ((offset >> 12) & 0b11111111) << 12;  // Add imm[19:12]
                        inst |= ((offset >> 11) & 0b1) << 20;         // Add imm[11]
                        inst |= ((offset >> 1) & 0b1111111111) << 21; // Add imm[10:1]
                        inst |= ((offset >> 20) & 0b1) << 31;         // Add imm[20]
                    } else if (c.label_locs.at(i).format == CJ) {
                        inst &= 0b1110000000000011;
                        inst |= ((offset >> 5) & 0b1) << 2;   // Add offset[5]
                        inst |= ((offset >> 1) & 0b111) << 3; // Add offset[3:1]
                        inst |= ((offset >> 7) & 0b1) << 6;   // Add offset[7]
                        inst |= ((offset >> 6) & 0b1) << 7;   // Add offset[6]
                        inst |= ((offset >> 10) & 0b1) << 8;  // Add offset[10]
                        inst |= ((offset >> 8) & 0b11) << 9;  // Add offset[9:8]
                        inst |= ((offset >> 4) & 0b1) << 11;  // Add offset[4]
                        inst |= ((offset >> 11) & 0b1) << 12; // Add offset[11]
                    } else if (c.label_locs.at(i).format == CB) {
                        inst &= 0b1110001110000011;
                        inst |= ((offset >> 5) & 0b1) << 2;   // Add offset[5]
                        inst |= ((offset >> 1) & 0b11) << 3;  // Add offset[2:1]
                        inst |= ((offset >> 6) & 0b11) << 5;  // Add offset[7:6]
                        inst |= ((offset >> 3) & 0b11) << 10; // Add offset[4:3]
                        inst |= ((offset >> 8) & 0b1) << 12;  // Add offset[8]
                    }

                    if (c.label_locs.at(i).i_bytes == 2) {
                        reinterpret_cast<uint16_t&>(c.machine_code.data()[c.label_locs.at(i).loc]) = inst;
                    } else if (c.label_locs.at(i).i_bytes == 4) {
                        reinterpret_cast<uint32_t&>(c.machine_code.data()[c.label_locs.at(i).loc]) = inst;
                    }
                }
            }
        }
    }
}

When we find a jump instruction that needs later TLC, we save its location and some other attributes to a special array. Then, after the rest of the code is done assembling, we go back through each jump instruction and calculate the correct offset and insert that offset in-place in the correct instruction format.
I believe this is faster than what some other assemblers do for instructions which jump to a location reachable within the constraints of the offset's size. However, it's not useful for far jumps, which require a separate helper instruction to extend the jump. Ultrassembler doesn't support those yet.
More optimizations
Here's a few more optimization tricks that aren't quite significant enough for their own sections but deserve a mention anyway.
Memory padding
There are a few strings which Ultrassembler frequently reads and writes. To insure against runtime memory pool allocation overhead, we preemptively allocate a good amount of memory.
c.inst.reserve(32);
c.arg1.reserve(32);
c.arg2.reserve(32);
c.arg3.reserve(32);
c.arg4.reserve(32);
c.arg5.reserve(32);
c.arg6.reserve(32);
c.arg_extra.reserve(32);
c.machine_code.reserve(128000);

I found that 32 bytes gave the biggest speedup for small strings, and sizes above a few kB are more appropriate for the machine code output.
Inline some functions
Sometimes, functions are faster when you mark them inline to suggest that the code have a copy for each invocation. This tends to work better for smaller functions.
inline const uint8_t decode_encoding_length(const uint8_t opcode) {
    if ((opcode & 0b11) != 0b11) {
        return 2;
    } else {
        return 4;
    }
}

Try it and see what works best for your own code.
Minimize string stripping copies
Here's a special case of minimizing string copying. This function removes the parentheses and optionally the number 0 from a string like "0(t4)":
void remove_extraneous_parentheses(ultrastring& str) {
    if (str.back() == ')') {
        str.pop_back();
    }
    if (str.front() == '0') {
        str.erase(0, 1);
    }
    if (str.front() == '(') {
        str.erase(0, 1);
    }
}

Why do we tackle the last character first? When you erase one or more characters from a string, C++ internally copies every individual character after setting the characters to erase to blank. In other words, it looks a little like this:
# Erase "foo" from "foobar"

foobar

 oobar

  obar

   bar

b  bar

ba bar

barbar

barba

barb

bar

That's a lot of copies. So it would be great if we can avoid copying more of these characters in the future. Then, we handle the case where the input string is like "(t4)" where there is no 0 at the beginning. Finally is the removal of the front parenthesis. 
This optimization yielded a surprising speedup (several percent overall) due to how often the case of "0(reg)" shows up in RISC-V assembly.
Call small lambda functions frequently
These three lambda functions both help make parsing faster and simplify the code:
auto is_whitespace = [](const char& c) {
    return c == '\t' || c == ' ';
};
auto ch = [&]() {
    return data[i];
};
auto not_at_end = [](const char& c) {
    return c != '\n' && c != '#';
};

Why do they work? The simplification part is obvious, but maybe not for speed. One reason might be because the compiler now knows how often we do the same comparisons over and over. If it knows we do the same thing many times, it can optimize with that known fact.
Also note how the first and last functions violate the earlier optimization trick regarding passing fundamental types by value. That trick does not entirely apply to lambda functions, which work differently, where they could be inline and incur zero function call overhead. Passing by reference enables the zero function call overhead optimization.
Strip out the compilation junk
By default, C++ compilers like GCC and Clang add in a lot of junk that we can safely strip out. Here's how we do it in CMake:
target_compile_options(objultra PRIVATE -fno-rtti -fno-stack-protector -fomit-frame-pointer)

-fno-rtti
RTTI is runtime type identification. Only some software uses this feature but it adds nonzero overhead to all. Therefore, we disable it to eliminate that overhead.
-fno-stack-protector
The stack protector is a feature that many Programming Furus¬©Ô∏è¬ÆÔ∏è‚Ñ¢Ô∏è peddle to improve security. However, it adds considerable overhead, and does nothing for security outside of a specific attack. Therefore, we disable it to eliminate that overhead.
-fomit-frame-pointer
The frame pointer is a specific feature on some CPU platforms (like x86). However, it's not actually needed anymore for modern CPUs, and it adds overhead. Therefore, we disable it to eliminate that overhead.
Link-time optimization
Link-time optimization, or LTO, is a more intelligent way for the compiler to optimize your code than regular optimization passes. It can enable some serious speedups if your code benefits from function inlining or has code across many files. It's been supported for a while now but isn't enabled by default. Here's how to enable it in CMake:
include(CheckIPOSupported)
check_ipo_supported(RESULT lto_supported)
if(lto_supported AND NOT NO_LTO)
  set_property(TARGET ${this_target} PROPERTY INTERPROCEDURAL_OPTIMIZATION TRUE)
  if(CMAKE_COMPILER_IS_GNUCXX)
    list(APPEND CMAKE_CXX_COMPILE_OPTIONS_IPO "-flto=auto") # set the thread amount to what is available on the CPU
  endif()
endif()

This has been nothing but a benefit for Ultrassembler.
Make structs memory-friendly
This struct holds variables that most of the Ultrassembler code uses:
struct assembly_context {
    ultrastring inst;
    ultrastring arg1;
    ultrastring arg2;
    ultrastring arg3;
    ultrastring arg4;
    ultrastring arg5;
    ultrastring arg6;
    ultrastring arg_extra;
    ultravector<uint8_t> machine_code;
    ultravector<RVInstructionSet> supported_sets;
    ultravector<std::pair<ultrastring, int>> labels;
    ultravector<label_loc> label_locs;
    ultravector<std::pair<ultrastring, ultrastring>> constants;
    ultravector<directive_options> options;
    int32_t custom_inst = 0;
    uint32_t line = 1;
    uint32_t column = 0;
    uint16_t inst_offset = 0;
};

We order them in descending memory size, from 32 bytes for ultrastring to 2 for uint16_t. This packs the members the most efficient way possible for memory usage.
Also, these variables are not in the global scope or a namespace because holding them all in a struct enables multithreaded operation. It would be possible to add thread_local to each one to enable multithreading easily, but in testing, this added enormous overhead compared to a plain old struct.
Memory locality
Memory locality is the general idea that the most frequently accessed memory should be close together. Ultrassembler has many such cases, and we already help ensure memory locality through preallocated memory pools. We go further by ensuring sections of code which frequently work on one area of memory get their own space to work with.
Here's an example:
void make_inst(assembly_context& c) {
    // boilerplate

    uint32_t inst = 0;

    // code which modifies this inst variable

    reinterpret_cast<uint32_t&>(c.machine_code[c.machine_code.size() - bytes]) = inst;
}

We work on the local inst variable to prevent far reaches across memory to the c.machine_code vector. When we're done, we write to c.machine_code once and invoke only one far memory access as a result.
Conclusion
Congrats if you read all the way here!
Hopefully you've learned something new and/or useful. Although I've crafted the optimizations here for Ultrassembler, there's nothing stopping you from applying the same underlying principles to your own code. 
Check out Ultrassembler: https://github.com/Slackadays/Chata/tree/main/ultrassembler



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Use One Big Server (2022)]]></title>
            <link>https://specbranch.com/posts/one-big-server/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45085029</guid>
            <description><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system ‚Ä¶]]></description>
            <content:encoded><![CDATA[A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind
this debate is about whether distributed system architecture is worth the developer time and
cost overheads.  By thinking about the real operational considerations of our systems, we can
get some insight into whether we actually need distributed systems for most things.
We have all gotten so familiar with virtualization and abstractions between our software
and the servers that run it.  These days, "serverless" computing is all the rage, and even
"bare metal" is a class of virtual machine.  However, every piece of software runs on a
server.  Since we now live in a world of virtualization, most of these servers are a lot
bigger and a lot cheaper than we actually think.
Meet Your Server



This is a picture of a server used by Microsoft Azure with AMD CPUs.  Starting from the left,
the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes
that the copper tubes are attached to are heat exchangers on each CPU.  The CPUs are AMD's
third generation server CPU, each of which has the following specifications:

64 cores
128 threads
~2-2.5 GHz clock
Cores capable of 4-6 instructions per clock cycle
256 MB of L3 cache

In total, this server has 128 cores with 256 simultaneous threads.  With all of the cores working
together, this server is capable of 4 TFLOPs of peak double precision computing performance. This
server would sit at the top of the top500 supercomputer list in early 2000. It would take until
2007 for this server to leave the top500 list.  Each CPU core is substantially more powerful than a
single core from 10 years ago, and boasts a much wider computation pipeline.
Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket.  The largest
capacity "cost effective" DIMMs today are 64 GB.  Populated cost-efficiently, this server can hold
1 TB of memory.  Populated with specialized high-capacity DIMMs (which are generally slower
than the smaller DIMMs), this server supports up to 8 TB of memory total.  At DDR4-3200, with
a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across
all of its cores.
In terms of I/O, each CPU offers 64 PCIe gen 4 lanes.  With 128 PCIe lanes total, this server is
capable of supporting 30 NVMe SSDs plus a network card.  Typical configurations you can buy will
offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is
in the top right, the network card.  This server is likely equipped with a 50-100 Gbps network
connection.
The Capabilities of One Server
One server today is capable of:

Serving video files at 400 Gbps (now 800 Gbps)
1 million IOPS on a NoSQL database
70k IOPS in PostgreSQL
500k requests per second to nginx
Compiling the linux kernel in 20 seconds
Rendering 4k video with x264 at 75 FPS

Among other things.  There are a lot of public benchmarks these days, and if you know how your
service behaves, you can probably find a similar benchmark.
The Cost of One Server
In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications
to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth
for $1,318/month.
Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores
and 128 GB of RAM for about ‚Ç¨140.00/month.  This is a smaller server than the one from OVHCloud
(1/4 the size), but it gives you some idea of the price spread between hosting providers.
In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps
of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour
in the US East region.  This comes out to $6,055/month.  The cloud premium is real!
A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs,
SSDs, and support contracts), can be purchased from the Dell website for about $40,000.  However,
if you are going to spend this much on a server, you should probably chat with a salesperson to
make sure you are getting the best deal you can.  You will also need to pay to host this server
and connect it to a network, though.
In comparison, buying servers takes about 8 months to break even compared to using cloud servers,
and 30 months to break even compared to renting.  Of course, buying servers has a lot of drawbacks,
and so does renting, so going forward, we will think a little bit about the "cloud premium" and
whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the
cloud companies want you to pay").
Thinking about the Cloud
The "cloud era" began in earnest around 2010.  At the time, the state of the art CPU was an
8-core Intel Nehalem CPU.  Hyperthreading had just begun, so that 8-core CPU offered a
whopping 16 threads.  Hardware acceleration was about to arrive for AES encryption, and
vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a
whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to
offer a 3 TB hard drive.  Each core offered 4 FLOPs per cycle, meaning that your 8-core
server running at 2.5 GHz offered a blazing fast 80 GFLOPs.
The boom in distributed computing rode on this wave: if you wanted to do anything that
involved retrieval of data, you needed a lot of disks to get the storage throughput you want.
If you wanted to do large computations, you generally needed a lot of CPUs. This meant that
you needed to coordinate between a lot of CPUs to get most things done.
Since that time began, the size of servers has increased a lot, and SSDs have increased available
IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased
much, and we still use virtualized drives that perform more like hard drives than SSDs (although
this gap is closing).
One Server (Plus a Backup) is Usually Plenty
If you are doing anything short of video streaming, and you have under 10k QPS, one server
will generally be fine for most web services.  For really simple services, one server could
even make it to a million QPS or so.  Very few web services get this much traffic - if you
have one, you know about it.  Even if you're serving video, running only one server for your
control plane is very reasonable.  A benchmark can help you determine where you are.
Alternatively, you can use common benchmarks of similar applications, or
tables of common performance numbers to estimate how big of a
machine you might need.
Tall is Better than Wide
When you need a cluster of computers, if one server is not enough, using fewer larger servers
will often be better than using a large fleet of small machines.  There is non-zero overhead
to coordinate a cluster, and that overhead is frequently O(n) on each server.  To reduce this
overhead, you should generally prefer to use a few large servers than to use many small servers.
In the case of things like serverless computing, where you allocate tiny short-lived containers,
this overhead accounts for a large fraction of the cost of use.  On the other extreme end,
coordinating a cluster of one computer is trivial.
Big Servers and Availability
The big drawback of using a single big server is availability.  Your server is going to need
downtime, and it is going to break.  Running a primary and a backup server is usually enough,
keeping them in different datacenters.  A 2x2 configuration should appease the truly paranoid: two
servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will
give you a lot of redundancy.  If you want a third backup deployment, you can often make that
smaller than your primary and secondary.
However, you may still have to be concerned about correlated hardware failures.  Hard drives
(and now SSDs) have been known to occasionally have correlated failures: if you see one disk
fail, you are a lot more likely to see a second failure before getting back up if your disks
are from the same manufacturing batch.  Services like Backblaze overcome this by using many
different models of disks from multiple manufacturers.  Hacker news learned this the hard way
recently when the primary and backup server went down at the same time.
If you are using a hosting provider which rents pre-built servers, it is prudent to rent two
different types of servers in each of your primary and backup datacenters.  This should avoid
almost every failure mode present in modern systems.
Use the Cloud, but don't be too Cloudy
A combination of availability and ease of use is one of the big reasons why I (and most other
engineers) like cloud computers.  Yes, you pay a significant premium to rent the machines, but
your cloud provider has so much experience building servers that you don't even see most failures,
and for the other failures, you can get back up and running really quickly by renting a new
machine in their nearly-limitless pool of compute.  It is their job to make sure that you don't
experience downtime, and while they don't always do it perfectly, they are pretty good at it.
Hosting providers who are willing to rent you a server are a cheaper alternative to cloud
providers, but these providers can sometimes have poor quality and some of them don't understand
things like network provisioning and correlated hardware failures. Also, moving from one rented
server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a
price premium for a good reason.
However, when you deal with clouds, your salespeople will generally push you towards
"cloud-native" architecture.  These are things like microservices in auto-scaling VM groups with
legions of load balancers between them, and vendor-lock-in-enhancing products like serverless
computing and managed high-availability databases.  There is a good reason that cloud
salespeople are the ones pushing "cloud architecture" - it's better for them!
The conventional wisdom is that using cloud architecture is good because it lets you scale up
effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people
is not one of them: most services can serve millions of people at a time with one server, and
will never give you a surprise five-figure bill.
Why Should I Pay for Peak Load?
One common criticism of the "one big server" approach is that you now have to pay for your peak
usage instead of paying as you go for what you use.  Thus, serverless computing or fleets of
microservice VMs more closely align your costs with your profit.
Unfortunately, since all of your services run on servers (whether you like it or not), someone
in that supply chain is charging you based on their peak load.  Part of the "cloud premium" for
load balancers, serverless computing, and small VMs is based on how much extra capacity your
cloud provider needs to build in order to handle their peak load.  You're paying for someone's
peak load anyway!
This means that if your workload is exceptionally bursty - like a simulation that needs
to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if
your workload is not so bursty, you will often have a cheaper system (and an easier time building
it) if you go for few large servers.  If your cloud provider's usage is more bursty than yours,
you are going to pay that premium for no benefit.
This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM
24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with
a salesperson if you are big enough.
Generally, the burstier your workload is, the more cloudy your architecture should be.
How Much Does it Cost to be Cloudy?
Being cloudy is expensive.  Generally, I would anticipate a 5-30x price premium depending on what
you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and
30.
Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM.  I
am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above.
Large ARM servers and serverless ARM compute are both cheaper.
Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:


1k QPS is 60k queries per minute, or 3.6M queries per hour


Each query here gets 0.768 GB-seconds of RAM (amortized)


Replacing this server would cost about $46/hour using serverless computing


The price premium for serverless computing over the instance is a factor of 5.5.  If you can keep
that server over 20% utilization, using the server will be cheaper than using serverless computing.
This is before any form of savings plan you can apply to that server - if you can rent those big
servers from the spot market or if you compare to the price you can get with a 1-year contract,
the price premium is even higher.
If you compare to the OVHCloud rental price for the same server, the price premium of buying your
compute through AWS lambda is a factor of 25
If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you
should prefer the hosting provider if you can keep the server operating at 5% capacity!
Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k
QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x
(or 25x) premium. Of course, you should scale the size of the server to fit your application.
Common Objections to One Big Server
If you propose using the one big server approach, you will often get pushback from people who are
more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns.  Use your
judgment when you think about it, but most people vastly underestimate how much "cloud
architecture" actually costs compared to the underlying compute.  Here are some common objections.
But if I use Cloud Architecture, I Don't Have to Hire Sysadmins
Yes you do.  They are just now called "Cloud Ops" and are under a different manager. Also, their
ability to read the arcane documentation that comes from cloud companies and keep up  with the
corresponding torrents of updates and deprecations makes them 5x more expensive than system
administrators.
But if I use Cloud Architecture, I Don't Have to Do Security Updates
Yes you do.  You may have to do fewer of them, but the ones you don't have to do are the easy ones
to automate.  You are still going to share in the pain of auditing libraries you use, and making
sure that all of your configurations are secure.
But if I use Cloud Architecture, I Don't Have to Worry About it Going Down
The "high availability" architectures you get from using cloudy constructs and microservices just
about make up for the fragility they add due to complexity.  At this point, if you use two
different cloud regions or two cloud providers, you can generally assume that is good enough to
avoid your service going down.  However, cloud providers have often had global outages in the past,
and there is no reason to assume that cloud datacenters will be down any less often than your
individual servers.
Remember that we are trying to prevent correlated failures.  Cloud datacenters have a lot of
parts that can fail in correlated ways.  Hosting providers have many fewer of these parts.
Similarly, complex cloud services, like managed databases, have more failure modes than simple
ones (VMs).
But I can Develop More Quickly if I use Cloud Architecture
Then do it, and just keep an eye on the bill and think about when it's worth it to switch.  This
is probably the strongest argument in favor of using cloudy constructs.  However, if you don't
think about it as you grow, you will likely end up burning a lot of money on your cloudy
architecture long past the time to switch to something more boring.
My Workload is Really Bursty
Cloud away.  That is a great reason to use things like serverless computing.  One of the big
benefits of cloud architecture constructs is that the scale down really well.  If your workload
goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud
architecture probably works really well for you.
What about CDNs?
It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings,
with one big server.  This is also true of other systems that need to be distributed, like backups.
Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of
thing to buy rather than build.
A Note On Microservices and Monoliths
Thinking about "one big server" naturally lines up with thinking about monolithic architectures.
However, you don't need to use a monolith to use one server.  You can run many containers on one
big server, with one microservice per container.  However, microservice architectures in general
add a lot of overhead to a system for dubious gain when you are running on one big server.
Conclusions
When you experience growing pains, and get close to the limits of your current servers, today's
conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture
that gives you horizontal scaling "for free."  It is often easier and more efficient to scale
vertically instead.  Using one big server is comparatively cheap, keeps your overheads at a
minimum, and actually has a pretty good availability story if you are careful to prevent correlated
hardware failures.  It's not glamorous and it won't help your resume, but one big server will serve
you well.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[When the sun will literally set on what's left of the British Empire]]></title>
            <link>https://oikofuge.com/sun-sets-on-british-empire/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084913</guid>
            <description><![CDATA[A while ago I treated you to a dissertation entitled ‚ÄúDoes The Sun Set On The British Empire?‚Äù, and concluded that it doesn‚Äôt. The UK‚Äôs widely scattered overseas territories, sparse tho‚Ä¶]]></description>
            <content:encoded><![CDATA[
		
Click to enlarge


A while ago I treated you to a dissertation entitled ‚ÄúDoes The Sun Set On The British Empire?‚Äù, and concluded that it doesn‚Äôt. The UK‚Äôs widely scattered overseas territories, sparse though they are, mean that the sun is still always shining, somewhere in the world, over British territory.



The most important territories in maintaining this late-empire sunlight are the Pitcairn Islands, in the Pacific, and the British Indian Ocean Territory, in the Indian Ocean. To illustrate that, I offered the sunlight chart below, showing how Pitcairn and BIOT catch the sunlight when it‚Äôs dark in the UK.



Click to enlarge


In fact, as my map at the head of this post shows, BIOT is pivotal. There, I‚Äôve plotted the distribution of light and darkness, across the globe, at 02:15 Greenwich Mean Time, during the June solstice of 2024.*



And here‚Äôs the situation at the December solstice:



Click to enlarge


Just after the sun sets in Pitcairn, it‚Äôs dark over every British territory except BIOT.



I‚Äôm revisiting the situation because the UK government has announced plans to hand over sovereignty of the Chagos Archipelago, which houses BIOT, to Mauritius. The announcement was made in October 2024, but the original agreement has now been contested by a new government in Mauritius. And the situation is further complicated by the fact that BIOT houses a large US military base on the island of Diego Garcia, so the new Trump administration also has a say in the process. (Meanwhile, the unfortunate Chagossians, evicted from their homeland in 1968 to make way for the military base, have so far been given no voice in the negotiations.)



The current proposal suggests that the military base would be maintained under a long-term lease agreement, in which case British sovereignty would be lost, and BIOT would cease to exist. At that point, the role of easternmost British territory would fall to the Sovereign Base Areas (SBAs), in Cyprus.



The SBAs are worth a few paragraphs, both because they‚Äôre relatively obscure, and because their existence, as sovereign military territories, perhaps has some slight relevance to how the situation on Diego Garcia might play out, should the Trump administration raise strong objections to the current plan.



The SBAs came into existence when Cyprus gained its independence from the UK in 1960. Under the Treaty of Establishment, the UK retained sovereignty over about 250 square kilometres of the island, in two separate areas‚Äîthe Western Sovereign Base Area of Akrotiri, and the Eastern Sovereign Base Area of Dhekelia. These have extremely complicated boundaries, designed to avoid Cypriot settlements while including British military establishments. The Eastern SBA contains three Cypriot enclaves‚Äîthe towns of Ormideia and Xylotymbou, and the area surrounding the Dhekelia power station (which is crossed by a British road). It also features a long northward extension along the road to the village of Ayios Nikolaos, which now houses a signals intelligence unit.



And the whole border situation became even more complicated after the Turkish invasion of Cyprus in 1974, which has left the island traversed by a UN buffer zone. British territory, including the Ayios Nikolaos road, forms part of the buffer zone. Elsewhere, the Turkish-controlled town of Kokkina has its very own buffer zone. Here‚Äôs an overview map, followed by some detail of the SBAs:







(Interestingly, the British military settlements within the SBAs are referred to as cantonments, a military term which, to me at least, has something of a colonial ring to it, given its association with British rule in India.)



The relevance, here, to the current situation of Diego Garcia, is because the UK government made plans to hand the SBAs back to Cyprus in 1974, but were persuaded to retain sovereignty by the USA, which valued access to signals intelligence in the Eastern Mediterranean, as well as a convenient location from which to fly, among other things, U2 spy planes. The difference, of course, is that the Cypriot government appears to have been compliant with that arrangement, whereas it seems unlikely, at time of writing, that the Mauritians would agree to such a deal.



We‚Äôll see how it goes. Meanwhile, I‚Äôve plotted another sunrise/sunset graph, showing how sunlight is handed off between the two key players in the absence of BIOT: 



Click to enlarge


(For my sunlight calculation, I‚Äôve plugged in the latitude and longitude of the easternmost part of the Eastern SBA‚ÄîAyios Nikolaos.)



It‚Äôs close‚Äîin June there‚Äôs less than an hour when it‚Äôs dark in both Pitcairn and the SBAs. But, if BIOT goes, when the sun sets on Pitcairn, it will also set on (what‚Äôs left of) the British Empire.







* I haven‚Äôt plotted British Antarctic Territory, because territorial claims in Antarctica are in abeyance under the Antarctic Treaty.





	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: VibeFlow (YC S25) ‚Äì Web app generator with visual, editable workflows]]></title>
            <link>https://news.ycombinator.com/item?id=45084759</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084759</guid>
            <description><![CDATA[I like this. Any chance you'll be bringing similar tactility (is that a word) to the frontend? Granular changes to components via prompts leaves a lot to be desired.]]></description>
            <content:encoded><![CDATA[
I like this. Any chance you'll be bringing similar tactility (is that a word) to the frontend? Granular changes to components via prompts leaves a lot to be desired.
In general to me it makes a lot of sense to lean much more into "templates" (I'm sure lovable etc already do it, because it's also a nice way to save money). And it's much easier to at least guarantee some basic security when it comes to auth, payments, db setup etc. Of course you can shoot yourself in the foot right after that.
Totally agree, security is a big point. It‚Äôs hard to trust LLMs on security, which is why we aim to make ‚Äòwhite box‚Äô backends
Quite like the positioning of "this is the backend to your lovable ui", probably how chef (the vibe coding tool from the makers of convex) should have positioned it. (and kind of do).
This looks great! Can I export my end code / app and host it elsewhere easily? Where else would easily be able to host it?
They say they use Convex for the backend, which means you could in principle run it on your own account or go through the hoops of self hosting convex infra
I want to like this and dig into it as someone who has recently used Lovable and Base44 (and been using Bubble for a while), but the YouTube ‚Äòdemo‚Äô video is really weak.The pace is too fast and you spend barely any time showing off your visual workflow feature, which according to your description is your differentiator.I would strongly recommend using some of your YC money to have a professional recreate that demo and show off what makes you unique. Even if it goes longer than two minutes - if I‚Äôm interested I‚Äôll keep watching.I‚Äôll still try it out because I‚Äôm a sucker for trying out new vibecoding tools, but you‚Äôre not doing yourself any favors with that video‚Ä¶
Thanks a lot for the feedback. The video was meant as a very spontaneous ‚Äòas it is‚Äô showcase, but we‚Äôll definitely make new demos that go deeper into the editor!
> recently used Lovable and Base44Are you happy with either product? I tried them earlier in the year, and it was also really slow to make changes. I felt like they got stuck after a bit, too.It's a neat concept, but I feel like they're expensive templates. I'd honestly prefer a template gallery with a smooth and fast editing UI.
Well I am/was building something that looks a lot like this, a shame I never applied to YC, wondering now if I should apply to other funds now so I can continue working on it, the prototype is ready so I have the main part figured out.
A question, perhaps, could you give some tips to pitch this specifically, just for incubators, based on your experience?
Well all incubators ask the same thing (and search for the same profile). Just blast it to every incubator you find. Take some time write a nice YC application. There are tools like acceleratorfiller.xyz to send them to multiple accelerators.
BTW (It's my company)
that what everyone always says. But i think pitcing to a incubator is actually a good way to focus your idea. Anyways what is 2 hours in the grand scheme of things
I've been playing around with vibeflow for a while, it's impressive how fast you can go from a prompt to a working full stack app. The visual workflow editor is a game changer.
I built a small url shortener and also experimented with a map‚Äëbased mood tracker. what stood out to me is how quickly I could go from a prompt to a working frontend + backend without boilerplate.For me, the most useful next nodes would be:
1) auth 
2) stripe
3) file upload
4) convex action nodes (for more complex workflows)
I think the evolution of vibe coding tool is definitely the editor. Having a black box with no way to maintain it is an absolute liability.That's why I think app generators must be a good editor before being able to generate anything. It seems you went this way with the cool node interface.I'm doing the same thing with https://luna-park.app, but for fullstack apps.
Will have to try this later, the YT video looks promising. Found tools similar to this promising to create early mockups or other pre-prototypes when developing products.
Glad to hear that. We want to make it as logical and white box as possible. Have you tried adding custom behavior after the first generation?
Congrats! Doesn't replit have an integrated database as well? Lovable has supabase, and I'm pretty sure Base44 as well, plus other agent integrations.
Thanks! Yes, Replit has KV store and managed Postgres, Lovable uses Supabase (requires manual setup). Base44 doesn't have a manual setup but has a black box backend.
In VibeFlow:
- no manual setup required
- low code backend editor n8n style - no black box anymore
- everything you do in the backend is code that you ownIt's not just about databases, think about all the users currently using n8n with Lovable separately, without even owning the full stack
I tried this but kept getting errors.
I asked it to build a TODO list that searches the internet to "augment" my todo list with advice
Great question! We chose Convex for multiple reasons:‚Äì We spin up isolated projects for each user. Convex handles this seamlessly with zero manual setup, while Supabase/Firebase have limitations and manual configuration needed
‚Äì We abstract backend logic as visual nodes, so Convex's modularity makes it logical to find the right granularity for workflow representation.
‚Äì Everything is reactive, so UIs and workflows stay in sync without bolting on listeners
‚Äì Everything is end-to-end TypeScript with transactions by default, so generated code is predictable and maintainable
congrats on the launch, lots of competition in this space. (leap.new, replit etc). Even convex has their own app-builder.
thank you! There‚Äôs definitely a lot happening in this space, our focus is on making backends secure, robust, and understandable rather than just black-box codegen
Don't get me wrong, I wish your start-up all the best, but this particular application seems so stereotypical by current standards. It's at least four buzzwords combined into one "idea". As someone who has never tried to apply, I wonder how difficult it was to get through Y Combinator's selection process.
I worry that almost all the 2025 startups I've seen are AI app builders. Where are the novel new applications? I get that codegen is currently one area where AI does well, but it also feels like we're struggling with other use cases.
I‚Äôve spent an enormous amount of time with practically every AI model out there, from coding to image-gen to video-genThe tech is still simply too hard to use effectively for the vast majority of lay people, especially for anything beyond a cool product demoSome of it is due to quality of the models, some of it due t quality of the toolingPrompt engineering is still a skill and that‚Äôs beyond what a casual user can figure out
My optimism says the good new stuff is coming slowly because people who care about their craft and taking things slowly aren‚Äôt in any rush to get to market.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Infisical (YC W23) Is Hiring Solutions Engineers to Scale the OSS Security Stack]]></title>
            <link>https://www.ycombinator.com/companies/infisical/jobs/yaEvock-solutions-engineer</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084757</guid>
            <description><![CDATA[About Infisical
Infisical (https://infisical.com) is the #1 open source secret management platform for developers. In other words, we help organizations manage API-keys, DB access tokens, certificates, and other credentials across all parts of their infra! In fact, we¬†process over 100M of such secrets per day.
Our customers range from some of the largest public enterprises to fastest-growing startups (e.g., companies like Hugging Face, Delivery Hero). Developers love us and our community is growing every day! Join us on a mission to make security easier for all developers ‚Äì¬†starting from secret management.
About this role
Infisical is looking for a customer success engineer to help grow Infisical‚Äôs customer base and ensure great product/onboarding experience. You will be working closely with our CEO and the rest of the engineering team on:
ensuring new customers are activated, happy, and have all of their questions answered;
expanding Infisical to new use cases within accounts (e.g., more integrations, feature announcements, cross-sell);
running proof-of-concept with leads, guiding customer through onboarding, or being able to recommend best practices for specific architectures and use cases;
becoming a trusted technical advisor to customer and business leaders;
talking to customers daily to find points of friction and potential improvement in any workflows, understanding customer's needs and provide feedback towards product roadmap;
improving documentation and public facing materials based on customer feedback;
managing/facilitating case studies and customer quotes/references;
hosting webinars and knowledge sharing sessions for customer's stakeholders;
understanding user journey and being able to guide UX/DX roadmap;
Overall, you‚Äôre going to be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.
About you
This job will require you to have the following pivotal skills:
technical experience in development or systems engineering (open source experience is a plus);
experience in a customer-facing role (technical/developer product is a plus);
understanding of the Infrastructure/DevSecOps space is a plus;
exceptional verbal, presentation, and written communication skills;
high energy and strong work ethic;
How you will grow?
With this role, you play the defining role in:
growing Infisical and our customer base;
shaping short- and long-term product roadmap;
building out the future customer success & sales team;
Team, Values & Benefits
Our team has worked across transformative tech companies, from Figma to AWS to Red Hat.
We have an office in San Francisco, but we are mostly a remote team. We try to get together as often as possible ‚Äì whether it's for an off-site, conferences, or just get-togethers. This is a full-time role open to anyone in North American time zones.
At Infisical, we will treat you well with a competitive salary and equity offer. Depending on your risk tolerance, we would love to talk more with you about the range of options available between the two. For some other benefits (including lunch stipend, work setup budget, etc), please check out our careers page: https://infisical.com/careers.]]></description>
            <content:encoded><![CDATA[Open-source secrets manager for developersSolutions Engineer$120K - $180K‚Ä¢0.10% - 0.30%‚Ä¢San Francisco, CA, US / Remote (US; CA)Job typeFull-timeRoleEngineering, Full stackExperience1+ yearsVisaWill sponsorConnect directly with founders of the best YC-funded¬†startups.Apply to role ‚Ä∫Vlad MatsiiakoFounderAbout the roleAbout Infisical
Infisical is the #1 open source secret management platform for developers. In other words, we help organizations manage API-keys, DB access tokens, certificates, and other credentials across all parts of their infra! In fact, we¬†process over 100M of such secrets per day.
Our customers range from some of the largest public enterprises to fastest-growing startups (e.g., companies like Hugging Face, Delivery Hero). Developers love us and our community is growing every day! Join us on a mission to make security easier for all developers ‚Äì¬†starting from secret management.
About this role
Infisical is looking for a customer success engineer to help grow Infisical‚Äôs customer base and ensure great product/onboarding experience. You will be working closely with our CEO and the rest of the engineering team on:

ensuring new customers are activated, happy, and have all of their questions answered;
expanding Infisical to new use cases within accounts (e.g., more integrations, feature announcements, cross-sell);
running proof-of-concept with leads, guiding customer through onboarding, or being able to recommend best practices for specific architectures and use cases;
becoming a trusted technical advisor to customer and business leaders;
talking to customers daily to find points of friction and potential improvement in any workflows, understanding customer's needs and provide feedback towards product roadmap;
improving documentation and public facing materials based on customer feedback;
managing/facilitating case studies and customer quotes/references;
hosting webinars and knowledge sharing sessions for customer's stakeholders;
understanding user journey and being able to guide UX/DX roadmap;

Overall, you‚Äôre going to be one of the defining pieces of our team as we scale to thousands of customers over the next 18 months.
About you
This job will require you to have the following pivotal skills:

technical experience in development or systems engineering (open source experience is a plus);
experience in a customer-facing role (technical/developer product is a plus);
understanding of the Infrastructure/DevSecOps space is a plus;
exceptional verbal, presentation, and written communication skills;
high energy and strong work ethic;

How you will grow?
With this role, you play the defining role in:

growing Infisical and our customer base;
shaping short- and long-term product roadmap;
building out the future customer success & sales team;

Team, Values & Benefits
Our team has worked across transformative tech companies, from Figma to AWS to Red Hat.
We have an office in San Francisco, but we are mostly a remote team. We try to get together as often as possible ‚Äì whether it's for an off-site, conferences, or just get-togethers. This is a full-time role open to anyone in North American time zones.
At Infisical, we will treat you well with a competitive salary and equity offer. Depending on your risk tolerance, we would love to talk more with you about the range of options available between the two. For some other benefits (including lunch stipend, work setup budget, etc), please check out our careers page: https://infisical.com/careers.
About InfisicalInfisical is the #1 open source secret management platform ‚Äì¬†used by tens of thousands of developers.
We raised $3M from Y Combinator, Gradient Ventures (Google's VC fund), and awesome angel investors like Elad Gil, Arash Ferdowsi (founder/ex-CTO of Dropbox), Paul Copplestone (founder/CEO of Supabase), James Hawkins (founder/CEO of PostHog), Andrew Miklas (founder/ex-CTO of PagerDuty), Diana Hu (GP at Y Combinator), and more.
We are default alive, and have signed many customers ranging from fastest growing startups to post-IPO enterprises.
Founded:2022Batch:W23Team Size:15Status:ActiveLocation:San FranciscoFoundersTony DangFounderMaidul IslamFounderVlad MatsiiakoFounderSimilar Jobs]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Plastic Before Plastic: How gutta-percha shaped the 19th century]]></title>
            <link>https://worldhistory.substack.com/p/plastic-before-plastic</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084193</guid>
        </item>
        <item>
            <title><![CDATA[10-20x Faster LLVM -O0 Back-End (2020)]]></title>
            <link>https://discourse.llvm.org/t/tpde-llvm-10-20x-faster-llvm-o0-back-end/86664</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45084111</guid>
        </item>
        <item>
            <title><![CDATA[Jujutsu for everyone]]></title>
            <link>https://jj-for-everyone.github.io/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45083952</guid>
            <description><![CDATA[A Jujutsu tutorial that requires no previous experience with Git or other version control systems.]]></description>
            <content:encoded><![CDATA[
    
            Keyboard shortcuts
            
                Press ‚Üê or ‚Üí to navigate between chapters
                Press S or / to search in the book
                Press ? to show this help
                Press Esc to hide this help
            
        
    
        
        

        
        

        

        
        

        
            
            
            
            
        

        

            
                    
                        Introduction
This is a tutorial for the Jujutsu version control system.
It requires no previous experience with Git or any other version control system.
At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu.
This tutorial is my attempt to fill the void of beginner learning material for Jujutsu.
If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.
This tutorial requires you to work in the terminal.
Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet.
The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac.
If you're on Windows (and can't switch to Linux), consider using WSL.
How to read this tutorial
The tutorial is split into levels, which are the top-level chapters in the sidebar.
The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned.
Once you're comfortable with those skills, come back for the next level.
There is one exception to this:
If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.
Here's an overview of the planned levels:
LevelDescription
1The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.
2The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.
3Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.
4History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.
5Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.
6Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.


Only a few levels are complete right now, the rest are on the way.
Reset your progress
Throughout the tutorial, you will build an example repository.
Later chapters depend on the state of previous ones.
Losing the state of the example repo can therefore block you from making smooth progress.
This might happen for several reasons:

You use the example repo for practice and experimentation.
You switch to a different computer or reinstall the OS.
You intentionally delete it to clean up your home directory.
The tutorial is updated significantly while you're taking a break.

To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter.
To identify the chapter you want to continue with, the script expects a keyword as an argument.
Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.



Always be careful when executing scripts from the internet!




The script is not complicated, you can verify that it's not doing anything malicious.
Basically, it's just the list of commands I tell you to run manually.
For convenience, it's included in the expandable text box below.
You can also download the script here and then execute it locally once you have inspected it.





Source of reset script




#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "‚úÖ‚úÖ‚úÖ Reset script completed successfully! ‚úÖ‚úÖ‚úÖ"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj > /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" > README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" >> README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" > hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" > README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" >> README.md

echo "*.tar.gz" > .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i < 10; i = i + 1):
    print('Hello, world!')" > hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" >> hello.py
echo "print('Bonjour, le monde!')" >> hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" > hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" > hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &> /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1



Stay up to date
Both this tutorial and Jujutsu are still evolving.
In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo.
You will be notified of important changes:

A new level becomes available.
An existing level is changed significantly.

I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content.
I consider this tutorial up-to-date with the latest version of Jujutsu (0.32) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.
You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".

Help make this tutorial better
If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner.
If you have general suggestions for improvement, please open an issue.
I am also very interested in experience reports, for example:

Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?
Was there a section that wasn't explained clearly?
(If you didn't understand something, it's probably the tutorial's fault, not yours!)
Did you complete a level but didn't feel like you had the skills that were promised in the level overview?
Is there something missing that's not being taught but should?
Do you feel like the content could be structured better?

Thank you for helping me improve this tutorial!
What is version control and why should you use it?
I will assume you're using version control for software development, but it can be used for other things as well.
For example, authoring professionally formatted documents with tools like Typst.
The source of this tutorial is stored in version control too!
What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time.
You don't want to lose any of it and you want to be able to go back to previous states of your work.
Often, several people need to work on the project at the same time.
A general-purpose backup solution can keep a few copies of your files around.
A graphical document editor can allow multiple people to edit the text simultaneously.
But sometimes, you need a sharper knife.
Jujutsu is the sharpest knife available.
Why Jujutsu instead of Git?
Git is by far the most commonly used VCS in the software development industry.
So why not use that?
Using the most popular thing has undeniable benefits.
There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc.
Why make life harder on yourself by using a lesser-known alternative?
Here's my elevator pitch:


Jujutsu is compatible with Git.
You're not actually losing anything by using Jujutsu.
You can work with it on any existing project that uses Git for version control without issues.
Tools that integrate with Git mostly work just as well with Jujutsu.


Jujutsu is easier to learn than Git.
(That is, assuming I did a decent job writing this tutorial.)
Git is known for its complicated, unintuitive user interface.
Jujutsu gives you all the functionality of Git with a lot less complexity.
Experienced users of Git usually don't care about this, because they've paid the price of learning Git already.
(I was one of these people once.)
But you care!


Jujutsu is more powerful than Git.
Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust.
Don't worry, you don't have to use that power right away.
But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back.
This is not a watered-down "we have Git at home" for slow learners!


Learning Jujutsu instead of Git as your first VCS does have some downsides:


When talking about version control with peers, they will likely use Git-centric vocabulary.
Jujutsu shares a lot of Git's concepts, but there are also differences.
Translating between the two in conversation can add some mental overhead.
(solution: convince your peers to use Jujutsu üòâ)


Jujutsu is relatively new and doesn't cover 100% of the features of Git yet.
When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly.
Still, having to use two tools instead of one is slightly annoying.
I plan to teach such Git features in this tutorial in later levels.
The tutorial should be a one-stop-shop for all Jujutsu users.


The command line interface of Jujutsu is not yet stable.
That means in future versions of Jujutsu, some commands might work a little differently or be renamed.
I personally don't think this should scare you away.
Many people including me have used Jujutsu as a daily driver for a long time.
Whenever something did change, my reaction was usually:
"Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!"
Consider subscribing to GitHub releases of this tutorial.
You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.


Despite some downsides, I think the benefits are well worth it.

                    

                    
                        

                            
                                
                            

                        
                    
                

            

                    
                        
                    
            

        




        


        
        
        

        
        
        

        


    
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Survey: a third of senior developers say over half their code is AI-generated]]></title>
            <link>https://www.fastly.com/blog/senior-developers-ship-more-ai-code</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45083635</guid>
            <description><![CDATA[Fastly‚Äôs survey shows senior developers trust gen AI tools enough to ship 2.5x more AI code, while juniors stick to traditional coding and caution.]]></description>
            <content:encoded><![CDATA[Fastly‚Äôs July 2025 survey of 791 developers found a notable difference in how much AI-generated code is making it into production. About a third of senior developers (10+ years of experience) say over half their shipped code is AI-generated ‚Äî nearly two and a half times the rate reported by junior developers (0‚Äì2 years of experience), at 13%.‚ÄúAI will bench test code and find errors much faster than a human, repairing them seamlessly. This has been the case many times,‚Äù one senior developer said. A junior respondent noted the trade-offs: ‚ÄúIt‚Äôs always hard when AI assumes what I‚Äôm doing and that‚Äôs not the case, so I have to go back and redo it myself.‚ÄùSenior developers were also more likely to say they invest time fixing AI-generated code. Just under 30% of seniors reported editing AI output enough to offset most of the time savings, compared to 17% of juniors. Even so, 59% of seniors say AI tools help them ship faster overall, compared to 49% of juniors.Senior Developers Are More Optimistic About AI Saving TimeJust over 50% of junior developers say AI makes them moderately faster. By contrast, only 39% of more senior developers say the same. But senior devs are more likely to report significant speed gains: 26% say AI makes them a lot faster, double the 13% of junior devs who agree.One reason for this gap may be that senior developers are simply better equipped to catch and correct AI‚Äôs mistakes. They have the experience to recognize when code ‚Äúlooks right‚Äù but isn‚Äôt. That makes them more confident at using AI tools efficiently, even for high-stakes or business-critical code. By contrast, junior developers may not fully trust their ability to spot errors, which can make them more cautious about relying on AI, or more likely to avoid using it in production at all.That tracks with how much AI-generated code actually makes it into production. Among junior devs, just 13% say over half of their shipped code is AI-generated. By contrast, 32% of senior developers say the same, suggesting that more experienced engineers are not only using AI more aggressively, but are also trusting it more in production environments. This is surprising given growing concerns about ‚Äúvibe coding‚Äù introducing vulnerabilities into applications.¬†Perception vs. RealityNearly 1 in 3 developers (28%) say they frequently have to fix or edit AI-generated code enough that it offsets most of the time savings. Only 14% say they rarely need to make changes. And yet, over half of developers still feel faster with AI tools like Copilot, Gemini, or Claude.Fastly‚Äôs survey isn‚Äôt alone in calling AI productivity gains into question. A recent randomized controlled trial (RCT) of experienced open-source developers found something even more striking: when developers used AI tools, they took 19% longer to complete their tasks.This disconnect may come down to psychology. AI coding often feels smooth: code autocompletes with a few keystrokes. This gives the impression of momentum, but the early speed gains are often followed by cycles of editing, testing, and reworking that eat into any gains. This pattern is echoed both in conversations we've had with Fastly developers and in many of the comments we received in our survey.One respondent put it this way: ‚ÄúAn AI coding tool like GitHub Copilot greatly helps my workflow by suggesting code snippets and even entire functions. However, it once generated a complex algorithm that seemed correct but contained a subtle bug, leading to several hours of debugging.‚ÄùAnother noted: ‚ÄúThe AI tool saves time by using boilerplate code, but it also needs manual fixes for inefficiencies, which keep productivity in check.‚ÄùYet, AI still seems to improve developer job satisfaction. Nearly 80% of developers say AI tools make coding more enjoyable. For some, it‚Äôs about skipping grunt work. For others, it might be the dopamine rush of code on demand.‚ÄúIt helps me complete a task that I‚Äôm stuck with. It allows me to find the answers necessary to finish the task,‚Äù one survey respondent says.Enjoyment doesn‚Äôt equal efficiency, but in a profession wrestling with burnout and backlogs, that morale boost might still count for something.The Hidden Cost of AI CodingFastly‚Äôs survey also explored developer awareness of green coding‚Äîthe practice of writing energy-efficient software‚Äî and the energy cost behind AI coding tools. The practice of green coding goes up sharply with experience. Just over 56% of junior developers say they actively consider energy use in their work, while nearly 80% among mid- and senior-level engineers consider this when coding.¬†Developers are very aware of the environmental cost of AI tools: roughly two-thirds of developers across all experience levels said they know that these tools can carry a significant carbon footprint. Only a small minority (under 8% even at the most junior levels) were completely unaware. Altogether, the data suggests that sustainability is increasingly embedded in developer culture.MethodologyThis survey was conducted by Fastly from July 10 to July 14, 2025, with 791 professional developers. All respondents confirm that writing or reviewing code is a core part of their job. The survey is distributed in the US and quality-controlled for accuracy, though, as with all self-reported data, some bias is possible.¬†]]></content:encoded>
        </item>
    </channel>
</rss>