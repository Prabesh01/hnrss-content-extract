<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 30 Aug 2025 12:14:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Tell HN: My advice after I applied to 450 positions before getting hired]]></title>
            <link>https://news.ycombinator.com/item?id=45073589</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45073589</guid>
            <description><![CDATA[I wanted to briefly share my experience as a senior engineer with 15 years of experience trying to find work in this market, because it was exhausting for me and i'm sure others will appreciate the perspective.]]></description>
            <content:encoded><![CDATA[Tell HN: My advice after I applied to 450 positions before getting hired40 points by usernamed7 1 hour ago  | hide | past | favorite | 24 commentsI wanted to briefly share my experience as a senior engineer with 15 years of experience trying to find work in this market, because it was exhausting for me and i'm sure others will appreciate the perspective.As the title says, I have applied to over 450 positions. Most companies did not even send me a rejection. Ghost jobs are a thing, so are fake roles to get you to signup/join some rando job board.I interviewed for a director of engineering role, and all interviews went well, but they ghosted me at the end.I did several take homes and all were accepted, but companies dragged their feet on next steps.I did reject a few kinds of roles: ones that used AI for interviewing me, ones that had me do a coding challenge as the first step, and jobs that had "no working hours" and expected you to be "on" 24/7.Many of the job applicant expected me to answer asinine questions like "what excited you about this role?" and would say things like "don't use AI! we want your true self" or would go so far as to try to get you to agree to their AI interview policy. As If.I eventually did get hired as a software architect. the company that hired me was very professional, respectful, forward thinking (i used windsurf during the interview) and did not play games with me. They had a 4-step interview process, and asked a lot of good questions. One of the best interview processes of my career.My advice to other engineers on the job market:  1) Spray and pray. If its vaguely a fit, apply. It's a numbers game. Be shameless. 
  2) Always be willing to walk. Protect your time. Don't waste your time on lengthy job applications that take too long to complete. Some hiring managers will gladly waste your time. (one job application explicitly wanted you to spend 20 minutes filling out theirs)
  3) Don't do coding exercises before you interview with someone, be weary of asymmetrical time expenditures. see #2. 
  4) You can probably do a lot of different roles, "prompt engineer" is a real job title companies are hiring for, for example. 
  5) Work a couple of different job platforms. For example I used linkedin, dice, ziprecruiter, weworkremotely, and rubyonremote and a few others.
  6) Use AI to generate your resume, but feed it all the context of your work history (don't misrepresent your skills)
  7) Use AI to fill out asinine job application questions, but if they ask you thoughtful questions answer those yourself. I got the interview for director of engineering because i answered authentically to thoughtful questions.
  8) Pace yourself. Spend a few hours a day at it then come back in a day or two and go again. 
  9) Work on a side project or learn a new lang/framework in parallel. 
  10) Interviewing is like dating, everyone is looking for something different, and some don't really know what they want. Not a you problem.
  11) If they use workday for their job applications, bounce. It's the worst. 
  12) It takes time as roles become available. The job you end up getting might not open until 2 months from now. see #1.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Amiga Hardware Reference Manual 3rd Edition (1991)]]></title>
            <link>https://archive.org/details/amiga-hardware-reference-manual-3rd-edition</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45073492</guid>
            <description><![CDATA[AMIGA TECHNICAL REFERENCE SERIESIntroductionCoprocessor hardwarePlayfield hardwareSprite hardwareAudio hardwareBlitter hardwareSystem Control hardwareInterface...]]></description>
            <content:encoded><![CDATA[
      
            
         
    
    AMIGA TECHNICAL REFERENCE SERIESIntroductionCoprocessor hardwarePlayfield hardwareSprite hardwareAudio hardwareBlitter hardwareSystem Control hardwareInterface hardware



        
        
                  
          Addeddate
    
            2020-12-18 13:37:57                
        
                      
          Identifier
    
            amiga-hardware-reference-manual-3rd-edition                
        
                      
          Identifier-ark
    
            ark:/13960/t9v21w19n                
        
                      
          Ocr
    
            tesseract 4.1.1                
        
                      
          Ocr_detected_lang
    
            en                
        
                      
          Ocr_detected_lang_conf
    
            1.0000                
        
                      
          Ocr_detected_script
    
            Latin                
        
                      
          Ocr_detected_script_conf
    
            1.0000                
        
                      
          Ocr_module_version
    
            0.0.10                
        
                      
          Ocr_parameters
    
            -l eng                
        
                      
          Page_number_confidence
    
            95.68                
        
                      
          Ppi
    
            300                
        
          
                            
            
        

        
    
    
    
        
    
      
                  
                comment
        Reviews 
      
      
      
      
          
      
            
      
                  
        
        8,297

        Views      

      
                  31
          Favorites
              

          
    
                                
      
        DOWNLOAD OPTIONS
      

      
                        
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                                
                            
      
                
        
    

              
      
        Uploaded by
                  
            retroGfx          
        
                  on December 18, 2020
              
    
          
          ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms]]></title>
            <link>https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45072160</guid>
            <description><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms
What is attention?
In any autoregressive model, the prediction of the future tokens is based on some preceding context …]]></description>
            <content:encoded><![CDATA[7 min read15 hours ago--Press enter or click to view image in full sizeWhat is attention?In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.“The animal didn’t cross the street because it was too tired”.In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.How is attention calculated?There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.Key Concepts in Attention MechanismsBefore diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vectorKV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.To understand how each of these vectors are scores are calculated you can refer to this blog.The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.Now, let's dive into each of these techniquesMulti-Head Attention (MHA)In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.Press enter or click to view image in full sizeImage credits: Illustrated TransformersIn multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.Models using MHA – Bert, RoBerta, T5, etc.Multi-Query Attention (MQA)A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHAModels using MQA – PaLM, FalconGrouped Query Attention (GQA)Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.Models using GQA – Llama2, Llama3, MistralMulti-Head Latent Attention (MHLA)While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.So during the inference:A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectorsThis is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectorsAdditionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embeddingAdditionally, the same process is done for attention Queries as well, which will reduce the activation memory during trainingPress enter or click to view image in full sizeMHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.Models using MHLA– Deepseek- V2, Deep seek V2ConclusionIn addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Hacker News em dash user leaderboard pre-ChatGPT]]></title>
            <link>https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45071722</guid>
        </item>
        <item>
            <title><![CDATA[SynthID – A tool to watermark and identify content generated through AI]]></title>
            <link>https://deepmind.google/science/synthid/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45071677</guid>
            <description><![CDATA[SynthID is a tool to watermark and identify AI-generated content, helping to foster transparency and trust in generative AI.]]></description>
            <content:encoded><![CDATA[
      
  
    
    
  
  
  
    
      

      
      
        
          
            
              
                
                
                  
                  





  A tool to watermark and identify content generated through AI
          
    
      Join the early tester waitlist
      
    
  
    
      Become a SynthID partner
      
    
  
        
  
    
    
    
    
      
      
    
    
  
  

                
              
            
          
        
      

      
    
  
    
      
        
        
        
        
          


        
      

      
      
        
          
            
    
    What is SynthID?
    Generative AI can help us all to be more creative, productive, and innovative. But it can be hard to tell the difference between content that’s been AI-generated, and content created without AI.SynthID is our new watermarking tool, designed specifically for AI-generated content. It empowers users to identify AI-generated (or altered) content, helping to foster transparency and trust in generative AI.
    
  
          
        
      
        
          
            
              
                
                
                  
                  


    
      
          
            
              
                
              
            
            
    
  
          
          
            
    
    
    
      
      
    
    
  
          
        
    
  
                
              
                
                
                  
                  
    
    How SynthID works
    SynthID embeds digital watermarks directly into AI-generated images, audio, text or video. The watermarks are embedded across Google’s generative AI consumer products, and are imperceptible to humans – but can be detected by SynthID's technology.
    
  
                
              
                
                
                  
                  
  
    
      
        



    
    
    
      
    
    
  AI-generated image and video
            Learn more
            
          
        
    
  
    
      
    
      
        



    
    
    
      
    
    
  AI-generated audio
            Learn more
            
          
        
    
  
    
      
    
      
        



    
    
    
      
    
    
  AI generated-text
            Learn more
            
          
        
    
  
    
      
    
  

                
              
            
          
        
      
        
          
            
              
                
                
                  
                  
    
    SynthID Detector
    Identify if something has been created by Google AI. Just upload an image, video, audio file, or text snippet.
    
      Join the early tester waitlist
      
    
  
                
              
                
                
                  
                  






  

  

                
              
            
          
        
      
        
          
            
    
    Partners
    We’re partnering with companies around the world to watermark their AI-generated content with SynthID. It’s part of our goal to improve transparency and trust in AI-generated content.
    
      Become a SynthID partner
      
    
  
          
        
      

      
        
        
      
    
  
    
      

      
      
        
          
            
          
        
      

      
    
  
  

  

  

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why Romania excels in international Olympiads]]></title>
            <link>https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45070793</guid>
            <description><![CDATA[Olympiads are international student intellectual competitions in which students from across the world go toe-to-toe answering questions in mathematics, physics, informatics, chemistry, and more. The best performers tend to be from countries like China, the United States, India, and Japan. But, somehow, the southeastern European country of Romania also frequently tops the list.]]></description>
            <content:encoded><![CDATA[
        Olympiads are international student intellectual competitions in which students from across the world go toe-to-toe answering questions in mathematics, physics, informatics, chemistry, and more. The best performers tend to be from countries like China, the United States, India, and Japan. But, somehow, the southeastern European country of Romania also frequently tops the list.
Since 2020, Romania’s performance in the International Mathematical Olympiad (IMO) has been nothing short of amazing. In 2022, Romania came in fifth overall, fourth in 2023, and twelfth in 2024. In 2023, Romania placed fourth globally and first in Europe at the International Physics Olympiad, seventeenth globally and third in Europe at the International Olympiad in Informatics, sixth globally and second in Europe in the European Girls’ Mathematical Olympiad, first in the Balkan Mathematical Olympiad—which also included France, Italy, and the United Kingdom—and first in the Central European Olympiad in Informatics. Romania also performed well in the International Chemistry Olympiad and many others.
It’s an understatement to call Romania’s skill in Olympiads merely “overperformance”. Romania’s lackluster performance in international assessments and its relatively small population size of just over 19 million people makes the things they do in Olympiads downright miraculous.
Average Romanian educational performance is unimpressive. Romanian youth routinely perform below the average of OECD countries and near the bottom of the pack of European nations. Romania has a poor-to-mediocre showing whether you include or exclude migrants from the calculations, and its scores on assessments like the PISA aren’t low due to being tainted by bias in the examinations. Romania genuinely underperforms. But underperformance is not the impression you would get if you only knew of Romanian education from Olympiads.
One possibility is that Romanian students have more variable performance on international assessments than students in other countries. No dice: they aren’t much more variable than the student populations in other countries, and a handful of comparably-sized nations with worse Olympiad performance are more variable. Another possibility is that, for some reason, there’s a fat right tail in Romanian educational performance. If this is true, it just doesn’t show up in any existing data. Given the fact that international assessments indicate Romania’s sampling tends to be population-representative, we should have a strong prior against this possibility. Romanian test scores tend to be distributed along a symmetrical bell curve. 
Yet another possibility is that Romania has an undersampled ethnic group that overperforms, but whose schools aren’t tested very well. The only group this might be is Romanian Jews and using them as an explanation is problematic for two reasons. The first is that there are too few to realistically explain Romanian Olympiad performance. The second is that we know the identities of Olympiad participants from Romania, and they don’t seem to be Jewish.
Something else, something more mysterious, explains why Romania is such an outlier in international intellectual competitions. That thing is, in fact, the unique design of the Romanian educational system.
In the late 19th century, Romanian prince regnant Alexandru Ioan Cuza attempted to raise the status of the nation by instituting a mass literacy campaign centered around building free schools that children were compelled to attend. This effort was largely a failure, with literacy failing to break 50% by the 1930s. But World War II precipitated change. In 1948, Romania’s new governing communist party began to bring about serious educational reform at a breakneck pace.The Education Law of 1948 was passed to provoke a military-grade offensive against illiteracy, involving the mass participation of the literate from all walks of life in uplifting the poor, the abandoned, and those who simply shunned education. By the end of the 1950s, illiteracy was practically eradicated among Romania’s youth.
The education system that existed in Romania’s communist period was modeled on the system in place in the Soviet Union, and it included a fair helping of political propaganda in addition to physical labor. The system also overproduced schools, resulting in shoddy but widely available facilities dotting the country. Like the Soviet school system, Romania’s was marked by increasing lengths of compulsory education, poor availability of qualified teachers and educational supplies, high budgetary costs, and an extreme level of credential inflation.
After the fall of communism, the new democratic government went on to shutter many of these schools and to immediately lower compulsory schooling requirements to put an end to the bureaucratic nightmare that Soviet influence had saddled the country with. In the following years, how Romania wished to ration scarce governmental resources for education was a matter of intense debate, and out of that debate came a strong sentiment that, whatever the system, Romanian education would be structured competitively.
Nowadays, the most prestigious Romanian high schools are the National Colleges, or Colegiu Național. These schools are often international and frequently uphold old educational traditions sometimes dating back more than a century. Below these schools are the Liceu Teoretic, which are the norm, offering standard educations. Romania also has three military colleges—Colegiu Militar—managed directly by the Ministry of National Defense. There are also schools focused on service, technical schools, vocational schools, and apprenticeship programs. The brightest students get their pick among these schools after they take the national placement test, the Evaluarea Națională, when they are graduating the 8th grade around ages fourteen to fifteen.
The high school placement test is a standardized test covering Romanian language and literature as well as mathematics. Performance on the examination is reported publicly when students are issued a score on a one-to-ten scale with precision to two decimal places. A student who receives a high grade—say 9.65—would have their pick from most any school, whereas a student scoring 5.00 or below would usually be constrained to a less academically-focused form of education like a vocational program. Most students elect to go to the best school they are able to test into, and so the degree of sorting across schools is very high. To make this setup even more extreme, there is also often—but not universally—sorting within schools, as students select into educational tracks. This is done directly when applying to schools.
At the end of the Romanian high school experience, there is a graduation test, the Bacalaureat, or bac. This test is marked like the entrance examination and, to pass, students must obtain a score of at least five in the subjects they have elected to take. This testing includes written and oral examinations, assessments of foreign language and computer skills, and, for ethnic minorities, assessment of their skill with their maternal language other than Romanian. The need for a given score on this examination can range from requiring just passing to requiring a high score, depending on the university one intends to attend, if that is their goal.
The design of Romania’s educational system makes it perhaps the most stratified educational system in the world. The fact that they have a centralized repository containing all student and teacher educational data makes their system perfect for a high-powered evaluation of exactly what happens when a country opts to hyper-stratify education.
One of the cruel parts of the Romanian system is that, though sorting is nationally available, students do not have equal opportunities to sort. Students located in smaller towns have fewer high school options to select from unless they’re among the few who opt into a military academy, which means joining the military. The extent of sorting is far more intense in areas with larger numbers of schools. In a recent paper, the Romanian economist Andrei Munteanu provided an illustration of how this works: essentially, the fewer schools in a locale, the more each individual school contains students with a wider range of ability and, the more schools in a locale, the more each individual school will be stratified into low, middle, or high ability. 
This combined sorting between schools and tracks means that low-ability students get stuck with other low-ability students, and high-ability students are surrounded by other high-ability students. In effect, peer groups throughout high school are extremely homogeneous. This matters because then low-performing students drag down low-performing students, and high performers cause each other to rise. Romania’s educational system has causal peer impacts on student performance on the graduation test that are very large in both directions, but primarily where there are opportunities for sorting to take place.
Jordan Lasker/The more schools a town has the more intense the sorting of students is. Graduation scores are positively impacted for top performers and negatively for bottom performers with more intense sorting.
But peer effects are not everything to Romania’s exceptional Olympiad performance; they are just the fertile ground in which exceptional performance is fostered. The next part has to do with teachers. Like students, Romania’s teachers must take tests to be able to do what they want to do. Teachers naturally prefer to lecture smarter students, and the smartest teachers have their pick of the schools, and even of the tracks. In a paper with extremely robust results, researchers from the last decade described this as such:
[Teachers] with higher certification standards are more likely to work at better-ranked schools. This sorting persists even within schools as one moves from a weaker to a stronger track, and even within tracks as one moves from a weaker to a stronger class.
The best teachers also opt into towns with more schools. It’s apparent, then, that teachers prefer teaching in the highest-achieving places they can be, both within and between towns. The effect of teacher-student ability pairing is accentuated even more by incentives to compete. The government of Romania is not unique in providing monetary rewards for those who win Olympiads, those who teach winners of Olympiads, or those schools Olympiad winners attend, but they are unique in having all the previously-mentioned institutional characteristics on top of providing comprehensive monetary incentives for Olympiad achievement. 
Romania’s immense success in Olympiads and the widely recognized importance of Olympiad wins for signaling student human capital has also spawned a small number of private schools that advertise their prominence and tutoring capabilities. Many teachers also recommend to parents that they obtain additional tutoring for their brighter pupils, and tutoring services are commonplace. The commonality of tutoring for Olympiad winners is a global constant, whereas the things distinguishing Romania are not.
Two notable factors do not increase performance in the same direction. These are very slight decrements in funding allocated to the highest-ability schools, and when parents reduce the time they spend helping their students with homework, conditional on their kids matching into better schools. Another potential factor that militates against the synchrony of resource allocation in Romania is that children in more selective schools report feeling marginalized because they realize that they’re not as strong of students as they believed. The decrements in funding are likely to be unproblematic, because higher-scoring schools tend to be larger and more urban, lending them economies of scale. Due to this, they may have effectively more funding.
With all the pieces on the board, the key to Romania’s Olympiad success is three-fold: put the best students in the same classrooms, put the best teachers with the best students, and then incentivize schools, teachers, and students each to win Olympiads.
This system has proved amazingly fruitful. Given its underlying human capital, the poverty from its communist legacy, and its modest population size, Romania should not perform the way it does in academic Olympiads. And yet it does. The trade-off for Romania, however, is palpable.
Large portions of Romania’s Olympiad winners leave the country. Because Romania is a member state of the European Union, the people the country has put great effort into training and credentialing are easily able to leave the country and acquire jobs elsewhere.
Losing the right tail to brain drain is damaging for many countries, but it’s arguably worse for Romania because its educational system is so zero-sum: the top performers do better, while the low-performers do worse. This sorting does not “lift all boats,” as it were. In Romania, the system makes for an incredibly well-trained right tail and a neglected left tail, and that left tail might hurt more than the right tail is helped, if effects on test scores are any indication. On its own, Romania’s system might be a stellar boon to the country. But with free movement of talent between countries, Romania ends up subsidizing talent discovery for other countries with less apt educational systems. 
Most of the growth we see around us is due to the innovations of the right tail, and if they do better, we all do better. Though I doubt Romania’s schooling raises the intelligence of the right tail, even raising aptitude is worth something, because we must get capable people to the frontiers of their respective fields in order to innovate, and Romania has fostered a system that seems to do just that. Moreover, even if Olympiad training does not make those on the right tail more capable but instead simply prepares them better, then it can still have large, socially beneficial effects simply through providing Romania’s highly capable people with a means of having their talents recognized internationally. 
But these benefits are returned only very indirectly to Romania, if at all on net. Rather than changing Romania’s educational system or closing the borders, the right solution is for more nations to choose to be like Romania, getting a lot more juice out of their smart kids by designing a system just for them.
Jordan Lasker is a bioinformatician. He writes on his website and you can follow him at @cremieuxrecueil.


            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nginx-CGI brings support for CGI to Nginx and angie]]></title>
            <link>https://github.com/pjincz/nginx-cgi</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45070602</guid>
            <description><![CDATA[run cgi scripts with nginx. Contribute to pjincz/nginx-cgi development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[nginx-cgi plugin
Brings CGI support to Nginx and
Angie webserver.



OS
Tested with
Nginx
Angie




Linux
AlmaLinux 9, Debian 12 and Ubuntu 24.04/20.04
okay
okay


Darwin
MacOS 15.1
okay
okay


BSD
FreeBSD 14.2 and OpenBSD 7.6
okay
okay


Solaris
OmniOS r1510521
okay
okay


Windows
No plan, nginx barely supports Windows





Before everything
CGI is neither a demon nor an angel. It is simply a tool. Just like a chef's
knife in the hands of a cook or a sword in the hands of a warrior, you won't use
a sword for cooking, nor you take a chef's knife to the battlefield. The same
goes for CGI, it has its appropriate scenarios, and it should not be misused or
demonized.
CGI is good for:

Low frequency applications, such as system management
Resource limited systems, such as embeding system
Low budget projects, such as personal websites
Prototyping, for fast iterate

CGI is bad for:

High QPS
High traffic
High concurrency

I created a discord channel. If:

You are also a fun of CGI
If you have any problem with nginx-cgi
If you want to get update of nginx-cgi
If you want to know more friends

Please join us: https://discord.gg/EJSfqHHmaR.
Quick start (with Debian 12+, Ubuntu 24.04+)
Build and install:
# checkout source code
git clone https://github.com/pjincz/nginx-cgi
cd nginx-cgi

# build deb package
./build-deb-package.sh

# install built package
dpkg -i ../libnginx-mod-http-cgi_*_amd64.deb 
Then enable cgi in nginx. If you have a newly installed nginx, you can find a
default site at /etc/nginx/sites-enabled/default. The default one looks like
this:
server {
    listen 80 default_server;
    listen [::]:80 default_server;

    root /var/www/html;

    index index.html index.htm index.nginx-debian.html;

    server_name _;

    location / {
        try_files $uri $uri/ =404;
    }
}

The default root points to /var/www/html, keep it as it as, and add
following section after location / section.
    location /cgi-bin {
        cgi on;
    }

The newly added section means, for all request under /cgi-bin, turns on cgi
support. Now restart nginx:
systemctl restart nginx
Save following content to /var/www/html/cgi-bin/hello.sh
#!/bin/bash

echo "Content-Type: text/plain"
echo

echo Hello CGI
Add x perm to cgi script:
chmod +x /var/www/html/cgi-bin/hello.sh
Now, try it:
curl http://127.0.0.1/cgi-bin/hello.sh
If you nothing wrong, you will get an output of Hello CGI.
Build
If you are using latest deb based system, such as Debian and Ubuntu, and not
willing to debug the plugin, you can just following the Quick start to get a
usable deb package.
If you are using Angie, the cgi plugin has already in Angie's official repo.
Please have a look here:
https://en.angie.software/angie/docs/installation/oss_packages/#install-thirdpartymodules-oss
Manual build guide:


Checkout nginx and this plugin
cd <some-where-you-like>
git clone https://github.com/nginx/nginx
git clone https://github.com/pjincz/nginx-cgi


Generate Makefile in nginx dir
cd nginx
./auto/configure --add-dynamic-module=$PWD/../nginx-cgi [...other option...]
If you want to debug the plugin, you may also want --with-debug.
If you want to build a module compatible with system's nginx, you need run
nginx -V to checkout system nginx's build options first.


Make the binary
make


If everything is good, then you will find ngx_http_cgi_module.so under objs
directory.
Usage
Loading plugin
If this plugin is installed to nginx's default module path (such as
/usr/lib/nginx/modules), the plugin will be loaded automatically.
Otherwise, you need to manually load the plugin by load_module.
Add following statement to nginx's top level context to load the plugin:
load_module <dir-of-plugin>/ngx_http_cgi_module.so;

Enable cgi
After loading the plugin, you can add cgi on to location contexts to enable
cgi. Example:
location /cgi-bin {
    cgi on;
}

Once cgi turned on on a location, all nested locations will also have cgi turned
on. If you want to disable cgi for a child location, just use cgi off.
When the location is accessed, nginx-cgi will find the script under the document
root (it's specified by root statement). For example, if you have specify the
document root as /var/www/html, then when you access /cgi-bin/hello.sh,
/var/www/html/cgi-bin/hello.sh will be executed.
Nginx-cgi also support alias, it like root statement in nginx, the only
difference is the location prefix will be removed from uri. For example, if you
want /cgi/hello.sh also reference to the same script, you can do this:
location /cgi {
    alias /var/www/html/cgi-bin;
    cgi on;
}

Hello script
A cgi script can be wrotten by any language. Here's an exmaple with shell. You
can save it to /var/www/html/cgi-bin/hello.sh for testing (if you didn't
change the default document root):
#!/bin/sh

echo "Status: 200 OK"
echo "Content-Type: text/plain"
echo

echo "Hello world"
The first line of the script is a shebang. If you clearly set cgi_interpreter,
it's okay to remove this line, otherwise missing of shebang will causes a 500
error. Some shell allows script be executable even without shebang, but it's not
allowed here. If a script runable by shell, but return 500 error, check the
shebang.
The output of cgi script contains 2 sections: the header section and body
section. The first 2 echo statements output the header section, and the last
echo statement outputs the body section. The echo statement in middle
outputs the separator. Both header section and body section can be empty, but
the separator is mandatory. Missing of separator will causes an 500 error.
All lines in header section will be parsed as normal http response header line.
And then passed to the client side. There's one special header Status, it will
be passed in response status line. If cgi_strict is on, nginx-cgi will check
all cgi output headers, and 500 error will be responsed if invalid header found.
Otherwise, invalid headers will be forwarded to client side too. It's fully
recommanded to keep cgi_strict on.
After separator, all output will be sent to client as body as it is.
x permission
After all, you need to add the x permission to the file:
chmod +x /var/www/html/cgi-bin/hello.sh
Normally, you need x-permission to make script runable. Missing of x-permission
can cause 403 error. If can't do this for any reason, cgi_interpreter can
help.
Request header
Request headers will be parsed and then translated to environment variables and
then passed to cgi script.
For example, you can find the query string in QUERY_STRING environment var.
And access Http-Accept by HTTP_ACCPET.
Here's an example:
#!/bin/sh
echo ""

echo "query string: $QUERY_STRING"
echo "http accept: $HTTP_ACCEPT"
For full list of environment variables, see environment section.
Request body
The request body will be passed via stdin. Here's an example to read all request
body and echo it:
#!/bin/sh
echo ""

body=$(cat)

echo "request body: $body"
Streaming
Nginx-cgi has streaming support for both request and response body. For example,
we can implement a simplest online caculator by bc:
#!/bin/sh
echo ""

bc 2>&1
Then we can test our caculator by curl:
curl 127.0.0.1/cgi-bin/bc.sh --no-progress-meter -T .
The nginx-cgi plugin is smart enough to choose the correct way to return the
request body. If it got all output soon enough, it will output the body in once.
If the output is delayed, it will output the body chunkly(HTTP 1.1) or
streamingly (HTTP 1.0).
Hop-by-hop http headers
Hop-by-hop http headers are not allowed in cgi script output. If it appears
in response here, a 500 error will response to the client.
For more information:
https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers#hop-by-hop_headers
Tricks && FAQ
I want to list all environment variables
Put following script to your cgi directory, and curl it form your terminal:
#!/bin/sh

echo 'Content-Type: text/plain'
echo

printenv
I want root permission
Put a sudo file to /etc/sudoers.d and run sudo in your script or set
cgi_interpreter as /usr/bin/sudo.
Here's an example of sudo config file:
# allow wwww-data run /var/www/bin/my-danger-script with root account
www-data ALL=(root) NOPASSWD: /var/www/bin/my-danger-script

# allow all CGI script be launched with sudo by nginx-cgi directly
www-data ALL=(root) NOPASSWD: SETENV: /var/www/html/cgi-bin/*

How can I run CGI scripts with chroot
It's highly not recommanded to run CGI script with chroot. Because chroot is not
designed for security purpose. It still shared a lot of kernel spaces with host
system. For example, run ps -ef in chrooted process, all processes in host
system will return. That sould not too aweful? No, that's really terrible,
because you can also do kill in chrooted script for the same reason. And
people normally run programs with root permission in chrooted environment.
That's terribly bad. It causes system on high risk than just run script with
www-data.
If you want a sandbox environment, lxc, docker and jails are much better
for this purpose.
If you still want chroot, okay let me show you how to do it.
In this example, I assume you're using /var/www/html as the document root.
Prepare a CGI script first:
mkdir -p /var/www/html/cgi-bin
cat > /var/www/html/cgi-bin/ls.sh <<EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh
Step 1: prepare a chroot directory.
That're a lot of ways to do this step. debootstrap is a popular way on debian
based system. busybox is the most light way. docker is a modern way.
Let's make a lightest directory with busybox here:
# In this example, I put everything to /var/www/chroot
# Be careful, I download x86_64 busybox version here, you may need to change it
# You need root permission to run all following commands, I'm too lazy to
# prepend sudo to every commands here.

root_dir=/var/www/chroot

mkdir -p "$root_dir/bin" && cd "$root_dir/bin"
wget https://www.busybox.net/downloads/binaries/1.35.0-x86_64-linux-musl/busybox
chmod +x busybox

cd "$root_dir"
mkdir -p $(dirname $(./bin/busybox --list-full) | sort -u)
./bin/busybox --list-full | while read line; do ln -sf /bin/busybox $line; done

# try it
chroot "$root_dir" ls
Step 2: mount document root into chroot dir
mkdir -p /var/www/chroot/var/www/html
mount --bind /var/www/html /var/www/chroot/var/www/html

# try it
ls /var/www/chroot/var/www/html
Notice:


I use a trick here, after chroot, the document root is still the same. By this
we can same some time to do path mapping.


The mounting will not persist after a reboot. You may need to add an entry to
/etc/fstab. Or move /var/www/html into chroot, and make a symbolic link
outside.


Step 3: allow www-data to run chroot with root permission.
cat >/etc/sudoers.d/www-run-with-chroot <<EOF
# allow and only allow www-data run chroot with /var/www/chroot
www-data ALL=(root) NOPASSWD: /usr/sbin/chroot /var/www/chroot *
EOF
Now everything is ready, add following section to your nginx/angie:
location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/bin/sudo /usr/sbin/chroot /var/www/chroot;
}

try it:
curl 127.0.0.1/cgi-bin/ls.sh
How can I run CGI scripts with docker
In this example, I assume you're using /var/www/html as the document root.
Prepare a CGI script first:
mkdir -p /var/www/html/cgi-bin
cat > /var/www/html/cgi-bin/ls.sh <<EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh
Create a container and keep running in the background:
# Change -v if necessary
# -d: runs background
# -i -t: keep a terminal
# --restart always: keep container alive
docker run -dit --restart always --name my_cgi_docker -v /var/www:/var/www busybox sh

# try it
docker exec my_cgi_docker /var/www/html/cgi-bin/ls.sh
Allow www-data to run docker commands:
sudo usermod -aG docker www-data

# try it
sudo -u www-data docker exec my_cgi_docker /var/www/html/cgi-bin/ls.sh
Now everything is ready, add following section to your nginx/angie:
location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/bin/docker exec my_cgi_docker;
}

How can I run CGI scripts with jails
Okay, you're a fan of FreeBSD? Me too.
It's really similar to running scripts with chroot.
Here I assume you're using /var/www/html as the document root too.
Prepare a CGI script first:
mkdir -p /var/www/html/cgi-bin
cat > /var/www/html/cgi-bin/ls.sh <<EOF
#!/bin/sh
echo "Status: 200"
echo "Content-Type: text-plain"
echo
echo "files under /:"
ls /
EOF
chmod +x /var/www/html/cgi-bin/ls.sh

# try it
/var/www/html/cgi-bin/ls.sh
Step 1: create a jail
Let's put the jail to /var/www/jail.
mkdir -p /var/www/jail && cd /var/www/jail
fetch https://download.freebsd.org/ftp/releases/$(uname -m)/$(uname -m)/$(uname -r)/base.txz
tar -xvf base.txz -C .

# create mount points
mkdir -p /var/www/jail/var/www/html
touch /var/www/jail/etc/resolv.conf
Put following config to /etc/jail.conf:
www-jail {
    path = "/var/www/jail";
    host.hostname = "www-jail.local";

    exec.clean;
    exec.start = "/bin/sh /etc/rc";
    exec.stop = "/bin/sh /etc/rc.shutdown";

    # mount /var/www/html => /var/www/jail/var/www/html
    exec.prestart += "mount_nullfs /var/www/html /var/www/jail/var/www/html || true";
    mount.devfs;

    # uncomment following lines, if you want to allow network access in jail
    # ip4 = inherit;
    # ip6 = inherit;
    # exec.prestart += "mount_nullfs /etc/resolv.conf /var/www/jail/etc/resolv.conf || true";

    # uncomment fowlling lines, if you also want `ping` available in jail
    # allow.raw_sockets = 1;

    persist; # keep jail if no process runs
}

And ensure that following line appears in /etc/rc.conf:
jail_enable="YES"

And start the jail:
service jail start www-jail

# try it
jexec www-jail ls /
jexec www-jail /var/www/html/cgi-bin/ls.sh
Step 2: allow www to run jexec with root permission.
I uses sudo here. I'm not familiar with doas, if you prefer doas you can
try it yourself. Anyhow, neither sudo nor doas preloaded with FreeBSD. You
need to manually install one of them.
cat >/usr/local/etc/sudoers.d/www-jexec <<EOF
# allow and only allow `www` run `jexec` with `www-jail`
www ALL=(root) NOPASSWD: /usr/sbin/jexec www-jail *
EOF

# try it
sudo -u www sudo jexec www-jail /var/www/html/cgi-bin/ls.sh
Now everything is ready, add following section to your nginx/angie:
location /cgi-bin {
    cgi on;
    cgi_interpreter /usr/local/bin/sudo /usr/sbin/jexec www-jail;
}

try it:
curl 127.0.0.1/cgi-bin/ls.sh
I want create a long-run background process
Just make sure not to inherit stdout when creating the process (ideally, avoid
inheriting stdin and stderr as well). Here's an example write in shell.
taskid=1234
logfile="/var/lib/my-project/$taskid"
./long-run-task.sh "$taskid" </dev/null >"$logfile" 2>&1 &
Or if you are familiar with pipe operation, just close stdout (also, it's
better to close stdin and stderr as well), http request will finished
immediently. And you can use the process as background process.
exec </dev/null >somewhere 2>&1

# now http response is done, do what every you like
sleep 9999
My http request hangs
As you see abvoing. In CGI world, http request's lifecycle depends on pipe's
(stdout's) lifecycle.
Each child process might inherit the CGI process's pipe. If any process that
inherited stdout remains alive, the HTTP request will never finish.
This may causes confiusing, when you want a long run background or killing
CGI process.
For creating long-run process, see aboving topic.
For killing CGI process, kill the whole process group rather than CGI process
itself.
cgi_pid=...

# don't do this
# kill "$cgi_pid"

# do this
kill -- "-$cgi_pid"
I want to kill my cgi script
See aboving topic.
I want to generate content dynamicaly
Traditionally, people use rewriting to archive this. But it's much easier here.
You can do it with cgi pass. Here's an example to render markdone dynamically:
{
    location ~ ^.*\.md$ {
        cgi_pass /var/www/bin/cgi/render-markdown.sh;
    }
}

#!/bin/sh

set -e

if [ ! -f "${DOCUMENT_ROOT}${PATH_INFO}" ]; then
    echo "Status: 404"
    echo
    exit
fi

echo "Status: 200"
echo "Content-Type: text/html"
echo

echo "<html><body>"
markdown "${DOCUMENT_ROOT}${PATH_INFO}"
echo "</body></html>"
I don't like suffixes in url
Way 1: Removing CGI script's suffix
Way 2: do rewriting
Way 3: cgi pass
How can I response status other than 200
#!/bin/sh

echo "Status: 404"
echo "Content-Type: text/plain"
echo

echo "Welcome to the void"
How can I response a redirection
#!/bin/sh

echo "Status: 302"
echo "Location: https://theuselessweb.com"
echo
How can I get http request body
You can read the request body from stdin. If you're using shell, cat can
quickly save request body to a file.
How can send file to the client
For small files, you can write file to stdout directly.
For large files, it's much better to send a 302 response. Because CGI response
is streaming, protocol cannot easily handle caching, chunked downloads, or
resume support.
I want to write CGI with python, ruby, perl, C, C++...
Go for it. Nginx-cgi don't care what language you use. Just grabs information
from environment var, and read request body from stdin, and write output to
stdout.
Manual
Options
cgi <on|off> or cgi pass <script_path> [script_args...]
Enable or disable cgi module on giving location block.
If you specify on here, the plugin will work in traditional mode. It parses
the request uri first, and then locate the script under document root directory
with request uri. After all it splits request uri to SCRIPT_NAME and
PATH_INFO. This is good if you have an old CGI project or you want to strictly
follow rfc3875.
I also provided a nginx style syntax here. If you specify cgi pass here, the
plugin will skip the step to locate the CGI script. It uses the the value you
provided directly. You can references nginx variables in the second argument,
eg: cgi pass $document_root$uri. The aboving example do something similar to
rfc3875, but not equal. In this form, request uri will be assigned to
PATH_INFO directly. And SCRIPT_NAME will be empty. This form is really good
for dynamic content generating. It gets around the complex and unnecessary uri
re-writing.
Additionally, the second form also provides you the ability to pass additional
args to script, eg: cgi pass my_script.sh $uri. With this, you can totally
avoid confusing rfc3875 environment variables.
If you specify off here, the plugin will be disabled.
Default: off
cgi_pass <script_path>
Alias of cgi pass <script_path>.
cgi_interpreter [interpreter] [args...]
Set interpreter and interpreter args for cgi script.
When this option is not empty, cgi script will be run with giving interpreter.
Otherwise, script will be executed directly.
This option can contains nginx variables, see
https://nginx.org/en/docs/varindex.html for more details.
This option is extremely useful in a lot of senarios, for example:

run CGI scripts missing x-perm
do sudo before executing CGI script
wrap general binary as CGI script
filter CGI script output
...

Default: empty
cgi_working_dir <dir>
Set the working directory of CGI script.
If this value is set to empty, CGI scripts will inherit nginx' working
directory.
If this value is set to an non-empty string, the CGI script will be launched
with giving working directory.
The action of changing working directory may failed. For example, giving
directory doesn't exist, no perm or name too long. In this case, script will
failed to execute.
This option doesn't change the way to find interpreter or script (if they are
specified with related path, they are always related to nginx' working
directory).
This option can contain nginx variable. Althrough I don't know what use this is.
Maybe you can setup different working dir for different server_name by this.
Default: empty
cgi_body_only <on|off>
A standard CGI script should output two parts: header and body. And an empty
line to split those two parts.
If you want to simply run a normal program as CGI program. You can turn this on.
Once this option is enabled, all outout will be treated as response body, and be
sent to the client.
Default: off
cgi_path <PATH>
Change cgi script PATH environment variable.
Default: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
cgi_strict <on|off>
Enable or disable strict mode.
When strict mode turns on, bad cgi header will cause 500 error. When strict mode
turns off, bad cgi header be forward as it is.
Default: on
cgi_set_var <name> <value>
Add and pass extra environment variables to CGI script. The first argument of
this command is the name of environment variable. It should contains only
alphabets, numbers and underscore, and doesn't start with number. The second
argument of this command is the value express of the var. It can contains nginx
variables, see https://nginx.org/en/docs/varindex.html for more details.
This option can appears more than 1 time to set multiple variables. If more than
one option set the same var, then the last one works. These directives are
inherited from the previous configuration level if and only if there's no
cgi_set_var directives defined on the current level.
This option can also be used to override standard CGI vars. This may be useful
in some case, for example hacking old CGI script or simulate standard vars that
are not supported by this plugin now (Such as PATH_TRANSLATED,
REMOTE_IDENT). But it's not recommanded, it may introduce confusing issues to
your system.
cgi_stderr <path>
Redirect cgi stderr to giving file.
By default, nginx-cgi grab cgi script's stderr output and dump it to nginx log.
But this action is somewhat expensive, because it need to create an extra
connection to listen stderr output. If you want to avoid this, you can use this
option to redirect cgi script's stderr output to a file. Or you can even discard
all stderr output by redirect to /dev/null. Also you can use this to redirect
all stderr output to nginx's stderr by set it as /dev/stderr.
cgi_rdns <on|off|double> [required]
Enable or disable reverse dns.
off: disable rdns feature.
on: Do reverse dns before launching cgi script, and pass rdns result to cgi
script via REMOTE_HOST environment variable.
double: After reverse dns, do a forward dns again to check the rdns result. if
result matches, pass result as REMOTE_HOST.
required: If rdns failed, 403, 503 or 500 returns to the client. Depends on
the failure reason of rdns.
If you turns this option on, you need to setup a resolver in nginx too.
Otherwise you will get an error of no resolver defined to resolve.
author notes: do not enable this option, it will makes every request slower.
this feature can be easily implemented by dig -x or nslookup in script. the
only reason I implement this is just to make the module fully compliant with the
rfc3875 standard.
cgi_timeout <t1> [t2]
Send TERM/KILL signals to the CGI process if it runs too long.
If both t1 and t2 equal to 0. Timeout feature is disabled.
If t1 or t2 doesn't equal to 0. A TERM or KILL signal will be sent to
the process after timeout.
If both t1 and t2 not zero. Send TERM at t1 timestamp first. And send
KILL again at t1+t2 timestamp (if process still alive at that timestamp).
If t2 doesn't present, it treated as 0.
Default: 0 0
Standard Environment Variables
Nginx-cgi implemented almost all rfc3875 standard variables. If they cannot
cover all of your usage, you can add your own variable by cgi_set_var. Also
those variables can be overrided by cgi_set_var if you really want to.

AUTH_TYPE, REMOTE_USER (rfc3875 standard)

If cgi script is behind an authorization module (such as
ngx_http_auth_basic_module), and the authorization is succeed, the value is
set to auth type (such as Basic) and authorized user.
If no authorization module enabled, no matter client passes autoriazation header
or not. Those 2 fields are not present.
Authorization header is not visible in cgi script for security reason. If you
really want to do authorization in CGI script, try cgi_set_var.

CONTENT_LENGTH, CONTENT_TYPE (rfc3875 standard)

Same to request header's Content-Length and Content-Type.

GATEWAY_INTERFACE (rfc3875 standard)

Always be CGI/1.1 in this plugin.

PATH_INFO (rfc3875 standard)

Let's say if you have a script under /cgi-bin/hello.sh, and you access
http://127.0.0.1/cgi-bin/hello.sh/somewhat.
Then PATH_INFO contains the string /somewhat.
Combination with url rewrite or cgi pass, this variable can be used for
dynamic content generating.

PATH_TRANSLATED (rfc3875 standard)

Note: this option is not implemented strictly compliant with rfc3875.
Please avoid this, if you are writing new CGI script.
This is related to PATH_INFO.
Let's say if you have a script under /cgi-bin/hello.sh, and you access
http://127.0.0.1/cgi-bin/hello.sh/somewhat.
The standard says, the server should try again with http://127.0.0.1/somewhat,
and found out where the uri should mapped to.
For technical reason, I just construct this variable by document root and
PATH_INFO.
The behaviour may be changed in future version.

QUERY_STRING (rfc3875 standard)

Contains the query string of the request. For example, if you are accessing
http://127.0.0.1/cgi-bin/hello.sh?a=1&b=2, QUERY_STRING will contains
a=1&b=2.

REMOTE_ADDR, (rfc3875 standard)

Client ip address.

REMOTE_HOST (rfc3875 standard)

Client host name. Only available if cgi_rdns is turns on.
If cgi_rdns is on, nginx-cgi will do a reverse DNS, and find a domain matches
REMOTE_ADDR. If any found, it will be set to REMOTE_HOST.
If cgi_rdns is double, after the RDNS, nginx-cgi will do a forward DNS again.
REMOTE_HOST will only be set if the forward DNS result matches the original
address.
See cgi_rdns for more information.

REMOTE_IDENT (rfc3875 standard)

Nginx-cgi plugin doesn't support this for security reason.

REQUEST_METHOD (rfc3875 standard)

Request method of the request, for example: GET, POST...

SCRIPT_NAME (rfc3875 standard)

Path to current script. Normally, you don't need this. It doesn't contains the
full path. See SCRIPT_FILENAME.
The only reason to use this is construct the URI after rewriting. You can use
SCRIPT_NAME + PATH_INFO to get the URI after rewriting.

SERVER_NAME (rfc3875 standard)

Server name, normally it equals to Host header without port part. If Host
header doesn't appear in the request (HTTP/1.0) or contains invalid value, then
this value is set to the reflect server ip address. If the ip address is an ipv6
address, it will be quoted with bracket like [::1].

SERVER_PORT (rfc3875 standard)

Server listening port, such as 80, 443...

SERVER_PROTOCOL (rfc3875 standard)

The protocol used between client and server. Such as HTTP/1.0, HTTP/1.1...

SERVER_SOFTWARE (rfc3875 standard)

Contains a string of nginx and version, such as nginx/1.27.4.

X_ (rfc3875 standard)

All X- prefixed http request header will be convert to X_ variables. For
example:
If X-a: 123 appears in header, X_A will be set to 123.

HTTP_ (rfc3875 standard)

All other http request header will be convert to HTTP_ variables, for example:
If Accept: */* appears in header, HTTP_ACCEPT will be set to */*.

DOCUMENT_ROOT (non-standard, impled by apache2)

Document root of current location block, see root stmt in nginx.

REMOTE_PORT (non-standard, impled by apache2)

Client port number.

REQUEST_SCHEME (non-standard, impled by apache2)

http or https.

REQUEST_URI (non-standard, impled by apache2)

The raw uri before rewriting. If you want the URL after rewriting, try
SCRIPT_NAME + PATH_INFO.
Note: this variable doesn't same to nginx varible $request_uri. You can find
the document at https://httpd.apache.org/docs/2.4/mod/mod_rewrite.html.

SCRIPT_FILENAME (non-standard, impled by apache2)

The full path to the CGI script.

SERVER_ADDR (non-standard, impled by apache2)

Server ip address. If the server has multiple ip addresses. The value of this
variable can be different if requests came from different interfaces.
Known Issues
PATH_TRANSLATED impl not accurate
By rfc3875, PATH_TRANSLATED should point to the file that as if $PATH_INFO
accessed as uri. But that's really hard to impl on nginx, it need re-trigger
nginx's location process. And those functions are private, cannot access by
plugin directly. The another way to impl it is starting a sub-request, but it's
too expensive, and this var is really rearly used. It's really not worth to do
it. So I simply construct this var by document root and path_info vars.
RDNS impl doesn't access /etc/hosts
Nginx's resolver impl doesn't access /etc/hosts. I don't want to impl an extra
resolver in plugin. So I just ignore this problem.
Reference
rfc3875
https://datatracker.ietf.org/doc/html/rfc3875
nginx
https://nginx.org/en/docs/dev/development_guide.html
https://hg.nginx.org/nginx-tests
Hop-by-hop headers
https://datatracker.ietf.org/doc/html/rfc2616#section-13.5.1
CGI environments
https://datatracker.ietf.org/doc/html/rfc3875#section-4.1
Apache CGI
https://httpd.apache.org/docs/2.4/howto/cgi.html
Lighttpd CGI
https://redmine.lighttpd.net/projects/lighttpd/wiki/Mod_cgi
License
2-clause BSD license
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Theoretical Limitations of Embedding-Based Retrieval]]></title>
            <link>https://arxiv.org/abs/2508.21038</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068986</guid>
            <description><![CDATA[Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks push embeddings to work for any query and any notion of relevance that could be given. While prior works have pointed out theoretical limitations of vector embeddings, there is a common assumption that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome with better training data and larger models. In this work, we demonstrate that we may encounter these theoretical limitations in realistic settings with extremely simple queries. We connect known results in learning theory, showing that the number of top-k subsets of documents capable of being returned as the result of some query is limited by the dimension of the embedding. We empirically show that this holds true even if we restrict to k=2, and directly optimize on the test set with free parameterized embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple nature of the task. Our work shows the limits of embedding models under the existing single vector paradigm and calls for future research to develop methods that can resolve this fundamental limitation.
    

    
    
      
          Subjects:
          
            Information Retrieval (cs.IR); Computation and Language (cs.CL); Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21038 [cs.IR]
        
        
           
          (or 
              arXiv:2508.21038v1 [cs.IR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.21038
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Orion Weller [view email]          [v1]
        Thu, 28 Aug 2025 17:43:53 UTC (195 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How did .agakhan, .ismaili and .imamat get their own TLDs?]]></title>
            <link>https://data.iana.org/TLD/tlds-alpha-by-domain.txt</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068215</guid>
            <content:encoded><![CDATA[# Version 2025083000, Last Updated Sat Aug 30 07:07:01 2025 UTC
AAA
AARP
ABB
ABBOTT
ABBVIE
ABC
ABLE
ABOGADO
ABUDHABI
AC
ACADEMY
ACCENTURE
ACCOUNTANT
ACCOUNTANTS
ACO
ACTOR
AD
ADS
ADULT
AE
AEG
AERO
AETNA
AF
AFL
AFRICA
AG
AGAKHAN
AGENCY
AI
AIG
AIRBUS
AIRFORCE
AIRTEL
AKDN
AL
ALIBABA
ALIPAY
ALLFINANZ
ALLSTATE
ALLY
ALSACE
ALSTOM
AM
AMAZON
AMERICANEXPRESS
AMERICANFAMILY
AMEX
AMFAM
AMICA
AMSTERDAM
ANALYTICS
ANDROID
ANQUAN
ANZ
AO
AOL
APARTMENTS
APP
APPLE
AQ
AQUARELLE
AR
ARAB
ARAMCO
ARCHI
ARMY
ARPA
ART
ARTE
AS
ASDA
ASIA
ASSOCIATES
AT
ATHLETA
ATTORNEY
AU
AUCTION
AUDI
AUDIBLE
AUDIO
AUSPOST
AUTHOR
AUTO
AUTOS
AW
AWS
AX
AXA
AZ
AZURE
BA
BABY
BAIDU
BANAMEX
BAND
BANK
BAR
BARCELONA
BARCLAYCARD
BARCLAYS
BAREFOOT
BARGAINS
BASEBALL
BASKETBALL
BAUHAUS
BAYERN
BB
BBC
BBT
BBVA
BCG
BCN
BD
BE
BEATS
BEAUTY
BEER
BERLIN
BEST
BESTBUY
BET
BF
BG
BH
BHARTI
BI
BIBLE
BID
BIKE
BING
BINGO
BIO
BIZ
BJ
BLACK
BLACKFRIDAY
BLOCKBUSTER
BLOG
BLOOMBERG
BLUE
BM
BMS
BMW
BN
BNPPARIBAS
BO
BOATS
BOEHRINGER
BOFA
BOM
BOND
BOO
BOOK
BOOKING
BOSCH
BOSTIK
BOSTON
BOT
BOUTIQUE
BOX
BR
BRADESCO
BRIDGESTONE
BROADWAY
BROKER
BROTHER
BRUSSELS
BS
BT
BUILD
BUILDERS
BUSINESS
BUY
BUZZ
BV
BW
BY
BZ
BZH
CA
CAB
CAFE
CAL
CALL
CALVINKLEIN
CAM
CAMERA
CAMP
CANON
CAPETOWN
CAPITAL
CAPITALONE
CAR
CARAVAN
CARDS
CARE
CAREER
CAREERS
CARS
CASA
CASE
CASH
CASINO
CAT
CATERING
CATHOLIC
CBA
CBN
CBRE
CC
CD
CENTER
CEO
CERN
CF
CFA
CFD
CG
CH
CHANEL
CHANNEL
CHARITY
CHASE
CHAT
CHEAP
CHINTAI
CHRISTMAS
CHROME
CHURCH
CI
CIPRIANI
CIRCLE
CISCO
CITADEL
CITI
CITIC
CITY
CK
CL
CLAIMS
CLEANING
CLICK
CLINIC
CLINIQUE
CLOTHING
CLOUD
CLUB
CLUBMED
CM
CN
CO
COACH
CODES
COFFEE
COLLEGE
COLOGNE
COM
COMMBANK
COMMUNITY
COMPANY
COMPARE
COMPUTER
COMSEC
CONDOS
CONSTRUCTION
CONSULTING
CONTACT
CONTRACTORS
COOKING
COOL
COOP
CORSICA
COUNTRY
COUPON
COUPONS
COURSES
CPA
CR
CREDIT
CREDITCARD
CREDITUNION
CRICKET
CROWN
CRS
CRUISE
CRUISES
CU
CUISINELLA
CV
CW
CX
CY
CYMRU
CYOU
CZ
DAD
DANCE
DATA
DATE
DATING
DATSUN
DAY
DCLK
DDS
DE
DEAL
DEALER
DEALS
DEGREE
DELIVERY
DELL
DELOITTE
DELTA
DEMOCRAT
DENTAL
DENTIST
DESI
DESIGN
DEV
DHL
DIAMONDS
DIET
DIGITAL
DIRECT
DIRECTORY
DISCOUNT
DISCOVER
DISH
DIY
DJ
DK
DM
DNP
DO
DOCS
DOCTOR
DOG
DOMAINS
DOT
DOWNLOAD
DRIVE
DTV
DUBAI
DUNLOP
DUPONT
DURBAN
DVAG
DVR
DZ
EARTH
EAT
EC
ECO
EDEKA
EDU
EDUCATION
EE
EG
EMAIL
EMERCK
ENERGY
ENGINEER
ENGINEERING
ENTERPRISES
EPSON
EQUIPMENT
ER
ERICSSON
ERNI
ES
ESQ
ESTATE
ET
EU
EUROVISION
EUS
EVENTS
EXCHANGE
EXPERT
EXPOSED
EXPRESS
EXTRASPACE
FAGE
FAIL
FAIRWINDS
FAITH
FAMILY
FAN
FANS
FARM
FARMERS
FASHION
FAST
FEDEX
FEEDBACK
FERRARI
FERRERO
FI
FIDELITY
FIDO
FILM
FINAL
FINANCE
FINANCIAL
FIRE
FIRESTONE
FIRMDALE
FISH
FISHING
FIT
FITNESS
FJ
FK
FLICKR
FLIGHTS
FLIR
FLORIST
FLOWERS
FLY
FM
FO
FOO
FOOD
FOOTBALL
FORD
FOREX
FORSALE
FORUM
FOUNDATION
FOX
FR
FREE
FRESENIUS
FRL
FROGANS
FRONTIER
FTR
FUJITSU
FUN
FUND
FURNITURE
FUTBOL
FYI
GA
GAL
GALLERY
GALLO
GALLUP
GAME
GAMES
GAP
GARDEN
GAY
GB
GBIZ
GD
GDN
GE
GEA
GENT
GENTING
GEORGE
GF
GG
GGEE
GH
GI
GIFT
GIFTS
GIVES
GIVING
GL
GLASS
GLE
GLOBAL
GLOBO
GM
GMAIL
GMBH
GMO
GMX
GN
GODADDY
GOLD
GOLDPOINT
GOLF
GOO
GOODYEAR
GOOG
GOOGLE
GOP
GOT
GOV
GP
GQ
GR
GRAINGER
GRAPHICS
GRATIS
GREEN
GRIPE
GROCERY
GROUP
GS
GT
GU
GUCCI
GUGE
GUIDE
GUITARS
GURU
GW
GY
HAIR
HAMBURG
HANGOUT
HAUS
HBO
HDFC
HDFCBANK
HEALTH
HEALTHCARE
HELP
HELSINKI
HERE
HERMES
HIPHOP
HISAMITSU
HITACHI
HIV
HK
HKT
HM
HN
HOCKEY
HOLDINGS
HOLIDAY
HOMEDEPOT
HOMEGOODS
HOMES
HOMESENSE
HONDA
HORSE
HOSPITAL
HOST
HOSTING
HOT
HOTELS
HOTMAIL
HOUSE
HOW
HR
HSBC
HT
HU
HUGHES
HYATT
HYUNDAI
IBM
ICBC
ICE
ICU
ID
IE
IEEE
IFM
IKANO
IL
IM
IMAMAT
IMDB
IMMO
IMMOBILIEN
IN
INC
INDUSTRIES
INFINITI
INFO
ING
INK
INSTITUTE
INSURANCE
INSURE
INT
INTERNATIONAL
INTUIT
INVESTMENTS
IO
IPIRANGA
IQ
IR
IRISH
IS
ISMAILI
IST
ISTANBUL
IT
ITAU
ITV
JAGUAR
JAVA
JCB
JE
JEEP
JETZT
JEWELRY
JIO
JLL
JM
JMP
JNJ
JO
JOBS
JOBURG
JOT
JOY
JP
JPMORGAN
JPRS
JUEGOS
JUNIPER
KAUFEN
KDDI
KE
KERRYHOTELS
KERRYPROPERTIES
KFH
KG
KH
KI
KIA
KIDS
KIM
KINDLE
KITCHEN
KIWI
KM
KN
KOELN
KOMATSU
KOSHER
KP
KPMG
KPN
KR
KRD
KRED
KUOKGROUP
KW
KY
KYOTO
KZ
LA
LACAIXA
LAMBORGHINI
LAMER
LAND
LANDROVER
LANXESS
LASALLE
LAT
LATINO
LATROBE
LAW
LAWYER
LB
LC
LDS
LEASE
LECLERC
LEFRAK
LEGAL
LEGO
LEXUS
LGBT
LI
LIDL
LIFE
LIFEINSURANCE
LIFESTYLE
LIGHTING
LIKE
LILLY
LIMITED
LIMO
LINCOLN
LINK
LIVE
LIVING
LK
LLC
LLP
LOAN
LOANS
LOCKER
LOCUS
LOL
LONDON
LOTTE
LOTTO
LOVE
LPL
LPLFINANCIAL
LR
LS
LT
LTD
LTDA
LU
LUNDBECK
LUXE
LUXURY
LV
LY
MA
MADRID
MAIF
MAISON
MAKEUP
MAN
MANAGEMENT
MANGO
MAP
MARKET
MARKETING
MARKETS
MARRIOTT
MARSHALLS
MATTEL
MBA
MC
MCKINSEY
MD
ME
MED
MEDIA
MEET
MELBOURNE
MEME
MEMORIAL
MEN
MENU
MERCKMSD
MG
MH
MIAMI
MICROSOFT
MIL
MINI
MINT
MIT
MITSUBISHI
MK
ML
MLB
MLS
MM
MMA
MN
MO
MOBI
MOBILE
MODA
MOE
MOI
MOM
MONASH
MONEY
MONSTER
MORMON
MORTGAGE
MOSCOW
MOTO
MOTORCYCLES
MOV
MOVIE
MP
MQ
MR
MS
MSD
MT
MTN
MTR
MU
MUSEUM
MUSIC
MV
MW
MX
MY
MZ
NA
NAB
NAGOYA
NAME
NAVY
NBA
NC
NE
NEC
NET
NETBANK
NETFLIX
NETWORK
NEUSTAR
NEW
NEWS
NEXT
NEXTDIRECT
NEXUS
NF
NFL
NG
NGO
NHK
NI
NICO
NIKE
NIKON
NINJA
NISSAN
NISSAY
NL
NO
NOKIA
NORTON
NOW
NOWRUZ
NOWTV
NP
NR
NRA
NRW
NTT
NU
NYC
NZ
OBI
OBSERVER
OFFICE
OKINAWA
OLAYAN
OLAYANGROUP
OLLO
OM
OMEGA
ONE
ONG
ONL
ONLINE
OOO
OPEN
ORACLE
ORANGE
ORG
ORGANIC
ORIGINS
OSAKA
OTSUKA
OTT
OVH
PA
PAGE
PANASONIC
PARIS
PARS
PARTNERS
PARTS
PARTY
PAY
PCCW
PE
PET
PF
PFIZER
PG
PH
PHARMACY
PHD
PHILIPS
PHONE
PHOTO
PHOTOGRAPHY
PHOTOS
PHYSIO
PICS
PICTET
PICTURES
PID
PIN
PING
PINK
PIONEER
PIZZA
PK
PL
PLACE
PLAY
PLAYSTATION
PLUMBING
PLUS
PM
PN
PNC
POHL
POKER
POLITIE
PORN
POST
PR
PRAXI
PRESS
PRIME
PRO
PROD
PRODUCTIONS
PROF
PROGRESSIVE
PROMO
PROPERTIES
PROPERTY
PROTECTION
PRU
PRUDENTIAL
PS
PT
PUB
PW
PWC
PY
QA
QPON
QUEBEC
QUEST
RACING
RADIO
RE
READ
REALESTATE
REALTOR
REALTY
RECIPES
RED
REDUMBRELLA
REHAB
REISE
REISEN
REIT
RELIANCE
REN
RENT
RENTALS
REPAIR
REPORT
REPUBLICAN
REST
RESTAURANT
REVIEW
REVIEWS
REXROTH
RICH
RICHARDLI
RICOH
RIL
RIO
RIP
RO
ROCKS
RODEO
ROGERS
ROOM
RS
RSVP
RU
RUGBY
RUHR
RUN
RW
RWE
RYUKYU
SA
SAARLAND
SAFE
SAFETY
SAKURA
SALE
SALON
SAMSCLUB
SAMSUNG
SANDVIK
SANDVIKCOROMANT
SANOFI
SAP
SARL
SAS
SAVE
SAXO
SB
SBI
SBS
SC
SCB
SCHAEFFLER
SCHMIDT
SCHOLARSHIPS
SCHOOL
SCHULE
SCHWARZ
SCIENCE
SCOT
SD
SE
SEARCH
SEAT
SECURE
SECURITY
SEEK
SELECT
SENER
SERVICES
SEVEN
SEW
SEX
SEXY
SFR
SG
SH
SHANGRILA
SHARP
SHELL
SHIA
SHIKSHA
SHOES
SHOP
SHOPPING
SHOUJI
SHOW
SI
SILK
SINA
SINGLES
SITE
SJ
SK
SKI
SKIN
SKY
SKYPE
SL
SLING
SM
SMART
SMILE
SN
SNCF
SO
SOCCER
SOCIAL
SOFTBANK
SOFTWARE
SOHU
SOLAR
SOLUTIONS
SONG
SONY
SOY
SPA
SPACE
SPORT
SPOT
SR
SRL
SS
ST
STADA
STAPLES
STAR
STATEBANK
STATEFARM
STC
STCGROUP
STOCKHOLM
STORAGE
STORE
STREAM
STUDIO
STUDY
STYLE
SU
SUCKS
SUPPLIES
SUPPLY
SUPPORT
SURF
SURGERY
SUZUKI
SV
SWATCH
SWISS
SX
SY
SYDNEY
SYSTEMS
SZ
TAB
TAIPEI
TALK
TAOBAO
TARGET
TATAMOTORS
TATAR
TATTOO
TAX
TAXI
TC
TCI
TD
TDK
TEAM
TECH
TECHNOLOGY
TEL
TEMASEK
TENNIS
TEVA
TF
TG
TH
THD
THEATER
THEATRE
TIAA
TICKETS
TIENDA
TIPS
TIRES
TIROL
TJ
TJMAXX
TJX
TK
TKMAXX
TL
TM
TMALL
TN
TO
TODAY
TOKYO
TOOLS
TOP
TORAY
TOSHIBA
TOTAL
TOURS
TOWN
TOYOTA
TOYS
TR
TRADE
TRADING
TRAINING
TRAVEL
TRAVELERS
TRAVELERSINSURANCE
TRUST
TRV
TT
TUBE
TUI
TUNES
TUSHU
TV
TVS
TW
TZ
UA
UBANK
UBS
UG
UK
UNICOM
UNIVERSITY
UNO
UOL
UPS
US
UY
UZ
VA
VACATIONS
VANA
VANGUARD
VC
VE
VEGAS
VENTURES
VERISIGN
VERSICHERUNG
VET
VG
VI
VIAJES
VIDEO
VIG
VIKING
VILLAS
VIN
VIP
VIRGIN
VISA
VISION
VIVA
VIVO
VLAANDEREN
VN
VODKA
VOLVO
VOTE
VOTING
VOTO
VOYAGE
VU
WALES
WALMART
WALTER
WANG
WANGGOU
WATCH
WATCHES
WEATHER
WEATHERCHANNEL
WEBCAM
WEBER
WEBSITE
WED
WEDDING
WEIBO
WEIR
WF
WHOSWHO
WIEN
WIKI
WILLIAMHILL
WIN
WINDOWS
WINE
WINNERS
WME
WOLTERSKLUWER
WOODSIDE
WORK
WORKS
WORLD
WOW
WS
WTC
WTF
XBOX
XEROX
XIHUAN
XIN
XN--11B4C3D
XN--1CK2E1B
XN--1QQW23A
XN--2SCRJ9C
XN--30RR7Y
XN--3BST00M
XN--3DS443G
XN--3E0B707E
XN--3HCRJ9C
XN--3PXU8K
XN--42C2D9A
XN--45BR5CYL
XN--45BRJ9C
XN--45Q11C
XN--4DBRK0CE
XN--4GBRIM
XN--54B7FTA0CC
XN--55QW42G
XN--55QX5D
XN--5SU34J936BGSG
XN--5TZM5G
XN--6FRZ82G
XN--6QQ986B3XL
XN--80ADXHKS
XN--80AO21A
XN--80AQECDR1A
XN--80ASEHDB
XN--80ASWG
XN--8Y0A063A
XN--90A3AC
XN--90AE
XN--90AIS
XN--9DBQ2A
XN--9ET52U
XN--9KRT00A
XN--B4W605FERD
XN--BCK1B9A5DRE4C
XN--C1AVG
XN--C2BR7G
XN--CCK2B3B
XN--CCKWCXETD
XN--CG4BKI
XN--CLCHC0EA0B2G2A9GCD
XN--CZR694B
XN--CZRS0T
XN--CZRU2D
XN--D1ACJ3B
XN--D1ALF
XN--E1A4C
XN--ECKVDTC9D
XN--EFVY88H
XN--FCT429K
XN--FHBEI
XN--FIQ228C5HS
XN--FIQ64B
XN--FIQS8S
XN--FIQZ9S
XN--FJQ720A
XN--FLW351E
XN--FPCRJ9C3D
XN--FZC2C9E2C
XN--FZYS8D69UVGM
XN--G2XX48C
XN--GCKR3F0F
XN--GECRJ9C
XN--GK3AT1E
XN--H2BREG3EVE
XN--H2BRJ9C
XN--H2BRJ9C8C
XN--HXT814E
XN--I1B6B1A6A2E
XN--IMR513N
XN--IO0A7I
XN--J1AEF
XN--J1AMH
XN--J6W193G
XN--JLQ480N2RG
XN--JVR189M
XN--KCRX77D1X4A
XN--KPRW13D
XN--KPRY57D
XN--KPUT3I
XN--L1ACC
XN--LGBBAT1AD8J
XN--MGB9AWBF
XN--MGBA3A3EJT
XN--MGBA3A4F16A
XN--MGBA7C0BBN0A
XN--MGBAAM7A8H
XN--MGBAB2BD
XN--MGBAH1A3HJKRD
XN--MGBAI9AZGQP6J
XN--MGBAYH7GPA
XN--MGBBH1A
XN--MGBBH1A71E
XN--MGBC0A9AZCG
XN--MGBCA7DZDO
XN--MGBCPQ6GPA1A
XN--MGBERP4A5D4AR
XN--MGBGU82A
XN--MGBI4ECEXP
XN--MGBPL2FH
XN--MGBT3DHD
XN--MGBTX2B
XN--MGBX4CD0AB
XN--MIX891F
XN--MK1BU44C
XN--MXTQ1M
XN--NGBC5AZD
XN--NGBE9E0A
XN--NGBRX
XN--NODE
XN--NQV7F
XN--NQV7FS00EMA
XN--NYQY26A
XN--O3CW4H
XN--OGBPF8FL
XN--OTU796D
XN--P1ACF
XN--P1AI
XN--PGBS0DH
XN--PSSY2U
XN--Q7CE6A
XN--Q9JYB4C
XN--QCKA1PMC
XN--QXA6A
XN--QXAM
XN--RHQV96G
XN--ROVU88B
XN--RVC1E0AM3E
XN--S9BRJ9C
XN--SES554G
XN--T60B56A
XN--TCKWE
XN--TIQ49XQYJ
XN--UNUP4Y
XN--VERMGENSBERATER-CTB
XN--VERMGENSBERATUNG-PWB
XN--VHQUV
XN--VUQ861B
XN--W4R85EL8FHU5DNRA
XN--W4RS40L
XN--WGBH1C
XN--WGBL6A
XN--XHQ521B
XN--XKC2AL3HYE2A
XN--XKC2DL3A5EE0H
XN--Y9A3AQ
XN--YFRO4I67O
XN--YGBI2AMMX
XN--ZFR164B
XXX
XYZ
YACHTS
YAHOO
YAMAXUN
YANDEX
YE
YODOBASHI
YOGA
YOKOHAMA
YOU
YOUTUBE
YT
YUN
ZA
ZAPPOS
ZARA
ZERO
ZIP
ZM
ZONE
ZUERICH
ZW
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Do the simplest thing that could possibly work]]></title>
            <link>https://www.seangoedecke.com/the-simplest-thing-that-could-possibly-work/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45068091</guid>
            <description><![CDATA[When designing software systems, do the simplest thing that could possibly work. It’s surprising how far you can take this piece of advice. I genuinely think…]]></description>
            <content:encoded><![CDATA[When designing software systems, do the simplest thing that could possibly work.
It’s surprising how far you can take this piece of advice. I genuinely think you can do this all the time. You can follow this approach for fixing bugs, for maintaining existing systems, and for architecting new ones.
A lot of engineers design by trying to think of the “ideal” system: something well-factored, near-infinitely scalable, elegantly distributed, and so on. I think this is entirely the wrong way to go about software design. Instead, spend that time understanding the current system deeply, then do the simplest thing that could possibly work.
Simple can be underwhelming
System design requires competence with a lot of different tools: app servers, proxies, databases, caches, queues, and so on. As they gain familiarity with these tools, junior engineers naturally want to use them. It’s fun to construct systems out of many different components! And it feels very satisfying to draw boxes and arrows on a whiteboard - like you’re doing real engineering.
However, as with many skills, real mastery often involves learning when to do less, not more. The fight between an ambitious novice and an old master is a well-worn cliche in martial arts movies: the novice is a blur of motion, flipping and spinning. The master is mostly still. But somehow the novice’s attacks never seem to quite connect, and the master’s eventual attack is decisive.
In software, this means that great software design looks underwhelming. It doesn’t look like anything much is happening at all. You can tell you’re in the presence of great software design because you start having thoughts like “oh, I didn’t realise the problem was that easy” or “oh nice, you don’t actually have to do anything difficult”.
Unicorn is great software design, because it delivers all the most important guarantees in a web server (request isolation, horizontal scaling, crash recovery) by leaning on Unix primitives1. The industry-standard Rails REST API is great software design, because it gives you exactly what you need for a CRUD app in the most boring way possible. I don’t think any of these are impressive software. But they’re impressive feats of design, because they do the simplest thing that could possibly work.
You should do that too! Suppose you’ve got a Golang application that you want to add some kind of rate limiting to. What’s the simplest thing that could possibly work? Your first idea might be to add some kind of persistent storage (say, Redis) to track per-user request counts with a leaky-bucket algorithm. That would work! But do you need a whole new piece of infrastructure? What if instead you kept those per-user request counts in-memory? Sure, you’d lose some rate limiting data when the application is restarted, but does that matter? Actually, are you sure your edge proxy2 doesn’t support rate limiting already? Could you just write a couple of lines in a config file instead of implementing the feature at all?
Maybe your edge proxy doesn’t support rate limiting. Maybe you can’t track it in-memory because you have too many server instances running in parallel, so the tightest rate limit you could enforce that way is too wide. Maybe it’s a dealbreaker if you ever lose rate limiting data, because people are hammering your service that hard. In that case, the simplest thing that could possibly work is adding persistent storage, so you should go and do that. But if you could do one of the easier approaches, wouldn’t you want to?
You really can build a whole application from scratch this way: start with the absolute simplest thing, and then only extend it when you have new requirements that force you to. It sounds silly, but it works. Think of it as taking YAGNI as the ultimate design principle: above single-responsibility, above choosing the best tool for the job, and above “good design”.
What’s wrong with doing the simplest thing?
Of course, there are three big problems with always doing the simplest thing that could possibly work. The first is that, by not anticipating future requirements, you end up with an inflexible system or a big ball of mud. The second is that it’s not clear what “simplest” means, so at worst I’m saying “to design well, always do good design”. The third is that you ought to be building systems that can scale, not systems that just work right now. Let’s take those objections in order.
Big balls of mud
To some engineers, “do the simplest thing that could possibly work” sounds like I’m telling them to stop doing engineering. If the simplest thing is usually a quick kludge, does that mean this advice will inevitably lead to a complete mess? We’ve all seen codebases with hacks stacked on top of hacks, and they definitely don’t look like good design.
But are hacks simple? I actually don’t think so. The problem with a hack or a kludge is precisely that it isn’t simple: that it adds complexity to the codebase by introducing another thing you have to always remember. Hacks are just easier to think of. Figuring out the proper fix is hard because it requires having to understand the entire codebase (or large sections of it). In fact, the proper fix is almost always much simpler than the hack.
It is not easy to do the simplest thing that could possibly work. When you’re looking at a problem, the first few solutions that come to mind are unlikely to be the simplest ones. Figuring out the simplest solution requires considering many different approaches. In other words, it requires doing engineering.
What is simplicity?
Engineers disagree a lot about what constitutes simple code. If “simplest” already means “with good design”, is it just a tautology to say “you should do the simplest thing that could possibly work?” In other words, is Unicorn really simpler than Puma3? Is adding in-memory rate limiting really simpler than using Redis? Here’s a rough, intuitive definition of simplicity4:

Simple systems have fewer “moving pieces”: fewer things you have to think about when you’re working with them
Simple systems are less internally-connected. They are composed from components with clear, straightforward interfaces

Unix processes are simpler than threads (and thus Unicorn is simpler than Puma) because processes are less connected: they do not share memory. This makes a lot of sense to me! But I don’t think it gives you the tools to figure out what’s simpler in every case.
What about in-memory rate limiting vs Redis? On the one hand, in-memory is simpler because you don’t have to think about all the things involved in standing up a separate service with persistent memory. On the other hand, Redis is simpler because the rate limiting guarantees it offers are more straightforward - you don’t have to worry about the case where one server instance thinks a user is rate limited and another one doesn’t.
When I’m not sure what “seems” simpler to me, I like to use this tiebreaker: simple systems are stable. If you’re comparing two states of a software system, and one will require more ongoing work if no requirements change, the other one is simpler. Redis must be deployed and maintained, it can have its own incidents, it requires its own monitoring, it requires a separate deployment in any new environments the service finds itself in, and so on. Thus in-memory rate limiting is simpler than Redis5.
Why wouldn’t you want to be scalable?
A certain type of engineer is now screaming to themselves “but in-memory rate limiting won’t scale!” Doing the simplest thing that could possibly work will emphatically not deliver the most web-scale system. It will deliver a system that works well at the current scale. Is this irresponsible engineering?
No. In my view, the cardinal sin of big tech SaaS engineering is an obsession with scale. I’ve seen so much unavoidable pain caused by over-engineering systems to prepare for several orders of magnitude more than the current scale.
The main reason to not try this is that it doesn’t work. In my experience, for any non-trivial codebase, you can’t anticipate how it will behave at several orders of magnitude more traffic, because you don’t know ahead of time where all the bottlenecks are going to be. At most you can try to make sure you’re ready for 2x or 5x the current traffic, and then stand by to deal with problems as they come in.
The other reason not to try this is that it makes your codebase inflexible. It’s fun to decouple your service into two pieces so they can be scaled independently (I have seen this happen maybe ten times, and I have seen them actually be usefully scaled independently maybe once). But that makes certain features very hard to implement, because they now require coordination over the wire. In the worst case, they require transactions over the wire, which is a genuinely hard engineering problem. Most of the time you just don’t have to do any of this!
Final thoughts
The longer I spend working in tech, the less optimistic I become about our collective ability to predict where a system is going. It’s hard enough to get your head around where a system currently is. And in fact, that’s the main practical difficulty in doing good design: getting an accurate big-picture understanding of the system. Most design is done without that understanding, and most design is thus pretty bad.
There are, broadly speaking, two ways to develop software. The first is to predict what your requirements might look like six months or a year from now, and then design the best system for that purpose. The second is to design the best system for what your requirements actually look like right now: in other words, to do the simplest thing that could possibly work.
edit: this article has gotten some comments on Hacker News.
One interesting comment thread says that simplicity of architecture doesn’t matter at scale, because the complexity of “state space exploration in implementation” (I think that means something like what I wrote about here) dominates any other complexity. I disagree - the more complex your feature interactions become, the more important a simple architecture becomes, because your “complexity budget” is almost exhausted.
I also want to credit Ward Cunningham and Kent Beck for inventing the expression - I genuinely thought I’d just come up with the wording myself, but I almost certainly just remembered it. Oops! Thanks to the HN user ternaryoperator for pointing this out.




It’s just Unix sockets and forked processes! I love Unicorn.
↩


Every tech company has some kind of edge proxy.
↩


I do like Puma and think it’s a good web server. There are definitely use cases where you’d pick it over Unicorn (though in those cases I would personally think hard about using a different language than Ruby).
↩


I’m influenced here by Rich Hickey’s great talk Simple Made Easy. I don’t agree with all of it (I think familiarity does in fact contribute to simplicity in practice) but it’s definitely worth watching.
↩


Of course, if the system has to scale horizontally more than a little bit, in-memory rate limiting won’t work and must be replaced with something like Redis. But in my experience a Golang service can scale a lot without having to scale horizontally to more than a handful of replicas.
↩


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Income Equality in Nordic Countries: Myths, Facts, and Lessons]]></title>
            <link>https://www.aeaweb.org/articles?id=10.1257/jel.20251636</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45067423</guid>
            <description><![CDATA[Income Equality in the Nordic Countries: Myths, Facts, and Lessons by Magne Mogstad, Kjell G. Salvanes and Gaute Torsvik. Published in volume 63, issue 3, pages 791-839 of Journal of Economic Literature, September 2025, Abstract: Policymakers, public commentators, and researchers often cite the Nord...]]></description>
            <content:encoded><![CDATA[

    
    


    Menu
    



    
    
            
    
         Kjell G. Salvanes    
    
         Gaute Torsvik    

    

		

			Journal of Economic Literature 

			                

		
                (pp. 791–839)
		
    
	
    
        
            Download Full Text PDF 
                    
    

	
	
    

		   
				
					Article Information
				
											
		
		
        

							
					Abstract
					Policymakers, public commentators, and researchers often cite the Nordic countries as examples of a socioeconomic model that combines low income inequality with prosperity and growth. This article critically assesses that claim by integrating theoretical perspectives and empirical evidence to explain how the Nordic model functions and why these countries experience low inequality. Our analysis suggests that income equality in the Nordics is largely driven by a significant compression of hourly wages, reducing returns to labor market skills and education. This appears to result from a wage bargaining system characterized by strong coordination within and across industries. This finding challenges other commonly cited explanations for Nordic income equality, such as redistribution through the tax transfer system, public spending on goods that complement employment, and public policies promoting equal skills and human capital. We consider broader lessons for economies aiming to reduce inequality and conclude by highlighting several under-explored or unresolved questions.				
			
							
					Citation
					

                        Mogstad, Magne, Kjell G. Salvanes, and Gaute Torsvik.
						2025.

						
						
							"Income Equality in the Nordic Countries: Myths, Facts, and Lessons."
						
						Journal of Economic Literature
							  
					63 (3):
					 791–839.
				  
						DOI: 10.1257/jel.20251636
					

					
				
								
					
						
							Additional Materials

						
						
							                    
                Replication Package                                            
                            
                Author Disclosure Statement(s)                                            
        
						
					


				
									
						JEL Classification
						
														
								D31
								Personal Income, Wealth, and Their Distributions
															
								E23
								Macroeconomics: Production
															
								H23
								Taxation and Subsidies: Externalities; Redistributive Effects; Environmental Taxes and Subsidies
															
								J24
								Human Capital; Skills; Occupational Choice; Labor Productivity
															
								J31
								Wage Level and Structure; Wage Differentials
															
								J52
								Dispute Resolution: Strikes, Arbitration, and Mediation; Collective Bargaining
															
						
					
				
			

		
		
        
		  
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[John Carmack's arguments against building a custom XR OS at Meta]]></title>
            <link>https://twitter.com/ID_AA_Carmack/status/1961172409920491849</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066395</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The web does not need gatekeepers: Cloudflare’s new “signed agents” pitch]]></title>
            <link>https://positiveblue.substack.com/p/the-web-does-not-need-gatekeepers</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066258</guid>
        </item>
        <item>
            <title><![CDATA[Wikipedia as a Graph]]></title>
            <link>https://wikigrapher.com/paths</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45066060</guid>
        </item>
        <item>
            <title><![CDATA[Essential Coding Theory [pdf]]]></title>
            <link>https://cse.buffalo.edu/faculty/atri/courses/coding-theory/book/web-coding-book.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45065705</guid>
        </item>
        <item>
            <title><![CDATA[Deploying DeepSeek on 96 H100 GPUs]]></title>
            <link>https://lmsys.org/blog/2025-05-05-large-scale-ep/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45064329</guid>
            <description><![CDATA[<p>DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which us...]]></description>
            <content:encoded><![CDATA[DeepSeek is a popular open-source large language model (LLM) praised for its strong performance. However, its large size and unique architecture, which uses Multi-head Latent Attention (MLA) and Mixture of Experts (MoE), require an advanced system for efficient serving at scale. In this blog, we explain how we match DeepSeek's inference system performance with SGLang.

Our implementation, shown in the figure above, runs on 12 nodes in the Atlas Cloud, each equipped with 8 H100 GPUs.
It uses prefill-decode disaggregation and large-scale expert parallelism (EP), achieving a speed of 52.3k input tokens per second and 22.3k output tokens per second per node for 2000-token input sequences.
To the best of our knowledge, this represents the first open-source implementation to nearly match the throughput reported in the official DeepSeek blog at large scale.
By deploying this implementation locally, it translates to a cost of $0.20/1M output tokens, which is about one-fifth the cost of the official DeepSeek Chat API.
Compared to vanilla tensor parallelism using the same resources, this optimized strategy improves the output throuhgput by up to 5x.
This blog dives into our parallelism design, optimization methods, and results. All components of our work are fully open-source, allowing others to explore and build on our efforts. The instructions for reproducing our experiments are fully available here.
Highlight
✅ SGLang now supports prefill-decode (PD) disaggregation and large-scale EP, including the full functionality of DeepEP, DeepGEMM, and EPLB.
✅ Leveraging these new features, our team successfully replicated DeepSeek's inference system using 12 nodes, each with 8 H100 GPUs. In total, SGLang achieves a throughput of 52.3k input tokens per second and 22.3k output tokens per second per node for input sequences of 2000 tokens.
✅ This blog explains technical details of our approach, focusing on optimizations for efficiency, peak memory usage reduction, and workload balancing. The profile results show that our implementation achieves nearly on-par performance with the official DeepSeek’s report.
✅ All experiments and code are fully open-sourced for community access and further development.
Outline

Parallelism Design
Prefill and Decode Disaggregation
Large-scale Expert Parallelism
Evaluation
Toolkits
Limitations and Future Work
Conclusion
Acknowledgment

Parallelism Design
Efficient parallelism is essential to manage the computational complexity and memory demands of DeepSeek's architecture. This section outlines our approach to optimizing key components: attention layers, dense feed-forward networks (FFNs), sparse FFNs, and the language model (LM) head. Each component leverages tailored parallelism strategies to enhance scalability, memory efficiency, and performance.
Attention Layers
DeepSeek employs Multi-head Latent Attention (MLA) to effectively model complex dependencies within input sequences. To optimize this mechanism, we implement DP Attention, a data parallelism strategy that eliminates KV cache duplication across devices, significantly reducing memory overhead. Introduced in SGLang v0.4, this approach has been extended to support hybrid data and tensor parallelism, offering flexibility for processing small batch sizes efficiently.
Dense FFNs
Despite using only three dense FFN layers, DeepSeek-V3's computation can significantly increase peak memory usage, potentially leading to system crashes if not carefully managed. To address this, we adopt Data Parallelism (DP) over tensor parallelism (TP), leveraging the following advantages:

Enhanced Scalability: With an intermediate dimension of 18,432, high TP degrees (e.g., TP32) result in inefficient fragmentation into small-unit segments (e.g., 576 units), which are not divisible by 128—a common alignment boundary for modern GPUs such as H100. This misalignment hampers computational efficiency and memory utilization. DP provides a more scalable solution by avoiding fragmentation, ensuring balanced workload distribution across devices.
Optimized Memory Efficiency: Traditionally, TP reduces memory usage as worker size increases, but this advantage diminishes under DP attention. In a pure TP setup, memory demand for a single-layer Transformer model scales with DP size as: $$\text{Memory}=\frac{N_{\text{param}}}{\text{TP}}+(1+k)N_{\text{hidden_state}}\cdot \text{DP}\notag$$ Here, $N_{\text{hidden_state}}=n_\text{token}\times n_\text{hidden_size}$ is the size of the hidden state on each device (DP rank), $N_{\text{param}}=n_\text{intermediate_size}\times n_\text{hidden_size}$ is the number of model parameters, and $k$ is a coefficient representing extra memory overhead from CUDA Graph duplication. By assuming $\text{DP}=\text{TP}$, this memory usage function is minimized when $\text{TP}=\sqrt{\frac{N_{\text{param}}}{(1+k)N_{\text{hidden_state}}}}$. DeepSeek-V3 uses an intermediate size of 18,432. During the prefill phase, CUDA Graph is typically disabled, so $k = 0$. However, the token size per device can easily exceed 2,048, resulting in an optimal TP size of 3 or less. In the decode phase, a practical configuration might use 128 tokens per device and set $k = 3$. In this case, the memory-optimal TP size is 6. In both phases, a lower TP degree minimizes memory usage per device. As a result, DP may offer a more memory-efficient approach for scaling compared to relying solely on TP.
Minimized Communication Overhead: In pure TP, each FFN necessitates two all-reduce operations, resulting in substantial communication overhead. By leveraging DP, we optimize this process to a single reduce-scatter following the prior attention layer and an all-gather before the next, reducing communication costs by 50%. Furthermore, when attention is also computed under pure DP, inter-device communication is entirely eliminated, significantly enhancing overall efficiency.

The integration of DP dense FFN with DP attention is illustrated in the left figure below. Users can enable this feature by setting --moe-dense-tp-size=1.

Sparse FFNs
In DeepSeek-V3's Mixture of Experts (MoE) architecture, sparse FFNs require substantial expert weights, creating a significant memory bottleneck. To address this, we implement Expert Parallelism (EP), which distributes expert weights across multiple devices. This approach effectively scales memory capacity while maintaining high performance, though it does introduce challenges like irregular all-to-all communication and workload imbalance.
The figure in the right figure above illustrates our EP implementation using the DeepEP framework, with further details on our EP design and optimizations provided in the following sections.
LM Head
The LM head computes output probabilities over a large vocabulary, a resource-intensive operation traditionally handled with vocabulary parallelism to aggregate token logits from TP groups. To enhance scalability and efficiency, we adopt Data Parallelism (DP), mirroring our dense FFN strategy. This reduces memory overhead and simplifies communication across devices, delivering a more streamlined solution.
Prefill and Decode Disaggregation
LLM inference comprises two distinct phases: Prefill and Decode. The Prefill phase is computation-intensive, processing the entire input sequence, while the Decode phase is memory-intensive, managing the Key-Value (KV) cache for token generation. Traditionally, these phases are handled within a unified engine, where combined scheduling of prefill and decode batches introduces inefficiencies. To address these challenges, we introduce Prefill and Decode (PD) Disaggregation in SGLang.
Issues with Unified Scheduling
The conventional unified engine, which processes prefill and decode batches together, results in three significant problems:

Prefill Interruption: Incoming prefill batches frequently interrupt ongoing decode batches, causing substantial delays in token generation.
DP Attention Imbalance: In DP attention, one DP worker may process a prefill batch while another handles a decode batch simultaneously, leading to increased decode latency.
Incompatible with DeepEP: As we will discuss in a later section, DeepEP executes different dispatch modes for prefill and decode, making unified scheduling imcompatible with DeepEP.

PD Disaggregation resolves these by separating the two stages, enabling tailored optimizations for each.
Implementation Details
The PD Disaggregation design in SGLang, depicted in the diagram below, interleaves execution between a Prefill Server and a Decode Server:

Upon receiving an input request, the workflow proceeds as follows:

A Prefill Server and a Decode Server pair via a handshake, establishing a local sender and receiver, respectively.
The Decode Server pre-allocates the KV cache, signaling the Prefill Server to begin the model forward pass and compute the KV caches.
Once computed, the data transfers to the Decode Server, which handles iterative token generation.

This separation ensures each phase operates under optimal conditions, maximizing GPU resource utilization. To further enhance performance, our implementation incorporates:

Non-blocking Transfer: Data send and receive operations run in a background thread, keeping the scheduler’s event loop uninterrupted.
RDMA-Based Transfer: Remote Direct Memory Access (RDMA) leverages queue pairs for connections and scatter-gather elements (SGE) for efficient transfer of non-contiguous memory chunks.
Flexible API Integration: SGLang offers adaptable APIs that integrate high-performance RDMA libraries like Mooncake and NIXL, streamlining data transfers.

More details can be found in our design document.
Large-scale Expert Parallelism
Expert Parallelism with DeepEP
DeepEP, implemented by the DeepSeek team, is a communication library designed to streamline EP in MoE models. It tackles the challenge of efficiently routing tokens to specific experts across multiple GPUs. By providing optimized communication kernels, DeepEP reduces latency and boosts throughput, making it ideal for large-scale inference tasks.
DeepEP provides two specialized dispatch modes to address varying workload demands:

Normal Dispatch: Optimized for handling long input sequences, such as during the prefill phase, this mode prioritizes maximum computational throughput. However, it generates symbolic shapes that are incompatible with CUDA Graph, rendering it less effective for the decode phase, where kernel launch overhead becomes a significant bottleneck.
Low-Latency Dispatch: Tailored for generating output tokens during the decode phase, this mode prioritizes minimal delay to ensure real-time performance. It supports CUDA Graph but requires preallocating a fixed memory size. If the memory demand exceeds this preallocation, a runtime error occurs.

In SGLang, the integration of DeepEP provides auto mode that dynamically selects between these two dispatch modes based on the workload. However, without PD disaggregation, the auto mode faces a limitation: it cannot simultaneously support both normal dispatch (for prefill) and low-latency dispatch (for decode) within the same communication group. This restriction hinders its compatibility with DP attention, which is crucial for memory-efficient inference. The compatibility of each mode is outlined in the table below:



Mode
Long Input
Long Output
DP Attention
CUDA Graph




Normal
✅
❌
✅
❌


Low-Latency
❌
✅
✅
✅


Auto
✅
✅
❌
✅



PD disaggregation addresses this by separating prefill and decode phases, allowing normal dispatch for the prefill phase and low-latency dispatch for the decode phase, both under DP attention. This integration optimizes resource utilization and enhances overall performance by aligning the dispatch mode with the specific needs of each phase.
DeepGEMM Integration
DeepGEMM is another high-efficient library developed by the DeepSeek team, specifically designed to optimize computations in MoE models. It provides two specialized functions for handling MoE-related matrix multiplications (Grouped GEMMs), each tailored to different phases of the inference process.

Grouped GEMMs (contiguous layout): This kernel is designed for dynamic input shapes, making it ideal for the prefill phase of MoE inference. It processes inputs where the data for different experts is concatenated contiguously, allowing for flexible handling of varying input sizes.
Grouped GEMMs (masked layout): This kernel assumes a fixed input shape and uses a mask tensor to compute only the valid portions of the input. It is compatible with CUDA Graph, which optimizes kernel launches, making it well-suited for the decode phase where reducing overhead is critical.

DeepGEMM integrates smoothly with the dispatch modes of DeepEP:

For the contiguous layout kernel, which is used with normal dispatch in the prefill phase, an additional step is required. Since normal dispatch outputs a symbolic shape, a permutation is needed to transform the output into the contiguous format expected by the kernel. We referred to the LightLLM project and implemented a custom Triton kernel for efficient permutation. This kernel ensures that the output from normal dispatch is correctly rearranged, enabling smooth integration with the contiguous GEMM kernel.
The masked layout kernel pairs seamlessly with DeepEP’s low-latency dispatch, as both are optimized for the decode phase and support CUDA Graph.

SGLang also integrates DeepGEMM for MoE computation under tensor parallelism. Additionally, DeepGEMM provides a highly efficient general GeMM kernel, which can be activated in SGLang by setting the environment variable SGL_ENABLE_JIT_DEEPGEMM to 1, offering even greater computational efficiency for non-MoE operations.
Two-batch Overlap
In multi-node environments, limited communication bandwidth can significantly increase overall latency. To tackle this challenge, we implemented Two-batch Overlap (TBO) following DeepSeek's system design. TBO splits a single batch into two micro-batches, allowing computation and communication to overlap, which also lowers peak memory usage by halving the effective batch size. However, putting TBO into practice introduces specific implementation difficulties.
Implementation Challenges
Although DeepSeek released the design framework of TBO, there are two slight implementation challenges.

Code Complexity: Directly coding TBO can lead to duplicated logic for managing multiple micro-batches. This increases the complexity of the codebase, making it harder to maintain and prone to errors, especially as the number of micro-batches or overlapping scenarios grows.
Synchronization Issues in the Prefill Phase: Achieving effective overlap between computation and communication needs consideration when the normal dispatch in DeepEP block the CPU. This blocking behavior can stall the pipeline, leaving the GPU idle and undermining the performance benefits of TBO.

Abstraction for Clean Implementation
To create a more maintainable and reusable codebase, we use an abstraction layer consisting of operations and yield points. This method simplifies development by allowing us to write code as if handling a single micro-batch, while strategically pausing execution by inserting yield points to let other micro-batches proceed. It eliminates code duplication, reduces the potential need for variable postfixes, and efficiently manages cases where some executions complete at a layer's end while others have not. Additionally, it supports easy adaptation to different overlapping region choices or future enhancements, like a three-batch overlap, with minimal code changes. Below is a concise demonstration of this approach:
operations = [
    self._forward_attn,
    YieldOperation(),  # Pause execution for other micro-batches
    self._forward_dispatch,
    self._forward_mlp,
    YieldOperation(),  # Another pause point
    self._forward_combine,
]

# Process a single micro-batch without duplicating code
def _forward_attn(self, state):
    state.hidden_states = self.self_attn(state.hidden_states, ...)

Prefill Overlapping Implementation
We refine the launch order during the prefill phase to avoid CPU-blocking via the dispatch operation in DeepEP, even though we are using its asynchronous mode. Specifically:

The dispatch operation blocks the CPU until the GPU receives metadata from other ranks to allocate correctly sized tensors.
An improper implementation would leave the computation stream idle during this period, as no computation tasks are submitted to the GPU.

To optimize, we prioritize submitting computation tasks to the GPU before launching CPU-blocking communication. This ensures the GPU remains active during communication. As illustrated in the figure below, TBO with a proper launch order, indicated by bolded borders, avoids bubble caused by a CPU-blocking operation (i.e., normal dispatch).

Expert Parallelism Load Balancer
In MoE models, EP often leads to uneven workload distribution across GPUs. This imbalance forces the system to wait for the slowest GPU computation or communication, wasting compute cycles and increasing memory usage due to expert activations. As the number of GPUs (EP size) increases, the imbalance issue gets more severe.
To address this, DeepSeek developed the Expert Parallelism Load Balancer (EPLB). EPLB takes expert distribution statistics as input and computes an optimal arrangement of experts to minimize imbalance. Users can allocate redundant experts (e.g., 32 additional experts), which, when combined with the original 256, create a pool of 288 experts. This pool allows EPLB to strategically place or replicate experts—for instance, duplicating the most frequently used expert multiple times or grouping a moderately used expert with rarely used ones on a single GPU.
Beyond balancing workloads, EPLB offers greater flexibility in parallelism design. With the original 256 experts, parallelism sizes are restricted to powers of two. EPLB’s use of 288 experts enables more diverse configurations, such as parallelism sizes of 12 or 72.
In the figure below, we demonstrate the effects of scale and EPLB algorithm to the imbalance issue via simulation. We compute GPU balancedness as the ratio between mean computation time and maximum computation time for a MoE layer among GPUs, and we use the number of tokens for a GPU to estimate the computation time for it. As can be seen, utilization rate decreases when the system scales with the number of nodes, and enabling EPLB significantly improves the utilization.

EPLB for Real-World Serving
For EPLB to be effective, the input distribution must closely match the actual serving workload. Two strategies enhance this alignment:

Increasing Batch Size: Larger batches reduce random fluctuations in expert usage, which improves balance, which can be achieved by scaling the cluster or using techniques like Multi-Token Prediction (MTP).
Periodic Rebalancing: Regularly updating the expert arrangement leverages temporal locality but requires efficient reloading of experts. This necessitates minimizing the cost of expert reloading operations.

Even with EPLB, some imbalance is inevitable, making further optimization a valuable future direction.
Implementation of Rebalancing
SGLang implements expert rebalancing in three stages to ensure efficiency and minimal disruption:

System Loading Stage: Weights are optionally preloaded from disk to main memory for faster rebalancing or kept on disk with memory mapping (mmap) for reduced memory usage.
Rebalance Preparation Stage: Required weights are asynchronously transferred to device memory in the background, utilizing free DMA hardware engines without interrupting ongoing GPU operations.
Rebalance Execution Stage: A device-to-device copy updates the weights. This step can be further optimized through physical memory rebinding techniques.

This staged approach ensures that rebalancing is both efficient and non-disruptive, maintaining system performance during updates.
Evaluation
End-to-end Performance
Experimental Setup
We evaluated the end-to-end performance of different configurations of SGLang using DeepSeek-V3 on a cluster of 12 nodes, connected via InfiniBand and each equipped with 8 H100 GPUs. This evaluation highlights the throughput improvements enabled by our advanced optimization techniques. We compared the following four settings:

SGLang with TP16 x 6: Every two nodes are paired with an independent group, running DeepSeek-V3 inference with a TP size of 16 and DP attention.
SGLang with PD Disaggregation: This version incorporates PD disaggregation and full EP optimization. For the EPLB, we adopt a distribution matching the input/output data, as real-time serving statistics are unavailable.
SGLang with PD Disaggregation and simulated MTP: To simulate MTP’s effects, we firstly double the batch size and halve the Key-Value KV cache length to maintain the same workload for GroupedGeMM computation and memory access. Moreover, we insert dummy kernels after the real attention computation to ensure the attention phase takes the same time as in DeepSeek’s profile, accurately reflecting the slowdown caused by MTP’s attention mechanism. We conservatively assume a 70% acceptance rate under MTP.
DeepSeek Profile Results: Throughput estimates are derived from DeepSeek’s official profiling data.

Performance Analysis of Prefill and Decode Phases
To accommodate varying workload demands, we independently evaluated the prefill (P) and decode (D) phases, assuming unlimited resources for the non-tested phase to isolate and maximize the load on the tested nodes—mirroring the setup used by DeepSeek. The results are summarized below:

Prefill Phase: On 4 nodes (4×8×H100, EP32), the system achieved per-node throughputs of 57,674, 54,543, and 50,302 tokens per second for prompt lengths of 1K, 2K, and 4K, respectively. As shown in the bar chart below, this represents up to a 3.3× improvement over the TP16 baseline, largely attributable to the optimized GroupedGeMM kernel (DeepGEMM) and two-batch overlap. Assuming a perfectly balanced workload, our system’s throughput is within 5.6% of DeepSeek's official profile.
Decode Phase: Evaluated on 9 nodes (9×8×H100, EP72; half the scale of DeepSeek), the system achieved 22,282 tokens/sec per node for 2K inputs—representing a 5.2× speedup over the TP16 baseline. Under simulated MTP conditions—with attention kernels intentionally slowed to reflect real-world latency—the system sustained a high throughput of 17,373 tokens/sec per node for 4K inputs, just 6.6% below DeepSeek’s official profile. As shown in the figure on the right, these performance gains are largely attributed to 4× larger batch sizes enabled by EP, which enhances scalability by significantly reducing per-GPU memory consumption of model weights.


Profile Results
This section compares SGLang’s performance with DeepSeek’s inference system, aligning our experimental setup as closely as possible to DeepSeek’s production environment. We analyze overall throughput and detailed kernel breakdowns, benchmarking against DeepSeek’s blog and public profile data.
Overall Throughput
For prefill, we tested a scenario with 16,384 tokens per device and an input length of 4,096. Due to uncertainty in DeepSeek’s expert distribution, we evaluated two cases: one with default expert distribution and another with simulated perfect EPLB (random expert selection following group-limited routing semantics) as a performance upper bound.
The results are presented below:




DeepSeek Blog (excl. cache hit)
DeepSeek Profile
SGLang (Default)
SGLang + Simulated Perfect EPLB




Batch Size
N/A
16,384
16,384
16,384


Input Length
N/A
4,096
4,096
4,096


Throughput (per node)
32,206
62,713
50,302
59,337



DeepSeek’s profile reports a throughput roughly twice that of its production environment. SGLang with default expert imbalance is 20% slower than DeepSeek’s profile, while the simulated perfect EPLB case narrows the gap to 6%.
For decode, the results are shown below:




DeepSeek Blog
DeepSeek Profile
SGLang (Default)
SGLang + Simulated MTP (Slow Attention)




Batch Size
N/A
128
256
128


KV Cache Length
4,989
4,096
2,000
4,000


Number of Nodes
18
16
9
9


Throughput (per node)
14,800
18,598
22,282
17,373



Using half the nodes of DeepSeek, SGLang with simulated MTP is only slightly slower than DeepSeek’s profile. In a higher batch size setting (256 sequences, 2,000 input length), SGLang achieves 22,282 tokens per second per node, demonstrating strong scalability.
Detail Breakdown
The figure below breaks down kernel execution times for prefill, including unit test results as a theoretical upper bound:


Default EPLB: Communication kernels exhibit longer execution times and higher variance compared to DeepSeek’s profile, likely due to greater expert imbalance. This leads to extended computation stream bubbles, slowing down overall performance.
Simulated Perfect EPLB: This setup aligns more closely with DeepSeek’s profile, though discrepancies remain, indicating potential areas for optimization.
Comparison with Unit Tests: Both DeepSeek and SGLang have a communication time slower than unit test results, while the latter is achievable when disabling TBO, revealing a potential optimization direction if communication is the bottleneck.

SGLang’s decode kernel breakdown aligns closely with DeepSeek’s, as shown below:

Key observations include:

Combine Time Discrepancy: SGLang’s combine operation appears 2x slower than DeepSeek’s due to shorter attention computation, causing communication kernels to busy-wait. In the simulated slow attention experiment, combine time matches DeepSeek’s, confirming this hypothesis.
MoE Performance: SGLang’s MoE kernels are 25% slower, possibly because DeepSeek’s 18 nodes (versus our 9) distribute experts more efficiently, reducing memory access overhead for GEMM operations.
Dispatch Optimization Potential: Both DeepSeek and SGLang show dispatch times of ~0.17ms per layer, but unit tests with DeepEP reveal a potential of 0.06ms occupying SMs. Currently, dispatch spends significant time busy-waiting for data. Inserting slow dummy kernels between send/receive operations reduces dispatch time to 0.09ms, and in-flight duration analysis using unit test data suggests further improvements are possible.

While minor enhancements remain—primarily in kernel fusion under "Other Kernels"—SGLang’s decode performance is largely aligned with DeepSeek’s, with prefill optimization as the next focus.
Ablation Study: Two-batch Overlap
Impact of Batch Size and Attention Time
This section investigates TBO performance across varying batch sizes and simulated MTP scenarios.

TBO delivers two significant benefits in the prefill phase, as evidenced by throughput comparisons and memory usage optimizations:

Support for Larger Batch Sizes: In the vanilla configuration, each device processes up to 8,192 tokens before encountering out-of-memory (OOM) errors at 16,384 tokens. TBO mitigates this by optimizing memory usage for input tokens, enabling inference with batches as large as 16,384 tokens per device. This further boosts performance to 40.5% increase when comparing the TBO flag with all other configurations made optimal.
Enhanced Throughput: By overlapping computation (e.g., attention and MLP phases) with communication (e.g., DeepEP Combine and Dispatch), TBO achieves a 27% to 35% throughput increase compared to the vanilla setup, even when processing the same token count per device.

TBO’s impact in the decode phase varies by scenario, with performance tied to batch size and attention processing time:

Real Test Cases: Speedup in practical scenarios is contingent on batch size exceeding a threshold between 64 and 128 tokens. Below this, TBO yields minimal or negative gains (e.g., -27% at 32 tokens/device), as small decode batch sizes hinder kernel efficiency. The speedup reaches 25.5% at 256 tokens with a performance of 22,310 tokens per second.
Simulated MTP Scenario: TBO provides the most substantial speedup in simulated MTP cases when processing 128 requests to generate 256 tokens per decode step. This is due to prolonged attention processing time, which aligns computation (e.g., DP Attention layers) with DeepEP communication overhead (e.g., combine and dispatch steps). The evaluation shows a 35% speedup at 128 sequences/device, with throughput 17,552 tokens per second compared to 12,929 without TBO.

Detail Breakdown
We evaluated three prefill scenarios: TBO with 16k tokens per batch, TBO with 8k tokens, and no-TBO with 8k tokens. The figure below reveals key insights:

TBO Efficiency: Comparing the 8k cases, TBO improves overall efficiency by overlapping computation and communication, as expected.
Batch Size Impact: Reducing the batch size from 16k to 8k with TBO results in a slight slowdown, reflecting diminished kernel efficiency with smaller batches.
Kernel Performance: Interestingly, the no-TBO 8k case outperforms the TBO 16k case in per-kernel speed, despite both having an effective batch size of 8k for kernels. This may stem from reduced streaming multiprocessors (SMs) with TBO, potential noisy neighbor effects during overlap, or kernel incompatibility between computation and communication. These findings suggest future optimization directions for SGLang.


For the decode phase, we analyzed three configurations: TBO with a batch size of 256, no-TBO with 256, and no-TBO with 128. The time breakdown is shown below:

TBO vs. No-TBO (Batch Size 256): Without TBO, communication time increases significantly due to the lack of overlap. However, computation kernels, particularly GEMM, benefit from a larger effective batch size, resulting in faster execution.
TBO (256) vs. No-TBO (128): Comparing cases with the same kernel batch size, only non-overlapped communication slows down in the no-TBO setup, while computation remains consistent. Unlike prefill, decode communication kernels either fully utilize SMs (during send/receive) or none (during inflight waiting), avoiding resource contention with computation kernels.


Ablation Study: EPLB
This section evaluates the impact of the EPLB on system performance through overall throughput analysis and detailed case studies. Given EPLB's sensitivity to workload distribution and distribution shifts in production environments, we focus on qualitative and generalizable insights rather than real-world performance, which requires production data.
Overall Results
The figure below illustrates EPLB's effect on throughput in large-scale settings. EPLB delivers a significant speedup of 1.49x (prefill) and 2.54x (decode), as expected, due to its ability to mitigate workload imbalances across GPUs. As the number of ranks scales, imbalances grow, and EPLB effectively addresses this in our large-scale experiments, leading to notable throughput improvements.

Case Study: Workload Imbalance Versus Overall Throughput
To explore the relationship between workload imbalance and throughput, we conducted a case study using a decode experiment with 1800 input tokens, 100 output tokens, and a batch size of 256. Throughput and balancedness (average token count divided by maximum token count across experts) were plotted against decoding steps:

The results reveal a strong correlation between balancedness and throughput, emphasizing the importance of maintaining high balancedness for optimal performance.
Case Study: Expert Distribution Statistics
The following figure presents expert distribution statistics for prefill and decode sample data:

Key observations include:

Imbalance in Expert Usage: Most experts are infrequently used, while a small subset is heavily utilized, underscoring the inherent imbalance in MoE models.
Prefill vs. Decode Differences: Although prefill and decode distributions share similarities, notable differences exist. This supports the use of PD disaggregation, which enables distinct expert placements for each phase, optimizing performance.

These findings highlight EPLB's role in addressing workload imbalances and the value of tailoring expert placement to phase-specific demands.
Toolkits
Disposable Tensor
Memory management in PyTorch can be challenging due to persistent object references, especially in GPU-intensive workflows where CUDA memory is a scarce resource. Consider the following example:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    del hidden_state  # Attempt to free memory, but no effect due to external reference
    return linear2(nn.ReLU(intermediate_state))

hidden_state = ffn(hidden_state, linear1, linear2)

In this code, del hidden_state is intended to release the memory occupied by hidden_state after intermediate_state is computed. However, as hidden_state is still referenced outside the function, the del operation has no effect. This increases peak memory usage, risking performance slowdowns or out-of-memory errors.
SGLang addresses this with the DisposableTensor class, a subclass of torch.Tensor which introduces a dispose() method to explicitly and immediately release a tensor’s memory, circumventing Python’s reference counting limitations. Here’s how it works:
def ffn(hidden_state: torch.Tensor, linear1: nn.Linear, linear2: nn.Linear):
    intermediate_state = linear1(hidden_state)
    hidden_state.dispose()  # Immediately releases CUDA memory
    return linear2(nn.ReLU(intermediate_state))

# Wrap the tensor in DisposableTensor
hidden_state = DisposableTensor(hidden_state)
hidden_state = ffn(hidden_state, linear1, linear2)

By wrapping hidden_state in a DisposableTensor and calling dispose() when it’s no longer needed, the CUDA memory is freed right away. This ensures that memory is released as soon as the tensor’s role in the computation is complete, reducing peak memory usage and improving overall efficiency.
Expert Workload Extraction and Simulation
SGLang also includes a toolset for analyzing and simulating expert workload distribution in MoE models. This feature enables users to:

Dump Expert Workload Statistics: Extract either accumulated statistics or per-batch workload data. Accumulated stats support the EPLB manager for real-time optimization, while per-batch data provides granular insights for analysis and simulation.
Simulate Expert Utilization: Model expert balance across various configurations without requiring costly hardware or repeated trials. For instance, users can gather workload data from a modest setup (e.g., 2x8xH100 or 8xH200) and simulate the performance for a large-scale 22-node deployment.

This simulation capability allows users to evaluate how factors like rebalancing frequency, node count, or batch size impact system performance. It’s a cost-effective way to fine-tune configurations before scaling up.
Limitations and Future Work
While our implementation of SGLang for DeepSeek-V3 inference demonstrates significant throughput improvements, several limitations and areas for future enhancement remain:

Latency Optimization: The current focus on throughput leaves Time to First Token (TTFT) at 2–5 seconds and Inter-Token Latency (ITL) at approximately 100ms, requiring further optimizations for real-time use cases.
Sequence Length Constraints: Limited to shorter sequences due to the use of 96 GPUs. Expanding GPU resources would support longer sequences, essential for specific applications.
Multi-Token Prediction (MTP) Integration: SGLang supports MTP but lacks full integration with DP attention, reducing efficiency in mixed parallelism configurations.
EPLB Distribution: The experiments in this blog utilizes in-distribution data for Expert Parallelism Load Balancer (EPLB), which may not reflect real-world variability. Future work should experiment performances when having distribution shifts.
Flexible Tensor Parallelism (TP) Sizes: For DeepSeek-V3, memory-optimal TP sizes for dense FFNs are small but larger than 1. Currently, SGLang only supports pure TP or DP, leading to suboptimal memory use. Flexible TP options are needed.
Blackwell Support: Currently, our implementation supports only the NVIDIA Hopper architecture. We are actively working to extend compatibility to the next-generation Blackwell architecture. If you are interested in supporting or sponsoring this development, welcome to contact lmsys.org@gmail.com.

Conclusion
By leveraging PD disaggregation, EP, and a carefully crafted parallelism design, we’ve reproduced DeepSeek’s inference framework in SGLang with exceptional performance. Our open-source efforts—achieving 52.3k input tokens per second and 22.3k output tokens per second—demonstrate SGLang’s power for large-scale LLM inference. We invite the community to explore, replicate, and extend this work to push the boundaries of efficient AI deployment.
Acknowledgment
We would like to express our heartfelt gratitude to the following teams and collaborators:

SGLang Core Team and Community Contributors — Jingyi Chen, Cheng Wan, Liangsheng Yin, Baizhou Zhang, Ke Bao, Jiexin Liang, Xiaoyu Zhang, Yanbo Yang, Fan Yin, Chao Wang, Laixin Xie, Runkai Tao, Yuhong Guo, Kaihong Zhang, Lei Yu, Yu-Hsuan Tseng, Qilin Tian, Peng Zhang, Yi Zhang, Yineng Zhang, Byron Hsu, and many others.
Atlas Cloud Team —  Jerry Tang, Wei Xu, Simon Xue, Harry He, Eva Ma, and colleagues — for providing a 96-device NVIDIA H100 cluster and offering responsive engineering support.
NVIDIA Solution Architect Team — Xuting Zhou, Jinyan Chen, and colleagues — for their work on the seamless integration of expert parallelism.
NVIDIA Enterprise Product Team — Trevor Morris, Elfie Guo, Kaixi Hou, Kushan Ahmadian, and colleagues — for optimizing the DeepSeek R1 kernels.
LinkedIn Team — Biao He, Qingquan Song, Chunan Zeng, Yun Dai, Yubo Wang, and colleagues — for optimizing the Flash-Attention 3 backend.
Mooncake Team — Shangming Cai, Teng Ma, Mingxing Zhang, and colleagues — for their collaboration on PD disaggregation in SGLang.
FlashInfer Team — Zihao Ye, Yong Wu, Yaxing Cai — for additional DeepSeek R1 kernel optimizations.
Dynamo Team - Kyle Kranen, Vikram Sharma Mailthody, and colleagues - for extra support on PD disaggregation in SGLang.

Thank you all for your invaluable support and collaboration.
Appendix
Related PRs: #1970 #2925 #4068 #4165 #4232 #4390 #4435 #4521 #4654 #4767 #4770 #4836 #4880 #4957 #5068 #5085 #5295 #5415 #5432 #5435 #5530 #5558 #5561 #5626 #5657 #5805 #5819 #5890 DeepEP#142
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Flunking my Anthropic interview again]]></title>
            <link>https://taylor.town/flunking-anthropic</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45064284</guid>
            <description><![CDATA[I didn't misclick any buttons. My best wasn't good enough. I'm not good enough.]]></description>
            <content:encoded><![CDATA[
Here's a vague overview of what just happened:

I recently applied for
Anthropic's Developer Relations role.
My friend who works there gave me a glowing recommendation (thanks again,
dude!).
I completed their secret take-home assignment.
On top of their secret take-home assignment, I independently published
diggit.dev and a companion blogpost
about my [sincerely] positive experiences with Claude. I was hoping that
some unsolicited "extra credit" would make me look like an
exceptional/ambitious candidate.
I
posted diggit.dev to HackerNews
and it hit the frontpage!
I submitted my take-home assignment and my unsolicited extra credit.
They sent me
the "unfortunately" email.

Anthropic obviously didn't do anything wrong. I'm just bummed.
Claude Code truly is one of my favorite dev tools ever, and if you've suffered
through my talks/interviews, you're probably sick of my
enthusiasm for software. I was particularly excited to interview with Anthropic
because I respect their approach to responsible AI adoption. This very blog
is too often a crazed celebration of humans, of software, of AI, of progress, of
sincerity -- I, I felt like I was a perfect fit.
The first time I flunked an Anthropic interview (ca. 2022), I accidentally
clicked a wrong button during their automated coding challenge. It was easy to
swallow that failure. I made an honest mistake; I expect companies to reject
candidates who make honest mistakes during interviews.
This is different. I didn't misclick any buttons. My best wasn't good enough.
I'm not good enough.
This essay started as a fantasy: some hero at Anthropic reads this on HackerNews
and vouches for me and I get the job and I help them guide humanity toward
post-scarity AI abundance, forever and ever, amen. I'm ashamed of these
thoughts. It's the same folly of explaining to an ex-girlfriend why she's wrong
about her own experience.
Dating was difficult for me. I don't mind feeling ugly or low-status or whatever
-- I know my place. But it hurts to feel seen, feel considered, but ultimately
rejected due to mysterious forces: "He's cute, but he's too weird."
Yes, I'm weird. My eccentric habits have been an overall boon for my career, for
my relationships, for my well-being. But it's moments like these when I just
want to turn all my weird off. I want to be a square peg for this square hole
and do honest work and feed my family and help humanity thrive.
I can't turn my weird off, so I think I defensively dial it up sometimes. I
exaggerate my eccentricities. It's easy to swallow criticism when it isn't the
real me, when it isn't my best, when it's honest mistakes -- what a load
of crap. This is me. This is my best. Hello, world.
Now it's all coming back in waves, in gasps -- I spent so much of my life being
an unlikable jerk. Becoming somebody else has been slow/painful and I'm so
deeply afraid of regressing. Over the past decade, I've been striving to spread
joy, to do good, to be better. I'm trying so hard.
And all this keyboard vomit is a promise to myself that I'm not giving up. I
hate this feeling, and I'm staring these nightmares straight in their stupid
eyeballs, and they're not blinking. I am still alive, and I have so much more to
do.
I'm okay. I mean it. I don't need (or deserve) your sympathy. I'm so lucky to be
alive, at this time, at this place, in this body, with these people. My life is
great, and it will get even better if I keep putting in this effort.
Spewing my insides like this onto The Internet is terrifying, but I suspect many
strangers are facing similar feelings. It's rough out there. Whatever it is,
wherever you are, I hope this helps. You've got this. You're not alone, and
we're only human.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Sosumi.ai – Convert Apple Developer docs to AI-readable Markdown]]></title>
            <link>https://sosumi.ai/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063874</guid>
            <description><![CDATA[sosumi.ai provides Apple Developer documentation in an AI-readable format by converting JavaScript-rendered pages into Markdown.]]></description>
            <content:encoded><![CDATA[
            
                Disclaimer: This is an unofficial, independent project and is not affiliated with or
                endorsed by Apple Inc. “Apple”, “Xcode”, and related marks are trademarks of Apple Inc.
            
            
                This service is an accessibility-first, on‑demand renderer. It converts a single Apple Developer page to
                Markdown only when requested by a user. It does not crawl, spider, or bulk download; it does not attempt
                to bypass authentication or security; and it implements rate limiting to avoid imposing unreasonable
                load.
            
            
                Content is fetched transiently and may be cached briefly to improve performance (approximately 30
                minutes). No permanent archives are maintained. All copyrights and other rights in the underlying
                content remain with Apple Inc. Each page links back to the original source.
            
            
                Your use of this service must comply with Apple’s Terms of Use and applicable law. You are solely
                responsible for how you access and use Apple’s content through this tool. Do not use this service to
                circumvent technical measures or for redistribution.
            
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Grok Code Fast 1]]></title>
            <link>https://x.ai/news/grok-code-fast-1</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45063559</guid>
        </item>
        <item>
            <title><![CDATA[The Grammar According to West]]></title>
            <link>https://dwest.web.illinois.edu/grammar.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45039662</guid>
            <description><![CDATA[I have been accumulating observations about writing mathematics for many years.
These conclusions arose both from writing textbooks and from noting writing
errors commonly made by my thesis students and in papers submitted to journals.]]></description>
            <content:encoded><![CDATA[
 
 by Douglas B. West

Summary

I have been accumulating observations about writing mathematics for many years.
These conclusions arose both from writing textbooks and from noting writing
errors commonly made by my thesis students and in papers submitted to journals.


My first objective was to train my students, thereby reducing the time needed
to edit their theses.  As the document grew, I made it publicly available in
the hope that others may find it useful.  I have received a broad range of
responses, mostly positive.  If you don't find it useful (or if you object to
it on principle), then please ignore it.  I hope to make some writers of
mathematics (especially students and non-native speakers of English) aware of
issues they may not have considered; small changes can produce mathematical
writing that is easier to read by wider audiences.


After an introductory explanation of why care in
writing mathematics is needed, I discuss
(1) mathematical style,
(2) notation and terminology,
(3) punctuation and English grammar as used in
mathematical writing, and
(4) English usage for non-native speakers.
Some points are minor distinctions, but even these make mathematical writing
clearer when used consistently.  My intent is not to make writing rigid, but
rather to make it transparent to avoid distracting the reader by ambiguities or
awkwardness in the flow of the narrative.


I note that a number of other articles and even books have been published on
writing mathematics; see some of these here.
I list these just to make alternative viewpoints available, without judgment
pro or con.  One contrast to note is that my list below focuses on relatively
specific items.  I have not yet included much discussion of the general
structure of an article; some of these references discuss that.


Index of specific items
Introduction and Motivation

Live mathematical conversations use many shortcuts that are inappropriate in
precise mathematical writing.  The context is known by all participants, and
shortcuts evolve to save time.  Furthermore, the speaker can immediately
clarify ambiguity.  Without immediate access to the author, written mathematics
must use language more carefully.  Also, mathematical concepts are abstract,
without context from everyday experience, so the writing must be more
consistent to make the meaning clear.  Outside mathematics, imprecise writing
can still be understood because the objects and concepts discussed are familiar.


Some mathematicians object to some of my recommendations.  Many time-honored
practices in the writing of mathematics are grammatically incorrect.  These
mistakes in writing cause no difficulty for readers with sufficient
mathematical sophistication or familiarity with the subject, but it is
unnecessary to restrict the audience to such readers.  A bit of care leads to
clearer writing that makes mathematics more easily accessible and readable to a
wider and less specialized audience.


Some languages have conventions of usage or grammar that lead to typical errors
in English mathematical writing by their native speakers.  I discuss some
such items in a separate section at the end.  I use some terminology for
English parts of speech and punctuation in the explanations.  I hope that
readers who are unfamiliar with these terms will still benefit from seeing 
the recommendations.


I apologize in advance for my own grammatical errors.  Habits die hard, and it
is easy to err in applying principles of writing.  In particular, there are
inconsistencies between what I propose here and what I wrote in my earlier
books.  Those books were written in the previous millennium, and I have learned
many things about writing since then.  Also, I am a speaker of American
English, and some points are consistently different in British English (such as
the treatment of "which" vs. "that" and the aversion to serial commas).


Some of my conclusions conflict with manuals of English style.  My conclusions
are intended to produce clear mathematical writing that is more logically
consistent than publishers' conventions.  This applies especially to
punctuation and to words that serve as logical connectives.


I welcome corrections, suggestions/inquiries, and "pet peeves" that may lead to
further items in later versions of this guide.


Mathematical style



Abstract, Introduction, and Conclusion.
We begin with the overall structure of a research article in mathematics.
The abstract states the results as fully as possible in a brief presentation.
Crucial specialized terms the reader needs to know to understand the statements
should be defined.  The abstract stands on its own, especially in the age of
electronic communication where it may be separate from the rest of the paper,
and hence it contains no numbered reference to the bibliography.


The first section of the paper is an "Introduction" that should motivate the
problem, discuss related results, state the results more completely, and
perhaps summarize the techniques or the structure of the paper or crucial
definitions.

 The introduction should also contain any concluding remarks or key
conjectures.  There is generally little or no value in a separate section of
concluding remarks.  Such remarks either are redundant or contain information
that readers usually seek in the introduction.  Readers who study the full
details of the proofs have no need of a summary in a concluding section.
Readers who do not read the full details have no desire to go on to a
concluding section.  A mathematical research article is not read like a
novel or even like an essay that seeks to "persuade" the reader; it does not
need an epilogue.


Definitions.
Words being defined should be distinguished by italics (or perhaps boldface in
a textbook context).  When italics are used to indicate a word being defined,
it is unnecessary to use "called" or "said to be"; the use of italics
announces that this is the term being defined and replaces these words.


Many definitions are phrased as "An object has property italicized term
if condition holds."  Here we use the word "if" even though
subsequently it is understood that an object satisfies the property being
defined if and only if the given condition holds.  The italicization
alerts the reader to this situation.  The convention can be justified by saying
that the property or object does not actually exist until the definition is
complete, so one does not yet in the definition formally say that the named
property is equivalent to the condition.


Definitions written by non-native speakers sometimes contain extra commas.
In each sentence below, the comma should be deleted.
  
"A bipartite graph, is a graph that is 2-colorable".
  
"A graph is bipartite, if it is 2-colorable".

The first example is a mistaken placement of a comma inside a clause (see 
discussion of Commas).


Note the difference in italicization above.  When written as an adjective-noun
combination, the term being defined is the name for structures that have the
property; hence the full term bipartite graph is italicized.  When the
property alone is being defined and is positioned as a predicate adjective,
only the adjective is italicized.



"Where".
A formula may contain notation that has not yet been defined, if the definition
of that notation follows immediately in the same sentence.  The formula is then
followed by a comma and the word "where" to introduce the definition.  For
example, "If G is a bipartite graph, then χ'(G)≤Δ(G),
where χ'(G) is the edge-chromatic number and Δ(G) is the
maximum degree of G."  (Technically, the second comma is needed because
the subsequent definition is an appositive).


Note the difference between "where" and "such that".
"Where" is used when the preceding notation is being defined; "such that"
is used when it is already defined and its value is being restricted.


Double-Duty Definitions.
One cannot make a statement about an object before the object has been defined.
Similarly, one cannot use notation as part of a formula making a statement
about the denoted object unless the notation has previously been defined.  In
particular, these tasks cannot correctly be accomplished at the same time with
one instance of the notation.
For example, "The neighborhood of a vertex v is
N(v)={u: uv∈ E(G)}"
is incorrect.  With a subject and a verb before the equation, the equation
is a single unit (see expressions as units).
This sentence defines the neighborhood of v to be a particular
equation, and it does not define the notation N(v).


Of course, readers sufficiently familiar with the context have no trouble
understanding what is meant, but why disenfranchise other readers?  One can
just as easily write "The neighborhood of a vertex v, denoted
N(v), is {u: uv∈ E(G)}".  Alternatively, one can introduce
the notation as an appositive in a conventional
position immediate after the term defined: "The neighborhood N(v)
of a vertex v is {u: uv∈ E(G)}".  



A common Double-Duty definition is "Let G=(V,E) be a graph".  The
sentence defines the equation G=(V,E) to be a graph.  Of course, the
writer intends simultaneously to introduce notation for a particular graph and
its vertex set and edge set, but that is not what the sentence says.  It is
better to write "Let G be a graph" and use operators V and
E to refer to the vertex and edge sets of G as V(G) and
E(G) (see also Operators vs. constants.)
 

A more subtle example is "For each 1≤ i≤ n,".  The introduction of
the notation i has been lost because the inequalities impose conditions
on it before it is defined.  Since the expression is a unit, grammatically the
phrase is referring to each inequality written in this way.  Correct
alternatives that express the intended meaning include "For all i such
that 1≤i≤n", "For i∈[n]", and "For
1≤i≤n".  The third option is slightly different from the others;
it means "whenever i is such that the conditions hold", implicitly
introducing i in a specified range but avoiding the grammatical problem.


Expressions as units.
Is an equation or inequality a noun unit, or is it read with the relational
symbol as a verb?  Treating the symbol as a verb often forces rereading to
clarify the meaning, and one often wants to have another verb in the sentence.
For these reasons, it is best to treat notational expressions as single objects
(that is, nouns), with some exceptions.


For example, "there exists i<j with
xi=xj" ascribes a property to the inequality
i<j (and is a Double-Duty Definition of i).  Without context,
it is hard to tell that the author meant "there exists i such that
i<j and xi=xj".  Consider also "The
number of nonneighbors is n-1-d(u)≥ i." The number of nonneighbors is
not an inequality, it is a number; the author is trying to make two statements
in one inequality.  For clarity, separate the statements: "The number of
nonneighbors is n-1-d(u), which is at least i".


Exceptions. Applying this principle with very simple expressions leads
to ponderous writing.  Here are two notable exceptions:
  
1) In "Choose x∈ V(G) such that x has minimum degree,"  we
are choosing x, not the expression "x∈ V(G)".  The
justification for this exception is that the membership or containment symbol
is read as "in", which is not a verb.  (One can treat nonmembership in the same
way.)
  
2) "Let G'=G-x".  When introducing notation for an object or expression
by a single imperative verb ("let", "set", "put", "choose", etc.), we read the
equality symbol as the verb "equal", truly an exception.  This exception can
be recognized by the lack of any English verb in the sentence.  Continuing with
another verb, as in "Let G'=G-x be ...", would produce
a Double Duty Definition.


If the introductory part of the sentence is longer, then we may already have a
noun and a verb, and the expression again becomes a unit.  For example,
"Include each vertex independently with probability p=(ln n)/n"
should be "Include each vertex independently with probability p, where
p=(ln n)/n".



Separation of formulas.
Avoid placing two formulas consecutively, separated only by a comma.  For
example, "For x<0, x²>0" may be read as something other than
a hypothesis and a conclusion.  Similarly, "For some k with
k<n, n-k+f(n)<n/2" requires the reader to stop and go back
to insert the missing words.  The difficulty arises because commas occur also
in notation, and the eye cannot distinguish between commas that occur in
notation and commas that are intended to cause a pause or to substitute for
words.
The mathematics will be easier to read when the formulas are separated by the
comma plus words that enable the reader to understand the sentence at the first
reading.  Such phrases are "it follows that", "we have", etc.


[On the other hand, "we have" is an awkward phrase that often should be 
dropped when not needed to separate formulas.  For example, instead of
"By the preceding theorem, we have A=B," prefer "By the preceding
theorem, A=B".]


When the second formula just specifies an object, the separation can be
accomplished by specifying the type of object, as in "When k=2, the
graph G is Eulerian" instead of "When k=2, G is Eulerian."
One can always rewrite notational expressions separated by a comma to avoid
the difficulty.  Usually this is easy, as in changing "For every bipartite
graph G, χ(G)≤2" to "If G is bipartite, then
χ(G)≤2".



Initial notation.
Never begin a sentence with notation.  Always one can prepend a specifier (such
as "The graph G is" instead of "G is") or rewrite the sentence in
another way to avoid starting with notation.  Following this rule makes
mathematics easier to read.  The principle here is similar to the separation
of formulas.  An exception is that the statement of a numbered theorem may
begin (or be entirely) a formula, because the numbered designation serves as
a label that begins the sentence.



Lists of size 2.
It is common but ungrammatical to write "Let x,y be vertices in
G"; we would not write "My friends John, Mary came to dinner."
The concatenation is an instance of two formulas separated by a comma.
To see what can go wrong, consider the following clause:
"Since a|b and a,b are maximal and minimal,".
What was meant was:
"Since a|b, with a maximal and b minimal,".
In general, the comma within a list of two elements should be replaced with
"and" when discussing the two elements as individual items.  For example, "If
x,y are adjacent" should be "If x and y are
adjacent" or "If {x,y} is a pair of adjacent vertices".



Exceptions.
With lists of size at least three, omission of "and" does not cause as much
confusion, and including it can be awkard.  Here the objection to the common
mathematical convention is much weaker: we accept "Let x,y,z be the
vertices of T," although writing "Let {x,y,z} be the vertex set
of T" would be more precise.  Still, "let x, y, and
z be the vertices" reads better.


Another sensible exception is "Choose x,y∈ V(G)".  Here the relation
is between each variable and the set, and we accept this as a single formula.
Again a justification is that we can read ∈ as the single word
"in", without a verb.  Similarly, many mathematicians write, "For
n,m≥2" to mean the conjunction of n≥2 and m≥2.
The exception for the membership symbol is consistent with other exceptions for
the membership symbol; doing it with inequalities is more questionable.  Avoid
doing it with equalities (see Variable equal to list).

it unnecessarily requires a pause for the reader to figure it out.


Parenthetic or wordless restrictions.
Many writers of mathematics impose restrictions parenthetically or via commas,
thereby omitting words in sentences and juxtaposing formulas.  This makes
reading unnecessarily difficult.  Parentheses around notation are mathematical
objects and hence cannot substitute for words.  A phrase like "Let
m(m≤n) be the size" is immediately clear only to the author.

Other examples: "Suppose there is an edge xy (≠e) in G"
should be "Suppose that G has an edge xy other than e".
Similarly, "For k≤m with k even"
improves on "For k≤m (k even)" or "For k≤m, k
even", and "Consider ai for 1≤i≤n" is better
than "Consider ai (1≤i≤n)".  One can also
separate by putting words into the parentheses: "For k≤ m (where
k is even)".  Note that "Suppose that there is an edge xy≠e in
G" is a Double-Duty Definition; "xy≠e"
is not an edge.



Mixing words and notation.
Words cannot be compared with notation via a relational symbol.  Do not write
"Consider a graph G with maximum degree ≤ k".  Grammatically,
the sentence does not indicate where the inequality starts.  If one side is
written in words, then the relation must also be written in words.
Furthermore, the sentence above says that the maximum degree of G
equals the expression "≤ k".


The same principle applies to logical symbols.  In written mathematics, do not
use the symbols ∃,∀,⇒,iff) to substitute for words in
sentences.  Shorthand notation used to save space on lecture slides need not
follow these restrictions, since the slides summarize the lecture and are
accompanied by oral explanation.
 


Statements of implication ("Let ... Then").
The common two-sentence mathematical construction
    
"Let hypothesis.  Then conclusion."

is grammatically incorrect.  The second sentence is not a sentence, since the
implicative sense of "then" plays the role of a
conjunction.  The simpler form 
    
"If hypothesis, then conclusion."

is less choppy, easier to read, grammatically correct, and faithful to the
mathematical sense of a conditional statement.  When there are many hypotheses,
resulting in too long a sentence, some creativity can be applied.  First a
sentence (perhaps beginning with "Let") sets the context.  The last crucial
hypothesis is saved for a statement of implication, using the "If/then" form.


Used at the beginning of a sentence, the English word "Then" is temporal, as
in "Then we left." Since the implicative sense of "then" is so common in
mathematics, the temporal sense should rarely be used, to avoid confusion.
Usually the temporal "then" at the beginning of a sentence can be changed to
"Now" or "Next" with less confusion and essentially the same (and more
accurate) meaning, especially in a proof.


Writing "Let . . ., then . . ." in one sentence is similarly problematic.
This sentence is constructed from one correct sentence by changing the first
clause to a completely different sentence.  The content is that of a
conditional statement, but it is not written as a conditional statement.
This style seems to result from a conscious effort to subordinate language
to jargon.




Words of hypothesis: "If", "When", "For", "Since".
For ease of understanding, a sentence that begins with "If" should later
have ", then" to start the conclusion.  The word "then" should not be omitted,
and a comma should precede it.  The comma can be omitted in a brief implication
contained within a clause already set off by a comma, as in "Since f is
the squaring function, if x=0 then f(x)=0".


When readability would be improved by omitting "then", the sentence should
instead start with "When" or "For", as in this sentence itself.  A comma still
follows the condition introduced by "When" or "For".  The structure of a
sentence beginning with "Since" is like those beginning with "When" or "For"; a
comma follows the first clause.  After "Since" or "Because", the concluding
clause cannot begin with "then" or "so"; "then" is used only with "If".



"As" and "For" introducing reasons.
In English, the words "as" and "for" may be used to introduce a reason given
after the statement of the conclusion from that reason.  For example:
"I ate early today, for I was hungry," or "He stopped writing his answer, as
time had expired."  Banish these uses from mathematical writing; they introduce
confusion, especially for non-native readers.  "As" also means "like", and
"for" is most often used to specify a universe.  Compare "The degree is at
least one, for a vertex in the neighborhood" with "The degree is at least one,
for a vertex in the neighborhood is not isolated."  The meanings of "for"
differ, but the reader does not discover that until the end of the sentence.



Words of conclusion: "Hence", "Thus", "Therefore"
A long proof does not fit in a single sentence; hence often one needs a word
to start a sentence that states a conclusion.  Among the choices are
"Therefore", "Hence", and "Thus".  Purists (and copy editors) desire a comma
after every such introductory word or phrase (as they do after "Finally", "On
the other hand", "In 1965", etc.).  This can make language overly formal.


Among these choices, I treat "Therefore" as the most formal, introducing a
major conclusion and hence taking a comma.  Because "Hence" and "Thus" are
single syllables, I use them without commas to indicate the flow of argument
without making the writing choppy.  This choice modifies strict English
punctuation in the service of mathematical understanding.  It is not incorrect
to put commas after all these introductory words, but it enhances mathematical
communication to omit the commas after short words introducing short
conclusions that are just a step along the way.  Copy editors put in the
commas, and I insist that they be removed again. 



"by theorem X".
Consider the sentence "Since G has at least 3n-5 edges, by
Theorem X, we know that G is not planar."  Does Theorem X imply
that G has at least 3n-5 edges or that G is not planar?
Since the reader will not know the author's intent, "by Theorem X" should
not be placed between a reason and a conclusion without clearer indication
of which meaning is intended.  Dropping one comma makes it clearer, as in
"Since G has at least 3n-5 edges, by Theorem X we know that
G is not planar" or "Since G has at least 3n-5 edges
by Theorem X, we know that G is not planar".  However, the second option
can be written better as "By Theorem X, G has at least 3n-5
edges, and therefore G is not planar".  Alternatively, a reader
suggested using parentheses, as in "Note that G has at least 3n-5
edges (by Theorem X), and hence G is not planar."



"So" and "so that".
Because of its other uses in English, "So" is too informal to introduce a
sentence of conclusion (with or without being followed by a comma).  It is best
to reserve "so" for use as a conjunction, like "but": "The graph is connected,
so each vertex is reachable from every other vertex."  In this usage, no word
is needed to introduce the reason that precedes the conclusion.
As a conjunction, "so" is preceded by a comma, not a
semicolon: "The graph has no odd cycles, so it is bipartite."  This form is
best used when the conclusion is short.  When "so" is used as a conjunction,
there is no "that".  Thus "We have x²=0, so that x=0"
should instead be "We have x²=0, so x=0".)




"Such that" vs. "so that".
"So that" means "in such a way that".  Use "such that" when imposing a
condition on an object and "so that" when performing an action in a certain
way.  That is, "so that" requires a verb and describes how the action is done,
while "such that" restricts a noun.  Compare "Consider a graph such that no
vertex is isolated" and "Color the graph so that no two adjacent vertices have
the same color.  What follows "such that" modifies "graph", but what follows
"so that" describes how the coloring is performed.



"Assume", "Suppose", and "Let"
A statement that is assumed is an axiom, considered throughout to be
true.  Something supposed is a hypothesis.  Hence "Suppose" or "Suppose
that" is more appropriate to introduce a case or an argument by contradiction.
In contrast, "we may assume" introduces a consequence of an argument or
symmetry and henceforth will be true.  I do not really understand the
phrase "Assume for a contradiction that"; use "Suppose to the contrary
that".  Similarly, do not use the incomprehensible "By way of contradiction";
a possibility is "Toward a contradiction, suppose that".


"Suppose" vs. "Suppose that".
After words of hypothesis or conclusion ("suppose", "assume", "implies", 
"conclude", etc), use "that" when what follows is a clause with an English
verb.  Omit "that" when what follows is just a noun unit, such as a notional
expression.  For example, "Assume the hypothesis" is a complete sentence with
imperative verb and object.  The structure is the same in "Suppose
x+y≤10".  When an English verb follows, we have "Suppose that
f is a proper coloring".


This distinction is a matter of some debate.  Some more formal authors use
"Suppose that" when what follows is a formula containing a relational symbol,
treating the symbol as a verb.  I think it is better to maintain the
consistency of treating formulas as noun units.  The clarification accomplished
by "that" when a verb follows become unnecessary when the clause is condensed
into notation.  When the notation is displayed, its role as a fact (noun) is
clearer and makes "that" especially unnecessary; the use of "that" should be
the same when the formula is not displayed.  A related example is "the case
k=2", as opposed to "the case that k=2"; here "k=2" is the
case, which is a noun, so there is no "that".


Writers who always drop "that" from "Suppose that" have a valid point.
In spoken English, we usually drop "that" in this conntext to avoid ponderous
language.  When the instruction is informal, without abstract concepts, it is
reasonable to drop "that".  For example, "Suppose the hypothesis is true" would
be awkward with "that".  Similarly, the very short "Suppose there is" would be
awkward with "that" after "Suppose".  Here the verb is gone before one even
notices it; this is almost like "Suppose [notation]". 


This exception may seem awkward.  A better solution when introducing notation
is to avoid "Suppose x is" entirely: "Let G be a graph" is better
than "Suppose G is a graph".  Compare "Suppose x=1" and "Let
x=1".  The first assumes the truth of an equality and treats the
equation is a unit.  The second is more active.  Because we never say "Let that
. . .", we either view "Let" as the verb or view the equality sign as the verb.
This usage of "Let" is an exception to the treatment of 
expressions as noun units; it is not used with
inequalities, because an inequality sign would need to be read as the lengthy
"be less than or equal to" to become a verb.



Universal quantifiers.
The word "any" can mean "some" or "all" in different contexts, so it can
be imprecise.  It is clearer to use "each" or "every" as a universal
quantifier when referring to a singular object.



Numbered plural variables cause difficulty.  In English, "for every two
elements" is awkward because "every" is singular.  Thus here it is better to
say "for any two elements".  The presence of "for" is suggestive of the
universal quantification and helps avoid ambiguity.  Confusion can still arise:
consider "Form G' from G by adding an edge joining any two
vertices with distance 2 in G."  Here some readers will think that only
one edge is added.


Avoiding "any" is not imperative.  Evaluate its use in context, making sure
to prevent misinterpretation.  "Any" is a good substitute for "an arbitrary",
and the meaning of "not any" is fairly clear.
("arbitrary" indicates that all ways of making the choice are allowed.)


Using an indefinite article ("a" or "an") as a universal quantifier can be
dangerous, as in "Prove that a bipartite graph has no odd cycle."  Some readers
(often students) may interpret "a" as "one" or "some", turning universality
into existence.  Using "every" is clearer.  Putting "must" before the
conclusion can suggest universality but is usually unnecessary.



Position of universal quantifiers.
In a logical formula, a quantifier specify the universe over which the formula
holds is placed before the formula.  In written words, a single universal
quantification may read better with the quantifier at the end.  This order also
better emphasizes the conclusion.  For example, one might prefer to change "For
every graph G that is bipartite, χ(G)≤2" into "Always
χ(G)≤2 when G is bipartite".  Similarly,
"ai∈S for 1≤i≤n" improves on "for
1≤i≤n, ai∈S".



"Less" vs. "fewer".
Use "less" when comparing numbers, and use "fewer" when referring to a set
of objects.  For example, "the number of edges is less than k" is 
correct, as is "the graph has fewer than k edges" or "G' has
fewer edges than G".



A set differs from its size.
Comparing incomparable quantities is often called "comparing apples and
oranges".  One cannot compare a set with an integer; it is incorrect to write
"Sperner proved that no antichain of subsets of an n-set is larger than
C(n,⌊n/2⌋)".  One must distinguish between a
set and its size.  Here one can write "no antichain has size greater than
C(n,⌊n/2⌋)" or "no antichain has more than
C(n,⌊n/2⌋) elements".  (Due to the inadequacy
of html, we use the notation C(n,k) for the binomial coefficient
"n choose k".)



"Estimate".
Many mathematicians, particularly analysts, use the English word "estimate" as
if it had the same meaning as the English word "bound" (both as a noun and
as a verb).  They write "now we estimate this quantity" when they mean "now we
prove an upper bound on this quantity".  In English, "estimate" means
"approximate"; both upper and lower bounds are needed to give an estimate.
This common usage by analysts is incorrect English and does not say what is
meant, even when they assume an unstated implicit lower bound of 0.



Possessives on notation.
Do not write "Let x and y be v's neighbors"; always
use "of" ("the neighbors of v") instead.  Similarly, do not pluralize
notation by referring to indexed elements or sets together as "the
ai's".  Usually "each ai" or
"a1,…an" or some other notation is
preferable.  Possessives and plurals of this sort should be reserved for
informal oral communication.



Nested proofs.
Do not nest proof environments.  No new proof label should occur before the
end-of-proof marker for the current proof.



"Best possible".
"Best possible" is an adjective used as a single term; it indicates sharpness.
We write "This result is best possible", just as we would write "This result is
sharp".  "This result is the best possible" indicates that this result is
better or more valuable aesthetically than all other results in the world,
which is not what is meant.  The definite article should not be used here. 
Think of "best possible" as a technical term that is already a specific
predicate adjective, so no definite article is needed.


The informal phrase "is most likely" is similar to "is best possible"; there
is no article because "most likely" is used as a single term.  It means that
the probability is high, whereas "is the most likely" means having higher
probability than any other outcome.  Another example is "best practice", which
is a single technical term in areas of management.  It is used as a single
term, without "the".  For example, I have seen the title "Best Practices in
Online and Blended Learning and Teaching".

Although "This result is best possible" is a complete sentence, it is
somewhat vague, since it does not specify the sense in which the result cannot
be improved.  Often it is more informative to say something like "the constant
in the upper bound cannot be improved".  For this reason, some writers suggest
avoiding the term "best possible".





Numerals and spelled numbers.
In standard English writing, numbers less than 10 usually are spelled in full,
while numbers more than 10 are written in numerals.  In mathematical writing,
the basis for the distinction is different.  Numbers less than 10 are spelled
out only when used as adjectives expressing the quantity of objects in a set.
They must remain as numerals when designating the value that a quantity equals.
For example, "The two vertices both have degree 3" or "A cycle of length 4 has
four edges" or "Consider a 4-vertex path" or "Consider a path with four
vertices".  A reader provided another excellent example; compare the two
sentences below:

    Although X is not a cycle, its Betti invariant is 1.

    Although X is not a cycle, its Betti invariant is one.

The first sentence says that the Betti invariant of X equals 1.
The second sentence says that the Betti invariant of X is a cycle.


Terminology and notation (especially in discrete
mathematics)



Definition symbol ":=".
Some mathematicians use this symbol to indicate that the preceding symbol
is being defined to mean the subsequent object.  If this occurs in a sentence
like "Let [n]:={1,…,n}", then the verb states that the notation is
being defined, and the special notation is unnecessary.  If it occurs in a
sentence about the object being defined, such as "Consider a coloring of
[n]:={1,…,n}", then it is an improper
Double-Duty Definition and should be rewritten:
"Consider a coloring of [n], where [n]={1,…,n}."  Reading
":=" requires thinking "be defined to be" when preceded by "let", and it
requires even more convoluted phrases when placed in a Double-Duty Definition.
This awkward notation is never needed and encourages grammatical errors.



"Such that" in set definitions: ":" vs. "|".
For many reasons, the colon ":" is a far better choice than the vertical bar "|"
to mean "such that" in a "notation/condition" definition for a set.  For
example, we may write "{3n+1: n∈N}".  The vertical bar
is heavily used in mathematics, most notably for size of sets, but also for
divisibility and other purposes.  Using it for this purpose leads to such
messes as "{|A|||A|||B|}", which purports to describe the set of
sizes of sets A whose size divides the size of B.  The colon is
far less used in mathematics.  Even so, the best reason for using the colon is
that this mathematical usage is similar to the meaning of the character in
English; it separates part of a sentence from some elaboration of that part.
Finally, since "such that" is not a binary operator, this usage should be
expressed in TeX using "\colon\," instead of ":".  As in English,
there should be space after the colon but not before it (or at least less space
before it).



Sequences, series, and lists.
In mathematics, a sequence is a function whose domain is the set of natural
numbers (perhaps with a shift of the initial element).  Discrete mathematicians
abuse this term in using it for an ordered finite set.  A good name for such an
object is list.  An n-tuple is a list of length n.  It is
an abuse of terminology to say "a sequence of length n".
(For finite graphs, in particular, "degree sequence" should be changed to
"degree list".  To avoid this problem, one can sometimes refer to the "vertex
degrees" rather than the "degree sequence" or "degree list".)


The usage of "series" in English is contrary to its usage in mathematics.
In English a "series" usually consists of finitely many occurrences in order,
as in the "World Series" or the title "A Series of Unfortunate Events".
In mathematics a series is an infinite sum.  So I believe, but one
correspondent tells me that a finite sum is also a series, though I would just
call it a summation or finite sum.



The second element of a list. 
The expression "v1,v2,…,vn" for
an indexed n-tuple is a style used to suggest that the elements are
indexed by the first n positive integers with no skips.  However, the
second element is not needed, since the default interpretation of 
"v1,…,vn" is exactly the same.  By
convention, indices in a list are consecutive unless explicitly indicated
otherwise.  Another reason to eliminate v2 from the
expression is that "v1,v2,…,vn"
forbids the case n=1.



A list with relations.
The sentence "Let x1≤…≤xn be a list of
integers" is a Double-Duty Definition; the writer
attempts simultaneously to introduce notation for the elements of a list and to
impose inequalities on them.
The expression "x1≤…≤xn" denotes a set
of relations, not a list; what is meant is "Let
x1,…,xn be integers such that 
x1≤…≤xn."  (To avoid repeating the
notation, write "Let x1,…,xn be integers,
indexed in nondecreasing order.")  Similarly, a chain of sets under inclusion
is a list A1,…,Ak such that
A1⊆…⊆Ak; the expression
"A1⊆…⊆Ak" is not itself a chain.


Although html does not provide line-centered dots, the ellipsis in an indexed
list with relations should be vertically centered on the line ("\cdots" in
tex), while the ellipsis in an indexed list separated by commas should be on
the baseline ("\ldots" in tex).



Variable equal to list.
Many mathematicians write "for m=1,2,…,n" (with or without the
"2") to mean "for m∈{1,…,n}" or "for 1≤ m≤ n".
The expression "for m=1,…,n" is mathematically incorrect; it sets
the value of m to be a list of numbers.  The same principle applies to
writing "i=1,2" to name two cases; this should be i∈{1,2}.



"Big Oh"
Common usage of "Big Oh" notation is another instance of setting expressions
equal when they cannot be equal.  The expression "f(n)=O(n²)" does
not mean that the value f(n) equals the set represented by the notation
O(n²).  What is meant is "f(n)∈ O(n²)"; Knuth has
written at length on this subject.  An alternative that is roughly correct is
to be more informal, writing "f(n) is O(n²)", in
which "is O(" can be read as "is on the order of".  Since it is
convenient to do arithmetic with these classes of functions, this problem will
not go away.  An unsatisfying compromise is to use the membership symbol where
the grammar of computation permits, in order to ensure that the meaning of the
concept is understood.


Operators vs. constants.
We never use f to denote the value of a function f at a point
x.  The same principle applies to graph parameters and other operators.
For example, the maximum degree of a graph G is denoted
Δ(G).  Here Δ is a function, not a number, and hence
Δ should not be used to denote the value of the function
Δ on a particular graph.


It is tempting for mnemonic reasons to write "We write V=V(G) and
Δ=Δ(G)".  Admittedly, this usage is not confusing when
discussing only one graph at a time; the difference between a graph invariant
and a real-valued function is that we rarely focus on the value of a real-valued
function at just one point.  Nevertheless, it is rare that a paper discusses
only one graph, and hence it is better to use V(G) and Δ(G)
for objects associated with G.  The problem is particularly bad with
Δ, since this character also occurs in mathematics as a difference
operator.  One often sees "Δn" meaning the change in the value of
n, so one should not use "Δn" to mean the maximum degree
times the number of vertices in a graph.  (In my textbook I violated this
principle by using n(G) and e(G) for the numbers of vertices and
edges in a graph G while using n for the number of vertices of
a particular graph and e as a particular edge; the error will be
corrected in the third edition.)



Hyphens for parameters.
The expression "k connected graphs" refers to k graphs that are
connected, in contrast to "k-connected graphs", which are graphs having
the property of being k-connected.  Note also that an expression
involving addition or subtraction used as a parameter modifying a noun
should be enclosed in parentheses before the hyphen.  For example, write
"(k+1)-connected graph", not "k+1-connected graph".

This hyphenation rule applies whenever notation is used to modify or 
further specify the subsequent word.  Examples include "k-cycle",
"n-vertex graph", "p-group", etc.

A related hyphenation issue is particularly important in graph theory.
A "k-edge connected graph" is a connected graph with k edges
(compare with "n-vertex connected graph"); the meaning is different
from "k-edge-connected graph".  When the hyphen is missing,
"k-edge" modifies "connected graph" because adjectives modify only
nouns, not other adjectives.  Similarly, a non-specialist reader would think
that a k-edge coloring is a coloring of k-edges, not a coloring
of edges using k colors.  This example can be viewed as following
the general rule: "k" is modifying "edge-coloring"



Vertex vs. edge terminology.
In graph theory, many fundamental concepts involving vertices have analogues
involving edges.  Here we do not need "vertex" as an adjective to specify
"connectivity" or "chromatic number", but we add "edge" for the analogous edge
concept.  We then hyphenate "edge-connectivity" and "edge-chromatic number".
In both cases, the problem for edges is a special case (for line graphs) of the
general problem.


Furthermore, using the hyphen in the edge context maintains
consistency with the needed usage explained in the preceding item.
When comparing "edge-coloring" and "list coloring", we are not coloring the
lists, so the hyphenation of the term is different.


The presence of the word "vertex" sometimes becomes an issue.  As mentioned
above, the fundamental parameters involve vertices and do not require the word
"vertex" as a modifier.  Similarly, when we speak of "disjoint subgraphs", it
must be that they cannot share anything, vertices or edges, so the word
"vertex" is unnecessary.  The concept "edge-disjoint" indicates a less
restrictive condition.  Saying "vertex disjoint" suggests vertices as an
alternative to edges; it is better just to say "disjoint".  Clearly disjoint
cycles share no vertices.



Two-word adjectives.
Two-word terms used as single concepts to modify nouns must be hyphenated when
placed before the noun, such as in this sentence (without the hyphen, we would
be discussing two "word terms").  This is particularly necessary when the first
of the two words is a noun, as in "vertex-transitive".  An especially common
instance of this error is "polynomial time algorithm"; "polynomial-time" must
be hyphenated when used as an adjective.  In constrast, the hyphen is dropped
when the expression is not being used to modify something else, as in
"The algorithm runs in polynomial time".


Further examples:
"graph-theoretic techniques",
"straight-line drawing" (what is a straight line drawing or a straight line
segment as opposed to one that is not straight?)



Adverbs and "well-known".
Unlike nouns or adjectives, adverbs can modify adjectives.  Thus we write
"strongly connected digraph" and "simply connected region" without hyphens.


The adverb "well" is a possible exception.  In "well-known theorem" we think of
the combination "well-known" as a single technical term, leading to "A
well-known theorem is a theorem that is well known."  The term "well-defined"
also behaves this way.  However, opinion on this point is sharply divided; some
authors insist that because "well" is an adverb the term should not be
hyphenated.  Further support for the hyphen: the mathematical usage of "well"
in the hyphenated term differs from the English usage of "well" is the
unhyphenated expression.  A "well defined function" is a function for which we
have done a good job of giving a definition, but a "well-defined function" is
an object that has been given a valid definition as a function, with 
every domain element given a unique image.



Notation for paths in graphs.
In "x-y path", "x-y" is not a word and has no notational meaning
by itself.  Even worse, often x-y is treated as a mathematical expression
in TeX and is typeset using a long minus sign with extra space around it.  The
intent is to specify a path with endpoints x and y.
Thus x and y are parameters designating a certain type of path.
Under the principles of hyphenation above, there must be a hyphen between
"y" and "path".  Furthermore, the endpoints are independently expressed
parameters, with no operation being performed on them.  Hence for consistency
with other instance of two-parameter terminology (such as "f,g-factor",
"x,y-chain"), the correct notation is "x,y-path".



Order and size of a graph.
Traditionally, the terms "order" and "size" refer to the number of vertices
and number of edges of a graph.  These terms are not as popular as they once
were; "order" has too many other mathematical uses, and "size" without a 
clear context is confusing, sometimes taken to mean the number of vertices.
Some writers thus prefer "number of vertices" and "number of edges".


It must be admitted that "order" and "size" are quite convenient, while overuse
of "number of vertices" and "number of edges" becomes quite awkward.
Introducing notation for the numbers of vertices and edges minimizes this
difficulty.  Unfortunately, there is no generally agreed notation for operators
returning the numbers of vertices and edges of a graph G.  The only
notation that cannot be misunderstood is the absence of special notation:
|V(G)| and |E(G)|, using notation for cardinality of finite
sets that is standard throughout mathematics.  Nevertheless, these expressions
seems cumbersome to use repeatedly, so it is often beneficial to write "Let
n=|V(G)| and m=|E(G)|."



Graphs are not sets.
When h is a vertex in a graph G, it makes no sense to write
h∈ G, since h could just as easily be an edge.  A graph
consists of a vertex set and an edge set; one should write v∈ V(G)
and e∈ E(G).  This is also a reason why the convenient notations
|G| and ||G|| for the order and size of a graph are problematic.
When one sees |G|, how does one know whether it is the number of
vertices or the number of edges?

On the other hand, it is true that we write G-v and G-e for
deletion of a vertex v or edge e from a graph G, defining
"-" in the context of the objects it operates on.  In this sense, it
would also be legitimate to define the meaning of the cardinality and norm
operators when operating on a graph.  However, until there is very strong and
widespread support for this usage, it seems wisest to stick to the notation
|V(G)| and |E(G)| that is understood by all mathematicians.


When A⊆V(G), it is clear that |A| is the size of the vertex
subset A.  It can then be useful to define ||A|| to be
|E(G[A])|, the number of edges in the subgraph of G induced by
A.



Directed graphs and hypergraphs.
These models are variations or generalizations of graphs.
In a digraph, the edge set consists of ordered pairs.  The redundancy of
saying "directed edge" or "directed path" or "directed cycle" is not helpful,
as it suggests that the digraph contains such objects that are not directed
(the term "weak path" is available for a path in the underlying undirected
graph).  Unnecessarily adding the adjective "directed" in a context where
the default should incorporate that notion also prevents making statements
that hold in the context of both graphs and digraphs, like Menger's Theorem.


Similarly, one should not use "hyperedges" to refer to the edges of a
hypergraph.  Hypergraphs generalize graphs by allowing edges to have arbitrary
size.  Calling them "hyperedges" eliminates the possibility of saying that
graphs arise as a special case, since graphs have edges, not hyperedges.



"Connected components".
Unnecessary redundancy has similar disadvantages.  We should not speak of the
"connected components" of a graph, because there are no disconnected components
of a graph.  Writing "connected components" suggests that there are components
that are not connected.



"Maximal" vs. "maximum".
Many mathematicians use these words interchangeably.  One can make a useful
distinction by using "maximum" to compare numbers or sizes and "maximal" to
compare sets or other objects.  Thus a maximal object of type A
is an object of type A that is not contained in any other object of type
A.  A maximum object of type A is a largest object of type
A; here "maximum" is an abbreviation for "maximum-sized".  For example,
in a graph we may speak of "maximal independent sets" and "maximum independent
sets"; these are convenient terms for distinct concepts that are both important.


Although this distinction is sensible and has become established in many
settings (such as "maximum antichain" and "maximum independent set"), potential
confusion can be reduced by using "largest" and "smallest" instead of "maximum"
and "minimum".  For example, it is harder to misinterpret "a largest matching"
than to misinterpret "a maximum matching".


For consistency, then, one should not write "a vertex of maximal degree" or
"the maximal number of edges"; that is, "maximal" should not be applied to
numerical values.  This is consistent with usage in continuous mathematics,
where we write that a continuous function "attains its maximum" on a closed
and bounded set.



Multicharacter operators.
A string of letters in notation denotes the product of individual quantities.
Therefore, any operator whose notation is more than one character should be
in a different font, generally roman.  This convention is well understood for
trigonometric, exponential, and logarithmic functions, and it applies equally
well to such operators as dimension (dim), crossing number (cr), choice number
(ch), Maximum average degree (Mad), etc.



"Induct on" and "By induction".
The phrase "We induct on n" is convenient but not correct.  From given
hypotheses, we deduce a conclusion; we don't "deduct" it.  When we announce the
method of induction, we must instead say "We use induction on n."  The
verb "to induct" is used when a person is inducted into an honorary society,
for example.


A different problem arises in the induction step.  When we cite the induction
hypothesis, we must write "By the induction hypothesis", not "By induction".
To obtain the conclusion for the smaller instance, we are invoking the
hypothesis that the claim holds for smaller values; we are not invoking the
principle of mathematical induction.



Cliques vs. complete subgraphs.
In an earlier era, these terms were used interchangeably in graph theory, but
it is more useful to distinguish them.  There is a difference between a set of
pairwise adjacent vertices in a graph (complementary to an independent set of
vertices) and a subgraph isomorphic to a complete graph.  Both concepts are
needed, and the appropriate terms for them are "clique" and "complete
subgraph".  Thus "clique" should be reserved for a set of vertices, and then
the meanings of "clique of size 5" and "5-clique" (the same) are clear.  In
previous centuries, also "clique" was sometimes used to mean "maximal clique",
which should not be done.



Isomorphism classes vs. subgraphs.
A graph is a pair consisting of a vertex set and an edge set.  Paths, cycles,
and complete graphs are graphs whose edge sets are described in specific ways.
The notations Pn, Cn, and
Kn do not distinguish a particular set of vertices, and hence
in specifying paths, cycles, and complete graphs they must refer to the
isomorphism classes.


Hence we should never write "a Pn" for a member of that class.
We can write that a graph "contains a path with n vertices",
because that is a structural description of the subgraph, but we cannot write
"contains a Pn" or "consider a Pn in
G".  We can say "contains ten copies of Pn" to refer
to subgraphs that are n-vertex paths; each such subgraph is a member of
the isomorphism class denoted by Pn.


Neverthless, some flexibility is helpful here.  When H is the notation
for an isomorphism class, we still write "H⊆G" to mean that some
subgraph of G belongs to the isomorphism class or is "isomorphic to
H" (or "G contains a copy of H"), even though we are not
specifying the particular vertices or edges of G used in the subgraph.



Proper coloring.
A k-coloring (or k-edge-coloring) of a graph is a partition of
the vertices (or edges, respectively) into k classes.  In combinatorics
generally, a k-coloring of a set partitions it into k classes,
arbitrarily.  This general concept appears in many areas of mathematics, 
including Ramsey theory, graph decomposition, and chromatic numbers.  In the
latter context, a proper [edge-]coloring is one in which adjacent [or
incident] elements do not receive the same color.


Some authors who write extensively about chromatic number and edge-chromatic
number drop the word "proper" and use k-[edge-]coloring for the
restricted concept.  The minor convenience gained by dropping this word is
overwhelmed by the negative influence of introducing inconsistency of
terminology in combinatorics.  Use "proper k-coloring" when that is what
is meant.  For other variations, such as "acyclic k-coloring" or
"dynamic k-coloring", the adjectives replace "proper" by imposing
further restrictions on the k-coloring, so the word "proper" is then no
longer needed.



Partitions vs. parts
A partition consists of blocks or "parts".  Do not use "partition" to refer
to the members of a partition.  (Students often make this mistake.)

A bipartition is a partition into two parts.  In particular,
we say that a bipartition of a bipartite graph is a partition of its vertex
set into two independent sets.  In the past I used "partite sets" to refer
to the parts of such a partition, but there are objections to that term,
and students never get it (for example, they refer to one "partite" of a graph,
and certainly "partite" is not a noun.  Hence I now refer to the "parts" of
a bipartite graph.  This is a slight abuse of terminology, but I think its
familiarity as a word better facilitates discussion.



"Pairwise" and "mutually".
Old-fashioned mathematics took the old-fashioned word "mutually" to describe
a binary relation satisfied by all pairs in a set, as in "a set of mutually
orthogonal Latin squares".  In English usage, "mutual" indicates symmetry in
a more global way.  Hence modern mathematics should avoid using "mutually" 
to mean "pairwise"; the word "pairwise" states exactly what is meant.  The
change becomes even more important in light of modern terms like "mutual
independence" in which "mutual" explicitly does not mean "pairwise".  (Thus
"mutually orthogonal Latin squares" is now ambiguous, but we cannot escape the
notation "MOLS(n,k)" in design theory.)



Pairwise disjoint/isomorphic.
The phrase "Consider disjoint sets A1,…Ak"
is technically incorrect; we should instead say "pairwise disjoint sets".
However, this is a universally understood abuse of terminology, and including
the word "pairwise" each time would be ponderous.  Hence we understand a
family of disjoint sets to be pairwise disjoint.  The point is that many
binary relations really make sense essentially only in a binary context.
This principle extends to other commonly used binary relations do not make
non-binary sense, such as "isomorphic".



Disjoint union vs. join.
In much of graph theory, the notation rG indicates a graph consisting of
r disjoint copies of G.  For consistency, G+H should
therefore denote the disjoint union of two graphs G and H.
For example, Pn1+…+Pnk
denotes a linear forest, consisting of k components that are paths with
orders n1,…,nk.


Some authors use G+H instead (or also!) to denote the join of
G and H, which consists of the disjoint union plus edges
joining every vertex of G to every vertex of H.  Other notation
has been used, such as G∨H, borrowing the join operation
(x∨y) in lattices or logic, but this is not satisfactory.
Instead the best notation for the graph join is \diamondplus (unavailable
in html?) which overstrikes a diamond and a plus, much like "⊕" except
with a rotated square whose corners are at the points of the "+"
("⊕" is unavailable because it represents symmetric difference
or binary sum).  The \diamondplus is consistent with the Nesetril notation
for graph products: the symbol is a picture of the result of applying the
operation to two copies of K2.  In addition, the use of
"+" indicates that the number of vertices is additive.



Between.
An object that is between two other objects separates them; this is the common
mathematical sense of "between".  Referring to an edge (or path) with endpoints
u and v as an edge "between" u and v is somewhat
inconsistent with the rest of mathematics.  One can say "an edge joining
u and v" instead.  In a planar embedding of a graph, an edge
shared by the boundaries of two faces is an an edge between the faces.



Setminus.
The operator \setminus most often denotes difference of sets.  Hence it
is somewhat misleading or old-fashioned (and looks rather pompous) to use it
for deletion of elements, as in "G\setminus e".  Use "G-e"
instead.  Also, the notation G\setminus H is easily confused with
G/H (especially by students).  Of course, there are some contexts
(matroids and various algebraic topics), where these notations have special
meanings and are quite important, but for simple set difference A-B is
preferable.



"Left hand side".
There is no "hand side", so this expression makes no sense.  Even if one 
correctly hyphenates to make it "left-hand side",
there is still no "hand".  Just write "left side". 

English usage in mathematical writing



Introductory words.
Words or phrases like "nevertheless", "for example", "to the contrary", and
"on the other hand" usually should be separated by commas from the rest of the
sentence.  Introductory prepositional phrases are a bit different.  I am told
that a phrase with one preposition ("In 1995") does not require a comma, but a
phrase with two prepositions ("In August of 1995") does.  Another reader tells
me that an introductory phrase with at least five words (perhaps we should say
five syllables) should be followed by a comma.  I would use the comma unless
the intent is to lead into what follows as a single thought (see
Hence/Thus/Therefore).



Quotations and ends of sentences.
It is traditional correct style in English grammar that all terminal 
punctuation comes inside quotation marks.  My understanding is that this
convention arose from the technical aspects of printing presses.  Its purpose
was to lessen the danger of breakage of fixed metal type in printing presses.
In the era of electronic publishing of mathematics, this justification is
obsolete, and we can replace the convention with logical punctuation.  When the
material being quoted is treated as an item within the sentence and is not
itself a sentence, the terminal punctuation logically comes outside the
quotation marks.  Copy editors trained in literary punctuation still object to
logical punctuation but should be overruled.



"Which" vs "that".
The following two sentences have different meanings:

1) "She will attend our meetings that concern calculus."

2) "She will attend our meetings, which concern calculus."

Sentence (1) states that among our meetings, she will attend those concerning
calculus and perhaps no others.  Sentence (2) states that all the meetings
concern calculus, and she will attend them all.  In common English, the
distinction is perhaps even clearer: compare

1) "I have two shirts that need cleaning."

2) "I have two shirts, which need cleaning."

In (1), two of my shirts need cleaning.  In (2), I have only two shirts.


When the phrase after the relative pronoun specifies a further restriction
of the class that has just been introduced, the correct pronoun is "that", and
the subsequent phrase tells which of the items in the class are those being
discussed.  If the subsequent phrase speaks about the totality of the class,
then the proper pronoun is "which".  When "that" and "which"
both seem usable, use "that" when the sense is "having the property that",
and use "which" when the sense is "all of which" or "the only one of
which".  Usually a comma is appropriate before "which".  Usually "that" is
correct when an indefinite article ("a" or "an") has been used on the word
being modified.  Beware: This distinction is not made or is made the
opposite way in British English.  Some American style manuals don't care,
but in mathematics there are two distinct meanings to be expressed.



Immediacy of antecedents.
When using "which", "that", "where", or other words to introduce
explanatory or descriptive phrases, the subsequent phrase modifies the most
recent item.  For example, "an embedding of G on a surface which has no
crossings" indicates that the surface has no crossings, not that the embedding
has no crossings.  Making the comment on crossings apply to the embedding
requires rewriting: "On a specified surface, consider an embedding of G,
which has no crossings".  Here "which" is proper because every embedding has
no crossings; an embedding is a drawing that has no crossings.



The naked "This".
When "This" is used as the subject of a sentence, its antecedent is the
most recent noun.  If the desired antecedent is the preceding paragraph or
some other object, then a noun should be inserted, as in "This discussion
implies" or "This inequality implies" instead of merely "This implies".
One way to understand this issue is to view "this" only as an adjective, not
as a pronoun.



"Every", "distinct", and "unique".
The word "every" is singular; it means "each one".  Because of this, we
write "all values" or "every value"; not "all value" or "every values".


The word "distinct" has the same meaning as "different".  Two things
can be distinct, but one thing cannot be distinct.  Thus the sentence "Every
value is distinct" is incorrect; it has no meaning.  Many beginning students
think it means that each value is different from every other value, but it does
not.

 The word "unique" indicates that there is only one of the items being
described.  It does not mean that this item is different from other items.
Some students think that "The function f maps the points in A to 
unique points in B" is a statement that f is injective, but it is
not.  Every function from A to B maps each point in A to a
unique point in B.


The distinction between the words "distinct" and "unique" is made clear by a
typical boast on the World Wide Web.  The sentence "Our website has one million
unique visitors" makes no sense.  The intent is to say that among millions of
hits there are one million distinct visitors; if there is a unique
visitor, then there is no other visitor.




Contractions.
Because mathematical writing is formal, contractions ("can't", "won't", etc.)
should be avoided.  They introduce a sudden informality that is inconsistent
with the tone of proof.



"I.e." vs. "e.g.".
"I.e." and "e.g." are abbreviations for Latin phrases.  "I.e." means "that is"
and is used to introduce an explanation or restatement of what came before.
"E.g." means "for example" and introduces an example.  In formal mathematical
writing, abbreviations (except as notation) are like contractions; it is better 
to avoid "i.e." and "e.g." altogether.  In addition, the expressions "that is"
and "for example" provide better visual separation than "i.e." and "e.g.".



"Different than".
It is not correct to write "A differs than B", and for the same
reason it is not correct to write "A is different than B".
The correct wording is "A is different from B".  The incorrect
wording is modern American laziness.



Abstract nouns and articles.
Nouns that specify abstract concepts rather than objects need no articles.  For
example, "graph colorability" is an indefinite concept, so we do not say "Next
we discuss the graph colorability".  In contrast, "chromatic number" may be
abstract or specific.  We may say "Next we discuss chromatic number", referring
to the general concept, or "Next we discuss the chromatic number of this
graph", since this graph has only one value as its chromatic number.


Functions or parameters assign a number to each domain object.  The resulting
value is specific for the object; there is only one choice for it.  Hence we
do not say "the graph has a chromatic number 3" or "the vertex has a degree 3".
These sentences suggest that the object may have more than one value of the
parameter.   The answer to the question "What is the degree of this vertex?"
may be "This vertex has degree 3", but it cannot be "This vertex has a degree
3".


We also do not say "This vertex has the degree 3", although "The degree of this
vertex is 3" is correct.  Consider the sentence "Every graph has an even number
of vertices with odd degree, which means that the list of vertex degrees has
even sum." The term "even number" takes the article "an" because we are saying
which type of number is being used (it is one of the even numbers).  The later
"odd degree" and "even sum" do not, because these are properties that the
vertices and the list do or do not satisfy.  Articles are inappropriate when
invoking a property.


Articles also are not used with conceptual nouns.  Compare with familiar
conversation: we say "This chair has value $100" and not "This chair has the
value
$100." "Value" and "degree" are abstract properties.  Here is another
non-mathematical example: We say "I receive compensation for my work,"
not "I receive a compensation for my work." Compensation is an amount, but here
only the abstract concept of receiving compensation is meant, not some number
of things.  Hence we do not use an article.


Similarly, abstract properties do not take articles.  We say "because
transitivity of A implies transitivity of B", not "because the transitivity of
A implies the transitivity of B".  The property in question is "transitivity",
not "the transitivity".



Possessives and titles.
The definite article "the" specifies uniqueness.  Possessives also play this
role.  It is incorrect to use both together, because the possessive already
provides definite specification.  For example, we write "Greene's Theorem"
but not "the Greene's Theorem"; this is a theorem proved by Greene, not by
"the Greene".


When discussing a result by two authors, we cannot put possessives on both
names, because there is only one object (compare with "Greene's and Kleitman's
theorems").  Making only the second name possessive would be correct in English
grammar but poor mathematical style ("Greene and Kleitman's Theorem", like
"Dick and Jane's house).  The main reason this is poor mathematical style
is that the entire phrase is a title.  Hence we write "the Greene--Kleitman
Theorem", like "the Woolbright-Abernathy House".  Here "the" serves as a
definite article for the unique object with the title "Greene--Kleitman
Theorem".  When the result is less celebrated and not known by its authors'
names, one can indicate the possessive by "of", as in "the theorem of Greene
and Kleitman".



Capitalization of titles.
In the examples above, "Theorem" is capitalized.  When there is only one
instance of an object, and the name of it involves a person, it plays the role
of a proper noun and its name is a title.  Another example is "the
Cauchy-Schwarz Inequality".  Although some style sheets vote against
capitalization based on "common usage", failing to capitalize does not 
reflect the true meaning and seems to be mostly a matter of laziness.
We would have the Chinese remainder theorem (as opposed to the French one),
the Hungarian method (as opposed to the Austrian one), and the 
mean value theorem (as opposed to the nice value theorem).




Adjectival forms of names.
Some graph theorists use "Hamilton cycle" to mean a spanning cycle in a graph,
but they would never say "Abel group".  When describing a type of cycle, the
modifier should be an adjective, so it is better to use an adjectival form of
the name when that is available: "Hamiltonian cycle".  The same applies to
"Euler circuit" and "Eulerian circuit".  However, some uses of names as
adjectives are heavily ingrained and unchangeable, such as "Fibonacci numbers"
and "Catalan numbers".  In these examples adjectival forms are not readily
available (we do have "Eulerian numbers").




Conjunctions and commas.
Punctuation shapes sentences; commas encourage the reader to pause at places
where doing so aides understanding.  Missing commas may require the reader to
stop and re-read in order to understand what has been said.  Excessive commas
delay the reader and impede the flow of logic.


Two clauses (in essence, two complete sentences) may be combined using a
conjunction; the conjuction must be preceded by a comma.  Examples of
conjunctions are "and", "but", "then", and "so" (the latter should be treated
as conjunctions in mathematical writing).  Since a conjunction joins two
things, sentences should not begin with these words.  This is a logical
approach that helps keep writing clear, though strict English usage (especially
British) may call some of these words adverbs.  See further comments on the use
of then and so.


Exception.
The situation is more complicated when the second clause itself contains a
conjunction.  Compare "If A, then B holds and C holds" with "If A, then B
holds, and C holds".  In the first sentence, it is clear that A implies both B
and C.  The proper grouping or meaning in the second sentence is unclear.
Since we only have one comma symbol and don't parenthesize sentences to
indicate grouping, a short conjunction of two sentences within a larger
conjunction is written without a comma.



Semicolons.
Compound sentences consist of two complete sentences with no conjunction
separating them.  This form is used especially when the second part clarifies
or comments on the first.  Such sentences need a semicolon (not a comma!) to
separate the two parts; this sentence is an example.  Do not use a semicolon
before a conjunction; in particular, there should never be a semicolon before
"and", "but", "then", or "so".  



Excessive commas.
A clause requires a subject and a verb.  When "and" joins two parts of a
sentence that do not both stand on their own with a subject and a verb, there
should be no comma before it.  The comma in "We will prove the lemma, and then
the theorem" is incorrect; one must delete the comma or add a subject and
verb to the second part.  This example came from a newspaper: "In February,
the graduate student in Electrical and Computer Engineering, was awarded the
A--B--C Prize"; here there should be no comma before "was".  (See further
examples of excessive commas in Definitions.)



Serial commas. A serial comma is a comma after the next-to-last element
of a list.  Wikipedia gives a discussion of its use.  It
is generally safest to use a comma in this situation.  For example, compare
"Under the conditions 1≤ i,k≤ r and m even" with
"Under the conditions 1≤ i, k≤ r, and m even";
the two sentences have different meanings.  The issue arises often when
listing three mathematical objects or three authors.  (For a list of lists,
clarity can be achieved by using semicolons or by changing "and" to "&" within
list items.)


One reason for using the serial comma in lists is to avoid confusion in
sentences that do not contain lists.  Consider the sentences
"Like a, b and c have the same property" and
"Later, Early and Jones proved the conjecture".  These are not lists, and using
a comma would be wrong, but when a document does not use serial commas these
examples initially appear to be lists.  Similarly, in that context an item in a
list that itself joins two subitems with "and" looks like the last two items in
a list.


Omitting the serial comma can also cause confusion mathematically, as in
"The value of f is positive at 2, negative at 1 and 0 at 0."



Appositives. An appositive is a noun or noun phrase that renames or
substitutes for another noun or noun phrase immediately preceding or following
it.  It can be recognized by the fact that omitting it would yield a clear and
complete sentence and that the additional information in it is not
grammatically essential to the statement being made.  It should be set off by
commas:  "His book, the best book on the subject, took years to write.  An
appositive in the middle of a sentence cannot have a comma on only one side.


When an appositive is short enough or contains essential information, the
commas are omitted: "My friend Bob is a student."  In mathematical writing, a
similar situation applies when notation is introduced:  "The degree
d(v) of a vertex v is the number of neighbors of v."  Here
"d(v)" is a brief appositive.  One could argue that the notation for
"degree" is not essential to the sense of the sentence, but putting commas
around very short appositives can produce very choppy sentences.  A speaker
need not pause for such appositives, and hence one may omit the commas.



Passive voice.
Good writers of English minimize the use of passive voice.  This accepted
principle applies also in writing mathematics.  Active verbs make the
exposition more engaging; for example, "It suffices to show" is preferable
to "It is sufficient to show".  Nevertheless, judicious use of the passive
voice can be appropriate.



"Above" and "Below".
These words are adverbs; they do not directly modify nouns.  Hence "the above
graph" and "the below figure" are incorrect.  We can write "the graph above"
or "the figure below" as a short form for "the graph shown above" or
"the figure located below".



"Either".
The word either is used to indicate exclusive or.  If the alternatives
are mutually exclusive by definition, then "either" is unnecessary.



"We have been proving"
Do not use the "perfect" tenses, which involve the helping word "have".
Phrases like "In Section 3 we have been analyzing" or "in [4] we had shown"
are either grammatically wrong or confusing.  The simple tenses, as in
"In Section 3 we analyzed" or "in [4] we showed", are almost always better.
Even the future can be eliminated: "in Section 4 we show" rather than
"in Section 4 we will show"; the justification for this is viewing the entire
article as a unit, happening in the present.



Words containing "non".
When a word in English initially has a negation introduced by prefixing "non",
the resulting word is hyphenated.  The initial sense is the negation, so the
hyphen is appropriate.  As decades pass and the word is accepted on its own,
it becomes a positive concept incorporating the "non".  This and familiarity
lead to dropping the hyphen.  Some of the most familiar examples in mathematics
are "nonsingular", "nontrivial", "nonzero", and "nonconstructive".  Adding
hyphens to these words is now jarring to more readers than is the absence of
hyphens.  I also use "nonempty", "nonnegative", "nonneighor", and "nonadjacent".
However, I would keep the hyphen in "non-word" and "non-edge", for clarity
and infrequency.



Placement of citations.
The numeral indicating an item in the bibliography should appear immediately
after the name(s) of the author(s), not after the statement of the result.

Mathematical English for non-native speakers



"Bound of".
Many non-native speakers use "bound of" when they mean "bound on".
If x≤ k, then we have an upper bound of k on x.  Using
"bound of x" for "bound on x" can become confusing when comparing
parameters.  We do not want to say that the maximum degree Δ(G) is
a bound of the chromatic number χ(G); when Δ(G)=k we
want to say that Δ(G) establishes a bound of k
on the chromatic number of the graph.  (Writers from Asia typically
overuse the preposition "of" when many others are more accurate, such as
"on", "for", "about", etc.)



"few" vs "a few".
In English, "few" means "not many", while "a few" means "several".
The sentence "In this paper we prove few good results" means that the
paper does nothing worthwhile, while "In this paper we prove a few good
results" means that it is worth reading.



"Usual".
It is a quirk of English that the word "usual" as an adjective usually requires
the definite article "the".  We cannot say "In this section we consider only
usual chromatic number"; it must be "In this section we consider only the usual
chromatic number".  (This is a common error by speakers of languages that
do not have articles.)



"Partial case".
In English, we do not say that one result is a "partial case" of another.
We say that it is a "special case".  (This is an issue of translation from one
language to another.)  However, it is correct to say that proving a special
case of a conjecture is a partial result.



"Pass" vs. "Pass through".
In English, the word "pass" means "go by without entering".  Thus a path that
passes a vertex does not visit that vertex.  To say that path P visits
vertex v, one should say that P passes through v
(this is a language translation issue).  Better yet, just use "visits".



"Can not" and "may be".
It appears that some writers of English now use "can not" to mean "cannot".
In speech the two cannot be distinguished, so it doesn't matter, but in written
mathematics we should avoid ambiguities.  The logical meaning of "can not fail"
is "may possibly succeed" while "cannot fail" means "must succeed".  At the
very least, when "can not" is used to mean "cannot" it can be read to have a
different meaning, so it is better to use "cannot" to eliminate the ambiguity.
As in many of these items, the abstractness of mathematical statements in
contrast to the everyday context of English language begs a higher level of
precision and avoidance of ambiguity.


The expression "may be" does exist in English, when used as a verb as in "It
may be true" or "This may be the only component".  However, when it appears at
the start of a clause most likely the word "maybe" is intended, as in "Maybe
this proof will work.  In this situation there is another verb ("work"), and
the initial expression means "Possibly", which is not a verb.



"A joint work".
We speak of "a theorem" or "a result", since these are definite specific items
but "work" is an abstract noun and does not take the
indefinite article "a".  We say simply "This is work of mine", not "This is a
work of mine", and "This is joint work with my colleague," not "This is a joint
work".  This usage of "work" is different from "a work of art" or "the complete
works of Shakespeare".  In mathematics, "work" is equivalent to "research"; we
do not say "this is a joint research".  The same error occurs in
"I will have a limited access to my email."



"Evidently".
Some nonnative speakers write "Evidently" to mean "Clearly".  Although this
word is not technically incorrect, it has other connotations to native
speakers.  Combining this with the fact that they would always write "Clearly"
and never "Evidently", they are confused about what the writer means.
Always change "Evidently" to "Clearly".  ("Evidently" is quite close to
"apparently", which in American English means "seems to be true" rather than
"is true".)


Similarly, "as evidenced by" generally is not used in English; change to "as
shown by".



"Principal" vs. "principle".
"Principal" is an adjective meaning "foremost", used mathematically in
"principal minor".  "Principle" is a noun similar to "idea" or "method",
as in "the Pigeonhole Principle".



More excess commas.
There is no comma before or after "is" in a definition.
There is no comma between "show" and "that".



More expressions not used in English.

"discuss about" ⇒ "discuss".

"studied about" ⇒ "studied".

"equals to" ⇒ "equals" or "is equal to".

"contradicts to" ⇒ "contradicts".

"necessary conditions of" ⇒ "necessary conditions for".

"to precise" ⇒ "to make precise" ("precise" is not a verb).

"a same argument" ⇒ "the same argument" or "a similar argument".

"decompose to" ⇒ "decompose into".

"joint" (as a verb) ⇒ "join" ("joint" is not a verb).

"specially" ⇒ "especially" or "special" ("specially" is not a word)

"usual coloring" ⇒ "ordinary proper coloring"

"We pick up" ⇒ "We consider"



Other material on mathematical writing

To be clear, I do not make positive or negative comments on the material
cited below.  I merely offer these alternatives to show the variety of opinions
on the subject and to indicate that this is an important topic on which people
have strong opinions.



R. Bell, B. Kadets, P. Srinivasan, N. Triantafillou, and I. Vogt,

Practical Suggestions for Mathematical Writing,
Notices AMS 68 (2021), 930-934.

C. Barwick, 

Notes on mathematical writing

B.C. Berndt,

How to write mathematical papers

K. Conrad,

Advice on Mathematical Writing

O. Goldreich,

How to write a paper

D. Goss,

Some hints on mathematical style

P.R. Halmos,

How to write mathematics,
Enseign. Math. (2) 16 (1970), 123-152.

N.J. Higham, Handbook of writing for the mathematical sciences, (SIAM, 1998),
302 pp.  

S.P. Jones, How to write a great research paper,
talk available

D.E. Knuth,

Course notes on mathematical writing (note for example Section 38 on
"which" versus "that").

D.E. Knuth, T.L. Larrabee, and P.M. Roberts, Mathematical Writing, (MAA,
1989), 115 pp.; see also course
notes (an alternative link to the reference above)

S.G. Krantz, A Primer of Mathematical Writing, arXiv:1612.04888, 276 pp.


I. Pak,

How to write a clear math paper: Some 21st century tips

B. Poonen,

Practical suggestions for mathematical writing.

A. Reiter,

Writing a research paper in mathematics

G.-C. Rota, Ten lessons I wish I had been taught, Notices AMS 44 (1997),
22-25.

J.-P. Serre,

How to write mathematics badly.

F. Su,

Some guidelines for good mathematical writing,
MAA Focus 35 (2015), no. 4, 20-22.

T. Tao, On writing,

blog

W. Zinsser, On Writing Well: The Classic Guide to Writing Nonfiction (30th Ed.),
(Harper, 2016), 336 pp.


]]></content:encoded>
        </item>
    </channel>
</rss>