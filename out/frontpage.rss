<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 06 Sep 2025 14:30:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Vibe Coding Through the Berghain Challenge]]></title>
            <link>https://www.nibzard.com/berghain/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149330</guid>
            <description><![CDATA[How my AI coding partner and I obsessed over a nightclub bouncer optimization problem for one intense day]]></description>
            <content:encoded><![CDATA[ # Part 1: The Billboard That Started Everything
Listen Labs just pulled off a solid growth play.
Picture this: Youâ€™re driving through San Francisco and spot a cryptic billboard. Five numbers. No explanation. Just:

Thatâ€™s it. SF billboards are basically expensive Reddit posts hoping to go viral online. And this one worked.
Someone cracked it pretty quicklyâ€”they were token IDs from OpenAIâ€™s tokenizer. Decode them and you get: listenlabs.ai/puzzle. The kind of puzzle that gets shared in Slack channels and Discord servers.
Hit that link and youâ€™re in the Berghain Challenge.
Context: Listen Labs runs an AI-powered customer insights platform. They help companies do qualitative research at scale using AI interviewers. Makes sense theyâ€™d want to attract technical talent with a smart puzzle. Plus, VCs love seeing this kind of creative marketing in their portfolio companies.
The Growth Hack Anatomy
Hereâ€™s what Listen did that was pure genius:

Stage 1: Cryptic billboard â†’ Curiosity
Stage 2: Token puzzle â†’ Technical community engagement
Stage 3: OEIS speculation â†’ Community-driven solving
Stage 4: Berghain Challenge â†’ Viral optimization addiction

They expected 10 concurrent users. They got 30,000 in first hours.
Thatâ€™s a 3000x viral coefficient. Let me repeat that: 3000x.

Alfredâ€™s announcement tweet hit 1.1M views. Zero paid acquisition. Just a billboard and decent understanding of how technical communities work.

The prize? All-expenses Berlin trip plus Berghain guest list. Smart audience targetingâ€”Berlinâ€™s techno scene meets Silicon Valley optimization nerds.
Youâ€™re not just solving a puzzle anymore. Youâ€™re the bouncer at Berlinâ€™s most exclusive nightclub. Your mission? Fill exactly 1,000 spots from a stream of random arrivals. Meet specific quotas. Donâ€™t reject more than 20,000 people.
Sounds simple?
Ha.
When Infrastructure Crashes Create FOMO
The official API wasâ€¦ problematic. Rate limits. Downtime. Maximum 10 parallel games. Slow response times.
But hereâ€™s the thing: Those crashes werenâ€™t bugs. They were features.

Listenâ€™s founder Alfred Wahlforss was tweeting in real-time: â€œwe thought weâ€™d get 10 concurrent users, not 30,000 ğŸ˜… just rebuilt the API to make run smoother ğŸš€â€

Users were refreshing frantically. â€œApplication error: a server-side exception has occurred.â€ Comments like â€œNot sure if this is part of the challenge or if it crashed.â€

Classic scarcity marketing. Canâ€™t access it? Want it more.
Meanwhile, Claude and I were building our own local simulator. Same game mechanics, same statistical distributions, but we could run hundreds of games in parallel without waiting for servers crashing under viral load.
The irony? Listenâ€™s infrastructure struggles created authenticity. Real startups have real scaling problems. The community bought in harder.
Full implementation: https://github.com/nibzard/berghain-challenge-bot
Why This Challenge Will Make You Question Everything
Let me paint the picture of why this problem is mathematically evil.
Youâ€™re standing at the door of Berghain. People arrive one by one. Each person has binary attributes: young/old, well_dressed/casual, male/female, and others. You know the rough frequenciesâ€”about 32.3% are young, 32.3% are well_dressed.
But hereâ€™s the kicker: You must decide immediately. Accept or reject. No takebacks. No â€œlet me think about this.â€ The line keeps moving.
Your constraints for Scenario 1:

Get at least 600 young people
Get at least 600 well_dressed people
Fill exactly 1,000 spots total
Donâ€™t reject more than 20,000 people

â€œEasy,â€ you think. â€œIâ€™ll just accept everyone who helps with a constraint.â€
Wrong.
The attributes are correlated. Some young people are also well_dressed. Accept too many of these â€œdualsâ€ early and youâ€™ll overshoot one quota while undershooting the other. Reject too many and youâ€™ll run out of people.
Itâ€™s a constrained optimization problem wrapped in a deceptively simple game. Youâ€™re essentially solving a real-time resource allocation problem with incomplete information and irreversible decisions.
The Numbers That Haunt Me
After one intense day of obsessive coding with my AI partner, hereâ€™s what we discovered in the arena of 30,000 concurrent solvers:
Listen created an accidental distributed computing experiment. Thousands of engineers, all attacking the same optimization problem. The collective compute power was staggering.
The top performers? Theyâ€™re getting around 650-700 rejections in this massive competitive landscape. The theoretical minimum is probably somewhere around 600-650 rejections, but with 30,000 people trying, nobodyâ€™s found it yet.
Our best algorithm? 781 rejections. We called it RBCR (Re-solving Bid-Price with Confidence Reserves). In a field of 30,000, that put us in serious competitive territory.
Iâ€™ll tell you how we built it, why it works, and why it nearly drove us both insane.
What Makes This So Addictive
Thereâ€™s something deeply satisfying about optimization problems. Each improvement feels like a small victory. Going from 1,200 rejections to 1,150 feels monumental. Then 1,100. Then 1,000. Then you hit a wall and obsess over shaving off single digits.
But this isnâ€™t just about the math. Itâ€™s about the collaboration.
I had an idea. My AI partner implemented it in seconds. We tested it immediately. Iterated. Failed. Learned. Repeated. The feedback loop was intoxicating.
Traditional solo programming? You spend hours implementing a solution only to discover it doesnâ€™t work. With AI assistance? You can test a dozen approaches in the time it used to take to implement one.
This is the story of that collaboration. How we went from clueless to competitive. How AI amplified human intuition. How domain expertise still matters in the age of artificial intelligence.
And how a startupâ€™s growth hack became a day-long obsession with optimization, game theory, and the future of collaborative programming.
This is a dual story: How Listen accidentally created the most engaging technical challenge of 2025, and how human-AI collaboration let us compete in their accidental arena.
Buckle up. Weâ€™re about to dive deep into viral growth mechanics, algorithms, failures, breakthroughs, and the beautiful chaos of when marketing meets engineering obsession.

# Part 2: The Dual Challenge
Iâ€™m a growth advisor with engineering fundamentals. When I saw Listenâ€™s campaign, I immediately recognized two fascinating challenges running in parallel:

Challenge 1: How did a startup 3000x their expected user base with zero paid acquisition?


Challenge 2: How do you solve a constrained optimization problem that has prob the smartest engineers in the world competing against you?

Both challenges required the same core skill: understanding systems, finding leverage points, and optimizing ruthlessly.
The Growth Marketing Masterclass
Listenâ€™s approach was textbook viral growth with a technical twist:
Mystery Phase: Cryptic billboard creates curiosity gap. No explanation = maximum speculation.
Community Phase: Token puzzle activates technical communities. Reddit threads explode. Twitter goes wild. Everyone becomes a detective.
Challenge Phase: Berghain game provides clear success metrics. Immediate feedback loop. Addictive optimization cycle.
Competition Phase: Leaderboard dynamics create retention. Status through technical skill. Perfect product-market fit for engineering egos.
The brilliant part? Each phase filtered for higher engagement. Casual observers dropped off. Technical obsessives doubled down.
The Viral Mechanics
From a growth perspective, Listen nailed every viral coefficient multiplier:

Curiosity Gap: Mysterious billboard â†’ high shareability
Community Solving: Group puzzle â†’ network effects
Status Competition: Technical leaderboard â†’ ego investment
Infrastructure Struggles: â€œCanâ€™t accessâ€ â†’ scarcity psychology

The 3000x multiplier wasnâ€™t luck. It was systematic exploitation of technical community psychology.
The Engineering Obsession
From a technical perspective, this problem was crack cocaine for optimization addicts:

Clear Success Metrics: Rejection count goes down = dopamine hit
Immediate Feedback: Test algorithm, get result instantly
Competitive Context: 30,000 people trying to beat you
Deep Complexity: Simple rules, emergent mathematical beauty

Perfect storm for engineering obsession.
Where Marketing Met Engineering
The genius of Listenâ€™s approach: They created a problem that required both growth mindset and technical depth.
Understanding the viral mechanics helped me see why the challenge was so engaging. Understanding the optimization problem helped me see why the growth worked so well.
Marketing created the arena. Engineering filled it with obsessives.
Time to tell you how we became one of those obsessives.

# Part 3: Day 1 - The Naive Optimism Phase
â€œHey Claude, I found this interesting challenge. Itâ€™s about being a nightclub bouncer and optimizing admissions. Want to help me solve it?â€
Famous last words.
I was expecting maybe an hour of casual problem-solving. You know, write a simple algorithm, test it, maybe optimize it a bit, call it a day.
By the end of the day, Iâ€™m staring at 30+ solver implementations, thousands of lines of code, and a monitoring dashboard that looks like mission control.
But letâ€™s start at the beginning.
The First Attempt: Greedy and Naive
Me: â€œLetâ€™s start simple. Just accept anyone who helps with our constraints.â€
Claude: â€œYouâ€™re absolutely right! Hereâ€™s a greedy approach:â€
def should_accept(person, game_state):
    # Accept if person helps with any unmet constraint
    for constraint in game_state.constraints:
        if person.has_attribute(constraint.attribute):
            shortage = constraint.min_count - game_state.admitted_attributes[constraint.attribute]
            if shortage > 0:
                return True, f"needed_for_{constraint.attribute}"

    # Otherwise, maybe accept a few randoms
    return random.random() < 0.05, "filler"
Me: â€œPerfect! This should work great.â€
Famous last words, part two.
We fired it up. Results: 1,247 rejections.
Ouch.
Claude: â€œThe issue is weâ€™re being too greedy early. We accept everyone whoâ€™s young OR well_dressed, but many people are both. We overshoot one constraint while undershooting the other.â€
The Second Attempt: Tracking Deficits
Me: â€œOkay, so we need to track how much we still need of each attribute and be smarter about it.â€
Claude: â€œI can implement a deficit-aware strategy:â€
def should_accept(person, game_state):
    shortage = game_state.constraint_shortage()

    # Calculate how much this person helps
    young = person.young and shortage['young'] > 0
    well_dressed = person.well_dressed and shortage['well_dressed'] > 0

    if young and well_dressed:
        return True, "dual_helper"  # Helps both constraints
    elif young or well_dressed:
        return random.random() < 0.7, "single_helper"
    else:
        return random.random() < 0.02, "filler"
Better! Down to 1,098 rejections.
Still terrible, but progress.
The Third Attempt: Getting Desperate
Me: â€œWhat if weâ€™re more selective early on? Only accept the really good candidates?â€
Claude: â€œWe could implement phases based on capacity usage:â€
def should_accept(person, game_state):
    capacity_ratio = game_state.admitted_count / 1000.0
    shortage = game_state.constraint_shortage()

    young_helps = person.young and shortage['young'] > 0
    dressed_helps = person.well_dressed and shortage['well_dressed'] > 0

    if capacity_ratio < 0.3:  # Early phase - be picky
        if young_helps and dressed_helps:
            return True, "early_dual"
        return False, "early_reject"

    elif capacity_ratio < 0.7:  # Mid phase - moderate
        if young_helps or dressed_helps:
            return random.random() < 0.6, "mid_helper"
        return False, "mid_reject"

    else:  # Late phase - panic mode
        if young_helps or dressed_helps:
            return True, "late_helper"
        return random.random() < 0.1, "late_filler"
Results: 943 rejections.
We were getting somewhere! But also realizing this problem was way harder than expected.
The Debugging Session
Me: â€œWait, letâ€™s actually understand whatâ€™s going wrong. Can you add detailed logging?â€
Claude: â€œOf course! Let me instrument everything:â€
def should_accept(person, game_state):
    # ... decision logic ...

    # Log everything
    logger.info(f"Person {game_state.person_count}: "
                f"young={person.young}, dressed={person.well_dressed}, "
                f"decision={decision}, reason='{reason}', "
                f"capacity={game_state.admitted_count}/1000, "
                f"young_deficit={shortage['young']}, "
                f"dressed_deficit={shortage['well_dressed']}")

    return decision, reason
Running this, we could see exactly what was happening. The logs were brutal:
Person 1247: young=True, dressed=False, decision=True, reason='young_needed'
Person 1248: young=False, dressed=True, decision=True, reason='dressed_needed'
Person 1249: young=True, dressed=True, decision=True, reason='dual_jackpot'
...
Person 15673: young=False, dressed=False, decision=False, reason='useless'
GAME OVER: young_deficit=127, dressed_deficit=43, capacity=953/1000
We were consistently undershooting our quotas while running out of capacity. Classic resource allocation failure.
The Facepalm Moment
Me: â€œOh god. Weâ€™re not accounting for the probabilities properly. If only 32% of people are young, and we need 600 young people out of 1000 total spots, we actually need to accept likeâ€¦ 90%+ of young people we see.â€
Claude: â€œExactly! And the correlation between attributes makes it even more complex. A person whoâ€™s both young and well_dressed is incredibly valuable because they satisfy both constraints simultaneously.â€
Me: â€œWe need to think about this probabilistically. Whatâ€™s the expected value of accepting this person given our current state and the remaining slots?â€
Claude: â€œThat sounds like we need to model this as an optimization problem with uncertaintyâ€¦â€
And thatâ€™s when I realized we werenâ€™t just building a simple algorithm anymore.
We were diving into operations research territory. Stochastic optimization. Dynamic programming. Multi-objective decision making under uncertainty.
All for a nightclub bouncer simulation.
Day 1 Wrap-Up: Reality Check
By the end of day one, our best solution was still sitting at 943 rejections. Respectable improvement from 1,200+, but nowhere near competitive.
More importantly, we had a much clearer picture of why this problem was hard:

Resource constraints: Limited capacity (1000 spots)
Correlated attributes: People who are young AND well_dressed are gold
Uncertain arrival patterns: You never know whatâ€™s coming next
Irreversible decisions: No takebacks once you decide
Multiple objectives: Two quotas plus capacity limit

Me: â€œTomorrow, weâ€™re going to need to get mathematical about this.â€
Claude: â€œIâ€™m ready. Should we start reading about constrained optimization?â€
Little did we know, we were about to discover Lagrangian multipliers, bid-price mechanisms, and the beautiful world of dual variable optimization.
Day two was going to be very different from day one.

# Part 4: The Statistical Awakening
A few hours later, I had a growth insight: viral challenges work because they create addiction loops.
Listen had nailed the psychology. Every algorithm improvement = dopamine hit. Every leaderboard check = social comparison. Every failed attempt = â€œjust one more try.â€
With 30,000 engineers now obsessing, the competition was heating up.
Me: â€œClaude, weâ€™ve been treating each decision independently. But this is really about managing scarce resources over time. We need to think about opportunity costs.â€
Claude: â€œYouâ€™re absolutely right! Each acceptance now affects our options later. If we accept too many single-attribute people early, we might not have room for dual-attribute people who are more efficient.â€
Me: â€œExactly! And we need to use statistics properly. What are the actual probabilities here?â€
Understanding the Data
First, we dove into the attribute frequencies. The challenge gives you some basic stats, but we needed to understand the correlations.
# From the game statistics
frequencies = {
    'young': 0.323,        # 32.3% of people are young
    'well_dressed': 0.323,  # 32.3% are well_dressed
}

# The correlation coefficient between young and well_dressed
correlation = 0.076  # Slight positive correlation
Claude: â€œLet me calculate the joint probabilities:â€
import math

def calculate_joint_probabilities(p_young, p_dressed, correlation):
    # Convert correlation to covariance
    denom = math.sqrt(p_young * (1-p_young) * p_dressed * (1-p_dressed))
    covariance = correlation * denom

    # Joint probabilities
    p_both = p_young * p_dressed + covariance
    p_young_only = p_young - p_both
    p_dressed_only = p_dressed - p_both
    p_neither = 1 - (p_both + p_young_only + p_dressed_only)

    return p_both, p_young_only, p_dressed_only, p_neither

# Results:
# P(both young AND well_dressed) â‰ˆ 0.110
# P(young only) â‰ˆ 0.213
# P(well_dressed only) â‰ˆ 0.213
# P(neither) â‰ˆ 0.464
This was eye-opening. About 11% of people help with BOTH constraints. These â€œdualâ€ people are incredibly valuableâ€”each one gets us closer to both quotas simultaneously.
The Value Function Epiphany
Me: â€œWe need to assign values to different types of people based on how much they help us.â€
Claude: â€œA value function based on remaining deficits! Hereâ€™s what Iâ€™m thinking:â€
def calculate_person_value(person, game_state):
    shortage = game_state.constraint_shortage()
    value = 0

    if person.young and shortage['young'] > 0:
        value += 1.0  # Base value for helping young quota

    if person.well_dressed and shortage['well_dressed'] > 0:
        value += 1.0  # Base value for helping dressed quota

    # Bonus for dual attributes (more efficient use of capacity)
    if person.young and person.well_dressed:
        if shortage['young'] > 0 and shortage['well_dressed'] > 0:
            value += 0.5  # Efficiency bonus

    return value
Me: â€œBut wait. The value should depend on scarcity too. If weâ€™re almost done with young people but need lots of well_dressed people, a well_dressed person is worth more than a young person.â€
Claude: â€œAh, like dynamic pricing! The scarcer the resource, the higher its value:â€œ
def calculate_person_value(person, game_state):
    shortage = game_state.constraint_shortage()
    remaining_slots = 1000 - game_state.admitted_count

    value = 0

    if person.young and shortage['young'] > 0:
        # Value increases as shortage becomes more critical
        scarcity_multiplier = shortage['young'] / remaining_slots
        value += scarcity_multiplier

    if person.well_dressed and shortage['well_dressed'] > 0:
        scarcity_multiplier = shortage['well_dressed'] / remaining_slots
        value += scarcity_multiplier

    return value
The Acceptance Probability Function
Now we had values, but we needed to convert them to acceptance probabilities. Accept everyone with high value? Too greedy. Accept nobody? Too conservative.
Me: â€œWhat if we use a sigmoid function? High value â†’ high probability, low value â†’ low probability, but with some randomness.â€
Claude: â€œPerfect! And we can tune the temperature parameter to control how selective we are:â€œ
import math

def acceptance_probability(value, temperature=2.0):
    """Convert value to acceptance probability using sigmoid"""
    return 1.0 / (1.0 + math.exp(-value / temperature))

# Example:
# value = 0.5 â†’ probability â‰ˆ 0.62
# value = 1.0 â†’ probability â‰ˆ 0.73
# value = 1.5 â†’ probability â‰ˆ 0.82
# value = 2.0 â†’ probability â‰ˆ 0.88
The First Statistical Solver
Putting it all together:
class StatisticalSolver:
    def __init__(self, temperature=2.0):
        self.temperature = temperature

    def should_accept(self, person, game_state):
        # Calculate person's value based on current needs
        value = self.calculate_person_value(person, game_state)

        # Convert to acceptance probability
        prob = self.acceptance_probability(value)

        # Make random decision based on probability
        decision = random.random() < prob

        reason = f"value={value:.2f}_prob={prob:.2f}"
        return decision, reason

    def calculate_person_value(self, person, game_state):
        shortage = game_state.constraint_shortage()
        remaining_slots = max(1, 1000 - game_state.admitted_count)

        value = 0.0

        if person.young and shortage['young'] > 0:
            urgency = shortage['young'] / remaining_slots
            value += urgency

        if person.well_dressed and shortage['well_dressed'] > 0:
            urgency = shortage['well_dressed'] / remaining_slots
            value += urgency

        return value
Results: 847 rejections!
Holy shit. We dropped from 943 to 847 with one key insight: think probabilistically, not deterministically.
Fine-Tuning the Parameters
Me: â€œThe temperature parameter is crucial. Too high and we accept too many low-value people. Too low and weâ€™re too picky.â€
Claude: â€œLet me run some parameter sweeps:â€
# Testing different temperatures
results = []
for temp in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:
    solver = StatisticalSolver(temperature=temp)
    avg_rejections = run_multiple_games(solver, num_games=10)
    results.append((temp, avg_rejections))
    print(f"Temperature {temp}: {avg_rejections:.1f} rejections")

# Results:
# Temperature 0.5: 1,245 rejections (too picky)
# Temperature 1.0: 934 rejections
# Temperature 1.5: 847 rejections  â† sweet spot
# Temperature 2.0: 892 rejections
# Temperature 2.5: 967 rejections (too accepting)
# Temperature 3.0: 1,078 rejections
Temperature = 1.5 was our sweet spot. Not too hot, not too cold.
Adding Phase-Based Logic
Me: â€œWe should probably be more aggressive late in the game when weâ€™re running out of people.â€
Claude: â€œAdaptive temperature based on game phase?â€
def get_adaptive_temperature(self, game_state):
    capacity_ratio = game_state.admitted_count / 1000.0

    if capacity_ratio < 0.4:
        return 1.2  # Early game: be selective
    elif capacity_ratio < 0.8:
        return 1.5  # Mid game: balanced
    else:
        return 2.2  # Late game: more aggressive
Results: 821 rejections.
We were getting there! Each insight was shaving off 20-50 rejections.
The Monitoring Dashboard
At this point, we had enough complexity that debugging became hard. So we built a real-time monitoring system.
Claude: â€œLet me create a TUI dashboard so we can watch the algorithm in action:â€
from rich.live import Live
from rich.table import Table
from rich.panel import Panel

class GameMonitor:
    def display_status(self, game_state, last_decision):
        table = Table(title="Berghain Bouncer Status")
        table.add_column("Metric", style="bold")
        table.add_column("Value", style="green")

        table.add_row("Admitted", f"{game_state.admitted_count}/1000")
        table.add_row("Young", f"{game_state.admitted_attributes['young']}/600")
        table.add_row("Well Dressed", f"{game_state.admitted_attributes['well_dressed']}/600")
        table.add_row("Rejections", str(game_state.rejection_count))
        table.add_row("Last Decision", last_decision)

        return Panel(table, title="Live Game Status")

Watching the dashboard was mesmerizing. You could see the deficits shrinking, the capacity filling up, the algorithm making split-second decisions.
Sometimes it would reject a dual-attribute person early in the game (seemed wasteful) but accept a single-attribute person later (made sense given the remaining needs).
Me: â€œItâ€™s actually working! The algorithm is learning to balance short-term and long-term value.â€
Claude: â€œThe statistical approach is much more robust than our previous heuristics. Weâ€™re making decisions based on actual probabilities rather than gut feelings.â€
End of Day 2: Statistical Success
By end of day two, we had:

âœ… Dropped from 943 to 821 rejections
âœ… Built a probabilistic decision framework
âœ… Implemented adaptive parameters
âœ… Created a real-time monitoring system
âœ… Understood the mathematical structure of the problem

Me: â€œ821 rejections puts us in decent territory, but I keep thinking thereâ€™s a more principled approach. This feels like an operations research problem.â€
Claude: â€œYouâ€™re thinking about optimal stopping theory? Or maybe linear programming?â€
Me: â€œExactly. Tomorrow, letâ€™s get serious about the math. I want to understand this problem from first principles.â€
Little did we know, day three would introduce us to Lagrangian multipliers, dual variables, and the most elegant algorithm weâ€™d build: RBCR (Re-solving Bid-Price with Confidence Reserves).
The statistical awakening was just the beginning.

# Part 5: The Mathematical Enlightenment
Later that day. Iâ€™m lying in bed thinking about Lagrangian multipliers.
This is what optimization problems do to you. They crawl into your brain and set up camp.
Me: â€œClaude, I canâ€™t sleep. I keep thinking about this problem as a constrained optimization. What if we model it with dual variables?â€
Claude: â€œAt 3 AM? Iâ€™m always available! Tell me what youâ€™re thinking.â€
Me: â€œIn economics, when you have scarce resources, you use prices to allocate them efficiently. What if we assign â€˜pricesâ€™ to our constraints? Higher price means we really need that attribute.â€
The Lagrangian Insight
Claude: â€œYouâ€™re talking about Lagrangian multipliers! In constrained optimization, the multipliers represent the shadow pricesâ€”how much the objective would improve if we relaxed each constraint slightly.â€
Me: â€œExactly! So if we desperately need young people, the â€˜priceâ€™ for young should be high. If we desperately need well_dressed people, that price should be high too.â€
Hereâ€™s the key insight: Instead of static value functions, we could have dynamic prices that adjust based on how urgent each constraint becomes.
Claude: â€œLet me formalize this. We want to minimize rejections subject to:â€
minimize: rejections
subject to: young_count >= 600
           dressed_count >= 600
           total_count <= 1000
Me: â€œAnd the Lagrangian multipliers Î»_young and Î»_dressed tell us the â€˜urgencyâ€™ of each constraint at any given moment.â€
Implementing Dual Variables
Claude: â€œHereâ€™s how we can compute the multipliers dynamically:â€
class DualVariableSolver:
    def __init__(self):
        self.lambda_young = 0.0
        self.lambda_dressed = 0.0

    def update_dual_variables(self, game_state):
        """Update dual variables based on current deficits"""
        shortage = game_state.constraint_shortage()
        remaining_slots = max(1, 1000 - game_state.admitted_count)

        # Expected helpful arrivals per remaining slot
        young_help_rate = self.estimate_helpful_rate('young', game_state)
        dressed_help_rate = self.estimate_helpful_rate('dressed', game_state)

        # Dual variables = deficit / expected helpful arrivals
        self.lambda_young = shortage['young'] / max(young_help_rate * remaining_slots, 1e-6)
        self.lambda_dressed = shortage['dressed'] / max(dressed_help_rate * remaining_slots, 1e-6)

    def estimate_helpful_rate(self, attribute, game_state):
        """Estimate probability that next person will help with this attribute"""
        if attribute == 'young':
            return 0.323  # Base frequency of young people
        elif attribute == 'dressed':
            return 0.323  # Base frequency of well_dressed people
        return 0.0

    def should_accept(self, person, game_state):
        # Update dual variables first
        self.update_dual_variables(game_state)

        # Calculate person's dual value
        dual_value = 0.0

        if person.young and game_state.constraint_shortage()['young'] > 0:
            dual_value += self.lambda_young

        if person.well_dressed and game_state.constraint_shortage()['dressed'] > 0:
            dual_value += self.lambda_dressed

        # Accept if dual value exceeds threshold
        threshold = 1.0  # Tunable parameter
        decision = dual_value >= threshold

        reason = f"dual_value={dual_value:.2f}_Î»y={self.lambda_young:.2f}_Î»d={self.lambda_dressed:.2f}"
        return decision, reason
Results: 782 rejections!
Weâ€™d broken through 800! This was our best result yet.
But Wait, Thereâ€™s More
Me: â€œThis is working, but I think weâ€™re missing something. The threshold is static, but it should probably adapt based on how full we are.â€
Claude: â€œYouâ€™re right! Early in the game we can be picky (high threshold). Late in the game we should be desperate (low threshold).â€
def get_adaptive_threshold(self, game_state):
    capacity_ratio = game_state.admitted_count / 1000.0
    rejection_ratio = game_state.rejection_count / 20000.0

    # Start high, end low
    base_threshold = 1.5 - capacity_ratio

    # Panic if we're running out of rejections
    if rejection_ratio > 0.8:
        base_threshold *= 0.5  # Emergency mode

    return max(0.1, base_threshold)
The RBCR Revolution
Me: â€œWhat if we resolve the dual variables periodically? Like every 50 arrivals, we re-estimate our helper rates and update our strategy?â€
Claude: â€œRe-solving Bid-Price with Confidence Reserves! We could call it RBCR.â€
This was the breakthrough moment. Instead of updating duals every single decision, weâ€™d batch them. Every 50 arrivals:

Look at our current deficit
Estimate remaining helpful arrival rates
Recompute dual variables
Set acceptance thresholds accordingly

class RBCRSolver:
    def __init__(self):
        self.lambda_young = 0.0
        self.lambda_dressed = 0.0
        self.resolve_counter = 0
        self.resolve_every = 50

    def should_accept(self, person, game_state):
        # Periodically resolve dual variables
        if self.resolve_counter % self.resolve_every == 0:
            self.resolve_duals(game_state)
        self.resolve_counter += 1

        # Calculate dual value for this person
        dual_value = self.calculate_dual_value(person, game_state)

        # Adaptive threshold based on game state
        threshold = self.get_adaptive_threshold(game_state)

        # Accept if value exceeds threshold
        decision = dual_value >= threshold

        return decision, f"dv={dual_value:.2f}_th={threshold:.2f}"

    def resolve_duals(self, game_state):
        """The heart of RBCR - recompute dual variables"""
        shortage = game_state.constraint_shortage()
        remaining_slots = max(1, 1000 - game_state.admitted_count)

        # Estimate help rates (this is where the magic happens)
        young_rate = self.estimate_young_help_rate(game_state)
        dressed_rate = self.estimate_dressed_help_rate(game_state)

        # Expected helpful arrivals = rate * remaining_slots
        expected_young_help = young_rate * remaining_slots
        expected_dressed_help = dressed_rate * remaining_slots

        # Dual variables = deficit / expected_help
        self.lambda_young = shortage['young'] / max(expected_young_help, 1e-6)
        self.lambda_dressed = shortage['dressed'] / max(expected_dressed_help, 1e-6)
Results: 781 rejections.
Weâ€™d found our winner! RBCR was consistently hitting the low 780s.
The Beautiful Math Behind RBCR
Hereâ€™s why this approach is so elegant:


Dual variables capture urgency: When you desperately need young people, Î»_young shoots up, making young people more valuable.


Periodic resolution is efficient: We donâ€™t need to recompute every single decisionâ€”every 50 arrivals is enough.


Adaptive thresholds handle phases: Early pickiness, late desperation, all handled automatically.


Self-correcting: If weâ€™re accepting too many of one type, the deficit shrinks, the dual variable drops, we become less likely to accept more.


The math was doing exactly what a good bouncer would do: pay attention to what you need most, be pickier when you have time, be desperate when youâ€™re running out of options.
The Debugging Session That Made Us Believers
Me: â€œLetâ€™s trace through a game step by step and see the duals in action.â€
Game Start:
shortage: young=600, dressed=600
Î»_young=1.85, Î»_dressed=1.85

Person 1: young=True, dressed=True
dual_value = 1.85 + 1.85 = 3.70
threshold = 1.50
ACCEPT (dual person is incredibly valuable)

...

Person 500: young=True, dressed=False
shortage: young=234, dressed=178
Î»_young=0.95, Î»_dressed=1.23
dual_value = 0.95
threshold = 1.20
REJECT (young is less urgent now)

Person 501: young=False, dressed=True
dual_value = 1.23
threshold = 1.20
ACCEPT (dressed is still urgent)
Claude: â€œItâ€™s beautiful! The dual variables automatically rebalance based on remaining needs. The algorithm develops intuition.â€
Me: â€œAnd look at the late game behavior:â€
Person 950: young=False, dressed=False
shortage: young=12, dressed=3
Î»_young=0.78, Î»_dressed=0.18
dual_value = 0.0
threshold = 0.30
REJECT (we're almost done, be picky)

Person 951: young=True, dressed=False
dual_value = 0.78
threshold = 0.30
ACCEPT (still need a few young people)
The algorithm had learned to be surgical in the endgame.
Why 781 Felt Like Victory
After two days of grinding, seeing that 781 was intoxicating. It wasnâ€™t just the numberâ€”it was the elegance.
RBCR felt right in a way our previous algorithms didnâ€™t. The decisions made intuitive sense. The math was principled. The performance was consistent.
Me: â€œI think we found our killer algorithm.â€
Claude: â€œThe dual variable approach captures the essence of the problem. Weâ€™re explicitly modeling scarcity and urgency.â€
Me: â€œBut I have a terrible feeling there are even more optimizations we could makeâ€¦â€
And thatâ€™s how day three ended. Not with satisfaction, but with the dangerous realization that we could probably make RBCR even better.
The mathematical enlightenment was complete. We understood the problem from first principles. We had elegant, principled algorithms.
Now came the dangerous part: the obsession with perfection.

# Part 6: The Kitchen Sink Era
Have you ever solved a problem so elegantly that you immediately want to ruin it with unnecessary complexity?
Thatâ€™s exactly what happened next.
RBCR was working beautifully at 781 rejections. Any reasonable person would have stopped there. But we werenâ€™t reasonable people anymore. We were optimization addicts, and 781 felt tantalizingly close to something even better.
Me: â€œWhat if we add a feasibility oracle?â€
Claude: â€œA what now?â€
Me: â€œA statistical confidence check. Before accepting someone, we simulate forward and check if we can still meet our constraints with high probability.â€
This is where things got complicated.
The Feasibility Oracle
The idea was seductive. Instead of just looking at current deficits, what if we could estimate whether accepting this person would put us in a mathematically impossible situation later?
Claude: â€œI can implement a Monte Carlo simulation approach:â€
class FeasibilityOracle:
    def __init__(self, p11, p10, p01, p00, confidence=0.95):
        """
        p11: P(young AND well_dressed)
        p10: P(young only)
        p01: P(well_dressed only)
        p00: P(neither)
        """
        self.p11, self.p10, self.p01, self.p00 = p11, p10, p01, p00
        self.confidence = confidence
        self.samples = 1000

    def is_feasible(self, admitted_young, admitted_dressed, admitted_total, target_capacity):
        """Check if we can still meet constraints with high probability"""
        remaining_slots = target_capacity - admitted_total
        young_needed = max(0, 600 - admitted_young)
        dressed_needed = max(0, 600 - admitted_dressed)

        if remaining_slots <= 0:
            return young_needed == 0 and dressed_needed == 0

        # Monte Carlo simulation
        successes = 0

        for _ in range(self.samples):
            sim_young = admitted_young
            sim_dressed = admitted_dressed

            # Simulate remaining arrivals
            for _ in range(remaining_slots):
                rand = random.random()

                if rand < self.p11:  # both young and dressed
                    sim_young += 1
                    sim_dressed += 1
                elif rand < self.p11 + self.p10:  # young only
                    sim_young += 1
                elif rand < self.p11 + self.p10 + self.p01:  # dressed only
                    sim_dressed += 1
                # else: neither (p00)

            # Check if constraints satisfied
            if sim_young >= 600 and sim_dressed >= 600:
                successes += 1

        return (successes / self.samples) >= self.confidence
Me: â€œNow we can check feasibility before every accept decision!â€
RBCR + Feasibility = RBCR2
We bolted the feasibility oracle onto RBCR:
class RBCR2Solver(RBCRSolver):
    def __init__(self):
        super().__init__()
        # Precompute joint probabilities from correlation data
        self.oracle = FeasibilityOracle(0.110, 0.213, 0.213, 0.464)

    def should_accept(self, person, game_state):
        # Run normal RBCR logic
        rbcr_decision, rbcr_reason = super().should_accept(person, game_state)

        if not rbcr_decision:
            return False, rbcr_reason

        # If RBCR says accept, check feasibility
        # Simulate accepting this person
        sim_young = game_state.admitted_attributes['young']
        sim_dressed = game_state.admitted_attributes['well_dressed']
        sim_total = game_state.admitted_count

        if person.young:
            sim_young += 1
        if person.well_dressed:
            sim_dressed += 1
        sim_total += 1

        # Check if this acceptance keeps us feasible
        if self.oracle.is_feasible(sim_young, sim_dressed, sim_total, 1000):
            return True, f"{rbcr_reason}_feasible"
        else:
            return False, f"{rbcr_reason}_infeasible"
Results: 823 rejections.
Wait. What?
The Paradox of Perfection
We made RBCR â€œsmarterâ€ and it got worse. This was our first taste of a crucial lesson: more sophistication doesnâ€™t always mean better performance.
Me: â€œThe feasibility oracle is being too conservative. Itâ€™s rejecting people because of low-probability failure scenarios.â€
Claude: â€œThe confidence threshold is too high. At 95% confidence, weâ€™re only accepting people if weâ€™re almost certain weâ€™ll succeed. Thatâ€™s overly cautious.â€
We tried tuning the confidence down to 80%, then 70%, then 60%. The performance improved but never matched the original RBCR.
Me: â€œLetâ€™s try a different approach. What if we build an ensemble of strategies?â€
The Ultimate Solver
This is where we completely lost our minds.
Claude: â€œWe could combine the best ideas from all our solvers!â€
class UltimateSolver:
    def __init__(self):
        self.rbcr = RBCRSolver()
        self.statistical = StatisticalSolver()
        self.oracle = FeasibilityOracle(0.110, 0.213, 0.213, 0.464)

        # Phase-based weights
        self.phase_weights = {
            'early': {'rbcr': 0.7, 'statistical': 0.3},
            'mid': {'rbcr': 0.8, 'statistical': 0.2},
            'late': {'rbcr': 0.6, 'statistical': 0.4}
        }

    def should_accept(self, person, game_state):
        # Get decisions from multiple strategies
        rbcr_decision, rbcr_reason = self.rbcr.should_accept(person, game_state)
        stat_decision, stat_reason = self.statistical.should_accept(person, game_state)

        # Determine current phase
        capacity_ratio = game_state.admitted_count / 1000.0
        if capacity_ratio < 0.4:
            phase = 'early'
        elif capacity_ratio < 0.8:
            phase = 'mid'
        else:
            phase = 'late'

        # Weighted vote
        weights = self.phase_weights[phase]
        score = (weights['rbcr'] * rbcr_decision +
                 weights['statistical'] * stat_decision)

        # Feasibility check
        if score > 0.5:
            # Check feasibility before final accept
            if self.is_acceptance_feasible(person, game_state):
                return True, f"ensemble_accept_{phase}"
            else:
                return False, f"ensemble_feasibility_reject_{phase}"
        else:
            return False, f"ensemble_reject_{phase}"
Results: 798 rejections.
Still not as good as vanilla RBCR!
The Naming Convention Goes Off the Rails
At this point, our naming started reflecting our desperation:

Ultimate2Solver: Added momentum terms to dual variables
Ultimate3Solver: Added multi-step lookahead
Ultimate3hSolver: Ultimate3 with â€œheuristic improvementsâ€
PerfectSolver: Attempt at mathematical perfection (spoiler: it wasnâ€™t)
ApexSolver: â€œThis is surely the apex of our workâ€ (it wasnâ€™t)

Each one had elaborate justifications. Each one performed slightly worse than RBCR.
The Moment of Clarity
After implementing our 15th variant, I had an epiphany:
Me: â€œClaude, I think weâ€™ve been overthinking this.â€
Claude: â€œHow so?â€
Me: â€œRBCR works because itâ€™s simple and principled. It models the core economics of the problemâ€”scarcity and urgencyâ€”without overengineering.â€
Claude: â€œYouâ€™re saying our sophisticated additions are fighting against the core algorithm?â€
Me: â€œExactly. The feasibility oracle makes us too conservative. The ensemble methods muddy the decision boundary. The multi-step lookahead assumes we can predict randomness.â€
The Law of Diminishing Returns
Hereâ€™s what we learned the hard way:





















































AlgorithmRejectionsKey InnovationWhy It FailedRBCR781Dual variablesâœ… (our winner)RBCR2823+ Feasibility oracleToo conservativeUltimate798+ Ensemble methodsCompeting signalsUltimate2789+ Momentum termsOversmoothingUltimate3795+ LookaheadUnpredictable randomnessPerfect812+ â€œMathematical perfectionâ€HubrisApex802+ Kitchen sinkToo much complexity
Every addition made the algorithm more complex but less effective.
The Code Generation Velocity
But hereâ€™s the thing: even though most of our elaborations failed, the speed at which we could generate and test them was incredible.
Me: â€œLetâ€™s try adding a confidence interval to the dual variables.â€
Claude: â€œHereâ€™s the implementation:â€ [30 seconds later, fully coded solution]
Me: â€œActually, what if we use a Bayesian update instead?â€
Claude: â€œUpdated:â€ [45 seconds later, completely different approach]
In traditional programming, each of these experiments would have taken hours to implement. With AI assistance, we could test a new approach every few minutes.
This velocity was both a blessing and a curse. It enabled rapid exploration but also made it easy to fall down rabbit holes.
The Performance Wall
After a day of kitchen-sink engineering, we hit a performance wall. Nothing we tried could consistently beat 781 rejections.
Me: â€œI think 781 might be close to optimal for our approach. To do better, we might need a completely different paradigm.â€
Claude: â€œWhat kind of different paradigm?â€
Me: â€œMachine learning. What if we train a neural network on optimal game play?â€
Famous last words, part three.
The Beautiful Failure
Looking back, the kitchen sink era wasnâ€™t a waste. We learned crucial lessons:

Simplicity often beats complexity in optimization problems
The first principled solution is usually close to optimal
Rapid iteration enables exploration but also enables overengineering
Domain expertise beats sophisticated algorithms that ignore problem structure

Most importantly, we learned that having an AI coding partner makes it dangerously easy to overcomplicate things. The speed of implementation can outpace the wisdom of restraint.
RBCR remained our champion at 781 rejections. Simple, elegant, and consistently effective.
But we werenâ€™t done yet. The siren call of deep learning was too strong to resist.

# Part 7: The ML Detour That Wasnâ€™t
Me: â€œClaude, what if we train a neural network to learn from our best games?â€
Claude: â€œI can build an LSTM policy network. We have 162 elite games with full decision histories. Thatâ€™s our training data.â€
This seemed logical. We had a dataset of high-quality gameplay. Why not learn from it?
The Data Pipeline
Claude got to work on the data preprocessing while I grabbed coffee.
# Elite game data structure
elite_game = {
    "strategy_name": "rbcr",
    "final_rejections": 781,
    "decisions": [
        {"person_id": 1, "young": True, "well_dressed": False, "decision": True, "reason": "dual_value_high"},
        {"person_id": 2, "young": False, "well_dressed": True, "decision": False, "reason": "threshold_low"},
        # ... thousands more decisions
    ],
    "final_stats": {"young": 612, "well_dressed": 603, "admitted": 1000}
}
[Full data pipeline: https://github.com/nibzard/berghain-challenge-bot/blob/main/berghain/training/enhanced_data_preprocessor.py]
The idea: convert each decision into a feature vector containing game state + person attributes, with the elite algorithmâ€™s decision as the target.
The LSTM Architecture
class LSTMPolicyNetwork(nn.Module):
    def __init__(self, input_dim=15, hidden_dim=256, num_layers=3):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # [reject_prob, accept_prob]
        )
[Full architecture: https://github.com/nibzard/berghain-challenge-bot/blob/main/berghain/training/lstm_policy.py]
The Training Reality Check
Me: â€œHowâ€™s the training going?â€
Claude: â€œIâ€™ve run 58 experiments. The model is learning the patterns, butâ€¦â€
Me: â€œBut?â€
Claude: â€œPerformance is underwhelming. Best result: 934 rejections.â€
934 rejections. Worse than our statistical solver from day two.
Why Deep Learning Failed Here
The post-mortem was brutal but educational:
1. Sparse Rewards Problem

Only 162 elite games out of 3,902 total
Most training data was suboptimal gameplayâ€”short simulation runs with poor performance
Not enough high-quality examples
Switched back to MacBook M4 for training (more than sufficient compute)

2. Sequential Decision Complexity

Each decision affects all future decisions
LSTM had to learn both tactics AND strategy
Credit assignment across 1000+ decisions is hard

3. The Goldilocks Problem

Too little data for deep learning to shine
Too much complexity for simple supervised learning
Stuck in the worst of both worlds

4. Distribution Mismatch

Training on RBCR decisions
Testing on novel game states
Model couldnâ€™t generalize beyond training distribution

The Google Colab Adventure
We tried scaling up. Free GPUs! More compute! Surely that would fix it.
[Colab instructions: https://github.com/nibzard/berghain-challenge-bot/blob/main/COLAB.md]
Results after 30 minutes of training: 912 rejections.
Slightly worse.
When GPT-5 Came to the Rescue
I was getting frustrated. Claude had hit some token limits. Time to call in reinforcements.
Me: â€œGPT-5, can you review this LSTM approach and tell me what weâ€™re missing?â€
GPT-5: â€œThe fundamental issue is that youâ€™re treating this as a supervised learning problem when itâ€™s really a reinforcement learning problem. Your labels arenâ€™t ground truthâ€”theyâ€™re just one algorithmâ€™s choices. Try policy gradient methods instead.â€
Me: â€œClaude, can you implement PPO?â€
Claude: â€œAbsolutely! Hereâ€™s the PPO implementation:â€ [But then Claude hit the session token limit and we had to wait 5 hours for the window to reset]
This is when things got interesting. During the wait, I brought in ampcode for strategic direction while planning the next phase. We had a proper AI ensemble going: Claude for implementation, GPT-5 for review, ampcode for strategy, and me orchestrating.
The RL Experiment
class PPOTrainer:
    def __init__(self, policy_network, value_network):
        self.policy = policy_network
        self.value = value_network
        # Standard PPO setup
[PPO implementation: https://github.com/nibzard/berghain-challenge-bot/blob/main/berghain/training/ppo_trainer.py]
Results after 48 hours: 889 rejections.
Still worse than RBCR.
The Humbling Realization
Me: â€œWhy isnâ€™t this working?â€
Claude: â€œI think weâ€™re solving the wrong problem. RBCR already found the principled solution. Weâ€™re trying to learn what we already know.â€
GPT-5: â€œThe issue is sample efficiency. Youâ€™d need millions of games to learn what RBCR encodes in 50 lines of math.â€
Ampcode: â€œConsider this: your dual variables already capture the optimal policy. The LSTM is trying to approximate Î» = deficit / expected_help through pattern matching instead of computation.â€
That hit hard.
The Beautiful Lesson
Machine learning shines when you have:

Massive datasets
Unclear problem structure
Complex pattern recognition needs

Our problem had:

Limited data (162 elite games)
Clear mathematical structure (constrained optimization)
Simple pattern (urgency-based decisions)

RBCR was elegant because it directly encoded the problem structure. The LSTM was trying to rediscover that structure through brute force pattern matching.
The Performance Comparison



































MethodRejectionsTraining TimeInterpretabilityRBCR7810 minutesHighLSTM Supervised9342 hoursLowPPO88948 hoursLowEnsemble85672 hoursMedium
The math won. Decisively.
What We Learned

Domain expertise beats data when you understand the problem structure
More compute doesnâ€™t fix fundamental approach problems
Machine learning is a tool, not a silver bullet
Sometimes the â€œboringâ€ solution is optimal

The ML detour wasnâ€™t a complete waste. It forced us to deeply understand why RBCR worked so well. But it also taught us humility.
Me: â€œSo we spent all day proving that our mathematical algorithm was actually pretty good?â€
Claude: â€œYouâ€™re absolutely right! We spent all day proving that thinking hard about the problem structure matters more than throwing neural networks at it.â€
GPT-5: â€œYou also demonstrated that human-AI collaboration works best when each party contributes their strengthsâ€”humans for insight, AI for implementation.â€
Ampcode: â€œAnd that sometimes the most sophisticated approach is knowing when not to be sophisticated.â€
RBCR remained undefeated at 781 rejections.

# Part 8: What Really Happened Here
After one intense day of obsessive optimization, I needed to step back and understand what had actually occurred.
This wasnâ€™t just about solving a nightclub simulation. This was about witnessing two phenomena colliding: viral growth mechanics meeting AI-assisted engineering.
From Listenâ€™s Perspective: Growth That Got Out of Hand
What started as a simple puzzle became the largest distributed optimization contest in history.
Their infrastructure crashed repeatedly. But those crashes? They became part of the story. Social proof of viral success. Alfred tweeting â€œsorry fixing this.. too many usersâ€ was pure authenticity marketing.
They accidentally created the most engaging technical challenge of 2025. Zero paid acquisition. 1.1M organic impressions. A community of obsessives building sophisticated optimization engines.
Perfect fit tooâ€”Listen Labs does AI-powered customer insights, so attracting technical talent with algorithmic challenges makes total sense for their hiring pipeline.
The prize was Berghain guest list access. The real reward? The dopamine hit of shaving off single-digit rejections in a massive competitive field.
From Our Perspective: AI-Human Collaboration at Speed
This wasnâ€™t traditional programming. This was a new kind of problem-solving in action.
Claudeâ€™s Superpowers
Let me be clear about who did the heavy lifting here: Claude wrote probably 95% of the code. I provided direction, but Claude was the implementation engine.
Instant Translation: Iâ€™d say â€œwhat if we use Lagrangian multipliersâ€ and 30 seconds later thereâ€™s a fully functional dual variable solver.
Perfect Memory: Claude never forgot what we tried before. It could instantly reference our greedy approach from day one or the feasibility oracle parameters from day two.
Infinite Patience: When I asked Claude to implement the 23rd variant of Ultimate solver, there was no eye-rolling. Just â€œHereâ€™s the implementation:â€
Pattern Recognition: Claude spotted mathematical connections I missed. The link between RBCR and bid-price mechanisms in auction theory? That was Claude.
[Full solver collection: https://github.com/nibzard/berghain-challenge-bot/tree/main/berghain/solvers]
The Human Contribution
So what did I actually add to this collaboration?
Domain Intuition: â€œThis feels like a resource allocation problemâ€ or â€œWe should panic more in the late game.â€
Problem Reframing: When we hit walls, Iâ€™d step back and ask â€œWhat are we really trying to optimize here?â€
Quality Control: I caught Claudeâ€™s occasional mathematical errors and suggested corrections.
Strategic Direction: Deciding when to explore new approaches vs. when to refine existing ones.
Context Switching: When Claude hit token limits, Iâ€™d bring in GPT-5 for code review or ampcode for strategic guidance.
The Beautiful Dance
The collaboration felt like a dance. Iâ€™d have an insight. Claude would implement it instantly. Weâ€™d test it immediately. Results would spark new ideas.
Traditional programming: Idea â†’ Hours of coding â†’ Testing â†’ Maybe it works
AI-assisted programming: Idea â†’ Seconds of coding â†’ Testing â†’ Rapid iteration
Me: â€œWhat if we track the acceptance rate and adjust thresholds dynamically?â€
Claude: [30 seconds later] â€œHereâ€™s the adaptive threshold implementation with exponential smoothing.â€
This velocity was intoxicating. We could test hypotheses as fast as we could think of them.
The Token Economics
Interesting challenge: Claude would occasionally hit context limits mid-conversation. This is where having multiple AI agents became crucial.
Me: â€œClaude, youâ€™re getting verbose. Can GPT-5 take a look at the RBCR implementation and suggest improvements?â€
GPT-5: â€œThe dual variable computation could use PI control instead of simple proportional. Hereâ€™s whyâ€¦â€
Claude: [Fresh context] â€œImplementing PI control for dual variablesâ€¦â€
This felt like managing a team of specialists, each with their own strengths and limitations.
What I Learned About AI Capabilities
Strengths:

Implementation speed is superhuman
Pattern matching across large codebases
Mathematical computation and optimization
Infinite patience for iteration
Perfect recall of previous attempts

Limitations:

Needs human guidance for problem framing
Can over-engineer when left unsupervised
Struggles with â€œgood enoughâ€ vs. â€œperfectâ€
Limited intuition about real-world constraints
Context window limitations require management

The Compound Effect
Individually, neither human intuition nor AI implementation is sufficient for complex problems like this.
But together? The combination was greater than the sum of parts.
Human insight: â€œThis is really about managing scarcity under uncertainty.â€
AI implementation: Fully functional RBCR solver in minutes.
Human refinement: â€œThe threshold feels too static.â€
AI adaptation: Adaptive threshold with multiple parameters.
Human stopping condition: â€œ781 is probably optimal for this approach.â€
The Speed of Discovery
In traditional programming, this project would have taken weeks:

Day 1: Set up environment, implement basic greedy approach
Week 1: Statistical analysis and probabilistic solver
Week 2: Research dual variables and implement RBCR
Week 3: Parameter tuning and optimization
Week 4: ML experiments and failure analysis

With AI assistance, we compressed weeks into days. Not because the AI was smarter, but because the iteration cycle was faster.
The Meta-Learning
By the end, I wasnâ€™t just learning about the Berghain Challenge. I was learning how to collaborate with AI systems effectively.
Good prompts: â€œImplement RBCR with periodic dual variable resolutionâ€
Bad prompts: â€œMake it betterâ€
Good delegation: Let Claude implement, human provides direction
Bad delegation: Human micromanages implementation details
Good exploration: Try fundamentally different approaches
Bad exploration: Endless parameter tuning
The Philosophical Shift
This experience changed how I think about programming and problem-solving.
Old paradigm: Human thinks, human implements, human tests
New paradigm: Human thinks, AI implements, both test and iterate
The bottleneck shifted from implementation speed to idea quality. When you can test any hypothesis in seconds, the limiting factor becomes generating good hypotheses.
The Humility Lesson
The ML failure was educational. Despite having superhuman implementation speed, we couldnâ€™t beat a principled mathematical approach with brute force learning.
Domain expertise still matters. Understanding problem structure still matters. Sometimes the â€œboringâ€ solution is optimal.
AI amplifies human capabilities, but it doesnâ€™t replace human judgment about what problems are worth solving and how to approach them.
What This Means for Software Development
I think we just got a preview of the future of programming:
Humans: Problem formulation, strategic direction, quality control
AI: Implementation, optimization, pattern recognition
Together: Rapid prototyping and iteration at unprecedented speed
The result isnâ€™t human replacement, but human amplification. We can explore the solution space much faster and more thoroughly.
But we still need to know where to look.

# Part 9: Technical Deep Dive - Why RBCR Dominates
Letâ€™s get into the mathematical guts of why RBCR consistently outperformed 30+ other approaches.
The Economics Foundation
RBCR works because it directly models the economic structure of the problem. Each person has a value based on scarcity and urgency.
The dual variables Î»_young and Î»_dressed represent shadow pricesâ€”what economists call the marginal value of relaxing a constraint by one unit.
# The core insight: deficit / expected help rate
lambda_young = max(0, young_shortage) / (young_frequency * remaining_slots)
lambda_dressed = max(0, dressed_shortage) / (dressed_frequency * remaining_slots)

# Person value = sum of their contributions
value = lambda_young * person.young + lambda_dressed * person.well_dressed
[Full RBCR implementation: https://github.com/nibzard/berghain-challenge-bot/blob/main/berghain/solvers/rbcr_solver.py]
When young people become scarce, Î»_young increases, making young people more valuable. When we have plenty, Î»_young drops. The algorithm automatically balances supply and demand.
The Self-Correction Mechanism
Beautiful property: RBCR is self-correcting. If it accepts too many young people early, the young deficit shrinks, Î»_young drops, and it becomes less likely to accept more young people.
This creates a natural equilibrium without explicit balancing logic.
Why Other Approaches Failed
Greedy Solvers: No global optimization. Accept anyone who helps immediately, leading to imbalanced allocations.
Static Threshold Methods: Fixed acceptance criteria donâ€™t adapt to changing game state.
Ensemble Methods: Multiple competing signals create inconsistent decisions. The left hand doesnâ€™t know what the right hand is doing.
ML Approaches: Trying to learn patterns that are better expressed mathematically. Using a neural network to approximate Î» = deficit/rate is like using a sledgehammer to solve arithmetic.
The Resolution Frequency Sweet Spot
Why resolve every 50 arrivals instead of every decision?
# Too frequent: Computational waste, noise from variance
if resolve_every == 1: overhead_cost = high, signal_quality = noisy

# Too infrequent: Slow adaptation to changing conditions
if resolve_every == 500: adaptation_speed = slow, missed_opportunities = many

# Just right: Balance efficiency with responsiveness
if resolve_every == 50: overhead_cost = low, adaptation_speed = fast
50 arrivals gives enough data to estimate rates reliably while adapting quickly to changes.
The Adaptive Threshold Magic
Static thresholds donâ€™t work because the game has phases:
Early Phase (0-40% capacity): Be selective. Plenty of time to find good candidates.
Mid Phase (40-80% capacity): Balanced. Accept reasonable matches.
Late Phase (80%+ capacity): Panic mode. Accept anything that helps.
def adaptive_threshold(capacity_ratio, rejection_ratio):
    base = 1.5 - capacity_ratio  # Start high, end low

    # Emergency mode if running out of rejections
    if rejection_ratio > 0.8:
        base *= 0.5

    return max(0.1, base)
This creates the right urgency curve automatically.
The Feasibility Oracle Paradox
We tried adding Monte Carlo feasibility checking. Why did it hurt performance?
The oracle was too conservative. It would reject borderline candidates because there was a 10% chance of failure down the road. But RBCRâ€™s dual variables already encode future value properly.
Adding â€œwhat ifâ€ simulation on top of principled optimization was redundant and harmful.
The Statistical Foundation
RBCR implicitly assumes arrivals follow the known statistical distribution. This is a strong assumption, but itâ€™s correct for the Berghain Challenge.
The dual variables are computing expected values:

E[young people in remaining arrivals] = young_frequency Ã— remaining_slots
E[well_dressed people in remaining arrivals] = dressed_frequency Ã— remaining_slots

When reality matches assumptions, RBCR excels. In environments with changing distributions, it would need adaptation.
Performance Consistency
RBCRâ€™s biggest advantage isnâ€™t just the 781 averageâ€”itâ€™s the consistency.

































SolverBestWorstStd Dev95th PercentileRBCR76182318.4812Ultimate377989131.7847Statistical79896742.1889
RBCRâ€™s tight distribution means reliable performance. Other solvers have higher varianceâ€”sometimes better, often much worse.
The Learning Component
RBCR includes meta-learning across games. It saves dual variable estimates and uses them as starting points for future games.
# Load previous dual estimates
self.duals = load_from_disk('rbcr_duals.json')

# Start with learned values instead of zero
self.lambda_young = self.duals.get('lambda_young', 0.0)
self.lambda_dressed = self.duals.get('lambda_dressed', 0.0)
This warm-start helps early-game decisions when we donâ€™t have enough data yet.
Computational Efficiency
RBCR is also computationally cheap:

No Monte Carlo simulations
No neural network forward passes
Simple arithmetic: deficit Ã· expected rate
O(1) per decision after dual resolution

Fast enough to run in real-time, simple enough to debug and tune.
The Theoretical Optimum
Is 781 rejections optimal? Probably not. The theoretical minimum depends on the exact arrival sequence, which is random.
But RBCR is likely near the optimal policy for this class of problems. Itâ€™s implementing a principled approximation to the optimal stopping strategy from stochastic control theory.
Why This Matters Beyond Berghain
The principles behind RBCR apply to many resource allocation problems:

Ad auction bidding (Google, Facebook)
Inventory management (Amazon, Walmart)
Hospital bed allocation
Cloud resource scheduling
Financial portfolio rebalancing

Anywhere you have:

Limited capacity
Uncertain arrivals
Multiple competing objectives
Irreversible decisions

RBCR-style dual variable approaches often dominate.
The Elegant Simplicity
RBCRâ€™s beauty isnâ€™t in its complexityâ€”itâ€™s in its simplicity. 50 lines of math that capture the essence of a complex optimization problem.
No ensemble methods. No neural networks. No Monte Carlo simulations.
Just economics: when something is scarce, make it valuable. When itâ€™s abundant, make it cheap.
The algorithm does exactly what a perfect economist would do, with perfect information about supply and demand.

# Part 10: Lessons for the Future of Coding
This project changed how I think about programming. Here are the key lessons for anyone working with AI coding assistants.
The New Development Cycle
Traditional: Think â†’ Code â†’ Test â†’ Debug â†’ Iterate
AI-Assisted: Think â†’ Prompt â†’ Test â†’ Refine â†’ Iterate
The time from idea to working code dropped from hours to seconds. This changes everything.
Old bottleneck: Implementation time
New bottleneck: Idea quality and problem understanding
When you can test any hypothesis instantly, the quality of your hypotheses becomes the limiting factor.
What Humans Should Focus On
Problem Framing: â€œThis is really a resource allocation problem with uncertaintyâ€
Domain Expertise: â€œReal bouncers would panic more in late gameâ€
Strategic Direction: â€œLetâ€™s try mathematical optimization before MLâ€
Quality Control: â€œThis threshold feels too staticâ€
Leave the implementation to AI. Focus on the thinking.
What AI Excels At
Instant Implementation: Mathematical concepts to working code in seconds
Perfect Memory: Never forgets what you tried before
Pattern Recognition: Spots connections you might miss
Infinite Patience: Will implement variant #23 without complaint
Rapid Iteration: Test-debug-refine cycles at superhuman speed
The Multi-Agent Orchestra
Donâ€™t limit yourself to one AI. Different models have different strengths:
Claude: Best at complex implementation and mathematical reasoning
GPT-5: Excellent for code review and getting unstuck
Specialized agents: Good for specific strategic decisions
Managing this ensemble becomes part of the skill.
Common Pitfalls
Over-Engineering: AI makes it too easy to add complexity. Resist.
The Perfectionism Trap: Every small improvement feels possible. Know when to stop.
Context Management: AI systems have token limits. Learn to work within them.
Prompt Quality: Vague instructions lead to mediocre results. Be specific.
Testing Neglect: Fast implementation can lead to inadequate testing. Donâ€™t skip verification.
The Collaboration Sweet Spot
Good division of labor:

Human: â€œLetâ€™s use dual variables to model urgencyâ€
AI: [Implements RBCR with proper mathematical formulation]
Human: â€œThe threshold should adapt based on game phaseâ€
AI: [Adds adaptive threshold with exponential decay]

Bad division of labor:

Human: â€œMake the algorithm betterâ€
AI: [Adds random complexity that doesnâ€™t help]

Be specific about what you want. AI is powerful but not psychic.
The Speed vs. Wisdom Tradeoff
AI enables incredibly fast iteration. This is powerful but dangerous.
You can now test 50 approaches in a day. But are they 50 good approaches?
Solution: Alternate between exploration and reflection. Sprint, then pause to understand what you learned.
Documentation Becomes Critical
With traditional coding, you remember what you built because you spent hours building it.
With AI coding, you can implement complex systems in minutes. But you might not fully understand them.
[Full project documentation: https://github.com/nibzard/berghain-challenge-bot]
Document your insights, not just your code. Future you will thank present you.
The Meta-Learning Effect
By the end of this project, I wasnâ€™t just better at optimization problems. I was better at collaborating with AI systems.
Good prompts: Specific, contextual, action-oriented
Bad prompts: Vague, assuming too much context
Good feedback: â€œThe threshold needs to be lower in late gameâ€
Bad feedback: â€œThis doesnâ€™t feel rightâ€
Learning to work with AI is a skill that improves with practice.
Implications for Software Teams
Individual Productivity: 10x improvement for complex algorithm development
Team Dynamics: Junior developers can implement senior-level solutions
Code Review: Becomes more important because humans didnâ€™t write every line
Architecture: System design becomes more critical than implementation details
The Domain Expertise Advantage
The ML failure taught us something important: understanding your problem domain matters more than ever.
When anyone can implement any algorithm in seconds, the competitive advantage shifts to:

Understanding what problems are worth solving
Knowing which approaches are likely to work
Recognizing when you have enough vs. need more

Domain expertise becomes a superpower when combined with AI implementation speed.
What This Means for Learning
Donâ€™t just learn syntax: Focus on algorithms, mathematics, system design
Learn problem patterns: Optimization, resource allocation, statistical inference
Understand tradeoffs: When to be complex vs. simple, fast vs. accurate
Study failures: Why approaches donâ€™t work is as important as why they do
The fundamentals matter more, not less, in an AI-assisted world.
The Future Landscape
I think weâ€™re heading toward a world where:
Coding becomes more like architecture: Designing systems rather than implementing details
AI handles the mechanical work: Converting specifications to working code
Humans focus on the creative work: Problem definition and solution strategy
Collaboration is the key skill: Managing human-AI teams effectively
This isnâ€™t about AI replacing programmers. Itâ€™s about amplifying what good programmers already do: solve problems thoughtfully.
The Democratization Effect
AI coding assistants lower the barrier to implementing complex algorithms. A developer who understands dual variables conceptually can now implement RBCR without years of optimization theory study.
This is powerful for innovation. More people can experiment with sophisticated approaches.
But it also means that understanding problem structure becomes even more important. Anyone can implement; not everyone can architect.
Final Advice
Start simple: Even with AI, begin with basic approaches and build complexity gradually.
Stay curious: Use AIâ€™s speed to explore more solution spaces, not just to implement faster.
Maintain understanding: Donâ€™t let AI implementation outpace your conceptual grasp.
Embrace failure: Fast iteration makes failure cheaper. Fail quickly and learn faster.
Focus on problems, not code: The hardest part isnâ€™t implementation anymoreâ€”itâ€™s knowing what to build.
The future of programming isnâ€™t human vs. AI. Itâ€™s human with AI, exploring solution spaces that neither could navigate alone.

# Part 11: Whatâ€™s Next & How to Win
So you want to tackle your own impossible optimization problem with AI? Hereâ€™s what I learned.
Start Simple, Then Get Mathematical
Donâ€™t jump straight to neural networks. Start with the dumbest possible approach. Get it working. Then ask: â€œWhat would the optimal solution look like mathematically?â€
For constrained optimization, that usually means Lagrangian multipliers. For scheduling, itâ€™s often dynamic programming. For graph problems, think shortest paths or maximum flows.
The pattern is always the same: naive approach â†’ mathematical insight â†’ implementation refinement.
Build Your Local Simulator
This was huge. The Berghain API had rate limits, downtime, and a 10-game parallel limit. Our local simulator removed all those constraints.
# Key insight: Perfect simulation beats imperfect reality
class BerghainSimulator:
    def __init__(self, scenario_config):
        self.constraints = scenario_config['constraints']
        self.attribute_frequencies = scenario_config['frequencies']
We generated thousands of games locally. Tested dozens of strategies. Found the edge cases. All without API limits.
Choose Your AI Partners Wisely
Claude was perfect for implementation. It understood the domain, wrote clean code, and never got impatient with iterations.
GPT-5 was better for code review and strategic thinking when we got stuck.
Ampcode helped with architectural decisions when Claude hit token limits.
Different models have different strengths. Use them strategically.
Embrace the Obsession
From 1,200 rejections to 781. Thatâ€™s not optimization. Thatâ€™s obsession.
But obsession drives discovery. Every 10-rejection improvement taught us something new about the problem space. The difference between â€œgood enoughâ€ and â€œoptimalâ€ is where the insights live.
Document Everything
Keep logs of what works and what doesnâ€™t. We had 162 elite games showing exactly which strategies succeeded. That data drove every major breakthrough.
Know When to Stop
ML felt like the â€œsophisticatedâ€ approach. But domain knowledge and mathematical intuition beat black-box learning every time.
The LSTM experiments taught us that sometimes the simple mathematical solution is actually the optimal one.
The Real Win: Speed of Iteration
Three days from problem discovery to 781-rejection solution. Thatâ€™s not normal software development. Thatâ€™s what happens when human intuition meets AI implementation speed.
The traditional cycle: Think â†’ Code â†’ Debug â†’ Test â†’ Deploy
The AI cycle: Think â†’ Prompt â†’ Test â†’ Refine
We compressed months of development into days.
For Your Next Project
Pick something with clear success metrics. Optimization problems work great because you get immediate feedback.
Build incrementally. Each improvement teaches you about the problem space.
Use multiple AI models for their strengths. But remember: youâ€™re the conductor. You decide the direction.
And when you find yourself checking results at 2 AM because youâ€™re convinced you can get just 5 more rejections? Youâ€™ll know youâ€™ve found the sweet spot of human-AI collaboration.
The future of coding isnâ€™t about replacing developers. Itâ€™s about amplifying obsession with implementation speed.

# Part 12: The Growth Marketing Playbook
As a growth advisor who watched this unfold, I have to break down Listenâ€™s accidental masterpiece. This wasnâ€™t just viral marketing. This was systematic exploitation of technical community psychology.

Stage 1: Mystery (Billboard)

Cryptic puzzle creates curiosity gap
No explanation = maximum speculation
Technical enough to filter for target audience
Physical billboard adds authenticity (not just another digital campaign)

Stage 2: Community (Token Puzzle)

Solvable but non-trivial puzzle
Requires technical knowledge (OpenAI tokenizer)
Activates Reddit, Twitter, Discord communities
Community solving = network effects at scale

Stage 3: Challenge (Berghain Game)

Clear success metrics (rejection count)
Immediate feedback loop
Competitive leaderboard dynamics
Deep complexity beneath simple rules

Stage 4: Status (Optimization Competition)

Technical skill as status symbol
30,000 participants = massive validation
Github repos, blog posts, Twitter threads
Organic content creation at scale

The Viral Coefficients
Letâ€™s break down the math:
Initial reach: Billboard + Reddit discovery â‰ˆ 1,000 people
Community amplification: 1,000 Ã— 30 (average shares/discussion participants) = 30,000
Retention rate: ~60% (technical challenges have high dropout but strong retention among engaged users)
Content multiplier: Each obsessive creates 3-5 pieces of content (Github repos, tweets, blog posts)
Total organic impressions: 1.1M
Cost per impression: ~$0.001 (just billboard cost)
Cost per engaged user: ~$1 (30,000 active participants)
Those are unicorn-level growth metrics.

Ego Investment: Complex problems = status signaling opportunity
Immediate Feedback: Algorithm performance = dopamine hits
Competitive Context: 30,000 participants = social proof
Deep Complexity: Simple rules with emergent mathematical beauty
Tool Building: Engineers love building sophisticated solutions
The Infrastructure Strategy (Accidental Genius)
Listenâ€™s API crashes werenâ€™t bugsâ€”they were features:
Scarcity Psychology: â€œCanâ€™t access it? Want it moreâ€
Authenticity Signals: Real startups have real scaling problems
Community Building: Users helping each other, sharing solutions
Distributed Load: Community built local simulators (like we did)
Alfredâ€™s real-time tweets about crashes created narrative tension. â€œFixing servers, too many usersâ€ is the best social proof possible.
Lessons for Startups
Pick Your Audience Carefully: Technical communities are high-value, low-volume. Perfect for complex challenges.
Underestimate Scale Publicly: Alfredâ€™s â€œexpected 10 users, got 30,000â€ became part of the story. Authentic surprise > polished launch.
Infrastructure as Marketing: Your technical problems can become engagement opportunities if handled transparently.
Community > Users: Donâ€™t optimize for user count. Optimize for obsession. Engaged obsessives > casual users at scale.
Prize-Market Fit: Berghain guest list for Berlin tech crowd = perfect audience targeting.
The Economics
Traditional SaaS Customer Acquisition:
$100-$500 CAC for technical audiences
3-6 month sales cycles
Low viral coefficients (1.1-1.3x)
Listenâ€™s Approach:
$1 cost per engaged user
Instant community building
3000x viral coefficient
Self-selecting for technical sophistication
ROI: Immeasurable. They got thousands of technical users to voluntarily stress-test their product concept, generate content, and build community around their brand.
The Replication Framework
Want to try this? Hereâ€™s the playbook:

Find a hard technical problem your target audience cares about
Wrap it in mystery (puzzles work, but so do challenges)
Make it solvable but non-trivial (high dropout, high engagement)
Add competitive elements (leaderboards, status, bragging rights)
Let infrastructure struggle visibly (authenticity > perfection)
Document the journey (real-time tweets, community updates)

What Listen Accidentally Discovered
They found the perfect intersection of:

Growth marketing (viral mechanics)
Product development (crowd-sourced optimization)
Community building (shared obsession)
Content creation (user-generated solutions)

They turned customer acquisition into a technical arms race. Users didnâ€™t just sign upâ€”they built sophisticated solutions and shared them publicly.
The Meta-Lesson
The best growth hacks donâ€™t feel like marketing. They feel like genuine problems worth solving.
Listen created a challenge so engaging that users built entire optimization frameworks just to participate. We werenâ€™t customers. We were collaborators.
Thatâ€™s not just viral growth. Thatâ€™s community-driven product development at scale.

# Conclusion
The Berghain Challenge started with a billboard and ended with two discoveries that changed everything.
Discovery 1: Listen accidentally created the most brilliant growth hack of 2025. A 3000x viral coefficient. 1.1M organic impressions. Zero paid acquisition. They turned customer acquisition into a technical arms race where users built sophisticated solutions and shared them publicly.
Discovery 2: Our 781-rejection algorithm that dominated a massive competitive field. But the real breakthrough wasnâ€™t RBCR or dual variables or Monte Carlo simulations. It was the collaboration pattern that emerges when human intuition meets AI implementation speed.
The Growth + Engineering Synthesis
As a growth advisor with engineering fundamentals, I saw both stories unfold:
Listen created viral mechanics by exploiting technical community psychology. Mystery â†’ Community â†’ Challenge â†’ Status. Each phase filtered for higher engagement, building a community of obsessives.
We solved the challenge through AI-human collaboration. I provided mathematical insights. Claude implemented them flawlessly. GPT-5 caught edge cases. Together, we explored solution spaces that none of us could navigate alone.
Two Futures Colliding
This is what happens when growth marketing meets AI-assisted engineering:
Growth creates challenges. Viral mechanisms need engaging problems.
Engineering solves challenges. AI collaboration makes solution iteration lightning-fast.
Solutions create content. Sophisticated approaches become community assets.
Content drives growth. Technical solutions as social proof.
Itâ€™s a positive feedback loop. Marketing creates problems worth solving. AI helps solve them faster than ever. Solutions become marketing content.
The Meta-Lesson
The future isnâ€™t just about AI replacing programmers or startups doing viral marketing.
Itâ€™s about communities formed around hard problems, solved collaboratively by humans and AI, creating value for everyone: the startup gets users, the engineers get dopamine hits, and the community gets shared knowledge.
Listen wanted attention. They got a technical revolution.
We wanted to solve a puzzle. We discovered new ways to collaborate with AI.
The community wanted to compete. They created a distributed optimization laboratory.
Everyone won.
Whatâ€™s Next
The Berghain Challenge was just the beginning. The real challenge is learning how to danceâ€”with AI, with communities, with hard problems that matter.
Want to see the technical solution? Check out the complete implementation at github.com/nibzard/berghain-challenge-bot.
Want to see the growth mechanics? Start with cryptic puzzles that technical communities canâ€™t resist.
Time to find your own intersection of viral growth and AI-powered problem-solving.
# The Reality Check
But hereâ€™s the sobering truth: despite all the mathematical sophistication, all the optimization theory, all the vibes I poured into RBCR, the current best score on the leaderboard is 716 rejections. Thatâ€™s still a massive gap from our 781.
This leaves me with a humbling realization. Vibes and intuition can get you surprisingly farâ€”further than I expected when we started this journey. The human-AI collaboration, the mathematical frameworks, the elegant dual variablesâ€”they all contributed to a genuinely competitive solution.
But at some point, youâ€™re on your own. The gap between 781 and 716 represents the difference between â€œimpressive engineeringâ€ and â€œworld-class optimization.â€ Itâ€™s the reminder that in competitive arenas with thousands of brilliant minds, good enough rarely wins.
Still, I regret nothing. The journey taught us about optimization, about collaboration, about the limits and possibilities of human-AI partnership. And maybe, just maybe, someone reading this will find that final insight we missed and claim that Berlin trip.
PS: And the kicker? Claude wrote this entire article too. I just provided the direction and feedback. The AI that helped me solve the Berghain Challenge also helped me tell you about it.
Meta-collaboration all the way down.  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[996]]></title>
            <link>https://lucumr.pocoo.org/2025/9/4/996/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149049</guid>
            <description><![CDATA[There is cost to your lifestyle.]]></description>
            <content:encoded><![CDATA[
        
  

  
  written on September 04, 2025
  

  
â€œAmazing salary, hackerhouse in SF, crazy equity.
996. Our mission is
OSS.â€ â€” Gregor Zunic
â€œThe current vibe is no drinking, no drugs, 9-9-6, [â€¦].â€ â€” Daksh
Gupta
â€œThe truth is, Chinaâ€™s really doing â€˜007â€™ nowâ€”midnight to midnight, seven
days a week [â€¦] if you want to build a $10 billion company, you have to work
seven days a week.â€ â€” Harry Stebbings

I love work.  I love working late nights, hacking on things.  This week I
didnâ€™t go to sleep before midnight once.  And yetâ€¦
I also love my wife and kids. I love long walks, contemplating life over good
coffee, and deep, meaningful conversations.  None of this would be possible if
my life was defined by 12 hour days, six days a week.  More importantly, a
successful company is not a sprint, itâ€™s a marathon.
And this is when this is your own company!  When you devote 72 hours a week to
someone elseâ€™s startup, you need to really think about that arrangement a few
times.  I find it highly irresponsible for a founder to promote that model.  As
a founder, you are not an employee, and your risks and leverage are
fundamentally different.
I will always advocate for putting the time
in because it is what brought me happiness.
Intensity, and giving a shit about what Iâ€™m doing, will always matter to me.
But you donâ€™t measure that by the energy you put in, or the hours youâ€™re
sitting in the office, but the output you produce.  Burning out on twelve-hour
days, six days a week, has no prize at the end.  Itâ€™s unsustainable, it
shouldnâ€™t be the standard and it sure as hell should not be seen as a positive
sign of a company.
Iâ€™ve pulled many all-nighters, and Iâ€™ve enjoyed them.  I still do.  But theyâ€™re
enjoyable in the right context, for the right reasons, and when that is a
completely personal choice, not the basis of company culture.
And that all-nighter?  It comes with a fucked up and unproductive morning the
day after.
When someone promotes a 996 work culture, we should push back.


  
  This entry was tagged
    
      thoughts
  

  
    copy as / view markdown
  
  
  

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We Hacked Burger King: How Auth Bypass Led to Drive-Thru Audio Surveillance]]></title>
            <link>https://bobdahacker.com/blog/rbi-hacked-drive-thrus/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148944</guid>
            <description><![CDATA[Critical authentication bypass vulnerabilities in Restaurant Brands International's assistant platform allowed complete control over 30,000+ Burger King, Tim Hortons, and Popeyes locations worldwide - including access to customer drive-thru audio recordings.]]></description>
            <content:encoded><![CDATA[
            
                The Setup
Picture this: Restaurant Brands International (RBI) â€“ the corporate overlords behind Burger King, Tim Hortons, and Popeyes â€“ control over 30,000 locations worldwide. That's a lot of chicken sandwiches, maple syrup, and flame-broiled beef.
What they also control is something called the "assistant" platform â€“ the digital brain behind every drive-thru screen, bathroom tablet review, and the slightly-too-cheerful burger king employee asking if you want to make it a combo.
Spoiler alert: Their security was about as solid as a paper Whopper wrapper in the rain.
We stumbled upon vulnerabilities so catastrophic that we could access every single store in their global empire. From a Burger King in Times Square to that lonely Tim Hortons where Bugs Bunny shoulda taken a left turn at Albuquerque. Oh, and did we mention we could listen to your actual drive-thru conversations? Yeah, that happened too.
The platforms were spread across three domains, each with the same delicious vulnerabilities:

https://assistant.bk.com
https://assistant.popeyes.com  
https://assistant.timhortons.com

Buckle up, this is going to be a wild ride. ğŸ”
The Vulnerabilities
The "Anyone Can Join This Party" Signup API
Our journey began innocently enough. We tried logging in with fake credentials and discovered they were using AWS Cognito. The good news? The system worked exactly as designed. The bad news? They forgot to disable user signups. Oops.
After a quick email verification dance with AWS's ConfirmSignup method, we were in. But wait, there's more!
Using GraphQL introspection (because who doesn't love a good schema leak), we found an even easier signup endpoint that completely bypassed email verification. It was like finding a secret menu item, except this one came with user privileges.
mutation SignUp {
  signUp(input: { email: "[emailÂ protected]", password: "password123" })
}

The cherry on top? They emailed us the password in plain text. In 2025. We're not even mad, just impressed by the commitment to terrible security practices.
When your password security is more outdated than the ice cream machine

"Gotta Catch 'Em All" - The Global Store Directory
Once authenticated, we hit the jackpot: a single GraphQL endpoint that returned a store in their global empire. (the store ids were incrementing) Not just the store name, but the good stuff â€“ store employees personal information, internal IDs, configuration details.

But wait, there's more! We also found a GraphQL query to search users who have access to assistant by name, revealing personal info faster than you can say "would you like fries with that data breach?"

The "No Authentication, No Problem" Token Generator
Here's where things got spicy. We discovered a GraphQL mutation called createToken that was about as secure as leaving your house key under a welcome mat labeled "HOUSE KEY HERE."
This beautiful piece of mutation accepted a storeId parameter and required absolutely zero authentication. None. Nada. Zilch.
mutation CreateToken {
  createToken(input: { storeId: "1" })
}

Authentication is optional, apparently
This token wasn't just a key to one store â€“ it was the master key to the entire kingdom. With it, we could promote ourselves to admin status across the entire platform:
mutation BecomeTheKing {
  updateUser(input: { id: "definitely-us", roles: "admin" }) {
    roles  # Spoiler: it's "admin" now
  }
}

Look at us. We're the captain now.
The Drive-Thru Equipment Store (A Detour Into Absurdity)
While exploring, we stumbled upon RBI's equipment ordering website. The password protection? Client-side only. The password? Hardcoded in da HTML. 
Security through obscurity, but the obscurity is in plain sight
Here you could order drive-thru essentials like "Single Lane Kits" (tablets included!) or just grab a tablet for those bathroom feedback screens because apparently everything needs to be digital now.
Your complete drive-thru starter pack, no security included

Welcome to the Drive-Thru Control Room
The tokens unlocked access to the actual tablet interfaces used in stores. The tablet is primarly composed of what seems to be a web app.
Main Screen (https://assistant.bk.com/screens/main?authToken=yourNewBestFriend)

Lists previous conversations (spoiler: you can listen to them)
Shows the "tone" employees should focus on


Diagnostic Screen (https://assistant.bk.com/screens/diagnostic?authToken=stillYourBestFriend)This one had an additional password protection. The password? "admin". Client-side, of course.
Advanced password security, circa 1995
When your password security is more predictable than McDonald's ice cream machines being broken
Once inside, we found APIs to control drive-thru audio levels. Want to blast customers' eardrums or make them whisper their order? We got you covered:

Any store token could also list any store's drive-thru config, because apparently access control is just a suggestion.

Notice something? Well it returns a JWT that can be used to call an API to return a signed AWS upload url, for any store's path, which is bad. Very bad. 
First it was kind enough to reveal the missing values:

Then it returned with the signed upload URL for "amongus.mp4":


The Drive-Thru Surveillance State
Now for the truly mind-blowing part: We could access actual voice recordings of customer orders. Which god knows how long they store those for.
Not just transcripts. Not just metadata. Raw audio files of real people ordering food, complete with background conversations, car radios, and sometimes personally identifiable information.

Big Brother is watching, and he wants to know if you'd like to supersize that
This audio goldmine was being fed into AI systems to analyze:

Customer sentiment 
Employee friendliness levels
Upsell success rates
Order processing times
How many times employees said "You rule" (because that's definitely a crucial business metric)

AI-powered friendliness monitoring: dystopian but thorough
Your drive-thru performance review, brought to you by artificial intelligence
The Bathroom Chronicles
We found the code for the bathroom rating screen for the tablet was in the assistant panel as well, For example, the bathroom rating screen is located at:
https://assistant.bk.com/feedback/storeid/
Which we assume the tablet loads in a webapp with its storeId
Rating bathroom experiences: because everything needs a digital feedback loop
The cherry on this porcelain throne? The API that gets called for the feedback sends zero authentication. That's right â€“ you could spam bathroom reviews for every Burger King location without even proving you've ever been within a mile of their restrooms.
Want to give a 5-star review to a bathroom in Tokyo while sitting in your pajamas in Ohio? The system says "why not!" ğŸš½

The Full Damage Report'
With our newfound admin powers, we could:

Add/remove/manage stores (Want to open a Burger King on the Moon? Now you can!)  
View/edit employee accounts (Everyone gets a promotion!)  
Send notifications to any store ids tablet
Access store analytics and sales data (Numbers, so many numbers)  
Upload files to any store's systems (Via convenient JWT-signed AWS URLs)  
AND More


Privacy Violations of Epic Proportions

Access to thousands, possibly hundreds of thousands of voice recordings containing PII, if you visited a burger king and went thru the drive thru, your voice is probably in the aws bucket, and analyzed by AI.
GDPR lawyers worldwide suddenly perking up
We now know about your embarrassing order of 47 chicken nuggets at 2 AM


Timeline: The Speed Run



When
What Happened



Day 1
"Hey, let's see how this drive-thru system works"


Day 1, 2 hours later
"Oh no... OH NO... OH NO"


Day 1, 3 hours later
"We can hear people ordering food. This is not good."


Day 1, same day
RBI fixes everything faster than you can say "code red"


Credit where it's due â€“ RBI's response time was impressive. However they never commented on the vulnerabilities or answered us. (I guess our report was too hot to handle.)

The Fine Print
No customer data was retained during this research. No drive-thru orders were harmed in the making of this blog post. Responsible disclosure protocols were followed throughout. We still think the Whopper is pretty good, but Wendys is better
So Long, and Thanks for All the Fish

  
  
    ğŸ¤
    
  
  
  Official Hacker Collab
  
  
    This blog is a proud and official @BobDaHacker & @BobTheShoplifter collab.
  
  
  Check out the other Bob here:
  
  
    
  
    
    
  



            
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5]]></title>
            <link>https://github.com/b4rtaz/distributed-llama/discussions/255</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148237</guid>
            <description><![CDATA[qwen3_30b.mov Setup [ğŸ”€ TP-Link LS1008G Switch] | | | | | | | |_______ ğŸ”¸ raspberrypi2 (ROOT) 10.0.0.2 | | |_________ ğŸ”¹ raspberrypi1 (WORKER 1) 10.0.0.1 | |___________ ğŸ”¹ raspberrypi3 (WORKER 2) 10.0....]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    




  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  




    

      






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    167

  

  
        
            
          Star
          2.4k

  



        

        


          

  
    


  

  




    

        
  


            
    
      
    
  
        
  
    
    qwen3_30b.mov
    
  

  

  


Setup
[ğŸ”€ TP-Link LS1008G Switch]
      | | | |
      | | | |_______ ğŸ”¸ raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ ğŸ”¹ raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ ğŸ”¹ raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ ğŸ”¹ raspberrypi4 (WORKER 3) 10.0.0.4

Device: 4 x Raspberry Pi 5 8GB
Distributed Llama version: 0.16.0
Model: qwen3_30b_a3b_q40
Benchmark




Evaluation
Prediction




4 x Raspberry Pi 5 8GB
14.33 tok/s
13.04 tok/s



b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt "<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant
" --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
ğŸ“„ AddBos: 0
ğŸ“„ BosId: 151643 (<|endoftext|>)
ğŸ“„ EosId: 151645 (<|im_end|>) 
ğŸ“„ RegularVocabSize: 151643
ğŸ“„ SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
ğŸ’¡ Arch: Qwen3 MoE
ğŸ’¡ HiddenAct: Silu
ğŸ’¡ Dim: 2048
ğŸ’¡ HeadDim: 128
ğŸ’¡ QDim: 4096
ğŸ’¡ KvDim: 512
ğŸ’¡ HiddenDim: 6144
ğŸ’¡ VocabSize: 151936
ğŸ’¡ nLayers: 48
ğŸ’¡ nHeads: 32
ğŸ’¡ nKvHeads: 4
ğŸ’¡ OrigSeqLen: 262144
ğŸ’¡ nExperts: 128
ğŸ’¡ nActiveExperts: 8
ğŸ’¡ MoeHiddenDim: 768
ğŸ’¡ SeqLen: 4096
ğŸ’¡ NormEpsilon: 0.000001
ğŸ’¡ RopeType: Falcon
ğŸ’¡ RopeTheta: 10000000
ğŸ“€ RequiredMemory: 5513 MB
â­• Socket[0]: connecting to 10.0.0.1:9999 worker
â­• Socket[0]: connected
â­• Socket[1]: connecting to 10.0.0.3:9999 worker
â­• Socket[1]: connected
â­• Socket[2]: connecting to 10.0.0.4:9999 worker
â­• Socket[2]: connected
â­• Network is initialized
ğŸ§  CPU: neon dotprod fp16
ğŸ’¿ Loading weights...
ğŸ’¿ Weights loaded
ğŸš Network is in non-blocking mode
<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant

ğŸ”·ï¸ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
ğŸ”¶ Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
ğŸ”¶ Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
ğŸ”¶ Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
ğŸ”¶ Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
ğŸ”¶ Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
ğŸ”¶ Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
ğŸ”¶ Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
ğŸ”¶ Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
ğŸ”¶ Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
ğŸ”¶ Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
ğŸ”¶ Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
ğŸ”¶ Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  â€“
ğŸ”¶ Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
ğŸ”¶ Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
ğŸ”¶ Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
ğŸ”¶ Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

ğŸ”¶ Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
â­• Network is closed

    
    


          

        

         







  

  

  

  

  

  

  

  


    




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Software Development Methodology for Disciplined LLM Collaboration]]></title>
            <link>https://github.com/Varietyz/Disciplined-AI-Software-Development</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148180</guid>
            <description><![CDATA[This methodology provides a structured approach for collaborating with AI systems on software development projects. It addresses common issues like code bloat, architectural drift, and context dilu...]]></description>
            <content:encoded><![CDATA[

Disciplined AI Software Development - Collaborative
A structured approach for working with AI on development projects. This methodology addresses common issues like code bloat, architectural drift, and context dilution through systematic constraints.
The Context Problem
AI systems work on Question â†’ Answer patterns. When you ask for broad, multi-faceted implementations, you typically get:

Functions that work but lack structure
Repeated code across components
Architectural inconsistency over sessions
Context dilution causing output drift
More debugging time than planning time

How This Works
The methodology uses four stages with systematic constraints and validation checkpoints. Each stage builds on empirical data rather than assumptions.
Planning saves debugging time. Planning thoroughly upfront typically prevents days of fixing architectural issues later.
The Four Stages
Stage 1: AI Configuration
Set up your AI model's custom instructions using AI-PREFERENCES.md. This establishes behavioral constraints and uncertainty flagging with âš ï¸ indicators when the AI lacks certainty.
Stage 2: Collaborative Planning
Share METHODOLOGY.md with the AI to structure your project plan. Work together to:

Define scope and completion criteria
Identify components and dependencies
Structure phases based on logical progression
Generate systematic tasks with measurable checkpoints

Output: A development plan following dependency chains with modular boundaries.
Stage 3: Systematic Implementation
Work phase by phase, section by section. Each request follows: "Can you implement [specific component]?" with focused objectives.
File size stays â‰¤150 lines. This constraint provides:

Smaller context windows for processing
Focused implementation over multi-function attempts
Easier sharing and debugging

Implementation flow:
Request specific component â†’ AI processes â†’ Validate â†’ Benchmark â†’ Continue

Stage 4: Data-Driven Iteration
The benchmarking suite (built first) provides performance data throughout development. Feed this data back to the AI for optimization decisions based on measurements rather than guesswork.
Why This Approach Works
Decision Processing: AI handles "Can you do A?" more reliably than "Can you do A, B, C, D, E, F, G, H?"
Context Management: Small files and bounded problems prevent the AI from juggling multiple concerns simultaneously.
Empirical Validation: Performance data replaces subjective assessment. Decisions come from measurable outcomes.
Systematic Constraints: Architectural checkpoints, file size limits, and dependency gates force consistent behavior.
Example Projects


Discord Bot Template - Production-ready bot foundation with plugin architecture, security, API management, and comprehensive testing. 46 files, all under 150 lines, with benchmarking suite and automated compliance checking. (View Project Structure)


PhiCode Runtime - Programming language runtime engine with transpilation, caching, security validation, and Rust acceleration. Complex system maintaining architectural discipline across 70+ modules. (View Project Structure)


PhiPipe - CI/CD regression detection system with statistical analysis, GitHub integration, and concurrent processing. Go-based service handling performance baselines and automated regression alerts. (View Project Structure)


You can compare the methodology principles to the codebase structure to see how the approach translates to working code.
Implementation Steps
Setup

Configure AI with AI-PREFERENCES.md as custom instructions
Share METHODOLOGY.md for planning session
Collaborate on project structure and phases
Generate systematic development plan

Execution

Build Phase 0 benchmarking infrastructure first
Work through phases sequentially
Implement one component per interaction
Run benchmarks and share results with AI
Validate architectural compliance continuously

Quality Assurance

Performance regression detection
Architectural principle validation
Code duplication auditing
File size compliance checking
Dependency boundary verification

Project State Extraction
Use the included project extraction tool systematically to generate structured snapshots of your codebase:
python scripts/project_extract.py
Configuration Options:

SEPARATE_FILES = False: Single THE_PROJECT.md file (recommended for small codebases)
SEPARATE_FILES = True: Multiple files per directory (recommended for large codebases and focused folder work)
INCLUDE_PATHS: Directories and files to analyze
EXCLUDE_PATTERNS: Skip cache directories, build artifacts, and generated files

Output:

Complete file contents with syntax highlighting
File line counts with architectural warnings (âš ï¸ for 140-150 lines, â€¼ï¸ for >150 lines on code files)
Tree structure visualization
Ready-to-share

output examples can be found here
Use the tool to share a complete or partial project state with the AI system, track architectural compliance, and create focused development context.
What to Expect
AI Behavior: The methodology reduces architectural drift and context degradation compared to unstructured approaches. AI still needs occasional reminders about principles - this is normal.
Development Flow: Systematic planning tends to reduce debugging cycles. Focused implementation helps minimize feature bloat. Performance data supports optimization decisions.
Code Quality: Architectural consistency across components, measurable performance characteristics, maintainable structure as projects scale.

Frequently Asked Questions
Origin & Development

What problem led you to create this methodology?

I kept having to restate my preferences and architectural requirements to AI systems. It didn't matter which language or project I was working on - the AI would consistently produce either bloated monolithic code or underdeveloped implementations with issues throughout.
This led me to examine the meta-principles driving code quality and software architecture. I questioned whether pattern matching in AI models might be more effective when focused on underlying software principles rather than surface-level syntax. Since pattern matching is logic-driven and machines fundamentally operate on simple question-answer pairs, I realized that functions with multiple simultaneous questions were overwhelming the system.
The breakthrough came from understanding that everything ultimately transpiles to binary - a series of "can you do this? â†’ yes/no" decisions. This insight shaped my approach: instead of issuing commands, ask focused questions in proper context. Rather than mentally managing complex setups alone, collaborate with AI to devise systematic plans.



How did you discover these specific constraints work?

Through extensive trial and error. AI systems will always tend to drift even under constraints, but they're significantly more accurate with structured boundaries than without them. You occasionally need to remind the AI of its role to prevent deviation - like managing a well-intentioned toddler that knows the rules but sometimes pushes boundaries trying to satisfy you.
These tools are far from perfect, but they're effective instruments for software development when properly constrained.



What failures or frustrations shaped this approach?

Maintenance hell was the primary driver. I grew tired of responses filled with excessive praise: "You have found the solution!", "You have redefined the laws of physics with your paradigm-shifting script!" This verbose fluff wastes time, tokens, and patience without contributing to productive development.
Instead of venting frustration on social media about AI being "just a dumb tool," I decided to find methods that actually work. My approach may not help everyone, but I hope it benefits those who share similar AI development frustrations.


Personal Practice

How consistently do you follow your own methodology?

Since creating the documentation, I haven't deviated. Whenever I see the model producing more lines than my methodology restricts, I immediately interrupt generation with a flag: "â€¼ï¸ ARCHITECTURAL VIOLATION, ADHERE TO PRINCIPLES â€¼ï¸" I then provide the method instructions again, depending on how context is stored and which model I'm using.



What happens when you deviate from it?

I become genuinely uncomfortable. Once I see things starting to degrade or become tangled, I compulsively need to organize and optimize. Deviation simply isn't an option anymore.



Which principles do you find hardest to maintain?

Not cursing at the AI when it drifts during complex algorithms! But seriously, it's a machine - it's not perfect, and neither are we.


AI Development Journey

When did you start using AI for programming?

In August 2024, I created a RuneLite theme pack, but one of the plugin overlays didn't match my custom layout. I opened a GitHub issue (creating my first GitHub account to do so) requesting a customization option. The response was: "It's not a priority - if you want it, build it yourself."
I used ChatGPT to guide me through forking RuneLite and creating a plugin. This experience sparked intense interest in underlying software principles rather than just syntax.



How has your approach evolved over time?

I view development like a book: syntax is the cover, logic is the content itself. Rather than learning syntax structures, I focused on core meta-principles - how software interacts, how logic flows, different algorithm types. I quickly realized everything reduces to the same foundation: question and answer sequences.
Large code structures are essentially chaotic meetings - one coordinator fielding questions and answers from multiple sources, trying to provide correct responses without mix-ups or misinterpretation. If this applies to human communication, it must apply to software principles.



What were your biggest mistakes with AI collaboration?

Expecting it to intuitively understand my requirements, provide perfect fixes, be completely honest, and act like a true expert. This was all elaborate roleplay that produced poor code. While fine for single-purpose scripts, it failed completely for scalable codebases.
I learned not to feed requirements and hope for the best. Instead, I needed to collaborate actively - create plans, ask for feedback on content clarity, and identify uncertainties. This gradual process taught me the AI's actual capabilities and most effective collaboration methods.


Methodology Specifics

Why 150 lines exactly?

Multiple benefits: easy readability, clear understanding, modularity enforcement, architectural clarity, simple maintenance, component testing, optimal AI context retention, reusability, and KISS principle adherence.



How did you determine Phase 0 requirements?

From meta-principles of software: if it displays, it must run; if it runs, it can be measured; if it can be measured, it can be optimized; if it can be optimized, it can be reliable; if it can be reliable, it can be trusted.
Regardless of project type, anything requiring architecture needs these foundations. You must ensure changes don't negatively impact the entire system. A single line modification in a nested function might work perfectly but cause 300ms boot time regression for all users.
By testing during development, you catch inefficiencies early. Integration from the start means simply hooking up new components and running tests via command line - minimal time investment with actual value returned. I prefer validation and consistency throughout development rather than programming blind.


Practical Implementation

How do you handle projects that don't fit the methodology?

I adapt them to fit, or if truly impossible, I adjust the method itself. This is one methodology - I can generate countless variations as needed. Having spent 6700+ hours in AI interactions across multiple domains (not just software), I've developed strong system comprehension that enables creating adjusted methodologies on demand.



What's the learning curve for new users?

I cannot accurately answer this question. I've learned that I'm neurologically different - what I perceive as easy or obvious isn't always the case for others. This question is better addressed by someone who has actually used this methodology to determine its learning curve.



When shouldn't someone use this approach?

If you're not serious about projects, despise AI, dislike planning, don't care about modularization, or are just writing simple scripts. However, for anything requiring reliability, I believe this is currently the most effective method.
You still need programming fundamentals to use this methodology effectively - it's significantly more structured than ad-hoc approaches.



Workflow Visualization

  
      ---
config:
  layout: elk
  theme: neo-dark
---
flowchart TD
    A["Project Idea"] --> B["ğŸ¤– Stage 1: AI Configuration<br>AI-PREFERENCES.md Custom Instructions"]
    B --> C["Stage 2: Collaborative Planning<br>Share METHODOLOGY.md"]
    C --> D["Define Scope & Completion Criteria"]
    D --> E["Identify Components & Dependencies"]
    E --> F["Structure Phases Based on Logic"]
    F --> G["Document Edge Cases - No Implementation"]
    G --> H["Generate Development Plan with Checkpoints"]
    H --> I["ğŸ”§ Stage 3: Phase 0 Infrastructure<br>MANDATORY BEFORE ANY CODE"]
    I --> J["Benchmarking Suite + Regression Detection"]
    J --> K["GitHub Workflows + Quality Gates"]
    K --> L["Test Suite Infrastructure + Stress Tests"]
    L --> M["Documentation Generation System"]
    M --> N["Centralized Configuration + Constants"]
    N --> O["ğŸ“ project_extract.py Setup<br>Single/Multiple File Config"]
    O --> P["Initial Project State Extraction"]
    P --> Q["Share Context with AI"]
    Q --> R["Start Development Session<br>Pre-Session Compliance Audit"]
    R --> S{"Next Phase Available?"}
    S -- No --> Z["Project Complete"]
    S -- Yes --> T["Select Single Component<br>Target â‰¤150 Lines"]
    T --> U{"Multi-Language Required?"}
    U -- Yes --> V["Document Performance Justification<br>Measurable Benefits Required"]
    V --> W["Request AI Implementation"]
    U -- No --> W
    W --> X{"AI Uncertainty Flag?"}
    X -- âš ï¸ Yes --> Y["Request Clarification<br>Provide Additional Context"]
    Y --> W
    X -- Clear --> AA["Stage 3: Systematic Implementation"]
    AA --> BB{"Automated Size Check<br>validate-phase Script"}
    BB -- >150 Lines --> CC["AUTOMATED: Split Required<br>Maintain SoC Boundaries"]
    CC --> W
    BB -- â‰¤150 Lines --> DD["Incremental Compliance Check<br>DRY/KISS/SoC Validation"]
    DD --> EE{"Architectural Principles Pass?"}
    EE -- No --> FF["Flag Specific Violations<br>Reference Methodology"]
    FF --> W
    EE -- Yes --> GG["ğŸ“Š Stage 4: Data-Driven Iteration<br>Run Benchmark Suite + Save Baselines"]
    GG --> HH["Compare Against Historical Timeline<br>Regression Analysis"]
    HH --> II{"Performance Gate Pass?"}
    II -- Regression Detected --> JJ["Share Performance Data<br>Request Optimization"]
    JJ --> W
    II -- Pass --> KK["Integration Test<br>Verify System Boundaries"]
    KK --> LL{"Cross-Platform Validation?"}
    LL -- Fail --> MM["Address Deployment Constraints<br>Real-World Considerations"]
    MM --> W
    LL -- Pass --> NN{"More Components in Phase?"}
    NN -- Yes --> T
    NN -- No --> OO["ğŸš¦ Phase Quality Gate<br>Full Architecture Audit"]
    OO --> PP["Production Simulation<br>Resource Cleanup + Load Test"]
    PP --> QQ{"All Quality Gates Pass?"}
    QQ -- No --> RR["Document Failed Checkpoints<br>Block Phase Progression"]
    RR --> T
    QQ -- Yes --> SS["End Development Session<br>Technical Debt Assessment"]
    SS --> TT["ğŸ“ Extract Updated Project State<br>Generate Fresh Context"]
    TT --> UU["Phase Results Documentation<br>Metrics + Outcomes + Timeline"]
    UU --> VV["Update Development Plan<br>Mark Phase Complete"]
    VV --> S
    WW["validate-phase<br>AUTOMATED: File Size + Structure"] -.-> BB
    XX["dry-audit<br>AUTOMATED: Cross-Module Duplication"] -.-> DD
    YY["CI/CD Workflows<br>AUTOMATED: Merge Gates"] -.-> GG
    ZZ["Performance Timeline<br>AUTOMATED: Historical Data"] -.-> HH
    AAA["Dependency Validator<br>AUTOMATED: Import Boundaries"] -.-> KK
    BBB["Architecture Auditor<br>AUTOMATED: SoC Compliance"] -.-> OO
    WW -. BUILD FAILURE .-> CC
    YY -. MERGE BLOCKED .-> JJ
    BBB -. AUDIT FAILURE .-> RR
    style Y fill:#7d5f00
    style CC fill:#770000
    style FF fill:#7d5f00
    style JJ fill:#7d5f00
    style MM fill:#770000
    style RR fill:#770000

    
  
    
      Loading

  


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Let us git rid of it, angry GitHub users say of forced Copilot features]]></title>
            <link>https://www.theregister.com/2025/09/05/github_copilot_complaints/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148167</guid>
            <description><![CDATA[: Unavoidable AI has developers looking for alternative code hosting options]]></description>
            <content:encoded><![CDATA[
Among the software developers who use Microsoft's GitHub, the most popular community discussion in the past 12 months has been a request for a way to block Copilot, the company's AI service, from generating issues and pull requests in code repositories.
The second most popular discussion â€“ where popularity is measured in upvotes â€“ is a bug report that seeks a fix for the inability of users to disable Copilot code reviews.
Both of these questions, the first opened in May and the second opened a month ago, remain unanswered, despite an abundance of comments critical of generative AI and Copilot.Â 

    

The author of the first, developer Andi McClure, published a similar request to Microsoft's Visual Studio Code repository in January, objecting to the reappearance of a Copilot icon in VS Code after she had uninstalled the Copilot extension.

        


        

Microsoft and GitHub, not to mention rivals like Google, have gone all-in on a technology that a sizable or at least vocal portion of their customers simply don't want. And with billions in capital expenditures to recoup, they're making it difficult to avoid.
During Microsoft's July 30, 2025 earnings call, CEO Satya Nadella said GitHub Copilot continued to exhibit strong momentum and had reached 20 million users.

        

"GitHub Copilot Enterprise customers increased 75 percent quarter over quarter as companies tailor Copilot to their own codebases," said Nadella, noting that AI adoption has increased usage of GitHub over the past year.


I deeply resent that on top of Copilot seemingly training itself on my GitHub-posted code in violation of my licenses, GitHub wants me to look at (effectively) ads for this project I will never touch

"I've been for a while now filing issues in the GitHub Community feedback area when Copilot intrudes on my GitHub usage," McClure told The Register in an email. "I deeply resent that on top of Copilot seemingly training itself on my GitHub-posted code in violation of my licenses, GitHub wants me to look at (effectively) ads for this project I will never touch. If something's bothering me, I don't see a reason to stay quiet about it. I think part of how we get pushed into things we collectively don't want is because we stay quiet about it."
It's not just the burden of responding to AI slop, an ongoing issue for Curl maintainer Daniel Stenberg. It's the permissionless copying and regurgitation of speculation as fact, mitigated only by small print disclaimers that generative AI may produce inaccurate results. It's also GitHub's disavowal of liability if Copilot code suggestions happen to have reproduced source code that requires attribution.
It's what the Servo project characterizes in its ban on AI code contributions as the lack of code correctness guarantees, copyright issues, and ethical concerns. Similar objections have been used to justify AI code bans in GNOME's Loupe project, FreeBSD, Gentoo, NetBSD, and QEMU.
McClure said she has been filing requests to opt out of Copilot for a few years now, but in the last six months, her posts have been attracting more community support.Â 

        

Two issues, about the abovementioned Copilot menu in VS Code and the inability to block Copilot-generated issues and pull requests, she said, have continued to attract comments.
"People keep finding these issues somehow and tacking on to them," McClure said. "Although Microsoft's been forcing the Copilot 'asks' into more and more places in the interface for a while, sometime this year they hit an inflection point where mass numbers of people don't feel like ignoring it anymore, where before they could shrug and ignore it or find the off switch."
In the past month, she said, there's been a second change in the way people see GitHub â€“ GitHub's demotion from distinct subsidiary to part of Microsoft's CoreAI group.


Bot shots: US Army enlists AI startup to provide target-tracking

OpenAI eats jobs, then offers to help you find a new one at Walmart

Boffins build automated Android bug hunting system

Atlassian acquisition drives dream of AI-powered ChromeOS challenger

"Despite being a symbolic change, it seems to have galvanized the open source community from just complaining about Copilot to now actively moving away from GitHub," said McClure. "Many of my contacts in the open source community have been talking about plans to move from GitHub to Codeberg or a self-hosted Forgejo (Forgejo is the software used by Codeberg) over the last month, and the comments in those two always-busy GitHub threads have increasingly been people describing how Copilot is inspiring them to move to Codeberg as well."
Calls to shun Microsoft and GitHub go back a long way in the open source community, but moved beyond simmering dissatisfaction in 2022 when the Software Freedom Conservancy (SFC) urged free software supporters to give up GitHub, a position SFC policy fellow Bradley M. Kuhn recently reiterated.
Some of the developers participating in the issues raised by McClure and by others have said they intend to move away from GitHub over its stance on AI.
"Today I rejected two Copilot-generated code suggestions on my PR," wrote a developer who posted to McClure's thread under the name Constantine. "This was very disturbing, so I started googling and found this discussion. I refuse using AI in the same way I don't take drugs or steal things - for me it's a matter of principle. So if this continues and Microsoft does not provide a way to opt out of AI for my repositories soon, I will move my code to a self-hosted solution and won't ever return to GitHub."
McClure said she has been slowly shifting toward Codeberg over the past few months. "I haven't been proactively moving repos but whenever I make a change to a repo I clone it to Codeberg, post the change there, and replace my main branch on the GitHub repo with a relocation notice," she said.
"Microsoft as a company has a running problem where they won't take no for an answer, whether with 'AI' or with any other product they want to ship," said McClure. "A favorite tactic of theirs recently is they will enable a thing by default and put an off switch, wait six months, and then slightly change or rename the feature you turned off, and create a new off switch you have to separately turn off. They did this with Bing in Windows 10 and now they're doing it with Copilot in their developer tools (and presumably Windows 11, I don't know, I don't use Windows 11)."
McClure said that when Microsoft began adding Copilot to everything, starting with Android keyboard SwiftKey, she concluded that the situation would reprise the handling of Bing/Cortana in Windows 10 and turning it off would not be enough.


If you really find Copilot unacceptable â€“ and I do, Copilot is so much more noxious than Microsoft's previous forced bundlings â€“ the only option is to stop using any Microsoft product that Copilot shows up in

"If you really find Copilot unacceptable â€“ and I do, Copilot is so much more noxious than Microsoft's previous forced bundlings â€“ the only option is to stop using any Microsoft product that Copilot shows up in," she said. "I stopped using SwiftKey; I started migrating from desktop Windows to Linux when it became clear mandatory AI surveillance would be a core part of Win11. GitHub and, more sporadically, Visual Studio Code I have had to keep using because they're monopolies in a way even Windows isn't. The network effects (projects whose sole method of communication is GitHub, software whose only IDE integration is a VSCode plugin) are too strong."
Things have progressed as expected, McClure said, with Copilot buttons appearing in VS Code even when Copilot has been uninstalled and poorly labeled buttons that redirect to Copilot searches. She suggests people are starting to tire of the situation and that if it continues, it will weaken the network effects that keep developers tied to GitHub, accelerating further migration.
"When this happens I have no idea if Microsoft will notice or care," said McClure. "The Copilot push at Microsoft appears to be completely top-down and the people at the top seem to have completely forgotten about conventional goals like customer retention. They want to pump up those 'AI' numbers, for whatever reason, and they view their customer base as just a resource to burn to get those metrics up."
GitHub did not respond to a request for comment. Â®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why Language Models Hallucinate]]></title>
            <link>https://openai.com/index/why-language-models-hallucinate/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45147385</guid>
        </item>
        <item>
            <title><![CDATA[Rug pulls, forks, and open-source feudalism]]></title>
            <link>https://lwn.net/SubscriberLink/1036465/e80ebbc4cee39bfb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45146967</guid>
            <description><![CDATA[Like almost all human endeavors, open-source software development involves a range of power dyn [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



Like almost all human endeavors, open-source software development involves
a range of power dynamics.  Companies, developers, and users are all
concerned with the power to influence the direction of the software â€” and,
often, to profit from it.  At the 2025 Open
Source Summit Europe, Dawn Foster talked about how those dynamics can
play out, with an eye toward a couple of tactics â€” rug pulls and forks â€” that
are available to try to shift power in one direction or another.
Power dynamics

Since the beginning of history, Foster began, those in power have tended to
use it against those who were weaker.  In the days of feudalism, control of
the land led to exploitation at several levels.  In the open-source world,
the large cloud providers often seem to have the most power, which they use
against smaller companies.  Contributors and maintainers often have less
power than even the smaller companies, and users have less power yet.  



We have built a world where it is often easiest to just use whatever a
cloud provider offers, even with open-source software.  Those providers may
not contribute back to the projects they turn into services, though,
upsetting the smaller companies that are, likely as not, doing the bulk of
the work to provide the software in question in the first place.  Those
companies can have a power of their own, however: the power to relicense
the software.  Pulling the rug out from under users of the software in this
way can change the balance of power with regard to cloud providers, but it
leaves contributors and users in a worse position than before.  But
there is a power at this level too: the power to fork the software,
flipping the power balance yet again.

Companies that control a software project have the power to carry out this
sort of rug pull, and they are often not shy about exercising it.
Single-company projects, clearly, are at a much higher risk of rug pulls;
the company has all the power in this case, and others have little
recourse.  So one should look at a company's reputation before adopting a
software project, but that is only so helpful.  Companies can change
direction without notice, be acquired, or go out of business, making
previous assessments of their reputation irrelevant.

The problem often comes down to the simple fact that companies have to
answer to their investors, and that often leads to pressure to relicense
the software they have created in order to increase revenue.  This is
especially true in cases where cloud providers are competing for the same
customers as the company that owns the project.  The result can be a switch
to a more restrictive license aimed at making it harder for other companies
to profit from the project.

A rug pull of this nature can lead to a fork of the project â€” a rebellious,
collective action aimed at regaining some power over the code.  But a fork
is not a simple matter; it is a lot of work, and will fail without people
and resources behind it.  The natural source for that is a large company;
cloud providers, too, can try to shift power via a fork, and they have the
ability to back their fork up with the resources it needs to succeed.



A relicensing event does not always lead to a popular fork; that did not
happen with MongoDB or Sentry, for example.  Foster said she had not looked
into why that was the case.  Sometimes rug pulls take other forms, such as
when Perforce, after acquiring Puppet in 2022, moved it development and
releases behind closed doors, with a reduced frequency of releases back to
the public repository.  That action kicked off the OpenVox fork.
Looking at the numbers

Foster has spent some time analyzing rug pulls, forks, and what happens
thereafter; a lot of the results are available
for download as Jupyter notebooks.  For each rug-pull event, she looked
at the contributor makeup of the project before and after the ensuing fork
in an attempt to see what effects are felt by the projects involved.

In 2021, Elastic relicensed Elasticsearch
under the non-free Server Side Public License (SSPL).  Amazon Web Services
then forked the project as OpenSearch.  Before the fork, most of
the Elasticsearch contributors were Elastic employees; that,
unsurprisingly, did not change afterward.  OpenSearch started with no
strong contributor base, so had to build its community from scratch.  As a
result, the project has been dominated by Amazon contributors ever since;
the balance has shifted slowly over time, but there was not a big uptick in
outside contributors even after OpenSearch became a Linux Foundation
project in 2024.  While starting a project under a neutral foundation can
help attract contributors, she said, moving a project under a foundation's
umbrella later on does not seem to provide the same benefit.

Terraform was
developed mostly by Hashicorp, which relicensed
the software under the non-free Business Source License in 2023.  One
month later, the OpenTofu fork was
started under the Linux Foundation.  While the contributor base for
Terraform, which was almost entirely Hashicorp employees, changed little
after the fork, OpenTofu quickly acquired a number of contributors from
several companies, none of whom had been Terraform contributors before.  In
this case, users drove the fork and placed it under a neutral foundation,
resulting in a more active developer community.

In 2024, Redis was relicensed under the
SSPL; the Valkey fork was quickly organized, under the Linux Foundation,
by Redis contributors.  The Redis project differed from the others
mentioned here in that, before the fork, it had nearly twice as many
contributors from outside the company as from within; after the fork, the
number of external Redis contributors dropped to zero.  All of the external
contributors fled to Valkey, with the result that Valkey started with a
strong community representing a dozen or so companies.

Looking at how the usage of these projects changes is harder, she
said, but there appears to be a correlation between the usage of a project
and the number of GitHub forks (cloned repository copies) it has.  There is
typically a spike in these clones after a relicensing event, suggesting
that people are considering creating a hard fork of the project.  In all
cases, the forks that emerged appeared to have less usage than the original
by the "GitHub forks" metric; both branches of the fork continue to go
forward.  But, she said, projects that are relicensed do tend to show
reduced usage, especially when competing forks are created under foundations.
What to do

This kind of power game creates problems for both contributors and users,
she said; we contribute our time to these projects, and need them to not be
pulled out from under us.  There is no way to know when a rug pull might
happen, but there are some warning signs to look out for.  At the top of
her list was the use of a contributor license agreement (CLA); these
agreements create a power imbalance, giving the company involved the power
to relicense the software.  Projects with CLAs more commonly are subject to
rug pulls; projects using a developers certificate of origin do not have the
same power imbalance and are less likely to be rug pulled.

One should also look at the governance of a project; while being housed
under a foundation reduces the chance of a rug pull, that can still happen,
especially in cases where the contributors are mostly from a single
company.  She mentioned the Cortex project, housed under
the Cloud Native Computing Foundation, which was controlled by Grafana; that
company eventually forked its own project to create Mimir.  To avoid this kind of
surprise, one should look for projects with neutral governance, with
leaders from multiple organizations.

Projects should also be evaluated on their contributor base; are there
enough contributors to keep things going?  Companies can help, of course,
by having their employees contribute to the projects they depend on,
increasing influence and making those projects more sustainable.  She
mentioned the CHAOSS project, which
generates metrics to help in the judgment of the viability of development
projects.  CHAOSS has put together a set of
"practitioner guides" intended to help contributors and maintainers
make improvements within a project.

With the sustained rise of the big cloud providers, she concluded, the
power dynamics around open-source software are looking increasingly feudal.
Companies can use relicensing to shift power away from those providers, but
they also take power from contributors when the pull the rug in this way.
Those contributors, though, are in a better position than the serfs of old,
since they have the ability to fork a project they care about, shifting
power back in their direction.


Hazel Weakly asked if there are other protections that contributors and
users might develop to address this problem.  Foster answered that at least
one company changed its mind about a planned relicensing action after
seeing the success of the Valkey and OpenTofu forks.  The ability to fork
has the effect of making companies think harder, knowing that there may be
consequences that follow a rug pull.  Beyond that, she reiterated that
projects should be pushed toward neutral governance.

Dirk Hohndel added that the best thing to do is to bring more outside
contributors into a project; the more of them there are, the higher the
risk associated with a rug pull.  Anybody who just sits back within a
project, he said, is just a passenger; it is better to be driving.

Foster's
slides are available for interested readers.

[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Developing a Space Flight Simulator in Clojure]]></title>
            <link>https://www.wedesoft.de/software/2025/09/05/clojure-game/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45145794</guid>
            <description><![CDATA[Developing a Space Flight Simulator in Clojure]]></description>
            <content:encoded><![CDATA[
  
  05 Sep 2025    

In 2017 I discovered the free of charge Orbiter 2016 space flight simulator which was proprietary at the time and it inspired me to develop a space flight simulator myself.
I prototyped some rigid body physics in C and later in GNU Guile and also prototyped loading and rendering of Wavefront OBJ files.
I used GNU Guile (a Scheme implementation) because it has a good native interface and of course it has hygienic macros.
Eventually I got interested in Clojure because unlike GNU Guile it has multi-methods as well as fast hash maps and vectors.
I finally decided to develop the game for real in Clojure.
I have been developing a space flight simulator in Clojure for almost 5 years now.
While using Clojure I have come to appreciate the immutable values and safe parallelism using atoms, agents, and refs.

In the beginning I decided to work on the hard parts first, which for me were 3D rendering of a planet, an atmosphere, shadows, and volumetric clouds.
I read the OpenGL Superbible to get an understanding on what functionality OpenGL provides.
When Orbiter was eventually open sourced and released unter MIT license here, I inspected the source code and discovered that about 90% of the code is graphics-related.
So starting with the graphics problems was not a bad decision.

Software dependencies

The following software is used for development.
The software libraries run on both GNU/Linux and Microsoft Windows.


  Clojure the programming language
  LWJGL provides Java wrappers for various libraries
    
      lwjgl-opengl for 3D graphics
      lwjgl-glfw for windowing and input devices
      lwjgl-nuklear for graphical user interfaces
      lwjgl-stb for image I/O and using truetype fonts
      lwjgl-assimp to load glTF 3D models with animation data
    
  
  Jolt Physics to simulate wheeled vehicles and collisions with meshes
  Fastmath for fast matrix and vector math as well as spline interpolation
  Comb for templating shader code
  Instaparse to parse NASA Planetary Constant Kernel (PCK) files
  Gloss to parse NASA Double Precision Array Files (DAF)
  Coffi as a foreign function interface
  core.memoize for least recently used caching of function results
  Apache Commons Compress to read map tiles from tar files
  Malli to add schemas to functions
  Immuconf to load the configuration file
  Progrock a progress bar for long running builds
  Claypoole to implement parallel for loops
  tools.build to build the project
  clj-async-profiler Clojure profiler creating flame graphs
  slf4j-timbre Java logging implementation for Clojure


The deps.edn file contains operating system dependent LWJGL bindings.
For example on GNU/Linux the deps.edn file contains the following:

{:deps {; ...
        org.lwjgl/lwjgl {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl$natives-linux {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-opengl {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-opengl$natives-linux {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-glfw {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-glfw$natives-linux {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-nuklear {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-nuklear$natives-linux {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-stb {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-stb$natives-linux {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-assimp {:mvn/version "3.3.6"}
        org.lwjgl/lwjgl-assimp$natives-linux {:mvn/version "3.3.6"}}
        ; ...
        }

In order to manage the different dependencies for Microsoft Windows, a separate Git branch is maintained.

Atmosphere rendering


    

For the atmosphere, Brunetonâ€™s precomputed atmospheric scattering was used.
The implementation uses a 2D transmittance table, a 2D surface scattering table, a 4D Rayleigh scattering, and a 4D Mie scattering table.
The tables are computed using several iterations of numerical integration.
Higher order functions for integration over a sphere and over a line segment were implemented in Clojure.
Integration over a ray in 3D space (using fastmath vectors) was implemented as follows for example:

(defn integral-ray
  "Integrate given function over a ray in 3D space"
  {:malli/schema [:=> [:cat ray N :double [:=> [:cat [:vector :double]] :some]] :some]}
  [{::keys [origin direction]} steps distance fun]
  (let [stepsize      (/ distance steps)
        samples       (mapv #(* (+ 0.5 %) stepsize) (range steps))
        interpolate   (fn interpolate [s] (add origin (mult direction s)))
        direction-len (mag direction)]
    (reduce add (mapv #(-> % interpolate fun (mult (* stepsize direction-len))) samples))))

Precomputing the atmospheric tables takes several hours even though pmap was used.
When sampling the multi-dimensional functions, pmap was used as a top-level loop and map was used for interior loops.
Using java.nio.ByteBuffer the floating point values were converted to a byte array and then written to disk using a clojure.java.io/output-stream:

(defn floats->bytes
  "Convert float array to byte buffer"
  [^floats float-data]
  (let [n           (count float-data)
        byte-buffer (.order (ByteBuffer/allocate (* n 4)) ByteOrder/LITTLE_ENDIAN)]
    (.put (.asFloatBuffer byte-buffer) float-data)
    (.array byte-buffer)))

(defn spit-bytes
  "Write bytes to a file"
  {:malli/schema [:=> [:cat non-empty-string bytes?] :nil]}
  [^String file-name ^bytes byte-data]
  (with-open [out (io/output-stream file-name)]
    (.write out byte-data)))

(defn spit-floats
  "Write floating point numbers to a file"
  {:malli/schema [:=> [:cat non-empty-string seqable?] :nil]}
  [^String file-name ^floats float-data]
  (spit-bytes file-name (floats->bytes float-data)))

When launching the game, the lookup tables get loaded and copied into OpenGL textures.
Shader functions are used to lookup and interpolate values from the tables.
When rendering the planet surface or the space craft, the atmosphere essentially gets superimposed using ray tracing.
After rendering the planet, a background quad is rendered to display the remaining part of the atmosphere above the horizon.

Templating OpenGL shaders

It is possible to make programming with OpenGL shaders more flexible by using a templating library such as Comb.
The following shader defines multiple octaves of noise on a base noise function:

#version 410 core

float <%= base-function %>(vec3 idx);

float <%= method-name %>(vec3 idx)
{
  float result = 0.0;
<% (doseq [multiplier octaves] %>
  result += <%= multiplier %> * <%= base-function %>(idx);
  idx *= 2;
<% ) %>
  return result;
}

One can then for example define the function fbm_noise using octaves of the base function noise as follows:

(def noise-octaves
  "Shader function to sum octaves of noise"
  (template/fn [method-name base-function octaves] (slurp "resources/shaders/core/noise-octaves.glsl")))

; ...

(def fbm-noise-shader (noise-octaves "fbm_noise" "noise" [0.57 0.28 0.15]))

Planet rendering


    

To render the planet, NASA Bluemarble data, NASA Blackmarble data, and NASA Elevation data was used.
The images were converted to a multi resolution pyramid of map tiles.
The following functions were implemented for color map tiles and for elevation tiles:


  a function to load and cache map tiles of given 2D tile index and level of detail
  a function to extract a pixel from a map tile
  a function to extract the pixel for a specific longitude and latitude


The functions for extracting a pixel for given longitude and latitude then were used to generate a cube map with a quad tree of tiles for each face.
For each tile, the following files were generated:


  A daytime texture
  A night time texture
  An image of 3D vectors defining a surface mesh
  A water mask
  A normal map


Altogether 655350 files were generated.
Because the Steam ContentBuilder does not support a large number of files, each row of tile data was aggregated into a tar file.
The Apache Commons Compress library allows you to open a tar file to get a list of entries and then perform random access on the contents of the tar file.
A Clojure LRU cache was used to maintain a cache of open tar files for improved performance.

At run time, a future is created, which returns an updated tile tree, a list of tiles to drop, and a path list of the tiles to load into OpenGL.
When the future is realized, the main thread deletes the OpenGL textures from the drop list, and then uses the path list to get the new loaded images from the tile tree, load them into OpenGL textures, and create an updated tile tree with the new OpenGL textures added.
The following functions to manipulate quad trees were implemented to realize this:

(defn quadtree-add
  "Add tiles to quad tree"
  {:malli/schema [:=> [:cat [:maybe :map] [:sequential [:vector :keyword]] [:sequential :map]] [:maybe :map]]}
  [tree paths tiles]
  (reduce (fn add-title-to-quadtree [tree [path tile]] (assoc-in tree path tile)) tree (mapv vector paths tiles)))

(defn quadtree-extract
  "Extract a list of tiles from quad tree"
  {:malli/schema [:=> [:cat [:maybe :map] [:sequential [:vector :keyword]]] [:vector :map]]}
  [tree paths]
  (mapv (partial get-in tree) paths))

(defn quadtree-drop
  "Drop tiles specified by path list from quad tree"
  {:malli/schema [:=> [:cat [:maybe :map] [:sequential [:vector :keyword]]] [:maybe :map]]}
  [tree paths]
  (reduce dissoc-in tree paths))

(defn quadtree-update
  "Update tiles with specified paths using a function with optional arguments from lists"
  {:malli/schema [:=> [:cat [:maybe :map] [:sequential [:vector :keyword]] fn? [:* :any]] [:maybe :map]]}
  [tree paths fun & arglists]
  (reduce (fn update-tile-in-quadtree
            [tree [path & args]]
            (apply update-in tree path fun args)) tree (apply map list paths arglists)))

Other topics

Solar system

The astronomy code for getting the position and orientation of planets was implemented according to the Skyfield Python library.
The Python library in turn is based on the SPICE toolkit of the NASA JPL.
The JPL basically provides sequences of Chebyshev polynomials to interpolate positions of Moon and planets as well as the orientation of the Moon as binary files.
Reference coordinate systems and orientations of other bodies are provided in text files which consist of human and machine readable sections.
The binary files were parsed using Gloss (see Wiki for some examples) and the text files using Instaparse.

Jolt bindings

The required Jolt functions for wheeled vehicle dynamics and collisions with meshes were wrapped in C functions and compiled into a shared library.
The Coffi Clojure library (which is a wrapper for Javaâ€™s new Foreign Function & Memory API) was used to make the C functions and data types usable in Clojure.

For example the following code implements a call to the C function add_force:

(defcfn add-force
  "Apply a force in the next physics update"
  add_force [::mem/int ::vec3] ::mem/void)

Here ::vec3 refers to a custom composite type defined using basic types.
The memory layout, serialisation, and deserialisation for ::vec3 are defined as follows:

(def vec3-struct
  [::mem/struct
   [[:x ::mem/double]
    [:y ::mem/double]
    [:z ::mem/double]]])


(defmethod mem/c-layout ::vec3
  [_vec3]
  (mem/c-layout vec3-struct))


(defmethod mem/serialize-into ::vec3
  [obj _vec3 segment arena]
  (mem/serialize-into {:x (obj 0) :y (obj 1) :z (obj 2)} vec3-struct segment arena))


(defmethod mem/deserialize-from ::vec3
  [segment _vec3]
  (let [result (mem/deserialize-from segment vec3-struct)]
    (vec3 (:x result) (:y result) (:z result))))

Performance

The clj-async-profiler was used to create flame graphs visualising the performance of the game.
In order to get reflection warnings for Java calls without sufficient type declarations, *warn-on-reflection* was set to true.

(set! *warn-on-reflection* true)

Furthermore to discover missing declarations of numerical types, *unchecked-math* was set to :warn-on-boxed.

(set! *unchecked-math* :warn-on-boxed)

To reduce garbage collector pauses, the ZGC low-latency garbage collector for the JVM was used.
The following section in deps.edn ensures that the ZGC garbage collector is used when running the project with clj -M:run:

{:deps {; ...
        }
 :aliases {:run {:jvm-opts ["-Xms2g" "-Xmx4g" "--enable-native-access=ALL-UNNAMED" "-XX:+UseZGC"
                            "--sun-misc-unsafe-memory-access=allow"]
                 :main-opts ["-m" "sfsim.core"]}}}

The option to use ZGC is also specified in the Packr JSON file used to deploy the application.

Building the project

In order to build the map tiles, atmospheric lookup tables, and other data files using tools.build, the project source code was made available in the build.clj file using a :local/root dependency:

{:deps {; ...
        }
 :aliases {; ...
           :build {:deps {io.github.clojure/tools.build {:mvn/version "0.10.10"}
                          sfsim/sfsim {:local/root "."}}
                   :ns-default build
                   :exec-fn all
                   :jvm-opts ["-Xms2g" "-Xmx4g" "--sun-misc-unsafe-memory-access=allow"]}}}

Various targets were defined to build the different components of the project.
For example the atmospheric lookup tables can be build by specifying clj -T:build atmosphere-lut on the command line.

The following section in the build.clj file was added to allow creating an â€œUberjarâ€ JAR file with all dependencies by specifying clj -T:build uber on the command-line.

(defn uber [_]
  (b/copy-dir {:src-dirs ["src/clj"]
               :target-dir class-dir})
  (b/compile-clj {:basis basis
                  :src-dirs ["src/clj"]
                  :class-dir class-dir})
  (b/uber {:class-dir class-dir
           :uber-file "target/sfsim.jar"
           :basis basis
           :main 'sfsim.core}))

To create a Linux executable with Packr, one can then run java -jar packr-all-4.0.0.jar scripts/packr-config-linux.json where the JSON file has the following content:

{
  "platform": "linux64",
  "jdk": "/usr/lib/jvm/jdk-24.0.2-oracle-x64",
  "executable": "sfsim",
  "classpath": ["target/sfsim.jar"],
  "mainclass": "sfsim.core",
  "resources": ["LICENSE", "libjolt.so", "venturestar.glb", "resources"],
  "vmargs": ["Xms2g", "Xmx4g", "XX:+UseZGC"],
  "output": "out-linux"
}

In order to distribute the game on Steam, three depots were created:


  a data depot with the operating system independent data files
  a Linux depot with the Linux executable and Uberjar including LWJGLâ€™s Linux native bindings
  and a Windows depot with the Windows executable and an Uberjar including LWJGLâ€™s Windows native bindings


When updating a depot, the Steam ContentBuilder command line tool creates and uploads a patch in order to preserve storage space and bandwidth.

Future work

Although the hard parts are mostly done, there are still several things to do:


  control surfaces and thruster graphics
  launchpad and runway graphics
  sound effects
  a 3D cockpit
  the Moon
  a space station


It would also be interesting to make the game modable in a safe way (maybe evaluating Clojure files in a sandboxed environment?).

Conclusion


    

You can find the source code on Github.
Currently there is only a playtest build, but if you want to get notified, when the game gets released, you can wishlist it here.

Anyway, let me know any comments and suggestions.

Enjoy!




  Flight dynamics model for simulating Venturestar style spacecraft
  Test Driven Development with OpenGL
  Implementing GUIs using Clojure and LWJGL Nuklear bindings
  Procedural Volumetric Clouds
  Procedural generation of global cloud cover
  Reversed-Z Rendering in OpenGL
  Specifying Clojure function schemas with Malli
  Implement an Interpreter using Clojure Instaparse
  Orbits with Jolt Physics
  Getting started with the Jolt Physics Engine
  Create Blender bones and animate and import with Assimp



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GLM 4.5 with Claude Code]]></title>
            <link>https://docs.z.ai/guides/llm/glm-4.5</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45145457</guid>
            <description><![CDATA[GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.
Both models share a similar training pipeline: an initial pretraining phase on 15 trillion tokens of general-domain data, followed by targeted fine-tuning on datasets covering code, reasoning, and agent-specific tasks. The context length has been extended to 128k tokens, and reinforcement learning was applied to further enhance reasoning, coding, and agent performance.
GLM-4.5 and GLM-4.5-Air are optimized for tool invocation, web browsing, software engineering, and front-end development. They can be integrated into code-centric agents such as Claude Code and Roo Code, and also support arbitrary agent applications through tool invocation APIs.
Both models support hybrid reasoning modes, offering two execution modes: Thinking Mode for complex reasoning and tool usage, and Non-Thinking Mode for instant responses. These modes can be toggled via the thinking.typeparameter (with enabled and disabled settings), and dynamic thinking is enabled by default.]]></description>
            <content:encoded><![CDATA[   Overview

GLM-4.5 and GLM-4.5-Air are our latest flagship models, purpose-built as foundational models for agent-oriented applications. Both leverage a Mixture-of-Experts (MoE) architecture. GLM-4.5 has a total parameter count of 355B with 32B active parameters per forward pass, while GLM-4.5-Air adopts a more streamlined design with 106B total parameters and 12B active parameters.
Both models share a similar training pipeline: an initial pretraining phase on 15 trillion tokens of general-domain data, followed by targeted fine-tuning on datasets covering code, reasoning, and agent-specific tasks. The context length has been extended to 128k tokens, and reinforcement learning was applied to further enhance reasoning, coding, and agent performance.
GLM-4.5 and GLM-4.5-Air are optimized for tool invocation, web browsing, software engineering, and front-end development. They can be integrated into code-centric agents such as Claude Code and Roo Code, and also support arbitrary agent applications through tool invocation APIs.
Both models support hybrid reasoning modes, offering two execution modes: Thinking Mode for complex reasoning and tool usage, and Non-Thinking Mode for instant responses. These modes can be toggled via the thinking.typeparameter (with enabled and disabled settings), and dynamic thinking is enabled by default.
   GLM-4.5 Serials

   Capability

   Introducting GLM-4.5
Overview
The first-principle measure of AGI lies in integrating more general intelligence capabilities without compromising existing functions. GLM-4.5 represents our first complete realization of this concept. It combines advanced reasoning, coding, and agent capabilities within a single model, achieving a significant technological breakthrough by natively fusing reasoning, coding, and agent abilities to meet the complex demands of agent-based applications.
To comprehensively evaluate the modelâ€™s general intelligence, we selected 12 of the most representative benchmark suites, including MMLU Pro, AIME24, MATH 500, SciCode, GPQA, HLE, LiveCodeBench, SWE-Bench, Terminal-bench, TAU-Bench, BFCL v3, and BrowseComp. Based on the aggregated average scores, GLM-4.5 ranks second globally among all models, first among domestic models, and first among open-source models.

Higher Parameter Efficiency
GLM-4.5 has half the number of parameters of DeepSeek-R1 and one-third that of Kimi-K2, yet it outperforms them on multiple standard benchmark tests. This is attributed to the higher parameter efficiency of GLM architecture. Notably, GLM-4.5-Air, with 106 billion total parameters and 12 billion active parameters, achieves a significant breakthroughâ€”surpassing models such as Gemini 2.5 Flash, Qwen3-235B, and Claude 4 Opus on reasoning benchmarks like Artificial Analysis, ranking among the top three domestic models in performance.
On charts such as SWE-Bench Verified, the GLM-4.5 series lies on the Pareto frontier for performance-to-parameter ratio, demonstrating that at the same scale, the GLM-4.5 series delivers optimal performance.
Low Cost, High Speed
Beyond performance optimization, the GLM-4.5 series also achieves breakthroughs in cost and efficiency, resulting in pricing far lower than mainstream models: API call costs are as low as $0.2 per million input tokens and $1.1 per million output tokens.
At the same time, the high-speed version demonstrates a generation speed exceeding 100 tokens per second in real-world tests, supporting low-latency and high-concurrency deployment scenariosâ€”balancing cost-effectiveness with user interaction experience.
Real-World Evaluation
Real-world performance matters more than leaderboard rankings. To evaluate GLM-4.5â€™s effectiveness in practical Agent Coding scenarios, we integrated it into Claude Code and benchmarked it against Claude 4 Sonnet, Kimi-K2, and Qwen3-Coder.
The evaluation consisted of 52 programming and development tasks spanning six major domains, executed in isolated container environments with multi-turn interaction tests.
As shown in the results (below), GLM-4.5 demonstrates a strong competitive advantage over other open-source models, particularly in tool invocation reliability and task completion rate. While there remains room for improvement compared to Claude 4 Sonnet, GLM-4.5 delivers a largely comparable experience in most scenarios.
To ensure transparency, we have released all 52 test problems along with full agent trajectories for industry validation and reproducibility.
   Usage
Core Capability: Coding Skills â†’ Intelligent code generation | Real-time code completion | Automated bug fixing
Supports major languages including Python, JavaScript, and Java.
Generates well-structured, scalable, high-quality code based on natural language instructions.
Focuses on real-world development needs, avoiding templated or generic outputs.
Use Case: Complete refactoring-level tasks within 1 hour; generate full product prototypes in 5 minutes.
   Resources

API Documentation: Learn how to call the API.

    Quick Start
Thinking Mode
GLM-4.5 offers a â€œDeep Thinking Modeâ€ that users can enable or disable by setting the thinking.type parameter. This parameter supports two values: enabled (enabled) and disabled (disabled). By default, dynamic thinking is enabled.
Simple Tasks (No Thinking Required): For straightforward requests that do not require complex reasoning (e.g., fact retrieval or classification), thinking is unnecessary. Examples include:

When was Z.AI founded?
Translate the sentence â€œI love youâ€ into Chinese.


Moderate Tasks (Default/Some Thinking Required): Many common requests require stepwise processing or deeper understanding. The GLM-4.5 series can flexibly apply thinking capabilities to handle tasks such as:

Why does Jupiter have more moons than Saturn, despite Saturn being larger?
Compare the advantages and disadvantages of flying versus taking the high-speed train from Beijing to Shanghai.



Difficult Tasks (Maximum Thinking Capacity): For truly complex challengesâ€”such as solving advanced math problems, network-related questions, or coding issuesâ€”these tasks require the model to fully engage its reasoning and planning abilities, often involving many internal steps before arriving at an answer. Examples include:
Explain in detail how different experts in a Mixture-of-Experts (MoE) model collaborate.
Based on the recent weekâ€™s fluctuations of the Shanghai Composite Index and current political information, should I invest in a stock index ETF? Why?

Samples Code
Basic Callcurl -X POST "https://api.z.ai/api/paas/v4/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "glm-4.5",
    "messages": [
      {
        "role": "user",
        "content": "As a marketing expert, please create an attractive slogan for my product."
      },
      {
        "role": "assistant",
        "content": "Sure, to craft a compelling slogan, please tell me more about your product."
      },
      {
        "role": "user",
        "content": "Z.AI Open Platform"
      }
    ],
    "thinking": {
      "type": "enabled"
    },
    "max_tokens": 4096,
    "temperature": 0.6
  }'
Streaming Callcurl -X POST "https://api.z.ai/api/paas/v4/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{
    "model": "glm-4.5",
    "messages": [
      {
        "role": "user",
        "content": "As a marketing expert, please create an attractive slogan for my product."
      },
      {
        "role": "assistant",
        "content": "Sure, to craft a compelling slogan, please tell me more about your product."
      },
      {
        "role": "user",
        "content": "Z.AI Open Platform"
      }
    ],
    "thinking": {
      "type": "enabled"
    },
    "stream": true,
    "max_tokens": 4096,
    "temperature": 0.6
  }'
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tesla changes meaning of 'Full Self-Driving', gives up on promise of autonomy]]></title>
            <link>https://electrek.co/2025/09/05/tesla-changes-meaning-full-self-driving-give-up-promise-autonomy/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144900</guid>
            <description><![CDATA[Tesla has changed the meaning of â€œFull Self-Drivingâ€, also known as â€œFSDâ€, to give up on its original promise of...]]></description>
            <content:encoded><![CDATA[
					

	

Tesla has changed the meaning of â€œFull Self-Drivingâ€, also known as â€œFSDâ€, to give up on its original promise of delivering unsupervised autonomy.



Since 2016, Tesla has claimed that all its vehicles in production would be capable of achieving unsupervised self-driving capability.



CEO Elon Musk has claimed that it would happen by the end of every year since 2018.



Tesla has even sold a software package, known as â€œFull Self-Driving Capabilityâ€ (FSD), for up to $15,000 to customers, promising that the advanced driver-assist system would become fully autonomous through over-the-air software updates.	
	



Almost a decade later, the promise has yet to be fulfilled, and Tesla has already confirmed that all vehicles produced between 2016 and 2023 donâ€™t have the proper hardware to deliver unsupervised self-driving as promised.



Musk has been discussing the upgrade of the computers in these vehicles to appease owners, but thereâ€™s no concrete plan to implement it.



While thereâ€™s no doubt that Tesla has promised unsupervised self-driving capabilities to FSD buyers between 2016 and 2023, the automaker has since updated its language and now only sells â€œFull Self-Driving (Supervised)â€ to customers:







The fine print mentions that it doesnâ€™t make the vehicle â€œautonomousâ€ and doesnâ€™t promise it as a feature. 



In other words, people buying FSD today are not really buying the capability of unsupervised self-driving as prior buyers did.



Furthermore, Teslaâ€™s board has just submitted a new, unprecedented CEO compensation package for shareholdersâ€™ approval, which could give Musk up to $1 trillionÂ in stock options pending the achievement of certain milestones.



One of these milestones is Tesla having â€œ10 Million Active FSD Subscriptions.â€



At first glance, this would be hopeful for FSD buyers since part of Muskâ€™s compensation would be dependent on delivering on the FSD promises.



However, Tesla has changed the definition of FSD in the compensation package with an extremely vague oneâ€




â€œFSDâ€ means an advanced driving system, regardless of the marketing name used, that is capable of performing transportation tasks that provide autonomous or similar functionality under specified driving conditions.




Tesla now considers FSD only an â€œadvanced driving systemâ€ that should be â€œcapable of performing transportation tasks that prove autonomous or similar functionalityâ€.



The current version of FSD, which requires constant supervising by the driver, could easily fit that description.



Therefore, FSD now doesnâ€™t come with the inital promise of Tesla owners being able to go to sleep in their vehicles and wake up at their destination â€“ a promise that Musk has used to sell Tesla vehicles for years.



Electrekâ€™s Take



The way Tesla discusses autonomy with customers and investors versus how it presents it in its court filings and legally binding documents is strikingly different.



It should be worrying to anyone with an interest in this.



With this very vague description in the new CEO compensation package, Tesla could literally lower the price of FSD and even remove base Autopilot to push customers toward FSD and give Musk hundreds of billions of dollars in shares in the process.




	Thereâ€™s precedent for Tesla decreasing pricing on FSD. Initially, Musk said that Tesla would gradually increase the price of the FSD package as the features improved and approached unsupervised autonomy.



That was true for a while, but then Tesla started slashing FSD prices, which are now down $7,000 from their high in 2023:







The trend is quite apparent and coincidentally began when Teslaâ€™s sales started to decline.



FSD is now a simple ADAS system without any promise of unsupervised self-driving. This might quite honestly be one of the biggest cases of false advertising or bait-and-switch ever.
	
			
			
		
			
	FTC: We use income earning auto affiliate links. More.				]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Universe Within 12.5 Light Years]]></title>
            <link>http://www.atlasoftheuniverse.com/12lys.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144337</guid>
            <description><![CDATA[This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.]]></description>
            <content:encoded><![CDATA[
About the Map
This map shows all the star systems that lie within 12.5
light years of our Sun.  Most of the stars are red dwarfs - stars with a tenth of
the Sun's mass and less than one hundredth the luminosity.  Roughly eighty percent
of all the stars in the universe are red dwarfs, and the nearest star - Proxima - is
a typical example.

Epsilon Eridani is orbited by a large planet which might look like this.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anthropic agrees to pay $1.5B to settle lawsuit with book authors]]></title>
            <link>https://www.nytimes.com/2025/09/05/technology/anthropic-settlement-copyright-ai.html?unlocked_article_code=1.jk8.bTTt.Zir9wmtPaTp2&amp;smid=url-share</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45142885</guid>
        </item>
        <item>
            <title><![CDATA[My Own DNS Server at Home â€“ Part 1: IPv4]]></title>
            <link>https://jan.wildeboer.net/2025/08/My-DNS-Part-1/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45142397</guid>
            <description><![CDATA[â€œItâ€™s always DNSâ€ is a famous meme among network people. Name resolution is technically quite simple. Itâ€™s â€œjustâ€ translating a hostname like jan.wildeboer.net to an IP address. What could possibly go wrong? I am a radical optimist and detail-obsessed knowledge collector, so I decided to find out. As part of my goal to make my home network a little island of Digital Sovereignty, meaning that everything at home should JustWorkâ„¢, even with no working internet connection, a DNS server is needed.]]></description>
            <content:encoded><![CDATA[
        â€œItâ€™s always DNSâ€ is a famous meme among network people. Name resolution is technically quite simple. Itâ€™s â€œjustâ€ translating a hostname like jan.wildeboer.net to an IP address. What could possibly go wrong? I am a radical optimist and detail-obsessed knowledge collector, so I decided to find out. As part of my goal to make my home network a little island of Digital Sovereignty, meaning that everything at home should JustWorkâ„¢, even with no working internet connection, a DNS server is needed.


  Based on and extended from my gist Bind on Fedora 42 as DNS server.


I admit, I have a lot of experience with DNS and BIND. But I still consider myself to be merely on the GoodEnoughâ„¢ side of things. I know how to get DNS configured for my domains. And I want you to feel fearless too. The best place to fail with DNS is the network at home. It limits the impact :)

So read this blog post either as report or as a HOWTO. Both ways can be fun!

In my homelab I have a Raspberry Pi 4 that runs infrastructure services. DNS is one of them, my private CA (Certificate Authority) another. The CA runs as a container on Podman. For DNS I use Bind. It thus has to serve 3 networks:


  192.168.1.0/24 My home IPv4 network
  172.16.0.0/16 IPv4 Network on the second ethernet ports of my homelab servers
  10.88.0.0/16 The (virtual) podman network


It uses my Fritz box (7490) as forwarder, so I can resolve all hosts, including the DHCP entries that the Fritz Box hands out under its default local domain name fritz.box. For my homelab however, I use the homelab.jhw domain name. Thatâ€™s what the Bind DNS server has to take care of.


  WARNING 
I really should use the official .internal TLD (Top Level Domain) for my homelab network, but I decided against it. This introduces the risk of name resolution problems, should someone offer a public .jhw TLD in future. Itâ€™s a risk I am willing to accept in exchange for using a 3 letter TLD at home. Donâ€™t be like me! Use .internal instead. With that out of the way, letâ€™s continue.


What we (well, I) have

Letâ€™s gather what I have in my home network.


  inf01.homelab.jhw at 192.168.1.10: A Raspberry Pi 4 4GB, running Fedora 42 and podman with my Certificate Authority as a container that should be reachable as ca.homelab.jhw. See Be the LetsEncrypt in your homelab with step-ca for more details.
  3 ThinkCentre Tiny PCs in the homelab.jhw zone, called hl01 (192.168.1.11), hl02 (192.168.1.12) and hl03 (192.168.1.13), running RHEL10 (Red Hat Enterprise Linux)
  A Fritz Box 7490 at 192.168.1.254


Letâ€™s install BIND on inf01

We need to do two things. Install BIND and some utilities on inf01 and open the firewall for DNS traffic.

dnf install bind bind-utils
firewall-cmd --add-service=dns --permanent


That was easy enough :)

Configure BIND

To run BIND in the correct way, we need to work on 4 configuration files.


  /etc/named.conf The main configuration file where we tell BIND on which networks it should listen and what zones it will serve.
  /var/named/forward.homelab.jhw The forward zone file that maps hostnames in the homelab.jhw domain to IP addresses on my home network
  /var/named/reverse.homelab.jhw The reverse zone for the 192.168.1.0/24 network range, that looks a bit confusing, that does the opposite. It maps IP addresses to hostnames.
  /var/named/reverse2.homelab.jhw The second reverse zone for the 172.16.0.0/16 network range.


Letâ€™s start with /etc/named.conf.

//
// named.conf
//

options {
  listen-on port 53 { 127.0.0.1; 192.168.1.10; 172.16.1.10; 10.88.0.1; };
  listen-on-v6 port 53 { ::1; fdda:a4da:69a5:0:2783:8c26:b2f1:a6f7; };
  allow-query     { localhost; 192.168.1.0/24; 172.16.0.0/16; 10.88.0.0/16; };

  directory       "/var/named";

  dump-file       "/var/named/data/cache_dump.db";
  statistics-file "/var/named/data/named_stats.txt";
  memstatistics-file "/var/named/data/named_mem_stats.txt";
  secroots-file   "/var/named/data/named.secroots";
  recursing-file  "/var/named/data/named.recursing";

  forwarders { 192.168.1.254; };
  recursion yes;

  dnssec-validation no;

  managed-keys-directory "/var/named/dynamic";
  geoip-directory "/usr/share/GeoIP";

  pid-file "/run/named/named.pid";
  session-keyfile "/run/named/session.key";

  /* https://fedoraproject.org/wiki/Changes/CryptoPolicy */
  include "/etc/crypto-policies/back-ends/bind.config";
};

logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        };
};

zone "." IN {
	type hint;
	file "named.ca";
};

zone "homelab.jhw" IN {
	type master;
	file "forward.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "1.168.192.in-addr.arpa" IN {
	type master;
	file "reverse.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "16.172.in-addr.arpa" IN {
        type master;
        file "reverse2.homelab.jhw";
        allow-update { none; };
        allow-query { any; };
};

include "/etc/named.rfc1912.zones";
include "/etc/named.root.key";


The first block declare the general options. Yes, it looks complicated and it is, but letâ€™s walk you through every relevant line (the lines not mentioned are default entries that donâ€™t need to be changed).

listen-on port 53 { 127.0.0.1; 192.168.1.10; 172.16.1.10; 10.88.0.1; };
listen-on-v6 port 53 { ::1; fdda:a4da:69a5:0:2783:8c26:b2f1:a6f7; };
allow-query     { localhost; 192.168.1.0/24; 172.16.0.0/16; 10.88.0.0/16; };


Here we tell BIND that it should listen for queries on port 53 on localhost, 192.168.1.10, the IPv4 address in my hoem network, 172.16.1.10, the second IPv4 address configured and 10.88.0.1, the virtual IPv4 address the Raspberry uses to bridge to the local podman containers.

The second line does the same for IPv6, but that is something we will discuss in Part 2.

The third line tells BIND from whom to accept queries. Essentially from everyone on the three IPv4 networks we are listening to.

directory       "/var/named";


This is the directory where BIND will look for its zone files, that we will define later.

forwarders { 192.168.1.254; };
recursion yes;


Now what if someone asks for a hostname that is outside of homelab.jhw? In that case we tell BIND to forward that question to 192.168.1.254, our Fritz Box. We will allow recursion and cache results we get from our Fritz box to avoid unneeded traffic.

dnssec-validation no;


Our simple setup will not bother with DNSSEC at the moment. Maybe we will have a Part 3 for that.

OK. That was the options part. We will ignore the logging part and the zone "." IN block.

Next (and finally) we define three zone entries (and zone files). A forward zone called homelab.jhw for our domain and two reverse zones for the IP addresses in the 192.168.1.0/24 range called 1.168.192.in-addr.arpa. Yep. Thatâ€™s 192.168.1 reversed. 1.168.192. Thatâ€™s why itâ€™s called the reverse zone ;) We also have 16.172.in-addr.arpa for the 172.16.0.0/16 range. Letâ€™s look at them.

zone "homelab.jhw" IN {
	type master;
	file "forward.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};


Itâ€™s a zone, all right. Itâ€™s the master for this zone, meaning that this DNS server will be the Source of Truth to  answer all queries for the homelab.jhw hostnames.

The exact mapping of all hostnames to IP addresses is in a file called forward.homelab.jhw in the directory /var/named. Remember how we defined that path at the beginning in the options part? Great! We also tell BIND that we do not allow dynamic updates for this zone, meaning that whatâ€™s in the file is all we will look at. Finally we tell BIND that any machine in the network is allowed to ask for a reply.

zone "1.168.192.in-addr.arpa" IN {
	type master;
	file "reverse.homelab.jhw";
	allow-update { none; };
	allow-query { any; };
};

zone "16.172.in-addr.arpa" IN {
        type master;
        file "reverse2.homelab.jhw";
        allow-update { none; };
        allow-query { any; };
};


The reverse zones with the weird looking zone names are almost the same, except that we define these in two files called reverse.homelab.jhw for the reverse lookup of the 192.168.1.0/24 range and reverse2.homelab.jhw for the 172.16.0.0/16 range. Why these zones have weird names will be explained later.

So now we go to the zone files!

Forward zone for homelab.jhw

The forward zone resolves names to IP addresses using A records (and other types like TXT, CAA and many more exist, but we wonâ€™t cover that in this post). It also contains CNAME entries, if you have services on one machine that should be reachable via more than one hostnames. In my homelab the CA (Certificate Authority) server is a container that runs on inf01.homelab.jhw, but should be reachable as ca.homelab.jhw in the home network. The CNAME entry does exactly that. It tells clients that when they want to talk to ca.homelab.jhw they can. By actually talking to inf01.homelab.jhw.

Now here is the big, important lessen for zone files. They have a serial number. Which MUST be incremented with every change. If you donâ€™t, weird things WILL happen. So:


  NEVER FORGET TO INCREASE THE SERIAL WITH EVERY CHANGE TO A ZONE FILE. OR RISK DNS HELL.


/var/named/forward.homelab.jhw

$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082706  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  A           192.168.1.10

inf01           IN  A     192.168.1.10
hl01            IN  A     192.168.1.11
hl02            IN  A     192.168.1.12
hl03            IN  A     192.168.1.13

ca              IN  CNAME inf01.homelab.jhw.

inf01-m         IN  A     172.16.1.10
hl01-m          IN  A     172.16.1.11
hl02-m          IN  A     172.16.1.12
hl03-m          IN  A     172.16.1.13


Again, letâ€™s go through this.

$TTL 3600


The default Time To Live (TTL) for DNS entries is set at 3600 seconds. Thatâ€™s 1 hour. This means that when a machine in the network gets a DNS reply, it will not ask again for the same thing until the TTL has passed.

@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082706  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)


The Start Of Authority (SOA) block. Here we say which DNS server is the owner of this domain. Itâ€™s inf01.homelab.jhw. (yes, that dot at the end is REALLY important). The root.homelab.jhw actually means root@homelab.jhw and is the email address responsible for this domain. Donâ€™t think to much about why and what :)

@       IN  NS          inf01.homelab.jhw.
@       IN  A           192.168.1.10


The first â€œrealâ€ DNS entries! They are special, as the @ indicates, which means they represent the domain itself. We first define the nameserver (again? yes, don*â€˜t ask) as NS record. And right after that we define the A record as the IP address 192.168.1.10.

Did you notice that . at the end of inf01.homelab.jhw.? Thatâ€™s another VERY important thing. The TL;DR is that this final . tells DNS to stop doing fancy recursion and lookups. Just look for the hostname `inf01.homelab.jhw. Period. (pun intended). Donâ€™t care too much about this. Just remember:

EVERY HOSTNAME RECORD ENDS WITH A . YOU WILL FORGET THIS. YOU WILL FIX THIS.

inf01           IN  A     192.168.1.10
hl01            IN  A     192.168.1.11
hl02            IN  A     192.168.1.12
hl03            IN  A     192.168.1.13


Here come the A records for 192.168.1.0/24! We finally get to map hostnames to IP addresses. For real! It now is quite self-explanatory, isnâ€™t it? The hostname gets an A record that is the IP address in my local network. And as these are IP addresses, no . is needed at the end.

ca              IN  CNAME inf01.homelab.jhw.


And here is the CNAME record. Which maps the hostname ca.homelab.jhw to the Canonical NAME (CNAME) inf01.homelab.jhw.. This is a hostname at the end! So it needs the . Period :)

inf01-m         IN  A     172.16.1.10
hl01-m          IN  A     172.16.1.11
hl02-m          IN  A     172.16.1.12
hl03-m          IN  A     172.16.1.13


And here we create another set of A records for the same machines, but this time in the 172.16.0.0/16 range. This range is used for management stuff, hence the -m.

And thatâ€™s the gist of it. If you add a new machine to your network, configure it with an IP address (statically or with DHCP) and add it as an A record to the forward zone. Increment the serial and tell DNS to read the updated zone with systemctl reload named. Done.

Reverse zones for 192.168.1.0/24 and 172.16.0.0/16

The reverse zone maps IP addresses to hostnames. Often called the PTR or pointer record. You have to make sure that the entries here are synced to the forward zone.


  NEVER FORGET TO INCREASE THE SERIAL WITH EVERY CHANGE TO A ZONE FILE. Or risk DNS hell.


Here is the reverse zone for the 192.168.1.0/24 range.

/var/named/reverse.homelab.jhw

$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082601  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  PTR         homelab.jhw.
10      IN  PTR         inf01.homelab.jhw.
11      IN  PTR         hl01.homelab.jhw.
12      IN  PTR         hl02.homelab.jhw.
13      IN  PTR         hl03.homelab.jhw.


As this is more or less the same but the other way round, I will not go through everything but instead explain the differences. Itâ€™s the reverse zone, so now we have PTR (pointer) entries that map an IPv4 address in the 192.168.1.0/24 range to hostnames. WITH A DOT AT THE END. DO NOT FORGET THE DOT!

As this is a /24 block, we only need to set the last digit of the IPv4 address.

You might wonder, where is ca here? Well, itâ€™s CNAME is info1.homelab.jhw and that already is in this reverse zone. That is good enough. No separate entry needed.

We also need the reverse zone for the 172.16.0.0/16 range:

/var/named/reverse2.homelab.jhw

$TTL 3600
@   IN  SOA     inf01.homelab.jhw. root.homelab.jhw. (
        2025082901  ;Serial
        3600        ;Refresh
        1800        ;Retry
        604800      ;Expire
        86400       ;Minimum TTL
)
@       IN  NS          inf01.homelab.jhw.
@       IN  PTR         homelab.jhw.
10.1      IN  PTR         inf01-m.homelab.jhw.
11.1      IN  PTR         hl01-m.homelab.jhw.
12.1      IN  PTR         hl02-m.homelab.jhw.
13.1      IN  PTR         hl03-m.homelab.jhw.


Looks deceivingly similar. But there is a big difference. This is a /16 network, so we have to define the last two parts of the IPv4 address. And as it is a reverse zone file, yep, we have to reverse it. So now we need 10.1 to define the entry for 172.16.1.10, which is the hostname inf01-m.homelab.jhw. WITH THE DOT AT THE END. AND DID YOU UPDATE THE SERIAL? :)

Phew. Thatâ€™s the config done!

A final check with the named-checkconf command, which should say nothing when all files are OK. If not, it will tell you what is wrong so you get the chance to fix stuff. You did add all the . at the end of hostnames and you did update the serial of that zone file after you made changes, yes?

Start Bind

The only thing remaining is to start BIND. And persist it as a service, so it starts after every boot. Itâ€™s DNS. It must always be available.

systemctl enable named
systemctl start named


You most likely will make typos in your config. So do check with named-checkconf  and systemctl status named and journalctl -u named. If something breaks, read this whole entry again. Find that missing . in a zone file. Increment the serial that you forgot to do. You will get there. Donâ€™t give up!

Result

Machines, containers etc can now be resolved in my home network. All with mow own DNS! Yay!

% nslookup jhwfritz.fritz.box
Server:		192.168.1.10
Address:	192.168.1.10#53

Non-authoritative answer:
Name:	jhwfritz.fritz.box
Address: 192.168.1.254

% nslookup ca.homelab.jhw    
Server:		192.168.1.10
Address:	192.168.1.10#53

ca.homelab.jhw	canonical name = inf01.homelab.jhw.
Name:	inf01.homelab.jhw
Address: 192.168.1.10


And now you should be able to ping the machines with their hostname. ssh into them. Get certificates with the CA that runs in the podman container. Life is good!

I hope you enjoyed this post and could learn something new! Feel free to comment or send corrections vie the Toot linked below that collects the comments!
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making a font of my handwriting]]></title>
            <link>https://chameth.com/making-a-font-of-my-handwriting/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45141636</guid>
            <description><![CDATA[Recently Iâ€™ve been on a small campaign to try to make my personal website moreâ€¦ personal. Little ways to
            make it obvious itâ€™s mine and personal, not just another piece of the boring corporate
            dystopia that is most of the web these days. I donâ€™t quite want to fully regress to the Geocities era and
            fill the screen with animated under construction GIFs, but I do want to capture some of that vibe.]]></description>
            <content:encoded><![CDATA[
          
            Recently Iâ€™ve been on a small campaign to try to make my personal website moreâ€¦ personal. Little ways to
            make it obvious itâ€™s mine and personal, not just another piece of the boring corporate
            dystopia that is most of the web these days. I donâ€™t quite want to fully regress to the Geocities era and
            fill the screen with animated under construction GIFs, but I do want to capture some of that vibe.
          
          
            Iâ€™d added some bits and pieces along those lines: floating images in articles now look like theyâ€™re stuck to
            the page with sellotape, related post links have a wavy border that animates when you hover over them, and
            so on. Next, I wanted to change the heading fonts from a monospace font to something cursive, to resemble
            handwriting. Less terminal output, more handwritten letter. I couldnâ€™t find one I liked, though. So why not
            make my own? It canâ€™t be that hard, right?
          
          Failing to do it myself
          
            I set out to try to make the font myself using open source tools. After doing a bit of research, it seemed
            like the general approach was to create vectors of each character and then import them into a font editor.
            That seems to mean either Adobe Illustrator and FontLab (if you have too much money) or Inkscape and
            FontForge (if you like open source). I fall firmly into the latter category, so I grabbed my graphics tablet
            and opened Inkscape.
          
          
            I wrote out my first three letters: capital A, B and C. Saved them in Inkscape, and attempted to import them
            into FontForge. Then I remembered one crucial thing that had slipped my mind: I absolutely loathe using
            FontForge. Itâ€™s a bit like when you open an old version of GIMP and get a bunch of weird looking windows
            floating all over the place; it feels like youâ€™re fighting against the tool to do even the most basic
            operations. The difference is I have cause to edit images a lot more than I edit fonts, and GIMP
            has actually significantly improved their UI over the years.
          
          Here are the rough steps I went through with FontForge:
          
            Launch Font Forge. It shows a weird bit of art in one window, and an open file dialog in another.
            I donâ€™t want to open a file, so I close that dialog. The program exits.
            Relaunch Font Forge, and realise that within the â€œOpen Fontâ€ dialog is a â€œNewâ€ button. Click it.
            
              Get to the standard font-editing UI. Right-click on the â€œAâ€ looking for a way to import an SVG. Donâ€™t see
              one.
            
            
              Click around a bit, exploring the menus. Everything feels a bit off. You canâ€™t open one menu then hover
              over the next to see its content, like basically every UI toolkit in existence. I think FontForge has
              eschewed QT and GTK in favour of doing things itself.
            
            Find the â€œImportâ€ option in the File menu. Hope itâ€™s for a single glyph not the whole font.
            
              A file picker opens. Again itâ€™s all a bit off from normal desktop conventions. Try to resize it, and just
              get blank gray space at the bottom.
            
            Type the absolute path I want to go to in the text field.
            Get a dialog saying â€œNot a bdf file /home/chris/etcâ€. Press OK.
            Get a dialog saying â€œCould not find a bitmap font inâ€. Press OK.
            
              Press Ctrl+L to see if that lets me enter a path. Click everything in the dialog to try to find a way to
              enter a path. Get annoyed. Give up. Click through folder-by-folder to get to where I want to be.
            
            
              Get to the folder and donâ€™t see any files. Change the format to â€œSVGâ€. Double-click the newly-visible SVG
              file.
            
            Get a dialog saying â€œYou must select a glyph before you can import an image into itâ€. Press OK.
            The import dialog goes away, having not imported.
            Select the glyph in the main tool area, then repeat the Fileâ†’Import dance.
            
              Itâ€™s actually there now! Open the glyph in the editor and see itâ€™s a complete mess of BÃ©zier curves. I
              canâ€™t click what I want without accidentally moving a handle for an adjacent curve.
            
            Rage-quit.
          
          
            Iâ€™m sure FontForge is less anger inducing once youâ€™re used to it. And you definitely could use it to build a
            font like this if you had much more patience than me. Iâ€™d had enough of death-by-a-thousand-paper-cuts
            though.
          
          
            I briefly tried Inkscapeâ€™s built-in support for making an SVG font. It annoyed me a lot less, but itâ€™s
            fiddly: it seemed like each font had to be a single path, so you had to convert the glyphs to paths, then
            merge them correctly. If you merge them incorrectly then the wrong bits of your letters end up filled (like
            the inside of the â€˜Bâ€™). Path manipulation is getting towards the limit of my knowledge of vector editing,
            and it took a bit of trial and error for each letter that had more than a single stroke. I didnâ€™t fancy
            doing that for every letter.
          
          
            Iâ€™m usually a big advocate of open source, but this was one of those painful times where it feels like it
            just falls short. Clunky, painful UI and processes where commercial tools just let you get on with your
            work.
          
          You can exchange money for goods and services
          
            When Iâ€™d been looking for open source tutorials, I found many mentions of a closed source, hosted tool:
            Calligraphr. It has a free version with limitations (no
            ligatures, no variations, 75 glyphs per font), and a pro version for Â£8/month. Iâ€™d normally balk at the idea
            of a subscription for this, but they have the perfect answer: you can make a one-time payment, and your
            account automatically downgrades back to free after a month. Itâ€™s not a hidden option, either, itâ€™s the most
            prominent button on the upgrade page. That made me happy to give them Â£8 to play around with the service for
            a month.
          
          
            Calligraphr works by having you print templates, write out the letters, then scan them in. It does some
            magical processing to extract the glyphs, provides tools to tidy them up, align them, etc, and then produces
            a TTF file for you. You can see some of my completed templates here:
          
          
            
              
              
              
            
            Most of the templates I used for the font
          
          
            Calligraphr has a nice UI to generate the templates, allowing you to select which glyphs to include. I added
            the â€œminimal Englishâ€, â€œbasic punctuationâ€ and â€œLigaturesâ€ sets. That gave me four pages to fill out, and I
            did them all twice. That let me filter out versions that didnâ€™t work well, and have variants for some
            letters so the font wasnâ€™t too repetitive. Later on, I went back and added some custom ligatures based on
            blog post titles that didnâ€™t look quite right: â€œReâ€, â€œToâ€, â€œersâ€, â€œeyâ€, â€œhyâ€, â€œraâ€, â€œreâ€ and â€œtyâ€. Ligatures
            like this help it look more natural: when we write we donâ€™t just stamp out identical letters regardless of
            their surroundings, instead they will connect to their neighbours, or overlap slightly, or even share a
            stroke.
          
          
            I filled these templates in with a Sharpie, as I wanted a fairly informal, scrap-booky look, and it would
            also give good solid shapes that should be easy to pick out of the template. I scanned them with the â€œScan
            Documentâ€ function on my iPhone, and uploaded the PDFs to Calligraphr.
          
          Iterating and tweaking
          
            The Calligraphr UI allows you to preview the font, but I found it a lot more useful to just download a copy
            and use it on a local copy of my website. That let me test it with real text, and see how itâ€™d look at the
            different font sizes I use on the site.
          
          
            The first version was not great. Despite the guidelines on the template, I apparently wasnâ€™t good at
            sticking to them. Some letters were floating way off the baseline, and some were sunken below. When those
            opposites met it looked terrible. Fortunately Calligraphr has a pretty easy tool to slide each letter up and
            down, and scale it up or down if needed, and you can see it next to other letters as you do it. It took a
            little bit of time to go through all the variants of all the letters, but the next version looked a lot
            better.
          
          
            Another tweak I ended up doing was reducing the spacing between letters. The defaults Calligraphr uses are
            probably good for a blocky font, but I wanted to put the letters close together to give it more of a
            joined-up look. Again, this is an easy tool to use, you just drag the sides in or out as desired. While
            these tweaking steps were probably as fiddly as some of the Inkscape steps I refused to do earlier, theyâ€™re
            a lot more rewarding as you see things improving with each one. Itâ€™s a lot easier for me to commit time and
            effort to improving something thatâ€™s already working reasonably, than put that time and energy into an
            unknown.
          
          
            Later, I noticed that occasionally there would be a huge gap in a title. Not â€œthe kerning is slightly offâ€
            but â€œthereâ€™s enough room to park a busâ€. It took me a while to figure out what was happening: a couple of
            glyphs hadnâ€™t been isolated perfectly and had picked up a few pixels from the template lines at the edge of
            their boxes. That meant the glyph had a width that covered the actual written glyph, a big gap, and then the
            rogue marks. At first, I fixed this by just adjusting the width, but that left the little pixels floating
            awkwardly down-sentence. The proper fix was to use the editing tool and simply delete them, and then
            Calligraphr snapped the width back to what it should be.
          
          
            These iterations took a while to do, but I just dipped in and out occasionally over the course of a week, so
            it didnâ€™t actually feel like too much work. I quite enjoy the process of refining things, too.
          
          Result and a surprise
          
            If youâ€™re viewing this post on my website[1], you can see the font in the headers, captions, and a few other places. Hereâ€™s how it compares to my
            actual handwriting:
          
          
            
              
              
              
            
            My handwriting vs my handwriting font
          
          
            Itâ€™s not close enough to forge documents, but I think it definitely gets across my style, and thatâ€™s exactly
            what I wanted. Itâ€™s surprisingly legible even at smaller font sizes â€” I think the weight of the Sharpie
            helps here â€” and at Â£8 and a bit of manual work was a lot more economical than spending days wresting with
            open source tools.
          
          
            A few weeks after I put the finishing touches on the font, I got an e-mail from Calligraphr. As my account
            had lapsed back to the free version, I was no longer eligible for the â€œserver-side backupâ€ feature. So what
            did they do? They e-mailed me an exported copy! Itâ€™s a JSON file with the properties of each glyph and a
            base64 encoded image. Not only can I re-upload this to Calligraphr if I resubscribe, I can probably hook
            something up to edit it should I ever need to. Iâ€™m blown away by how pro-user Calligraphrâ€™s business
            practices are. Theyâ€™re up-front about pricing, donâ€™t try and get you stuck on an auto-renewing subscription,
            and automatically export your data. Itâ€™s like a breath of fresh air compared to the barrage of dark patterns
            that other websites foist on us. If you want to make this kind of font, Iâ€™d definitely recommend them just
            because of how nice they are.
          
          
          
            
              
                
                  And I havenâ€™t changed everything since writing this postâ€¦
                  â†©ï¸
                
              
            
          
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MentraOS â€“ open-source Smart glasses OS]]></title>
            <link>https://github.com/Mentra-Community/MentraOS</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45140381</guid>
            <description><![CDATA[Smart glasses OS, with dozens of built-in apps. Users get AI assistant, notifications, translation, screen mirror, captions, and more. Devs get to write 1 app that runs on any pair of smart glases....]]></description>
            <content:encoded><![CDATA[

    
  
  
    
  

Supported Smart Glasses
Works with Even Realities G1, Mentra Mach 1, Mentra Live. See smart glasses compatibility list here.
Apps on Mentra Store
The Mentra Store already has a ton of useful apps that real users are running everyday. Here are some apps already published by developers on the Mentra Store:

Write Once, Run on Any Smart Glasses
MentraOS is how developers build smart glasses apps. We handle the pairing, connection, data streaming, and cross-compatibility, so you can focus on creating amazing apps. Every component is 100% open source (MIT license).
Why Build with MentraOS?

Cross Compatibility: Your app runs on any pair of smart glasses
Speed: TypeScript SDK means you're making apps in minutes, not months
Control: Access smart glasses I/O - displays, microphones, cameras, speakers
Distribution: Get your app in front of everyone using smart glasses

MentraOS Community
The MentraOS Community is a group of developers, companies, and users dedicated to ensuring the next personal computer is open, cross-compatible, and user-controlled. That's why we're building MentraOS.
To get involved, join the MentraOS Community Discord server.
Contact
Have questions or ideas? We'd love to hear from you!

Email: team@mentra.glass
Discord: Join our community
Twitter: Follow @mentralabs

Contributing
MentraOS is made by a community and we welcome PRs. Here's the Contributors Guide: docs.mentra.glass/contributing
License
MIT License Copyright 2025 MentraOS Community


  
  Â© 2025 Mentra Labs

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Protobuffers Are Wrong (2018)]]></title>
            <link>https://reasonablypolymorphic.com/blog/protos-are-wrong/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139656</guid>
            <description><![CDATA[Iâ€™ve spent a good deal of my professional life arguing against using protobuffers. Theyâ€™re clearly written by amateurs, unbelievably ad-hoc, mired in gotchas, tricky to compile, and solve a problem that nobody but Google really has. If these problems of protobuffers remained quarantined in serialization abstractions, my complaints would end there. But unfortunately, the bad design of protobuffers is so persuasive that these problems manage to leak their way into your code as well.]]></description>
            <content:encoded><![CDATA[
    Iâ€™ve spent a good deal of my professional life arguing against using protobuffers. Theyâ€™re clearly written by amateurs, unbelievably ad-hoc, mired in gotchas, tricky to compile, and solve a problem that nobody but Google really has. If these problems of protobuffers remained quarantined in serialization abstractions, my complaints would end there. But unfortunately, the bad design of protobuffers is so persuasive that these problems manage to leak their way into your code as well.
Ad-Hoc and Built By Amateurs
Stop. Put away your email client that is half-way through writing me about how â€œGoogle is filled with the worldâ€™s best engineers,â€ and that â€œanything they build is, by definition, not built by amateurs.â€ I donâ€™t want to hear it.
Letâ€™s just get this out of the way. Full disclosure: I used to work at Google. It was the first (but unfortunately, not the last) place I ever used protobuffers. All of the problems I want to talk about today exist inside of Googleâ€™s codebase; itâ€™s not just a matter of â€œusing protobuffers wrongâ€ or some such nonsense like that.
By far, the biggest problem with protobuffers is their terrible type-system. Fans of Java should feel right at home with protobuffers, but unfortunately, literally nobody considers Java to have a well-designed type-system. The dynamic typing guys complain about it being too stifling, while the static typing guys like me complain about it being too stifling without giving you any of the things you actually want in a type-system. Lose lose.
The ad-hoc-ness and the built-by-amateurs-itude go hand-in-hand. So much of the protobuffer spec feels bolted on as an afterthought that it clearly was bolted on as an afterthought. Many of its restrictions will make you stop, scratch your head and ask â€œwat?â€ But these are just symptoms of the deeper answer, which is this:
Protobuffers were obviously built by amateurs because they offer bad solutions to widely-known and already-solved problems.
No Compositionality
Protobuffers offer several â€œfeaturesâ€, but none of them see to work with one another. For example, look at the list of orthogonal-yet-constrained typing features that I found by skimming the documentation.

oneof fields canâ€™t be repeated.
map<k,v> fields have dedicated syntax for their keys and values, but this isnâ€™t used for any other types.
Despite map fields being able to be parameterized, no user-defined types can be. This means youâ€™ll be stuck hand-rolling your own specializations of common data structures.
map fields cannot be repeated.
map keys can be strings, but can not be bytes. They also canâ€™t be enums, even though enums are considered to be equivalent to integers everywhere else in the protobuffer spec.
map values cannot be other maps.

This insane list of restrictions is the result of unprincipled design choices and bolting on features after the fact. For example, oneof fields canâ€™t be repeated because rather than resulting in a coproduct type, instead the code generator will give you a product of mutually-exclusive optional fields. Such a transformation is only valid for a singular field (and, as weâ€™ll see later, not even then.)
The restriction behind map fields being unable to be repeated is related, but shows off a different limitation of the type-system. Behind the scenes, a map<k,v> is desugared into something spiritually similar to repeated Pair<k,v>. And because repeated is a magical language keyword rather than a type in its own right, it doesnâ€™t compose with itself.
Your guess is as good as mine for why an enum canâ€™t be used as a map key.
Whatâ€™s so frustrating about all of this is a little understanding of how modern type-systems work would be enough to drastically simplify the protobuffer spec and simultaneously remove all of the arbitrary restrictions.
The solution is as follows:

Make all fields in a message required. This makes messages product types.
Promote oneof fields to instead be standalone data types. These are coproduct types.
Give the ability to parameterize product and coproduct types by other types.

Thatâ€™s it! These three features are all you need in order to define any possible piece of data. With these simpler pieces, we can re-implement the rest of the protobuffer spec in terms of them.
For example, we can rebuild optional fields:
product Unit {
  // no fields
}

coproduct Optional<t> {
  t    value = 0;
  Unit unset = 1;
}
Building repeated fields is simple too:
coproduct List<t> {
  Unit empty = 0;
  Pair<t, List<t>> cons = 1;
}
Of course, the actual serialization logic is allowed to do something smarter than pushing linked-lists across the networkâ€”after all, implementations and semantics donâ€™t need to align one-to-one.
Questionable Choices
In the vein of Java, protobuffers make the distinction between scalar types and message types. Scalars correspond more-or-less to machine primitivesâ€”things like int32, bool and string. Messages, on the other hand, are everything else. All library- and user-defined types are messages.
The two varieties of types have completely different semantics, of course.
Fields with scalar types are always present. Even if you donâ€™t set them. Did I mention that (at least in proto31) all protobuffers can be zero-initialized with absolutely no data in them? Scalar fields get false-y valuesâ€”uint32 is initialized to 0 for example, and string is initialized as "".
Itâ€™s impossible to differentiate a field that was missing in a protobuffer from one that was assigned to the default value. Presumably this decision is in place in order to allow for an optimization of not needing to send default scalar values over the wire. Presumably, though the encoding guide makes no mention of this optimization being performed, so your guess is as good as mine.
As weâ€™ll see when we discuss protobuffersâ€™ claim to being godâ€™s gift to backwards- and forwards-compatible APIs, this inability to distinguish between unset and default values is a nightmare. Especially if indeed itâ€™s a design decision made in order to save one bit (set or not) per field.
Contrast this behavior against message types. While scalar fields are dumb, the behavior for message fields is outright insane. Internally, message fields are either there or theyâ€™re notâ€”but their behavior is crazy. Some pseudocode for their accessor is worth a thousand words. Pretend this is Java or something similar:
private Foo m_foo;

public Foo foo {
  // only if `foo` is used as an expression
  get {
    if (m_foo != null)
      return m_foo;
    else
      return new Foo();
  }

  // instead if `foo` is used as an lvalue
  mutable get {
    if (m_foo = null)
      m_foo = new Foo();
    return m_foo;
  }
}
The idea is that if the foo field is unset, youâ€™ll see a default-initialized copy whenever you ask for it, but wonâ€™t actually modify its container. But if you modify foo, it will modify its parent as well! All of this just to avoid using a Maybe Foo type and the associated â€œheadachesâ€ of the nuance behind needing to figure out what an unset value should mean.
This behavior is especially egregious, because it breaks a law! Weâ€™d expect the assignment msg.foo = msg.foo; to be a no-op. Instead the implementation will actually silently change msg to have a zero-initialized copy of foo if it previously didnâ€™t have one.
Unlike scalar fields, at least itâ€™s possible to detect if a message field is unset. Language bindings for protobuffers offer something along the lines of a generated bool has_foo() method. In the frequent case of copying a message field from one proto to another, iff it was present, youâ€™ll need to write the following code:
if (src.has_foo(src)) {
  dst.set_foo(src.foo());
}
Notice that, at least in statically-typed languages, this pattern cannot be abstracted due to the nominal relationship between the methods foo(), set_foo() and has_foo(). Because all of these functions are their own identifiers, we have no means of programmatically generating them, save for a preprocessor macro:
#define COPY_IFF_SET(src, dst, field) \
if (src.has_##field(src)) { \
  dst.set_##field(src.field()); \
}
(but preprocessor macros are verboten by the Google style guide.)
If instead all optional fields were implemented as Maybes, youâ€™d get abstract-able, referentially transparent call-sites for free.
To change tack, letâ€™s talk about another questionable decision. While you can define oneof fields in protobuffers, their semantics are not of coproduct types! Rookie mistake my dudes! What you get instead is an optional field for each case of the oneof, and magic code in the setters that will just unset any other case if this one is set.
At first glance, this seems like it should be semantically equivalent to having a proper union type. But instead it is an accursed, unutterable source of bugs! When this behavior teams up with the law-breaking implementation of msg.foo = msg.foo;, it allows this benign-looking assignment to silently delete arbitrary amounts of data!
What this means at the end of the day is that oneof fields do not form law-abiding Prisms, nor do messages form law-abiding Lenses. Which is to say good luck trying to write bug-free, non-trivial manipulations of protobuffers. It is literally impossible to write generic, bug-free, polymorphic code over protobuffers.
Thatâ€™s not the sort of thing anybody likes to hear, let alone those of us who have grown to love parametric polymorphismâ€”which gives us the exact opposite promise.
The Lie of Backwards- and Forwards-Compatibility
One of the frequently cited killer features of protobuffers is their â€œhassle-free ability to write backwards- and forwards-compatible APIs.â€ This is the claim that has been pulled over your eyes to blind you from the truth.
What protobuffers are is permissive. They manage to not shit the bed when receiving messages from the past or from the future because they make absolutely no promises about what your data will look like. Everything is optional! But if you need it anyway, protobuffers will happily cook up and serve you something that typechecks, regardless of whether or not itâ€™s meaningful.
This means that protobuffers achieve their promised time-traveling compatibility guarantees by silently doing the wrong thing by default. Of course, the cautious programmer can (and should) write code that performs sanity checks on received protobuffers. But if at every use-site you need to write defensive checks ensuring your data is sane, maybe that just means your deserialization step was too permissive. All youâ€™ve managed to do is decentralize sanity-checking logic from a well-defined boundary and push the responsibility of doing it throughout your entire codebase.
One possible argument here is that protobuffers will hold onto any information present in a message that they donâ€™t understand. In principle this means that itâ€™s nondestructive to route a message through an intermediary that doesnâ€™t understand this version of its schema. Surely thatâ€™s a win, isnâ€™t it?
Granted, on paper itâ€™s a cool feature. But Iâ€™ve never once seen an application that will actually preserve that property. With the one exception of routing software, nothing wants to inspect only some bits of a message and then forward it on unchanged. The vast majority of programs that operate on protobuffers will decode one, transform it into another, and send it somewhere else. Alas, these transformations are bespoke and coded by hand. And hand-coded transformations from one protobuffer to another donâ€™t preserve unknown fields between the two, because itâ€™s literally meaningless.
This pervasive attitude towards protobuffers always being compatible rears its head in other ugly ways. Style guides for protobuffers actively advocate against DRY and suggest inlining definitions whenever possible. The reasoning behind this is that it allows you to evolve messages separately if these definitions diverge in the future. To emphasize that point, the suggestion is to fly in the face of 60 yearsâ€™ worth of good programming practice just in case maybe one day in the future you need to change something.
At the root of the problem is that Google conflates the meaning of data with its physical representation. When youâ€™re at Google scale, this sort of thing probably makes sense. After all, they have an internal tool that allows you to compare the finances behind programmer hours vs network utilization vs the cost to store \(x\) bytes vs all sorts of other things. Unlike most companies in the tech space, paying engineers is one of Googleâ€™s smallest expenses. Financially it makes sense for them to waste programmersâ€™ time in order to shave off a few bytes.
Outside of the top five tech companies, none of us is within five orders of magnitude of being Google scale. Your startup cannot afford to waste engineer hours on shaving off bytes. But shaving off bytes and wasting programmersâ€™ time in the process is exactly what protobuffers are optimized for.
Letâ€™s face it. You are not Google scale and you never will be. Stop cargo-culting technology just because â€œGoogle uses itâ€ and therefore â€œitâ€™s an industry best-practice.â€
Protobuffers Contaminate Codebases
If it were possible to restrict protobuffer usage to network-boundaries I wouldnâ€™t be nearly as hard on it as a technology. Unfortunately, while there are a few solutions in principle, none of them is good enough to actually be used in real software.
Protobuffers correspond to the data you want to send over the wire, which is often related but not identical to the actual data the application would like to work with. This puts us in the uncomfortable position of needing to choose between one of three bad alternatives:

Maintain a separate type that describes the data you actually want, and ensure that the two evolve simultaneously.
Pack rich data into the wire format for application use.
Derive rich information every time you need it from a terse wire format.

Option 1 is clearly the â€œrightâ€ solution, but its untenable with protobuffers. The language isnâ€™t powerful enough to encode types that can perform double-duty as both wire and application formats. Which means youâ€™d need to write a completely separate datatype, evolve it synchronously with the protobuffer, and explicitly write serialization code between the two. Seeing as most people seem to use protobuffers in order to not write serialization code, this is obviously never going to happen.
Instead, code that uses protobuffers allows them to proliferate throughout the codebase. True story, my main project at Google was a compiler that took â€œprogramsâ€ written in one variety of protobuffer, and spit out an equivalent â€œprogramâ€ in another. Both the input and output formats were expressive enough that maintaining proper parallel C++ versions of them could never possibly work. As a result, my code was unable to take advantage of any of the rich techniques weâ€™ve discovered for writing compilers, because protobuffer data (and resulting code-gen) is simply too rigid to do anything interesting.
The result is that a thing that could have been 50 lines of recursion schemes was instead 10,000 lines of ad-hoc buffer-shuffling. The code I wanted to write was literally impossible when constrained by having protobuffers in the mix.
While this is an anecdote, itâ€™s not in isolation. By virtue of their rigid code-generation, manifestations of protobuffers in languages are never idiomatic, nor can they be made to beâ€”short of rewriting the code-generator.
But even then, you still have the problem of needing to embed a shitty type-system into the targeted language. Because most of protobuffersâ€™ features are ill-conceived, these unsavory properties leak into our codebases. It means weâ€™re forced to not only implement, but also use these bad ideas in any project which hopes to interface with protobuffers.
While itâ€™s easy to implement inane things out of a solid foundation, going the other direction is challenging at best and the dark path of Eldrich madness at worst.
In short, abandon all hope ye who introduce protobuffers into your projects.



To this day, thereâ€™s a raging debate inside Google itself about proto2 and whether fields should ever be marked as required. Manifestos with both titles â€œoptional considered harmfulâ€ and â€œrequired considered harmful.â€ Good luck sorting that out.â†©ï¸




    
        â†
    
    
        â†’
    


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Purposeful animations]]></title>
            <link>https://emilkowal.ski/ui/you-dont-need-animations</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139088</guid>
            <description><![CDATA[Why you are animating more often than you should.]]></description>
            <content:encoded><![CDATA[When done right, animations make an interface feel predictable, faster, and more enjoyable to use. They help you and your product stand out.
But they can also do the opposite. They can make an interface feel unpredictable, slow, and annoying. They can even make your users lose trust in your product.
So how do you know when and how to animate to improve the experience?
Step one is making sure your animations have a purpose.
Purposeful animations
Before you start animating, ask yourself: whatâ€™s the purpose of this animation? As an example, whatâ€™s the purpose of this marketing animation we built at Linear?

This animation explains how Product Intelligence (Linearâ€™s feature) works. We could have used a static asset, but the animated version helps the user understand what this feature does, straight in the initial viewport of the page.
Another purposeful animation is this subtle scale down effect when pressing a button. Itâ€™s a small thing, but it helps the interface feel more alive and responsive.

Sonnerâ€™s enter animation, on the other hand, has two purposes:

- Having a toast suddenly appear would feel off, so we animate it in.
- Because it comes from and leaves in the same direction, it creates spatial consistency, making the swipe-down-to-dismiss gesture feel more intuitive.


But sometimes the purpose of an animation might just be to bring delight.
Morphing of the feedback component below helps make the experience more unique and memorable. This works as long as the user will rarely interact with it. Itâ€™ll then become a pleasant surprise, rather than a daily annoyance.
Press on the button to see it morph.
Used multiple times a day, this component would quickly become irritating. The initial delight would fade and the animation would slow users down.
How often users will see an animation is a key factor in deciding whether to animate or not. Letâ€™s dive deeper into it next.
Frequency of use
I use Raycast hundreds of times a day. If it animated every time I opened it, it would be very annoying. But thereâ€™s no animation at all. Thatâ€™s the optimal experience.
To see it for yourself, try to toggle the open state of the menu below by using the buttons belowpressing J and then K. Which one feels better if used hundreds of times a day?
Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommand
When I open Raycast, I have a clear goal in mind. I donâ€™t expect to be â€œdelightedâ€, I donâ€™t need to be. I just want to do my work with no unnecessary friction.
Think about what the user wants to achieve and how often they will see an animation. A hover effect is nice, but if used multiple times a day, it would likely benefit the most from having no animation at all.
Imagine you interact with this list often during the day.
Imagine you interact with this list often during the day.The same goes for keyboard-initiated actions. These actions may be repeated hundreds of times a day, an animation would make them feel slow, delayed, and disconnected from the userâ€™s actions. You should never animate them.
Since we canâ€™t really use a keyboard on touch devices, you can press the buttons below to see how it feels with and without animation.
To see it for yourself, focus on the input below and use arrow keys to navigate through the list. Notice how the highlight feels delayed compared to the keys you press. Now press  (shift) and see how this interaction feels without animation.Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommandPress shift to toggle the animation
But even if your animation wonâ€™t be used too often and it fulfills a clear purpose, you still have to think about its speedâ€¦
Perception of speed
Unless you are working on marketing sites, your animations have to be fast. They improve the perceived performance of your app, stay connected to userâ€™s actions, and make the interface feel as if itâ€™s truly listening to the user.
To give you an example, a faster-spinning spinner makes the app seem to load faster, even though the load time is the same. This improves perceived performance.
Which one works harder to load the data?
A 180ms dropdown animation feels more responsive than a 400ms one:
Click on the buttons to compare the speed.
As a rule of thumb, UI animations should generally stay under 300ms.
Another example of the importance of speed: tooltips should have a slight delay before appearing to prevent accidental activation. Once a tooltip is open however, hovering over other tooltips should open them with no delay and no animation.
This feels faster without defeating the purpose of the initial delay.
Radix UI and Base UI skip the delay once a tooltip is shown.
Radix UI and Base UI skip the delay once a tooltip is shown.Building great interfaces
The goal is not to animate for animationâ€™s sake, itâ€™s to build great user interfaces. The ones that users will happily use, even on a daily basis. Sometimes this requires animations, but sometimes the best animation is no animation.
Knowing when to animate is just one of many things you need to know in order to craft great animations. If youâ€™d like to dive deeper into the theory and practice of it, Iâ€™ve created a course that covers everything you need to know:
Check out "Animations on the Web"]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I ditched Docker for Podman]]></title>
            <link>https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137525</guid>
        </item>
        <item>
            <title><![CDATA[ML needs a new programming language â€“ Interview with Chris Lattner]]></title>
            <link>https://signalsandthreads.com/why-ml-needs-a-new-programming-language/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137373</guid>
            <description><![CDATA[Listen in on Jane Streetâ€™s Ron Minsky as he has conversations with engineers working on everything from clock synchronization to reliable multicast, build systems to reconfigurable hardware. Get a peek at how Jane Street approaches problems, and how those ideas relate to tech more broadly.]]></description>
            <content:encoded><![CDATA[
		Why ML Needs a New Programming Language
		with Chris Lattner
		
			
				Season 3, Episode 10 Â Â |Â Â 
			
			September 3rd, 2025
		
		
	
		BLURB

Chris Lattner is the creator of LLVM and led the development of the Swift language at
Apple. With Mojo, heâ€™s taking another big swing: How do you make the process of getting
the full power out of modern GPUs productive and fun? In this episode, Ron and Chris
discuss how to design a language thatâ€™s easy to use while still providing the level of
control required to write state of the art kernels. A key idea is to ask programmers to
fully reckon with the details of the hardware, but making that work manageable and
shareable via a form of type-safe metaprogramming. The aim is to support both
specialization to the computation in question as well as to the hardware platform.
â€œSomebody has to do this work,â€ Chris says, â€œif we ever want to get to an ecosystem where
one vendor doesnâ€™t control everything.â€

SUMMARY

Chris Lattner is the creator of LLVM and led the development of the Swift language at
Apple. With Mojo, heâ€™s taking another big swing: How do you make the process of getting
the full power out of modern GPUs productive and fun? In this episode, Ron and Chris
discuss how to design a language thatâ€™s easy to use while still providing the level of
control required to write state of the art kernels. A key idea is to ask programmers to
fully reckon with the details of the hardware, but making that work manageable and
shareable via a form of type-safe metaprogramming. The aim is to support both
specialization to the computation in question as well as to the hardware platform.
â€œSomebody has to do this work,â€ Chris says, â€œif we ever want to get to an ecosystem where
one vendor doesnâ€™t control everything.â€

Some links to topics that came up in the discussion:


  Democratizing AI
compute
(an 11-part series)
  Modular AI
  Mojo
  MLIR
  Swift


TRANSCRIPT

00:00:03
Ron
Welcome to Signals and Threads, in-depth conversations about every layer of the tech
stack, from Jane Street. Iâ€™m Ron Minsky. It is my great pleasure to have Chris Lattner on
the show. Typically on Signals and Threads, we end up talking to engineers who work here
at Jane Street, but sometimes we like to grab outside folk, and Chris is an amazing figure
to bring on because heâ€™s been so involved in a bunch of really foundational pieces of
computing that we all useâ€”LLVM, and Clang, and MLIR, and OpenCL, and Swift, and now
Mojo. And this has happened at a bunch of different storied institutionsâ€”Apple, and Tesla,
and Google, and SiFive, and now Modular. So anyway, itâ€™s a pleasure to have you joining
us, Chris.

00:00:43
Chris
Thank you, Ron. Iâ€™m so happy to be here.

00:00:45
Ron
I guess I want to start by just hearing a little bit more about your origin story. How did
you get into computing and how did you get into this world of both compiler engineering
and programming language design?

00:00:54
Chris
So I grew up in the â€™80s and back before computers were really a thing. We had PCs, but
they werenâ€™t considered cool. And so I fell in love with understanding how the computer
worked. And back then, things were way simpler. I started with a BASIC interpreter, for
example, and youâ€™d get a book from the store. Remember when we had books? [laughs] And
youâ€™d learn things from books?

00:01:14
Ron
Did you do the thing where youâ€™d get the hobbyist magazine and copy out the listing of the
program?

00:01:19
Chris
Thatâ€™s exactly right. And so we didnâ€™t have vibe coding, but we did have books. And so
just by typing things in, you could understand how things work, and then when you broke
itâ€”because inevitably youâ€™re typing something in and you donâ€™t really know what youâ€™re
doingâ€”you have to figure out what went wrong and so it encouraged a certain amount of
debugging. I really love computer games. Again, back then, things were a little bit
simpler. Computer games drove graphics and performance and things like this. And so I
spent some time on these things called bulletin board systems and the early internet
reading about how game programmers are trying to push the limits of the hardware. And so
thatâ€™s where I got interested in performance and computers and systems. I went on to
college and had an amazing professor at my school, shout out to University of Portland in
Portland, Oregon, and he was a compiler nerd.

And so, I think that his love for compilers was infectious. His name was Steven Vegdahl,
and that caused me to go on to pursue compilers at University of Illinois. And there
again, continue to fall down this rabbit hole of compilers and systems, and build
LLVM. And ever since I got into the compiler world, I loved it. I love compilers because
theyâ€™re large-scale systems, thereâ€™s multiple different components that all work
together. And in the university setting, it was really cool in the compiler class, because
unlike most of the assignments where you do an assignment, turn it in, forget about itâ€”in
compilers, you would do an assignment, turn it in, get graded, and then build on it. And
it felt much more realistic like software engineering, rather than just doing a project to
get graded.

00:02:35
Ron
Yeah, I think for a lot of people, the OS class is their first real experience of doing a
thing where you really are building layer on top of layer. I think itâ€™s an incredibly
important experience for people as they start engineering.

00:02:44
Chris
Itâ€™s also one where you get to use some of those data structures. I took this, almost
academic, hereâ€™s what a binary tree is, and hereâ€™s what a graph is. And particularly when
I went through it, it was taught from a very math-forward perspective, but it really made
it useful. And so that was actually really cool. Iâ€™m like, â€˜Oh, this is why I learned this
stuff.â€™

00:02:59
Ron
So one thing that strikes me about your career is that youâ€™ve ended up going back and
forth between compiler engineering and language design space, whereas I feel like a lot of
people are on one side or the otherâ€”theyâ€™re mostly compilers people and they donâ€™t care
that much about the language, and just, how do we make this thing go fast? And there are
some people who are really focusing on language design and the work on the compiler is a
secondary thing towards that design. And youâ€™ve both popped back and forth. And then also
a lot of your compiler engineering work, really starting with LLVM, in some sense is
itself, very language-forward. With LLVM, thereâ€™s a language in there thatâ€™s this
intermediate language that youâ€™re surfacing as a tool for people to use. So Iâ€™m just
curious to hear more about how you think about the back and forth between compiler
engineering and language design.

00:03:39
Chris
The reason I do this is that effectively, my career is following my own interests. And so
my interests are not static. I want to work on different kinds of problems and solve
useful problems and build into things. And so the more technology and capability you have,
the higher you can reach. And so with LLVM, for example, built and learned a whole bunch
of cool stuff about deep code generation for an X86 chip and that category of technology
with register allocation, stuff like this. But then it made it possible to go, say, letâ€™s
go tackle C++ and letâ€™s go use this to build the worldâ€™s best implementation of something
that lots more people use and understand than deep backend code generation technology. And
then with Swift, it was, build even higher and say, â€˜Okay, well C++, maybe some people
like it, but I think we can do better and letâ€™s reach higher.â€™ Iâ€™ve also been involved in
AI systems, been involved in building an iPad app to help teach kids how to code. And so,
lots of different things over time. And so for me, the place I think Iâ€™m most useful and
where a lot of my experience is valuable ends up being at this hardware-software boundary.

00:04:36
Ron
Iâ€™m curious how you ended up making the leap to working on Swift. From my perspective,
Swift looks from the outside, like one of these points of arrival in mainstream
programming contexts of a bunch of ideas that I have long thought are really great ideas
in other programming languages. And Iâ€™m curious, in some ways a step away from like, oh,
Iâ€™m going to work on really low-level stuff and compiler optimization, and then we will go
much higher level and do a C++ implementation, which is still a pretty low level. How did
the whole Swift thing happen?

00:05:00
Chris
Great question. I mean, the timeframe for people that arenâ€™t familiar is that LLVM started
in 2000. So by 2005, I had exited university and I joined Apple. And so LLVM was an
advanced research project at that point. By the 2010 timeframe, LLVM was much more mature
and we had just shipped C++ support in Clang, and so it could bootstrap itself, which
means the compiler could compile itself. Itâ€™s all written in C++, it could build advanced
libraries like the Boost template library, which is super crazy advanced template
stuff. And so the C++ implementation that I and the team had built was real. Now, C++ in
my opinion, is not a beautiful programming language. And so implementing it is a very
interesting technical challenge. For me, a lot of problem-solving ends up being, how do
you factor the system the right way?

And so Clang has some really cool stuff that allowed it to scale and things like that, but
I was also burned out. We had just shipped it. It was amazing. Iâ€™m like, there has to be
something better. And so, Swift really came starting in 2010. It was a nights and weekends
project. It wasnâ€™t like top-down management said, â€˜Letâ€™s go build a new programming
language.â€™ It was â€˜Chris being burned outâ€™â€”I was running a 20 to 40 person team at the
time, being an engineer during the day, and being a technical leader, but then needing an
escape hatch. And so I said, â€˜Okay, well, I think we can have something better. I have a
lot of good ideas. Turns out, programming languages are a mature space. Itâ€™s not like you
need to invent pattern matching at this point. Itâ€™s embarrassing that C++ doesnâ€™t have
good pattern matching.

00:06:23
Ron
We should just pause for a second, because I think this is like a small but really
essential thing. I think the single best feature coming out of language like ML in the
mid-seventies is, first of all, this notion of an algebraic data type, meaning every
programming language on earth has a way of saying this and that and the other, a record,
or a class, or a tuple.

00:06:38
Chris
A weird programming language, I think it was Barbara Liskov?

00:06:41
Ron
Yeah. And she did a lot of the early theorizing about, â€˜What are abstract data types?â€™ But
the ability to do this or that or the other, to have data types that are a union of
different possible shapes of the dataâ€”and then having this pattern matching facility that
lets you basically in a reliable way do the case analysis so you can break down what the
possibilities areâ€”is just incredibly useful. And very few mainstream languages have picked
it up. I mean Swift again is an example, but languages like ML, SML, and Haskell, and
OCamlâ€”

00:07:09
Chris
Standard!

00:07:10
Ron
Thatâ€™s right. SML. Standard ML. Itâ€™s been there for a long time.

00:07:12
Chris
I mean pattern matching, it is not an exotic feature. Here weâ€™re talking about 2010. C#
didnâ€™t have it. C++ didnâ€™t have it. Obviously Java didnâ€™t have it. I donâ€™t think
JavaScript had it. None of these mainstream languages had it, but itâ€™s obvious. And so
part of my opinion about thatâ€”and so by the way, I represent as engineer, Iâ€™m not actually
a mathematician, and so type theory goes way over my head. I donâ€™t really understand
this. The thing that gets me frustrated about the academic approach to programming
languages is that people approach it by saying thereâ€™s sum types, and thereâ€™s intersection
types, and thereâ€™s these types, and they donâ€™t start from utility forward. And so pattern
matching, when I learned OCaml, itâ€™s so beautiful. It makes it so easy and expressive to
build very simple things. And so to me, I always identify to the utility and then yes,
thereâ€™s amazing formal type theory behind it, and thatâ€™s great and thatâ€™s why it actually
works and composes. But bringing that stuff forward and focusing on utility and the
problems it solves, and how it makes people happy, ends up being the thing that I think
moves the needle in terms of adoption, at least in mainstream.

00:08:09
Ron
Yeah, I mean I think thatâ€™s right. My approach also, and my interest in language is also
very much not from the mathematical perspective, although my undergraduate degree is in
math. I like math a lot, but I mostly approach these things as a practitioner. But the
thing Iâ€™ve been struck by over the years is the value of having these features have a
really strong mathematical foundation is they generalize, and as you were saying, compose
much better. If they are in the end mathematically simple, youâ€™re way more likely to have
a feature that actually pans out as it gets used way beyond your initial view as to what
the thing was for.

00:08:39
Chris
Thatâ€™s right. This is actually a personal defect because I donâ€™t understand the math in
the way that maybe theoretically would be ideal. I end up having to rediscover certain
truths that are obvious. The cliche, â€˜If the Russian mathematician invented it 50 years
agoâ€¦â€™ And so a lot of what I find is that I can find truth and beauty when things compose
and things fit together, and often Iâ€™ll find out itâ€™s already been discovered because
everything in programming language has been done. Thereâ€™s almost nothing novel, but still
that design process of saying, letâ€™s pull things together, letâ€™s reason about why it
doesnâ€™t quite fit together. Letâ€™s go figure out how to better factor this. Letâ€™s figure
out how to make it simpler these days. That process to me, I think is kind of like people
working on physics, [from what] I hear. The simpler the outcome becomes, the more close to
truth it feels like it is. And so I share thatâ€”and maybe itâ€™s more design gene or
engineer-design combination, but itâ€™s probably what you mathematicians actually know
inherently, and I just havenâ€™t figured it out yet.

00:09:33
Ron
Do you find yourself doing things after you come to it from an engineering perspective,
trying to figure out whether there are useful mathematical insights? Do you go back and
read the papers? Do you have other PL people who are more mathematically oriented who you
talk to? How do you extend your thinking to cover some of that other stuff?

00:09:47
Chris
See, the problem is math is scary to me. So I see Greek letters and I run away. I do
follow arXiv and things like this, and thereâ€™s a programming language section on that. And
so I get into some of it, but what I get attracted to in that is the examples and the
results section and the future-looking parts of it. And so itâ€™s not necessarily the â€˜how,â€™
itâ€™s the â€˜what it means.â€™ And so I think a lot of that really speaks to me. The other
thing that really speaks to me when you talk about language design and things like this is
blog posts from some obscure academic programming language that Iâ€™ve never heard of. You
just have somebody talking about algebraic effect systems for this and that and the other
thing, or something really fancy, but they figure out how to explain it in a way thatâ€™s
useful. And so when itâ€™s not just, â€˜Let me explain to you the type system,â€™ but itâ€™s, â€˜Let
me explain this problem this fancy feature enables,â€™ thatâ€™s where I get excited. Thatâ€™s
where it speaks to me because, again, Iâ€™m problem-oriented, and having a beautiful way to
express and solve problems, I appreciate.

00:10:38
Ron
I think thereâ€™s a lot of value in the work thatâ€™s done in papers of really working out in
detail the theory and the math and how it all fits together. [And] I think the fact that
the world has been filled with a lot of interesting blog posts from the same people has
been great because I think itâ€™s another modality where it often encourages you to pull out
the simpler and easier-to-consume versions of those ideas. And I think that is just a
different kind of insight and itâ€™s valuable to surface that too.

00:10:59
Chris
And also when I look at those blog posts, sometimes they design smell. Particularly the
C++ community, thereâ€™s a lot of really good work to fix C++. Theyâ€™re adding a lot of stuff
to it, and C++ will never get simplerâ€”you canâ€™t really remove things, right? And so a lot
of the challenge there is, itâ€™s constrained problem-solving. And so when I look at that,
often what Iâ€™ll see when Iâ€™m reading one of those posts, and again, these are brilliant
people and theyâ€™re doing Godâ€™s work trying to solve problems with C++, best of luck with
that. But you look at that and you realize thereâ€™s a grain of sand in the system that
didnâ€™t need to be there. And so to me, itâ€™s like if you remove that grain of sand, then
the entire system gets relaxed and suddenly all these constraints fall away and you can
get to something much simpler. Swift, for example, itâ€™s a wonderful language and itâ€™s
grown really well and the community is amazing, but it has a few grains of sand in it that
cause it to be a lot more complicated. And so this is where Iâ€™m not just happy with things
that got built. LLVM is amazing, itâ€™s very practical, but it has lots of problems. Thatâ€™s
why when I get a chance to build a next generation system, I want to learn from that and
actually try to solve these problems.

00:11:56
Ron
So this is the great privilege of getting to work on a new language, which is a thing
youâ€™re doing now. Thereâ€™s this new language called Mojo, and itâ€™s being done by this
company that you co-founded called Modular. Maybe just so we understand the context a
little bit, can you tell me a little bit about, what is Modular? Whatâ€™s the basic
offering? Whatâ€™s the business model?

00:12:12
Chris
Before I even get there, Iâ€™ll share more of how I got here. If you oversimplify my
background, I did this LLVM thing and its foundational compiler technology for CPUs. It
helped unite a lot of CPU-era infrastructure and it provided a platform for languages like
Swift, but also Rust, and Julia, and many different systems that all got built on top of,
and I think it really catalyzed and enabled a lot of really cool applications of
accelerated compiler technology. People use LLVM in databases and for query engine
optimization, lots of cool stuff. Maybe you use it for trading or something. I mean, there
can be tons of different applications for this kind of technologyâ€”and then [I] did
programming language stuff with Swift. But in the meantime, AI happened. And so with AI
brought this entirely new generation of compute: GPUs, tensor processing units,
large-scale AI training systems, FPGAs, and ASICs and all this complexity for compute, and
LLVM never really worked in that system.

And so one of the things that I built when I was at Google was a bunch of foundational
compiler technology for that category of systems. And thereâ€™s this compiler technology
called MLIR. MLIR is basically LLVM 2.0. And so take everything you learn from building
LLVM and helping solve this, but then bring it forward into this next generation of
compiler technology so that you can go hopefully unify the worldâ€™s compute for this GPU
and AI and ASIC kind of world. MLIR has been amazingly successful, and I think itâ€™s used
in roughly every one of these AI systems and GPUs. Itâ€™s used by Nvidia, itâ€™s used by
Google, itâ€™s used by roughly everybody in this space. But one of the challenges is that
there hasnâ€™t been unification. And so you have these very large-scale AI software
platforms. You have CUDA from Nvidia, you have XLA from Google, you have ROCm from AMD.

Itâ€™s countless. Every company has their own software stack. And one of the things that I
discovered and encountered, and I think the entire world sees, is that thereâ€™s this
incredible fragmentation driven by the fact that each of these software stacks built by a
hardware maker are just all completely different. And some of them work better than
others, but regardless, itâ€™s a gigantic mess. And thereâ€™s these really cool high-level
technologies like PyTorch that we all love and we want to use. But if PyTorch is built on
completely different stacks and schooling together these megalithic worlds from different
vendors, itâ€™s very difficult to get something that works.

00:14:17
Ron
Right. Theyâ€™re both complicated trade-offs around the performance that you get out of
different tools and then also a different set of complicated trade-offs around how hard
they are to use, how complicated it is to write something in them, and then what hardware
you can target from each individual one. And each of these ecosystems is churning just
incredibly fast. Thereâ€™s always new hardware coming out and new vendors in new places, and
thereâ€™s also new little languages popping up into existence, and it makes the whole thing
pretty hard to wrangle.

00:14:42
Chris
Exactly. And AI is moving so fast. Thereâ€™s a new model every week. Itâ€™s crazy. And new
applications, new research, the amount of money being dumped into this by everybody is
just incredible. And so how does anybody keep up? Itâ€™s a structural problem in the
industry. And so the structural problem is that the people doing this kind of work, the
people doing code generation for advanced GPUs and things like this, theyâ€™re all at
hardware companies. And the hardware companies, every single one of them is building their
own stack because they have to. There is nothing to plug into. Thereâ€™s nothing like â€˜LLVM
but for AI,â€™ that doesnâ€™t exist. And so as they go and build their own vertical software
stack, of course theyâ€™re focused on their hardware, they got advanced roadmaps, they have
a new chip coming out next year, theyâ€™re plowing their energy and time into solving for
their hardware. But we, out in the industry, we actually want something else. We want to
be able to have software that runs across multiple pieces of hardware. And so, if
everybody doing the work is at a hardware company, itâ€™s very natural that you get this
fragmentation across vendors because nobodyâ€™s incentivized to go work together. And even
if theyâ€™re incentivized, they donâ€™t have time to go work on somebody elseâ€™s chip. AMD is
not going to pay to work on Nvidia GPUs or something like this.

00:15:45
Ron
Thatâ€™s true when you think about this, kind of, a split between low-level and high-level
languages. So Nvidia has CUDA and AMD has ROCm, which is mostly a clone of CUDA, and then
the XLA tools from Google work incredibly well on TPUs, and so on and so forth. Different
vendors have different things. Then thereâ€™s the high-level tools, PyTorch, and JAX, and
Triton, and various things like that. And those are typically actually not made by the
hardware vendors. Those are made by different kinds of usersâ€”I guess Google is responsible
for some of these and theyâ€™re also sometimes a hardware vendorâ€”but a lot of the time itâ€™s
more stepped back. Although even there, the cross-platform support is complicated and
messy and incomplete.

00:16:22
Chris
Because theyâ€™re built on top of fundamentally incompatible things. And so thatâ€™s the
fundamental nature. And so again, you go back to Chrisâ€™s dysfunction and my weird career
choices, I always end up back at the hardware-software boundary, and thereâ€™s a lot of
other folks that are really good at adding very high-level abstractions. If you go back a
few years ago, MLOps was the cool thing, and it was, â€˜Letâ€™s build a layer of Python on top
of TensorFlow and PyTorch and build a unified AI platform.â€™ But the problem with that, is
that building abstractions on top of two things that donâ€™t work very well, canâ€™t solve
performance, or liability, or management, or these other problems. You can only add a
layer of duct tape, but as soon as something goes wrong, you end up having to debug this
entire crazy stack of stuff that you really didnâ€™t want to have to know about.

And so itâ€™s a leaky abstraction. And so the genesis of Modular (bringing it back to this)
was realizing there are structural problems in the industry. There is nobody thatâ€™s
incentivized to go build a unifying software platform and do that work at the bottom
level. And so what we set off to do is we said, â€˜Okay, letâ€™s go buildâ€¦â€™â€”and thereâ€™s
different ways of explaining this. You could say â€˜a replacement for CUDA,â€™ thatâ€™s like a
flamboyant way to say this, but â€˜letâ€™s go build a successor to all of this technology that
is better than what the hardware makers are building, and is portable.â€™ And so what this
takes, is doing the work that these hardware companies are doing, and I set the goal for
the team of saying, letâ€™s do it better than, for example, Nvidia is doing it for their own
hardware.

00:17:38
Ron
Which is no easy feat, right? Theyâ€™ve got a lot of very strong engineers and they
understand their hardware better than anyone does. Beating them on their own hardware is
tough.

00:17:45
Chris
That is really hard. And theyâ€™ve got a 20-year head start, because CUDA is about 20 years
old. Theyâ€™ve got all the momentum. Theyâ€™re a pretty big company. As you say, lots of smart
people. And so that was a ridiculous goal. Why did I do that? Well, I mean a certain
amount of confidence in understanding how the technology worked, having a bet on what I
thought we could build and the approach, and some insight and intuition, but also
realizing that itâ€™s actually destiny. Somebody has to do this work. If we ever want to get
to an ecosystem where one vendor doesnâ€™t control everything, if we want to get the best
out of the hardware, if we want to get new programming language technologies, if we want
pattern matching on a GPUâ€”I mean, come on, this isnâ€™t rocket scienceâ€”then we need at some
point to do this. And if nobody else is going to do it, Iâ€™ll step up and do that. Thatâ€™s
where Modular came fromâ€”saying, â€˜Letâ€™s go crack this thing open. I donâ€™t know how long it
will take, but sometimes itâ€™s worthwhile doing really hard things if theyâ€™re valuable to
the world.â€™ And the belief was it could be profoundly impactful and hopefully get more
people into even just being able to use this new form of compute with GPUs and
accelerators and all this stuff, and just really redemocratize AI compute.

00:18:48
Ron
So you pointed out that thereâ€™s a real structural problem here, and Iâ€™m actually wondering
how, at a business model level, do you want to solve the structural problem? Which is, the
history of computing is these days littered with the bodies of companies that try to sell
a programming language. Itâ€™s a really hard business. How is Modular set up so that itâ€™s
incented to build this platform in a way that can be a shared platform that isnâ€™t subject
to just one other vendorâ€™s lock-in?

00:19:11
Chris
First answer is, donâ€™t sell a programming language. As you say, thatâ€™s very difficult. So
weâ€™re not doing that. Go take Mojo, go use it for free. Weâ€™re not selling a programming
language. What weâ€™re doing is weâ€™re investing in this foundational technology to unify
hardware. Our view is, as weâ€™ve seen in many other domains, once you fix the foundation,
now you can build high-value services for enterprises. And so our enterprise layer, often
what we talk to, you end up with these groups where you have hundreds or thousands of
GPUs. Often itâ€™s rented from a cloud on a three-year commit. You have a platform team
thatâ€™s carrying pagers and they need to keep all this stuff running and all the production
workloads running. And then you have these product teams that are inventing new stuff all
the time, and thereâ€™s new research, thereâ€™s a new model that comes out and they want to
get it on the production infrastructure, but none of this stuff actually works.

And so the software ecosystem we have with all these brilliant but crazy open source tools
that are thrashing around, all these different versions of CUDA and libraries, all this
different hardware happening, is just a gigantic mess. And so, helping solve this for the
platform engineering team that actually needs to have stuff work, and want to be able to
reason about it, and want good observability and manageability and scalability and things
like this is actually, we think, very interesting. Weâ€™ve gotten a lot of good responses
from people on that. The cost of doing this is we want to actually make it work, thatâ€™s
where we do fundamental language compiler underlying systems technology and help bring
together these accelerators so that we can get, for example, the best performance on an
AMD GPU and get it so that the software comes out in the same release train as support for
an Nvidia GPU. And being able to pull that together, again, it just multiplicatively
reduces complexity, which then leads to a product that actually works, which is really
cool and very novel in AI.

00:20:49
Ron
So the way that Mojo plays in here, is it basically lets you provide the best possible
performance and I guess the best possible performance across multiple different hardware
platforms. Are you primarily thinking about this as an inference platform, or, how does
the training world fit in?

00:20:57
Chris
So let me zoom in and Iâ€™ll explain our technology components. I have a blog post series I
encourage you and any viewers or listeners to check out, called, â€˜Democratizing AI
Compute.â€™ It goes through the history of all the systems and the problems and challenges
that theyâ€™ve run into, and it gets to, â€˜What is Modular doing about it?â€™ So Part 11 talks
about our architecture and the inside is Mojo, which is a programming language. Iâ€™ll
explain Mojo in a second. Next level out is called MAX. And so you can think of MAX as
being a PyTorch replacement or a vLLM replacement, something that you can run on a single
node and then get high performance LLM surveying, that kind of use case. And then the next
level out is called Mammoth, and this is the cluster management Kubernetes layer. And so
if you zoom in all the way back to Mojo, you sayâ€”your experience, you know what
programming languages are, theyâ€™re incredibly difficult and expensive to build.

Why would you do that in the first place? And the answer is, we had to. In fact, when we
started Modular, I was like, â€˜Iâ€™m not going to invent a programming language.â€™ I know
thatâ€™s a bad idea, it takes too long, itâ€™s too much work. You canâ€™t convince people to
adopt a new language. I know all the reasons why creating language is actually a really
bad idea. But it turns out, we were forced to do this because there is no good way to
solve the problem. And the problem is, how do you write code that is portable across
accelerators? So, that problem, I want portability acrossâ€”for example, make it simple AMD
and Nvidia GPUs, but then you layer on the fact that youâ€™re using a GPU because you want
performance. And so I donâ€™t want a simplified, watered downâ€”I want Java that runs on a
GPU.

I want the full power of the GPU. I want to be able to deliver performance that meets and
beats Nvidia on their own hardware. I want to have portability and unify this crazy
compute where you have these really fancy heterogeneous systems and you have tensor cores
and you have this explosion of complexity and innovation happening in this hardware
platform layer. Most programming languages donâ€™t even know that thereâ€™s an 8-bit floating
point that exists. And so we looked around and I really did not want to have to do this,
but it turns out that there really is no good answer. And again, we decided that, hey, the
stakes are high, we want to do something impactful. Weâ€™re willing to invest. I know what
it takes to build a programming language. Itâ€™s not rocket science, itâ€™s just a lot of
really hard work and you need to set the team up to be incentivized the right way. But we
decided that, yeah, letâ€™s do that.

00:23:08
Ron
So I want to talk more about Mojo and its design, but before we do, maybe letâ€™s talk a
little bit more about the pre-existing environment. I did actually read that blog post
series. I recommended it to everyone. I think itâ€™s really great, and I want to talk a
little bit about what the existing ecosystem of languages looks like, but even before
then, can we talk more about the hardware? What does the space of hardware look like that
people want to run these ML models on?

00:23:29
Chris
Yeah, so the one that most people zero in on is the GPU. And so GPUs are, I think, getting
better understood now. And so if you go back before that though, you have CPUs. So, modern
CPUs in a data center, often youâ€™ll haveâ€”I mean today you guys are probably riding quite
big iron, but you got 100 cores in a CPU and you got a server with two-to-four CPUs on a
motherboard, and then you go and you scale that. And so, youâ€™ve got traditional threaded
workloads that have to run on CPUs, and we know how to scale that for internet servers and
things like this. If you get to a GPU, the architecture shifts. And so they have basically
these things called SMs. And now the programming model is that you have effectively much
more medium-sized compute thatâ€™s now put together on much higher performance memory
fabrics and the programming model shifts. And one of the things that really broke CUDA,
for example, was when GPUs got this thing called a tensor coreâ€”and the way to think about
a tensor core is itâ€™s a dedicated piece of hardware for matrix multiplication. And so,
whyâ€™d we get that? Well, a lot of AI is matrix multiplication. And so, if you design the
hardware to be good at a specific workload, you can have dedicated silicon for that and
you can make things go really fast.

00:24:36
Ron
There are really these two quite different models sitting inside of the GPU space. Of
course, the name itself is weird. GPU is â€˜graphics processing unit,â€™ which is what they
were originally for. And then this SM model is really interesting. They have this notion
of a warp. A warp is a collection of typically 32 threads that are operating together in
lockstep, always doing the same thingâ€”a slight variation on whatâ€™s called the SIMD model,
same instruction, multiple data. Itâ€™s a little more general than that, but more or less,
you can think of it as the same thing. And you just have to run a lot of them. And then
thereâ€™s a ton of hardware inside of these systems basically to make switching between
threads incredibly cheap. So you pay a lot of silicon to add extra registers. So the
context switch is super cheap, so you can do a ton of stuff in parallel.

Each thing youâ€™re doing is itself 32-wise parallel. And then because you can do all this
very fast context switching, you can hide a lot of latency. And that worked for a
while. And then weâ€™re like, actually, we need way more of this matrix multiplication
stuff. And you can sort of do reasonably efficient matrix multiplication through this warp
model, but not really that good. And then thereâ€™s a bunch of quite idiosyncratic hardware,
which changes its performing characteristics from generation to generation, just for doing
these matrix multiplications. So thatâ€™s the Nvidia GPU story, and Volta is like V100 and
A100 and H100. They just keep on going and changing, pretty materially from generation to
generation in terms of the performance characteristics, and then also the memory model,
which keeps on changing.

00:25:57
Chris
You go back to intuition, CUDA was never designed for this world. CUDA was not designed
for modern GPUs. It was designed for a much simpler world. And CUDA being 20 years old, it
hasnâ€™t really caught up. And itâ€™s very difficult because, as you say, the hardware keeps
changing. And so CUDA was designed from a world whereâ€”almost like C is designed for a very
simple programming model that it expected to scale, but then as the hardware changed, it
couldnâ€™t adapt. Now, if you get beyond GPUs, you get to Google TPU and many other
dedicated AI systems. They blow this way out and they say, â€˜Okay, well, letâ€™s get rid of
the threads that you have on a GPU and letâ€™s just have matrix multiplication units and
have really big matrix multiplication units and build the entire chip around that. And you
get much more specialization, but you get a much higher throughput for those AI workloads.

Going back to, â€˜Why Mojo?â€™ Well, Mojo was designed from first principles to support this
kind of system. Each of these chips, as youâ€™re saying, even within Nvidiaâ€™s family, from
Volta, to Ampere, to Hopper, to Blackwell, these things are not compatible with each
other. Actually, Blackwell just broke compatibility with Hopper, so it canâ€™t run Hopper
kernels always on Blackwell. Oops, well, why are they doing that? Well, AI software is
moving so fast. They decided that was the right trade-off to make. And meanwhile, we all
software people need the ability to target this. When you look at other existing systems,
with Triton for example, their goal was, â€˜Letâ€™s make it easier to program a GPU,â€™ which I
love, thatâ€™s awesome. But then they said, â€˜Weâ€™ll just give up 20% of the performance of
the silicon to do it.â€™ Wait a second. I want all the performance. And so if Iâ€™m using a
GPUâ€”GPUs are quite expensive by the wayâ€”

I want all the performance. And if itâ€™s not going to be able to deliver the same quality
of results you get by writing CUDA, well then, youâ€™re always going to run to this head
room, where you get going quickly, but then you run into a ceiling and then have to switch
to a different system to get full performance. And so this is where Mojo is really trying
to solve this problem where we can get more usability, more portability, and full
performance of the silicon because itâ€™s designed for these wacky architectures like tensor
cores.

00:27:51
Ron
And if we look at the other languages that are out there, thereâ€™s languages like CUDA, and
OpenCL, which are low level, typically look like variations on C++, in that tradition are
unsafe languages, which means that thereâ€™s a lot of rules you have to follow. And if you
donâ€™t exactly follow the rules, youâ€™re in undefined behavior land, itâ€™s very hard to
reason about your program.

00:28:10
Chris
And just let me make fun of my C++ heritage because Iâ€™ve spent so many years, like, you
just have a variable that you forget to initialize, it just shoots your foot off. [laughs]
Like, itâ€™s just unnecessary violence to programmers.

00:28:21
Ron
Right. And itâ€™s done in the interest of making performance better because the idea is C++
and its related languages donâ€™t really give you enough information to know when youâ€™re
making a mistake, and they want to have as much space as they can to optimize the programs
they get. So the stance is just, if you do anything thatâ€™s not allowed, we have no
obligation to maintain any kind of reasonable semantics or debug ability around that
behavior. And weâ€™re just going to try really, really hard to optimize correct programs,
which is a super weird stance to take, because nobodyâ€™s programs are correct. There are
bugs and undefined behavior in almost any C++ program of any size. And so, youâ€™re in a
very strange position in terms of the guarantees that you get from the compiler system
youâ€™re using.

00:29:02
Chris
Well, so I mean, I can be dissatisfied. I can also be sympathetic with people that work on
C++. So again, Iâ€™ve spent decades in this language and around this ecosystem, and building
compilers for it. I know quite a lot about it. The challenge is that C++ is established,
and so thereâ€™s tons of code out there. By far, the code thatâ€™s already written is the code
thatâ€™s the most valuable. And so if youâ€™re building a compiler, or you have a new chip, or
you have an optimizer, your goal is to get value out of the existing software. And so you
canâ€™t invent a new programming paradigm thatâ€™s a better way of doing things and defines
away the problem. Instead, you have to work with what youâ€™ve got. You have a SPEC
benchmark youâ€™re trying to make go fast, and so you invent some crazy heroic hack that
makes some important benchmark work because you canâ€™t go change the code.

In my experience, particularly for AI, but also Iâ€™m sure within Jane Street, if
somethingâ€™s going slow, go change the code. You have control over the architecture of the
system. And so, what I think the world really benefits from, unlike benchmark hacking, is
languages that give control and power and expressivity to the programmer. And this is
something where I think that, again, you take a step back and you realize history is the
way it is for lots of structural and very valid reasons, but the reasons donâ€™t apply to
this new age of compute. Nobody has a workload that they can pull forward to next yearâ€™s
GPUâ€”doesnâ€™t exist. Nobody solved this problem. I donâ€™t know the timeframe, but once we
solve that problem, once we solve portability, you can start this new era of software that
can actually go forward. And so now, to me, the burden isâ€”make sure itâ€™s actually
good. And so, to your point about memory safety, donâ€™t make it so that forgetting to
initialize a variable is just going to shoot your foot off. [Instead] produce a good
compiler error saying, â€˜Hey, you forgot to initialize a variable,â€™ right? These basic
things are actually really profound and important, and the tooling and all this usability
and this DNA, these feelings and thoughts, are what flow into Mojo.

00:30:49
Ron
And GPU programming is just a very different world from traditional CPU programming just
in terms of the basic economics and how humans are involved. You end up dealing with much
smaller programs. You have these very small but very high-value programs whose performance
is super critical, and in the end, a relatively small coterie of experts who end up
programming in it. And so it pushes you ever in the direction, youâ€™re saying, of
performance engineering, right? You want to give people the control they need to make the
thing behave as it should, and you want to do it in a way that allows people to be highly
productive. And the idea that you have an enormous amount of legacy code that you need to
bring over, itâ€™s like, actually you kind of donâ€™t. The entire universe of software is
actually shockingly small, and itâ€™s really about how to write these small programs as well
as possible.

00:31:32
Chris
And also thereâ€™s another huge change. And so this is something that I donâ€™t think that the
programming language community has recognized yet, but AI coding has massively changed the
game because now you can take a CUDA kernel and say, â€˜Hey, Claude, go make that into
Mojo.â€™

00:31:45
Ron
And actually, how good have you guys found the experience of that? Of doing translation?

00:31:48
Chris
Well, we do hackathons and people do amazing things, having never touched Mojo, having
never done GPU programming, and within a day they can make things happen that are just
shocking. Now, AI coding tools are not magic. You cannot just vibe code DeepSeek-R1 or
something, right? But itâ€™s amazing what that can do in terms of learning new languages,
learning new tools, and getting into and catalyzing ecosystems. And so this is one of the
things where, again, you go back five or 10 yearsâ€”everybody knows nobody can learn a new
language, and nobodyâ€™s willing to adopt new things. But the entire system has changed.

00:32:20
Ron
So letâ€™s talk a little bit more in detail about the architecture of Mojo. What kind of
language is Mojo, and what are the design elements that you chose in order to make it be
able to address this set of problems?

00:32:30
Chris
Yeah, again, just to relate how different the situation isâ€”back when I was working on
Swift, one of the major problems to solve was, objective C was very difficult for people
to use, and you had pointers, and you had square brackets, and it was very weird. And so
the goal in the game of the day was, invent new syntax and bring together modern
programming language features to build a new language. Fast forward to today, actually,
some of that is true. AI people donâ€™t like C++. C++ has pointers, and itâ€™s ugly, and itâ€™s
a 40-year-old-plus language, and has actually the same problem that Swift had to solve
back in the day. But today thereâ€™s something different, which is that AI people do
actually love a thing. Itâ€™s called Python. And so, one of the really important things
about Mojo is, itâ€™s a member of the Python family. And so, this is polarizing to some,
because yesâ€”I get it that some people love curly braces, but itâ€™s hugely powerful because
so much of the AI community is Pythonic already.

And so we started out by saying, letâ€™s keep the syntax like Python and only diverge from
that if thereâ€™s a really good reason. But then what are the good reasons? Well, the good
reasons are, we wantâ€”as we were talking aboutâ€”performance, power, full control over the
system. And for GPUs, thereâ€™s these very important things you want to do that require
metaprogramming. And so Mojo has a very fancy metaprogramming system, kind of inspired by
this language called Zig, that brings runtime and compile time together to enable really
powerful library designs. And the way you crack open this problem with tensor cores and
things like this, is you enable really powerful libraries to be built in the language as
libraries, instead of hard coding into the compiler.

00:33:57
Ron
Letâ€™s take it a little bit to the metaprogramming idea. What is metaprogramming and why
does it matter for performance in particular?

00:34:03
Chris
Yeah, itâ€™s a great question, and I think you know the answer to this too, and I know you,
butâ€”

00:34:08
Ron
[Laughs] We are also working on metaprogramming features in our own world.

00:34:11
Chris
Exactly. And so the observation here is, when youâ€™re writing a for loop in a programming
language, for example, typically that for loop executes at runtime, so youâ€™re writing code
that when you execute the program, itâ€™s the instructions that the computer will follow to
execute the algorithm within your code. But when you get into designing higher level type
systems, suddenly you want to be able to run code at compile time as well. And so thereâ€™s
many languages out there. Some of them have macro systems, C++ has templates. What you end
up getting is, you end up getting, in many languages, this duality between what happens at
runtime, and then a different language almost that happens at compile time. And C++ is the
most egregious, because templates that you have a for loop in runtime, but then you have
unrolled recursive templates, or something like that at compile time.

Well, so the insight is, hey, these two problems are actually the same. They just run at
different times. And so what Mojo does is says, letâ€™s allow the use of effectively any
code that you would use at runtime to also work at compile time. And so you can have a
list, or a string, or whatever you want in the algorithmsâ€”go do memory allocation,
deallocationâ€”and you can run those at compile time, enabling you to build really powerful
high-level abstractions and put them into libraries. So why is this cool? Well, the reason
itâ€™s cool is that on a GPU, for example, youâ€™ll have a tensor core. Tensor cores are
weird. We probably donâ€™t need to deep dive into all the reasons why, but the indexing and
the layout that tensor cores use is very specific and very vendor different. And so the
tensor core you have on AMD, or the tensor cores you have on different versions of Nvidia
GPUs are all very different.

And so what you want, is you want to build as a GP programmer a set of abstractions so you
can reason about all of these things in one common ecosystem and have the layouts much
higher level. And so what this enables, it enables very powerful librariesâ€”and very
powerful libraries where a lot of the logic is actually done at compile time, but you can
debug it because itâ€™s the same language that you use at runtime. And it makes the language
much more simpler, much more powerful, and just be able to scale into these complexities
in a way thatâ€™s possible with C++. But in C++, you get some crazy template stack trace
that is maddening and impossible to understand. In Mojo, you can get a very simple error
message. You can actually debug your code, and debugger things like this.

00:36:17
Ron
So maybe an important point here is that metaprogramming is really an old solution to this
performance problem. Maybe a good way of thinking about this is, imagine you have some
piece of data that you have that represents a little embedded domain-specific language
that youâ€™ve written, that you want to execute via a program that you wrote. You can, in a
nice high-level way, write a little interpreter for that language that justâ€”you know, I
have maybe a Boolean expression language or who knows what else. Maybe itâ€™s a language for
computing on tensors in a GPU. And you could write a program that just executes that mini
domain-specific language and does the thing that you want and you can do it, but itâ€™s
really slow. Writing an interpreter is just inherently slow because of all this
interpretation overhead where you are dynamically making decisions about what the behavior
of the program is. And sometimes what you want, is, you just want to actually emit exactly
the code that you want and boil away the control structure and just get the direct lines
of machine code that you want to do the thing thatâ€™s necessary.

And various forms of code generation let you get past in a simpler way, lets you get past
all of this control structure that you have to execute at runtime and instead be able to
execute it at compile time and get this minified program that just does exactly the thing
that you want. So thatâ€™s a really old idea. It goes back to all sorts of programming
languages. Thereâ€™s a lot of Lisps that did a lot of this metaprogramming stuff, but then
the problem is this stuff is super hard to think about and reason about and debug. And
thatâ€™s certainly true if you think about in C, all this macro language, if you use the
various C preprocessors to do this kind of stuff in C, itâ€™s pretty painful to reason
about. And then C++ made it richer and more expressive, but still really hard to reason
about. And you write a C++ template and you donâ€™t really know what itâ€™s going to do or if
itâ€™s going to compile until you give it all the inputs and let it go and itâ€”

00:37:55
Chris
Feels good in the simple case. But then when you get to more advanced cases, suddenly the
complexity compounds and it gets out of hand.

00:38:01
Ron
And it sounds like the thing that youâ€™re going for in Mojo is it feels like one
language. It has one type system that covers both the stuff youâ€™re generating statically
and the stuff that youâ€™re doing at runtime. It sounds like debugging works in the same way
across both of these layers, but you still get the actual runtime behavior you want from a
language that you could more explicitly just be like, hereâ€™s exactly the code that I want
to generate.

00:38:24
Chris
[â€¦] metaprogramming is one of the fancy features. One of the cool features is it feels
and looks like Python, but with actual types.

00:38:31
Ron
Right.

00:38:32
Chris
And letâ€™s not forget the basics. Having something that looks and feels like Python but
itâ€™s a thousand times faster or something is actually pretty cool. For example, if youâ€™re
on a CPU, you have access to SIMD, the SIMD registers that allow you to do multiple
operations at a time and [to] be able to get the full power of your hardware even without
using the fancy features is also really cool. And so the challenge with any of these
systems is, how do you make something thatâ€™s powerful, but itâ€™s also easy to use? I think
your teamâ€™s been playing with Mojo and doing some cool stuff. I mean, what have you seen
and whatâ€™s your experience been?

00:39:02
Ron
Weâ€™re all still pretty new to it, but I think itâ€™s got a lot of exciting things going for
it. I mean, the first thing is, yeah, it gives you the kind of programming model you want
to get the performance that you need. And actually, in many ways the same kind of
programming model that you get out of something like CUTLASS or CuTe DSL, which are these
Nvidia-specific, some at the C++ level, some at the Python DSL levelâ€”and by the way, every
tool you can imagine nowadays is done once in C++ and once in Python. We donâ€™t need to
implement programming languages in any other way anymore. Theyâ€™re all either skins on C++
or skins on Python. But depending on which path you go down, whether you go the C++ path
or the Python path, you get all sorts of complicated trade-offs.

Like in the C++ path in particular, you get very painful compilation times. The thing you
said about template metaprogramming is absolutely true. The error messages are super
bad. If you look at these more Python-embedded DSLs, the compile times tend to be
better. It still can be hard to reason about though. One nice thing about Mojo is the
overall discipline seems very explicit when you want to understand: Is this a value thatâ€™s
happening at execution time at the end, or is it a value that is going to be dealt with at
compile time? Itâ€™s just very explicit in the syntax, you can look and understand. Whereas
in some of these DSLs, you have to actively go and poke the value and ask it what kind of
value it is. And I think that kind of explicitness is actually really important for
performance engineering, making it easy to understand just what precisely youâ€™re doing.

You actually see this a ton, not even with these very low-level things, but if you look at
PyTorch, which is a much higher level tool, PyTorch does this thing where you get to write
a thing that looks like an ordinary Python program, but really itâ€™s got a much trickier
execution model. Pythonâ€™s an amazing and terrible ecosystem in which to do this kind of
stuff, because what guarantees do you have when youâ€™re using Python? None. What can you
do? Anything. You have an enormous amount of freedom. The PyTorch people in particular
have leveraged this freedom in a bunch of very clever ways, where you can write a Python
program that looks like itâ€™s doing something very simple and straightforward that would be
really slow, but noâ€”itâ€™s very carefully delaying and making some operations lazy so it can
overlap compute on the GPU and CPU and make stuff go really fast. And thatâ€™s really nice,
except sometimes it just doesnâ€™t work.

00:41:04
Chris
This is the trap again, this is my decades of battle scars now. So as a compiler guy, I
can make fun of other compiler people. Thereâ€™s this trap and itâ€™s an attractive trap,
which is called the â€˜sufficiently smart compiler.â€™ And so what you can do is you can take
something and you can make it look good on a demo and you can say, â€˜Look! I make it super
easy and Iâ€™m going to make my compiler super smart, and itâ€™s going to take care of all
this and make it easy through magic.â€™ But magic doesnâ€™t exist. And so anytime you have one
of those â€˜sufficiently smart compilers,â€™ if you go back in the days, it was like
auto-parallelization, just write C code is sequential logic, and then weâ€™re going to
automatically map it into running on 100 cores on a supercomputer or something like that.

They often actually do work, they work in very simple cases and they work in the
demos. But the problem is that you go and youâ€™re using them and then you change one thing
and suddenly everything breaks. Maybe the compiler crashes, it just doesnâ€™t work. Or you
go and fix a bug and now instead of 100-times speedup, you get 100-times slowdown because
it foiled the compiler. A lot of AI tools, a lot of these systems, particularly these
DSLs, have this design point of, let me pretend like itâ€™s easy and then I will take care
of it behind the scenes. But then when something breaks, you have to end up looking at
compiler dumps, right? And this is because magic doesnâ€™t exist. And so this is where
predictability and control is really, I think, the name of the game, particularly if you
want to get the most out of a piece of hardware, which is how we ended up here.

00:42:23
Ron
Itâ€™s funny, the same issue of, â€œHow clever is the underlying system youâ€™re using?â€ comes
up when you look at the difference between CPUs and GPUs. CPUs themselves are trying to do
a weird thing where a chip is a fundamentally parallel substrate. Itâ€™s got all of these
circuits that in principle could be running in parallel and then it is yoked to running
this extremely sequential programming language, which is just trying to do one thing after
another. And then how does that actually work with any reasonable efficiency? Well,
thereâ€™s all sorts of clever dirty tricks happening under the covers where itâ€™s trying to
predict what youâ€™re going to do, this speculation that allows it to dispatch multiple
instructions in a row by guessing what youâ€™re going to do in the future. Thereâ€™s things
like memory prefetching where it has heuristics to estimate what memory youâ€™re going to
ask in the future so it can dispatch multiple memory requests at the same time.

And then if you look at things like GPUs, and I think even more, TPUs, and then also
totally other things like FPGAs, the field-programmable gate arrays where you put
basically a circuit design on it. Itâ€™s a very different kind of software system. But all
of them are in some sense simpler and more deterministic and more explicitly
parallel. Like when you write down your program, you have to write an explicitly parallel
programâ€”thatâ€™s actually harder to write. I donâ€™t want to complain too much about CPUs. The
great thing about CPUs is theyâ€™re extremely flexible and incredibly easy to use and all of
that dark magic actually works a pretty large fraction of the time.

00:43:42
Chris
Yeah, remarkably well. But your point here, I think itâ€™s really great, and what youâ€™re
saying is, youâ€™re saying CPUs are the magic box that makes sequential code go in parallel
pretty fast. And then we have new, more explicit machines, somewhat harder to program
because theyâ€™re not a magic box, but you get something from it. You get performance and
power because that magic box doesnâ€™t come without a cost. It comes with a very significant
cost, often the amount of power that your machine dissipates. And so itâ€™s not
efficient. And so a lot of the reasons weâ€™re getting these new accelerators is because
people really do care about it being a hundred times faster, or using way less power, or
things like this. And Iâ€™d never thought about it, but your analogy of Triton to Mojo kind
of follows a similar pattern, right? Triton is trying to be the magic box, and it doesnâ€™t
give you the full performance, and it burns more power, and all that kind of stuff. And so
Mojo is saying, look, letâ€™s go back to being simple. Letâ€™s give the programmer more
control. And that more explicit approach, I think, is a good fit for people that are
building crazy advanced hardware like youâ€™re talking aboutâ€”but also people that want to
get the best performance out of the existing hardware we have.

00:44:42
Ron
So we talked about how metaprogramming lets you write faster programs by boiling away this
control structure that you donâ€™t really need. So that partâ€™s good. How does it give you
portable performance? How does it help you on the portability front?

00:44:54
Chris
Yeah, so this is another great question. So in this category of â€˜sufficiently smart
compilers,â€™ and particularly for AI compilers, thereâ€™s been years of work and MLIR has
catalyzed a lot of this work building these magic AI compilers that take TensorFlow or
even the new PyTorch stuff and trying to generate optimal code for some chip. So take some
PyTorch model and put it through a compiler, and magically get out high performance. And
so thereâ€™s tons of these things, and thereâ€™s a lot of great work done here, and a lot of
people have shown that you can take kernels and accelerate them with compilers. The
challenge with this is that people donâ€™t ever measureâ€”what is the full performance of the
chip? And so people always measure from a somewhat unfortunate baseline and then try to
climb higher instead of sayingâ€”what is the speed of light? And so if you measure from
speed of light, suddenly you say, okay, how do I achieve several different things?

Even if you zero into one piece of silicon, how do I achieve the best performance for one
use case? And then how do I make it so the software I write can generalize even within the
domain? And so for example, take a matrix multiplication, well, you want to work on maybe
float32, but then you want to generalize it to float16. Okay, well, templates and things
like this are easy ways to do this. Then programming allows you to say, okay, I will
tackle that. And then the next thing that happens is, because you went from float32 to
float16, your effective cache size has doubled, because twice as many elements fit into
cache if thereâ€™s 16 bits than if there are 32 bits. Well, if thatâ€™s the case, now suddenly
the access pattern needs to change. And so you get a whole bunch of this conditional logic
that now changes in a very parametric way as a result of one simple change that happened
with float32 to float16.

Now you play that forward and you say, okay, well actually matrix multiplication is a
recursive hierarchical problem. Thereâ€™s specializations for tall and skinny matrices, and
a dimension is one or something. Thereâ€™s all these special cases. Just one algorithm for
one chip becomes this very complicated subsystem that you end up wanting to do a lot of
transformations to so you can go specialize it for different use cases. And so Mojo with
the metaprogramming allows you to tackle that. Now you bring in other hardware, and so
think of matrix multiplication these days as being almost an operating system, and thereâ€™s
so many different subsystems, and special cases, and different D types, and crazy float4
and six and other stuff going on.

00:47:07
Ron
At some point theyâ€™re going to come out with a floating point number so small that it will
be a joke. But every time I think that theyâ€™re just kidding, it turns out itâ€™s real.

00:47:14
Chris
Seriously, I heard somebody talking about 1.2-bit floating point, right? Itâ€™s exactly like
youâ€™re saying, is that a joke? You canâ€™t be serious. And so now when you bring in other
hardware, other hardware brings in more complexity because suddenly the tensor core has a
different layout in AMD than it does on Nvidia. Or maybe to your point about warps, you
have 64 threads in a warp on one and 32 threads in a warp on the other. But what you
realize is, wait a secondâ€”this really has nothing to do with hardware vendors. This is
actually true even within, for example, the Nvidia line, because across these different
data types, the tensor cores are changing. The way the tensor core works for float32 is
different from the way it works for float4 or something. And so you alreadyâ€”within one
vendorâ€”have to have this very powerful metaprogramming to be able to handle the complexity
and do so in the scaffolding of a single algorithm like matrix multiplication.

And so now as you bring in other vendors, well it turns out hey, they all have things that
look roughly like tensor cores. And so weâ€™re coming at this with a software engineering
perspective, and so weâ€™re forced to build abstractions. We have this powerful
metaprogramming system so we can actually achieve this. And so even for one vendor, we get
this thing called LayoutTensor. LayoutTensor is saying, okay, well I have the ability to
reason about not just an array of numbers or a multidimensional array of numbers, but also
how itâ€™s laid out in memory and how it gets accessed. And so now we can declaratively map
these things onto the hardware that you have and these abstractions stack. And so itâ€™s
this really amazing triumvirate between having a type system that works well and this very
important basis. I know youâ€™re a fan of type systems also.

You then bring in metaprogramming, and so you can build powerful abstractions and run a
compile time so you get no runtime overhead. And then you bring in the most important part
of this entire equation, which is programmers who understand the domain. I am not going to
write a fast matrix multiplication. Iâ€™m sorry, thatâ€™s not my experience. But there are
people in that space that are just fricking brilliant. They understand exactly how the
hardware works, they understand the use cases and the latest research and the new crazy
quantized format of the day, but theyâ€™re not compiler people. And so the magic of Mojo is
it says, â€˜Hey, you have a type system, you have metaprogramming, you have effectively the
full power of a compiler that you have when youâ€™re building libraries.â€™ And so now these
people that are brilliant at unlocking the power of the hardware can actually do this. And
now they can write software that scales both across the complexity of the domain but also
across hardware. And to me, thatâ€™s what I find so exciting and so powerful about
this. Itâ€™s unlocking the power of the Mojo programmer instead of trying to put it into the
compiler, which is what a lot of earlier systems have tried to do.

00:49:49
Ron
So maybe the key point here is that you get to build these abstractions that allow you to
represent different kinds of hardware, and then you can conditionally have your code
execute based on the kind of hardware that itâ€™s on. Itâ€™s not like an #ifdef where youâ€™re
picking between different hardware platforms. There are complicated data structures like
these layout values that tell you how you traverse data.

00:50:07
Chris
Which is kind of a tree. This isnâ€™t just a simple int that youâ€™re passing around. This is
like a recursive hierarchical tree that you need at compile time.

00:50:13
Ron
The critical thing is you get to write a thing that feels like one synthetic program with
one understandable behavior, but then parts of it are actually going to execute at compile
time, so that the thing that you generate is in fact specialized for the particular
platform that youâ€™re going to run it on. So one concern I have over this is it sounds like
the configuration space of your programs is going to be massive, and I feel like there are
two directions where this seems potentially hard to do from an engineering
perspective. One is, can you really create abstractions that within the context of the
program hide the relevant complexity? So itâ€™s possible for people to think in a modular
way about the program theyâ€™re building, so their brains donâ€™t explode with the 70
different kinds of hardware that they might be running it on. And then the other question
is, how do you think about testing? Because thereâ€™s just so many configurations. How do
you know whether itâ€™s working in all the places? Because it sounds like it has an enormous
amount of freedom to do different things, including wrong things in some cases. How do you
deal with those two problems, both controlling the complexity of the abstractions and then
having a testing story that works out?

00:51:11
Chris
Okay, Ron, Iâ€™m going to blow your mind. I know youâ€™re going to be resistant to this, but
let me convince you that types are cool.

00:51:16
Ron
Okay!

00:51:18
Chris
I know youâ€™re going to fight me on this. Well, so this is again, you go back to the
challenges and opportunities of working with either Python or C++. Python doesnâ€™t have
types really. I mean it has some stuff, but it doesnâ€™t really have a type system. C++ has
a type system, but itâ€™s just incredibly painful to work with. And so what Mojo does is it
says, again, itâ€™s not rocket science. We see it all around us. Letâ€™s bring in
traits. Letâ€™s bring in a reasonable way to write code so that we can build abstractions
that are domain-specific and they can be checked modularly. And so one of the big problems
with C++ is that you get error messages when you instantiate layers and layers and layers
and layers of templates. And so if you get some magic number wrong, it explodes
spectacularly in a way that you canâ€™t reason about. And so what Mojo does, it says, cool,
letâ€™s bring in traits that feel very much like protocols in Swift, or traits in Rust, or
type classes in Haskell. Like, this isnâ€™t novel.

00:52:08
Ron
This is like a mechanism for whatâ€™s called ad hoc polymorphism, meaning I want to have
some operation or function that has some meaning, but actually itâ€™s going to get
implemented in different ways for different types. And these are basically all mechanisms
of a way of, given the thing that youâ€™re doing and the types involved, looking up the
right implementation thatâ€™s going to do the thing that you want.

00:52:25
Chris
Yeah, I mean a very simple case is an iterator. So Mojo has an iterator trait and you can
say, â€˜Hey, what is an iterator over a collection?â€™ Well, you can either check, see if
thereâ€™s an element, or you can get the value at the current element. And then as you keep
pulling things out of an iterator, it will eventually decide to stop. And so this concept
can be applied to things like a linked list, or an array, or a dictionary, or an unbounded
sequence of packets coming off a network. And so you can write code thatâ€™s generic across
these differentâ€”call them â€œbackendsâ€ or â€œmodelsâ€â€”that implement this trait. And what the
compiler will do for you is it will check to make sure when youâ€™re writing that generic
code, youâ€™re not using something that wonâ€™t work. And so what that does, is it means that
you can check the generic code without having to instantiate it, which is good for compile
time. Itâ€™s good for user experience, because if you get something wrong as a programmer,
thatâ€™s important. Itâ€™s good for reasoning about the modularity of these different
subsystems, because now you have an interface that connects the two components.

00:53:22
Ron
I think itâ€™s an underappreciated problem with the C++ templates approach to the world,
where C++ templates seem like a deep language feature, but really theyâ€™re just a code
generation feature.

00:53:32
Chris
Theyâ€™re like C macros.

00:53:33
Ron
Thatâ€™s right. It both means theyâ€™re hard to think about and reason about because it sort
of seems at first glance not to be so badâ€”this property that you donâ€™t really know when
your template expands, if itâ€™s actually going to compile. But as you start composing
things more deeply, it gets worse and worse because something somewhere is going to fail,
and itâ€™s just going to be hard to reason about and understand. Whereas when you have
type-level notions of genericity that are guaranteed to compose correctly and wonâ€™t just
blow up, you just drive that error right down. So thatâ€™s one thing thatâ€™s nice about
getting past templates as a language feature. And then the other thing is itâ€™s just
crushingly slow. Youâ€™re generating the code, almost exactly the same code, over and over
and over again. And so that just means you canâ€™t save any of the compilation work. You
just have to redo the whole thing from scratch.

00:54:21
Chris
Thatâ€™s exactly right. And so this is where again, we were talking about the sand in the
systemâ€”these little things that if you get wrong, they play forward and they cause huge
problems. The metaprogramming approach in Mojo is cool, both for usability and compile
time and correctness. Coming back to your point about portability, itâ€™s also valuable for
portability because what it means is that the compiler parses your code, and it parses it
generically and has no idea what the target is. And so when Mojo generates the first level
of intermediate representation, the compiler representation for the code, itâ€™s not hard
coding and the pointers are 32 bit or 64 bit, or that youâ€™re on a x86 or whatever. And
what this means is that you can take generic code in Mojo and you can put it on a CPU and
you can put it on a GPU. Same code, same function. And again, these crazy compilery things
that Chris gets obsessed about, it means that you can slice out the chunk of code that you
want to put onto your GPU in a way that it looks like a distributed system, but itâ€™s a
distributed system where the GPU is actually a crazy embedded device that wants this tiny
snippet of code and it wants it fully self-contained. These worlds of things that normal
programming languages havenâ€™t even thought about.

00:55:29
Ron
So does that mean when I compile a Mojo program, I get a shippable executable that
contains within it another little compiler that can take the Mojo code and specialize it
to get the actual machine code for the final destination that you need? Do I bundle
together all the compilers for all the possible platforms in every Mojo executable?

00:55:45
Chris
The answer is no. The worldâ€™s not ready for that. And there are use cases for JIT
compilers and things like this, and thatâ€™s cool, but the default way of building, if you
just run mojo build, then it will give you just an a.out executable, a normal thing. But
if you build a Mojo package, the Mojo package retains portability. This is a big
difference. This is what Java does. If you think about Java in a completely different way
and for different reasons in a different ecosystem universe, it parses all your source
code without knowing what the target is, and it generates Java bytecode. And so itâ€™s not
1995 anymore. The way we do this is completely different. And weâ€™re not Java obviously,
and we have a type system thatâ€™s very different. But this concept is something thatâ€™s been
well known, and is something that at least the world of compiled languages like Swift, and
C++, and Rust have kind of forgotten.

00:56:28
Ron
So the Mojo package is kind of shipped with the compiler technology required to specialize
to the different domains.

00:56:34
Chris
Yes. And so again, by default, if youâ€™re a user, youâ€™re sitting on your laptop and you
say, â€˜Compile a Mojo program,â€™ you just want an executable. But the compiler technology
has all of these powerful features and they can be used in different ways. This is similar
to LLVM, where LLVM had a just-in-time compiler, and thatâ€™s really important if youâ€™re
Sony Pictures and youâ€™re rendering shaders for some fancy movie, but thatâ€™s not what youâ€™d
want to use if youâ€™re just running a C++ code that needs to be ahead-of-time compiled.

00:56:57
Ron
I mean, thereâ€™s some echoes here also of the PTX story with Nvidia. Nvidia has this thing
that they sort of hide that itâ€™s an intermediate representation, but this thing called
PTX, which is a portable bytecode essentially. And they for many years maintained
compatibility across many, many different generations of GPUs. They have a thing called
the assembler thatâ€™s part of the driver thing for loading on, and itâ€™s really not an
assembler. Itâ€™s like a real compiler that takes the PTX and compiles it down to SASS, the
accelerator-specific machine code, which they very carefully do not fully document because
they donâ€™t want to give away all of their secrets. And so thereâ€™s a built-in portability
story there where itâ€™s meant to actually be portable in the future across new
generations. Although as you were pointing out before, it in fact doesnâ€™t always
succeed. And there are now some programs that will not actually make the transition to
Blackwell.

00:57:42
Chris
So thatâ€™s in the category that Iâ€™d consider to be like a virtual machine, a very low-level
virtual machine by the way. And so when youâ€™re looking at these systems, the thing Iâ€™d ask
is, what is the type system? And so if you look at PTX, because as youâ€™re saying, youâ€™re
totally right, itâ€™s an abstraction between a whole bunch of source code on the top end and
then that specific SASS hardware thing on the backend, but the type system isnâ€™t very
interesting. Itâ€™s pointers and registers and memory. And so Java, what is the type system?
Well, Java achieves portability by making the type system in its bytecode expose
objects. And so itâ€™s a much higher level abstraction, dynamic virtual dispatch, thatâ€™s all
part of the Java ecosystem. Itâ€™s not a bytecode, but the representation thatâ€™s portable
maintains the full generic system. And so this is what makes it possible to say, â€˜Okay,
well Iâ€™m going to take this code, compile it once to a package, and now go specialize and
instantiate this for a device.â€™ So the way that works is a little bit different, but it
enables, coming back to your original question of safety and correctness, it enables all
the checking to happen the right way.

00:58:40
Ron
Right, thereâ€™s also a huge shift in control. With PTX, the machine-specific details of how
itâ€™s compiled are totally out of the programmerâ€™s control. You can generate the best PTX
you can, and then itâ€™s going to get compiled. How? Somehow, donâ€™t ask too many questions,
itâ€™s going to do what itâ€™s going to do. Whereas here, youâ€™re preserving in the portable
object, the programmer-driven instructions about how the specialization is going to
work. Youâ€™ve just partially executed your compilation, youâ€™ve got partway down, and then
thereâ€™s some more thatâ€™s going to be done at the end when you pick actually where youâ€™re
going to run it.

00:59:08
Chris
Exactly. And so these are all very nerdy pieces that go into the stack, but the thing that
I like is if you bubble out of that, itâ€™s easy to use. It works. It gives good error
messages, right? I donâ€™t understand the Greek letters, but I do understand a lot of the
engineering that goes into this. The way this technology stack builds up, the whole
purpose is to unlock compute, and we want new programmers to be able to get into the
system. And if they know Python, if they understand some of the basics of the hardware,
they can be effective and then they donâ€™t get limited to 80% of the performance. They can
keep driving and keep growing in sophistication, and maybe not everybody wants to do
that. They can stop at 80%, but if you do want to go all the way, then you can get there.

00:59:44
Ron
One thing Iâ€™m curious about is, how do you actually manage to keep it simple? You said
that Mojo is meant to be Pythonic and you talked a bunch about the syntax, but actually
one of the nice things about Python is itâ€™s simple in some ways in a deeper sense. The
fact that there isnâ€™t by default a complicated type system with complicated type errors to
think aboutâ€”thereâ€™s a lot of problems with that, but itâ€™s also a real source of simplicity
for users who are trying to learn the system. Dynamic errors at runtime are in some ways
easier to understand. â€˜I wrote a program and it tried to do a thing and it tripped over
this particular thing and you can see it tripping over,â€™ and in some ways thatâ€™s easier to
understand when youâ€™re going to a language which, for both safety and performance reasons,
needs much more precise type level control. How do you do that in a way that still feels
Pythonic in terms of the base simplicity that youâ€™re exposing to users?

01:00:28
Chris
I canâ€™t give you the perfect answer, but I can tell you my current thoughts. So again,
learn from history. Swift had a lot of really cool features, but it spiraled and got a lot
of complexity that got layered in over time. And also one of the challenges with Swift is
it had a team that was paid to add features to swift.

01:00:46
Ron
Itâ€™s never a good thing.

01:00:47
Chris
Well, you have a C++ committee, what is the C++ committee going to do? Theyâ€™re going to
keep adding features to C++. Donâ€™t expect C++ to get smaller. Itâ€™s common sense. And so
with Mojo, thereâ€™s a couple of different things. So one of which is, start from Python. So
Python being the surface-level syntax enables me as management to be able to push back and
say, â€˜Look, letâ€™s make sure weâ€™re implementing the full power of the Python
ecosystem. Letâ€™s have lists, and for-comprehensions, and all this stuff before just
inventing random stuff because it might be useful.â€™ But thereâ€™s also, for me personally, a
significant back pressure on complexity. How can we factor these things? How can we get,
for example, the metaprogramming system to subsume a lot of complexity that would
otherwise exist? And there are fundamental things that I want us to add.

For example, checked generics, things like this because they have a better UX, theyâ€™re
part of the metaprogramming system, theyâ€™re part of the core addition that weâ€™re adding,
but I donâ€™t want Mojo to turn into a â€˜add every language featureâ€™ that every other
language has just because itâ€™s useful to somebody. I was actually inspired by and learned
a lot from Go, and itâ€™s a language that people are probably surprised to hear me talk
about. Go, I think, did a really good job of intentionally constraining the language with
Go 1. And they took a lot of heat for that. They didnâ€™t add a generic system, and
everybody, myself included, were like, â€˜Ha ha ha, why doesnâ€™t this language even have a
generic system? Youâ€™re not even a modern language.â€™ But they held the line, they
understood how far people could get, and then they did a really good job of adding
generics to Go 2, and I thought they did a great job.

There was a recent blog post I was reading, talking about Go, and apparently they have an
80-20 rule, and they say they want to have 80% of the features with 20% of the complexity,
something like that. And the observation is that thatâ€™s a point in the space that annoys
everybody, because everybody wants 81% of the features, but 81% of the features maybe
gives you 35% of the complexity. And so, figuring out where to draw that line and figuring
out where to say noâ€”for example, we have people in the community that are asking for very
reasonable things that exist in Rust. And Rust is a wonderful language. I love it. Thereâ€™s
a lot of great ideas and we shamelessly pull good ideas from everywhere. But I donâ€™t want
the complexity.

01:03:02
Ron
I often like to say that one of the most critical things about a language design is
maintaining the power-to-weight ratio.

You want to get an enormous amount of good functionality, and power, and good user
experience while minimizing that complexity. I think it is a very challenging thing to
manage, and itâ€™s actually a thing that we are seeing a lot as well. We are also doing a
lot to extend OCaml in all sorts of ways, pulling from all sorts of languages, including
Rust, and again, doing it in a way where the language maintains its basic character and
maintains its simplicity is a real challenge. And itâ€™s kind of hard to know if youâ€™re
hitting the actual right point on that. And itâ€™s easier to do in a world where you can
take things back, try things out and decide that maybe they donâ€™t work, and then adjust
your behavior. And weâ€™re trying to iterate a lot in that mode, which is a thing you can do
under certain circumstances. It gets harder as you have a big open-source language that
lots of people are using.

01:03:47
Chris
Thatâ€™s a really great point. And so one of the other lessons Iâ€™ve learned with Swift, is
that with Swift, I pushed very early to have an open design process where anybody could
come in, write a proposal, and then it would be evaluated by the language committee, and
then if it was good, it would be implemented and put into Swift. Again, be careful what
you wish for. That enabled a lot of people with really good ideas to add a bunch of
features to Swift. And so with Mojo as a counterbalance, I really want the core team to be
small. I want the core team not just to be able to add a whole bunch of stuff because it
might be useful someday, but to be really deliberate about how we add things, how we
evolve things.

01:04:20
Ron
How are you thinking about maintaining backwards compatibility guarantees as you evolve it
forward?

01:04:25
Chris
Weâ€™re actively debating and discussing what Mojo 1.0 looks like. And so Iâ€™m not going to
give you a timeframe, but it will hopefully not be very far away. And what I am fond of is
this notion of semantic versioning, and saying weâ€™re going to have a 1.0, and then weâ€™re
going to have a 2.0, and weâ€™re going to have a 3.0, and weâ€™re going to have a 4.0, et
cetera. And each of these will be able to be incompatible, but they can link together. And
so one of the big challenges and a lot of the damage in the Python ecosystem was from the
Python two-to-three conversion. It took 15 years and it was a heroic mess for many
different reasons. The reason it took so long is because you have to convert the entire
package ecosystem before you can be 3.0. And so if you contrast that to something like
C++, let me say good things about C++, they got the ABI right.

And so once the ABI was set, then you could have one package built in C++ 98, and one
package built in C++ 23, and these things would interoperate and be compatible even if you
took new keywords or other things in the future language version. And so what I see for
Mojo is much more similar to theâ€”maybe the C++ ecosystem or something like this, but that
allows us to be a little bit more aggressive in terms of migrating code, in terms of
fixing bugs, and in moving language forward. But I want to make sure that Mojo 2.0 and
Mojo 1.0 packages work together and that thereâ€™s good tooling, probably AI-driven, but
good tooling to move from 1.0 to 2.0 and be able to manage the ecosystem that way.

01:05:49
Ron
I think the type system also helps an enormous amount. I think one of the reasons the
Python migration was so hard is that you couldnâ€™t be like, â€˜And then let me try and build
this with Python 3 and see whatâ€™s broken.â€™ You could only see whatâ€™s broken by actually
walking all of the execution paths of your program. And if you didnâ€™t have enough testing,
that would be very hard. And even if you did, it wasnâ€™t that easy. Whereas with a strong
type system, you can get an enormous amount of very precise guidance. And actually the
combination of a strong type system and an agentic coding system is awesome. We actually
have a bunch of experience of just trying these things out now, where you make some small
change to the type of something and then youâ€™re like, â€˜Hey, AI system, please run down all
the type errors, fix them all.â€™ And it does surprisingly well.

01:06:26
Chris
I absolutely agree. Thereâ€™s other components to it. So Rust has done a very good job with
the stabilization approach with crates and APIs. And I think thatâ€™s a really good
thing. And so I think weâ€™ll take good ideas from many of these different ecosystems and
hopefully do something that works well, and works well for the ecosystem, and allows us to
scale without being completely constrained by never being able to fix something once you
ship a 1.0.

01:06:45
Ron
Iâ€™m actually curious, just to go to the agentic programming thing for a second, which is
having AI agents that write good kernels is actually pretty hard. And Iâ€™m curious what
your experience is of how things work with Mojo. Mojo is obviously not a language deeply
embedded in the training set that these models were built on, but on the other hand, you
have this very strong type structure that can guide the process of the AI agent trying to
write and modify code. Iâ€™m curious how that pans out in practice as you try and use these
tools.

01:07:12
Chris
So this is why Mojo being open source, andâ€”so we have hundreds of thousands of lines of
Mojo code that are public with all these GPU kernels, and like, all this other cool
stuff. And we have a community of people writing more code. Having hundreds of thousand
lines of Mojo code is fantastic. You can point your coding tool cursor, or whatever it is,
at that repo and say, â€˜Go learn about this repo and index it.â€™ So itâ€™s not that you have
to train the model to know the language, just having access to itâ€”that enables it to do
good work. And these tools are phenomenal. And so thatâ€™s been very, very, very
important. And so we have instructions on our webpage for how to set up these tools, and
thereâ€™s a huge difference if you set it up right, so that it can index that, or if you
donâ€™t, and make sure to follow that markdown file that explains how to set up the tool.

01:07:54
Ron
So, I want to talk a little bit about the future of Mojo. I think that the current way
that Modular and you have been talking about Mojo, these days at leastâ€”itâ€™s a replacement
for CUDA, an alternate full top-to-bottom stack for building GPU kernels, for writing
programs that execute on GPUs. But thatâ€™s not the only way youâ€™ve ever talked about
Mojo. Youâ€™ve also, especially earlier on I think, there was more discussion of Mojo as an
extension, and maybe evolution of, and maybe eventually replacement of Python. And Iâ€™m
curious, how do you think about that now? To what degree do you think of Mojo as its own
new language that takes inspiration and syntax from Python, and to what degree do you want
something thatâ€™s more deeply integrated over time?

01:08:32
Chris
So today, to pull it back to, â€˜What is Mojo useful for today, and how do we explain it?â€™
Mojo is useful if you want code to go fast. If you have code on a CPU or a GPU and you
want it to go fast, Mojo is a great thing. One of the really cool things that is available
nowâ€”but itâ€™s in preview and itâ€™ll solidify in the next month or somethingâ€”is itâ€™s also the
best way to extend Python. And so if you have a large-scale Python code base, again, tell
me if this sounds familiar, you are coding away and youâ€™re doing cool stuff in Python and
then it starts to get slow. Typically what people do is, they have to either go rewrite
the whole thing in Rust or C++, or they carve out some chunk of it and move some chunk of
that package to C++ or Rust. This is what NumPy, or PyTorch, or all modern large-scale
Python code bases end up doing.

01:09:13
Ron
If you look up on the mirrors and look at the percentage of programs that have C
extensions in them, itâ€™s shockingly high. A really large fraction of Python stuff is
actually part Python and part some other language, almost always C and C++, a little bit
of Rust.

01:09:27
Chris
Thatâ€™s right. And so todayâ€”this isnâ€™t distant futureâ€”today, you can take your Python
package and you can create a Mojo file and you can say, â€˜Okay, well these for loops are
slow, move it over to Mojo.â€™ And we have people, for example, doing bioinformatics and
other crazy stuff I know nothing about, saying, â€˜Okay, well Iâ€™m just taking my Python
code, I move it over to Mojo. Wow, now I get types, I get these benefits, but thereâ€™s no
bindings. The pip experience is beautiful. Itâ€™s super simple.â€™ You donâ€™t have to have
FFIâ€™s and nanobind and all this complexity to be able to do this. You also are not moving
from Python with its syntax to curly braces and borrow checkers and other craziness. You
now get a very simple and seamless way to extend your Python package. And we have people
that say, okay, well I did that and I got it first 10x, and 100x, and 1000x faster on CPU.

But then because it was easy, I just put it on a GPU. And so to me, this is amazing
because these are people that didnâ€™t even think and would never have gotten it on a GPU if
they switched to Rust or something like that. Again, the way I explain it is, Mojo is good
for performance. Itâ€™s good if you want to go fast on a GPU, on a CPU, if you want to make
Python go fast, or if you want toâ€”I mean, some people are crazy enough to go whole hog and
just write entirely from scratch Mojo programs, and thatâ€™s super cool. If you fast forward
six, nine months, something, I think that Mojo will be a very credible top-to-bottom
replacement for Rust.

And so we need a few more extensions to the generic system. And thereâ€™s a few things I
want to bake out a little bit. Some of the dynamic features that Rust has for the
existentials, the ability to make a runtime trait is missing in Mojo. And so weâ€™ll add a
few of those kinds of features. And as we do that, I think thatâ€™ll be really interesting
as an applications-level programming language for people who care about this kind of
stuff. You fast forward, I might even project a timeframe, maybe a year, 18 months from
now, it depends on how we prioritize things, and weâ€™ll add classes. And so as we add
classes, suddenly it will look and feel to a Python programmer much more familiar. The
classes in Mojo will be intentionally designed to be very similar to Python, and at that
point weâ€™ll have something that looks and feels kind of like a Python 4.

Itâ€™s very much cut from the same mold as Python. It integrates really well from
Python. Itâ€™s really easy to extend Python, and so itâ€™s very much a member of the Python
family, but itâ€™s not compatible with Python. And so what weâ€™ll do over the course of N
years, and I canâ€™t predict exactly how long that is, is continue to run down the line of,
okay, well how much compatibility do we want to add to this thing? And then I think that
at some point people will consider it to be a Python superset, and effectively it will
feel just like the best way to do Python in general. And I think that that will come in
time. But to bring it all the way back, I want us to be very focused on, â€˜What is Mojo
useful for today?â€™ Great claims require great proof.

We have no proof that we can do this. I have a vision and a future in my brain, and Iâ€™ve
built a few languages and some scale things before, and so I have quite high confidence
that we can do this. But I want people to zero back into, okay, if youâ€™re writing
performance code, if youâ€™re writing GPU kernels or AI, if you have Python code, you donâ€™t
want it to go slow, a few of us have that problem, then Mojo can be very useful. And
hopefully itâ€™ll be even more useful to more people in the future.

01:12:26
Ron
And I think that already, the practical short-term thing is already plenty ambitious and
exciting on its own. Seems like a great thing to focus on.

01:12:32
Chris
Yeah, letâ€™s solve heterogeneous compute and AI. Thatâ€™s actually a pretty useful thing,
right?

01:12:37
Ron
Alright, that seems like a great place to stop. Thank you so much for joining me.

01:12:41
Chris
Yeah, well thank you for having me. I love nerding out with you and I hope itâ€™s useful and
interesting to other people too. But even if not, I had a lot of fun with you.

01:12:49
Ron
Youâ€™ll find a complete transcript of the episode along with show notes and links at
signalsandthreads.com. Thanks for joining us. See you next time.

	]]></content:encoded>
        </item>
    </channel>
</rss>