<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Tue, 02 Sep 2025 23:50:49 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Parallel AI agents are a game changer]]></title>
            <link>https://morningcoffee.io/parallel-ai-agents-are-a-game-changer.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45110075</guid>
            <description><![CDATA[I’ve been in this industry long enough to watch technologies come and go. I’ve
seen the excitement around new frameworks, the promises of revolutionary...]]></description>
            <content:encoded><![CDATA[
        I’ve been in this industry long enough to watch technologies come and go. I’ve
seen the excitement around new frameworks, the promises of revolutionary tools,
and the breathless predictions about what would “change everything.” Most of the
time, these technologies turned out to be incremental improvements wrapped in
marketing hyperbole.

But parallel agents? This is different. This is the first time I can say,
without any exaggeration, that I’m witnessing technology that will fundamentally
transform how we develop software.

How We Got Here

To understand where we are today, we need to look at the full history of AI-assisted
coding. It started with GitHub Copilot, which introduced the concept of AI pair
programming. Copilot could autocomplete code as you typed, suggesting functions,
completing implementations, and helping with repetitive tasks.

Then came the AI-powered editors like Windsurf and Cursor. These took the concept
further by integrating AI deeply into the development environment. Instead of just
autocomplete, you could have conversations with AI about your code, ask for
refactoring suggestions, and get help with debugging. The AI understood your
entire codebase and could provide contextual assistance.

This year, we’ve been working with what’s called “vibe coding” — AI tools where you
describe what you want in natural language, and the AI generates complete functions,
classes, or implementations from scratch. You tell it “create a sign up form with
google, github, and microsoft login options” and it produces working code that
captures the vibe of what you asked for.

The term “vibe coding” was coined by Andrej Karpathy in
this tweet, which perfectly
captured what this new way of programming felt like.

There’s a new kind of coding I call “vibe coding”, where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It’s possible because the LLMs (e.g. Cursor Composer w Sonnet) are getting too good. Also I just talk to Composer with SuperWhisper…— Andrej Karpathy (@karpathy) February 2, 2025

This was genuinely revolutionary. Suddenly, you could generate boilerplate code,
build simple functions, create UI components, and even tackle complex implementations
just by describing them. Many engineers adopted these tools and found them
incredibly useful for certain types of work.

The technology worked well enough that it changed how many of us approached
coding. Instead of starting from a blank file, you could start with a working
implementation and refine from there. It made prototyping faster, reduced the
tedium of writing repetitive code, and opened up possibilities for rapid
development.

Running Agents in Parallel

Here’s what’s different now: you can run multiple AI agents at the same time,
each working on different problems. Instead of waiting for one agent to finish
a task before starting the next one, you can have several agents running
simultaneously - one building a user interface, another writing API endpoints,
and a third creating database schemas.

The core advantage is simple: you can do multiple things at once. This isn’t
about smarter AI or better algorithms - it’s about parallelization. The same
vibe coding tools we had before, just running multiple instances simultaneously.

The first company that offered a good solution for this was GitHub, with their
GitHub Co-Pilots that were running in the cloud. You basically go to an issue
and describe it on GitHub. When you are ready with all the descriptions that
you think should be able to describe the function, you assign it to Co-Pilot
and then wait for the result.

In practice, this means that you can go to your existing issues, check if
they have enough context to be handed over to AI. And then you wait
for the system to send you a notification that you can review the results.

This transforms the way you write code, and instead of focusing on the microsteps,
you are playing the role of a senior engineer who is guiding and providing context
to multiple agents who are implementing the features in your codebase. Your job as an
engineer now becomes reviewing the code for correctness, ensuring that proper
architectural decisions were taken, that a feature makes sense from the user’s
perspective, and that the code meets all the security and compliance standards
that you need.

The agent itself has the same limitations as you would when you’re vibe coding,
which means that they will have the same tendency to make bugs, to lack enough
context, to not understand the code. But you are as an engineer, and I would say
partly a product owner and designer, would guide the system to implement it
for you.

How to work with multiple parallel agents

Parallel agents are changing the way engineers work. Instead of focusing on one
task at a time, you can now coordinate several agents working on different
features or bug fixes in parallel. You’re actively managing multiple streams
of development, reviewing code, and providing feedback as each agent completes
its work.

With this approach, I can manage to have 10–20 pull requests open at once, each
handled by a dedicated agent.

Here are some practical steps to take:

1. Prepare issues with sufficient context

Start by ensuring each GitHub issue contains enough context for agents to
understand what needs to be built and how it integrates with the system. This
might include details about feature behavior, file locations, database
structure, or specific requirements such as displaying certain fields or
handling edge cases.

2. Assign agents in batches

Once issues are ready, assign them to AI agents (such as @copilot). Each
assignment typically starts a new pull request, where the agent creates a plan,
builds a checklist, and begins implementation. Multiple issues can be assigned
at once, allowing agents to work in parallel. Each individual agent takes
around 5-20 minutes to complete its work.

3. Review and iterate locally

After an agent completes its tasks, review the resulting pull requests locally.
Testing features and verifying correctness is essential. If changes are needed,
leave comments or feedback on the pull request, and the agent will continue
refining the solution.

4. Maintain flow between reviews

Unlike traditional workflows, parallel agent orchestration keeps me engaged
and focused. Instead of waiting for one agent to finish, it’s possible to move
between active pull requests—reviewing, testing, and providing feedback
as needed. This enables simultaneous progress on multiple tasks without
significant mental overhead.

Here is a recording of how this works in practice:



What to Expect from Parallel Agents

Working with parallel agents requires a different mindset than traditional or
vibe coding. The shift is as significant as moving from traditional coding to
AI-assisted development in the first place.

Mental Model Changes

Control shifts from precision to orchestration. Instead of controlling every
line of code, you’re managing multiple problems simultaneously. Think like a
system engineer managing Kubernetes pods rather than babysitting individual
servers - each task is expendable and replaceable.

Everything becomes asynchronous. Unlike vibe coding where you wait and
watch, parallel agents work asynchronously by default. The context you provide
upfront determines the result 30 minutes later. You can’t do half-hearted
prompts and fix as you go, because those fixes come an hour later.

Batch thinking replaces linear thinking. Instead of picking one perfect task
from the backlog, identify several problems you could tackle in a day. A good
approach is focusing on 2 critical deliverables while running 5-10 small
background tasks - copy changes, UI fixes, minor bugs that can process while
you focus on important work.

Realistic Success Rates

Don’t expect 100% success rates. Here’s what typically happens based on my personal
observation while writing code.


10%: Perfect one-shot solution, ready to ship.
20%: Almost there, needs 10 minutes of local refinement.
40%: Needs manual intervention.
20%: Completely wrong. Close the issue and write down learnings.
10%: Bad product idea.


Even if only 10% of the issues are solved perfectly by the agent, the process is
still valuable. Agents reliably handle the initial setup—finding the right
files, writing boilerplate, and adding tests. By the time you review, much of
the groundwork is done, so you can focus on investigating and fixing specific
problems.

The frustration of engineers comes when they don’t have a properly aligned
expectation of what they should expect from a coding agent. Some engineers
simply give up if they don’t get the perfect 100% solution. I think you should
move past this limitation and just learn to extract the goodness while jumping
in with proper engineering knowledge where it needs to be.

What Works Well vs. What Doesn’t

Parallel agents excel at:


Bug fixes and race conditions
Backend logic, controllers, validations
Database changes and migrations
Package version bumps and code transformations
Small, well-defined implementation tasks


They struggle with:


New UI development (you need to see changes as you build)
Tasks requiring real-time visual feedback
Implementing undocumented additions to existing PRs
Complex architectural decisions requiring context beyond the issue


Skills That Become More Important

Several traditional skills become even more valuable with parallel agents:

Full-stack understanding is valuable when working with parallel agents. If
your expertise is limited to either frontend or backend, you’ll quickly
encounter roadblocks. Agents often need guidance across the entire stack, from
database migrations to UI updates, so being able to navigate both worlds
ensures smoother collaboration and better results.

Problem decomposition becomes a critical skill. Large, complex issues are
difficult for agents to tackle effectively. Breaking down big problems into
smaller, well-defined tasks allows agents to work independently and in
parallel, increasing the overall throughput and making it easier to review and
integrate their work.

Good writting skills are important. Agents rely on the clarity and detail of your
issue descriptions to produce accurate results. Avoid vague language, unnecessary
jargon, or ambiguous requirements. The more specific and structured your
instructions, the higher the quality of the agent’s output.

QA and Code Review skills take center stage in this workflow. Since the
review cycle is the main bottleneck, being able to quickly assess code quality,
spot bugs, and verify that requirements are met is crucial. Efficient testing
and validation ensure that parallel development doesn’t lead to a backlog of
unreviewed or faulty code.

When you are working with parallel agents, you should optimize for review speed.
You can start 50 issues, but you still need to review, understand, and verify
each one. Making that review cycle fast—ideally under 10 seconds to check out,
rebuild, and test—becomes critical to the entire workflow.


  Learned a lot while using parallel @github agents yesterday:1/ My mental model was not prepared for parallel async work with agents2/ You can’t expect 100% success, but you can make a series of small bets3/ Strategies to overcome blocking issues4/ When to use Claude Code and… pic.twitter.com/yKNNkNZnby— Igor Šarčević (@igor_sarcevic) August 22, 2025 


Engineering Practices That Enable Parallel Agents

Working with parallel agents requires a well-structured engineering environment
that supports rapid iteration and review.

Fast CI/CD Pipeline

A robust CI/CD flow makes it easy to test and verify results. When agents
complete their work, you need to quickly validate that the changes work
correctly without manual deployment overhead. Automated testing, fast builds,
and seamless deployment processes remove friction from the review cycle. Without
this foundation, the bottleneck shifts from agent completion time to deployment
and testing time.

System Documentation

System architecture documentation helps when multiple agents work on different
parts of your codebase. Agents need to understand how components interact, where
files are located, what conventions are followed, and how different systems
integrate. Well-documented APIs, architectural decisions, coding standards, and
system boundaries help agents make better decisions and reduce the need for
manual corrections.

Preview and Staging Environments

A reliable staging environment where you can manually test features is required.
Since agents work asynchronously, you need a consistent place to validate their
output without affecting production systems. This environment should mirror
production, deploy quickly, and allow easy testing of multiple concurrent
changes. The ability to spin up isolated environments for different branches or
pull requests streamlines the parallel review process.

Monorepo Architecture Benefits

Keeping related services and components in a single monorepo works better when
working with parallel agents. A monorepo gives agents context about the entire
system within a single codebase.

When agents work across multiple repositories, they lose context about service
interactions, shared libraries, and dependencies. This leads to solutions that
work in isolation but break integration points. With a monorepo, agents
understand the full scope of changes needed - updating API contracts, adjusting
client code, and modifying shared utilities all in one pull request.

The unified view enables better architectural decisions. Agents can see existing
patterns, reuse common components, and maintain consistency across the system.
Code reviews are more effective because all related changes are visible in one
place, making it easier to verify that agents haven’t introduced integration
issues.

Monorepos simplify deployment and testing for parallel development. Instead of
coordinating releases across multiple repositories, you can test complete system
changes together and deploy atomically. This reduces complexity when managing
multiple concurrent agent-generated changes across service boundaries.

Tools That Support Parallel Agents

Several development tools now support running parallel agents, each with
different strengths and maturity levels.

GitHub Agents offer the most polished experience. They’re integrated
directly into GitHub Issues and work seamlessly with VSCode. You assign issues
to @copilot, and agents create pull requests that you can review locally. There
are still some rough edges, but GitHub is addressing these issues one by one
with regular improvements.

Cursor is currently experimenting with parallel agent support through a beta
program. They’ve invited select users to test this functionality, and early
reports suggest it’s a promising implementation. If you’re already using Cursor
for vibe coding, their parallel agents might be worth exploring once they open
up broader access.

OpenAI’s Codex CLI also supports running agents in the cloud, which enables
this type of parallel development workflow. The CLI lets you start agents that
continue running remotely, allowing you to manage multiple concurrent coding
tasks without keeping your local environment tied up.

Each tool takes a slightly different approach to parallel execution, so the best
choice depends on your existing development workflow and tool preferences.

Wrapping Up

I’ve been working with parallel agents for a few weeks now, and it’s changed
how I approach development. The ability to work on multiple problems
simultaneously, rather than sequentially, makes a real difference in
productivity.

The technology isn’t perfect - you’ll spend time reviewing and fixing
agent-generated code. But when you can kick off 10 tasks and have most of them
move forward without your direct involvement, it frees up mental bandwidth for
the problems that actually need human judgment.

If you’re curious about trying this approach, start with small, well-defined
issues in your backlog. Write clear descriptions and see what happens. The worst
case is you spend a few minutes reviewing code that doesn’t work. The best case
is you discover a new way of working that fits your development style.

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You're Not Interviewing for the Job. You're Auditioning for the Job Title]]></title>
            <link>https://idiallo.com/blog/performing-for-the-job-title</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45109324</guid>
            <description><![CDATA[I once had a job interview for a backend position. Their stack was Node.js, MySQL, nothing exotic. The interviewer asked: "If you have an array containing a million entries, how would you sort the dat]]></description>
            <content:encoded><![CDATA[
	

I once had a job interview for a backend position. Their stack was Node.js, MySQL, nothing exotic. The interviewer asked: "If you have an array containing a million entries, how would you sort the data by name?"

My immediate thought was: If you have a JavaScript array with a million entries, you're certainly doing something wrong.

The interviewer continued: "There are multiple fields that you should be able to sort by."

This felt like a trick question. Surely the right answer was to explain why you shouldn't be sorting millions of records in JavaScript. Pagination, database indexing, server-side filtering. So I said exactly that.

I was wrong. He wanted me to show him Array.prototype.sort().

My crime? Prioritizing real-world efficiency over theatrical scale. The interviewer didn't see a practical engineer, he saw a candidate who "lacked vision."

The Theater of Technical Interviews

I once read that "a complex system usually reflects an absence of good design." It's brilliant. True. And if you're prepping for a system design interview, forget it immediately.

In real-world engineering, simplicity is king. In interviews, complexity is currency.

Job interviews aren't assessments. They're auditions for a job title: The Architect Who Solves Hard Problems™.

You're not being evaluated on whether you can build the described system efficiently. You're being evaluated on whether you can perform the role of someone who could theoretically build Google.

The Unwritten Script

Every system design interview follows the same theatrical formula:

Act I: Summon the Dragons
First, you assume infinite users and planetary-scale traffic. Multiply every reasonable number by 1,000. Treat a todo app like it's handling the New York Stock Exchange. The interviewer nods approvingly as you describe millions of concurrent users for what is essentially a digital notepad.

Act II: Draw the Arcane Symbols
Next, you cover the whiteboard in boxes, arrows, and at least one redundant Kubernetes cluster. Add a message queue, Kafka obviously, regardless of whether you need one. Sprinkle in some microservices because monoliths are for peasants, and draw load balancers like protective talismans around every component.

Act III: Invoke the Gods of Tech
Finally, you debate consensus algorithms for a blog comment system, mention observability and SLOs for good measure, and whisper "eventual consistency" like an incantation that will ward off all future scaling problems.

The performance has nothing to do with the actual job. It's about proving you know the vocabulary of scale.

Interview World: Complexity = Competence


Show knowledge of distributed systems patterns
Demonstrate familiarity with enterprise-grade tools
Prove you can architect for theoretical millions


Real World: Simplicity = Good Design


Start with the simplest solution that works
Add complexity only when genuinely needed
Optimize for maintainability, not theoretical scale


I've seen great engineers fail interviews because they suggested using a relational database instead of a distributed NoSQL cluster, for a system that would handle 100 requests per day.

I've also seen mediocre engineers get hired because they could fluently discuss sharding strategies for problems that didn't need sharding.

Why the Theater Exists

This isn't malicious. It's structural, driven by several interconnected forces:

Signal vs. Noise: Complexity is easier to evaluate than judgment. Anyone can memorize CAP theorem and recite the differences between SQL and NoSQL databases. But knowing about when not to use distributed systems? That's harder to assess in a 45-minute conversation. Interviewers gravitate toward what they can measure.

Risk Aversion: Hiring committees fear false negatives more than false positives. A "simple" answer feels risky. What if this person can't handle complexity when it's genuinely needed? A "complex" answer feels safer, even if it's complete overkill for the actual problem at hand.

The Aspiration Gap: Companies hire for their aspirational selves, not their current reality. They want "Senior Staff Engineers" who can theoretically scale to millions of users, not "People Who Write Maintainable Code" for their current 10,000 daily active users. The fantasy of future scale drives present hiring decisions.

Interviewer Incentives: Discussing load balancing strategies and distributed consensus is more intellectually stimulating than talking about boring CRUD operations. Even if CRUD is 90% of what the actual job entails. Interviewers are human, and humans prefer interesting conversations over practical ones.

Once you're hired, the script flips entirely. You'll spend your days deleting unused microservices that "future-proofed" against scale that never came, arguing against message queues for problems that cron jobs solve perfectly, and consolidating databases that were split "for scalability" but now just create maintenance overhead. You'll find yourself explaining to eager junior developers why the new feature doesn't need its own Kubernetes namespace, and why sometimes a simple bash script is more reliable than a distributed system.

The very complexity that got you hired becomes the tech debt you'll spend years dismantling. I've worked with engineers who aced system design interviews by proposing elaborate architectures, then spent their first year on the job simplifying systems that previous hires had over-engineered using the exact same interview logic. It's a cycle of complexity creation and subsequent complexity destruction, driven entirely by hiring theater.

The Cost of Honesty

What happens when you try to be pragmatic in interviews?

The Backpressure Question: "How do you handle backpressure in your system?"


Honest answer: "We don't generate enough load to need backpressure handling."
Interview answer: "We implement exponential backoff with circuit breakers and queue depth monitoring."


The Database Question: "SQL or NoSQL for this use case?"


Honest answer: "Postgres handles this fine and the team knows SQL."
Interview answer: "We'll use a polyglot persistence strategy with Redis for caching, Cassandra for time-series data, and Neo4j for the social graph."


The Scaling Question: "How do you handle 10 million users?"


Honest answer: "We don't have 10 million users. We have 10,000. Let's solve that first."
Interview answer: "We'll implement horizontal sharding with consistent hashing and auto-scaling Kubernetes pods."


The honest answers demonstrate experience. The interview answers demonstrate vocabulary. Guess which ones get job offers?

How to Win the Game

I'm not advocating dishonesty, I'm acknowledging reality. Interviews are a ritual, and rituals have rules. Here's how to navigate them:

Separate Performance from Practice: Playing the interview game doesn't make you a hypocrite. It makes you pragmatic about a broken system. You can excel at interview theater while still being a principled engineer once you're hired.

Learn the Sacred Texts: Study distributed systems patterns even if you'll never use them. Memorize the CAP theorem even if it's mostly irrelevant to your daily work. Practice drawing architecture diagrams that look impressive on whiteboards. Think of it as learning a foreign language you'll only speak during interviews.

Embrace the Tropes: Always start discussions with "At scale, we'd need to consider..." Mention monitoring and observability early and often, even for simple systems. Add redundancy everywhere, even for non-critical components. Use the magic words that signal competence in interview-land.

Then Drop the Act: Once hired, advocate ruthlessly for simplicity. Be the voice of reason who asks "Do we actually need this complexity?" Use your hard-earned credibility to push back against unnecessary over-engineering. This is where the real engineering work begins.

This system is frustrating but not permanent. Some companies are starting to recognize the disconnect and experiment with better approaches. They're giving candidates real problems the company actually faces rather than theoretical scaling challenges. They're rewarding engineers who question complexity, not just those who can implement it. They're asking candidates to identify over-engineering in existing systems rather than create new complexity from scratch.

But until these practices become widespread, we're stuck with the performance. The theater continues because it serves too many psychological needs: it makes interviewers feel smart, companies feel rigorous, and candidates feel like they're solving important problems.



Technical interviews exist in a parallel universe where different rules apply. The stage demands a performance of complexity; the job rewards mastery of simplicity.

The system is broken, but individual actors can't fix it by martyring themselves. Play your part, get the role, then use your position to advocate for sanity.

Remember: the goal isn't to perpetuate the theater forever. It's to get inside the building where you can help others see that the emperor's distributed microservices architecture has no clothes.

The best revenge against interview theater isn't failing the performance, it's succeeding at it. Then you can spend your career deleting the unnecessary complexity it celebrated.

	

	
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google can keep its Chrome browser but will be barred from exclusive contracts]]></title>
            <link>https://www.cnbc.com/2025/09/02/google-antitrust-search-ruling.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45108548</guid>
            <description><![CDATA[The ruling comes nearly a year after a U.S. judge ruled that Google holds an illegal monopoly in its core market of internet search.]]></description>
            <content:encoded><![CDATA[Google CEO Sundar Pichai during the press conference after his meeting with Polish PM Donald Tusk at Google for Startups Campus In Warsaw in Warsaw, Poland on February 13, 2025. Images)Jakub Porzycki | Nurphoto | Getty ImagesAlphabet shares popped 8% in extended trading as investors celebrated what they viewed as minimal consequences from a historic defeat last year in the landmark antitrust case.Last year, Google was found to hold an illegal monopoly in its core market of internet search.U.S. District Judge Amit Mehta ruled against the most severe consequences that were proposed by the Department of Justice, including the forced sale of Google's Chrome browser, which provides data that helps its advertising business deliver targeted ads. "Google will not be required to divest Chrome; nor will the court include a contingent divestiture of the Android operating system in the final judgment," the decision stated. "Plaintiffs overreached in seeking forced divestiture of these key assets, which Google did not use to effect any illegal restraints."Mehta, who oversaw the remedies trial in May, ordered the parties to meet by Sept. 10 for the final judgment.In August 2024, the U.S. District Court for the District of Columbia ruled that Google violated Section 2 of the Sherman Act and held a monopoly in search and related advertising.The antitrust trial started in September 2023."Now the Court has imposed limits on how we distribute Google services, and will require us to share Search data with rivals," Google said in a blog post. "We have concerns about how these requirements will impact our users and their privacy, and we're reviewing the decision closely. The Court did recognize that divesting Chrome and Android would have gone beyond the case's focus on search distribution, and would have harmed consumers and our partners."Read more CNBC tech newsKlarna aims to raise up to $1.27 billion in U.S. IPOTesla asks for $243 million verdict to be tossed in fatal Autopilot crash suitAlibaba is developing a new AI chip — here's what we know so farMeta changes teen AI chatbot responses as Senate begins probe into 'romantic' conversationsOne of the key areas of focus was the exclusive contracts Google held for distribution.In his decision Tuesday, Mehta said the company can make payments to preload products, but it cannot have exclusive contracts that condition payments or licensing.The DOJ had asked Google to stop the practice of "compelled syndication," which refers to the practice of making certain deals with companies to ensure its search engine remains the default choice in browsers and smartphones."The court's ruling today recognizes the need for remedies that will pry open the market for general search services, which has been frozen in place for over a decade," the DOJ said in a press release. "The ruling also recognizes the need to prevent Google from using the same anticompetitive tactics for its GenAI products as it used to monopolize the search market, and the remedies will reach GenAI technologies and companies."Google pays Apple billions of dollars per year to be the default search engine on iPhones. It's lucrative for Apple and a valuable way for Google to get more search volume and users.Apple stock rose 4% on Tuesday after hours."Google will not be barred from making payments or offering other consideration to distribution partners for preloading or placement of Google Search, Chrome, or its GenAI products. Cutting off payments from Google almost certainly will impose substantial—in some cases, crippling—downstream harms to distribution partners, related markets, and consumers, which counsels against a broad payment ban."Google was also ordered to loosen its hold on search data.During the remedies trial in May, the DOJ asked the judge to force Google to share the data it uses for generating search results, such as data about what users click on.Mehta ruled Tuesday that Google will have to make available certain search index data and user interaction data, though "not ads data."Google does not have to share or provide access to granular data with advertisers.The court narrowed the datasets Google will be required to share and said they must occur on "ordinary commercial terms that are consistent with Google's current syndication services."Stock Chart IconStock chart iconGoogle and Apple one-day stock chart.watch now]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Making a Linux home server sleep on idle and wake on demand (2023)]]></title>
            <link>https://dgross.ca/blog/linux-home-server-auto-sleep</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45108066</guid>
            <description><![CDATA[Daniel P. Gross is a computer software and hardware professional based in Toronto.]]></description>
            <content:encoded><![CDATA[
        
  

  
    2023-04-16
      in
        linux,
        backup,
        networking,
        wireshark,
        ruby,
        wake-on-lan,
        efficiency,
        homelab
  

  It began with what seemed like a final mundane touch to my home server setup for hosting Time Machine backups: I wanted it to automatically sleep when idle and wake up again when needed. You know, sleep on idle — hasn’t Windows had that built in since like Windows 98? How hard could it be to configure on a modern Ubuntu install?

To be fair, I wanted more than just sleep on idle, I also wanted wake on request — and that second bit turns out to be the hard part. There were a bunch of dead ends, but I stuck out it to find something that “just works” without the need to manually turn on the server for every backup. Join me on the full adventure further down, or cut to the chase with the setup instructions below.



tl;dr


  
  
  
  
    
  
  Home Server PC- High power consumption!- Ubuntu Linux- Mostly sleeps, wakes up on demandWake-on-LAN: unicast packetsRaspberry Pi (or similar)- Low power consumption- Ubuntu Linux- Always-onSSHAFP...Network servicesNetwork servicesARP Stand-inAvahi...Time machine backupsARP queries for HomeServermDNS queries for Home Server


Outcome:

  Server automatically suspends to RAM when idle
  Server automatically wakes when needed by anything else on the network, including SSH, Time Machine backups, etc.


You’ll need:

  An always-on Linux device on the same network as your server, e.g. a Raspberry Pi
  A network interface device for your server that supports wake-on-LAN with unicast packets


On the server:

  Enable wake-on-LAN with unicast packets (not just magic packets), make it persistent



sudo ethtool -s eno1 wol ug
sudo tee /etc/networkd-dispatcher/configuring.d/wol << EOF
#!/usr/bin/env bash

ethtool -s eno1 wol ug || true
EOF
sudo chmod 755 /etc/networkd-dispatcher/configuring.d/wol



  Set up a cron job to sleep on idle (replace /home/ubuntu with your desired script location)



tee /home/ubuntu/auto-sleep.sh << EOF
#!/bin/bash
logged_in_count=$(who | wc -l)
# We expect 2 lines of output from `lsof -i:548` at idle: one for output headers, another for the 
# server listening for connections. More than 2 lines indicates inbound connection(s).
afp_connection_count=$(lsof -i:548 | wc -l)
if [[ $logged_in_count < 1 && $afp_connection_count < 3 ]]; then
  systemctl suspend
else
  echo "Not suspending, logged in users: $logged_in_count, connection count: $afp_connection_count"
fi
EOF
chmod +x /home/ubuntu/auto-sleep.sh
sudo crontab -e
# In the editor, add the following line:
*/10 * * * * /home/ubuntu/auto-sleep.sh | logger -t autosuspend



  Disable IPv6: this approach relies on ARP, which IPv6 doesn’t use



sudo nano /etc/default/grub
# Find GRUB_CMDLINE_LINUX=""
# Change to GRUB_CMDLINE_LINUX="ipv6.disable=1"
sudo update-grub
sudo reboot



  Optional: Configure network services (e.g. Netatalk) to stop before sleep to prevent unwanted wakeups due to network activity



sudo tee /etc/systemd/system/netatalk-sleep.service << EOF
[Unit]
Description=Netatalk sleep hook
Before=sleep.target
StopWhenUnneeded=yes

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=-/usr/bin/systemctl stop netatalk
ExecStop=-/usr/bin/systemctl start netatalk

[Install]
WantedBy=sleep.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable netatalk-sleep.service


On the always-on device:

  Install ARP Stand-in: a super simple Ruby script that runs as a system service and responds to ARP requests on behalf of another machine. Configure it to respond on behalf of the sleeping server.
  Optional: Configure Avahi to advertise network services on behalf of the server when it’s sleeping.



sudo apt install avahi-daemon
sudo tee /etc/systemd/system/avahi-publish.service << EOF
[Unit]
Description=Publish custom Avahi records
After=network.target avahi-daemon.service
Requires=avahi-daemon.service

[Service]
ExecStart=/usr/bin/avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable avahi-publish.service --now
systemctl status avahi-publish.service


Caveats

  The server’s network device needs to support wake-on-LAN from unicast packets
  To prevent unwanted wake-ups, you’ll need to ensure no device on the network is sending extraneous packets to the server


How I got there
First, a bit about my hardware, as this solution is somewhat hardware-dependent:

  HP ProDesk 600 G3 SFF
  CPU: Intel Core i5-7500
  Network adapter: Intel I219-LM


Sleeping on idle
I started with sleep-on-idle, which boiled down to two questions:


  How to determine if the server is idle or busy at any given moment
  How to automatically suspend to RAM after being idle for some time


Most of the guides I found for sleep-on-idle, like this one, were for Ubuntu Desktop — sleep-on-idle doesn’t seem to be something that’s commonly done with Ubuntu Server. I came across a few tools that looked promising, the most notable being circadian. In general, though, there didn’t seem to be a standard/best-practice way to do it, so I decided I’d roll it myself the simplest way I could.

Determining idle/busy state
I asked myself what server activity would constitute being busy, and landed on two things:

  Logged in SSH sessions
  In-progress Time Machine backups


Choosing corresponding metrics was pretty straightforward:

  Count of logged in users, using who
  Count of connections on the AFP port (548), using lsof (I’m using AFP for Time Machine network shares)


For both metrics, I noted the values first at idle, and then again when the server was busy.

Automatically suspending to RAM
To keep things simple, I opted for a cron job that triggers a bash script — check out the final version shared above. So far it’s worked fine; if I ever need to account for more metrics in detecting idle state, I’ll consider using a more sophisticated option like circadian.

Waking on request
With sleep-on-idle out of the way, I moved on to figuring out how the server would wake on demand.

Could the machine be configured to automatically wake upon receiving a network request? I knew Wake-on-LAN supported waking a computer up using a specially crafted “magic packet”, and it was straightforward to get this working. The question was if a regular, non-“magic packet” could somehow do the same thing.

Wake on PHY?
Some online searching yielded a superuser discussion that looked particularly promising. It pointed to the man page for ethtool, the Linux utility used to configure network hardware. It shared ethtool’s complete wake-on-LAN configuration options:
wol p|u|m|b|a|g|s|f|d...
      Sets Wake-on-LAN options.  Not all devices support
      this.  The argument to this option is a string of
      characters specifying which options to enable.

      p   Wake on PHY activity
      u   Wake on unicast messages
      m   Wake on multicast messages
      b   Wake on broadcast messages
      a   Wake on ARP
      g   Wake on MagicPacket™
      s   Enable SecureOn™ password for MagicPacket™
      f   Wake on filter(s)
      d   Disable (wake on nothing).  This option
          clears all previous options.


It pointed in particular to the Wake on PHY activity option, which seemed perfect for this use-case. It seemed to mean that any packet sent to the network interface’s MAC address would wake it. I enabled the flag using ethtool, manually put the machine to sleep, then tried logging back in using SSH and sending pings. No dice: the machine remained asleep despite multiple attempts. So much for that 😕

Breakthrough: wake on unicast
None of ethtool’s other wake-on-LAN options seemed relevant, but some more searching pointed to the Wake on unicast messages as another option to try. I enabled the flag using ethtool, manually put the machine to sleep, then tried logging back in using SSH. Bingo! This time, the machine woke up. 🙌 With that, I figured I was done.

Not so fast — there were two problems:

  Sometimes, the server would wake up without any network activity that I knew of
  Some period of time after the server went to sleep, it would become impossible to wake it again using network activity other than a magic packet


A closer look at the same superuser discussion above revealed exactly the reason for the second problem: shortly after going to sleep, the machine was effectively disappearing from the network because it was no longer responding to ARP requests.

ARP
So the cached ARP entry for other machines on the network was expiring, meaning that they had no way to resolve the server’s IP address to its MAC address. In other words, an attempt to ping my server at 192.168.1.2 was failing to even send a packet to the server, because the server’s MAC address wasn’t known. Without a packet being sent, there was no way that server was going to wake up.

Static ARP?
My first reaction: let’s manually create ARP cache entries on each network client. This is indeed possible on macOS using:
sudo arp -s [IP address] [MAC address]


But it also didn’t meet the goal of having things “just work”: I was not interested in creating static ARP cache entries on each machine that would be accessing the server. On to other options.

ARP protocol offload?
Some more searching revealed something interesting: this problem had already been solved long ago in the Windows world.

It was called ARP protocol offload, and it goes like this:

  The network hardware is capable of responding to ARP requests independently of the CPU
  Before going to sleep, the OS configures the network hardware to respond to ARP requests
  While sleeping, the network hardware responds to ARP requests on its own, without waking the rest of the machine to use the CPU


Voila, this was exactly what I needed. I even looked at the datasheet for my network hardware, which lists ARP Offload as a feature on the front page.

The only problem? No Linux support. I searched the far reaches of the internet, then finally dug into the Linux driver source code to find that ARP offload isn’t supported by the Linux driver. This was when I briefly pondered trying to patch the driver to add ARP offload… before reminding myself that successfully patching Linux driver code is far beyond what I could hope to achieve in a little free-time project like this one. (Though maybe one day…)

Other solutions using magic packets
Some more searching led me to some other clever and elaborate solutions involving magic packets. The basic idea was to automate sending magic packets. One solution (wake-on-arp) listens for ARP requests to a specified host to trigger sending a magic packet to that host. Another solution implements a web interface and Home Assistant integration to enable triggering a magic packet from a smartphone web browser. These are impressive, but I wanted something simpler that didn’t require manually waking up the server.

I considered a few other options, but abandoned them because they felt too complex and prone to breaking:

  Writing a script to send a magic packet and then immediately trigger a Time Machine backup using tmutil. The script would need to be manually installed and scheduled to run periodically on each Mac.
  Using HAProxy to proxy all relevant network traffic through the Raspberry Pi and using a hook to send a magic packet to the server on activity.


Breakthrough: ARP Stand-in
What I was attempting didn’t seem much different from the static IP mapping that’s routinely configured on home routers, except that it was for DHCP instead of ARP. Was there no way to make my router do the same thing for ARP?

Some more digging into the ARP protocol revealed that ARP resolution doesn’t even require a specific, authoritative host to answer requests — any other network device can respond to ARP requests. In other words, my router didn’t need to be the one resolving ARP requests, it could be anything. Now how could I just set up something to respond on behalf of the sleeping server?

Here’s what I was trying to do:


  
  
  
    
  
  Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...1Multicast ARP: What's 192.168.1.3'sMAC address?ARP: 192.168.1.3 is atAA:BB:CC:DD:EE Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...2Unicast TCP packet to port 22 on AA:BB:CC:DD:EE Home server PCMAC Address: AA:BB:CC:DD:EEIP Address:   192.168.1.3SSHAFP...Raspberry PiMAC Address: ZZ:YY:XX:WW:VVIP Address:   192.168.1.2ARP Stand-inAvahi...3Communication continues normallyUnicast packet triggers wakeupStarts SSH sessionto home server123

I thought it must be possible to implement as a Linux network configuration, but the closest thing I found was Proxy ARP, which accomplished a different goal. So I went one level deeper, to network programming.

Now, how to go about listening for ARP request packets? This is apparently possible to do using a raw socket, but I also knew that tcpdump and Wireshark were capable of using filters to capture only packets of a given type. That led me to look into libpcap, the library that powers both of those tools. I learned that using libpcap had a clear advantage over a raw socket: libpcap implements very efficient filtering directly in the kernel, whereas a raw socket would require manual packet filtering in user space, which is less performant.

Aiming to keep things simple, I decided to try writing the solution in Ruby, which led me to the pcaprub Ruby bindings for libpcap. From there, I just needed to figure out what filter to use with libpcap. Some research and trial/error yielded this filter:

arp and arp[6:2] == 1 and arp[24:4] == [IP address converted to hex]


For example, using a target IP address of 192.168.1.2:

arp and arp[6:2] == 1 and arp[24:4] == 0xc0a80102


Let’s break this down, using the ARP packet structure definition for byte offets and lengths:

  arp — ARP packets
  arp[6:2] == 1 — ARP request packets. [6:2] means “the 2 bytes found at byte offset 6”.
  arp[24:4] == [IP address converted to hex] — ARP packets with the specified target address. [24:4] means “the 4 bytes found at byte offset 24”.


The rest is pretty straightforward and the whole solution comes out to only ~50 lines of Ruby code. In short, arp_standin is a daemon that does the following:


  Starts up, taking these configuration options:
    
      IP and MAC address of the machine it’s standing in for (the “target”)
      Network interface to operate on
    
  
  Listens for ARP requests for the target’s IP address
  On detecting an ARP request for the target’s IP address, responds with the target’s MAC address


Since the server’s IP → MAC address mapping is defined statically through the arp_standin daemon’s configuration, it doesn’t matter if the Raspberry Pi’s ARP cache entry for the server is expired.

Check out the link below to install it or explore the source code further:


  
  danielpgross/arp_standin on GitHub


ARP is used in IPv4 and is replaced by Neighbor Discovery Protocol (NDP) in IPv6. I don’t have any need for IPv6 right now, so I disabled IPv6 entirely on the server using the steps shown above. It should be possible to add support for Neighbor Discovery to the ARP-Standin service as a future enhancement.

With the new service running on my Raspberry Pi, I used Wireshark to confirm that ARP requests being sent to the server were triggering responses from the ARP Stand-in. It worked 🎉 — things were looking promising.

Getting it all working
The big pieces were in place:

  the server went to sleep after becoming idle
  the server could wake up from unicast packets
  other machines could resolve the server’s MAC address using ARP, long after it went to sleep


With the ARP Stand-in running, I turned on the server and ran a backup from my computer. When the backup was finished, the server went to sleep automatically. But there was a problem: the server was waking up immediately after going to sleep.

Unwanted wake-ups

First thing I checked was the Linux system logs, but these didn’t prove too helpful, since they didn’t show what network packet actually triggered the wakeup. Wireshark/tcpdump were no help here either, because they wouldn’t be running when the computer was sleeping. That’s when I thought to use port mirroring: capturing packets from an intermediary device between the server and the rest of the network. After a brief, unsuccessful attempt to repurpose an extra router running OpenWRT, a search for the least expensive network switch with port mirroring support yielded the TP-Link TL-SG105E for ~$30.


  
  TL-SG105E: a simple, inexpensive switch with port-mirroring support


With the switch connected and port mirroring enabled, I started capturing with Wireshark and the culprits immediately became clear:


  My Mac, which was configured to use the server as a Time Machine backup host using AFP, was sending AFP packets to the server after it had gone to sleep
  My Netgear R7000, acting as a wireless access point, was sending frequent, unsolicited NetBIOS NBTSTAT queries to the server


Eliminating AFP packets
I had a hunch about why the Mac was sending these packets:

  The Mac mounted the AFP share to perform a Time Machine backup
  The Time Machine backup finished, but the share remained mounted
  The Mac was checking on the status of the share periodically, as would be done normally for a mounted network share


I also had a corresponding hunch that the solution would be to make sure the share got unmounted before the server went to sleep, so that the Mac would no longer ping the server for its status afterwards. I figured that shutting down the AFP service would trigger unmounting of shares on all its clients, achieving the goal. Now I just needed to ensure the service would shut down when the server was going to sleep, then start again when it woke back up.

Fortunately, systemd supports exactly that, and relatively easily — I defined a dedicated systemd service to hook into sleep/wake events (check out the configuration shared above). A Wireshark capture confirmed that it did the trick.

Eliminating NetBIOS packets
This one proved to be harder, because the packets were unsolicited — they seemed random and unrelated to any activity being done by the server. I thought they might be related to Samba services running on the server, but the packets persisted even after I completely removed Samba from the server.

Why was my network router sending NetBIOS requests, anyway? Turns out that Netgear routers have a feature called ReadySHARE for sharing USB devices over the network using the SMB protocol. Presumably, the router firmware uses Samba behind the scenes, which uses NetBIOS queries to build and maintain its own representation of NetBIOS hosts on the network. Easy — turn off ReadySHARE, right? Nope, there’s no way to do that in Netgear’s stock firmware 😒.

That led me to take the plunge and flash the router with open-source FreshTomato firmware. I’m glad I did, because the firmware is much better than the stock one anyway, and it immediately stopped the unwanted NetBIOS packets.

Time Machine not triggering wake-up
I was getting close now: the server remained asleep, and I could reliably wake it up by logging in with SSH, even long after it went to sleep.

This was great, but one thing wasn’t working: when starting a backup on my Mac, Time Machine would show a loading state indefinitely with Connecting to backup disk... and eventually give up. Was the server failing to wake up from packets the Mac was sending, or was the Mac not sending packets at all?


  


A port-mirrored Wireshark capture answered that question: the Mac wasn’t sending any packets to the server, even long after it started to say Connecting to backup disk.... Digging into the macOS Time Machine logs with:

log show --style syslog --predicate 'senderImagePath contains[cd] "TimeMachine"' --info


A few entries made it clear:
(TimeMachine) [com.apple.TimeMachine:Mounting] Attempting to mount 'afp://backup_mbp@homeserver._afpovertcp._tcp.local./tm_mbp'
...
(TimeMachine) [com.apple.TimeMachine:General] Failed to resolve CFNetServiceRef with name = homeserver type = _afpovertcp._tcp. domain = local.


The Mac was using mDNS (a.k.a. Bonjour, Zeroconf) to resolve the backup server’s IP address using its hostname. The server was asleep and therefore not responding to the requests, so the Mac was failing to resolve its IP address. This explained why the Mac wasn’t sending any packets to the server, leaving it asleep.

mDNS stand-in
I already had an ARP stand-in service, now I needed my Raspberry Pi to also respond to mDNS queries for the server while it slept. I knew that Avahi was one of the main mDNS implementations for Linux. I first tried these instructions using .service files to configure my Raspberry Pi to respond to mDNS queries on behalf of the server. I used the following on the Mac to check the result:
dns-sd -L homeserver _afpovertcp._tcp local


For some reason, that approach just didn’t work; Avahi didn’t respond on behalf of the server. I experimented instead with avahi-publish (man page), which (to my pleasant surprise) worked right away using the following:
avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local


With that, I just needed to create a systemd service definition that would automatically run the avahi-publish command on boot (check out the configuration shared above).

🏁 Finish
With all the wrinkles ironed out, everything has been working well now for over a month. I hope you’ve enjoyed following along and that this approach works for you too.

This post was discussed on Hacker News and Reddit.



      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A staff engineer's journey with Claude Code]]></title>
            <link>https://www.sanity.io/blog/first-attempt-will-be-95-garbage</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107962</guid>
            <description><![CDATA[This started as an internal Sanity workshop where I demoed how I actually use AI. Spoiler: it's running multiple agents like a small team with daily amnesia.]]></description>
            <content:encoded><![CDATA[First attempt will be 95% garbage: A staff engineer's 6-week journey with Claude Code | Sanity]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[iNaturalist keeps full species classification models private]]></title>
            <link>https://github.com/inaturalist/inatVisionAPI</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107939</guid>
            <description><![CDATA[Contribute to inaturalist/inatVisionAPI development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[computervision
We're doing some computer vision stuff at iNat.
models
iNaturalist makes a subset of its machine learning models publicly available while keeping full species classification models private due to intellectual property considerations and organizational policy. We provide “small” models trained on approximately 500 taxa, including taxonomy files and a geographic model, which are suitable for on-device testing and other applications. Additionally, researchers have independently developed and released open-source models based on iNaturalist data, which can be found in various model distribution venues (for example Hugging Face or Kaggle).
os x dependencies

brew install libmagic

python

python3 -m venv venv
source ./venv/bin/activate
pip3 install -U pip
pip3 install -r requirements.txt

installation
Here's a rough script for OS X assuming you already have homebrew, Python, and virtualenv installed.
# Get dependencies
brew install libmagic

# Get the repo
git clone git@github.com:inaturalist/inatVisionAPI.git
cd inatVisionAPI/

# Set up your python environment
python3 -m venv venv
source venv/bin/activate
pip3 install -U pip
pip3 install -r requirements.txt

# Copy your config file (and edit, of course)
cp config.yml.example config.yml

# Run the app
python app.py

Now you should be able to test at http://localhost:6006 via the browser.
Notes
If the device you're installing on has AVX extensions (check flags in /proc/cpuinfo), try compiling tensorflow for better performance:
https://www.tensorflow.org/install/install_sources
This is a good idea on AWS or bare metal, but won't make a difference on Rackspace due to them using an old hypervisor.
If you're not compiling, install tensorflow from pip: pip install tensorflow
If the device you're installing on has AVX2 or SSE4, install pillow-simd for faster image resizing:
pip install pillow-simd if you only have SSE4, or CC="cc -mavx2" pip install pillow-simd if you have AVX2. I saw a significant increase in performance from pillow to pillow-simd with SSE4, less of an increase for AVX2.
otherwise, install pillow from pip: pip install pillow
tensorflow seems to want to compile against your system copy of numpy on OS X regardless of the virtualenv, so if you see stupid errors like ImportError: numpy.core.multiarray failed to import, try running deactivate to get out the virtualenv, then pip install -U numpy or somesuch to update your system copy of numpy. Then source inatvision-venv/bin/activate to get back in your virtualend and try again.
Some performance data from a 15" MBP, 2.5GHz i7:



task
pip tensorflow
compiled tensorflow
compiled tensorflow + pillow-simd




100x medium.jpg
25 seconds
17 seconds
15 seconds


100x iphone photos
81 seconds
72 seconds
46 seconds



The larger the images coming into the pipeline, the more important optimized resize (like pillow-simd) is.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Amazon must face US nationwide class action over third-party sales]]></title>
            <link>https://www.reuters.com/legal/government/amazon-must-face-us-nationwide-class-action-over-third-party-sales-2025-09-02/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107891</guid>
        </item>
        <item>
            <title><![CDATA[Civics is boring, so, let's encrypt something (2024)]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3703126</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107505</guid>
            <content:encoded><![CDATA[
	
		
		

	





		The Bike Shed
	

December 2, 2024Volume 22, issue 5 


  
	
			
				
				PDF
			
		



 
   
  IT professionals can either passively suffer political solutions or participate in the process to achieve something better. 
  Poul-Henning Kamp 
  It's a common trope in entertainment for some character to deliver a nonlinear response to something seemingly trivial, only for that to later prove to have been a vitally important clue. So, that room the janitor won't let anybody into? Right, that isn't actually a storage closet, but instead it's the Portal to Hell. Governments have a quirk like that in the sense that you can get away with a lot of crap—in particular, if it looks like it might benefit the economy—But Nobody Messes with Fundamental Human Rights, OK? 
  As I write this, the founder of the encrypted communication service Telegram is under arrest in France. And, depending on where you get your news, he's either a freedom fighter subject to political persecution or a criminal mastermind getting his due. He probably is a bit of both, but he's under arrest now because he messed with the Fundamental Human Rights of people in France. 
  I'll spare you a long civics lesson, but I will provide two important clues to figure out what is going on with politicians and encryption right now. First, when legislators write laws to protect human rights, they decide who has to take responsibility for the problem, and what happens if they fail to lift the burden. So, if you're present when somebody falls off a ladder, the law has made it your problem to try to save that person's life. If you witness a crime, the law has made it your problem to tell the truth about what you saw in court. Similarly, if you publish something that somebody else wrote, the law makes you responsible for ensuring it doesn't endanger national security. 
  Second clue: Judges are superusers. To perform their job, which is to correct wrongs, judges are empowered to write court orders that sanction otherwise illegal violations of human rights. So, a judge who is convinced you're about to kill somebody can unleash the police to follow you everywhere in hopes of preventing that crime. Similarly, a judge who thinks your computer system contains information related to financial crimes can allow the police to hack that system. Likewise, a judge who thinks you're stalking your ex can order you to stay out of a certain part of town. And if there doesn't seem to be any other way to keep you from harming somebody else's human rights, you can be jailed. 
  Then, should you fail to comply with a court order, that's considered contempt of court and can be addressed with punishments far more severe than most people imagine, since court orders are deemed to be crucially important to the maintenance of law and order. What's more, a judge who becomes convinced you are planning a crime or human-rights violation—or have participated in one—can order that the privacy of your communications be violated as part of a search for evidence. 
  The problem for law enforcement in all this is that modern computer-aided encryption is fast, effortless, omnipresent, and unbreakable, thus negating many of these efforts. This is the frustration law-enforcement types are referring to whenever they complain about "criminals going dark." It's also what leads some politicians to say silly things about "banning encryption." 
  It's not as if people didn't communicate in code previously, if only to save on telco expenses. But this used to be slow, bothersome, and error prone, which limited usage and left law enforcement with places to insert the knife—so it was somewhat tolerated. 
  IT libertarians have gone so far as to set up "offshore" services that employ encryption specifically designed to make it impossible for anyone to comply with a court order. So, because the Internet is global, now even petty criminals in Hoople, North Dakota, can effortlessly prevent judges from employing their superuser privileges. 
  This is a direct, in-your-face challenge to any state that considers itself to be a nation built on laws. Predictably, a response delivered with all due force is certain to come. The United Nations' new "cybercrime" treaty, readied for signatures at the time of this writing, is very much focused on how to get court orders to work quickly and efficiently across borders. Bear in mind that international bodies don't fashion treaties like this unless they think an urgent response is vital. 
  Which means we, as IT professionals, now have a choice to make. We can either sit by passively and suffer the consequences of whatever ill-conceived solution the politicians cook up for us, or we can participate in the process in hopes of achieving a less awful solution. 
  In terms of what might be done in that way, here's one straw-man proposal to consider. 
  First, we provide legislators with the essential technical tools. 
  We can make it possible for one side of a TLS protocol negotiation to declare, "I'll deal with court orders related to this communication," in such a way that law enforcement can find out where to send the court order for their wiretap without learning more than they already know. 
  Moreover, parties to a TLS connection should be able to insist that the session key starts with a certain number of zero bits. If the other party thinks that isn't good enough, the TLS handshake fails. 
  Then the legislators can get to work. First, they'll need to make it a crime to force or trick anyone into using stronger encryption than they consent to, no matter how that might be done. (Note that IT liberalists who claim encryption is a human right never realize this should also include the right not to be forced to use encryption against one's will.) 
  Second, they'll need to lay out what it takes for an attestation to handle court orders to be validated—along with the consequences for noncompliance. This will probably be something along the lines of: "The attestation must be signed with Interpol's or XYZ government's certificate." 
  Third, it will need to be legislated that, if the other end attested to handling court orders or if the session key requires fewer than N bits to brute force, you will not be subject to any adverse treatment for using encryption. (N is a political choice since the hardware that law enforcement will need in order to brute-force the N bits will be paid for out of your taxes. Don't argue here; take it up with your politicians.) 
  Then, fourth and finally (drum roll, please!), they'll need to allow courts to jail the accused until: (a) the communication has been decrypted by someone; (b) the maximum penalty for the charged crime has been exceeded; or (c) the court decides to release the accused. 
  Following a bit of implementation work, your browser or mobile phone will then work as follows: 
  You'll configure your jurisdiction—for example, USA, EU, or China—so that the browser will know how to validate attestations from the other end. 
  Whenever you connect to a site that attests, you'll be able to use any kind of encryption with any key size, and since almost all commercial sites, such as your bank, already are legally required to keep records and respond to court orders, they'll have no trouble attesting. 
  Should you contact a site that does not attest—be it Crimes R Us in Elbonia or your Homeowner Association's "50 Rules for Appropriate Lawn Maintenance," your browser will keep you out of jail by refusing to use a session key longer than the N legal bits. 
  If for some reason, however, you think that isn't nearly enough encryption, you'll also be at liberty to go into your browser settings to select whatever session key size you are willing to use—provided, of course, that the other end accepts that as well. 
  The slider should probably be graduated in units of time, days, weeks, months, and years since what you're really setting is the length of time you're willing to rot in jail while refusing to comply with a court order. 
  It goes without saying that you'll suffer no ill consequences even if you set the slider to "eternity," provided you keep a logfile of all your session keys and then hand them over whenever a court order demands it. Just make sure you don't lose that file. 
  Companies can also set up client-side proxies that attest to handling court orders and insist upon proper session key sizes, according to company policy, so their employees won't even have to think about it. 
  Which is to say that this straw-man proposal, in theory, ought to make everybody happy. What's not to like? Law enforcement will have ways to gain access to communications, provided they can convince a judge it's necessary. All important communications will be able to continue using the same strength of encryption they use today. Communications that didn't require encryption in the first place, like that HOA guide to proper lawn maintenance, will be able to employ sufficient encryption to prevent trivial wiretapping, but nothing strong enough to prevent brute-force access should a judge decide that's necessary. And if legislators think that too much or too little encryption is being brute-forced, they can always revise the law to change N. 
  IT libertarians, meanwhile, will have the freedom to encrypt any way they please, and they can even throw away their session keys if they so choose, but they won't be able to force anyone else to do so. If they try, they'll have to stand up in court for it—just like that IT libertarian who's currently in French custody. 
  In reality, I expect that law enforcement will demand more access and that IT libertarians will consider any kind of compromise to be treasonous. So, no, I do not expect my proposed compromise has any chance of adoption whatsoever. 
  But, then, don't tell me 10 or 20 years from now that we didn't have any other options. 
   
  Poul-Henning Kamp has haunted the Unix world for 40 years and written a lot of widely used open-source software, including bits of FreeBSD and the Varnish HTTP Cache. Living in Denmark with his wife, two cats, and three lawn-mower robots, he remains unconvinced that an older/wiser correlation exists. 
  Copyright © 2024 held by owner/author. Publication rights licensed to ACM.  
  

	
	
		
	
	Originally published in Queue vol. 22, no. 5—
 	
	Comment on this article in the ACM Digital Library
	
































More related articles:

	  
	  Jinnan Guo, Peter Pietzuch, Andrew Paverd, Kapil Vaswani - Trustworthy AI using Confidential Federated Learning
	  
	  The principles of security, privacy, accountability, transparency, and fairness are the cornerstones of modern AI regulations. Classic FL was designed with a strong emphasis on security and privacy, at the cost of transparency and accountability. CFL addresses this gap with a careful combination of FL with TEEs and commitments. In addition, CFL brings other desirable security properties, such as code-based access control, model confidentiality, and protection of models during inference. Recent advances in confidential computing such as confidential containers and confidential GPUs mean that existing FL frameworks can be extended seamlessly to support CFL with low overheads.
	  
	  

	  
	  Raluca Ada Popa - Confidential Computing or Cryptographic Computing?
	  
	  Secure computation via MPC/homomorphic encryption versus hardware enclaves presents tradeoffs involving deployment, security, and performance. Regarding performance, it matters a lot which workload you have in mind. For simple workloads such as simple summations, low-degree polynomials, or simple machine-learning tasks, both approaches can be ready to use in practice, but for rich computations such as complex SQL analytics or training large machine-learning models, only the hardware enclave approach is at this moment practical enough for many real-world deployment scenarios.
	  
	  

	  
	  Matthew A. Johnson, Stavros Volos, Ken Gordon, Sean T. Allen, Christoph M. Wintersteiger, Sylvan Clebsch, John Starks, Manuel Costa - Confidential Container Groups
	  
	  The experiments presented here demonstrate that Parma, the architecture that drives confidential containers on Azure container instances, adds less than one percent additional performance overhead beyond that added by the underlying TEE. Importantly, Parma ensures a security invariant over all reachable states of the container group rooted in the attestation report. This allows external third parties to communicate securely with containers, enabling a wide range of containerized workflows that require confidential access to secure data. Companies obtain the advantages of running their most confidential workflows in the cloud without having to compromise on their security requirements.
	  
	  

	  
	  Charles Garcia-Tobin, Mark Knight - Elevating Security with Arm CCA
	  
	  Confidential computing has great potential to improve the security of general-purpose computing platforms by taking supervisory systems out of the TCB, thereby reducing the size of the TCB, the attack surface, and the attack vectors that security architects must consider. Confidential computing requires innovations in platform hardware and software, but these have the potential to enable greater trust in computing, especially on devices that are owned or controlled by third parties. Early consumers of confidential computing will need to make their own decisions about the platforms they choose to trust.
	  
	  










	
	
	
	© ACM, Inc. All Rights Reserved.
	

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Amber – better Beeper, a modern all-in-one messenger]]></title>
            <link>https://useamber.app/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45107364</guid>
            <description><![CDATA[Respond to your messages 2x as fast with more context than ever. Use WhatsApp, Telegram, and iMessage enabled by AI. Split inboxes. Message scheduling. Reminders. Personal CRM.]]></description>
            <content:encoded><![CDATA[We spend hours on messages. Yet we often reply late, sometimes completely forget to reply. We then end up losing deals, opportunities for connection, and missing connections.It's not anybody's fault. Messaging itself has not changed a decade – it has just gotten messier. Our conversations are scattered across different social networks with distinct UI full of distractions. Finding the right thread takes minutes. The context and the small details are forgotten.Meanwhile, some people manage thousands of relationships and only grow stronger connections. What’s their secret?John D. Rockefeller had a rolodex filled with contact details of 100,000 people. Marlon Brando, Holly Solomon, Akio Morita, David Ogilvy – each relied on personal CRMs and skilled assistants to maintain and deepen their networks. They had systems. They had tools.With the arrival of the latest AI models and well-crafted software, these capabilities are now within everyone’s reach.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vijaye Raji to become CTO of Applications with acquisition of Statsig]]></title>
            <link>https://openai.com/index/vijaye-raji-to-become-cto-of-applications-with-acquisition-of-statsig/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106981</guid>
        </item>
        <item>
            <title><![CDATA[Physically based rendering from first principles]]></title>
            <link>https://imadr.me/pbr/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106846</guid>
            <description><![CDATA[In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.]]></description>
            <content:encoded><![CDATA[
    
    
    In this interactive article, we will explore the physical phenomena that create light and the fundamental laws governing its interaction with matter. We will learn how our human eyes capture light and how our brains interpret it as visual information. We will then model approximations of these physical interactions and learn how to create physically realistic renderings of various materials.
    Chapter 1: What is light?
    We are all familiar with light: it’s the thing that allows us to see the world, distinguish colors and textures, and keeps the universe from being a dark, lifeless void. But precisely defining what light is has proven to be a tricky question.Throughout history, many philosophers (and later, physicists) studied light in an effort to demystify its nature. Some ancient Greeks considered it to be one of the four fundamental elements that composed the universe: beams of fire emanating from our eyes.
    Descartes proposed that light behaved like waves, while Newton thought that it consisted of tiny particles of matter called corpuscles.

    Each of these more or less scientific theories explained some aspects of light's behavior, but none could account for all of them in a single, unified framework. That was until the 1920s when physicists came up with quantum electrodynamics. This theory is, as of now, the most accurate way to describe every interaction of light and matter.

    You can hover the diagram below to see which light's phenomena can be explained using each model:


    
            Quantum Optics
            
                Electromagnetic Optics
                
                    Wave Optics
                    
                        Ray Optics
                    
                
            
        
    
        
            Reflection / Refraction / Transmission
            Diffraction
            Interference
            Polarization
            Dispersion
            Fluorescence
            Phosphorescence
        
    

    

    For the purpose of computer graphics, the ray optics model is accurate enough at simulating light interactions. But for the sake of scientific curiosity, we will explore some aspects of the other models, starting with electromagnetism.

    The Electric force
    One of the fundamental properties of matter is the electric charge, and it comes in two types: positive and negative.Charges determine how particles interact: charges of the same type repel each other, while opposite charges attract.
    The amount of force affecting two charged particles is calculated using Coulomb's law:
    


    Where  is a constant,   and  are the quantities of each charge, and  is the distance between them.

    You can drag around these charges to see how the electric force affects them:
    
    Every charge contributes to the electric field, it represents the force exerted on other charges at each point in space. We can visualize the electric field with a  or a  :
    
    
    Another way to visualize the electric field is by coloring each point in space with a color gradient representing the force experienced by a small charge at that point:
    

    Special relativity and magnetism
    Imagine a moving object carrying a positive electric charge placed under a cable carrying an electrical current.
    From , the object and the negative charges in the wire are moving, and since the positive and negative charges in the cable compensate each other, the object doesn't experience any force.
    In the , it appears to be static alongside the negative charges, while the positive charges are moving to the left, and the object still doesn't get affected by any force.
    Now if we take into account , the moving charges in the wire appear "stretched" due to relativistic effects, causing a change in the distribution of charges. This stretching leads to a repulsive force between the object and the wire, which we interpret as magnetism.
    
    
    Maxwell's equations
    Maxwell's equations describe how electric and magnetic fields are created and interact with each other. We will focus on the third and fourth equations.
    Maxwell's third equation, known as Faraday's law of induction, shows how changing magnetic fields can generate electric currents.An example of this is moving a magnet inside a coil, which induces an electric current in the wire due to the changing magnetic field.
    This is the principle behind electric generators: Mechanical energy (like the flow of steam) is used to move magnets inside coils (a turbine), converting it to electrical energy through electromagnetic induction.
    By moving the magnet left and right, we can see the voltmeter picking up a current and the electric charges in the coil moving back and forth:
    
    
        Show magnetic field
            
        
        Slide the magnet
        
    

    
    Maxwell's fourth and final equation, Ampère's Law, illustrates how electric currents (moving charges) produce magnetic fields around them. This is the basis of how electromagnets function:
    
    
        Voltage: 0 volts
        
    
    
    Together, these laws demonstrate how electric and magnetic fields are interdependent. A changing magnetic field generates an electric field, and a changing electric field generates a magnetic field.
    This continuous cycle enables self-sustaining, self-propagating electromagnetic waves, which can travel through space without requiring a medium.
    Electromagnetic radiation
    Electromagnetic radiation consists of waves created by synchronized oscillations of electric and magnetic fields. These waves travel at the speed of light in a vacuum.
    The amplitude of a wave determines the maximum strength of its electric or magnetic field. It represents the wave's intensity or "brightness". In quantum terms, a higher amplitude corresponds to a greater number of photons.
    The frequency of a wave determines the energy of the individual photons that compose it. Higher frequencies correspond to shorter wavelengths and more energetic photons.
    
    Amplitude
    
    Frequency
    

    When the wavelength falls between approximately 400 nm and 700 nm, the human eye perceives it as visible light.
    While other wavelengths are invisible to the human eye, many are quite familiar in everyday life.
For example, microwaves are used for Wi-Fi and cooking, X-rays are used in medical imaging, and radio waves enable communication.
Some insects, like bees, can see ultraviolet light, which helps them locate flowers by revealing hidden patterns and markings created by specialized pigments, such as flavonoids, that reflect UV wavelengths.
On the other end of the spectrum, gamma rays are highly energetic and can be dangerous, they are generated by radioactive decay, nuclear bombs, and space phenomena like supernovas.
    
    Frequency
    

    Generating Light
    There are many ways for light to be generated, the two most common ones we encounter everyday are incandescence and electroluminescence.

    Incandescence is the process by which a material emits visible light due to high temperature. It is how incandescent lightbulbs and the sun generates light.
    An incandescent lightbulb produces light through the heating of a filament until it starts glowing. The filament is made of tungsten, an element with a high melting point, high durability, and a positive temperature coefficient of resistance, which means its resistance increases with temperature.

    When we increase the current flowing through the filament, it starts heating up (Due to Joule heating), which increases the resistance in turn causing more heat to get dissipated. This feedback loop stabilizes at around 2500°C.

    This heat makes the electrons in the filament wiggle and collide with each other, releasing photons in the process. This radiation can be approximated as Black-body radiation.



Voltage


The Sun also generates light by incandescence, but unlike the lightbulb's filament glowing via Joule heating, the Sun’s energy is produced by nuclear fusion in the core, where hydrogen nuclei fuse to form helium and release photons as gamma rays.These photons travel from the core through the radiative zone, getting absorbed and remitted countless times while shifting to longer wavelengths. After hundreds of thousands of years of bouncing around, the photons make it to the surface of the Sun, called the photosphere, where they get radiated away.
Most (~49%) of the sun's emissions are in infrared, which is responsible for the heat we get on Earth, ~43% is visible light and the ~8% left is ultraviolet.

An interesting fact is that illustrations of the Sun's cross-section typically depict the interior with bright orange or yellow colors. However, if we could actually see a cross-section of the Sun, even the hottest regions like the core would appear dark and opaque, because the radiation generated there isn't in the visible spectrum.





Another way to generate light is by electroluminescence, this is the phenomenon that powers LEDs

The main component of a light-emitting diode is a semiconductor chip. Semiconductors are materials whose electrical conductivity can be modified by mixing them with impurities in a process known as doping.

Depending on the type of impurity (called the dopant) used in the mix, the semiconductor can be turned into either an n-type, which has extra electrons freely moving around, or a p-type, which has a lack of electrons and instead carrying an electron "hole", also moving around and acting as a positive charge.

When you stick a p-type and an n-type semiconductor side by side, they form a p-n junction. When a current flows through the junction, the electrons and the holes recombine and emit photons in the process.


Aside from incandescence and electroluminescence, which are the two most common sources of light we encounter in everyday life, light can come from other places. Some materials glow when exposed to ultraviolet radiation, others absorb that radiation and re-emit it after some time. Some animals like fireflies use special enzymes to produce light. You can read this page to learn more about other sources of luminescence.

Chapter 2: Abstracting Away

In the previous chapter, we examined the nature of light and the various methods by which it can be emitted, we will now focus on how it interacts with matter.

When a photon hits a material, it interacts with the electrons in the atoms and molecules of that material, then two things can happen, it can either be absorbed or scattered.

The electrons occupy atomic orbitals: regions around the nucleus of the atom where an electron is most likely to be found. A higher orbital corresponds to a higher energy level of the electron.


If the photon has the energy needed to excite the electron to a higher energy level, the photon can be absorbed. Eventually the electron returns to a lower level and releases the energy as heat.

If the photon does not get absorbed, its electric field will make the electrons oscillate in return and generate secondary waves that interfere constructively and destructively with the photon waves in complicated ways.

We can simplify these complicated interactions by making a few assumptions about the material:

    The material is homogeneous, as in the material has the same properties everywhere
    The material is a perfectly smooth surface

We can use Maxwell's equations to show that such a perfect flat material splits the incoming light waves into two parts: reflected and refracted.

The angle of reflection is equal to the angle of incidence relative to the normal of the surface, as per the law of reflection:


        
        Angle 
    


The angle of refraction is determined by how much slower (or faster) light travels through the material, that speed is defined by the index of refraction, and the angle is calculated using Snell's law:






        
        
        
        Angle 
        Index of refraction 
        Index of refraction 
    


At a  and refractive indices light is no longer refracted and seems to disappear.

The amount of light that is reflected and refracted is calculated using Fresnel equations.

However, computing the full Fresnel equation in real time can be slow, so in 1994 Christophe Schlick came up with an approximation.
First we compute the reflectance at zero degrees from the normal:


Then we plug  in the approximation function for the reflectance:


The transmitted (or refracted) light simply becomes:



        
        
        Angle 
        Index of refraction 
        Index of refraction 
    

If we try the  where the refracted ray disappeared, we can now see it getting reflected back inside the medium, this is called total internal reflection.

Total internal reflection gives rise to an interesting phenomenon called Snell's window. If you dive underwater and look up, the light above the surface is refracted through a circular window 96 degrees wide, and everything outside is a reflection of the bottom of the water.


Angle



This is what it looks underwater:




The Microfacet Model

Like we saw earlier, we can explain light reflecting and refracting using different models, depending on the size of the surface irregularities we are considering.
For example, wave optics explains light interacting with matter as light waves diffracting on the surface nanogeometry.
If we zoom out a bit and use ray optics, we consider light as straight line rays that reflect and refract on the surface microgeometry. With this model we can use the optical laws we described earlier: law of reflection, Snell's law, Fresnel equations.
Now for rendering, we can zoom out even further and consider one pixel at a time, each pixel contains many microgeometry surfaces that we call a microfacet. We can use a statistical average of the microfacets in a pixel to simulate the appearance of the surface at that pixel, without considering each individual microfacet which would be unfeasible in real time.

    
        Size
        Model
        Phenomenon
    
    
        Nanogeometry
        Wave optics
        Light diffraction
    
    
        Microgeometry
        Ray optics
        Reflection/refraction, change in local normal
    
    
        Macrogeometry
        BRDF
        Statistical average over a pixel, wider cone -> more roughness
    


Here we can see a microgeometry surface, changing the roughness makes it more bumpy and the microfacets normals aren't aligned anymore:

Roughness


At the macrogeometry level, a bigger roughness value means light rays have a wider cone where they can spread out. The function that describes this cone is called bidirectional reflectance distribution function, we will discuss it in the next chapter.

Roughness


In our microfacet model, we distinguish two types of materials by the nature of their interaction with light: metals and non-metals.

Metals

Metals have a sea of free electrons that absorb light very easily when the photons enter a few nanometers deep inside the surface. The light that isn't absorbed is reflected equally across the visible light spectrum, this is why metals have that distinct "silvery" gray color.
Notable exceptions are gold, copper, osmium and caesium.



Changing the roughness of a metal only changes its specular reflection, making it more or less mirror-like. But there is no diffuse reflection at all.

Roughness


Non-metals

Also called dielectrics, these are materials that do not conduct electricity (insulators). They include plastic, wood, glass, water, diamond, air...



When a photon hits a dielectric material, it only gets absorbed if it's energy matches the electron's energy in the material. So light either gets reflected, and the specular reflection depends on the roughness of the surface.
The light can also get refracted inside the dielectric material, it bounces around and interacts with the pigments inside the material until it exits the surface, this is called diffuse reflection.


Roughness


Spectral Power Distribution

If we take the example of a red apple. When we shine a white light (which contains all visible wavelengths) on it, the apple's pigments (anthocyanins) absorb most of the wavelengths like violet, blue and green wavelengths, thus decreasing the intensity of those colors from the light. The remaining wavelengths, mostly red, gets scattered off the apple's surface making us perceive the apple as red.



We can characterize the incoming light by describing the amount of energy it carries at each wavelength using a function called the Spectral Power Distribution or SPD for short.
For example, below is the SPD for D65, a theoretical source of light standardized by The International Commission on Illumination (CIE). It represents the spectrum of average midday light in Western Europe or North America:

We can compare this SPD to AM0, which is the measured solar radiation in outer space before entering Earth's atmosphere. Notice the absence of a dip in the ultraviolet range:


And here is the SPD of a typical tungsten incandescent light:


Spectral Reflectance Curve
The SPD shows us how much of each "color" a light is composed of. Another interesting function we can look at is called the spectral reflectance curve, which shows the fraction of incident light reflected by an object at each wavelength, effectivly representing the color of said object.
Going back to our apple example, since it reflects most of its light in the red wavelength, its spectral reflectance curve might look like this:


The light we see is the combination of the light spectral power distribution with the object spectral reflectance.
If we shine a light on our red apple, depending on the wavelengths of the light, the final color we see changes. A  makes the apple appear red, because it's like multiplying the apple's color by one. We get the same result with a , because the apple reflects mostly in the red spectrum.However if we shine a , besides the leaf, the rest of the apple doesn't reflect any light, thus appearing black.
On the top right you can see the SPD of the flashlight, under it the reflectance curve of the apple, and the resulting reflected light below it:



If we now add a banana and shine a , we can obviously tell the apple and the banana apart, one being red while the other is yellow.But what happens when the light is ? Both objects appear reddish to our eyes, because the banana doesn't have any green light to reflect, making it lose its yellow color. This phenomenon is called metamerism.
You can display the  or the  :





There are different types of metamerism, depending on when it happens during the light transport process. The apple and banana example is called illuminant metamerism, where objects that reflect light differently appear the same under some specific illumination.
Observer metamerism is when objects appear different between observers, a good example of this is colorblindness.

Chapter 3 : The Rendering equation




 is the outgoing light at point  to the direction 
 is the incoming light at point  from the direction 
The BRDF (Bidirectional reflectance distribution function) is a function that tells use how much of the incoming light  is reflected to the outgoing direction  at point , this function characterizes the surface of our material.
The dot product is called the cosine term.




The rendering equation gives us the light reflected towards a direction  at a point  by summing all the incoming lights  at that point coming from direction  in the hemisphere , weighted by the BRDF at that point and the cosine term.

Let's peel off this equation step by step, starting with the easiest part:

Lambert's cosine law

When a beam of light hits a surface, the area it touches is inversly proportional to the cosine of the angle of incidence. When the angle of incidence is , the area is at minimum and the intensity is concentrated, but the more  the angle gets, the larger the area and the intensity gets spread out.



Angle


The BRDF

The BRDF is arguably the most important part of the rendering equation, it characterizes the surface of our material and its appearance. This is where the we can apply the microfacet theory and energy conservation to make our rendering model physically based.

It takes as input the incoming  and outgoing  light direction, and the roughness of the surface . It equals the diffuse and the specular components weighted by their respective coefficients  and .
There are many different BRDFs, the most common in realtime rendering is the Cook-Torrance specular microfacet model combined with Lambertian diffuse model.



The lambertian diffuse component is the diffuse color, called albedo, multiplied by the cosine factor. But since we already have the cosine factor in the rendering equation, the diffuse equation becomes: 



The Cook-Torrance specular component itself has three components: the normal distribution function , the geometric function  and the Fresnel equation .

Normal Distribution Function

The normal distribution function is an approximation of the number of microfacets oriented in such a way that they will reflect light from the incoming direction  to the outgoing direction .

The one we will use is the Trowbridge-Reitz GGX function:



 is the halfway vector between the incoming and outgoing directions, we calculate it like this:




Roughness


Geometric Function

Some incoming rays get occluded by some microfacets before they get a chance to bounce off to the outgoing direction, this is called shadowing. Other rays get occluded by microfacets on their way to the outgoing direction, this is called masking. The geometric function approximates this effect.

Here we can see the shadowed rays in red and the masked rays in blue. The yellow rays succesfully reflected to the outgoing direction:

Angle


We will use the Schlick-GGX geometric function:




Where:



Roughness


Fresnel Equation

Like we discussed in the previous chapter, we will use the Fresnel-Schlick approximation which is fast for realtime rendering and accurate enough:



Base reflectance F0


Combining everything

Now we can combine the diffuse and specular components to get our final PBR render:


Roughness
    Metallic
    Albedo


Here is a grid of spheres with different roughness and metallic values on each axis:


Usually the metallic values is either 0 or 1, but it is useful in PBR rendering to consider intermediate values to smoothly interpolate between metals and non-metals. Take this rusted metal material for example:


To be continued...

Physically based rendering is a very vast topic and there is a lot more to cover.
In the chapter about the physics of light, I omitted the quantum explanation of light's behaviour using probability amplitudes. We didn't talk about the double slit experiment or the wave-particle duality. I may cover this in the future when I learn more about it, for now I'll leave you with this quote from Richard Feynman's QED book:
The theory of quantum electrodynamics describes Nature as absurd from the point of view of common sense. And it agrees fully with experiment. So I hope you accept Nature as She is — absurd.


We didn't talk about polarization and assumed all our light sources are unpolarized, this isn't very important for general rendering but can be useful for research.

We focused on surface rendering, in the future I will cover volume rendering, subsurface scattering, effects like optical dispersion, thin-film interference/iridescence...etc

There are a lot more implementation specific details. Whether we are implementing PBR in raytracing or rasterization, we need to use optimization techniques to make the rendering faster while still being accurate. Examples that come to mind are prefiltred envmaps and importance sampling (or efficient sampling in general).

Further reading
This article is mainly based on this SIGGRAPH talk by Naty Hoffman and Physically Based Rendering: From Theory To Implementation
My main inspiration for writing interactive articles is this fantastic blog by Bartosz Ciechanowski. A lot of interactive demos in this article are similar to the ones in this post.
Other resources include LearnOpenGL, the ScienceClic youtube channel, and 3Blue1Brown of course.
I can't recommend enough the famous book QED: The Strange Theory of Light and Matter by Richard Feynman.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introduction to Ada: a project-based exploration with rosettas]]></title>
            <link>https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106314</guid>
            <description><![CDATA[by Romain Gora – Sep 01, 2025. Discover Ada through a fun, project-based tutorial! Learn the language’s clarity, safety, and modern features while building an SVG rosetta generator. A fresh, visual way to explore Ada 2022.]]></description>
            <content:encoded><![CDATA[ContextThis practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, I’ll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirograph™ toy.In this walkthrough, we’ll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.Let's dive in!A brief note on AdaThis section leans a bit more into background context, with a slightly encyclopedic flavor that's especially useful for readers new to Ada. If you're already familiar with Ada’s history and principles, feel free to joyfully skip ahead to the next section!Ada was created in the late 1970s after a call from the U.S. Department of Defense to unify its fragmented software landscape. The winning proposal became Ada, a language that's been literally battle-tested (!) and built on a deeply thought-out design that continues to evolve today.While Ada is absolutely a general-purpose programming language, it has carved out a strong niche in fields where software correctness and reliability are mission-critical:Embedded and real-time systemsAerospace and defenseRail, automotive, and aviationAny system where failure is not just a bug, but a riskIts strict compile-time checks, safety features, and clear structure make it particularly appealing when you need your software to be dependable from day one and still maintainable ten years later.Ada's design is grounded in a strong and principled philosophy:Readability over conciseness: Ada favors clarity. It avoids symbols and abbreviations in favor of full keywords, making the language more accessible and less error-prone.Strong and explicit typing: It is extremely easy to declare new types in Ada, with precise constraints, which makes it much harder to accidentally misuse data. While some functional languages share this strong typing discipline, Ada stands out by requiring the programmer to be very explicit. It uses little to no type inference.Explicit is better than implicit: Unlike many modern languages that prioritize convenience, Ada leans heavily toward precision. Most types must be explicitly named and matched.Defined semantics and minimal undefined behavior: Ada offers a level of predictability and safety unmatched in many languages. This makes it a strong choice not only for safety-critical systems, but also for codebases where long-term maintenance, verifiability, and correctness are essential.Compiler as a partner: Ada compilers are strict by design, not to frustrate, but to help the programmer write clearer, more correct code. This philosophy encourages the developer to communicate intent clearly, both to the compiler and to future readers.How the program worksSometimes the best way to figure out how something works is to start at the end. Let's do that!In this tutorial, we'll walk through how the program produces its final output — a rosetta SVG file — and use that as a way to explore how Ada's structure, type system, and tooling come together.This is a simple command-line program that generates an SVG file. You run it like this:./bin/rosettaThe idea was to create something visual: learning is more fun when there's an immediate, satisfying result and generating rosettas fits that goal perfectly.Why SVG? Because it's a lightweight and portable vector format that you can view in any modern browser. I wanted to avoid relying on a graphical library, which would have added extra weight and gone beyond the scope of this approach. And while XML isn't the most pleasant format to write by hand, generating it from code is straightforward and gives a surprisingly clean result.Tooling & setupTo build and run the project, I used Alire, the Ada package manager. It plays a similar role in the Ada ecosystem as Cargo does for Rust or npm for JavaScript. It's well-documented, and while we won't dive deep into it here, it's a solid and accessible way to manage Ada projects. I encourage anyone curious to get it from https://alire.ada.dev. Interestingly, "Alire" is also the French expression for "à lire" — which means "for reading." A fitting name for a tool that supports a language so focused on clarity and readability!Once Alire is set up, the natural next step is choosing where to write the code. You have two excellent options for your development environment. For a dedicated experience, you can download the latest release of GNAT Studio from its GitHub repository. If you prefer a more general-purpose editor, you can install the official Ada & SPARK for Visual Studio Code extension from AdaCore.As a new learner, I also kept https://learn.adacore.com close at hand. It’s a particularly clear and comprehensive resource — and I especially appreciated being able to download the ebook version and read through it on my phone.Entry pointwith Rosetta_Renderer;

procedure Main is
begin
   Rosetta_Renderer.Put_SVG_Rosettas;
end Main;There are several interesting things to notice right away:The with clause is not a preprocessor directive like in C or C++. It’s a compiled, checked reference to another package — a reliable and explicit way to express a dependency. This eliminates entire classes of bugs related to fragile #include chains, macro collisions, or dependency order issues.This procedure is not a function: it does not return a value. In Ada, procedures are used to perform actions (like printing or modifying state), and functions are used to compute and query values.The syntax is designed for readability. You’ll find begin and end here instead of {} as in C/C++, reinforcing Ada’s philosophy that clarity matters more than brevity.Put_SVG_Rosettas uses the idiomatic Pascal_Snake_Case naming style. This reflects a common Ada convention and avoids acronyms or compressed identifiers in favor of more descriptive names.The entry point is minimal but meaningful: it simply calls a procedure which generates the output we'll explore in the next sections.Geometry and computation (package Rosetta)In Ada, a package is a modular unit that groups related types, procedures, and functions. Following the convention from GNAT (the Ada compiler, part of the GNU Compiler Collection, fondly known as GCC), each package has a specification file (with the .ads extension — short for Ada Specification) and an implementation file (with the .adb extension — short for Ada Body). This clear and enforced split means you always know where to find interface definitions versus their implementation.The following code is the package specification for Rosetta. It defines the data types for the rosetta shapes and declares the public interface of operations available to manipulate them.with Ada.Strings.Text_Buffers;

package Rosetta is

   --  A mathematical description of a rosetta (specifically, a hypotrochoid).
   --  formed by tracing a point attached to a circle rolling inside another circle.
   type Hypotrochoid is record
      Outer_Radius : Float;     --  Radius of the fixed outer circle.
      Inner_Radius : Float;     --  Radius of the rolling inner circle.
      Pen_Offset   : Float;     --  From the center of the inner circle to the drawing point.
      Steps        : Positive;  --  Number of steps (points) used to approximate the curve.
   end record;

   --  A 2D coordinate in Cartesian space.
   type Coordinate is record
      X_Coord, Y_Coord : Float;
   end record
     with Put_Image => Put_Image_Coordinate;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate 
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class; 
      Value  : Coordinate);

   --  A type for an unconstrained array of 2D points forming a curve.
   --  The actual bounds are set when an array object of this type is declared.
   type Coordinate_Array is array (Natural range <>) of Coordinate;

   --  Computes the coordinates of the rosetta curve defined by Curve (a hypotrochoid).
   --  Returns a centered array of coordinates.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array;

end Rosetta;The Rosetta package is responsible for all the math and curve computation. It defines:Hypotrochoid, type describing the geometry of the rosettaCoordinate, type representing points in 2D spaceCoordinate_Array, type holding a series of such pointsCompute_Points, function which calculates all the points of the curve based on the Hypotrochoid parameters and recenters them around the originThis package is focused solely on computation. It doesn’t concern itself with how the result is rendered.Fun fact for the curious: when the rolling circle rolls outside the fixed circle rather than inside, the resulting curve is called an epitrochoid.In Ada, a record is similar to a struct in C or a class with only data members in other languages. It's a user-defined type composed of named components, making it ideal for modeling structured data.Using a record for Hypotrochoid was particularly appropriate: it allows grouping all geometric parameters (outer radius, inner radius, pen offset, and steps) into a single, cohesive unit. This improves readability and maintainability. The compiler enforces correctness by ensuring all required values are present and of the expected type — reinforcing Ada’s philosophy of clarity and safety.The type Coordinate_Array is an unconstrained array type that holds a range of Coordinate records. In this context, ‘unconstrained’ simply means that we don’t define the array’s size when we declare the type. Instead, the size is defined when we declare an object of that type. This gives us the flexibility to use this type for a variety of shapes.You may also notice the use of Natural range <>. Natural is a predefined subtype of Integer that only allows non-negative values. And yes, I mean subtype: Ada’s powerful type system allows you to take an existing type and create a more specific, constrained version of it.Highlights from the .adb fileHere are a few notable aspects from the implementation (rosetta.adb) that illustrate Ada’s strengths for writing safe, clear, and structured code:Declarative and modular design: Both Generate_Point and Compute_Points are pure functions that operate only on their inputs. Their behavior is fully deterministic and encapsulated.Safe bounds and array handling: The Points array is statically bounded using (0 .. Curve.Steps), and its access is strictly safe. The compiler ensures that any index outside this range would raise an error at runtime. This immediate error is a feature, not a bug. It stops silent memory corruption and security flaws by ensuring the program fails predictably and safely at the source of the problem.Use of constants for robustness: Variables such as Pi, R_Diff, and Ratio are declared as constant, enforcing immutability. This helps ensure clarity of intent and prevents accidental reassignment, a common source of subtle bugs in more permissive languages. Ada encourages this explicit declaration style, promoting safer code.with Ada.Numerics;
with Ada.Numerics.Elementary_Functions;

use Ada.Numerics;
use Ada.Numerics.Elementary_Functions;

package body Rosetta is

   --  Computes a single point on the hypotrochoid curve for a given angle Theta.
   --  Uses the standard parametric equation of a hypotrochoid.
   function Generate_Point (Curve : Hypotrochoid; Theta : Float) return Coordinate is
      R_Diff : constant Float := Curve.Outer_Radius - Curve.Inner_Radius;
      Ratio  : constant Float := R_Diff / Curve.Inner_Radius;
   begin
      return (
              X_Coord => R_Diff * Cos (Theta) + Curve.Pen_Offset * Cos (Ratio * Theta),
              Y_Coord => R_Diff * Sin (Theta) - Curve.Pen_Offset * Sin (Ratio * Theta)
             );
   end Generate_Point;

   --  Computes all the points of the hypotrochoid curve and recenters them.
   --  The result is an array of coordinates centered around the origin.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array is
      Points : Coordinate_Array (0 .. Curve.Steps);
      Max_X  : Float := Float'First;
      Min_X  : Float := Float'Last;
      Max_Y  : Float := Float'First;
      Min_Y  : Float := Float'Last;
      Offset : Coordinate;
   begin
      --  Computes raw points and updates the bounding box extents.
      for J in 0 .. Curve.Steps loop
         declare
            Theta : constant Float := 2.0 * Pi * Float (J) / Float (Curve.Steps) * 50.0;
            P     : constant Coordinate := Generate_Point (Curve, Theta);
         begin
            Points (J) := P;
            Max_X := Float'Max (Max_X, P.X_Coord);
            Min_X := Float'Min (Min_X, P.X_Coord);
            Max_Y := Float'Max (Max_Y, P.Y_Coord);
            Min_Y := Float'Min (Min_Y, P.Y_Coord);
         end;
      end loop;

      --  Computes the center offset based on the bounding box.
      Offset := (
                 X_Coord => (Max_X + Min_X) / 2.0,
                 Y_Coord => (Max_Y + Min_Y) / 2.0
                );

      --  Recenters all points by subtracting the center offset.
      for J in Points'Range loop
         Points (J).X_Coord := @ - Offset.X_Coord;
         Points (J).Y_Coord := @ - Offset.Y_Coord;
      end loop;

      return Points;
   end Compute_Points;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class;
      Value  : Coordinate)
   is   
      X_Text : constant String := Float'Image (Value.X_Coord);
      Y_Text : constant String := Float'Image (Value.Y_Coord);
   begin
      Output.Put (X_Text & "," & Y_Text);
   end Put_Image_Coordinate;

end Rosetta;On style: strict and predictable (and satisfying!)Ada is one of those rare languages that not only compiles your code but asks you to write it properly. With the compiler switch -gnaty, you can enforce a comprehensive set of style rules, many of which are stricter than what you'd see in most languages.This includes things like:No trailing whitespace at the end of linesNo consecutive blank linesProper indentation and alignment of keywords and parametersA space before “(“ when calling a procedure or functionConsistent casingAt first, this can feel surprisingly strict. But once you get used to it, the benefits are clear: it helps enforce a consistent and clean coding style across a codebase. That in turn improves readability, reduces ambiguity, and leads to more maintainable programs.Rather than leaving formatting up to personal taste or optional linter tools, Ada integrates this attention to detail into the compilation process itself. The result is not only more elegant: it's genuinely satisfying. And you can do even more with GNATcheck and GNATformat but it’s outside of the scope of this post.Outputting to SVG (package Rosetta_Renderer)The Rosetta_Renderer package is responsible for producing the SVG output. It defines a single high-level procedure:package Rosetta_Renderer is

   --  Renders a predefined set of rosettas into an SVG output.
   procedure Put_SVG_Rosettas;

end Rosetta_Renderer;This procedure generates an SVG file directly. It takes care of formatting the SVG structure (header, shapes, animations, and footer) and calls into the math logic defined in the Rosetta package to generate point data.This separation of concerns is deliberate and beneficial: the math logic doesn’t need to know anything about SVG, and the renderer doesn’t care how the coordinates were generated.Now let's talk about the body of the package... but not for long. We're keeping it brief because its core is essentially the SVG plumbing required to draw and animate the curves, so we'll skip the fine details. And for those who enjoy seeing how the sausage is made, I've made the fully commented source code available for you right here.The procedure Put_Path handles the creation of the SVG path. Its main job is to take an array of coordinates and write the corresponding command string to the d attribute of a <path> element. In SVG, this attribute defines the geometry of the shape. The code iterates over each coordinate, using M (moveto) for the first point and L (lineto) for all the others to draw the connecting lines.--  Puts coordinates to a single SVG path string ("d" attribute).
   procedure Put_Path (Stream : File_Type; Points : Coordinate_Array) is
   begin
      Put (Stream, "M "); -- Moves the pen without drawing.
      for J in Points'Range loop
         declare 
            Coord_Text : constant String := Coordinate'Image (Points (J));
         begin   
            Put (Stream, Coord_Text);
            if J < Points'Last then
               Put (Stream, " L "); --  Draws a line.
            end if;
         end;
      end loop;
   end Put_Path;AfterwordThis small project was an enjoyable and useful way to get back into Ada. It helped me reconnect with the language’s main strengths and refamiliarize myself with its tools and design. It was a great reminder of how fun, easy to learn, and remarkably modern Ada can be, especially for developers focused on building robust, maintainable, and efficient software.I hope this short walkthrough gives a good idea of that feeling, whether you're already into Ada or just starting to explore it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Python has had async for 10 years – why isn't it more popular?]]></title>
            <link>https://tonybaloney.github.io/posts/why-isnt-python-async-more-popular.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106189</guid>
            <description><![CDATA[A deep-dive into the challenges and misconceptions surrounding async programming in Python]]></description>
            <content:encoded><![CDATA[
                The Python Documentary dropped this morning. In the middle of the documentary, there’s a dramatic segment about how the transition from Python 2 to 3 divided the community (spoiler alert: it didn’t in the end).
The early versions of Python 3 (3.0-3.4) were mostly focused on stability and offering pathways for users moving from 2.7. Along came 3.5 in 2015 with a new feature: async and await keywords for executing coroutines.
Ten years and nine releases later, Python 3.14 is weeks away.
Whilst everyone will be distracted by the shiny, colorful REPL features in 3.14, there are some big announcements nestled in the release notes — both related to concurrency and parallelism


PEP779 Free-Threading is Officially Supported. 
PEP 734: Multiple interpreters in the stdlib

Both of these features are huge advancements in how Python can be used to execute concurrent code. But if async has been here for 10 years, why do we need them?
The killer use-case for async is web development. Coroutines lend well to out-of-process network calls, like HTTP requests and database queries. Why block the entire Python interpreter waiting for a SQL query to run on another server?
Yet, among the three most popular Python web frameworks, async support is still not universal. FastAPI is async from the ground-up, Django has some support, but is “still working on async support” in key areas like the ORM (database). Then Flask is and probably always will be synchronous (Quart is an async alternative with similar APIs). The most popular ORM for Python, SQLAlchemy, only added asyncio support in 2023 (changelog).
I posed the question “Why isn’t async more popular” to a couple of other developers to get their thoughts.
Christopher Trudeau, co-host of the Real Python Podcast, shared his perspective:

Certain kinds of errors get caught by the compiler, others just disappear. Why didn’t that function run? Oops, forgot to await it. Error in the coroutine? Did you remember to launch with the right params, if not, it doesn’t percolate up. I still find threads easier to wrap my head around.

Michael Kennedy offered some additional insight:

The [GIL] is so omnipresent that most Python people never developed multithreaded/async thinking. Because async/await only works for I/O bound work, not CPU as well, it’s of much less use. E.g. You can use in on the web, but most servers fork out to 4-8 web workers anyway

So what’s going on here and can we apply the lessons to Free-Threading and Multiple Interpreters in 3.14 so that in another ten years we’re looking back and wondering why they aren’t more popular?
Problem 1: What is an asynchronous-shaped problem?¶
Coroutines are most valuable with IO-related tasks. In Python, you can start hundreds of coroutines to make network requests, then wait for them all to finish without running them one at a time. The concepts behind coroutines are quite straightforward. You have a loop (the event loop) and you pass it coroutines to evaluate.
Let’s go back to the classic use-case, HTTP requests:
def get_thing_sync():
    return http_client.get('/thing/which_takes?ages=1')


The equivalent async function is clean and readable:
async def get_thing_async():
    return await http_client.get('/thing/which_takes?ages=1')


If you call function get_thing_sync() versus await get_thing_async(), they take the same amount of time. Calling it “✨ asynchronously ✨” does not somehow make it faster. The gains are when you have more than one coroutine running at once. 
When fetching multiple HTTP resources you can start all the requests at once via the OS network stack, then handle each response as it arrives. The important point is that the actual work — sending packets and waiting for remote servers — happens outside your Python process while your code waits. Async is most effective here: you start operations, receive awaitable handles (tasks/futures), and the event loop efficiently notifies the coroutine when each operation completes without wasting CPU on busy‑polling.
This scenario works well because:

The remote end is handling the work in another process
The local end (asyncio HTTP library) can yield control while waiting for the response
Operating-Systems have stacks and APIs for managing sockets and network

That’s all fine, but I started with the statement Coroutines are most valuable with IO-related tasks. I then picked the one task that asyncio can handle really well, HTTP requests.
What about disk IO? I have far more applications in Python which read and write from files on disks or memory than I do making HTTP requests. I also have Python programs which run other programs using subprocess.
Can I make all of those async?  
No, not really. From the asyncio Wiki:

asyncio does not support asynchronous operations on the filesystem. Even if files are opened with O_NONBLOCK, read and write will block.

The solution is to use a third-party package, aiofiles, which gives you async file I/O capabilities:
async with aiofiles.open('filename', mode='r') as f:
    contents = await f.read()


So, mission accomplished? No because aiofiles uses a thread pool to offload the blocking file I/O operations. 
Side-Quest: Why isn’t file IO async?¶
Windows has an async file IO API called IoRing. Linux has this availability in newer Kernels via io_uring. All I could find for a Python implementation of io_uring is this synchronous API written in Cython.
There were io_uring APIs for other platforms, Rust has implementations with tokio, C++ has Asio and Node.JS has libuv.
So, the asyncio Wiki is a little out of date, but

Most production Python applications run on Linux, where the implementation is io_uring
io_uring has been plagued by security issues so bad that RedHat, Google and others have restricted or removed its availability. After paying out $1 million in bug bounties related to io_uring, Google disabled it on some products. The issue was severe; many of the related bug‑bounty reports involved io_uring exploits.

So we should hold our horses a little while longer. Operating Systems have long held a file IO API that handles threads for concurrent IO. It does the job just fine for now.
So in summary, Coroutines are most valuable with IO-related tasks is only really true for network I/O and network sockets in Python were never blocking operations in the first place. Socket open in Python is one of the few operations which releases the GIL and works concurrently in a thread pool as a non-blocking operation.
Recap: What are the async operations in asyncio?¶



Operation
Asyncio API
Description




Sleep
asyncio.sleep()
Asynchronously sleep for a given duration.


TCP/UDP Streams
asyncio.open_connection()
Open a TCP/UDP connection.


HTTP
aiohttp.ClientSession()
Asynchronous HTTP client.


Run Subprocesses
asyncio.subprocess
Asynchronously run subprocesses.


Queues
asyncio.Queue
Asynchronous queue implementation.



Problem 2: No matter how fast you run, you can’t escape the GIL¶
Will McGugan, the creator of Rich, Textualize, and several other extremely popular Python libraries offered his perspective on async:

I really enjoy async programming, but it isn’t intuitive for most devs that don’t have a background writing network code. A reoccurring problem I see with Textual is folk testing concurrency by dropping in a time.sleep(10) call to simulate the work they are planning. Of course, that blocks the entire loop. But that’s a class of issue which is difficult to explain to devs who haven’t used async much. i.e. what does it mean for code to “block”, and when is it necessary to defer to threads. Without that grounding in the fundamentals, your async code is going to misbehave, but its not going to break per se. So devs don’t get the rapid iteration and feedback that we expect from Python.

Now that we’ve covered the limited use cases for async, another challenge keeps coming up. The Python GIL.
I’ve been working on this C#/Python bridge project called CSnakes, one of the features that caused the most head-scratching was async.
C#, the language from which the async/await syntax was borrowed, has far broader async support in its core I/O libraries because it implements a Task‑based Asynchronous Pattern (TAP), where tasks are dispatched onto a managed thread pool. Disk, network, and memory I/O operations commonly provide both async and sync methods.
In fact, the C# implementation goes all the way up from the disk to the higher-level APIs, such as serialization libraries. JSON deserialization is async, so is XML. 
The C# Async model and the Python Async models have some important differences:

C# creates a task pool and tasks are scheduled on this pool. The number of threads is managed automatically by the runtime.
Python event loops belong to the thread that created them. C# tasks can be scheduled on any thread.
Python async functions are coroutines that are scheduled on the event loop. C# async functions are tasks that are scheduled on the task pool.

The benefit of C#’s model is that a Task is a higher-level abstraction over a thread or coroutine. This means that you don’t have to worry about the underlying thread management, you can schedule several tasks to be awaited concurrently or you can run them in parallel with Task Parallel Library (TPL).
In Python “An event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. While a Task is running in the event loop, no other Tasks can run in the same thread. When a Task executes an await expression, the running Task gets suspended, and the event loop executes the next Task.” 1
Going back to Will’s comment “Of course, that blocks the entire loop”, he’s talking about operations inside async functions which are blocking and therefore block the entire event loop. Since we covered in Problem 1, that’s practically everything except network calls and sleeping. 
With Python’s GIL, it doesn’t matter if you’re running 1 thread or 10, the GIL will lock everything so that only 1 is operating at a time.

There are some operations don’t block the GIL (e.g. File IO) and in those cases you can run them in threads. For example, if you used httpx‘s streaming feature to stream a large network download onto disk:
import httpx
import tempfile

def download_file(url: str):
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        with httpx.stream("GET", url) as response:
            for chunk in response.iter_bytes():
                tmp_file.write(chunk)
    return tmp_file.name


Neither the httpx stream iterator nor tmp_file.write is GIL-blocking, so they benefit from running in separate threads.
We can merge this behavior with an asyncio API, by using the Event Loop run_in_executor() function and passing it a thread pool:
import asyncio
import concurrent.futures

async def main():
    loop = asyncio.get_running_loop()

    URLS = [
        "https://example.place/big-file-1",
        "https://example.place/big-file-2",
        "https://example.place/big-file-3",
        # etc.
    ]

    tasks = set()
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as pool:
        for url in URLS:
            tasks.add(loop.run_in_executor(pool, download_file, url))
        files = await asyncio.gather(*tasks)
    print(files)


It’s not immediately clear to me what the benefit of this is over running a thread-pool and calling pool.submit. We retain an async API, so if that is important this is an interesting workaround. 
I find that memorizing, documenting, and explaining what is and isn’t “blocking” in Python to be confusing and continually changing.
Does the free-threaded mode make asyncio more useful or redundant?¶
Python 3.13 introduced a very-unstable “free-threaded” build of Python where the GIL is removed and replaced with smaller, more granular locks. See my PyCon US 2024 Talk for a summary of parallelism.
The 3.13 build wasn’t stable enough for any production use. 3.14 is looking far improved and I think we can start to introduce free-threading in 2026 in some narrow, well-tested scenarios.
One major benefit to coroutines versus threads is that they have a far smaller memory footprint, a lower context-switching overhead, and faster startup times. async APIs are also easier to reason about and compose.
Because parallelism in Python using threads has always been so limited, the APIs in the standard library are quite rudimentary. I think there is an opportunity to have a task-parallelism API in the standard library once free-threading is stabilized. 
Last week I was implementing a registry function that did two discrete tasks. One calls a very slow sync-only API and the other calls several async APIs. 
I want the behavior that:

Both are started at the same time
If one fails, it cancels the other and raises an exception with the exception details of the failed function
The result is only combined when both are complete

flowchart LR
  Start([Start]) --> Invoke["tpl.invoke()"]
  Invoke --> f1["f1()"]
  Invoke --> f2["f2()"]
  f1 -->|f1 -> T1| Join["Tuple[T1, T2]"]
  f2 -->|f2 -> T2| Join
  Join --> End([End])


Since there are only two tasks, I don’t want to have to define a thread-pool or a set number of workers. I also don’t want to have to map or gather the callees. I want to retain my typing information so that the resulting variables are strongly typed from the return types of function_a and function_a. Essentially an API like this:
import tpl


def function_a() -> T1:
    ...

def function_b() -> T2:
    ...

result_a: T1, result_b: T2 = tpl.invoke(function_a, function_b)


This is all possible today but there are many constraints with the GIL. Free-threading will make parallel programming more popular in Python and we’ll have to revisit some of the APIs.
Problem 3: Maintaining two APIs is hard¶
As a package maintainer, supporting both synchronous and asynchronous APIs is a big challenge. You also have to be selective with where you support async. Much of the stdlib doesn’t support async natively (e.g. logging backends).
Python’s Magic (__dunder__) methods cannot be async. __init__ cannot be async for example, so none of your code can use network requests in the initializer.
Async properties¶
This is an odd-pattern but I’ll keep it simple to illustrate my point. You have a class User with a property records. This property gives a list of records for that user. A synchronous API is straightforward:
class User:
    @property
    def records(self) -> list[RecordT]:
        # fetch records from database lazily
        ...


We can even use a lazily-initialized instance variable to cache this data.
Porting this API to async is a challenge because whilst @property methods can be async, standard attributes are not. Having to await some instance attributes and not others leaves a very odd API:
class AsyncDatabase:
    @staticmethod
    async def fetch_many(id: str, of: Type[RecordT]) -> list[RecordT]:
        ...

class User:
    @property
    async def records(self) -> list[RecordT]:
        # fetch records from database lazily
        return await AsyncDatabase.fetch_many(self.id, RecordT)


Anytime you access that property, it needs to be awaited:
user = User(...)
# single access
await user.records
# if
if await user.records:
    ...
# comprehension?
[record async for record in user.records]


The further we go into this implementation, the more we wait for the user to accidentally forget to await the property and it fails silently.
Duplicated implementations¶
The Azure Python SDK, a ginormous Python project supports both sync and async. Maintaining both is achieved via a lot of code-generation infrastructure. This is ok for a project with tens of full-time engineers, but for anything small or voluntary you need to copy + paste a lot of your code base to create an async version. Then you need to patch and backport fixes and changes between the two. The differences (mostly await calls) are big enough to confuse Git. I was reviewing some langchain implementations last year which had both sync and async implementation. Every method was copied+pasted, with little behavioral differences and their own bugs. People would submit bug fix PR’s to one implementation and not the other so instead of merging directly, maintainers had to port the fix, skip it, or ask the contributors to do both.
Backend fragmentation¶
Since we’re largely talking about HTTP/Network IO, you also need to pick a backend for sync and async. For synchronous HTTP calls, requests, httpx are suitable backends. For async, its aiohttp and httpx. Since neither are part of the Python standard library, the adoption and support for CPython’s main platforms is out of sync. E.g. as of today, aiohttp has no Python 3.14 wheels, nor free-threaded support. UV Loop, the alternative implementation of the event loop has no Python 3.14 support, nor any Windows support. (Python 3.14 isn’t out yet, so it’s reasonable to not have support in either open-source project).
Testing overhead¶
Further down the copy+paste maintainer overhead is the testing of these APIs. Testing your async code requires different mocks, different calls and in the case of Pytest a whole set of extensions and patterns for fixtures. This situation is so confusing I wrote a post about it and it’s one of the most popular on my blog.
Summary¶
In summary, I think the use cases for asyncio are limited (mostly for reasons beyond the control of asyncio) and this has constrained it’s popularity. Maintaining duplicate code-bases is a burden.
FastAPI, the web framework that’s async from-the-ground-up grew in popularity again from 29% to 38% share of the web frameworks for Python, taking the #1 spot. It has over 100-million downloads a month. Considering the big use-case for async is HTTP and network IO, having the #1 web framework be an async one is a sign of asyncio’s success. 
I think in 3.14 the sub-interpreter executor and free-threading features make more parallel and concurrency use cases practical and useful. For those, we don’t need async APIs and it alleviates much of the issues I highlighted in this post.

                
                Related Posts
                
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[<template>: The Content Template element]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/template</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45106049</guid>
            <description><![CDATA[The <template> HTML element serves as a mechanism for holding HTML fragments, which can either be used later via JavaScript or generated immediately into shadow DOM.]]></description>
            <content:encoded><![CDATA[
            
            
    Attributes
    This element includes the global attributes.

shadowrootmode

Creates a shadow root for the parent element.
It is a declarative version of the Element.attachShadow() method and accepts the same enumerated values.

open

Exposes the internal shadow root DOM for JavaScript (recommended for most use cases).

closed

Hides the internal shadow root DOM from JavaScript.


Note:
The HTML parser creates a ShadowRoot object in the DOM for the first <template> in a node with this attribute set to an allowed value.
If the attribute is not set, or not set to an allowed value — or if a ShadowRoot has already been declaratively created in the same parent — then an HTMLTemplateElement is constructed.
A HTMLTemplateElement cannot subsequently be changed into a shadow root after parsing, for example, by setting HTMLTemplateElement.shadowRootMode.
Note:
You may find the non-standard shadowroot attribute in older tutorials and examples that used to be supported in Chrome 90-110. This attribute has since been removed and replaced by the standard shadowrootmode attribute.

shadowrootclonable

Sets the value of the clonable property of a ShadowRoot created using this element to true.
If set, a clone of the shadow host (the parent element of this <template>) created with Node.cloneNode() or Document.importNode() will include a shadow root in the copy.

shadowrootdelegatesfocus

Sets the value of the delegatesFocus property of a ShadowRoot created using this element to true.
If this is set and a non-focusable element in the shadow tree is selected, then focus is delegated to the first focusable element in the tree.
The value defaults to false.

shadowrootserializable 
Experimental


Sets the value of the serializable property of a ShadowRoot created using this element to true.
If set, the shadow root may be serialized by calling the Element.getHTML() or ShadowRoot.getHTML() methods with the options.serializableShadowRoots parameter set true.
The value defaults to false.


  
    Usage notes
    This element has no permitted content, because everything nested inside it in the HTML source does not actually become the children of the <template> element. The Node.childNodes property of the <template> element is always empty, and you can only access said nested content via the special content property. However, if you call Node.appendChild() or similar methods on the <template> element, then you would be inserting children into the <template> element itself, which is a violation of its content model and does not actually update the DocumentFragment returned by the content property.
Due to the way the <template> element is parsed, all <html>, <head>, and <body> opening and closing tags inside the template are syntax errors and are ignored by the parser, so <template><head><title>Test</title></head></template> is the same as <template><title>Test</title></template>.
There are two main ways to use the <template> element.
  
    Template document fragment
    By default, the element's content is not rendered.
The corresponding HTMLTemplateElement interface includes a standard content property (without an equivalent content/markup attribute). This content property is read-only and holds a DocumentFragment that contains the DOM subtree represented by the template.
This fragment can be cloned via the cloneNode method and inserted into the DOM.
Be careful when using the content property because the returned DocumentFragment can exhibit unexpected behavior.
For more details, see the Avoiding DocumentFragment pitfalls section below.
  
    Declarative Shadow DOM
    If the <template> element contains the shadowrootmode attribute with a value of either open or closed, the HTML parser will immediately generate a shadow DOM. The element is replaced in the DOM by its content wrapped in a ShadowRoot, which is attached to the parent element.
This is the declarative equivalent of calling Element.attachShadow() to attach a shadow root to an element.
If the element has any other value for shadowrootmode, or does not have the shadowrootmode attribute, the parser generates a HTMLTemplateElement.
Similarly, if there are multiple declarative shadow roots, only the first one is replaced by a ShadowRoot — subsequent instances are parsed as HTMLTemplateElement objects.
  
    Examples
    
  
    Generating table rows
    First we start with the HTML portion of the example.
<table id="producttable">
  <thead>
    <tr>
      <td>UPC_Code</td>
      <td>Product_Name</td>
    </tr>
  </thead>
  <tbody>
    <!-- existing data could optionally be included here -->
  </tbody>
</table>

<template id="productrow">
  <tr>
    <td class="record"></td>
    <td></td>
  </tr>
</template>

First, we have a table into which we will later insert content using JavaScript code. Then comes the template, which describes the structure of an HTML fragment representing a single table row.
Now that the table has been created and the template defined, we use JavaScript to insert rows into the table, with each row being constructed using the template as its basis.
// Test to see if the browser supports the HTML template element by checking
// for the presence of the template element's content attribute.
if ("content" in document.createElement("template")) {
  // Instantiate the table with the existing HTML tbody
  // and the row with the template
  const tbody = document.querySelector("tbody");
  const template = document.querySelector("#productrow");

  // Clone the new row and insert it into the table
  const clone = template.content.cloneNode(true);
  let td = clone.querySelectorAll("td");
  td[0].textContent = "1235646565";
  td[1].textContent = "Stuff";

  tbody.appendChild(clone);

  // Clone the new row and insert it into the table
  const clone2 = template.content.cloneNode(true);
  td = clone2.querySelectorAll("td");
  td[0].textContent = "0384928528";
  td[1].textContent = "Acme Kidney Beans 2";

  tbody.appendChild(clone2);
} else {
  // Find another way to add the rows to the table because
  // the HTML template element is not supported.
}

The result is the original HTML table, with two new rows appended to it via JavaScript:
table {
  background: black;
}
table td {
  background: white;
}


  
    Implementing a declarative shadow DOM
    In this example, a hidden support warning is included at the beginning of the markup. This warning is later set to be displayed via JavaScript if the browser doesn't support the shadowrootmode attribute. Next, there are two <article> elements, each containing nested <style> elements with different behaviors. The first <style> element is global to the whole document. The second one is scoped to the shadow root generated in place of the <template> element because of the presence of the shadowrootmode attribute.
<p hidden>
  ⛔ Your browser doesn't support <code>shadowrootmode</code> attribute yet.
</p>
<article>
  <style>
    p {
      padding: 8px;
      background-color: wheat;
    }
  </style>
  <p>I'm in the DOM.</p>
</article>
<article>
  <template shadowrootmode="open">
    <style>
      p {
        padding: 8px;
        background-color: plum;
      }
    </style>
    <p>I'm in the shadow DOM.</p>
  </template>
</article>

const isShadowRootModeSupported = Object.hasOwn(
  HTMLTemplateElement.prototype,
  "shadowRootMode",
);

document
  .querySelector("p[hidden]")
  .toggleAttribute("hidden", isShadowRootModeSupported);


  
    Declarative Shadow DOM with delegated focus
    This example demonstrates how shadowrootdelegatesfocus is applied to a shadow root that is created declaratively, and the effect this has on focus.
The code first declares a shadow root inside a <div> element, using the <template> element with the shadowrootmode attribute.
This displays both a non-focusable <div> containing text and a focusable <input> element.
It also uses CSS to style elements with :focus to blue, and to set the normal styling of the host element.
<div>
  <template shadowrootmode="open">
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

The second code block is identical except that it sets the shadowrootdelegatesfocus attribute, which delegates focus to the first focusable element in the tree if a non-focusable element in the tree is selected.
<div>
  <template shadowrootmode="open" shadowrootdelegatesfocus>
    <style>
      :host {
        display: block;
        border: 1px dotted black;
        padding: 10px;
        margin: 10px;
      }
      :focus {
        outline: 2px solid blue;
      }
    </style>
    <div>Clickable Shadow DOM text</div>
    <input type="text" placeholder="Input inside Shadow DOM" />
  </template>
</div>

Last of all we use the following CSS to apply a red border to the parent <div> element when it has focus.
div:focus {
  border: 2px solid red;
}

The results are shown below.
When the HTML is first rendered, the elements have no styling, as shown in the first image.
For the shadow root that does not have shadowrootdelegatesfocus set you can click anywhere except the <input> and the focus does not change (if you select the <input> element it will look like the second image).

For the shadow root with shadowrootdelegatesfocus set, clicking on the text (which is non-focusable) selects the <input> element, as this is the first focusable element in the tree.
This also focuses the parent element as shown below.

  
    Avoiding DocumentFragment pitfalls
    When a DocumentFragment value is passed, Node.appendChild and similar methods move only the child nodes of that value into the target node. Therefore, it is usually preferable to attach event handlers to the children of a DocumentFragment, rather than to the DocumentFragment itself.
Consider the following HTML and JavaScript:
  
    HTML
    <div id="container"></div>

<template id="template">
  <div>Click me</div>
</template>

  
    JavaScript
    const container = document.getElementById("container");
const template = document.getElementById("template");

function clickHandler(event) {
  event.target.append(" — Clicked this div");
}

const firstClone = template.content.cloneNode(true);
firstClone.addEventListener("click", clickHandler);
container.appendChild(firstClone);

const secondClone = template.content.cloneNode(true);
secondClone.children[0].addEventListener("click", clickHandler);
container.appendChild(secondClone);

  
    Result
    Since firstClone is a DocumentFragment, only its children are added to container when appendChild is called; the event handlers of firstClone are not copied. In contrast, because an event handler is added to the first child node of secondClone, the event handler is copied when appendChild is called, and clicking on it works as one would expect.

  
    Technical summary
    
  
    
      
        Content categories
      
      
        Metadata content,
        flow content,
        phrasing content,
        script-supporting element
      
    
    
      Permitted content
      Nothing (see Usage notes)
    
    
      Tag omission
      None, both the starting and ending tag are mandatory.
    
    
      Permitted parents
      
        Any element that accepts
        metadata content,
        phrasing content, or
        script-supporting elements. Also allowed as a child of a <colgroup>
        element that does not have a
        span attribute.
      
    
    
      Implicit ARIA role
      
        No corresponding role
      
    
    
      Permitted ARIA roles
      No role permitted
    
    
      DOM interface
      HTMLTemplateElement
    
  

  
    Specifications
    
    
      
        Specification
      
    
    
      
              HTML# the-template-element
            
    
  
  
    Browser compatibility
    
  
    See also
    
part and exportparts HTML attributes
<slot> HTML element
:has-slotted, :host, :host(), and :host-context() CSS pseudo-classes
::part and ::slotted CSS pseudo-elements
ShadowRoot interface
Using templates and slots
CSS scoping module
Declarative Shadow DOM (with html) in Using Shadow DOM
Declarative shadow DOM on web.dev (2023)

   
      
    
          ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA['World Models,' an old idea in AI, mount a comeback]]></title>
            <link>https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45105710</guid>
            <description><![CDATA[You’re carrying around in your head a model of how the world works. Will AI systems need to do the same?]]></description>
            <content:encoded><![CDATA[
    The latest ambition of artificial intelligence research — particularly within the labs seeking “artificial general intelligence,” or AGI — is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Bengio (of Mila, the Quebec Artificial Intelligence Institute) all believe world models are essential for building AI systems that are truly smart, scientific and safe. 
The fields of psychology, robotics and machine learning have each been using some version of the concept for decades. You likely have a world model running inside your skull right now — it’s how you know not to step in front of a moving train without needing to run the experiment first. 
So does this mean that AI researchers have finally found a core concept whose meaning everyone can agree upon? As a famous physicist once wrote: Surely you’re joking. A world model may sound straightforward — but as usual, no one can agree on the details. What gets represented in the model, and to what level of fidelity? Is it innate or learned, or some combination of both? And how do you detect that it’s even there at all?

It helps to know where the whole idea started. In 1943, a dozen years before the term “artificial intelligence” was coined, a 29-year-old Scottish psychologist named Kenneth Craik published an influential monograph in which he mused that “if the organism carries a ‘small-scale model’ of external reality … within its head, it is able to try out various alternatives, conclude which is the best of them … and in every way to react in a much fuller, safer, and more competent manner.” Craik’s notion of a mental model or simulation presaged the “cognitive revolution” that transformed psychology in the 1950s and still rules the cognitive sciences today. What’s more, it directly linked cognition with computation: Craik considered the “power to parallel or model external events” to be “the fundamental feature” of both “neural machinery” and “calculating machines.”
The nascent field of artificial intelligence eagerly adopted the world-modeling approach. In the late 1960s, an AI system called SHRDLU wowed observers by using a rudimentary “block world” to answer commonsense questions about tabletop objects, like “Can a pyramid support a block?” But these handcrafted models couldn’t scale up to handle the complexity of more realistic settings. By the late 1980s, the AI and robotics pioneer Rodney Brooks had given up on world models completely, famously asserting that “the world is its own best model” and “explicit representations … simply get in the way.”
It took the rise of machine learning, especially deep learning based on artificial neural networks, to breathe life back into Craik’s brainchild. Instead of relying on brittle hand-coded rules, deep neural networks could build up internal approximations of their training environments through trial and error and then use them to accomplish narrowly specified tasks, such as driving a virtual race car. In the past few years, as the large language models behind chatbots like ChatGPT began to demonstrate emergent capabilities that they weren’t explicitly trained for — like inferring movie titles from strings of emojis, or playing the board game Othello — world models provided a convenient explanation for the mystery. To prominent AI experts such as Geoffrey Hinton, Ilya Sutskever and Chris Olah, it was obvious: Buried somewhere deep within an LLM’s thicket of virtual neurons must lie “a small-scale model of external reality,” just as Craik imagined.

The truth, at least so far as we know, is less impressive. Instead of world models, today’s generative AIs appear to learn “bags of heuristics”: scores of disconnected rules of thumb that can approximate responses to specific scenarios, but don’t cohere into a consistent whole. (Some may actually contradict each other.) It’s a lot like the parable of the blind men and the elephant, where each man only touches one part of the animal at a time and fails to apprehend its full form. One man feels the trunk and assumes the entire elephant is snakelike; another touches a leg and guesses it’s more like a tree; a third grasps the elephant’s tail and says it’s a rope. When researchers attempt to recover evidence of a world model from within an LLM — for example, a coherent computational representation of an Othello game board — they’re looking for the whole elephant. What they find instead is a bit of snake here, a chunk of tree there, and some rope.
Of course, such heuristics are hardly worthless. LLMs can encode untold sackfuls of them within their trillions of parameters — and as the old saw goes, quantity has a quality all its own. That’s what makes it possible to train a language model to generate nearly perfect directions between any two points in Manhattan without learning a coherent world model of the entire street network in the process, as researchers from Harvard University and the Massachusetts Institute of Technology recently discovered. 
So if bits of snake, tree and rope can do the job, why bother with the elephant? In a word, robustness: When the researchers threw their Manhattan-navigating LLM a mild curveball by randomly blocking 1% of the streets, its performance cratered. If the AI had simply encoded a street map whose details were consistent — instead of an immensely complicated, corner-by-corner patchwork of conflicting best guesses — it could have easily rerouted around the obstructions.
        
        
Given the benefits that even simple world models can confer, it’s easy to understand why every large AI lab is desperate to develop them — and why academic researchers are increasingly interested in scrutinizing them, too. Robust and verifiable world models could uncover, if not the El Dorado of AGI, then at least a scientifically plausible tool for extinguishing AI hallucinations, enabling reliable reasoning, and increasing the interpretability of AI systems.
That’s the “what” and “why” of world models. The “how,” though, is still anyone’s guess. Google DeepMind and OpenAI are betting that with enough “multimodal” training data — like video, 3D simulations, and other input beyond mere text — a world model will spontaneously congeal within a neural network’s statistical soup. Meta’s LeCun, meanwhile, thinks that an entirely new (and non-generative) AI architecture will provide the necessary scaffolding. In the quest to build these computational snow globes, no one has a crystal ball — but the prize, for once, may just be worth the hype.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps]]></title>
            <link>https://news.ycombinator.com/item?id=45104974</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104974</guid>
            <description><![CDATA[Hey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.]]></description>
            <content:encoded><![CDATA[Launch HN: Datafruit (YC S25) – AI for DevOps41 points by nickpapciak 7 hours ago  | hide | past | favorite | 32 commentsHey HN! We’re Abhi, Venkat, Tom, and Nick and we are building Datafruit (https://datafruit.dev/), an AI DevOps agent. We’re like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.Demo video: https://www.youtube.com/watch?v=2FitSggI7tg.Right now, we have two main methods to interact with Datafruit:(1) automated infrastructure audits— agents periodically scan your environment to find cost optimization opportunities, detect infrastructure drift, and validate your infra against compliance requirements.(2) chat interface (available as a web UI and through slack) — ask the agent questions for real-time insights, or assign tasks directly, such as investigating spend anomalies, reviewing security posture, or applying changes to IaC resources.Working at FAANG and various high-growth startups, we realized that infra work requires an enormous amount of context, often more than traditional software engineering. The business decisions, codebase, and cloud itself are all extremely important in any task that has been assigned. To maximize the success of the agents, we do a fair amount of context engineering. Not hallucinating is super important!One thing which has worked incredibly well for us is a multi-agent system where we have specialized sub-agents with access to specific tool calls and documentation for their specialty. Agents choose to “handoff” to each other when they feel like another agent would be more specialized for the task. However, all agents share the same context (https://cognition.ai/blog/dont-build-multi-agents). We’re pretty happy with this approach, and believe it could work in other disciplines which require high amounts of specialized expertise.Infrastructure is probably the most mission-critical part of any software organization, and needs extremely heavy guardrails to keep it safe. Language models are not yet at the point where they can be trusted to make changes (we’ve talked to a couple of startups where the Claude Code + AWS CLI combo has taken their infra down). Right now, Datafruit receives read-only access to your infrastructure and can only make changes through pull requests to your IaC repositories. The agent also operates in a sandboxed virtual environment so that it could not write cloud CLI commands if it wanted to!Where LLMs can add significant value is in reducing the constant operational inefficiencies that eat up cloud spend and delay deadlines—the small-but-urgent ops work. Once Datafruit indexes your environment, you can ask it to do things like:  "Grant @User write access to analytics S3 bucket for 24 hours"
    -> Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrow

  "Find where this secret is used so I can rotate it without downtime"
    -> Discovers all instances of your secret, including old cron-jobs you might not know about, so you can safely rotate your keys


  "Why did database costs spike yesterday?"
    -> Identifies expensive queries, shows optimization options, implements fixes


We charge a straightforward subscription model for a managed version, but we also offer a bring-your-own-cloud model. All of Datafruit can be deployed on Kubernetes using Helm charts for enterprise customers where data can’t leave your VPC.
For the time being, we’re installing the product ourselves on customers' clouds. It doesn’t exist in a self-serve form yet. We’ll get there eventually, but in the meantime if you’re interested we’d love for you guys to email us at founders@datafruit.dev.We would love to hear your thoughts! If you work with cloud infra, we are especially interested in learning about what kinds of work you do which you wish could be offloaded onto an agent.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anthropic raises $13B Series F]]></title>
            <link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104907</guid>
            <description><![CDATA[Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management & Research Company and Lightspeed Venture Partners. The investment reflects Anthropic’s continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.]]></description>
            <content:encoded><![CDATA[Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management & Research Company and Lightspeed Venture Partners. The investment reflects Anthropic’s continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.Significant investors in this round include Altimeter, Baillie Gifford, affiliated funds of BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price Associates, Inc., T. Rowe Price Investment Management, Inc., WCM Investment Management, and XN.“From Fortune 500 companies to AI-native startups, our customers rely on Anthropic’s frontier models and platform products for their most important, mission-critical work,” said Krishna Rao, Chief Financial Officer of Anthropic. “We are seeing exponential growth in demand across our entire customer base. This financing demonstrates investors’ extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.”Anthropic has seen rapid growth since the launch of Claude in March 2023. At the beginning of 2025, less than two years after launch, Anthropic’s run-rate revenue had grown to approximately $1 billion. By August 2025, just eight months later, our run-rate revenue reached over $5 billion—making Anthropic one of the fastest-growing technology companies in history.Anthropic’s trajectory has been driven by our leading technical talent, our focus on safety, and our frontier research, including pioneering alignment and interpretability work, all of which underpin the performance and reliability of our models. Every day more businesses, developers, and consumer power users are trusting Claude to help them solve their most challenging problems. Anthropic now serves over 300,000 business customers, and our number of large accounts—customers that each represent over $100,000 in run-rate revenue—has grown nearly 7x in the past year.This growth spans the entire Anthropic platform, with advancements for businesses, developers, and consumers. For businesses, our API and industry-specific products make it easy to add powerful AI to their critical applications without complex integration work. Developers have made Claude Code their tool of choice since its full launch in May 2025. Claude Code has quickly taken off—already generating over $500 million in run-rate revenue with usage growing more than 10x in just three months. For individual users, the Pro and Max plans for Claude deliver enhanced AI capabilities for everyday tasks and specialized projects.“Anthropic is on an exceptional trajectory, combining research excellence, technological leadership, and relentless focus on customers. We’re honored to partner with Dario and the team, and our lead investment in their Series F reflects our belief in their values and their ability to shape the future of responsible AI,” said Divesh Makan, Partner at ICONIQ. “Enterprise leaders tell us what we’re seeing firsthand—Claude is reliable, built on a trustworthy foundation, and guided by leaders truly focused on the long term.”The Series F investment will expand our capacity to meet growing enterprise demand, deepen our safety research, and support international expansion as we continue building reliable, interpretable, and steerable AI systems.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Static sites enable a good time travel experience]]></title>
            <link>https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104303</guid>
            <description><![CDATA[A static site with version control history enables me to travel into any point in the project’s past and serve the site as it was back in the day.]]></description>
            <content:encoded><![CDATA[
        
        
        
          Aug 30th, 2025
          by Juha-Matti Santala
          
        
	
          
        
	

        
          
        

        
          

  Varun wrote about
  gamifying blogging and personal website maintenance
  which reminded me of the time when
  I awarded myself some badges for blogging.



  I mentioned this to Varun who asked if I had any screenshots of what it looked
  like on my website. My initial answer was “no”, then I looked at Wayback
  Machine but there were not pictures of the badges.



  Then, a bit later it hit me. I don’t need any archived screenshots: my website
  is built with Eleventy and it's static so I can check out a git commit from
  the time I had those badges up, fire up Eleventy and see the website — as it
  was in the spring of 2021.


  That’s a beauty of a static site generator combined with my workflow of
  fetching posts from CMS before build time so each commit contains a full
  snapshot of the website.



  Comparing this to a website that uses a database for posts (like WordPress) or
  a flow where posts from CMS are not stored in version control but rather
  fetched at build time only, my solution makes time travel to (almost) any
  given moment in time a two-command operation (git checkout
  with the right commit hash and
  @11ty/eleventy serve to serve a dev
  server). I say almost because back in the day I wasn’t quite as diligent in
  commiting every change as I was deploying manually and not through version
  control automation.



  A year ago, inspired by Alex Chan’s blog post
  Taking regular screenshots of my website
  I set up a GitHub Action that takes a snapshot of my front page once a month
  to keep a record. At the time, I felt bit sad that I hadn’t started it before.
  However, now that I realised how easy it is for me to go back in time thanks
  to Eleventy and git, I’m not so worried anymore. Maybe I should do a collage
  of changes on my design one day by going through my project history.


One more major point for static site generators!



            
          If something above resonated with you, let's start a discussion about it! Email me at juhamattisantala at gmail dot com and share your thoughts. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.

        

        

        

        
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The staff ate it later]]></title>
            <link>https://en.wikipedia.org/wiki/The_staff_ate_it_later</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45104289</guid>
            <description><![CDATA[From Wikipedia, the free encyclopedia]]></description>
            <content:encoded><![CDATA[
							

						From Wikipedia, the free encyclopedia
					


"The staff ate it later" as shown on screen
"The staff ate it later" (Japanese: この後、スタッフが美味しくいただきました, romanized: Kono ato, sutaffu ga oishiku itadakimashita) is a caption shown on screen when food appears in a Japanese TV program to indicate that it was not thrown away after filming (it is generally not socially acceptable to discard food in Japan). Some question the authenticity of this statement or believe the caption lowers the quality of TV programs.



It is thought TV stations first began showing the caption to protect themselves against complaints from viewers who disliked food being handled without consideration in TV variety shows.[1] It is uncertain when this note was first used, but TV producer Kenji Suga [ja] stated viewers complained about the waste of food when a performance using small watermelons was broadcast in Downtown no Gaki no Tsukai ya Arahende!! on Nippon TV. The TV station then showed this note on screen the following year in response.[2]


There are various claims as to whether or not staff actually eat the food that appears in the programs.[1][3][4]


According to AOL News in 2014, the crew on one information program claimed: "It's sometimes impossible for the reporter to eat all the food provided by the restaurant. The reporter is told not to eat it all, but the crew will eat the rest out of consideration and a feeling of obligation towards the restaurant."[4]
Food comic artist Raswell Hosoki [ja] claimed in Meshizanmai Furusatonoaji (Meshizanmai Taste of Hometown) that the note is true. Eriko Miyazaki [ja], a reporter and TV personality for food shows, also claimed the note is true and stated: "The crew eats the rest of the food, at least at the shows I appear in."[5]
In January 2018, Miwa Asao, former professional beach volleyball player and TV personality, posted photos on her blog of staff eating food after recording "Saturday Night! Otona na TV [ja]". She wrote: "This is an on-site photo. The staff ate the rest of the food."[6]


Hitoshi Matsumoto, a comedian and TV host, was asked by sociologist Noritoshi Furuichi about this note in 2014 during the "Wide na Show [ja]" (Fuji Television). He said: "To be honest, I've never seen the crew eat the food. But that just means I haven't seen it. They might've eaten it."[7]
Takeshi Kitano (also known as Beat Takeshi), a Japanese comedian, actor, and filmmaker, referred to an instance where cake was smeared on the floor and said in his book Bakaron: "Liars. Who's going to enjoy cake they splattered all over the floor?"[3] Commentator Tsunehira Furuya also stated that the food featured in the show is not eaten by the staff later and is instead simply thrown into garbage bags.[1]


Commentator Tetsuya Uetaki has commented on displaying the note, saying: "Producers have become more aware as viewers have become more critical after issues such as the Aru Aru Mondai (a natto shortage caused by a program claiming eating natto would make people lose weight), and it's fine as one method for dealing with that." However, Uetaki went on to say: "This shifts responsibility onto the viewers. We can't let it end as simply an empty concession. I want to see variety shows strive to properly handle information and properly put the show together, from the moment they start building it."[8]
Broadcast writer Sotani [ja] commented on the fact that production teams have become more sensitive to this in programs and have begun displaying such notes as an attempt to preempt criticism. He claims this sort of extreme self regulation risks leading to a decline.[9] TV producer Kenji Suga [ja] claims it is necessary for programs to be disconnected from real life and society, to be "dumb and idiotic" to produce laughs.[2]
Columnist Takashi Matsuo argues that adults, not TV shows, should teach children the ethics surrounding the importance of food. He also argues that if a parent is uncomfortable with what a comedian expresses on TV, the right course of action would be to change the channel or turn off the TV, not send a complaint to the TV station.[10] Matsuo also points out the inconsistency that "the staff ate it later" caption is not displayed when large numbers of tomatoes are thrown at the festival of Tomatina in Spain or when athletes spray each other with champagne in celebration of a victory.[10]



^ a b c Furuya, Tsunehira (2017). 「道徳自警団」がニッポンを滅ぼす. East Shinsho: East Press. pp. 35–36. ISBN 978-4-7816-5095-1.

^ a b Wake, Shinya (7 February 2016). "グローブ176号 笑いの力 インタビュー 笑わせるってむずかしい プロデューサー・菅賢治". Asahi Shimbun. p. 6.

^ a b Kitano, Takeshi (2017). バカ論. Shinchosha. pp. 36–37. ISBN 978-4-10-610737-5.

^ a b "テレビ番組の食リポ、完食しているのか？「この後スタッフが美味しく...」は本当か" [Is the staff really eating the rest of the dishes used in the TV show?]. AOL News. 16 April 2014. Archived from the original on 16 September 2014. Retrieved 9 January 2020.

^ Raswell Hosoki, Mayumi Kato, Takako Aonuma, Sachiko Orihara, Junko Kubota, Eiko Kasai, Riyo Mizuki, Takotsumuri, Usami☆, and Somei Yoshino, (2017) Meshizanmai Hurusatonoaji, Bunkasha, BUNKASHA COMICS, ISBN 978-4-8211-3416-8

^ "バラエティの「この後スタッフが美味しく頂きました」　 予防線を張るテロップどこまで必要？" [Variety's "The staff enjoyed the food afterwards": How much precautionary captioning is necessary?]. Oricon News. 13 February 2018. Archived from the original on 18 September 2024. Retrieved 26 December 2020.

^ "松本人志 バラエティならでの葛藤を吐露「食べ物も笑いの1つの小道具として認めてもらえたら」" [Hitoshi Matsumoto, revealing his struggles with variety: "If people would accept food as a prop for laughter..."]. Nagai Times. 28 October 2014. Archived from the original on 2 December 2024. Retrieved 26 December 2020.

^ "近ごろよく見る『お断りテロップ』『視聴者への配慮』か苦情抗議"先逃れ"か ないよりましだが…『番組精査こそ肝心』識者指摘". Chunichi Shimbun. 4 July 2007. p. 15.

^ "1番ものがたり 人物編 売れっ子放送作家 そーたに氏「見せたくない」で金字塔 PTAの土俵に乗らず". Hokkoku Shimbun. 23 February 2012. p. 36.

^ a b Matsuo, Takashi (17 September 2017). "テレビの過剰なテロップ　苦情逃れの保身が目的？" [Is over-annotation on television a self-protection to escape complaints?]. Mainichi Shimbun Digital. Retrieved 26 December 2020.[dead link]







]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Removing Guix from Debian]]></title>
            <link>https://lwn.net/SubscriberLink/1035491/d8100135a8ae4246/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45103857</guid>
            <description><![CDATA[As a rule, if a package is shipped with a Debian release, users can count on it being available [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



As a rule, if a package is shipped with a Debian release, users can
count on it being available, and updated, for the entire
life of the release. If package foo is included in the stable
release—currently Debian 13
("trixie")—a user can
reasonably expect that it will continue to be available with security
backports as long as that release is supported, though it may not be
included in Debian 14 ("forky"). However, it is likely that the
Guix package manager will soon
be removed from the repositories for Debian 13 and
Debian 12 ("bookworm", also called oldstable).

Debian has the Advanced
Package Tool (APT) for package management, of course, but Guix
offers a different approach and can be used in conjunction with other
distribution package managers. Guix is inspired by Nix's
functional package management; it offers transactional upgrades and
rollbacks, package management for unprivileged users, and more. Unlike
Nix, its packages are defined
using the Guile
implementation of the Scheme programming
language. There is also a GNU Guix distribution as well; LWN covered both NixOS and
Guix in February 2024, and looked at Nix alternative Lix in July 2024.

On June 24, the Guix project disclosed
several security vulnerabilities that affected the guix-daemon,
which is a program that is used to build software and
access the store
where successful builds are kept. Two of the vulnerabilities, CVE-2025-46415
and CVE-2025-46416,
would allow a local user to gain the privileges of any build users,
manipulate build output, as well as gain the privileges of the daemon
user. The vulnerabilities also impacted Nix
and Lix
package managers.

The disclosure blog post gave instructions on how to mitigate the
vulnerabilities by updating guix-daemon using the
"guix pull" command, but the project did not make a new
Guix release. The last actual release from the project
was version 1.4.0,
which was announced in December 2022. The Guix project has a
rolling-release model, with sporadic releases, and does not maintain
stable branches with security updates. This may not pose a problem for
users getting Guix directly from the project, but it poses some
obstacles for inclusion in other Linux distributions.

Debian package

Salvatore Bonaccorso filed a bug
against Debian's
guix package
on June 25 to report the vulnerabilities. Vagrant
Cascadian, the maintainer of the package, replied
on July 15, and said that the fixes for the security
vulnerabilities had been "commingled with a lot of other upstream
changes", and it would be "trickier than in the past" to try
to backport the fixes without the other changes in Guix. He said he
had just managed to "get something to compile" with the
security fixes applied, using a backport
repository maintained by Denis 'GNUtoo' Carikli.

Carikli had brought
up the difficulty of backporting Guix fixes on the guix-devel
mailing list on July 8. Various distributions had Guix versions
1.2.0, 1.3.0, and 1.4.0, with Debian shipping 1.2.0 and 1.4.0 and used
as the upstream for other distributions' packages:


But the Debian package maintainer has the almost impossible task to
backport all the security fixes without a community nor help behind
[maintaining it] and as things are going, this will probably lead to
the Debian guix package being removed with cascading effect for the
other distributions.


He said he had applied about 50 patches that involve guix-daemon
between the 1.4.0 release and the last-known security fix. He also
noted that his effort would probably not be suitable for Linux
distributions that ship a Guix package. He wondered what the best way
would be to collaborate on a branch that distributions could pull from
for updates.

Liam Hupfer said
that "we gave up and shipped the last commit on master mentioned in
the recent CVE disclosure" for Nix. He said he would also like to
see Guix figure out backporting patches, but could Cascadian consider
the Nix approach until then?

No, that approach would not make sense, Cascadian said. If
Guix was "truly a rolling release", then it may just not make
sense to maintain distribution packages:


Up till recently, it has always been possible to backport changes with
relative ease, but that was perhaps just lack of development... the
recent unprivileged daemon stuff really made backporting patches
harder. [...]

In the Debian release model, ideally we would avoid bringing in
unrelated patchsets (e.g. the unprivileged daemon code bringing in an
entire network stack?) but that might be too hard to pull off. Not sure
if the security team would accept a patchset that includes more than the
minimum necessary to fix the security issues.


Without a branch that contained backported patches, they would be
inclined "to recommend dropping the Guix package in Debian".

On August 27, they did just that. Cascadian filed a bug
to remove guix from Debian 11 ("bullseye"), bookworm,
and trixie. They also filed a bug to
remove the package from the upcoming forky release. Adam D. Barratt replied
and said that it would not be possible to remove guix from
bullseye, which is now an LTS
release; only updates from the security archive were now
allowed.

There are no dependencies on guix, so the removal of the
package should not affect any other Debian packages in the trixie and
bookworm releases.

What's next?

I emailed Cascadian to find out what the next steps would be for
the package in those releases. They said that the package is likely to
be removed from the upcoming point
releases for trixie and bookworm. Users who have it installed
already will be stuck at the old version, unless they take manual
steps to update. Cascadian said this was not a great outcome "but
better than keeping it available for people to install the vulnerable
version".

The guix package should not have landed in trixie at all,
Cascadian said, "my understanding was that [the bug report] should
have blocked it from being released". However, it seems there was
enough activity on the bug that prevented Debian's autoremoval
mechanisms from triggering and pulling the package from the final
release. "There apparently is not manual review of all blockers in
the current Debian release process". Even if the timing had been
better for the trixie release, though, the same fixes would need to
have been applied to bookworm and older releases.

Cascadian said that it has been a fair amount of work to keep Guix
working on Debian. For example, he has had to maintain various Guile
dependencies, and deal with the fact that Guix uses "fairly
old" GCC versions whereas Debian usually ships the latest GCC
version available for a given release. At some point, they said,
"you have to evaluate whether that work is worth it" when the
upstream provides a binary that people can install.

Guix is better for having been packaged for Debian and run through
Debian's Lintian
tool. Cascadian said that they have probably fixed more typos in Guix
than anyone else, and have found other problems while checking that
the builds of Guix are reproducible. "Any time you run a piece of
software through processes outside of the primary development
workflow, you find surprises worth fixing."

Regular releases

There is an effort in the Guix project to create yearly releases. In
May, Steve George proposed that the project adopt a Guix
Consensus Document for "Regular
and efficient releases", and it was accepted by the project in
July. It calls for a release every year in June, with a one-time
exception for a November 2025 release, and a short development
cycle for the June 2026 release. Even so, that will not provide
stable branches for Debian and other distributions to pull from; it
will just shorten the interval and feature differences between major
releases.

Debian users will, of course, still be able to use Guix by installing
it using binaries provided by the Guix project. That will, at
least potentially, leave some users out in the cold—Debian
currently provides a guix package for x86-64, Arm64, PowerPC,
RISC-V, 32-bit Arm, and 32-bit x86. The Guix project itself does does
not have RISC-V binaries, though it does cover the other
architectures.

It is fairly unusual for a package to be removed from a stable or
oldstable release. For example, the bookworm release has been out for
more than two years, but a search
of the Debian bug database only shows one
package—guix—that has the "RM" tag in the subject
to flag a package for removal. 

According to Debian's popularity
contest (popcon) statistics, there are not quite 230 systems with
Guix installed. Popcon statistics only hint at the actual number of
package installs, but assuming they are approximately accurate, then
removing Guix will not inconvenience a significant number of Debian
users. It will, however, mean that fewer people are poking at Guix
with an intent to making it work on distributions like Debian, while finding
distribution-specific bugs.


               
               
            ]]></content:encoded>
        </item>
    </channel>
</rss>