<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 14 Sep 2025 02:17:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[RFC9460: SVCB and HTTPS DNS Records]]></title>
            <link>https://datatracker.ietf.org/doc/html/rfc9460</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236444</guid>
            <description><![CDATA[This document specifies the "SVCB" ("Service Binding") and "HTTPS" DNS resource record (RR) types to facilitate the lookup of information needed to make connections to network services, such as for HTTP origins. SVCB records allow a service to be provided from multiple alternative endpoints, each with associated parameters (such as transport protocol configuration), and are extensible to support future uses (such as keys for encrypting the TLS ClientHello). They also enable aliasing of apex domains, which is not possible with CNAME. The HTTPS RR is a variation of SVCB for use with HTTP (see RFC 9110, "HTTP Semantics"). By providing more information to the client before it attempts to establish a connection, these records offer potential benefits to both performance and privacy.]]></description>
            <content:encoded><![CDATA[


RFC 9460
SVCB and HTTPS RRs for DNS
November 2023


Schwartz, et al.
Standards Track
[Page]





Service Binding and Parameter Specification via the DNS (SVCB and HTTPS Resource Records)

      Abstract
This document specifies the "SVCB" ("Service Binding") and "HTTPS" DNS resource record (RR)
types to facilitate the lookup of information needed to make connections
to network services, such as for HTTP origins.  SVCB records
allow a service to be provided from multiple alternative endpoints,
each with associated parameters (such as transport protocol
configuration), and are extensible to support future uses
(such as keys for encrypting the TLS ClientHello).  They also
enable aliasing of apex domains, which is not possible with CNAME.
The HTTPS RR is a variation of SVCB for use with HTTP (see RFC 9110, "HTTP Semantics").
By providing more information to the client before it attempts to
establish a connection, these records offer potential benefits to
both performance and privacy.¶


        
Status of This Memo
        

            This is an Internet Standards Track document.¶

            This document is a product of the Internet Engineering Task Force
            (IETF).  It represents the consensus of the IETF community.  It has
            received public review and has been approved for publication by
            the Internet Engineering Steering Group (IESG).  Further
            information on Internet Standards is available in Section 2 of 
            RFC 7841.¶

            Information about the current status of this document, any
            errata, and how to provide feedback on it may be obtained at
            https://www.rfc-editor.org/info/rfc9460.¶


        
Copyright Notice
        

            Copyright (c) 2023 IETF Trust and the persons identified as the
            document authors. All rights reserved.¶

            This document is subject to BCP 78 and the IETF Trust's Legal
            Provisions Relating to IETF Documents
            (https://trustee.ietf.org/license-info) in effect on the date of
            publication of this document. Please review these documents
            carefully, as they describe your rights and restrictions with
            respect to this document. Code Components extracted from this
            document must include Revised BSD License text as described in
            Section 4.e of the Trust Legal Provisions and are provided without
            warranty as described in the Revised BSD License.¶


        ▲
Table of Contents
        


            1.  Introduction


                1.1.  Goals

              
                1.2.  Overview of the SVCB RR

              
                1.3.  Terminology

            

          
            2.  The SVCB Record Type


                2.1.  Zone-File Presentation Format

              
                2.2.  RDATA Wire Format

              
                2.3.  SVCB Query Names

              
                2.4.  Interpretation


                    2.4.1.  SvcPriority

                  
                    2.4.2.  AliasMode

                  
                    2.4.3.  ServiceMode

                

              
                2.5.  Special Handling of "." in TargetName


                    2.5.1.  AliasMode

                  
                    2.5.2.  ServiceMode

                

            

          
            3.  Client Behavior


                3.1.  Handling Resolution Failures

              
                3.2.  Clients Using a Proxy

            

          
            4.  DNS Server Behavior


                4.1.  Authoritative Servers

              
                4.2.  Recursive Resolvers


                    4.2.1.  DNS64

                

              
                4.3.  General Requirements

              
                4.4.  EDNS Client Subnet (ECS)

            

          
            5.  Performance Optimizations


                5.1.  Optimistic Pre-connection and Connection Reuse

              
                5.2.  Generating and Using Incomplete Responses

            

          
            6.  SVCB-Compatible RR Types

          
            7.  Initial SvcParamKeys


                7.1.  "alpn" and "no-default-alpn"


                    7.1.1.  Representation

                  
                    7.1.2.  Use

                

              
                7.2.  "port"

              
                7.3.  "ipv4hint" and "ipv6hint"

              
                7.4.  "mandatory"

            

          
            8.  ServiceMode RR Compatibility and Mandatory Keys

          
            9.  Using Service Bindings with HTTP


                9.1.  Query Names for HTTPS RRs

              
                9.2.  Comparison with Alt-Svc


                    9.2.1.  ALPN Usage

                  
                    9.2.2.  Untrusted Channels

                  
                    9.2.3.  Cache Lifetime

                  
                    9.2.4.  Granularity

                

              
                9.3.  Interaction with Alt-Svc

              
                9.4.  Requiring Server Name Indication

              
                9.5.  HTTP Strict Transport Security (HSTS)

              
                9.6.  Use of HTTPS RRs in Other Protocols

            

          
            10. Zone Structures


                10.1.  Structuring Zones for Flexibility

              
                10.2.  Structuring Zones for Performance

              
                10.3.  Operational Considerations

              
                10.4.  Examples


                    10.4.1.  Protocol Enhancements

                  
                    10.4.2.  Apex Aliasing

                  
                    10.4.3.  Parameter Binding

                  
                    10.4.4.  Multi-CDN Configuration

                  
                    10.4.5.  Non-HTTP Uses

                

            

          
            11. Interaction with Other Standards

          
            12. Security Considerations

          
            13. Privacy Considerations

          
            14. IANA Considerations


                14.1.  SVCB RR Type

              
                14.2.  HTTPS RR Type

              
                14.3.  New Registry for Service Parameters


                    14.3.1.  Procedure

                  
                    14.3.2.  Initial Contents

                

              
                14.4.  Other Registry Updates

            

          
            15. References


                15.1.  Normative References

              
                15.2.  Informative References

            

          
            Appendix A.  Decoding Text in Zone Files


                A.1.  Decoding a Comma-Separated List

            

          
            Appendix B.  HTTP Mapping Summary

          
            Appendix C.  Comparison with Alternatives


                C.1.  Differences from the SRV RR Type

              
                C.2.  Differences from the Proposed HTTP Record

              
                C.3.  Differences from the Proposed ANAME Record

              
                C.4.  Comparison with Separate RR Types for AliasMode and ServiceMode

            

          
            Appendix D.  Test Vectors


                D.1.  AliasMode

              
                D.2.  ServiceMode

              
                D.3.  Failure Cases

            

          
            Acknowledgments and Related Proposals

          
            Authors' Addresses

        



      
1. Introduction
      
The SVCB ("Service Binding") and HTTPS resource records (RRs) provide clients with complete instructions
for access to a service.  This information enables improved
performance and privacy by avoiding transient connections to a suboptimal
default server, negotiating a preferred protocol, and providing relevant
public keys.¶
For example, HTTP clients currently resolve only A and/or AAAA records for
the origin hostname, learning only its IP addresses.  If an HTTP client learns
more about the origin before connecting, it may be able to upgrade "http" URLs
to "https", enable HTTP/3 or Encrypted ClientHello [ECH],
or switch to an
operationally preferable endpoint.  It is highly desirable to minimize the
number of round trips and lookups required to
learn this additional information.¶
The SVCB and HTTPS RRs also help when the operator of a service
wishes to delegate operational control to one or more other domains, e.g.,
aliasing the origin "https://example.com" to a service
operator endpoint at "svc.example.net".  While this case can sometimes
be handled by a CNAME, that does not cover all use cases.  CNAME is also
inadequate when the service operator needs to provide a bound
collection of consistent configuration parameters through the DNS
(such as network location, protocol, and keying information).¶
This document first describes the SVCB RR as a general-purpose RR that can be applied directly and efficiently to a wide range
of services (Section 2).  It also describes the rules for defining other
SVCB-compatible RR types (Section 6), starting with the HTTPS
RR type (Section 9), which provides improved efficiency and convenience
with HTTP by avoiding the need for an Attrleaf label [Attrleaf]
(Section 9.1).¶
The SVCB RR has two modes: 1) "AliasMode", which simply delegates operational
control for a resource and 2) "ServiceMode", which binds together
configuration information for a service endpoint.
ServiceMode provides additional key=value parameters
within each RDATA set.¶

        
1.1. Goals
        
The goal of the SVCB RR is to allow clients to resolve a single
additional DNS RR in a way that:¶

Provides alternative endpoints that are authoritative for the service,
along with parameters associated with each of these endpoints.¶

          Does not assume that all alternative endpoints have the same parameters
or capabilities, or are even
operated by the same entity.  This is important, as DNS does not
provide any way to tie together multiple RRsets for the same name.
For example, if "www.example.com" is a CNAME alias that switches
between one of three Content Delivery Networks (CDNs) or hosting environments, successive queries
for that name may return records that correspond to different environments.¶

          Enables CNAME-like functionality at a zone apex (such as
"example.com") for participating protocols and generally
enables extending operational authority for a service identified
by a domain name to other instances with alternate names.¶

        
Additional goals specific to HTTPS RRs and the HTTP use cases include:¶

Connecting directly to HTTP/3 (QUIC transport)
alternative endpoints [HTTP/3].¶

          Supporting non-default TCP and UDP ports.¶

          Enabling SRV-like benefits (e.g., apex aliasing, as mentioned above) for HTTP,
where SRV [SRV] has not been widely adopted.¶

          Providing an indication signaling that the "https" scheme should
          be used instead of "http" for all HTTP requests to this host and port,
          similar to HTTP Strict Transport Security [HSTS] (see
Section 9.5).¶

          Enabling the conveyance of Encrypted ClientHello keys [ECH] associated
with an alternative endpoint.¶

        


        
1.2. Overview of the SVCB RR
        
This subsection briefly describes the SVCB RR with forward references to
the full exposition of each component.  (As discussed in Section 6, this all
applies equally to the HTTPS RR, which shares
the same encoding, format, and high-level semantics.)¶
The SVCB RR has two modes: 1) AliasMode (Section 2.4.2), which aliases a name
to another name and 2) ServiceMode (Section 2.4.3), which provides connection
information bound to a service endpoint domain.  Placing both forms in a single
RR type allows clients to
fetch the relevant information with a single query (Section 2.3).¶
The SVCB RR has two required fields and one optional field.  The fields are:¶

          SvcPriority (Section 2.4.1):
          The priority of this record (relative to others,
with lower values preferred).  A value of 0 indicates AliasMode.¶

          
TargetName:
          The domain name of either the alias target (for
AliasMode) or the alternative endpoint (for ServiceMode).¶

          
SvcParams (optional):
          A list of key=value pairs
describing the alternative endpoint at
TargetName (only used in ServiceMode and otherwise ignored).
SvcParams are described in Section 2.1.¶

        

Cooperating DNS recursive resolvers will perform subsequent record
resolution (for SVCB, A, and AAAA records) and return them in the
Additional section of the response (Section 4.2).  Clients either use responses
included in the Additional section returned by the recursive resolver
or perform necessary SVCB, A, and AAAA record resolutions (Section 3).  DNS
authoritative servers can attach in-bailiwick SVCB, A, AAAA, and CNAME
records in the Additional section to responses for a SVCB query (Section 4.1).¶
In ServiceMode, the SvcParams of the SVCB RR
provide an extensible data model for describing alternative
endpoints that are authoritative for a service, along with
parameters associated with each of these alternative endpoints (Section 7).¶
For HTTP use cases, the HTTPS RR (Section 9) enables many of the benefits of Alt-Svc
[AltSvc]
without waiting for a full HTTP connection initiation (multiple round trips)
before learning of the preferred alternative,
and without necessarily revealing the user's
intended destination to all entities along the network path.¶


        
1.3. Terminology
        
Terminology in this document is based on the common case where the SVCB record is used to
access a resource identified by a URI whose authority field contains a DNS
hostname as the host.¶

The "service" is the information source identified by the authority and
scheme of the URI, capable of providing access to the resource.  For "https"
URIs, the "service" corresponds to an "origin" [RFC6454].¶

          The "service name" is the host portion of the authority.¶

          The "authority endpoint" is the authority's hostname and a port number implied
by the scheme or specified in the URI.¶

          An "alternative endpoint" is a hostname, port number, and other associated
instructions to the client on how to reach an instance of a service.¶

        
Additional DNS terminology intends to be consistent
with [DNSTerm].¶
SVCB is a contraction of "service binding".  The SVCB RR, HTTPS RR,
and future RR types that share SVCB's formats and registry are
collectively known as SVCB-compatible RR types.  The contraction "SVCB" is also
used to refer to this system as a whole.¶
The key words "MUST", "MUST NOT",
       "REQUIRED", "SHALL",
       "SHALL NOT", "SHOULD",
       "SHOULD NOT",
       "RECOMMENDED", "NOT RECOMMENDED",
       "MAY", and "OPTIONAL" in this document
       are to be interpreted as described in BCP 14
       [RFC2119] [RFC8174] when, and only
       when, they appear in all capitals, as shown here.¶



      
2. The SVCB Record Type
      
The SVCB DNS RR type (RR type 64)
is used to locate alternative endpoints for a service.¶
The algorithm for resolving SVCB records and associated
address records is specified in Section 3.¶
Other SVCB-compatible RR types
can also be defined as needed (see Section 6).  In particular, the
HTTPS RR (RR type 65) provides special handling
for the case of "https" origins as described in Section 9.¶
SVCB RRs are extensible by a list of SvcParams, which are pairs consisting of a
SvcParamKey and a SvcParamValue. Each SvcParamKey has a presentation name and a
registered number. Values are in a format specific to the SvcParamKey.  Each
SvcParam has a specified presentation format (used in zone files) and
wire encoding
(e.g., domain names, binary data, or numeric values). The initial SvcParamKeys
and their formats are defined in Section 7.¶

        
2.1. Zone-File Presentation Format
        
The presentation format <RDATA> of the record ([RFC1035], Section 5.1) has
the form:¶

SvcPriority TargetName SvcParams
¶

The SVCB record is defined specifically within
the Internet ("IN") Class ([RFC1035], Section 3.2.4).¶
SvcPriority is a number in the range 0-65535,
TargetName is a <domain-name> ([RFC1035], Section 5.1),
and the SvcParams are a whitespace-separated list with each SvcParam
consisting of a SvcParamKey=SvcParamValue pair or a standalone SvcParamKey.
SvcParamKeys are registered by IANA (Section 14.3).¶
Each SvcParamKey SHALL appear at most once in the SvcParams.
In presentation format, SvcParamKeys are lowercase alphanumeric strings.
Key names contain 1-63 characters from the ranges "a"-"z", "0"-"9", and "-".
In ABNF [RFC5234],¶

alpha-lc      = %x61-7A   ; a-z
SvcParamKey   = 1*63(alpha-lc / DIGIT / "-")
SvcParam      = SvcParamKey ["=" SvcParamValue]
SvcParamValue = char-string ; See Appendix A.
value         = *OCTET ; Value before key-specific parsing
¶

The SvcParamValue is parsed using the
character-string decoding algorithm (Appendix A), producing a value.
The value is then validated and converted into wire format in a manner
specific to each key.¶
When the optional "=" and SvcParamValue are omitted, the value is
interpreted as empty.¶
Arbitrary keys can be represented using the unknown-key presentation format
"keyNNNNN" where NNNNN is the numeric
value of the key type without leading zeros.
A SvcParam in this form SHALL be parsed as specified above, and
the decoded value SHALL be used as its wire-format encoding.¶
For some SvcParamKeys, the value corresponds to a list or set of
items.  Presentation formats for such keys SHOULD use a comma-separated list
(Appendix A.1).¶
SvcParams in presentation format MAY appear in any order, but keys MUST NOT be
repeated.¶


        
2.2. RDATA Wire Format
        
The RDATA for the SVCB RR consists of:¶

a 2-octet field for SvcPriority as an integer in network
byte order.¶

          the uncompressed, fully qualified TargetName, represented as
a sequence of length-prefixed labels per Section 3.1 of [RFC1035].¶

          the SvcParams, consuming the remainder of the record
(so smaller than 65535 octets and constrained by the RDATA
and DNS message sizes).¶

        
When the list of SvcParams is non-empty, it contains a series of
SvcParamKey=SvcParamValue pairs, represented as:¶

a 2-octet field containing the SvcParamKey as an
integer in network byte order.  (See Section 14.3.2 for the defined values.)¶

          a 2-octet field containing the length of the SvcParamValue
as an integer between 0 and 65535 in network byte order.¶

          an octet string of this length whose contents are the SvcParamValue in a
format determined by the SvcParamKey.¶

        
SvcParamKeys SHALL appear in increasing numeric order.¶
Clients MUST consider an RR malformed if:¶

the end of the RDATA occurs within a SvcParam.¶

          SvcParamKeys are not in strictly increasing numeric order.¶

          the SvcParamValue for a SvcParamKey does not have the expected format.¶

        
Note that the second condition implies that there are no duplicate
SvcParamKeys.¶
If any RRs are malformed, the client MUST reject the entire RRset and
fall back to non-SVCB connection establishment.¶


        
2.3. SVCB Query Names
        
When querying the SVCB RR, a service is translated into a QNAME by prepending
the service name with a label indicating the scheme, prefixed with an underscore,
resulting in a domain name like "_examplescheme.api.example.com.".  This
follows the Attrleaf naming pattern [Attrleaf], so the scheme MUST be
registered appropriately with IANA (see Section 11).¶
Protocol mapping documents MAY specify additional underscore-prefixed labels
to be prepended.  For schemes that specify a port (Section 3.2.3 of [URI]), one reasonable possibility is to prepend the indicated port
number if a non-default port number is specified.  This document terms this behavior
"Port Prefix Naming" and uses it in the examples throughout.¶
See Section 9.1 for information regarding HTTPS RR behavior.¶
When a prior CNAME or SVCB record has aliased to
a SVCB record, each RR SHALL be returned under its own owner name, as in
ordinary CNAME processing ([RFC1034], Section 3.6.2).  For details, see
the recommendations regarding aliases for clients (Section 3),
servers (Section 4), and zones (Section 10).¶
Note that none of these forms alter the origin or authority for validation
purposes.
For example, TLS clients MUST continue to validate TLS certificates
for the original service name.¶
As an example, the owner of "example.com" could publish this record:¶

_8443._foo.api.example.com. 7200 IN SVCB 0 svc4.example.net.
¶

This record would indicate that "foo://api.example.com:8443" is aliased to "svc4.example.net".
The owner of "example.net", in turn, could publish this record:¶

svc4.example.net.  7200  IN SVCB 3 svc4.example.net. (
    alpn="bar" port="8004" )
¶

This record would indicate that these services are served on port number 8004,
which supports the protocol "bar" and its associated transport in
addition to the default transport protocol for "foo://".¶
(Parentheses are used to ignore a line break in DNS zone-file presentation
format, per Section 5.1 of [RFC1035].)¶


        
2.4. Interpretation
        

          
2.4.1. SvcPriority
          
When SvcPriority is 0, the SVCB record is in AliasMode (Section 2.4.2).
Otherwise, it is in ServiceMode (Section 2.4.3).¶
Within a SVCB RRset,
all RRs SHOULD have the same mode.
If an RRset contains a record in AliasMode, the recipient MUST ignore
any ServiceMode records in the set.¶
RRsets are explicitly unordered collections, so the
SvcPriority field is used to impose an ordering on SVCB RRs.
A smaller SvcPriority indicates that the domain owner recommends the use of this
record over ServiceMode RRs with a larger SvcPriority value.¶
When receiving an RRset containing multiple SVCB records with the
same SvcPriority value, clients SHOULD apply a random shuffle within a
priority level to the records before using them, to ensure uniform
load balancing.¶


          
2.4.2. AliasMode
          
In AliasMode, the SVCB record aliases a service to a
TargetName.  SVCB RRsets SHOULD only have a single RR in AliasMode.  If multiple AliasMode RRs are present, clients or recursive
resolvers SHOULD pick one at random.¶
The primary purpose of AliasMode is to allow aliasing at the zone
apex, where CNAME is not allowed (see, for example, [RFC1912], Section 2.4).
In AliasMode, the TargetName will
be the name of a domain that resolves to SVCB,
AAAA, and/or A records.  (See Section 6 for aliasing of SVCB-compatible RR types.)
Unlike CNAME, AliasMode records do not affect the resolution of other RR
types and apply only to a specific service, not an entire domain name.¶
The AliasMode TargetName SHOULD NOT be equal
to the owner name, as this would result in a loop.
In AliasMode, recipients MUST ignore any SvcParams that are present.
Zone-file parsers MAY emit a warning if an AliasMode record has SvcParams.
The use of SvcParams in AliasMode records is currently not defined, but a
future specification could extend AliasMode records to include SvcParams.¶
For example, the operator of "foo://example.com:8080" could
point requests to a service operating at "foosvc.example.net"
by publishing:¶

_8080._foo.example.com. 3600 IN SVCB 0 foosvc.example.net.
¶

Using AliasMode maintains a separation of concerns: the owner of
"foosvc.example.net" can add or remove ServiceMode SVCB records without
requiring a corresponding change to "example.com".  Note that if
"foosvc.example.net" promises to always publish a SVCB record, this AliasMode
record can be replaced by a CNAME at the same owner name.¶
AliasMode is especially useful for SVCB-compatible RR types that do not
require an underscore prefix, such as the HTTPS RR type.  For example,
the operator of "https://example.com" could point requests to a server
at "svc.example.net" by publishing this record at the zone apex:¶

example.com. 3600 IN HTTPS 0 svc.example.net.
¶

Note that the SVCB record's owner name MAY be the canonical name
of a CNAME record, and the TargetName MAY be the owner of a CNAME
record. Clients and recursive resolvers MUST follow CNAMEs as normal.¶
To avoid unbounded alias chains, clients and recursive resolvers MUST impose a
limit on the total number of SVCB aliases they will follow for each resolution
request.  This limit MUST NOT be zero, i.e., implementations MUST be able to
follow at least one AliasMode record.  The exact value of this limit
is left to implementations.¶
Zones that require following multiple AliasMode records could encounter
compatibility and performance issues.¶
As legacy clients will not know to use this record, service
operators will likely need to retain fallback AAAA and A records
alongside this SVCB record, although in a common case
the target of the SVCB record might offer better performance, and
therefore would be preferable for clients implementing this specification
to use.¶
AliasMode records only apply to queries for the specific RR type.
For example, a SVCB record cannot alias to an HTTPS record or vice versa.¶


          
2.4.3. ServiceMode
          
In ServiceMode, the TargetName and SvcParams within each RR
associate an alternative endpoint for the service with its connection
parameters.¶
Each protocol scheme that uses SVCB MUST define a protocol mapping that
explains how SvcParams are applied for connections of that scheme.
Unless specified otherwise by the
protocol mapping, clients MUST ignore any SvcParam that they do
not recognize.¶
Some SvcParams impose requirements on other SvcParams in the RR.  A
ServiceMode RR is called "self-consistent" if its SvcParams all comply with
each other's requirements.  Clients MUST reject any RR whose recognized
SvcParams are not self-consistent and MAY reject the entire RRset.  To
help zone operators avoid this condition, zone-file implementations SHOULD
enforce self-consistency as well.¶



        
2.5. Special Handling of "." in TargetName
        
If TargetName has the value "." (represented in the wire format as a
zero-length label), special rules apply.¶

          
2.5.1. AliasMode
          
For AliasMode SVCB RRs, a TargetName of "." indicates that the service
is not available or does not exist.  This indication is advisory:
clients encountering this indication MAY ignore it and attempt to connect
without the use of SVCB.¶


          
2.5.2. ServiceMode
          
For ServiceMode SVCB RRs, if TargetName has the value ".", then the
owner name of this record MUST be used as the effective TargetName.
If the record has a wildcard owner name in the zone file, the recipient
SHALL use the response's synthesized owner name as the effective TargetName.¶
Here, for example, "svc2.example.net" is the effective TargetName:¶

example.com.      7200  IN HTTPS 0 svc.example.net.
svc.example.net.  7200  IN CNAME svc2.example.net.
svc2.example.net. 7200  IN HTTPS 1 . port=8002
svc2.example.net. 300   IN A     192.0.2.2
svc2.example.net. 300   IN AAAA  2001:db8::2
¶





      
3. Client Behavior
      
"SVCB resolution" is the process of enumerating and ordering the available endpoints
for a service, as performed by the client.  SVCB resolution is implemented as follows:¶

Let $QNAME be the service name plus appropriate prefixes for the
scheme (see Section 2.3).¶

        Issue a SVCB query for $QNAME.¶

        If an AliasMode SVCB record is returned for $QNAME (after following CNAMEs
as normal), set $QNAME to its TargetName (without
additional prefixes) and loop back to Step 2,
subject to chain length limits and loop detection heuristics (see
Section 3.1).¶

        If one or more "compatible" (Section 8) ServiceMode records are returned,
these represent the alternative endpoints. Sort the records by ascending SvcPriority.¶

        Otherwise, SVCB resolution has failed, and the list of available endpoints is
empty.¶

      
This procedure does not rely on any recursive or authoritative DNS server to
comply with this specification or have any awareness of SVCB.¶
A client is called "SVCB-optional" if it can connect without the use of
ServiceMode records; otherwise, it is called "SVCB-reliant".  Clients for pre-existing
protocols (e.g., HTTP) SHALL implement SVCB-optional behavior (except as
noted in Section 3.1 or when modified by future specifications).¶
SVCB-optional clients SHOULD issue in parallel any other DNS queries that might
be needed for connection establishment if the SVCB record is absent, in order to minimize delay
in that case and enable the optimizations discussed in Section 5.¶
Once SVCB resolution has concluded, whether successful or not,
if at least one AliasMode record was processed,
SVCB-optional clients SHALL append to the list of endpoints an
endpoint consisting of the final value of $QNAME, the authority
endpoint's port number, and no SvcParams.  (This endpoint will be
attempted before falling back to non-SVCB connection modes.  This ensures that
SVCB-optional clients will make use of an AliasMode record whose TargetName has
A and/or AAAA records but no SVCB records.)¶
The client proceeds with connection establishment using this list of
endpoints.  Clients SHOULD try higher-priority alternatives first, with
fallback to lower-priority alternatives.  Clients resolve AAAA and/or A
records for the selected TargetName and MAY choose between them using an
approach such as Happy Eyeballs [HappyEyeballsV2].¶
If the client is SVCB-optional and connecting using this list of endpoints has
failed, the client now attempts to use non-SVCB connection modes.¶
Some important optimizations are discussed in Section 5
to avoid additional latency in comparison to ordinary AAAA/A lookups.¶

        
3.1. Handling Resolution Failures
        
If DNS responses are cryptographically protected (e.g., using DNSSEC or
TLS [DoT] [DoH]) and SVCB resolution fails
due to an authentication error, SERVFAIL response, transport error, or
timeout, the client SHOULD abandon its attempt to reach the service, even
if the client is SVCB-optional.  Otherwise, an active attacker
could mount a downgrade attack by denying the user access to the SvcParams.¶
A SERVFAIL error can occur if the domain is DNSSEC-signed, the recursive
resolver is DNSSEC-validating, and the attacker is between the recursive
resolver and the authoritative DNS server.  A transport error or timeout can
occur if an active attacker between the client and the recursive resolver is
selectively dropping SVCB queries or responses, based on their size or
other observable patterns.¶
If the client enforces DNSSEC validation on A/AAAA responses, it SHOULD
apply the same validation policy to SVCB.  Otherwise, an attacker could
defeat the A/AAAA protection by forging SVCB responses that direct the
client to other IP addresses.¶
If DNS responses are not cryptographically protected, clients MAY treat
SVCB resolution failure as fatal or nonfatal.¶
If the client is unable to complete SVCB resolution due to its chain length
limit, the client MUST fall back to the authority endpoint, as if the
service's SVCB record did not exist.¶


        
3.2. Clients Using a Proxy
        
Clients using a domain-oriented transport proxy like HTTP CONNECT
([RFC7231], Section 4.3.6) or SOCKS5 [RFC1928] have the option of
using named destinations, in which case the client does not perform
any A or AAAA queries for destination domains.  If the client is configured
to use named
destinations with a proxy that does not provide SVCB query capability
(e.g., through an affiliated DNS resolver), the client would have to perform
SVCB resolution separately, likely disclosing the destinations to additional parties and not just the proxy.
Clients in this configuration SHOULD arrange for a separate SVCB resolution
procedure with appropriate privacy properties.  If this is not possible,
SVCB-optional clients MUST disable SVCB resolution entirely, and SVCB-reliant
clients MUST treat the configuration as invalid.¶
If the client does use SVCB and named destinations, the client SHOULD follow
the standard SVCB resolution process, selecting the smallest-SvcPriority
option that is compatible with the client and the proxy.  When connecting
using a SVCB record, clients MUST provide the final TargetName and port to the
proxy, which will perform any required A and AAAA lookups.¶
This arrangement has several benefits:¶


            Compared to disabling SVCB:¶

It allows the client to use the SvcParams, if present, which are
only usable with a specific TargetName.  The SvcParams may
include information that enhances performance (e.g., supported protocols) and privacy.¶

              It allows a service on an apex domain to use aliasing.¶

            

          
            Compared to providing the proxy with an IP address:¶

It allows the proxy to select between IPv4 and IPv6 addresses for the
server according to its configuration.¶

              It ensures that the proxy receives addresses based on its network
geolocation, not the client's.¶

              It enables faster fallback for TCP destinations with multiple addresses
of the same family.¶

            

        



      
4. DNS Server Behavior
      

        
4.1. Authoritative Servers
        
When replying to a SVCB query, authoritative DNS servers SHOULD return
A, AAAA, and SVCB records in the Additional section for any TargetNames
that are in the zone.  If the zone is signed, the server SHOULD also
include DNSSEC records authenticating the existence or nonexistence of these records
in the Additional section.¶
See Section 4.4 for exceptions.¶


        
4.2. Recursive Resolvers
        
Whether the recursive resolver is aware of SVCB or not, the normal response
construction process used for unknown RR types [RFC3597]
generates the Answer section of the response.
Recursive resolvers that are aware of SVCB SHOULD help the client to
execute the procedure in Section 3 with minimum overall
latency by incorporating additional useful information into the
Additional section of the response as follows:¶

Incorporate the results of SVCB resolution.  If the recursive resolver's
local chain length limit (which may be different from the client's limit) has
been reached, terminate.¶

          
            If any of the resolved SVCB records are in AliasMode, choose one of them
at random, and resolve SVCB, A, and AAAA records for its
TargetName.¶

If any SVCB records are resolved, go to Step 1.¶

              Otherwise, incorporate the results of A and AAAA resolution, and
terminate.¶

            

          All the resolved SVCB records are in ServiceMode.  Resolve A and AAAA
queries for each TargetName (or for the owner name if TargetName
is "."), incorporate all the results, and terminate.¶

        
In this procedure, "resolve" means the resolver's ordinary recursive
resolution procedure, as if processing a query for that RRset.
This includes following any aliases that the resolver would ordinarily
follow (e.g., CNAME, DNAME [DNAME]).  Errors or anomalies in
obtaining additional records MAY cause this process to terminate but
MUST NOT themselves cause the resolver to send a failure response.¶
See Section 2.4.2 for additional safeguards for recursive resolvers
to implement to mitigate loops.¶
See Section 5.2 for possible optimizations of this procedure.¶

          
4.2.1. DNS64
          
DNS64 resolvers synthesize responses to AAAA queries for names that only
have an A record (Section 5.1.7 of [RFC6147]).  SVCB-aware DNS64
resolvers SHOULD apply the same synthesis logic when resolving AAAA
records for the TargetName for inclusion in the Additional section (Step 2 in
Section 4.2) and MAY omit the A records from this section.¶
DNS64 resolvers MUST NOT extrapolate the AAAA synthesis logic to the IP
hints in the SvcParams (Section 7.3).  Modifying the IP hints
would break DNSSEC validation for the SVCB record and would not improve
performance when the above recommendation is implemented.¶



        
4.3. General Requirements
        
Recursive resolvers MUST be able to convey SVCB records with unrecognized
SvcParamKeys.  Resolvers MAY accomplish this by treating
the entire SvcParams portion of the record as opaque, even if the contents
are invalid.  If a recognized SvcParamKey is followed by a value that is
invalid according to the SvcParam's specification, a recursive resolver
MAY report an error such as SERVFAIL instead of returning
the record.
For complex value types whose interpretation might differ
between implementations or have additional future
allowed values added (e.g., URIs or "alpn"), resolvers
SHOULD limit validation to specified constraints.¶
When responding to a query that includes the DNSSEC OK bit [RFC3225],
DNSSEC-capable recursive and authoritative DNS servers MUST accompany
each RRset in the Additional section with the same DNSSEC-related records
that they would send when providing that RRset as an Answer (e.g., RRSIG, NSEC,
NSEC3).¶
According to Section 5.4.1 of [RFC2181], "Unauthenticated RRs received
and cached from ... the additional data section ... should not be cached in
such a way that they would ever be returned as answers to a received query.
They may be returned as additional information where appropriate."
Recursive resolvers therefore MAY cache records from the Additional section
for use in populating Additional section responses and MAY cache them
for general use if they are authenticated by DNSSEC.¶


        
4.4. EDNS Client Subnet (ECS)
        
The EDNS Client Subnet (ECS) option [RFC7871] allows recursive
resolvers to request IP addresses that are suitable for a particular client
IP range.  SVCB records may contain IP addresses (in ipv*hint SvcParams)
or direct users to a subnet-specific TargetName, so recursive resolvers
SHOULD include the same ECS option in SVCB queries as in A/AAAA queries.¶
According to Section 7.3.1 of [RFC7871], "Any records from [the
Additional section] MUST NOT be tied to a network."  Accordingly,
when processing a response whose QTYPE is SVCB-compatible,
resolvers SHOULD treat any records in the Additional section as having
SOURCE PREFIX-LENGTH set to zero and SCOPE PREFIX-LENGTH as specified
in the ECS option.  Authoritative servers MUST omit such records if they are
not suitable for use by any stub resolvers that set SOURCE PREFIX-LENGTH to
zero.  This will cause the resolver to perform a follow-up query that can
receive a properly tailored ECS.  (This is similar to the usage of CNAME with
the ECS option as discussed in [RFC7871], Section 7.2.1.)¶
Authoritative servers that omit Additional records can avoid the added
latency of a follow-up query by following the advice in Section 10.2.¶



      
5. Performance Optimizations
      
For optimal performance (i.e., minimum connection setup time), clients
SHOULD implement a client-side DNS cache.
Responses in the Additional section of a SVCB response SHOULD be placed
in cache before performing any follow-up queries.
With this behavior, and with conforming DNS servers,
using SVCB does not add network latency to connection setup.¶
To improve performance when using a non-conforming recursive resolver, clients
SHOULD issue speculative A and/or AAAA queries in parallel with each SVCB
query, based on a predicted value of TargetName (see Section 10.2).¶
After a ServiceMode RRset is received, clients MAY try more than one option
in parallel and MAY prefetch A and AAAA records for multiple TargetNames.¶

        
5.1. Optimistic Pre-connection and Connection Reuse
        
If an address response arrives before the corresponding SVCB response, the
client MAY initiate a connection as if the SVCB query returned NODATA but
MUST NOT transmit any information that could be altered by the SVCB response
until it arrives.  For example, future SvcParamKeys could be defined that
alter the TLS ClientHello.¶
Clients
implementing this optimization SHOULD wait for 50 milliseconds before
starting optimistic pre-connection, as per the guidance in
[HappyEyeballsV2].¶
A SVCB record is consistent with a connection
if the client would attempt an equivalent connection when making use of
that record. If a SVCB record is consistent with an active or in-progress
connection C, the client MAY prefer that record and use C as its connection.
For example, suppose the client receives this SVCB RRset for a protocol
that uses TLS over TCP:¶

_1234._bar.example.com. 300 IN SVCB 1 svc1.example.net. (
    ipv6hint=2001:db8::1 port=1234 )
                               SVCB 2 svc2.example.net. (
    ipv6hint=2001:db8::2 port=1234 )
¶

If the client has an in-progress TCP connection to [2001:db8::2]:1234,
it MAY proceed with TLS on that connection, even
though the other record in the RRset has higher priority.¶
If none of the SVCB records are consistent
with any active or in-progress connection,
clients proceed with connection establishment as described in
Section 3.¶


        
5.2. Generating and Using Incomplete Responses
        
When following the procedure in Section 4.2, recursive
resolvers MAY terminate the procedure early and produce a reply that omits
some of the associated RRsets.  This is REQUIRED when the chain length limit
is reached (Step 1 in Section 4.2) but might also be appropriate
when the maximum response size is reached or when responding before fully
chasing dependencies would improve performance.  When omitting certain
RRsets, recursive resolvers SHOULD prioritize information for
smaller-SvcPriority records.¶
As discussed in Section 3, clients MUST be able to fetch additional
information that is required to use a SVCB record, if it is not included
in the initial response.  As a performance optimization, if some of the SVCB
records in the response can be used without requiring additional DNS queries,
the client MAY prefer those records, regardless of their priorities.¶



      
6. SVCB-Compatible RR Types
      
An RR type is called "SVCB-compatible" if it permits an implementation that is
identical to SVCB in its:¶

RDATA presentation format¶

        RDATA wire format¶

        IANA registry used for SvcParamKeys¶

        Authoritative server Additional section processing¶

        Recursive resolution process¶

        Relevant Class (i.e., Internet ("IN") [RFC1035])¶

      
This allows authoritative and recursive DNS servers to apply identical
processing to all SVCB-compatible RR types.¶
All other behaviors described as applying to the SVCB RR also apply
to all SVCB-compatible RR types unless explicitly stated otherwise.
When following an AliasMode record (Section 2.4.2) of RR type $T, the
follow-up query to the TargetName MUST also be for type $T.¶
This document defines one SVCB-compatible RR type (other than SVCB itself):
the HTTPS RR type (Section 9), which avoids Attrleaf label prefixes [Attrleaf] in order to improve
compatibility with wildcards and CNAMEs, which are widely used with HTTP.¶
Standards authors should consider carefully whether to use SVCB or define a
new SVCB-compatible RR type, as this choice cannot easily be reversed after
deployment.¶


      
7. Initial SvcParamKeys
      
A few initial SvcParamKeys are defined here.  These keys are useful for the
"https" scheme, and most are expected to be generally applicable to other
schemes as well.¶
Each new protocol
mapping document MUST specify which keys are applicable and safe to use.
Protocol mappings MAY alter the interpretation of SvcParamKeys but MUST NOT
alter their presentation or wire formats.¶

        
7.1. "alpn" and "no-default-alpn"
        
The "alpn" and "no-default-alpn" SvcParamKeys together
indicate the set of Application-Layer Protocol Negotiation (ALPN)
protocol identifiers [ALPN]
and associated transport protocols supported by this service endpoint (the
"SVCB ALPN set").¶
As with Alt-Svc [AltSvc], each ALPN protocol identifier is used to
identify the application protocol and associated suite
of protocols supported by the endpoint (the "protocol suite").
The presence of an ALPN protocol identifier in the SVCB ALPN set indicates that this
service endpoint, described by TargetName and the other parameters (e.g.,
"port"), offers service with the protocol suite associated with this ALPN identifier.¶
Clients filter the set of ALPN identifiers to match the protocol suites they
support, and this informs the underlying transport protocol used (such
as QUIC over UDP or TLS over TCP).  ALPN protocol identifiers that do not uniquely
identify a protocol suite (e.g., an Identification Sequence that
can be used with both TLS and DTLS) are not compatible with this
SvcParamKey and MUST NOT be included in the SVCB ALPN set.¶

          
7.1.1. Representation
          
ALPNs are identified by their registered "Identification Sequence"
(alpn-id), which is a sequence of 1-255 octets.¶

For "alpn", the presentation value SHALL be
a comma-separated list (Appendix A.1)
of one or more alpn-ids.  Zone-file implementations MAY disallow the
"," and "\" characters in ALPN IDs instead of implementing the value-list escaping
procedure, relying on the opaque key format (e.g., key1=\002h2) in the
event that these characters are needed.¶
The wire-format value for "alpn" consists of at least one
alpn-id prefixed by its length as a single octet, and these length-value
pairs are concatenated to form the SvcParamValue.  These pairs MUST exactly
fill the SvcParamValue; otherwise, the SvcParamValue is malformed.¶
For "no-default-alpn", the presentation and wire-format values MUST be
empty.  When "no-default-alpn" is specified in an RR,
"alpn" must also be specified in order for the RR
to be "self-consistent" (Section 2.4.3).¶
Each scheme that uses this SvcParamKey defines a "default set" of ALPN IDs
that are supported by nearly all clients and servers; this set MAY
be empty.  To determine the SVCB ALPN set, the client starts with the list of
alpn-ids from the "alpn" SvcParamKey, and it adds the default set unless the
"no-default-alpn" SvcParamKey is present.¶


          
7.1.2. Use
          
To establish a connection to the endpoint, clients MUST¶

Let SVCB-ALPN-Intersection be the set of protocols in the SVCB ALPN set
that the client supports.¶

            Let Intersection-Transports be the set of transports (e.g., TLS, DTLS, QUIC)
implied by the protocols in SVCB-ALPN-Intersection.¶

            For each transport in Intersection-Transports, construct a ProtocolNameList
containing the Identification Sequences of all the client's supported ALPN
protocols for that transport, without regard to the SVCB ALPN set.¶

          
For example, if the SVCB ALPN set is ["http/1.1", "h3"] and the client
supports HTTP/1.1, HTTP/2, and HTTP/3, the client could attempt to connect using
TLS over TCP with a ProtocolNameList of ["http/1.1", "h2"] and could also
attempt a connection using QUIC with a ProtocolNameList of ["h3"].¶
Once the client has constructed a ClientHello, protocol negotiation in that
handshake proceeds as specified in [ALPN], without regard to the SVCB ALPN
set.¶
Clients MAY implement a fallback procedure, using a less-preferred transport
if more-preferred transports fail to connect.  This fallback behavior is
vulnerable to manipulation by a network attacker who blocks the more-preferred
transports, but it may be necessary for compatibility with existing networks.¶
With this procedure in place, an attacker who can modify DNS and network
traffic can prevent a successful transport connection but cannot otherwise
interfere with ALPN protocol selection.  This procedure also ensures that
each ProtocolNameList includes at least one protocol from the SVCB ALPN set.¶
Clients SHOULD NOT attempt connection to a service endpoint whose SVCB
ALPN set does not contain any supported protocols.¶
To ensure
consistency of behavior, clients MAY reject the entire SVCB RRset and fall
back to basic connection establishment if all of the compatible RRs indicate
"no-default-alpn", even if connection could have succeeded using a
non-default ALPN protocol.¶
Zone operators SHOULD ensure that at least one RR in each RRset supports the
default transports.  This enables compatibility with the greatest number of
clients.¶



        
7.2. "port"
        
The "port" SvcParamKey defines the TCP or UDP port
that should be used to reach this alternative endpoint.
If this key is not present, clients SHALL use the authority endpoint's port
number.¶
The presentation value of the SvcParamValue is a single decimal integer
between 0 and 65535 in ASCII.  Any other value (e.g., an empty value)
is a syntax error.  To enable simpler parsing, this SvcParamValue MUST NOT contain
escape sequences.¶
The wire format of the SvcParamValue
is the corresponding 2-octet numeric value in network byte order.¶
If a port-restricting firewall is in place between some client and the service
endpoint, changing the port number might cause that client to lose access to
the service, so operators should exercise caution when using this SvcParamKey
to specify a non-default port.¶


        
7.3. "ipv4hint" and "ipv6hint"
        
The "ipv4hint" and "ipv6hint" keys convey IP addresses that clients MAY use to
reach the service.  If A and AAAA records for TargetName are locally
available, the client SHOULD ignore these hints.  Otherwise, clients
SHOULD perform A and/or AAAA queries for TargetName per
Section 3, and clients SHOULD use the IP address in those
responses for future connections. Clients MAY opt to terminate any
connections using the addresses in hints and instead switch to the
addresses in response to the TargetName query. Failure to use A and/or
AAAA response addresses could negatively impact load balancing or other
geo-aware features and thereby degrade client performance.¶
The presentation value SHALL be a comma-separated list (Appendix A.1)
of one or more IP addresses of the appropriate
family in standard textual format [RFC5952] [RFC4001].  To enable simpler parsing,
this SvcParamValue MUST NOT contain escape sequences.¶
The wire format for each parameter is a sequence of IP addresses in network
byte order (for the respective address family).
Like an A or AAAA RRset, the list of addresses represents an
unordered collection, and clients SHOULD pick addresses to use in a random order.
An empty list of addresses is invalid.¶
When selecting between IPv4 and IPv6 addresses to use, clients may use an
approach such as Happy Eyeballs [HappyEyeballsV2].
When only "ipv4hint" is present, NAT64 clients may synthesize
IPv6 addresses as specified in [RFC7050] or ignore the "ipv4hint" key and
wait for AAAA resolution (Section 3).
For best performance, server operators SHOULD include an "ipv6hint" parameter
whenever they include an "ipv4hint" parameter.¶
These parameters are intended to minimize additional connection latency
when a recursive resolver is not compliant with the requirements in
Section 4 and SHOULD NOT be included if most clients are using
compliant recursive resolvers.  When TargetName is the service name
or the owner name (which can be written as "."), server operators
SHOULD NOT include these hints, because they are unlikely to convey any
performance benefit.¶




      
8. ServiceMode RR Compatibility and Mandatory Keys
      
In a ServiceMode RR, a SvcParamKey is considered "mandatory" if the RR will not
function correctly for clients that ignore this SvcParamKey.  Each SVCB
protocol mapping SHOULD specify a set of keys that are "automatically
mandatory", i.e., mandatory if they are present in an RR.  The SvcParamKey
"mandatory" is used to indicate any mandatory keys for this RR, in addition to
any automatically mandatory keys that are present.¶
A ServiceMode RR is considered "compatible" by a client if the client
recognizes all the mandatory keys and their values indicate that successful
connection establishment is possible.  Incompatible RRs are ignored (see step 5 of the procedure defined in Section 3).¶
The presentation value SHALL be a comma-separated list
(Appendix A.1) of one or more valid
SvcParamKeys, either by their registered name or in the unknown-key format
(Section 2.1).  Keys MAY appear in any order but MUST NOT appear more
than once.  For self-consistency (Section 2.4.3), listed keys MUST also
appear in the SvcParams.¶
To enable simpler parsing, this
SvcParamValue MUST NOT contain escape sequences.¶
For example, the following is a valid list of SvcParams:¶

ipv6hint=... key65333=ex1 key65444=ex2 mandatory=key65444,ipv6hint
¶

In wire format, the keys are represented by their numeric values in
network byte order, concatenated in strictly increasing numeric order.¶
This SvcParamKey is always automatically mandatory and MUST NOT appear in its
own value-list.  Other automatically mandatory keys SHOULD NOT appear in the
list either.  (Including them wastes space and otherwise has no effect.)¶


      
9. Using Service Bindings with HTTP
      
The use of any protocol with SVCB requires a protocol-specific mapping
specification.  This section specifies the mapping for the "http" and "https"
URI schemes [HTTP].¶
To enable special handling for HTTP use cases,
the HTTPS RR type is defined as a SVCB-compatible RR type,
specific to the "https" and "http" schemes.  Clients MUST NOT
perform SVCB queries or accept SVCB responses for "https"
or "http" schemes.¶
The presentation format of the record is:¶

Name TTL IN HTTPS SvcPriority TargetName SvcParams
¶

All the SvcParamKeys defined in Section 7 are permitted for use in
HTTPS RRs.  The default set of ALPN IDs is the single value "http/1.1".
The "automatically mandatory" keys (Section 8) are "port"
and "no-default-alpn".  (As described in Section 8, clients must
either implement these keys or ignore any RR in which they appear.)
Clients that restrict the destination port in "https" URIs
(e.g., using the "bad ports" list from [FETCH]) SHOULD apply the
same restriction to the "port" SvcParam.¶
The presence of an HTTPS RR for an origin also indicates
that clients should connect securely and use the "https" scheme, as
discussed in Section 9.5.  This allows HTTPS RRs to apply to
pre-existing "http" scheme URLs, while ensuring that the client uses a
secure and authenticated connection.¶
The HTTPS RR parallels the concepts
introduced in "HTTP Alternative Services" [AltSvc].  Clients and servers that implement HTTPS RRs are
not required to implement Alt-Svc.¶

        
9.1. Query Names for HTTPS RRs
        
The HTTPS RR uses Port Prefix Naming (Section 2.3),
with one modification: if the scheme is "https" and the port is 443,
then the client's original QNAME is
equal to the service name (i.e., the origin's hostname),
without any prefix labels.¶
By removing the Attrleaf labels [Attrleaf]
used in SVCB, this construction enables offline DNSSEC signing of
wildcard domains, which are commonly used with HTTP.  Using the
service name as the owner name of the HTTPS record, without prefixes,
also allows the targets of existing CNAME chains
(e.g., CDN hosts) to start returning HTTPS RR responses without
requiring origin domains to configure and maintain an additional
delegation.¶
The procedure for following HTTPS AliasMode RRs and CNAME aliases is unchanged from SVCB (as described in Sections 2.4.2 and 3).¶
Clients always convert "http" URLs to "https" before performing an
HTTPS RR query using the process described in Section 9.5, so domain owners
MUST NOT publish HTTPS RRs with a prefix of "_http".¶
Note that none of these forms alter the HTTPS origin or authority.
For example, clients MUST continue to validate TLS certificate
hostnames based on the origin.¶


        
9.2. Comparison with Alt-Svc
        
Publishing a ServiceMode HTTPS RR in DNS is intended
to be similar to transmitting an Alt-Svc field value over
HTTP, and receiving an HTTPS RR is intended to be similar to
receiving that field value over HTTP.  However, there are some
differences in the intended client and server behavior.¶

          
9.2.1. ALPN Usage
          
Unlike Alt-Svc field values, HTTPS RRs can contain multiple ALPN IDs.  The
meaning and use of these IDs are discussed in Section 7.1.2.¶


          
9.2.2. Untrusted Channels
          
HTTPS records do not require or provide any assurance of authenticity.  (DNSSEC
signing and verification, which would provide such assurance, are OPTIONAL.)
The DNS resolution process is modeled as an untrusted channel that might be
controlled by an attacker, so
Alt-Svc parameters that cannot be safely received in this model MUST NOT
have a corresponding defined SvcParamKey.  For example, there is no
SvcParamKey corresponding to the Alt-Svc "persist" parameter, because
this parameter is not safe to accept over an untrusted channel.¶


          
9.2.3. Cache Lifetime
          
There is no SvcParamKey corresponding to the Alt-Svc "ma" (max age) parameter.
Instead, server operators encode the expiration time in the DNS TTL.¶
The appropriate TTL value might be different from the "ma" value
used for Alt-Svc, depending on the desired efficiency and
agility.  Some DNS caches incorrectly extend the lifetime of DNS
records beyond the stated TTL, so server operators cannot rely on
HTTPS RRs expiring on time.  Shortening the TTL to compensate
for incorrect caching is NOT RECOMMENDED, as this practice impairs the
performance of correctly functioning caches and does not guarantee
faster expiration from incorrect caches.  Instead, server operators
SHOULD maintain compatibility with expired records until they observe
that nearly all connections have migrated to the new configuration.¶


          
9.2.4. Granularity
          
Sending Alt-Svc over HTTP allows the server to tailor the Alt-Svc
field value specifically to the client.  When using an HTTPS RR,
groups of clients will necessarily receive the same SvcParams.
Therefore, HTTPS RRs are not suitable for uses that require
single-client granularity.¶



        
9.3. Interaction with Alt-Svc
        
Clients that implement support for both Alt-Svc and HTTPS records and
are making a connection based on a cached Alt-Svc response SHOULD
retrieve any HTTPS records for the Alt-Svc alt-authority and ensure that
their connection attempts are consistent with both the Alt-Svc parameters
and any received HTTPS SvcParams.  If present, the HTTPS record's TargetName
and port are used for connection establishment (per Section 3).
For example, suppose that
"https://example.com" sends an Alt-Svc field value of:¶

Alt-Svc: h2="alt.example:443", h2="alt2.example:443", h3=":8443"
¶

The client would retrieve the following HTTPS records:¶

alt.example.              IN HTTPS 1 . alpn=h2,h3 foo=...
alt2.example.             IN HTTPS 1 alt2b.example. alpn=h3 foo=...
_8443._https.example.com. IN HTTPS 1 alt3.example. (
    port=9443 alpn=h2,h3 foo=... )
¶

Based on these inputs, the following connection attempts would always be
allowed:¶

HTTP/2 to alt.example:443¶

          HTTP/3 to alt3.example:9443¶

          Fallback to the client's non-Alt-Svc connection behavior¶

        
The following connection attempts would not be allowed:¶

HTTP/3 to alt.example:443 (not consistent with Alt-Svc)¶

          Any connection to alt2b.example (no ALPN ID consistent with both the HTTPS
record and Alt-Svc)¶

          HTTPS over TCP to any port on alt3.example (not consistent with Alt-Svc)¶

        
Suppose that "foo" is a SvcParamKey that renders the client SVCB-reliant.
The following Alt-Svc-only connection attempts would be allowed only if
the client does not support "foo", as they rely on SVCB-optional fallback
behavior:¶

HTTP/2 to alt2.example:443¶

          HTTP/3 to example.com:8443¶

        
Alt-authorities SHOULD carry the same SvcParams as the origin unless
a deviation is specifically known to be safe.
As noted in Section 2.4 of [AltSvc], clients MAY disallow any Alt-Svc
connection according to their own criteria, e.g., disallowing Alt-Svc
connections that lack support for privacy features that are available on
the authority endpoint.¶


        
9.4. Requiring Server Name Indication
        
Clients MUST NOT use an HTTPS RR response unless the
client supports the TLS Server Name Indication (SNI) extension and
indicates the origin name in the TLS ClientHello (which might be
encrypted via a future specification such as [ECH]).
This supports the conservation of IP addresses.¶
Note that the TLS SNI (and also the HTTP "Host" or ":authority") will indicate
the origin, not the TargetName.¶


        
9.5. HTTP Strict Transport Security (HSTS)
        
An HTTPS RR directs the client to communicate with this host only over a
secure transport, similar to HSTS [HSTS].
Prior to making an "http" scheme request, the client SHOULD perform a lookup
to determine if any HTTPS RRs exist for that origin.  To do so,
the client SHOULD construct a corresponding "https" URL as follows:¶

Replace the "http" scheme with "https".¶

          If the "http" URL explicitly specifies port 80, specify port 443.¶

          Do not alter any other aspect of the URL.¶

        
This construction is equivalent to Section 8.3 of [HSTS], Step 5.¶
If an HTTPS RR query for this "https" URL returns any AliasMode HTTPS RRs
or any compatible ServiceMode HTTPS RRs (see Section 8), the client
SHOULD behave as if it has received an HTTP 307 (Temporary Redirect) status code
with this "https" URL in the "Location" field.  (Receipt of an incompatible ServiceMode RR does not
trigger the redirect behavior.)
Because HTTPS RRs are received over an often-insecure channel (DNS),
clients MUST NOT place any more trust in this signal than if they
had received a 307 (Temporary Redirect) response over cleartext HTTP.¶
Publishing an HTTPS RR can potentially lead to unexpected results
or a loss in functionality in cases where the "http" resource neither
redirects to the "https" resource nor references the same underlying resource.¶
When an "https" connection fails due to an error in the underlying secure
transport, such as an error in certificate validation, some clients
currently offer a "user recourse" that allows the user to bypass the
security error and connect anyway.
When making an "https" scheme request to an origin with an HTTPS RR,
either directly or via the above redirect, such a client MAY remove the user
recourse option.  Origins that publish HTTPS RRs therefore MUST NOT rely
on user recourse for access.  For more information, see Sections 8.4 and 12.1 of [HSTS].¶


        
9.6. Use of HTTPS RRs in Other Protocols
        
All HTTP connections to named origins are eligible to use HTTPS RRs, even
when HTTP is used as part of another protocol or without an explicit HTTP-related URI
scheme (Section 4.2 of [HTTP]).  For example, clients that
support HTTPS RRs and implement [WebSocket] using the altered
opening handshake from [FETCH-WEBSOCKETS] SHOULD use HTTPS RRs
for the requestURL.¶
When HTTP is used in a context where URLs or redirects are not applicable
(e.g., connections to an HTTP proxy), clients that find a corresponding HTTPS RR
SHOULD implement security upgrade behavior equivalent to that
specified in
Section 9.5.¶
Such protocols MAY define their own SVCB mappings, which MAY
be defined to take precedence over HTTPS RRs.¶



      
10. Zone Structures
      

        
10.1. Structuring Zones for Flexibility
        
Each ServiceMode RRset can only serve a single scheme.  The scheme is indicated
by the owner name and the RR type.  For the generic SVCB RR type, this means that
each owner name can only be used for a single scheme.  The underscore prefixing
requirement (Section 2.3) ensures that this is true for the initial query,
but it is the responsibility of zone owners to choose names that satisfy this
constraint when using aliases, including CNAME and AliasMode records.¶
When using the generic SVCB RR type with aliasing, zone owners SHOULD choose alias
target names that indicate the scheme in use (e.g., "foosvc.example.net" for
"foo" schemes).  This will help to avoid confusion when another scheme needs to
be added to the configuration.  When multiple port numbers are in use, it may be
helpful to repeat the prefix labels in the alias target name (e.g.,
"_1234._foo.svc.example.net").¶


        
10.2. Structuring Zones for Performance
        
To avoid a delay for clients using a non-conforming recursive resolver,
domain owners SHOULD minimize the use of AliasMode records and SHOULD
choose TargetName according to a predictable convention that is known
to the client, so that clients can issue A and/or AAAA queries for TargetName
in advance (see Section 5).  Unless otherwise specified, the
convention is to set TargetName to the service name for an initial
ServiceMode record, or to "." if it is reached via an alias.¶

          
$ORIGIN example.com. ; Origin
foo                  3600 IN CNAME foosvc.example.net.
_8080._foo.foo       3600 IN CNAME foosvc.example.net.
bar                   300 IN AAAA 2001:db8::2
_9090._bar.bar       3600 IN SVCB 1 bar key65444=...

$ORIGIN example.net. ; Service provider zone
foosvc               3600 IN SVCB 1 . key65333=...
foosvc                300 IN AAAA 2001:db8::1


Figure 1:
"foo://foo.example.com:8080" Is Available at "foosvc.example.net", but "bar://bar.example.com:9090" Is Served Locally
          
Domain owners SHOULD avoid using a TargetName that is below a DNAME, as
this is likely unnecessary and makes responses slower and larger.
Also, zone structures that require following more than eight aliases
(counting both AliasMode and CNAME records) are NOT RECOMMENDED.¶


        
10.3. Operational Considerations
        
Some authoritative DNS servers may not allow A or AAAA records on names
starting with an underscore (e.g., [BIND-CHECK-NAMES]).
This could create an operational issue when the TargetName contains an Attrleaf label,
or when using a TargetName of "." if the owner name contains an Attrleaf label.¶


        
10.4. Examples
        

          
10.4.1. Protocol Enhancements
          
Consider a simple zone of the form:¶

$ORIGIN simple.example. ; Simple example zone
@ 300 IN A    192.0.2.1
         AAAA 2001:db8::1
¶

The domain owner could add this record:¶

@ 7200 IN HTTPS 1 . alpn=h3
¶

This record would indicate that "https://simple.example" supports QUIC
in addition to HTTP/1.1 over TLS over TCP (the implicit default).
The record could also include other information (e.g., a non-standard port).
For "https://simple.example:8443", the record would be:¶

_8443._https 7200 IN HTTPS 1 . alpn=h3
¶

These records also respectively tell clients to replace the scheme with "https" when
loading "http://simple.example" or "http://simple.example:8443".¶


          
10.4.2. Apex Aliasing
          
Consider a zone that is using CNAME aliasing:¶

$ORIGIN aliased.example. ; A zone that is using a hosting service
; Subdomain aliased to a high-performance server pool
www             7200 IN CNAME pool.svc.example.
; Apex domain on fixed IPs because CNAME is not allowed at the apex
@                300 IN A     192.0.2.1
                     IN AAAA  2001:db8::1
¶

With HTTPS RRs, the owner of aliased.example could alias the apex by
adding one additional record:¶

@               7200 IN HTTPS 0 pool.svc.example.
¶

With this record in place, HTTPS-RR-aware clients will use the same
server pool for aliased.example and www.aliased.example.  (They will
also upgrade "http://aliased.example/..." to "https".)  Non-HTTPS-RR-aware
clients will just ignore the new record.¶
Similar to CNAME, HTTPS RRs have no impact on the origin name.
When connecting, clients will continue to treat the authoritative
origins as "https://www.aliased.example" and "https://aliased.example",
respectively, and will validate TLS server certificates accordingly.¶


          
10.4.3. Parameter Binding
          
Suppose that svc.example's primary server pool supports HTTP/3 but its
backup server pool does not.  This can be expressed in the following form:¶

$ORIGIN svc.example. ; A hosting provider
pool  7200 IN HTTPS 1 . alpn=h2,h3
              HTTPS 2 backup alpn=h2 port=8443
pool   300 IN A        192.0.2.2
              AAAA     2001:db8::2
backup 300 IN A        192.0.2.3
              AAAA     2001:db8::3
¶

This configuration is entirely compatible with the "apex aliasing" example,
whether the client supports HTTPS RRs or not.  If the client does support
HTTPS RRs, all connections will be upgraded to HTTPS, and clients will
use HTTP/3 if they can.  Parameters are "bound" to each server pool, so
each server pool can have its own protocol, port number, etc.¶


          
10.4.4. Multi-CDN Configuration
          
The HTTPS RR is intended to support HTTPS services operated by
multiple independent entities, such as different CDNs or different hosting providers.  This includes
the case where a service is migrated from one operator to another,
as well as the case where the service is multiplexed between
multiple operators for performance, redundancy, etc.¶
This example shows such a configuration, with www.customer.example
having different DNS responses to different queries, either over time
or due to logic within the authoritative DNS server:¶

 ; This zone contains/returns different CNAME records
 ; at different points in time.  The RRset for "www" can
 ; only ever contain a single CNAME.

 ; Sometimes the zone has:
 $ORIGIN customer.example.  ; A multi-CDN customer domain
 www 900 IN CNAME cdn1.svc1.example.

 ; and other times it contains:
 $ORIGIN customer.example.
 www 900 IN CNAME customer.svc2.example.

 ; and yet other times it contains:
 $ORIGIN customer.example.
 www 900 IN CNAME cdn3.svc3.example.

 ; With the following remaining constant and always included:
 $ORIGIN customer.example.  ; A multi-CDN customer domain
 ; The apex is also aliased to www to match its configuration.
 @     7200 IN HTTPS 0 www
 ; Non-HTTPS-aware clients use non-CDN IPs.
               A    203.0.113.82
               AAAA 2001:db8:203::2

 ; Resolutions following the cdn1.svc1.example
 ; path use these records.
 ; This CDN uses a different alternative service for HTTP/3.
 $ORIGIN svc1.example.  ; domain for CDN 1
 cdn1     1800 IN HTTPS 1 h3pool alpn=h3
                  HTTPS 2 . alpn=h2
                  A    192.0.2.2
                  AAAA 2001:db8:192::4
 h3pool 300 IN A 192.0.2.3
            AAAA 2001:db8:192:7::3

 ; Resolutions following the customer.svc2.example
 ; path use these records.
 ; Note that this CDN only supports HTTP/2.
 $ORIGIN svc2.example. ; domain operated by CDN 2
 customer 300 IN HTTPS 1 . alpn=h2
           60 IN A    198.51.100.2
                 A    198.51.100.3
                 A    198.51.100.4
                 AAAA 2001:db8:198::7
                 AAAA 2001:db8:198::12

 ; Resolutions following the cdn3.svc3.example
 ; path use these records.
 ; Note that this CDN has no HTTPS records.
 $ORIGIN svc3.example. ; domain operated by CDN 3
 cdn3      60 IN A    203.0.113.8
                 AAAA 2001:db8:113::8
¶

Note that in the above example, the different CDNs have different
configurations and different capabilities, but clients will use HTTPS RRs
as a bound-together unit.¶
Domain owners should be cautious when using a multi-CDN configuration, as it
introduces a number of complexities highlighted by this example:¶

If CDN 1 supports a desired protocol or feature and CDN 2 does not, the
client is vulnerable to
downgrade by a network adversary who forces clients to get CDN 2 records.¶

            Aliasing the apex to its subdomain simplifies the zone file but likely
increases resolution latency, especially when using a non-HTTPS-aware
recursive resolver.  An alternative would be to alias the zone
apex directly to a name managed by a CDN.¶

            The A, AAAA, and HTTPS resolutions are independent lookups, so resolvers may
observe and follow different CNAMEs to different CDNs.
Clients may thus find that the A and AAAA responses do not correspond to the
TargetName in the HTTPS response; these clients will need to perform additional
queries to retrieve the correct IP addresses.
Including ipv6hint and ipv4hint will reduce the performance
impact of this case.¶

            If not all CDNs publish HTTPS records, clients will sometimes
receive NODATA for HTTPS queries (as with cdn3.svc3.example above)
but could receive A/AAAA records from a different CDN.  Clients will
attempt to connect to this CDN without the benefit of its HTTPS
records.¶

          


          
10.4.5. Non-HTTP Uses
          
For protocols other than HTTP, the SVCB RR and an Attrleaf label [Attrleaf]
will be used.  For example, to reach an example resource of
"baz://api.example.com:8765", the following SVCB
record would be used to alias it to "svc4-baz.example.net.",
which in turn could return AAAA/A records and/or SVCB
records in ServiceMode:¶

_8765._baz.api.example.com. 7200 IN SVCB 0 svc4-baz.example.net.
¶

HTTPS RRs use similar Attrleaf labels if the origin contains
a non-default port.¶




      
11. Interaction with Other Standards
      
This standard is intended to reduce connection latency and
improve user privacy.  Server operators implementing this standard
SHOULD also implement TLS 1.3 [RFC8446] and
Online Certificate Status Protocol (OCSP) Stapling (i.e., Certificate Status
Request in Section 8 of [RFC6066]),
both of which confer substantial performance and privacy
benefits when used in combination with SVCB records.¶
To realize the greatest privacy benefits, this proposal is intended for
use over a privacy-preserving DNS transport (like DNS over TLS
[DoT] or DNS over HTTPS [DoH]).
However, performance improvements, and some modest privacy improvements,
are possible without the use of those standards.¶
Any specification for the use of SVCB with a protocol MUST have an entry for its
scheme under the SVCB RR type in the IANA DNS "Underscored and Globally Scoped DNS Node Names" registry [Attrleaf].  The scheme MUST have an entry in the "Uniform Resource Identifier (URI) Schemes" registry [RFC7595] and MUST have a defined specification for use
with SVCB.¶


      
12. Security Considerations
      
SVCB/HTTPS RRs permit distribution over untrusted
channels, and clients are REQUIRED to verify that the alternative endpoint
is authoritative for the service (similar to Section 2.1 of [AltSvc]).
Therefore, DNSSEC signing and validation are OPTIONAL for publishing
and using SVCB and HTTPS RRs.¶
Clients MUST ensure that their DNS cache is partitioned for each local
network, or flushed on network changes, to prevent a local adversary in one
network from implanting a forged DNS record that allows them to
track users or hinder their connections after they leave that network.¶
An attacker who can prevent SVCB resolution can deny clients any associated
security benefits.  A hostile recursive resolver can always deny service to
SVCB queries, but network intermediaries can often prevent resolution as well,
even when the client and recursive resolver validate DNSSEC and use a secure
transport.  These downgrade attacks can prevent the "https" upgrade provided by
the HTTPS RR (Section 9.5) and can disable any other protections coordinated via
SvcParams.  To prevent downgrades, Section 3.1
recommends that clients abandon the connection attempt when such an attack is
detected.¶
A hostile DNS intermediary might forge AliasMode "." records (Section 2.5.1) as
a way to block clients from accessing particular services.  Such an adversary
could already block entire domains by forging erroneous responses, but this
mechanism allows them to target particular protocols or ports within a domain.
Clients that might be subject to such attacks SHOULD ignore AliasMode "."
records.¶
A hostile DNS intermediary or authoritative server can return SVCB records indicating any IP
address and port number, including IP addresses inside the local network and
port numbers assigned to internal services.  If the attacker can influence the
client's payload (e.g., TLS session ticket contents) and an internal service
has a sufficiently lax parser, the attacker could gain access to the
internal service.  (The same concerns apply to SRV records, HTTP Alt-Svc,
and HTTP redirects.)  As a mitigation, SVCB mapping documents SHOULD indicate
any port number restrictions that are appropriate for the supported transports.¶


      
13. Privacy Considerations
      
Standard address queries reveal the user's intent to access a particular
domain.  This information is visible to the recursive resolver, and to
many other parties when plaintext DNS transport is used.  SVCB queries,
like queries for SRV records and other specific RR types, additionally
reveal the user's intent to use a particular protocol.  This is not
normally sensitive information, but it should be considered when adding
SVCB support in a new context.¶


      
14. IANA Considerations
      

        
14.1. SVCB RR Type
        
IANA has registered the following new DNS RR type in the "Resource Record (RR) TYPEs"
registry on the "Domain Name System (DNS) Parameters" page:¶

          Type:
          SVCB¶

          
Value:
          64¶

          
Meaning:
          General-purpose service binding¶

          
Reference:
          RFC 9460¶

        



        
14.2. HTTPS RR Type
        
IANA has registered the following new DNS RR type
   in the "Resource Record (RR) TYPEs" registry
   on the "Domain Name System (DNS) Parameters" page:¶

          Type:
          HTTPS¶

          
Value:
          65¶

          
Meaning:
          SVCB-compatible type for use with HTTP¶

          
Reference:
          RFC 9460¶

        



        
14.3. New Registry for Service Parameters
        
IANA has created the "Service Parameter Keys (SvcParamKeys)"
registry in the "Domain Name System (DNS) Parameters" category
on a new page entitled "DNS Service Bindings (SVCB)".  This registry
defines the namespace
for parameters, including string representations and numeric
SvcParamKey values.  This registry is shared with other SVCB-compatible
RR types, such as the HTTPS RR.¶

          
14.3.1. Procedure
          
A registration MUST include the following fields:¶

            Number:
            Wire-format numeric identifier (range 0-65535)¶

            
Name:
            Unique presentation name¶

            
Meaning:
            A short description¶

            
Reference:
            Location of specification or registration source¶

            
Change Controller:
            Person or entity, with contact information if appropriate¶

          

The characters in the registered Name field entry MUST be lowercase alphanumeric or "-"
(Section 2.1).  The name MUST NOT start with "key" or "invalid".¶
The registration policy for new entries is Expert Review ([RFC8126], Section 4.5).  The designated expert MUST ensure that
the reference is stable and publicly available and that it specifies
how to convert the SvcParamValue's presentation format to wire format.  The
reference MAY be any individual's Internet-Draft or a document from
any other source with similar assurances of stability and availability.
An entry MAY specify a reference of
the form "Same as (other key name)" if it uses the same presentation and wire
formats as an existing key.¶
This arrangement supports the development of new parameters while ensuring that
zone files can be made interoperable.¶


          
14.3.2. Initial Contents
          
The "Service Parameter Keys (SvcParamKeys)" registry has been
populated with the following initial registrations:¶

            Table 1

              
                Number
                Name
                Meaning
                Reference
                Change Controller
              
            
            
              
                0
                mandatory
                Mandatory keys in this RR
                RFC 9460, Section 8

                IETF
              
              
                1
                alpn
                Additional supported protocols
                RFC 9460, Section 7.1

                IETF
              
              
                2
                no-default-alpn
                No support for default protocol
                RFC 9460, Section 7.1

                IETF
              
              
                3
                port
                Port for alternative endpoint
                RFC 9460, Section 7.2

                IETF
              
              
                4
                ipv4hint
                IPv4 address hints
                RFC 9460, Section 7.3

                IETF
              
              
                5
                ech
                RESERVED (held for Encrypted ClientHello)
                N/A
                IETF
              
              
                6
                ipv6hint
                IPv6 address hints
                RFC 9460, Section 7.3

                IETF
              
              
                65280-65534
                N/A
                Reserved for Private Use
                RFC 9460
                IETF
              
              
                65535
                N/A
                Reserved ("Invalid key")
                RFC 9460
                IETF
              
            
          



        
14.4. Other Registry Updates
        
Per [Attrleaf], the following entry has been added to the DNS "Underscored and Globally Scoped DNS Node Names" registry:¶

          Table 2

            
              RR Type
              _NODE NAME
              Reference
            
          
          
            
              HTTPS
              _https
              RFC 9460
            
          
        



      
15. References
      

        
15.1. Normative References
        

[ALPN]
        
Friedl, S., Popov, A., Langley, A., and E. Stephan, "Transport Layer Security (TLS) Application-Layer Protocol Negotiation Extension", RFC 7301, DOI 10.17487/RFC7301, July 2014, <https://www.rfc-editor.org/info/rfc7301>. 

[Attrleaf]
        
Crocker, D., "Scoped Interpretation of DNS Resource Records through "Underscored" Naming of Attribute Leaves", BCP 222, RFC 8552, DOI 10.17487/RFC8552, March 2019, <https://www.rfc-editor.org/info/rfc8552>. 

[DoH]
        
Hoffman, P. and P. McManus, "DNS Queries over HTTPS (DoH)", RFC 8484, DOI 10.17487/RFC8484, October 2018, <https://www.rfc-editor.org/info/rfc8484>. 

[DoT]
        
Hu, Z., Zhu, L., Heidemann, J., Mankin, A., Wessels, D., and P. Hoffman, "Specification for DNS over Transport Layer Security (TLS)", RFC 7858, DOI 10.17487/RFC7858, May 2016, <https://www.rfc-editor.org/info/rfc7858>. 

[HappyEyeballsV2]
        
Schinazi, D. and T. Pauly, "Happy Eyeballs Version 2: Better Connectivity Using Concurrency", RFC 8305, DOI 10.17487/RFC8305, December 2017, <https://www.rfc-editor.org/info/rfc8305>. 

[HTTP]
        
Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke, Ed., "HTTP Semantics", STD 97, RFC 9110, DOI 10.17487/RFC9110, June 2022, <https://www.rfc-editor.org/info/rfc9110>. 

[RFC1034]
        
Mockapetris, P., "Domain names - concepts and facilities", STD 13, RFC 1034, DOI 10.17487/RFC1034, November 1987, <https://www.rfc-editor.org/info/rfc1034>. 

[RFC1035]
        
Mockapetris, P., "Domain names - implementation and specification", STD 13, RFC 1035, DOI 10.17487/RFC1035, November 1987, <https://www.rfc-editor.org/info/rfc1035>. 

[RFC1928]
        
Leech, M., Ganis, M., Lee, Y., Kuris, R., Koblas, D., and L. Jones, "SOCKS Protocol Version 5", RFC 1928, DOI 10.17487/RFC1928, March 1996, <https://www.rfc-editor.org/info/rfc1928>. 

[RFC2119]
        
Bradner, S., "Key words for use in RFCs to Indicate Requirement Levels", BCP 14, RFC 2119, DOI 10.17487/RFC2119, March 1997, <https://www.rfc-editor.org/info/rfc2119>. 

[RFC2181]
        
Elz, R. and R. Bush, "Clarifications to the DNS Specification", RFC 2181, DOI 10.17487/RFC2181, July 1997, <https://www.rfc-editor.org/info/rfc2181>. 

[RFC3225]
        
Conrad, D., "Indicating Resolver Support of DNSSEC", RFC 3225, DOI 10.17487/RFC3225, December 2001, <https://www.rfc-editor.org/info/rfc3225>. 

[RFC3597]
        
Gustafsson, A., "Handling of Unknown DNS Resource Record (RR) Types", RFC 3597, DOI 10.17487/RFC3597, September 2003, <https://www.rfc-editor.org/info/rfc3597>. 

[RFC4001]
        
Daniele, M., Haberman, B., Routhier, S., and J. Schoenwaelder, "Textual Conventions for Internet Network Addresses", RFC 4001, DOI 10.17487/RFC4001, February 2005, <https://www.rfc-editor.org/info/rfc4001>. 

[RFC5234]
        
Crocker, D., Ed. and P. Overell, "Augmented BNF for Syntax Specifications: ABNF", STD 68, RFC 5234, DOI 10.17487/RFC5234, January 2008, <https://www.rfc-editor.org/info/rfc5234>. 

[RFC5952]
        
Kawamura, S. and M. Kawashima, "A Recommendation for IPv6 Address Text Representation", RFC 5952, DOI 10.17487/RFC5952, August 2010, <https://www.rfc-editor.org/info/rfc5952>. 

[RFC6066]
        
Eastlake 3rd, D., "Transport Layer Security (TLS) Extensions: Extension Definitions", RFC 6066, DOI 10.17487/RFC6066, January 2011, <https://www.rfc-editor.org/info/rfc6066>. 

[RFC6147]
        
Bagnulo, M., Sullivan, A., Matthews, P., and I. van Beijnum, "DNS64: DNS Extensions for Network Address Translation from IPv6 Clients to IPv4 Servers", RFC 6147, DOI 10.17487/RFC6147, April 2011, <https://www.rfc-editor.org/info/rfc6147>. 

[RFC7050]
        
Savolainen, T., Korhonen, J., and D. Wing, "Discovery of the IPv6 Prefix Used for IPv6 Address Synthesis", RFC 7050, DOI 10.17487/RFC7050, November 2013, <https://www.rfc-editor.org/info/rfc7050>. 

[RFC7231]
        
Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content", RFC 7231, DOI 10.17487/RFC7231, June 2014, <https://www.rfc-editor.org/info/rfc7231>. 

[RFC7595]
        
Thaler, D., Ed., Hansen, T., and T. Hardie, "Guidelines and Registration Procedures for URI Schemes", BCP 35, RFC 7595, DOI 10.17487/RFC7595, June 2015, <https://www.rfc-editor.org/info/rfc7595>. 

[RFC7871]
        
Contavalli, C., van der Gaast, W., Lawrence, D., and W. Kumari, "Client Subnet in DNS Queries", RFC 7871, DOI 10.17487/RFC7871, May 2016, <https://www.rfc-editor.org/info/rfc7871>. 

[RFC8126]
        
Cotton, M., Leiba, B., and T. Narten, "Guidelines for Writing an IANA Considerations Section in RFCs", BCP 26, RFC 8126, DOI 10.17487/RFC8126, June 2017, <https://www.rfc-editor.org/info/rfc8126>. 

[RFC8174]
        
Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174, May 2017, <https://www.rfc-editor.org/info/rfc8174>. 

[RFC8446]
        
Rescorla, E., "The Transport Layer Security (TLS) Protocol Version 1.3", RFC 8446, DOI 10.17487/RFC8446, August 2018, <https://www.rfc-editor.org/info/rfc8446>. 

[WebSocket]
      
Fette, I. and A. Melnikov, "The WebSocket Protocol", RFC 6455, DOI 10.17487/RFC6455, December 2011, <https://www.rfc-editor.org/info/rfc6455>. 




        
15.2. Informative References
        

[AltSvc]
        
Nottingham, M., McManus, P., and J. Reschke, "HTTP Alternative Services", RFC 7838, DOI 10.17487/RFC7838, April 2016, <https://www.rfc-editor.org/info/rfc7838>. 

[ANAME-DNS-RR]
        
Finch, T., Hunt, E., van Dijk, P., Eden, A., and W. Mekking, "Address-specific DNS aliases (ANAME)", Work in Progress, Internet-Draft, draft-ietf-dnsop-aname-04, 8 July 2019, <https://datatracker.ietf.org/doc/html/draft-ietf-dnsop-aname-04>. 

[BIND-CHECK-NAMES]
        
Internet Systems Consortium, "BIND v9.19.11 Configuration Reference: "check-names"", September 2023, <https://bind9.readthedocs.io/en/v9.19.11/reference.html#namedconf-statement-check-names>. 

[DNAME]
        
Rose, S. and W. Wijngaards, "DNAME Redirection in the DNS", RFC 6672, DOI 10.17487/RFC6672, June 2012, <https://www.rfc-editor.org/info/rfc6672>. 

[DNSTerm]
        
Hoffman, P., Sullivan, A., and K. Fujiwara, "DNS Terminology", BCP 219, RFC 8499, DOI 10.17487/RFC8499, January 2019, <https://www.rfc-editor.org/info/rfc8499>. 

[ECH]
        
Rescorla, E., Oku, K., Sullivan, N., and C. A. Wood, "TLS Encrypted Client Hello", Work in Progress, Internet-Draft, draft-ietf-tls-esni-17, 9 October 2023, <https://datatracker.ietf.org/doc/html/draft-ietf-tls-esni-17>. 

[FETCH]
        
WHATWG, "Fetch Living Standard", October 2023, <https://fetch.spec.whatwg.org/>. 

[FETCH-WEBSOCKETS]
        
WHATWG, "WebSockets Living Standard", September 2023, <https://websockets.spec.whatwg.org/>. 

[HSTS]
        
Hodges, J., Jackson, C., and A. Barth, "HTTP Strict Transport Security (HSTS)", RFC 6797, DOI 10.17487/RFC6797, November 2012, <https://www.rfc-editor.org/info/rfc6797>. 

[HTTP-DNS-RR]
        
Bellis, R., "A DNS Resource Record for HTTP", Work in Progress, Internet-Draft, draft-bellis-dnsop-http-record-00, 3 November 2018, <https://datatracker.ietf.org/doc/html/draft-bellis-dnsop-http-record-00>. 

[HTTP/3]
        
Bishop, M., Ed., "HTTP/3", RFC 9114, DOI 10.17487/RFC9114, June 2022, <https://www.rfc-editor.org/info/rfc9114>. 

[RFC1912]
        
Barr, D., "Common DNS Operational and Configuration Errors", RFC 1912, DOI 10.17487/RFC1912, February 1996, <https://www.rfc-editor.org/info/rfc1912>. 

[RFC6454]
        
Barth, A., "The Web Origin Concept", RFC 6454, DOI 10.17487/RFC6454, December 2011, <https://www.rfc-editor.org/info/rfc6454>. 

[SRV]
        
Gulbrandsen, A., Vixie, P., and L. Esibov, "A DNS RR for specifying the location of services (DNS SRV)", RFC 2782, DOI 10.17487/RFC2782, February 2000, <https://www.rfc-editor.org/info/rfc2782>. 

[URI]
      
Berners-Lee, T., Fielding, R., and L. Masinter, "Uniform Resource Identifier (URI): Generic Syntax", STD 66, RFC 3986, DOI 10.17487/RFC3986, January 2005, <https://www.rfc-editor.org/info/rfc3986>. 





      
Appendix A. Decoding Text in Zone Files
      
DNS zone files are capable of representing arbitrary octet sequences in
basic ASCII text, using various delimiters and encodings, according to an algorithm
defined in Section 5.1 of [RFC1035].
The following summarizes some allowed inputs to that algorithm, using ABNF:¶

; non-special is VCHAR minus DQUOTE, ";", "(", ")", and "\".
non-special = %x21 / %x23-27 / %x2A-3A / %x3C-5B / %x5D-7E
; non-digit is VCHAR minus DIGIT.
non-digit   = %x21-2F / %x3A-7E
; dec-octet is a number 0-255 as a three-digit decimal number.
dec-octet   = ( "0" / "1" ) 2DIGIT /
              "2" ( ( %x30-34 DIGIT ) / ( "5" %x30-35 ) )
escaped     = "\" ( non-digit / dec-octet )
contiguous  = 1*( non-special / escaped )
quoted      = DQUOTE *( contiguous / ( ["\"] WSP ) ) DQUOTE
char-string = contiguous / quoted
¶

The decoding algorithm allows char-string to represent any *OCTET,
using quoting to group values (e.g., those with internal whitespace), and
escaping to represent each non-printable octet as a single escaped sequence.
In this document, this algorithm is referred to as "character-string decoding", because
Section 5.1 of [RFC1035] uses this
algorithm to produce a <character-string>.  Note that while
the length of a <character-string> is limited to 255 octets, the
character-string decoding algorithm can produce output of any length.¶

        
A.1. Decoding a Comma-Separated List
        
In order to represent lists of items in zone files, this specification uses
comma-separated lists.  When the allowed items in the list cannot contain ","
or "\", this is trivial.  (For simplicity, empty items are not allowed.)
A value-list parser that splits on "," and prohibits items containing "\"
is sufficient to comply with all requirements in this document.  This
corresponds to the simple-comma-separated syntax:¶

; item-allowed is OCTET minus "," and "\".
item-allowed           = %x00-2B / %x2D-5B / %x5D-FF
simple-item            = 1*item-allowed
simple-comma-separated = [simple-item *("," simple-item)]
¶

For implementations that allow "," and "\" in item values, the following
escaping syntax applies:¶

item            = 1*OCTET
escaped-item    = 1*(item-allowed / "\," / "\\")
comma-separated = [escaped-item *("," escaped-item)]
¶

Decoding of value-lists happens after character-string decoding.
For example, consider these char-string SvcParamValues:¶

"part1,part2,part3\\,part4\\\\"
part1\,\p\a\r\t2\044part3\092,part4\092\\
¶

These inputs are equivalent: character-string decoding either of them would
produce the same value:¶

part1,part2,part3\,part4\\
¶

Applying comma-separated list decoding to this value would produce a list
of three items:¶

part1
part2
part3,part4\
¶




      
Appendix B. HTTP Mapping Summary
      
This table serves as a non-normative summary of the HTTP mapping for SVCB
(Section 9).  Future protocol mappings may provide a similar summary table.¶

        Table 3

          
            
              Mapped scheme

            "https"
          
          
            
              Other affected schemes

            "http", "wss", "ws", (other HTTP-based)
          
          
            
              RR type

            HTTPS (65)
          
          
            
              Name prefix

            None for port 443, else _$PORT._https

          
          
            
              Automatically mandatory keys

            
              port, no-default-alpn

          
          
            
              SvcParam defaults

            
              alpn: ["http/1.1"]
          
          
            
              Special behaviors

            Upgrade from HTTP to HTTPS
          
          
            
              Keys that records must include

            None
          
        
      


      
Appendix C. Comparison with Alternatives
      
The SVCB and HTTPS RR types closely resemble,
and are inspired by, some existing
record types and proposals.  One complaint regarding all of the alternatives
is that web clients have seemed unenthusiastic about implementing
them.  The hope here is that an extensible solution that
solves multiple problems will overcome this inertia and have a path
to achieve client implementation.¶

        
C.1. Differences from the SRV RR Type
        
An SRV record [SRV] can perform a function similar
to that of the SVCB record,
informing a client to look in a different location for a service.
However, there are several differences:¶

SRV records are typically mandatory, whereas SVCB is intended to be optional
when used with pre-existing protocols.¶

          SRV records cannot instruct the client to switch or upgrade
protocols, whereas SVCB can signal such an upgrade (e.g., to
HTTP/2).¶

          SRV records are not extensible, whereas SVCB and HTTPS RRs
can be extended with new parameters.¶

          SRV records specify a "weight" for unbalanced randomized load balancing.
SVCB only supports balanced randomized load balancing, although weights
could be added via a future SvcParam.¶

        


        
C.2. Differences from the Proposed HTTP Record
        
Unlike [HTTP-DNS-RR], this approach is
extensible to cover Alt-Svc and Encrypted ClientHello use cases.  Like that
proposal, this addresses the zone-apex CNAME challenge.¶
Like that proposal, it remains necessary to continue to include
address records at the zone apex for legacy clients.¶


        
C.3. Differences from the Proposed ANAME Record
        
Unlike [ANAME-DNS-RR], this approach is extensible to
cover Alt-Svc and Encrypted ClientHello use cases.  This approach also does not
require any changes or special handling on either authoritative or
primary servers, beyond optionally returning in-bailiwick additional records.¶
Like that proposal, this addresses the zone-apex CNAME challenge
for clients that implement this.¶
However, with this SVCB proposal, it remains necessary to continue
to include address records at the zone apex for legacy clients.
If deployment of this standard is successful, the number of legacy clients
will fall over time.  As the number of legacy clients declines, the operational
effort required to serve these users without the benefit of SVCB indirection
should fall.  Server operators can easily observe how much traffic reaches this
legacy endpoint and may remove the apex's address records if the observed legacy
traffic has fallen to negligible levels.¶


        
C.4. Comparison with Separate RR Types for AliasMode and ServiceMode
        
Abstractly, functions of AliasMode and ServiceMode are independent,
so it might be tempting to specify them as separate RR types.  However,
this would result in serious performance impairment, because clients
cannot rely on their recursive resolver to follow SVCB aliases (unlike
CNAME).  Thus, clients would have to issue queries for both RR types
in parallel, potentially at each step of the alias chain.  Recursive
resolvers that implement the specification would, upon receipt of a
ServiceMode query, emit both a ServiceMode query and an AliasMode query to
the authoritative DNS server.  Thus, splitting the RR type would double, or in
some cases triple, the load on clients and servers, and would not
reduce implementation complexity.¶



      
Appendix D. Test Vectors
      
These test vectors only contain the RDATA portion of SVCB/HTTPS records in
presentation format, generic format [RFC3597], and wire format. The wire
format uses hexadecimal (\xNN) for each non-ASCII byte. As the wire format is
long, it is broken into several lines.¶

        
D.1. AliasMode
        

          
example.com.   HTTPS   0 foo.example.com.

\# 19 (
00 00                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 63 6f 6d 00 ; target
)

\x00\x00                                           # priority
\x03foo\x07example\x03com\x00                      # target


Figure 2:
AliasMode
          


        
D.2. ServiceMode
        

          
example.com.   SVCB   1 .

\# 3 (
00 01      ; priority
00         ; target (root label)
)

\x00\x01   # priority
\x00       # target (root label)


Figure 3:
TargetName Is "."
          

          
example.com.   SVCB   16 foo.example.com. port=53

\# 25 (
00 10                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 63 6f 6d 00 ; target
00 03                                              ; key 3
00 02                                              ; length 2
00 35                                              ; value
)

\x00\x10                                           # priority
\x03foo\x07example\x03com\x00                      # target
\x00\x03                                           # key 3
\x00\x02                                           # length 2
\x00\x35                                           # value


Figure 4:
Specifies a Port
          

          
example.com.   SVCB   1 foo.example.com. key667=hello

\# 28 (
00 01                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 63 6f 6d 00 ; target
02 9b                                              ; key 667
00 05                                              ; length 5
68 65 6c 6c 6f                                     ; value
)

\x00\x01                                           # priority
\x03foo\x07example\x03com\x00                      # target
\x02\x9b                                           # key 667
\x00\x05                                           # length 5
hello                                              # value


Figure 5:
A Generic Key and Unquoted Value
          

          
example.com.   SVCB   1 foo.example.com. key667="hello\210qoo"

\# 32 (
00 01                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 63 6f 6d 00 ; target
02 9b                                              ; key 667
00 09                                              ; length 9
68 65 6c 6c 6f d2 71 6f 6f                         ; value
)

\x00\x01                                           # priority
\x03foo\x07example\x03com\x00                      # target
\x02\x9b                                           # key 667
\x00\x09                                           # length 9
hello\xd2qoo                                       # value


Figure 6:
A Generic Key and Quoted Value with a Decimal Escape
          

          
example.com.   SVCB   1 foo.example.com. (
                      ipv6hint="2001:db8::1,2001:db8::53:1"
                      )

\# 55 (
00 01                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 63 6f 6d 00 ; target
00 06                                              ; key 6
00 20                                              ; length 32
20 01 0d b8 00 00 00 00 00 00 00 00 00 00 00 01    ; first address
20 01 0d b8 00 00 00 00 00 00 00 00 00 53 00 01    ; second address
)

\x00\x01                                           # priority
\x03foo\x07example\x03com\x00                      # target
\x00\x06                                           # key 6
\x00\x20                                           # length 32
\x20\x01\x0d\xb8\x00\x00\x00\x00
     \x00\x00\x00\x00\x00\x00\x00\x01              # first address
\x20\x01\x0d\xb8\x00\x00\x00\x00
     \x00\x00\x00\x00\x00\x53\x00\x01              # second address


Figure 7:
Two Quoted IPv6 Hints
          

          
example.com.   SVCB   1 example.com. (
                        ipv6hint="2001:db8:122:344::192.0.2.33"
                        )
\# 35 (
00 01                                              ; priority
07 65 78 61 6d 70 6c 65 03 63 6f 6d 00             ; target
00 06                                              ; key 6
00 10                                              ; length 16
20 01 0d b8 01 22 03 44 00 00 00 00 c0 00 02 21    ; address
)

\x00\x01                                           # priority
\x07example\x03com\x00                             # target
\x00\x06                                           # key 6
\x00\x10                                           # length 16
\x20\x01\x0d\xb8\x01\x22\x03\x44
     \x00\x00\x00\x00\xc0\x00\x02\x21              # address


Figure 8:
An IPv6 Hint Using the Embedded IPv4 Syntax
          

          
example.com.   SVCB   16 foo.example.org. (
                      alpn=h2,h3-19 mandatory=ipv4hint,alpn
                      ipv4hint=192.0.2.1
                      )

\# 48 (
00 10                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 6f 72 67 00 ; target
00 00                                              ; key 0
00 04                                              ; param length 4
00 01                                              ; value: key 1
00 04                                              ; value: key 4
00 01                                              ; key 1
00 09                                              ; param length 9
02                                                 ; alpn length 2
68 32                                              ; alpn value
05                                                 ; alpn length 5
68 33 2d 31 39                                     ; alpn value
00 04                                              ; key 4
00 04                                              ; param length 4
c0 00 02 01                                        ; param value
)

\x00\x10                                           # priority
\x03foo\x07example\x03org\x00                      # target
\x00\x00                                           # key 0
\x00\x04                                           # param length 4
\x00\x01                                           # value: key 1
\x00\x04                                           # value: key 4
\x00\x01                                           # key 1
\x00\x09                                           # param length 9
\x02                                               # alpn length 2
h2                                                 # alpn value
\x05                                               # alpn length 5
h3-19                                              # alpn value
\x00\x04                                           # key 4
\x00\x04                                           # param length 4
\xc0\x00\x02\x01                                   # param value


Figure 9:
SvcParamKey Ordering Is Arbitrary in Presentation Format but Sorted in Wire Format
          

          
example.com.   SVCB   16 foo.example.org. alpn="f\\\\oo\\,bar,h2"
example.com.   SVCB   16 foo.example.org. alpn=f\\\092oo\092,bar,h2

\# 35 (
00 10                                              ; priority
03 66 6f 6f 07 65 78 61 6d 70 6c 65 03 6f 72 67 00 ; target
00 01                                              ; key 1
00 0c                                              ; param length 12
08                                                 ; alpn length 8
66 5c 6f 6f 2c 62 61 72                            ; alpn value
02                                                 ; alpn length 2
68 32                                              ; alpn value
)

\x00\x10                                           # priority
\x03foo\x07example\x03org\x00                      # target
\x00\x01                                           # key 1
\x00\x0c                                           # param length 12
\x08                                               # alpn length 8
f\oo,bar                                           # alpn value
\x02                                               # alpn length 2
h2                                                 # alpn value


Figure 10:
An "alpn" Value with an Escaped Comma and an Escaped Backslash in Two Presentation Formats
          


        
D.3. Failure Cases
        
This subsection contains test vectors that are not
compliant with this document. The various reasons for non-compliance
are explained with each example.¶

          
example.com.   SVCB   1 foo.example.com. (
                       key123=abc key123=def
                       )


Figure 11:
Multiple Instances of the Same SvcParamKey
          

          
example.com.   SVCB   1 foo.example.com. mandatory
example.com.   SVCB   1 foo.example.com. alpn
example.com.   SVCB   1 foo.example.com. port
example.com.   SVCB   1 foo.example.com. ipv4hint
example.com.   SVCB   1 foo.example.com. ipv6hint


Figure 12:
Missing SvcParamValues That Must Be Non-Empty
          

          
example.com.   SVCB   1 foo.example.com. no-default-alpn=abc


Figure 13:
The "no-default-alpn" SvcParamKey Value Must Be Empty
          

          
example.com.   SVCB   1 foo.example.com. mandatory=key123


Figure 14:
A Mandatory SvcParam Is Missing
          

          
example.com.   SVCB   1 foo.example.com. mandatory=mandatory


Figure 15:
The "mandatory" SvcParamKey Must Not Be Included in the Mandatory List
          

          
example.com.   SVCB   1 foo.example.com. (
                      mandatory=key123,key123 key123=abc
                      )


Figure 16:
Multiple Instances of the Same SvcParamKey in the Mandatory List
          




      
Authors' Addresses
      

        Ben Schwartz
Meta Platforms, Inc.



        Mike Bishop
Akamai Technologies



        Erik Nygren
Akamai Technologies



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[If my kids excel, will they move away?]]></title>
            <link>https://jeffreybigham.com/blog/2025/where-will-my-kids-go.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236411</guid>
            <description><![CDATA[If my kids excel, will they move away?]]></description>
            <content:encoded><![CDATA[If my kids excel, will they move away?Jeffrey P. BighamI grew up on a farm outside of a rural town about an hour southeast of Columbus, Ohio. Like many small towns in America, my town knows “brain drain” – all of my friends from high school who went to college (~30% of my class) now live elsewhere, although most are pretty close by (e.g., several live in the suburbs of Columbus and Cincinnati).Sometimes my hometown feels a million miles away, but it only takes two hours and fifty minutes for me to drive there from Pittsburgh, which is where I live now.In Pittsburgh, I’m a professor at Carnegie Mellon University in the top computer science school in the world. I’ve also worked in various large technology companies, who have offices in Pittsburgh to connect with and employ Carnegie Mellon faculty and students.I may not live in my small town anymore, but the fact that the best place in the world to study and do research in computer science is in Pittsburgh means I’m really not that far away. My four kids see their grandparents often, they’re known in my parents’ church and have spent a lot of time on my dad’s farm. The photo above is of me on the farm, wearing some of my dad’s clothes, trying to help out when my dad fell ill a few years ago.Most of my story we’ve been able to take for granted in the United States for the past few decades. If you grow up in the United States, and you’re among the best in the world in your field, you could count on the center of excellence for your field also being in the United States, oftentimes pretty close by, like Pittsburgh being close to my hometown.As a professor, I’m able to recruit the very best students in the world to work on my research. Sometimes that means recruiting Americans and sometimes that means recruiting from elsewhere.  Students come to Pittsburgh from around the world (I’ve advised PhD students and postdocs from about 10 different countries). Five or six years after they start our intensive graduation program, successful students receive their PhDs and that’s when I tend to meet their parents for the first time. Oftentimes, this is the first trip they’ve made to the United States, and they may have only seen their kids a few times during their degree. It hits home because usually these students choose to stay in the United States – after successfully completing their degree with me, they are in high demand not only in our universities but also in technology companies.


These days the students I talk to are less confident about coming to the United States to study and less confident about staying here after they’re done. They have seen a student grabbed off the street apparently because she wrote an essay expressing concern about the on-going humanitarian crisis in Gaza. They have seen graduate students jailed for what used to be minor immigration offenses. They have seen even greater uncertainty in applying or reapplying for the visas they need to study. And, they have seen their status as students arbitrarily used as leverage in attacking premier universities like Harvard[1]. Most of these incidents have or probably will be resolved, but the message and fear it causes are real and long-lasting.I am worried that policies that have the intention (or effect) of introducing chaos and cruelty to superstar students will make it less likely for the best of the best to come to America, and this in turn will mean centers of excellence will move elsewhere. While incumbents have an advantage, it doesn’t take much to influence group behavior and movements can be self-reinforcing. The best people in a field like to be where other amazing people are, so they can learn and build off of each other. If the centers of excellence move elsewhere, I’m worried my kids will end up feeling compelled to move away (should they become superstars, as is my hope for them).The brain drain from our small rural communities is real, but many of us have found ways to stay close by and keep those ties. There’s a bunch of reasons to treat international students better than we have over the past months, but these concerns are not thousands of miles away as they seem to some – to me, it's incredibly close to home, and not only because I see the effect on students I work with closely.If we cause centers of excellence to move away from Pittsburgh, and away from the United States entirely, that’s the difference between my grandkids living near or very far, and whether they’re likely to grow up visiting me and my dad’s farm often or hardly at all.[1] I’ve owned exactly two Harvard t-shirts in my lifetime – the first when I was an undergrad at Princeton said, ‘Harvard Sucks’, and the second is a normal Harvard t-shirt that I bought this past May.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EFF to court: The Supreme Court must rein in secondary copyright liability]]></title>
            <link>https://www.eff.org/deeplinks/2025/09/eff-court-supreme-court-must-rein-expansive-secondary-copyright-liability</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236314</guid>
            <description><![CDATA[If the Supreme Court doesn’t reverse a lower court’s ruling, internet service providers (ISPs) could be forced to terminate people’s internet access based on nothing more than mere accusations of copyright infringement. This would threaten innocent users who rely on broadband for essential aspects...]]></description>
            <content:encoded><![CDATA[
            
  
  
  If the Supreme Court doesn’t reverse a lower court’s ruling, internet service providers (ISPs) could be forced to terminate people’s internet access based on nothing more than mere accusations of copyright infringement. This would threaten innocent users who rely on broadband for essential aspects of daily life. EFF—along with the American Library Association, the Association of Research Libraries, and Re:Create—filed an amicus brief urging the Court to reverse the decision.
The Stakes: Turning ISPs into Copyright Police
Among other things, the Supreme Court approving the appeals court’s findings will radically change the amount of risk your ISP takes on if a customer infringes on copyright, forcing the ISP to terminate access to the internet for those users accused of copyright infringement—and everyone else who uses that internet connection.
This issue turns on what courts call “secondary liability,” which is the legal idea that someone can be held responsible not for what they did directly, but for what someone else did using their product or service.
The case began when music companies sued Cox Communications, arguing that the ISP should be held liable for copyright infringement committed by some of its subscribers. The Court of Appeals for the Fourth Circuit agreed, adopting a “material contribution” standard for contributory copyright liability (a rule for when service providers can be held liable for the actions of users). The lower court said that providing a service that could be used for infringement is enough to create liability when a customer infringes.
In the Patent Act, where Congress has explicitly defined secondary liability, there’s a different test: contributory infringement exists only where a product is incapable of substantial non-infringing use. Internet access, of course, is overwhelmingly used for lawful purposes, making it the very definition of a “staple article of commerce” that can’t be liable under the patent framework. Yet under the Fourth Circuit’s rule, ISPs could face billion-dollar damages if they fail to terminate users on the basis of even flimsy or automated infringement claims.
Our Argument: Apply Clear Rules from the Patent Act, Not Confusing Judge-Made Tests
Our brief urges the Court to do what it has done in the past: look to patent law to define the limits of secondary liability in copyright. That means contributory infringement must require more than a “material contribution” by the service provider—it should apply only when a product or service is especially designed for infringement and lacks substantial non-infringing uses.
The Human Cost: Losing Internet Access Hurts Everyone 
The Fourth Circuit’s rule threatens devastating consequences for the public. Terminating an ISP account doesn’t just affect a person accused of unauthorized file sharing—it cuts off entire households, schools, libraries, or businesses that share an internet connection.

Public libraries, which provide internet access to millions of Americans who lack it at home, could lose essential service.
Universities, hospitals, and local governments could see internet access for whole communities disrupted.
Households—especially in low-income and communities of color, which disproportionately share broadband connections with other people—would face collective punishment for the alleged actions of a single user.

With more than a third of Americans having only one or no broadband provider, many users would have no way to reconnect once cut off. And given how essential internet access is for education, employment, healthcare, and civic participation, the consequences of termination are severe and disproportionate.
What’s Next
The Supreme Court has an opportunity to correct course. We’re asking the Court to reject the Fourth Circuit’s unfounded “material contribution” test, reaffirm that patent law provides the right framework for secondary liability, and make clear that the Constitution requires copyright to serve the public good. The Court should ensure that copyright enforcement doesn’t jeopardize the internet access on which participation in modern life depends.
We’ll be watching closely as the Court considers this case. In the meantime, you can read our amicus brief here.



          
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Two Slice, a font that's only 2px tall]]></title>
            <link>https://joefatula.com/twoslice.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236263</guid>
            <description><![CDATA[A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.]]></description>
            <content:encoded><![CDATA[
		
		A font that's only 2px tall, and somewhat readable!  Uppercase and lowercase have some different variants, in case you find one more readable than the other.  Numbers (sort of) and some punctuation marks are included.
		You can probably read this, even if you wish you couldn't.It tends to be easier to read at smaller sizes.
		Try it out below, or download it (under CC BY-SA license, so you can use it commercially but you have to give credit).
		
	
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pass: Unix Password Manager]]></title>
            <link>https://www.passwordstore.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45236079</guid>
            <description><![CDATA[Pass is the standard unix password manager, a lightweight password manager that uses GPG and Git for Linux, BSD, and Mac OS X.]]></description>
            <content:encoded><![CDATA[

Introducing pass

Password management should be simple and follow Unix philosophy. With pass, each password lives inside of a gpg encrypted file whose filename is the title of the website or resource that requires the password. These encrypted files may be organized into meaningful folder hierarchies, copied from computer to computer, and, in general, manipulated using standard command line file management utilities.

pass makes managing these individual password files extremely easy. All passwords live in ~/.password-store, and pass provides some nice commands for adding, editing, generating, and retrieving passwords. It is a very short and simple shell script. It's capable of temporarily putting passwords on your clipboard and tracking password changes using git.

You can edit the password store using ordinary unix shell commands alongside the pass command. There are no funky file formats or new paradigms to learn. There is bash completion so that you can simply hit tab to fill in names and commands, as well as completion for zsh and fish available in the completion folder. The very active community has produced many impressive clients and GUIs for other platforms as well as extensions for pass itself.

The pass command is extensively documented in its man page.



Using the password store

We can list all the existing passwords in the store:

zx2c4@laptop ~ $ pass
Password Store
├── Business
│   ├── some-silly-business-site.com
│   └── another-business-site.net
├── Email
│   ├── donenfeld.com
│   └── zx2c4.com
└── France
    ├── bank
    ├── freebox
    └── mobilephone


And we can show passwords too:

zx2c4@laptop ~ $ pass Email/zx2c4.com
sup3rh4x3rizmynam3


Or copy them to the clipboard:

zx2c4@laptop ~ $ pass -c Email/zx2c4.com
Copied Email/jason@zx2c4.com to clipboard. Will clear in 45 seconds.


There will be a nice password input dialog using the standard gpg-agent (which can be configured to stay authenticated for several minutes), since all passwords are encrypted.

We can add existing passwords to the store with insert:

zx2c4@laptop ~ $ pass insert Business/cheese-whiz-factory
Enter password for Business/cheese-whiz-factory: omg so much cheese what am i gonna do


This also handles multiline passwords or other data with --multiline or -m, and passwords can be edited in your default text editor using pass edit pass-name.

The utility can generate new passwords using /dev/urandom internally:

zx2c4@laptop ~ $ pass generate Email/jasondonenfeld.com 15
The generated password to Email/jasondonenfeld.com is:
$(-QF&Q=IN2nFBx


It's possible to generate passwords with no symbols using --no-symbols or -n, and we can copy it to the clipboard instead of displaying it at the console using --clip or -c.

And of course, passwords can be removed:

zx2c4@laptop ~ $ pass rm Business/cheese-whiz-factory
rm: remove regular file ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’? y
removed ‘/home/zx2c4/.password-store/Business/cheese-whiz-factory.gpg’


If the password store is a git repository, since each manipulation creates a git commit, you can synchronize the password store using pass git push and pass git pull, which call git-push or git-pull on the store.

You can read more examples and more features in the man page.

Setting it up

To begin, there is a single command to initialize the password store:

zx2c4@laptop ~ $ pass init "ZX2C4 Password Storage Key"
mkdir: created directory ‘/home/zx2c4/.password-store’
Password store initialized for ZX2C4 Password Storage Key.


Here, ZX2C4 Password Storage Key is the ID of my GPG key. You can use your standard GPG key or use an alternative one especially for the password store as shown above. Multiple GPG keys can be specified, for using pass in a team setting, and different folders can have different GPG keys, by using -p.

We can additionally initialize the password store as a git repository:

zx2c4@laptop ~ $ pass git init
Initialized empty Git repository in /home/zx2c4/.password-store/.git/
zx2c4@laptop ~ $ pass git remote add origin kexec.com:pass-store


If a git repository is initialized, pass creates a git commit each time the password store is manipulated.

There is a more detailed initialization example in the man page.

Download

The latest version is 1.7.4.

Ubuntu / Debian

$ sudo apt-get install pass

Fedora / RHEL

$ sudo yum install pass

openSUSE

$ sudo zypper in password-store

Gentoo

# emerge -av pass

Arch

$ pacman -S pass

Macintosh

The password store is available through the Homebrew package manager:

$ brew install pass

FreeBSD

# pkg install password-store

Tarball


Version 1.7.4
Latest Git

The tarball contains a generic makefile, for which a simple sudo make install should do the trick.

Git Repository

You may browse the git repository or clone the repo:


$ git clone https://git.zx2c4.com/password-store

All releases are tagged, and the tags are signed with 0xA5DE03AE.

Data Organization

Usernames, Passwords, PINs, Websites, Metadata, et cetera

The password store does not impose any particular schema or type of organization of your data, as it is simply a flat text file, which can contain arbitrary data. Though the most common case is storing a single password per entry, some power users find they would like to store more than just their password inside the password store, and additionally store answers to secret questions, website URLs, and other sensitive information or metadata. Since the password store does not impose a scheme of it's own, you can choose your own organization. There are many possibilities.

One approach is to use the multi-line functionality of pass (--multiline or -m in insert), and store the password itself on the first line of the file, and the additional information on subsequent lines. For example, Amazon/bookreader might look like this:

Yw|ZSNH!}z"6{ym9pI
URL: *.amazon.com/*
Username: AmazonianChicken@example.com
Secret Question 1: What is your childhood best friend's most bizarre superhero fantasy? Oh god, Amazon, it's too awful to say...
Phone Support PIN #: 84719

This is the preferred organzational scheme used by the author. The --clip / -c options will only copy the first line of such a file to the clipboard, thereby making it easy to fetch the password for login forms, while retaining additional information in the same file.

Another approach is to use folders, and store each piece of data inside a file in that folder. For example Amazon/bookreader/password would hold bookreader's password inside the Amazon/bookreader directory, and Amazon/bookreader/secretquestion1 would hold a secret question, and Amazon/bookreader/sensitivecode would hold something else related to bookreader's account. And yet another approach might be to store the password in Amazon/bookreader and the additional data in Amazon/bookreader.meta. And even another approach might be use multiline, as outlined above, but put the URL template in the filename instead of inside the file.

The point is, the possibilities here are extremely numerous, and there are many other organizational schemes not mentioned above; you have the freedom of choosing the one that fits your workflow best.

Extensions for pass
In order to faciliate the large variety of uses users come up with, pass supports extensions. Extensions installed to /usr/lib/password-store/extensions (or some distro-specific variety of such) are always enabled. Extensions installed to ~/.password-store/.extensions/COMMAND.bash are enabled if the PASSWORD_STORE_ENABLE_EXTENSIONS environment variable is true Read the man page for more details.

The community has produced many such extensions:

	pass-tomb: manage your password store in a Tomb
	pass-update: an easy flow for updating passwords
	pass-import: a generic importer tool from other password managers
	pass-extension-tail: a way of printing only the tail of a file
	pass-extension-wclip: a plugin to use wclip on Windows
	pass-otp: support for one-time-password (OTP) tokens


Compatible Clients
The community has assembled an impressive list of clients and GUIs for various platforms:


	passmenu: an extremely useful and awesome dmenu script
	qtpass: cross-platform GUI client
	Android-Password-Store: Android app
	passforios: iOS app
	pass-ios: (older) iOS app
	passff: Firefox plugin
	browserpass: Chrome plugin
	Pass4Win: Windows client
	pext_module_pass: module for Pext
	gopass: Go GUI app
	upass: interactive console UI
	alfred-pass: Alfred integration
	pass-alfred: Alfred integration
	simple-pass-alfred: Alfred integration
	pass.applescript: OS X integration
	pass-git-helper: git credential integration
	password-store.el: an emacs package
	XMonad.Prompt.Pass: prompt for Xmonad


Migrating to pass
To free password data from the clutches of other (bloated) password managers, various users have come up with different password store organizations that work best for them. Some users have contributed scripts to help import passwords from other programs:


	1password2pass.rb: imports 1Password txt or 1pif data
	keepassx2pass.py: imports KeepassX XML data
	keepass2csv2pass.py: imports Keepass2 CSV data
	keepass2pass.py: imports Keepass2 XML data
	fpm2pass.pl: imports Figaro's Password Manager XML data
	lastpass2pass.rb: imports Lastpass CSV data
	kedpm2pass.py: imports Ked Password Manager data
	revelation2pass.py: imports Revelation Password Manager data
	gorilla2pass.rb: imports Password Gorilla data
	pwsafe2pass.sh: imports PWSafe data
	kwallet2pass.py: imports KWallet data
	roboform2pass.rb: imports Roboform data
	password-exporter2pass.py: imports password-exporter data
	pwsafe2pass.py: imports pwsafe data
	firefox_decrypt: full blown Firefox password interface, which supports exporting to pass


Credit & License

pass was written by Jason A. Donenfeld of zx2c4.com and is licensed under the GPLv2+.

Contributing

This is a very active project with a healthy dose of contributors. The best way to contribute to the password store is to join the mailing list and send git formatted patches. You may also join the discussion in #pass on Libera.Chat.


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Will AI be the basis of many future industrial fortunes, or a net loser?]]></title>
            <link>https://joincolossus.com/article/ai-will-not-make-you-rich/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235676</guid>
            <description><![CDATA[The disruption is real. It's also predictable.]]></description>
            <content:encoded><![CDATA[
                        
Fortunes are made by entrepreneurs and investors when revolutionary technologies enable waves of innovative, investable companies. Think of the railroad, the Bessemer process, electric power, the internal combustion engine, or the microprocessor—each of which, like a stray spark in a fireworks factory, set off decades of follow-on innovations, permeated every part of society, and catapulted a new set of inventors and investors into power, influence, and wealth.



Yet some technological innovations, though societally transformative, generate little in the way of new wealth; instead, they reinforce the status quo. Fifteen years before the microprocessor, another revolutionary idea, shipping containerization, arrived at a less propitious time, when technological advancement was a Red Queen’s race, and inventors and investors were left no better off for non-stop running.



Anyone who invests in the new new thing must answer two questions: First, how much value will this innovation create? And second, who will capture it? Information and communication technology (ICT) was a revolution whose value was captured by startups and led to thousands of newly rich founders, employees, and investors. In contrast, shipping containerization was a revolution whose value was spread so thin that in the end, it made only a single founder temporarily rich and only a single investor a little bit richer.



Is generative AI more like the former or the latter? Will it be the basis of many future industrial fortunes, or a net loser for the investment community as a whole, with a few zero-sum winners here and there?



There are ways to make money investing in the fruits of AI, but they will depend on assuming the latter—that it is once again a less propitious time for inventors and investors, that AI model builders and application companies will eventually compete each other into an oligopoly, and that the gains from AI will accrue not to its builders but to customers. A lot of the money pouring into AI is therefore being invested in the wrong places, and aside from a couple of lucky early investors, those who make money will be the ones with the foresight to get out early.



    




The microprocessor was revolutionary, but the people who invented it at Intel in 1971 did not see it that way—they just wanted to avoid designing desktop calculator chipsets from scratch every time. But outsiders realized they could use the microprocessor to build their own personal computers, and enthusiasts did. Thousands of tinkerers found configurations and uses that Intel never dreamed of. This distributed and permissionless invention kicked off a “great surge of development,” as the economist Carlota Perez calls it, triggered by technology but driven by economic and societal forces.[1]



There was no real demand for personal computers in the early 1970s; they were expensive toys. But the experimenters laid the technical groundwork and built a community. Then, around 1975, a step-change in the cost of microprocessors made the personal computer market viable. The Intel 8080 had an initial list price of $360 ($2,300 in today’s dollars). MITS could barely turn a profit on its Altair at a bulk price of $75 each ($490 today). But when MOS Technologies started selling its 6502 for $25 ($150 today), Steve Wozniak could afford to build a prototype Apple. The 6502 and the similarly priced Zilog Z80 forced Intel’s prices down. The nascent PC community started spawning entrepreneurs and a score of companies appeared, each with a slightly different product.



You couldn’t have known in the mid-1970s that the PC (and PC-like products, such as ATMs, POS terminals, smartphones, etc.) would revolutionize everything. While Steve Jobs was telling investors that every household would someday have a personal computer (a wild underestimate, as it turned out), others questioned the need for personal computers at all. As late as 1979, Apple’s ads didn’t tell you what a personal computer could do—it asked what you did with it.[2] The established computer manufacturers (IBM, HP, DEC) had no interest in a product their customers weren’t asking for. Nobody “needed” a computer, and so PCs weren’t bought—they were sold. Flashy startups like Apple and Sinclair used hype to get noticed, while companies with footholds in consumer electronics like Atari, Commodore, and Tandy/RadioShack used strong retail connections to put their PCs in front of potential customers. 



        
            
            

            
                    

    



The market grew slowly at first, accelerating only as experiments led to practical applications like the spreadsheet, introduced in 1979. As use grew, observation of use caused a reduction in uncertainty, leading to more adoption in a self-reinforcing cycle. This kind of gathering momentum takes time in every technological wave: It took almost 30 years for electricity to reach half of American households, for example, and it took about the same amount of time for personal computers.[3] When a technological revolution changes everything, it takes a huge amount of innovation, investment, storytelling, time, and plain old work. It also sucks up all the money and talent available. Like Kuhn’s paradigms in science, any technology not part of the wave’s techno-economic paradigm will seem like a sideshow.[4]



        
            
            

            
                            Source: [3]
                    

    



The nascent growth of PCs attracted investors—venture capitalists—who started making risky bets on new companies. This development incentivized more inventors, entrepreneurs, and researchers, which in turn drew in more speculative capital.



Companies like IBM, the computing behemoth before the PC, saw poor relative performance. They didn’t believe the PC could survive long enough to become capable in their market and didn’t care about new, small markets that wanted a cheaper solution.



Retroactively, we give the PC pioneers the powers of prophets rather than visionaries. But at the time, nobody outside of a small group of early adopters paid any attention. Establishment media like The New York Times didn’t take the PC seriously until after IBM’s was introduced in August 1981. In the entire year of 1976, when Apple Computer was founded, the NYT mentioned PCs only four times.[5] Apparently, only the crazy ones, the misfits, the rebels, and the troublemakers were paying attention.



        
            
            

            
                            Source: [5]
                    

    



It’s the element of surprise that should strike us most forcefully when we compare the early days of the computer revolution to today. No one took note of personal computers in the 1970s. In 2025, AI is all we seem to talk about.



    




Big companies hate surprises. That’s why uncertainty makes a perfect moat for startups. Apple would never have survived IBM entering the market in 1979, and only lived to compete another day after raising $100 million in its 1980 IPO. It was the only remaining competitor after the IBM-induced winnowing.[6]



        
            
            

            
                            Source: [6]
                    

    



As the tech took hold and started to show promise, innovations in software, memory, and peripherals like floppy disk drives and modems joined it. They reinforced one another, with each advance putting pressure on the technologies adjacent to it. When any part of the system held back the other parts, investors rushed to fund that sector. As increases in PC memory allowed more complicated software, for example, there became a need for more external storage, which caused VC Dave Marquardt to invest in disk drive manufacturer Seagate in 1980. Seagate gave Marquardt a 40x return when it went public in 1981. Other investors noticed, and some $270 million was plowed into the industry in the following three years.[7]



Money also poured into the underlying infrastructure—fiber optic networks, chip making, etc.—so that capacity was never a bottleneck. Companies which used the new technological system to outperform incumbents began to take market share, and even staid competitors realized they needed to adopt the new thing or die. The hype became a froth which became an investment bubble: the dot-com frenzy of the late 1990s. The ICT wave was therefore similar to previous ones—like the investment mania of the 1830s and the Roaring ‘20s, which followed the infrastructure buildout of canals and railways, respectively—in which the human response to each stage predictably generated the next.



When the dot-com bubble popped, society found it disapproved of the excesses in the sector and governments found they had the popular support to reassert authority over the tech companies and their investors. This put a brake on the madness. Instead of the reckless innovation of the bubble, companies started to expand into proven markets, and financiers moved from speculating to investing. Entrepreneurs began to focus on finding applications rather than on innovating the underlying technologies. Technological improvements continued, but change became more evolutionary than revolutionary.



As change slowed, companies gained the confidence to invest for the longer term. They began to combine various parts of the system in new ways to create value for a wider group of users. The massive overbuilding of fiber optic telecom networks and other infrastructure during the frenzy left plenty of cheap capacity, keeping the costs of expansion down. It was a great time to be a businessperson and investor.



        
            
            

            
                    

    



In contrast, society did not need a bubble to pop to start excoriating AI. Given that the backlash to tech has been going on for a decade, this seems normal to us. But the AI backlash differs from the general high regard, earlier in the cycle, enjoyed by the likes of Bill Gates, Steve Jobs, Jeff Bezos, and others who built big tech businesses. The world hates change, and only gave tech a pass in the ‘80s and ‘90s because it all still seemed reversible: it could be made to go away if it turned out badly. This gave the early computer innovators some leeway to experiment. Now that everyone knows computers are here to stay, AI is not allowed the same wait-and-see attitude. It is seen as part of the ICT revolution.



    




Perez, the economist, breaks each technological wave into four predictable phases: irruption, frenzy, synergy, and maturity. Each has a characteristic investment profile.



        
            
            

            
                    

    



The middle two, frenzy and synergy, are the easy ones for investors. Frenzy is when everyone piles in and investors are rewarded for taking big risks on unproven ideas, culminating in the bubble, when paper profits disappear. When rationality returns, the synergy phase begins, as companies make their products usable and productive for a wide array of users. Synergy pays those who are patient, picky, and can bring more than just money to the table.



Irruption and maturity are more difficult to invest in.



Investing in the 1970s was harder than it might look in hindsight. To invest from 1971 through 1975, you had to be either a true believer or a conglomerator with a knuckle-headed diversification strategy. Intel was a great investment, though it looked at first like a previous-wave electronics company. MOS Technologies was founded in 1969 to compete with Texas Instruments but sold a majority of itself to Allen-Bradley to stay afloat. Zilog was funded in 1975 by Exxon (Exxon!). Apple was a great investment, but it had none of the hallmarks of what VCs look for, as the PC was still a solution in search of a problem.



It was later irruption, in the early 1980s, when great opportunities proliferated: PC makers (Compaq, Dell), software and operating systems (Microsoft, Electronic Arts, Adobe), peripherals (Seagate), workstations (Sun), and computer stores (Businessland), among others. If you invested in the winners, you did well. But there was still more money than ideas, which meant that it was no golden age for investing. By 1983, there were more than 70 companies competing in the disk drive sector alone, and valuations collapsed. There were plenty of people whose fortunes were established in the 1970s and 1980s, and many VCs made their names in that era. But the biggest advantage to being an irruption-stage investor was building institutional knowledge to invest early and well in the frenzy and synergy phases.



Investing in the maturity phase is even more difficult. In irruption, it’s hard to see what will happen; in maturity, nothing much happens at all. The uncertainty about what will work and how customers and society will react is almost gone. Things are predictable, and everyone acts predictably.



The lack of dynamism allows the successful synergy companies to remain entrenched (see: the Nifty 50 and FAANG), but growth becomes harder. They start to enter each other’s markets, conglomerate, raise prices, and cut costs. The era of products priced to entice new customers ends, and quality suffers. The big companies continue to embrace the idea of revolutionary innovation, but feel the need to control how their advances are used. R&D spending is redirected from product and process innovation toward increasingly fruitless attempts to find ways to extend the current paradigm. Companies frame this as a drive to win, but it’s really a fear of losing.



Innovation can happen during maturity, sometimes spectacularly. But because these innovations only find support if they fit into the current wave’s paradigm, they are easily captured in the dominant companies’ gravity wells. This means making money as an entrepreneur or investor in them is almost impossible. Generative AI is clearly being captured by the dominant ICT companies, which raises the question of whether this time will be different for inventors and investors—a different question from whether AI itself is a revolutionary technology.



    




Shipping containerization was a late-wave innovation that changed the world, kicked off our modern era of globalization, resulted in profound changes to society and the economy, and contributed to rapid growth in well-being. But there were, perhaps, only one or two people who made real money investing in it.



The year 1956 was late in the previous wave. But that year, the company soon to be known as SeaLand revolutionized freight shipping with the launch of the first containership, the Ideal-X. SeaLand’s founder, Malcom McLean, had an epiphany that the job to be done by truckers, railroads, and shipping lines was to move goods from shipper to destination, not to drive trucks, fill boxcars, or lade boats. SeaLand allowed freight to transfer seamlessly from one mode to another, saving time, making shipping more predictable, and cutting costs—both the costs of loading, unloading, and reloading, and the cost of a ship sitting idly in port as it was loaded and unloaded.[8]



The benefits of containerization, if it could be made to happen, were obvious. Everybody could see the efficiencies, and customers don’t care how something gets to where they can buy it, as long as it does. But longshoremen would lose work, politicians would lose the votes of those who lost work, port authorities would lose the support of the politicians, federal regulators would be blamed for adverse consequences, railroads might lose freight to shipping lines, shipping lines might lose freight to new shipping lines, and it would all cost a mint. Most thought McLean would never be able to make it work.



McLean squeezed through the cracks of the opposition he faced. He bought and retrofitted war surplus ships, lowering costs. He went after the coastal shipping trade, a dying business in the age of the new interstates, to avoid competition. He set up shop in Newark, NJ, rather than the shipping hub of Hell’s Kitchen, to get buy-in from the port authority and avoid Manhattan congestion. And he made a deal with the New York longshoremen’s union, which was only possible because he was a small player whom they figured was not a threat.



        
            
            

            
                            Source: [10]
                    

    



But competitors and regulators moved too quickly for McLean to seize the few barriers to entry that might have been available to him: domination of the ports, exclusive agreements with shippers or other forms of transportation, standardization on proprietary technology, etc.[9] When it started to look like it might work, around 1965, the obvious advantages of containerization meant that every large shipping line entered the business, and competition took off. Even though containerized freight was less than 1% of total trade by 1968, the number of containerships was already ramping fast.[10] Capacity outstripped demand for years. 



The increase in competition led to a rate war, which led to squeezed profits, which in turn led to consolidation and cartels. Meanwhile, the cost of building ever-larger container ships and the port facilities to deal with them meant the business became hugely capital intensive. McLean saw the writing on the wall and sold SeaLand to R.J. Reynolds in January 1969. He was, perhaps, the only entrepreneur to get out unscathed.



It took a long time for the end-to-end vision to be realized. But around 1980, a dramatic drop began in the cost of sea freight.[11] This contributed to a boom in international trade[12] and allowed manufacturers to move away from higher-wage to lower-wage countries, making containerization irreversible.



        
            
            

            
                            Source: [11]
                    

    



Some people did make money, of course; someone always does. McLean did, as did shipping magnate Daniel Ludwig, who had invested $8.5 million in SeaLand’s predecessor, McLean Industries, at $8.50 per share in 1965 and sold in 1969 for $50 per share.[13] Shipbuilders made money, too: between 1967 and 1972, some $10 billion ($80 billion in 2025 dollars) was spent building containerships. The contractors that built the new container ports also made money. And, later, shipping lines that consolidated and dominated the business, like Maersk and Evergreen, became very large. But, “for R.J. Reynolds, and for other companies that had chased fast growth by buying into container shipping in the late 1960s, their investments brought little but disappointment.”[14] Aside from McLean and Ludwig, it is hard to find anyone who became rich from containerization itself, because competition and capex costs made it hard to grow fast or achieve high margins.



        
            
            

            
                            Source: [12]
                    

    



The business ended up being dominated primarily by the previous incumbents, and the margins went to the companies shipping goods, not the ones they shipped through. Companies like IKEA benefited from cheap shipping, going from a provincial Scandinavian company in 1972 to the world’s largest furniture retailer by 2008; container shipping was a perfect fit for IKEA’s flat-pack furniture. Others, like Walmart, used the predictability enabled by containerization to lower inventory and its associated costs.



With hindsight, it’s easy to see how you could have invested in containerization: not in the container shipping industry itself, but in the industries that benefited from containerization. But even here, the success of companies like Walmart, Costco, and Target was coupled with the failure of others. The fallout from containerization set Sears and Woolworth on downward spirals, put the final nail in the coffin of Montgomery Ward and A&P, and drove Macy’s into bankruptcy before it was rescued and downsized by Federated. Meanwhile, in North Carolina, “the furniture capital of the world,” furniture makers tried to compete with IKEA by importing cheap pieces from China. They ended up being replaced by their suppliers.[15]



If there had been more time to build moats, there might have been a few dominant containerization companies, and the people behind them would be at the top of the Forbes 400, while their investors would be legendary. But moats take time to build and, unlike the personal computer, the adoption of containerization wasn’t a surprise—every business with interests at stake had a strategic plan immediately.



The economist Joseph Schumpeter said “perfect competition is and always has been temporarily suspended whenever anything new is being introduced.”[16] But containerization shows this isn’t true at the end of tech waves. And because there is no economic profit during perfect competition, there is no money to be made by innovators during maturity. Like containerization, the introduction of AI did not lead to a period of protected profits for its innovators. It led to an immediate competitive free-for-all.



    




Let’s grant that generative AI is revolutionary (but also that, as is becoming increasingly clear, this particular tech is now already in an evolutionary stage). It will create a lot of value for the economy, and investors hope to capture some of it. When, who, and how depends on whether AI is the end of the ICT wave, or the beginning of a new one. 



If AI had started a new wave, there would have been an extended period of uncertainty and experimentation. There would have been a population of early adopters experimenting with their own models. When thousands or millions of tinkerers use the tech to solve problems in entirely new ways, its uses proliferate. But because they are using models owned by the big AI companies, their ability to fully experiment is limited to what’s allowed by the incumbents, who have no desire to permit an extended challenge to the status quo.



This doesn’t mean AI can’t start the next technological revolution. It might, if experimentation becomes cheap, distributed and permissionless—like Wozniak cobbling together computers in his garage, Ford building his first internal combustion engine in his kitchen, or Trevithick building his high-pressure steam engine as soon as James Watt’s patents expired. When any would-be innovator can build and train an LLM on their laptop and put it to use in any way their imagination dictates, it might be the seed of the next big set of changes—something revolutionary rather than evolutionary. But until and unless that happens, there can be no irruption.



AI is instead the epitome of the ICT wave. The computing visionaries of the 1960s set out to build a machine that could think, which their successors eventually did, by extending gains in algorithms, chips, data, and data center infrastructure. Like containerization, AI is an extension of something that came before, and therefore no one is surprised by what it can and will do. In the 1970s, it took time for people to wrap their heads around the desirability of powerful and ubiquitous computing. But in 2025, machines that think better than previous machines are easy for people to understand.



Consider the extent to which the progress of AI rhymes with the business evolution of containerization:



        
            
            

            
                    

    



In the “AI rhymes” column, the first four items are already underway. How you should invest depends on whether you believe Nos. 5–7 are next.



    




Economists are predicting that AI will increase global GDP somewhere between 1%[17] to more than 7%[18] over the next decade, which is $1–7 trillion of new value created. The big question is where that money will stick as it flows through the value chain.



Most AI market overviews have a score or more categories, breaking each of them into customer and industry served. But these will change dramatically over the next few years. You could, instead, just follow the money to simplify the taxonomy of companies:



        
            
            

            
                    

    



What the history of containerization suggests is that, if you aren’t already an investor in a model company, you shouldn’t bother. Sam Altman and a few other early movers may make a fortune, as McLean and Ludwig did. But the huge costs of building and running a model, coupled with intense competition, means there will, in the end, be only a few companies, each funded and owned by the largest tech companies. If you’re already an investor, congratulations: There will be consolidation, so you might get an exit.



Domain-specific models—like Cursor or Harvey—will be part of the consolidation. These are probably the most valuable models. But fine-tuning is relatively cheap, and there are big economies of scope. On the other hand, just as Google had to buy Invite Media in 2010 to figure out how to sell to ad agencies, domain-specific model companies that have earned the trust of their customers will be prime acquisition targets. And although it seems possible that models which generate things other than language—like Midjourney or Runway—might use their somewhat different architecture to carve out a separate technological path, the LLM companies have easily entered this space as well. Whether this applies to companies like Osmo remains to be seen.



While it’s too late to invest in the model companies, the profusion of those using the models to solve specific problems is ongoing: Perplexity, InflectionAI, Writer, Abridge, and a hundred others. But if any of these become very valuable, the model companies will take their earnings, either through discriminatory pricing or vertical integration. Success, in other words, will mean defeat—always a bad thesis. At some point, model companies and app companies will converge: There will simply be AI companies, and only a few of them. There will be some winners, as always, but investments in the app layer as a whole will lose money. 



The same caveat applies, however: If an app company can build a customer base or an amazing team, it might be acquired. But these companies aren’t really technology companies at all; they are building a market on spec and have to be priced as such. A further caveat is that there will be investors who make a killing arbitraging FOMO-panicked acquirors willing to massively overpay. But this is not really “investing.”



There might be an investment opportunity in companies that manage the interface between the AI giants and their customers, or protect company data from the model companies—like Hugging Face or Glean—because these businesses are by nature independent of the models. But no analogue in the post-containerization shipping market became very large. Even the successful intermediation companies in the AI space will likely end up mid-sized because the model companies will not allow them to gain strategic leverage—another consequence of the absence of surprise.



    




When an industry is going to be big but there is uncertainty about how it will play out, it often makes sense to swim upstream to the industry’s suppliers. In the case of AI, this means the chip providers, data companies, and cloud/data center companies: SambaNova, Scale AI, and Lambda, as well as those that have been around for a long time, like Nvidia and Bloomberg.



The case for data is mixed. General data—i.e., things most people know, including everything anyone knew more than, say, 10 years ago, and most of what was learned after that—is a commodity. There may be room for a few companies to do the grunt work of collating and tagging it, but since the collating and tagging might best be done by AI itself, there will not be a lot of pricing leverage. Domain-specific models will need specialist data, and other models will try to answer questions about the current moment. Specific, timely, and hard to reproduce data will be valuable. This is not a new market, of course—Bloomberg and others have done well by it. A more concentrated customer base will lower prices for this data, while wider use will raise revenues. On balance, this will probably be a plus for the industry, though not a huge one. There will be new companies built, but only a couple worth investing in.



The high capex of AI companies will primarily be spent with the infrastructure companies. These companies are already valued with this expectation, so there won’t be an upside surprise. But consider that shipbuilding benefited from containerization from 1965 until demand collapsed after about 1973.[19] If AI companies consolidate or otherwise act in concert, even a slight downturn that forces them to conserve cash could turn into a serious, sudden, and long-lasting decline in infrastructure spending. This would leave companies like Nvidia and its emerging competitors—who must all make long-term commitments to suppliers and for capacity expansion—unable to lower costs to match the new, smaller market size. Companies priced for an s-curve are overpriced if there’s a peak and decline.



        
            
            

            
                            Source: [19]
                    

    



All of which means that investors shouldn’t swim upstream, but fish downstream: companies whose products rely on achieving high-quality results from somewhat ambiguous information will see increased productivity and higher profits. These sectors include professional services, healthcare, education, financial services, and creative services, which together account for between a third and a half of global GDP and have not seen much increased productivity from automation. AI can help lower costs, but as with containerization, how individual businesses incorporate lower costs into their strategies—and what they decide to do with the savings—will determine success. To put it bluntly, using cost savings to increase profits rather than grow revenue is a loser’s game.



The companies that will benefit most rapidly are those whose strategies are already conditional on lowering costs. IKEA’s longtime strategy was to sell quality furniture for low prices and make it up on volume. After containerization made it possible for them to go worldwide, IKEA became the world’s largest retailer and Ingvar Kamprad (the IK of IKEA) became a billionaire. Similarly, Walmart, whose strategy was high volume and low prices in underserved markets, benefited from both cost savings and just-in-time supply chains, allowing increased product variety and lower inventory costs.



Today’s knowledge-work companies that already prioritize the same values are the least risky way to bet on AI, but new companies will form or re-form with a high-volume, low-cost strategy, just as Costco did in the early 1980s. New companies will compete with the incumbents, but with a clean slate and hindsight. Regardless, there are few barriers to entry, so each of these firms will face stiff competition and operate in fragmented markets. Experienced management and flawless execution will be key.



Being an entrepreneur will be a fabulous proposition in these sectors. Being an investor will be harder. Companies will not need much private capital—IKEA never needed to raise risk capital, and Costco raised only one round in 1983 before going public in 1985—because implementing cost-savings technology is not capital intensive. As with containerization, there will be a long lag between technology trigger and the best investments. The opportunities will be later.



Stock pickers will also make money, but they need to be choosy. At the high end of projections, an additional 7% in GDP growth over ten years within one third of the economy gives a tailwind of only about 2% per year to these companies—even less if productivity growth from older ICT products abates. The primary value shift will be to companies that are embracing the strategic implications of AI from companies that are not, the way Walmart benefited from Sears, which took advantage of cheaper goods prices but did not reinvent itself.



Consumers, however, will be the biggest beneficiaries. Previous waves of mechanization benefited labor productivity in manufacturing, driving prices down and saving consumers money. But increased labor productivity in manufacturing also led to higher manufacturing wages. Wages in services businesses had to rise to compete, even though these businesses did not benefit from productivity gains. This caused the price of services to rise.[20] The share of household spending on food and clothing went from 55% in 1918 to 16% in 2023,[21] but the cost of knowledge-intensive services like healthcare and education have grown well above inflation. 



Something similar will happen with AI: Knowledge-intensive services will get cheaper, allowing consumers to buy more of them, while services that require person-to-person interaction will get more expensive, taking up a greater percentage of household spending. This points to obvious opportunities in both. But the big news is that most of the new value created by AI will be captured by consumers, who should see a wider variety of knowledge-intensive goods at reasonable prices, and wider and more affordable access to services like medical care, education, and advice.



    




There is nothing better than the beginning of a new wave, when the opportunities to envision, invent, and build world-changing companies leads to money, fame, and glory. But there is nothing more dangerous for investors and entrepreneurs than wishful thinking. The lessons learned from investing in tech over the last 50 years are not the right ones to apply now. The way to invest in AI is to think through the implications of knowledge workers becoming more efficient, to imagine what markets this efficiency unlocks, and to invest in those. For decades, the way to make money was to bet on what the new thing was. Now, you have to bet on the opportunities it opens up.



    




Jerry Neumann is a retired venture investor, writing and teaching about innovation.



    

                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Myocardial infarction may be an infectious disease]]></title>
            <link>https://www.tuni.fi/en/news/myocardial-infarction-may-be-infectious-disease</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235648</guid>
            <description><![CDATA[A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional und...]]></description>
            <content:encoded><![CDATA[A pioneering study by researchers from Finland and the UK has demonstrated for the first time that myocardial infarction may be an infectious disease. This discovery challenges the conventional understanding of the pathogenesis of myocardial infarction and opens new avenues for treatment, diagnostics, and even vaccine development.According to the recently published research, an infection may trigger myocardial infarction. Using a range of advanced methodologies, the research found that, in coronary artery disease, atherosclerotic plaques containing cholesterol may harbour a gelatinous, asymptomatic biofilm formed by bacteria over years or even decades. Dormant bacteria within the biofilm remain shielded from both the patient’s immune system and antibiotics because they cannot penetrate the biofilm matrix.A viral infection or another external trigger may activate the biofilm, leading to the proliferation of bacteria and an inflammatory response. The inflammation can cause a rupture in the fibrous cap of the plaque, resulting in thrombus formation and ultimately myocardial infarction.Professor Pekka Karhunen, who led the study, notes that until now, it was assumed that events leading to coronary artery disease were only initiated by oxidised low-density lipoprotein (LDL), which the body recognises as a foreign structure.“Bacterial involvement in coronary artery disease has long been suspected, but direct and convincing evidence has been lacking. Our study demonstrated the presence of genetic material – DNA – from several oral bacteria inside atherosclerotic plaques,” Karhunen explains.The findings were validated by developing an antibody targeted at the discovered bacteria, which unexpectedly revealed biofilm structures in arterial tissue. Bacteria released from the biofilm were observed in cases of myocardial infarction. The body’s immune system had responded to these bacteria, triggering inflammation which ruptured the cholesterol-laden plaque.The observations pave the way for the development of novel diagnostic and therapeutic strategies for myocardial infarction. Furthermore, they advance the possibility of preventing coronary artery disease and myocardial infarction by vaccination.The study was conducted by Tampere and Oulu Universities, Finnish Institute for Health and Welfare and the University of Oxford. Tissue samples were obtained from individuals who had died from sudden cardiac death, as well as from patients with atherosclerosis who were undergoing surgery to cleanse carotid and peripheral arteries.The research is part of an extensive EU-funded cardiovascular research project involving 11 countries. Significant funding was also provided by the Finnish Foundation for Cardiovascular Research and Jane and Aatos Erkko Foundation. The research article Viridans Streptococcal Biofilm Evades Immune Detection and Contributes to Inflammation and Rupture of Atherosclerotic Plaques was published in the Journal of the American Heart Association on 6 August 2025. Read the article onlineFurther informationProfessor Pekka KarhunenFaculty of Medicine and Health TechnologyTampere Universitypekka.j.karhunen [at] tuni.fi (pekka[dot]j[dot]karhunen[at]tuni[dot]fi)Tel. +358 400 511361]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AMD's RDNA4 GPU Architecture at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/amds-rdna4-gpu-architecture-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45235293</guid>
            <description><![CDATA[RDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs.]]></description>
            <content:encoded><![CDATA[RDNA4 is AMD’s latest graphics-focused architecture, and fills out their RX 9000 line of discrete GPUs. AMD noted that creating a good gaming GPU requires understanding both current workloads, as well as taking into account what workloads might look like five years in the future. Thus AMD has been trying to improve efficiency across rasterization, compute, and raytracing. Machine learning has gained importance including in games, so AMD’s new GPU architecture caters to ML workloads as well.From AMD’s perspective, RDNA4 represents a large efficiency leap in raytracing and machine learning, while also improving on the rasterization front. Improved compression helps keep the graphics architecture fed. Outside of the GPU’s core graphics acceleration responsibility, RDNA4 brings improved media and display capabilities to round out the package.The Media Engine provides hardware accelerated video encode and decode for a wide range of codecs. High end RDNA4 parts like the RX 9070XT have two media engines. RDNA4’s media engines feature faster decoding speed, helping save power during video playback by racing to idle. For video encoding, AMD targeted better quality in H.265, H.265, and AV1, especially in low latency encoding.Low latency encoder modes are mostly beneficial for streaming, where delays caused by the media engine ultimately translate to a delayed stream. Reducing latency can make quality optimizations more challenging. Video codecs strive to encode differences between frames to economize storage. Buffering up more frames gives the encoder more opportunities to look for similar content across frames, and lets it allocate more bitrate budget for difficult sequences. But buffering up frames introduces latency. Another challenge is some popular streaming platforms mainly use H.264, an older codec that’s less efficient than AV1. Newer codecs are being tested, so the situation may start to change as the next few decades fly by. But for now, H.264 remains important due to its wide support.Testing with an old gameplay clip from Elder Scrolls Online shows a clear advantage for RDNA4’s media engine when testing with the latency-constrained VBR mode and encoder tuned for low latency encoding (-usage lowlatency -rc vbr_latency). Netflix’s VMAF video quality metric gives higher scores for RDNA4 throughout the bitrate range. Closer inspection generally agrees with the VMAF metric.RDNA4 does a better job preserving high contrast outlines. Differences are especially visible around text, which RDNA4 handles better than its predecessor while using a lower bitrate. Neither result looks great with such a close look, with blurred text on both examples and fine detail crushed in video encoding artifacts. But it’s worth remembering that the latency-constrained VBR mode uses a VBV buffer of up to three frames, while higher latency modes can use VBV buffer sizes covering multiple seconds of video. Encoding speed has improved slightly as well, jumping from ~190 to ~200 FPS from RDNA3.5 to RDNA4.The display engine fetches on-screen frame data from memory, composites it into a final image, and drives it to the display outputs. It’s a basic task that most people take for granted, but the display engine is also a good place to perform various image enhancements. A traditional example is using a lookup table to apply color correction. Enhancements at the display engine are invisible to user software, and are typically carried out in hardware with minimal power cost. On RDNA4, AMD added a “Radeon Image Sharpening” filter, letting the display engine sharpen the final image. Using dedicated hardware at the display engine instead of the GPU’s programmable shaders means that the sharpening filter won’t impact performance and can be carried out with better power efficiency. And, AMD doesn’t need to rely on game developers to implement the effect. Sharpening can even apply to the desktop, though I’m not sure why anyone would want that.Power consumption is another important optimization area for display engines. Traditionally that’s been more of a concern for mobile products, where maximizing battery life under low load is a top priority. But RDNA4 has taken aim at multi-monitor idle power with its newer display engine. AMD’s presentation stated that they took advantage of variable refresh rates on FreeSync displays. They didn’t go into more detail, but it’s easy to imagine what AMD might be doing. High resolution and high refresh rate displays translate to high pixel rates. That in turn drives higher memory bandwidth demands. Dynamically lowering refresh rates could let RDNA4’s memory subsystem enter a low power state while still meeting refresh deadlines.Power and GDDR6 data rates for various refresh rate combinations. AMD’s monitoring software (and others) read out extremely low memory clocks when the memory bus is able to idle, so those readings aren’t listed.I have a RX 9070 hooked up to a Viotek GN24CW 1080P display via HDMI, and a MSI MAG271QX 1440P capable of refresh rates up to 360 Hz. The latter is connected via DisplayPort. The RX 9070 manages to keep memory at idle clocks even at high refresh rate settings. Moving the mouse causes the card to ramp up memory clocks and consume more power, hinting that RDNA4 is lowering refresh rates when screen contents don’t change. Additionally, RDNA4 gets an intermediate GDDR6 power state that lets it handle the 1080P 60 Hz + 1440P 240 Hz combination without going to maximum memory clocks. On RDNA2, it’s more of an all or nothing situation. The older card is more prone to ramping up memory clocks to handle high pixel rates, and power consumption remains high even when screen contents don’t change.RDNA4’s Workgroup Processor retains the same high level layout as prior RDNA generations. However, it gets major improvements targeted towards raytracing, like improved raytracing units and wider BVH nodes, a dynamic register allocation mode, and a scheduler that no longer suffers false memory dependencies between waves. I covered those in previous articles. Besides those improvements, AMD’s presentation went over a couple other details worth discussing.AMD has a long history of using a scalar unit to offload operations that are constant across a wave. Scalar offload saves power by avoiding redundant computation, and frees up the vector unit to increase performance in compute-bound sequences. RDNA4’s scalar unit gains a few floating point instructions, expanding scalar offload opportunities. This capability debuted on RDNA3.5, but RDNA4 brings it to discrete GPUs.While not discussed in AMD’s presentation, scalar offload can bring additional performance benefits because scalar instructions sometimes have lower latency than their vector counterparts. Most basic vector instructions on RDNA4 have 5 cycle latency. FP32 adds and multiples on the scalar unit have 4 cycle latency. The biggest latency benefits still come from offloading integer operations though.GPUs use barriers to synchronize threads and enforce memory ordering. For example, a s_barrier instruction on older AMD GPUs would cause a thread to wait until all of its peers in the workgroup also reached the s_barrier instruction. Barriers degrade performance because any thread that happened to reach the barrier faster would have to stall until its peers catch up.RDNA4 splits the barrier into separate “signal” and “wait” actions. Instead of s_barrier, RDNA4 has s_barrier_signal and s_barrier_wait. A thread can “signal” the barrier once it produces data that other threads might need. It can then do independent work, and only wait on the barrier once it needs to use data produced by other threads. The s_barrier_wait will then stall the thread until all other threads in the workgroup have signalled the barrier.The largest RDNA4 variants have a 8 MB L2 cache, representing a substantial L2 capacity increase compared to prior RDNA generations. RDNA3 and RDNA2 maxed out at 6 MB and 4 MB L2 capacities, respectively. AMD found that difficult workloads like raytracing benefit from the larger L2. Raytracing involves pointer chasing during BVH traversal, and it’s not surprising that it’s more sensitive to accesses getting serviced from the slower Infinity Cache as opposed to L2. In the initial scene in 3DMark’s DXR feature test, run in Explorer Mode, RDNA4 dramatically cuts down the amount of data that has to be fetched from beyond L2.RDNA2 still does a good job of keeping data in L2 in absolute terms. But it’s worth noting that hitting Infinity Cache on both platforms adds more than 50 ns of extra latency over a L2 hit. That’s well north of 100 cycles because both RDNA2 and RDNA4 run above 2 GHz. While AMD’s graphics strategy has shifted towards making the faster caches bigger, it still contrasts with Nvidia’s strategy of putting way more eggs in the L2 basket. Blackwell’s L2 cache serves the functions of both AMD’s L2 and Infinity Cache, and has latency between those two cache levels. Nvidia also has a flexible L1/shared memory allocation scheme that can give them more low latency caching capacity in front of L2, depending on a workload’s requested local storage (shared memory) capacity.A mid-level L1 cache was a familiar fixture on prior RDNA generations. It’s conspicuously missing from RDNA4, as well as AMD’s presentation. One possibility is that L1 cache hitrate wasn’t high enough to justify the complexity of an extra cache level. Perhaps AMD felt its area and transistor budget was better allocated towards increasing L2 capacity. To support this theory, L1 hitrate on RDNA1 was often below 50%. At the same time, the RDNA series always enjoyed a high bandwidth and low latency L2. Putting more pressure on L2 in exchange for reducing L2 misses may have been an enticing tradeoff. Another possibility is that AMD ran into validation issues with the L1 cache and decided to skip it for this generation. There’s no way to verify either possibility of course, but I think the former reasons make more sense.Beyond tweaking the cache hierarchy, RDNA4 brings improvements to transparent compression. AMD emphasized that they’re using compression throughout the SoC, including at points like the display engine and media engine. Compressed data can be stored in caches, and decompressed before being written back to memory. Compression cuts down on data transfer, which reduces bandwidth requirements and improves power efficiency.Transparent compression is not a new feature. It has a long history of being one tool in the GPU toolbox for reducing memory bandwidth usage, and it would be difficult to find any modern GPU without compression features of some sort. Even compression in other blocks like the display engine have precedent. Intel’s display engines for example use Framebuffer Compression (FBC), which can write a compressed copy of frame data and keep fetching the compressed copy to reduce data transfer power usage as long as the data doesn’t change. Prior RDNA generations had compression features too, and AMD’sdocumentation summarizes some compression targets. While AMD didn’t talk about compression efficiency, I tried to take similar frame captures using RGP on both RDNA1 and RDNA4 to see if there’s a large difference in memory access per frame. It didn’t quite work out the way I expected, but I’ll put them here anyway and discuss why evaluating compression efficacy is challenging.The first challenge is that both architectures satisfy most memory requests from L0 or L1. AMD slides on RDNA1 suggest the L0 and L1 only hold decompressed data, at least for delta color compression. Compression does apply to L2. For RDNA4, AMD’s slides indicate it applies to the Infinity Cache too. However, focusing on data transfer to and from the L2 wouldn’t work due the large cache hierarchy differences between those RDNA generations.DCC, or delta color compression, is not the only form of compression. But this slide shows one example of compression/decompression happening in front of L2Another issue is, it’s easy to imagine a compression scheme that doesn’t change the number of cache requests involved. For example, data might be compressed to only take up part of a cacheline. A request only causes a subset of the cacheline to be read out, which a decompressor module expands to the full 128B. Older RDNA1 slides are ambiguous about this, indicating that DCC operates on 256B granularity (two cachelines) without providing further details.In any case, compression may be a contributing factor in RDNA4 being able to achieve better performance while using a smaller Infinity Cache than prior generations, despite only having a 256-bit GDDR6 DRAM setup.AMD went over RAS, or reliability, availability, and serviceability features in RDNA4. Modern chips use parity and ECC to detect errors and correct them, and evidently RDNA4 does the same. Unrecoverable errors are handled with driver intervention, by “re-initializing the relevant portion of the SoC, thus preventing the platform from shutting down”. There’s two ways to interpret that statement. One is that the GPU can be re-initialized to recover from hardware errors, obviously affecting any software relying on GPU acceleration. Another is that some parts of the GPU can be re-initialized while the GPU continues handling work. I think the former is more likely, though I can imagine the latter being possible in limited forms too. For example, an unrecoverable error reading from GDDR6 can hypothetically be fixed if that data is backed by a duplicate in system memory. The driver could transfer known-good data from the host to replace the corrupted copy. But errors with modified data would be difficult to recover from, because there might not be an up-to-date copy elsewhere in the system.On the security front, microprocessors get private buses to “critical blocks” and protected register access mechanisms. Security here targets HDCP and other DRM features, which I don’t find particularly amusing. But terminology shown on the slide is interesting, because MP0 and MP1 are also covered in AMD’s CPU-side documentation. On the CPU side, MP0 (microprocessor 0) handles some Secure Encrypted Virtualization (SEV) features. It’s sometimes called the Platform Security Processor (PSP) too. MP1 on CPUs is called the System Management Unit (SMU), which covers power control functions. Curiously AMD’s slide labels MP1 and the SMU separately on RDNA4. MP0/MP1 could have completely different functions on GPUs of course. But the common terminology raises the possibility that there’s a lot of shared work between CPU and GPU SoC design. RAS is also a very traditional CPU feature, though GPUs have picked up RAS features over time as GPU compute picked up steam.One of the most obvious examples of shared effort between the CPU and GPU sides is Infinity Fabric making its way to graphics designs. This started years ago with Vega, though back then using Infinity Fabric was more of an implementation detail. But years later, Infinity Fabric components provided an elegant way to implement a large last level cache, or multi-socket coherent systems with gigantic iGPUs (like MI300A).Slide from Hot Chips 29, covering Infinity Fabric used in AMD’s older Vega GPUThe Infinity Fabric memory-side subsystem on RDNA4 consists of 16 CS (Coherent Station) blocks, each paired with a Unified Memory Controller (UMC). Coherent Stations receive requests coming off the graphics L2 and other clients. They ensure coherent memory access by either getting data from a UMC, or by sending a probe if another block has a more up-to-date copy of the requested cacheline. The CS is a logical place to implement a memory side cache, and each CS instance has 4 MB of cache in RDNA4.To save power, Infinity Fabric supports DVFS (dynamic voltage and frequency scaling) to save power, and clocks between 1.5 and 2.5 GHz. Infinity Fabric bandwidth is 1024 bits per clock, which suggests the Infinity Cache can provide 2.5 TB/s of theoretical bandwidth. That roughly lines up with results from Nemes’s Vulkan-based GPU cache and memory bandwidth microbenchmark.AMD also went over their ability to disable various SoC components to harvest dies and create different SKUs. Shader Engines, WGPs, and memory controller channels can be disabled. AMD and other manufacturers have used similar harvesting capabilities in the past. I’m not sure what’s new here. Likely, AMD wants to re-emphasize their harvesting options.Finally, AMD mentioned that they chose a monolithic design for RDNA4 because it made sense for a graphics engine of its size. They looked at performance goals, package assembly and turnaround time, and cost. After evaluating those factors, they decided a monolithic design was the right option. It’s not a surprise. After all, AMD used monolithic designs for lower end RDNA3 products with smaller graphics engines, and only used chiplets for the largest SKUs. Rather, it’s a reminder that there’s no one size fits all solution. Whether a monolithic or chiplet-based design makes more sense depends heavily on design goals.RDNA4 brings a lot of exciting improvements to the table, while breaking away from any attempt to tackle the top end performance segment. Rather than going for maximum performance, RDNA4 looks optimized to improve efficiency over prior generations. The RX 9070 offers similar performance to the RX 7900XT in rasterization workloads despite having a lower power budget, less memory bandwidth, and a smaller last level cache. Techspot also shows the RX 9070 leading with raytracing workloads, which aligns with AMD's goal of enhancing raytracing performance.Slide from RDNA4’s Launch Presentation not Hot Chips 2025AMD achieves this efficiency using compression, better raytracing structures, and a larger L2 cache. As a result, RDNA4 can pack its performance into a relatively small 356.5 mm² die and use a modest 256-bit GDDR6 memory setup. Display and media engine improvements are welcome too. Multi-monitor idle power feels like a neglected area for discrete GPUs, even though I know many people use multiple monitors for productivity. Lowering idle power in those setups is much appreciated. On the media engine side, AMD’s video encoding capabilities have often lagged behind the competition. RDNA4’s progress at least prevents AMD from falling as far behind as they have before.If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[An open-source maintainer's guide to saying “no”]]></title>
            <link>https://www.jlowin.dev/blog/oss-maintainers-guide-to-saying-no</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45234593</guid>
            <description><![CDATA[Stewardship in the age of cheap code]]></description>
            <content:encoded><![CDATA[  One of the hardest parts of maintaining an open-source project is saying “no” to a good idea. A user proposes a new feature. It’s well-designed, useful, and has no obvious technical flaws. And yet, the answer is “no.” To the user, this can be baffling. To the maintainer, it’s a necessary act of stewardship.
Having created and maintained two highly successful open-source projects, Prefect and FastMCP, helped establish a third in Apache Airflow, and cut my OSS teeth contributing to Theano, I’ve learned that this stewardship is the real work. The ultimate success of a project isn’t measured by the number of features it has, but by the coherence of its vision and whether it finds resonance with its users. As Prefect’s CTO Chris White likes to point out:

“People choose software when its abstractions agree with their mental model.”

Your job as an open-source maintainer is to first establish that mental model, then relentlessly build software that reflects it. A feature that is nominally useful but not spiritually aligned can be a threat just as much as an enhancement.
This threat can take many forms. The most obvious is a feature that’s wildly out of scope, like a request to add a GUI to a CLI tool — a valid idea that likely belongs in a separate project. More delicate is the feature that brilliantly solves one user’s niche problem but adds complexity and maintenance burden for everyone else. The most subtle, and perhaps most corrosive, is the API that’s simply “spelled” wrong for the project: the one that breaks established patterns and creates cognitive dissonance for future users. In many of the projects I’ve been fortunate to work on, both open- and closed-source, we obsess over this because a consistent developer experience is the foundation of a framework that feels intuitive and trustworthy.
So how does a maintainer defend this soul, especially as a project scales? It starts with documenting not just how the project works, but why. Clear developer guides and statements of purpose are your first line of defense. They articulate the project’s philosophy, setting expectations before a single line of code is written. This creates a powerful flywheel: the clearer a project is about why it exists, the more it attracts contributors who share that vision. Their contributions reinforce and refine that vision, which in turn justifies the project’s worldview. Process then becomes a tool for alignment, not bureaucracy. As a maintainer, you can play defense on the repo, confident that the burden of proof is on the pull request to demonstrate not just its own value, but its alignment with a well-understood philosophy.
This work has gotten exponentially harder in the age of LLMs. Historically, we could assume that since writing code is an expensive, high-effort activity, contributors would engage in discussion before doing the work, or at least seek some sign that time would not be wasted. Today, LLMs have inverted this. Code is now cheap, and we see it offered in lieu of discourse. A user shows up with a fully formed PR for a feature we’ve never discussed. It’s well-written, it “works,” but it was generated without any context for the framework’s philosophy. Its objective function was to satisfy a user’s request, not to uphold the project’s vision.
This isn’t to say all unsolicited contributions are unwelcome. There is nothing more delightful than the drive-by PR that lands, fully formed and perfectly aligned, fixing a bug or adding a small, thoughtful feature. We can’t discourage these contributors. But in the last year, the balance of presumption has shifted. The signal-to-noise ratio has degraded, and the unsolicited PR is now more likely to be a high-effort review of a low-effort contribution.
So what’s the playbook? In FastMCP, we recently tried to nudge this behavior by requiring an issue for every PR. In a perfect example of unintended consequences, we now get single-sentence issues opened a second before the PR… which is actually worse. More powerful than this procedural requirement is sharing a simple sentence that we are unconvinced that the framework should take on certain responsibilities for users. If a contributor wants to convince us, we all only benefit from that effort! But as I wrote earlier, the burden of proof is on the contributor, never the repo.
A more nuanced pushback against viable code is that as a maintainer, you may be uncomfortable or unwilling to maintain it indefinitely. I think this is often forgotten in fast-moving open-source projects: there is a significant transfer of responsibility when a PR is merged. If it introduces bugs, confusion, inconsistencies, or even invites further enhancements, it is usually the maintainer who is suddenly on the hook for it. In FastMCP, we’ve introduced and documented the contrib module as one solution to this problem. This module contains useful functionality that may nonetheless not be appropriate for the core project, and is maintained exclusively by its author. No guarantee is made that it works with future versions of the project. In practice, many contrib modules might have better lives as standalone projects, but it’s a way to get the ball rolling in a more communal fashion.
One regret I have is that I observe a shift in my own behavior. In the early days of Prefect, we did our best to maintain a 15-minute SLA on our responses. Seven years ago, a user question reflected an amazing degree of engagement, and we wanted to respond in kind. Today, if I don’t see a basic attempt to engage, I find myself mirroring that low-effort behavior. Frankly, if I’m faced with a choice between a wall of LLM-generated text or a clear, direct question with an MRE, I’ll take the latter every time.
I know this describes a fundamentally artisanal, hand-made approach to open source that may seem strange in an age of vibe coding and YOLO commits. I’m no stranger to LLMs. Quite the opposite. I use them constantly in my own work and we even have an AI agent (hi Marvin!) that helps triage the FastMCP repo. But in my career, this thoughtful, deliberate stewardship has been the difference between utility projects and great ones. We used to call it “community” and I’d like to ensure it doesn’t disappear.
I think I need to be clear that nothing in this post should be construed as an invitation to be rude or to stonewall users. As an open-source maintainer, you should be ecstatic every time someone engages with your project. After all, if you didn’t want those interactions, you could have kept your code to yourself! The goal in scalable open-source must always be to create a positive, compounding community, subject to whatever invitation you choose to extend to your users. Your responsibility is to ensure that today’s “no” helps guide a contributor toward tomorrow’s enthusiastic “yes!”
When this degree of thoughtfulness is well applied, it translates into a better experience for all users—into software whose abstractions comply with a universal mental model. It’s a reminder that this kind of stewardship is worth fighting for.
Two weeks ago, I was in a room that reminded me this fight is being won at the highest level. I had the opportunity to join the MCP Committee for meetings in New York and saw a group skillfully navigating a version of this very problem. MCP is a young protocol, and its place in the AI stack has been accelerated more by excitement than its own maturity. As a result, it is under constant assault that it should simultaneously do more, do less, and everything in between.
A weak or rubber-stamp committee would be absolutely overwhelmed by this pressure, green-lighting any plausible feature to appease the loudest voices in this most-hyped corner of tech. And yet, over a couple of days, what I witnessed was the opposite. The most important thing I saw was a willingness to debate, and to hold every proposal up to a (usually) shared opinion of what the protocol is supposed to be. There was an overriding reverence for MCP’s teleological purpose: what it should do and, more critically, what it should not do. I especially admired David’s consistent drumbeat as he led the committee: “That’s a good idea. But is it part of the protocol’s responsibilities?”
Sticking to your guns like that is the hard, necessary work of maturing a technology with philosophical rigor. I left New York more confident than ever in the team and MCP itself, precisely because of how everyone worked not only to build the protocol, but to act as its thoughtful custodians. It was wonderful to see that stewardship up close, and I look forward to seeing it continue in open-source more broadly.   Subscribe     ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Safe C++ proposal is not being continued]]></title>
            <link>https://sibellavia.lol/posts/2025/09/safe-c-proposal-is-not-being-continued/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45234460</guid>
            <description><![CDATA[One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, …]]></description>
            <content:encoded><![CDATA[One year ago, the Safe C++ proposal was made. The goal was to add a safe subset/context into C++ that would give strong guarantees (memory safety, type safety, thread safety) similar to what Rust provides, without breaking existing C++ code. It was an extension or superset of C++. The opt-in mechanism was to explicitly mark parts of the code that belong to the safe context. The authors even state:Code in the safe context exhibits the same strong safety guarantees as code written in Rust.The rest remains “unsafe” in the usual C++ sense. This means that existing code continues to work, while new or refactored parts can gain safety. For those who write Rust, Safe C++ has many similarities with Rust, sometimes with adjustments to fit C++’s design. Also, because C++ already has a huge base of “unsafe code”, Safe C++ has to provide mechanisms for mixing safe and unsafe, and for incremental migration. In that sense, all of Safe C++’s safe features are opt-in. Existing code compiles and works as before. Introducing safe context doesn’t break code that doesn’t use it.The proposal caught my interest. It seemed like a good compromise to make C++ safe, although there were open or unresolved issues, which is completely normal for a draft proposal. For example, how error reporting for the borrow checker and lifetime errors would work, or how generic code and templates would interact with lifetime logic and safe/unsafe qualifiers. These are just some of the points, the proposal is very long and elaborate. Moreover, I am not a programming language designer, so there might be better alternatives.Anyway, today I discovered that the proposal will no longer be pursued. When I thought about the proposal again this morning, I realized I hadn’t read any updates on it for some time. So I searched and found some answers on Reddit.The response from Sean Baxter, one of the original authors of the Safe C++ proposal:The Safety and Security working group voted to prioririze Profiles over Safe C++. Ask the Profiles people for an update. Safe C++ is not being continued.And again:The Rust safety model is unpopular with the committee. Further work on my end won’t change that. Profiles won the argument. All effort should go into getting Profile’s language for eliminating use-after-free bugs, data races, deadlocks and resource leaks into the Standard, so that developers can benefit from it.So I went to read the documents related to Profiles[1][2][3][4]. I try to summarize what I understood: they are meant to define modes of C++ that impose constraints on how you use the language and library, in order to guarantee certain safety properties. They are primarily compile-time constraints, though in practice some checks may be implemented using library facilities that add limited runtime overhead. Instead of introducing entirely new language constructs, profiles mostly restrict existing features and usages. The idea is that you can enable a profile, and any code using it agrees to follow the restrictions. If you don’t enable it, things work as before. So it’s backwards-compatible.Profiles seem less radical and more adoptable, a safer-by-default C++ without forcing the Rust model that aims to tackle the most common C++ pitfalls. I think Safe C++ was more ambitious: introducing new syntax, type qualifiers, safe vs unsafe contexts, etc. Some in the committee felt that was too heavy, and Profiles are seen as a more pragmatic path. The main objection is obvious: one could say that Profiles restrict less than what Safe C++ aimed to provide.Reading comments here and there, there is visible resistance in the community toward adopting the Rust model, and from a certain point of view, I understand it. If you want to write like Rust, just write Rust. Historically, C++ is a language that has often taken features from other worlds and integrated them into itself. In this case, I think that safety subsets of C++ already exist informally somehow. Profiles are an attempt to standardize and unify something that already exists in practice. Technically, they don’t add new fundamental semantics. Instead, they provide constraints, obligations and guarantees.In my opinion, considering the preferences of the committee and the entire C++ community, although I appreciated the Safe C++ proposal and was looking forward to seeing concrete results, considering the C++ context I believe that standardizing and integrating the Profiles as proposed is a much more realistic approach. Profiles might not be perfect, but they are better than nothing. They will likely be uneven in enforcement and weaker than Safe C++ in principle. They won’t give us silver-bullet guarantees, but they are a realistic path forward.[1] Core safety profiles for C++26[2] C++ Profiles: The Framework[3] What are profiles?[4] Note to the C++ standards committee members]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The case against social media is stronger than you think]]></title>
            <link>https://arachnemag.substack.com/p/the-case-against-social-media-is</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45234323</guid>
        </item>
        <item>
            <title><![CDATA[RIP pthread_cancel]]></title>
            <link>https://eissing.org/icing/posts/rip_pthread_cancel/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233713</guid>
            <description><![CDATA[I posted about adding pthread_cancel use in curl about three weeks ago, we released this in curl 8.16.0 and it blew up right in our faces. Now, with #18540 we are ripping it out again. What happened?
short recap pthreads define “Cancelation points”, a list of POSIX functions where a pthread may be cancelled. In addition, there is also a list of functions that may be cancelation points, among those getaddrinfo().
getaddrinfo() is exactly what we are interested in for libcurl. It blocks until it has resolved a name. That may hang for a long time and libcurl is unable to do anything else. Meh. So, we start a pthread and let that call getaddrinfo(). libcurl can do other things while that thread runs.]]></description>
            <content:encoded><![CDATA[I posted about adding pthread_cancel use in curl
about three weeks ago, we released this in curl 8.16.0 and it blew
up right in our faces. Now, with
#18540 we are ripping it
out again. What happened?
short recap
pthreads
define “Cancelation points”, a list of POSIX functions where
a pthread may be cancelled. In addition, there is also a list of functions
that may be cancelation points, among those getaddrinfo().
getaddrinfo() is exactly what we are interested in for libcurl. It blocks
until it has resolved a name. That may hang for a long time and libcurl
is unable to do anything else. Meh. So, we start a pthread and let that
call getaddrinfo(). libcurl can do other things while that thread runs.
But eventually, we have to get rid of the pthread again. Which means we
either have to pthread_join() it - which means a blocking wait. Or we
call pthread_detach() - which returns immediately but the thread keeps
on running. Both are bad when you want to do many, many transfers. Either we block and
stall or we let pthreads pile up in an uncontrolled way.
So, we added pthread_cancel() to interrupt a running getaddrinfo()
and get rid of the pthread we no longer needed. So the theory. And, after
some hair pulling, we got this working.
cancel yes, leakage also yes!
After releasing curl 8.16.0 we got an issue reported in
#18532 that cancelled
pthreads leaked memory.

Digging into the glibc source
shows that there is this thing called
/etc/gai.conf
which defines how getaddrinfo() should sort returned answers.
The implementation in glibc first resolves the name to addresses. For these,
it needs to allocate memory. Then it needs to sort them if there is more
than one address. And in order
to do that it needs to read /etc/gai.conf. And in order to do that
it calls fopen() on the file. And that may be a pthread “Cancelation Point”
(and if not, it surely calls open() which is a required cancelation point).
So, the pthread may get cancelled when reading /etc/gai.conf and leak all
the allocated responses. And if it gets cancelled there, it will try to
read /etc/gai.conf again the next time it has more than one address
resolved.
At this point, I decided that we need to give up on the whole pthread_cancel()
strategy. The reading of /etc/gai.conf is one point where a cancelled
getaddrinfo() may leak. There might be others. Clearly, glibc is not really
designed to prevent leaks here (admittedly, this is not trivial).
RIP
Leaking memory potentially on something libcurl does over and over again is
not acceptable. We’d rather pay the price of having to eventually wait on
a long running getaddrinfo().
Applications using libcurl can avoid this by using c-ares which resolves
unblocking and without the use of threads. But that will not be able to do
everything that glibc does.
DNS continues to be tricky to use well.

  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Magical systems thinking]]></title>
            <link>https://worksinprogress.co/issue/magical-systems-thinking/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233266</guid>
            <description><![CDATA[Systems thinking promises to give us a toolkit to design complex systems that work from the ground up. It fails because it ignores that systems fight back.]]></description>
            <content:encoded><![CDATA[The systems that enable modern life share a common origin. The water supply, the internet, the international supply chains bringing us cheap goods: each began life as a simple, working system. The first electric grid was no more than a handful of electric lamps hooked up to a water wheel in Godalming, England, in 1881. It then took successive decades of tinkering and iteration by thousands of very smart people to scale these systems to the advanced state we enjoy today. At no point did a single genius map out the final, finished product.



But this lineage of (mostly) working systems is easily forgotten. Instead, we prefer a more flattering story: that complex systems are deliberate creations, the product of careful analysis. And, relatedly, that by performing this analysis – now known as ‘systems thinking’ in the halls of government – we can bring unruly ones to heel. It is an optimistic perspective, casting us as the masters of our systems and our destiny.



The empirical record says otherwise, however. Our recent history is one of governments grappling with complex systems and coming off worse. In the United States, HealthCare.gov was designed to simplify access to health insurance by knitting together 36 state marketplaces and data from eight federal agencies. Its launch was paralyzed by technical failures that locked out millions of users. Australia’s disability reforms, carefully planned for over a decade and expected to save money, led to costs escalating so rapidly that they will soon exceed the pension budget. The UK’s 2014 introduction of Contracts for Difference, intended to speed the renewables rollout by giving generators a guaranteed price, overstrained the grid and is a major contributor to the 15-year queue for new connections. Systems thinking is more popular than ever; modern systems thinkers have analytical tools that their predecessors could only have dreamt of. But the systems keep kicking back.



There is a better way. A long but neglected line of thinkers going back to chemists in the nineteenth century has argued that complex systems are not our passive playthings. Despite friendly names like ‘the health system’, they demand extreme wariness. If broken, a complex system often cannot be fixed. Meanwhile, our successes, when they do come, are invariably the result of starting small. As the systems we have built slip further beyond our collective control, it is these simple working systems that offer us the best path back. 



The world model



In 1970, the ‘Club of Rome’, a group of international luminaries with an interest in how the problems of the world were interrelated, invited Jay Wright Forrester to peer into the future of the global economy. An MIT expert on electrical and mechanical engineering, Forrester had cut his teeth on problems like how to keep a Second World War aircraft carrier’s radar pointed steadily at the horizon amid the heavy swell of the Pacific. 



The Club of Rome asked an even more intricate question: how would social and economic forces interact in the coming decades? Where were the bottlenecks and feedback mechanisms? Could economic growth continue, or would the world enter a new phase of equilibrium or decline? 



Forrester labored hard, producing a mathematical model of enormous sophistication. Across 130 pages of mathematical equations, computer graphical printout, and DYNAMO code,World Dynamics tracks the myriad relationships between natural resources, capital, population, food, and pollution: everything from the ‘capital-investment-in-agriculture-fraction adjustment time’ to the ominous ‘death-rate-from-pollution multiplier’.




          
            
              
                A section of Forrester’s World Model.
              
              
                Image
                
                  WAguirre 2017
                
              
            
          
        


World leaders had assumed that economic growth was an unalloyed good. But Forrester’s results showed the opposite. As financial and population growth continued, natural resources would be consumed at an accelerating rate, agricultural land would be paved over, and pollution would reach unmanageable levels. His model laid out dozens of scenarios and in most of them, by 2025, the world would already be in the first throes of an irreversible decline in living standards. By 2070, the crunch would be so painful that industrialized nations might regret their experiment with economic growth altogether. As Forrester put it, ‘[t]he present underdeveloped countries may be in a better condition for surviving forthcoming worldwide environmental and economic pressures than are the advanced countries.’



But, as we now know, the results were also wrong. Adjusting for inflation, world GDP is now about five times higher than it was in 1970 and continues to rise. More than 90 percent of that growth has come from Asia, Europe, and North America, but forest cover across those regions has increased, up 2.6 percent since 1990 to over 2.3 billion hectares in 2020. The death rate from air pollution has almost halved in the same period, from 185 per 100,000 in 1990 to 100 in 2021. According to the model, none of this should have been possible. 



What happened? The blame cannot lie with Forrester’s competence: it’s hard to imagine a better systems pedigree than his. To read his prose today is to recognize a brilliant, thoughtful mind. Moreover, the system dynamics approach Forrester pioneered had already shown promise beyond the mechanical and electrical systems that were its original inspiration. 



In 1956, the management of a General Electric refrigerator factory in Kentucky had called on Forrester’s help. They were struggling with a boom-and-bust cycle: acute shortages became gluts that left warehouses overflowing with unsold fridges. The factory based its production decisions on orders from the warehouse, which in turn got orders from distributors, who heard from retailers, who dealt with customers. Each step introduced noise and delay. Ripples in demand would be amplified into huge swings in production further up the supply chain. 



Looking at the system as a whole, Forrester recognized the same feedback loops and instability that could bedevil a ship’s radar. He developed new decision rules, such as smoothing production based on longer-term sales data rather than immediate orders, and found ways to speed up the flow of information between retailers, distributors, and the factory. These changes dampened the oscillations caused by the system’s own structure, checking its worst excesses. 



The Kentucky factory story showed Forrester’s skill as a systems analyst. Back at MIT, Forrester immortalized his lessons as a learning exercise (albeit with beer instead of refrigerators). In the ‘Beer Game’, now a rite of passage for students at the MIT Sloan School of Management, players take one of four different roles in the beer supply chain: retailer, wholesaler, distributor, and brewer. Each player sits at a separate table and can communicate only through order forms. As their inventory runs low, they place orders with the supplier next upstream. Orders take time to process, and shipments to arrive, and each player can see only their small part of the chain.



The objective of the Beer Game is to minimize costs by managing inventory effectively. But, as the GE factory managers had originally found, this is not so easy. Gluts and shortages arise mysteriously, without obvious logic, and small perturbations in demand get amplified up the chain by as much as 800 percent (‘the bullwhip effect’). On average, players’ total costs end up being ten times higher than the optimal solution. 



With the failure of his World Model, Forrester had fallen into the same trap as his MIT students. Systems analysis works best under specific conditions: when the system is static; when you can dismantle and examine it closely; when it involves few moving parts rather than many; and when you can iterate fixes through multiple attempts. A faulty ship’s radar or a simple electronic circuit are ideal. Even a limited human element – with people’s capacity to pursue their own plans, resist change, form political blocs, and generally frustrate best-laid plans – makes things much harder. The four-part refrigerator supply chain, with the factory, warehouse, distributor and retailer all under the tight control of management, is about the upper limit of what can be understood. Beyond that, in the realm of societies, governments and economies, systems thinking becomes a liability, more likely to breed false confidence than real understanding. For these systems we need a different approach.



Le Chatelier’s Principle



In 1884, in a laboratory at the École des Mines in Paris, Henri Louis Le Chatelier noticed something peculiar: chemical reactions seemed to resist changes imposed upon them. Le Chatelier found that if, say, you have an experiment where two molecules combine in a heat-generating exothermic reaction (in his case, it was two reddish-brown nitrogen dioxide molecules combining into colorless dinitrogen tetroxide and giving off heat in the process), then you can speed things up by cooling the reactants. To ‘resist’ the drop in temperature, the system restores its equilibrium by creating more of the products that release heat. 



Le Chatelier’s Principle, the idea that the system always kicks back, proved to be a very general and powerful way to think about chemistry. It was instrumental in the discovery of the Haber-Bosch process for creating ammonia that revolutionized agriculture. Nobel Laureate Linus Pauling hoped that, even after his students had ‘forgotten all the mathematical equations relating to chemical equilibrium’, Le Chatelier’s Principle would be the one thing they remembered. And its usefulness went beyond chemistry. A century after Le Chatelier’s meticulous lab work, another student of systems would apply the principle to the complex human systems that had stymied Forrester and his subsequent followers in government.



John Gall was a pediatrician with a long-standing practice in Ann Arbor, Michigan. Of the same generation as Forrester, Gall came at things from a different direction. Whereas Forrester’s background was in mechanical and electrical systems, which worked well and solved new problems, Gall was immersed in the human systems of health, education, and government. These systems often did not work well. How was it, Gall wondered, that they seemed to coexist happily with the problems – crime, poverty, ill health – they were supposed to stamp out? 



Le Chatelier’s Principle provided an answer: systems should not be thought of as benign entities that will faithfully carry out their creators’ intentions. Rather, over time, they come to oppose their own proper functioning. Gall elaborated on this idea in his 1975 book Systemantics, named for the universal tendency of systems to display antics. A brief, weird, funny book, Systemantics (The Systems Bible in later editions) is arguably the best field guide to contemporary systems dysfunction. It consists of a series of pithy aphorisms, which the reader is invited to apply to explain the system failures (‘horrible examples’) they witness every day.



These aphorisms are provocatively stated, but they have considerable explanatory power. For example, an Australian politician frustrated at the new headaches created by ‘fixes’ to the old disability system might be reminded that ‘NEW SYSTEMS CREATE NEW PROBLEMS’. An American confused at how there can now be 190,000 pages in the US Code of Federal Regulations, up from 10,000 in 1950, might note that this is the nature of the beast: ‘SYSTEMS TEND TO GROW, AND AS THEY GROW THEY ENCROACH’. During the French Revolution, in 1793 and 1794, the ‘Committee of Public Safety’ guillotined thousands of people, an early example of the enduring principles that ‘THE SYSTEM DOES NOT DO WHAT IT SAYS IT IS DOING’ and that ‘THE NAME IS EMPHATICALLY NOT THE THING’. And, just like student chemists, government reformers everywhere would do well to remember Le Chatelier’s Principle: ‘THE SYSTEM ALWAYS KICKS BACK’.



    
        
    




These principles encourage a healthy paranoia when it comes to complex systems. But Gall’s ‘systems-display-antics’ philosophy is not a counsel of doom. His greatest insight was a positive one, explaining how some systems do succeed in spite of the pitfalls. Known as ‘Gall’s law’, it’s worth quoting in full:



A complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over with a working simple system.



Starting with a working simple system and evolving from there is how we went from the water wheel in Godalming to the modern electric grid. It is how we went from a hunk of germanium, gold foil, and hand-soldered wires in 1947 to transistors being etched onto silicon wafers in their trillions today.



This is a dynamic we can experience on a personal as well as a historical level. A trivial but revealing example is the computer game Factorio. Released in 2012 and famously hazardous to the productivity of software engineers everywhere, Factorio invites players to construct a factory. The ultimate goal is to launch a rocket, a feat that requires the player to produce thousands of intermediate products through dozens of complicated, interlocking manufacturing processes. 



It sounds like a nightmare. An early flow chart (pictured – it has grown much more complicated since) resembles the end product of a particularly thorny systems thinking project. But players complete its daunting mission successfully, without reference to such system maps, in their thousands, and all for fun.




          
            
              
                Factorio production map.
              
              
                Image
                
              
            
          
        


The genius of the game is that it lets players begin with a simple system that works. As you learn to produce one item, another is unlocked. If you get something wrong, the factory visibly grinds to a halt while you figure out a different approach. The hours tick by, and new systems – automated mining, oil refining, locomotives – are introduced and iterated upon. Before you realize it, you have built a sprawling yet functioning system that might be more sophisticated than anything you have worked on in your entire professional career.



How to build systems that work



Government systems, however, are already established, complicated, and relied upon by millions of people every day. We cannot simply switch off the health system and ask everyone to wait a few years while we build something better. The good news is that the existence of an old, clunky system does not stop us from starting something new and simple in parallel.



In the 1950s, the US was in a desperate race against a technologically resurgent Soviet Union. The USSR took the lead in developing advanced rockets of the type that launched Sputnik into orbit and risked launching a nuclear device into Washington, DC. In 1954, the Eisenhower administration tasked General Bernard Schriever with helping the US develop its own Intercontinental Ballistic Missile (ICBM). An experienced airman and administrator, the top brass felt that Schriever’s Stanford engineering master’s degree would make him a suitable go-between for the soldiers and scientists on this incredibly technical project (its scope was larger even than the Manhattan Project, costing over $100 billion in 2025 dollars versus the latter’s $39 billion). 



The organizational setup Schriever inherited was not fit for the task. With many layers of approvals and subcommittees within subcommittees, it was a classic example of a complex yet dysfunctional system. The technological challenges posed by the ICBM were extreme: everything from rocket engines to targeting systems to the integration with nuclear warheads had to be figured out more or less from scratch. This left no room for bureaucratic delay. 



Schriever produced what many systems thinkers would recognize as a kind of systems map: a series of massive boards setting out all the different committees and governance structures and approvals and red tape. But the point of these ‘spaghetti charts’ was not to make a targeted, systems thinking intervention. Schriever didn’t pretend to be able to navigate and manipulate all this complexity. He instead recognized his own limits. With the Cold War in the balance, he could not afford to play and lose his equivalent of the Beer Game. Charts in hand, Schriever persuaded his boss that untangling the spaghetti was a losing battle: they needed to start over.



They could not change the wider laws, regulations, and institutional landscape governing national defense. But they could work around them, starting afresh with a simple system outside the existing bureaucracy. Direct vertical accountability all the way to the President and a free hand on personnel enabled the program to flourish. Over the following years, four immensely ambitious systems were built in record time. The uneasy strategic stalemate that passed for stability during the Cold War was restored, and the weapons were never used in anger.



When we look in more detail at recent public policy successes, we see that this pattern tends to hold. Operation Warp Speed in the US played a big role in getting vaccines delivered quickly. It did so by bypassing many of the usual bottlenecks. For instance, it made heavy use of ‘Other Transaction Authority agreements’ to commit $12.5 billion of federal money by March 2021, circumventing the thousands of pages of standard procurement rules. Emergency powers were deployed to accelerate the FDA review process, enabling clinical trial work and early manufacturing scale-up to happen in parallel. These actions were funded through an $18 billion commitment made largely outside the typical congressional appropriation oversight channels – enough money to back not just one vaccine candidate but six, across three different technology platforms.



In France, the rapid reconstruction of Notre-Dame after the April 2019 fire has become a symbol of French national pride and its ability to get things done despite a reputation for moribund bureaucracy. This was achieved not through wholesale reform of that bureaucracy but by quickly setting up a fresh structure outside of it. In July 2019, the French Parliament passed Loi n° 2019-803, creating an extraordinary legal framework for the project. Construction permits and zoning changes were fast-tracked. President Macron personally appointed the veteran General Jean-Louis Georgelin to run the restoration, exempting him from the mandatory retirement age for public executives in order to do so.



The long-term promise of a small working system is that over time it can supplant the old, broken one and produce results on a larger scale. This creative destruction has long been celebrated in the private sector, where aging corporate giants can be disrupted by smaller, simpler startups: we don’t have to rely on IBM to make our phones or laptops or Large Language Models. But it can work in the public sector too. Estonia, for example, introduced electronic ID in the early 2000s for signing documents and filing online tax returns. These simple applications, which nonetheless took enormous focus to implement, were popular, and ‘digital government’ was gradually expanded to new areas: voting in 2005, police in 2007, prescriptions in 2010, residency in 2014, and even e-divorce in 2024. By 2025, 99 percent of residents will have an electronic ID card, digital signatures are estimated to save two percent of GDP per year, and every state service runs online. 



In desperate situations, such as a Cold War arms race or COVID-19, we avoid complex systems and find simpler workarounds. But, outside of severe crises, much time is wasted on what amounts to magical systems thinking. Government administrations around the world, whose members would happily admit their incompetence to fix a broken radio system, publish manifestos, strategies, plans, and priorities premised on disentangling systems problems that are orders of magnitude more challenging. With each ‘fix’, oversight bodies, administrative apparatus, and overlapping statutory obligations accumulate. Complexity is continuing to rise, outcomes are becoming worse, and voters’ goodwill is being eroded.



We will soon be in an era where humans are not the sole authors of complex systems. Sundar Pichai estimated in late 2024 that over 25 percent of Google’s code was AI generated; as of mid-2025, the figure for Anthropic is 80–90 percent. As in the years after the Second World War, the temptation will be to use this vast increase in computational power and intelligence to ‘solve’ systems design for once and for all. But the same laws that limited Forrester continue to bind: ‘NEW SYSTEMS CREATE NEW PROBLEMS’ and ‘THE SYSTEM ALWAYS KICKS BACK’. As systems become more complex, they become more chaotic, not less. The best solution remains humility, and a simple system that works.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Recreating the US time zone situation]]></title>
            <link>https://rachelbythebay.com/w/2025/09/12/tz/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45233237</guid>
        </item>
        <item>
            <title><![CDATA[486Tang – 486 on a credit-card-sized FPGA board]]></title>
            <link>https://nand2mario.github.io/posts/2025/486tang_486_on_a_credit_card_size_fpga_board/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232565</guid>
            <description><![CDATA[Yesterday I released 486Tang v0.1 on GitHub. It’s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I’ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here’s a short write‑up of the project.]]></description>
            <content:encoded><![CDATA[Yesterday I released 486Tang v0.1 on GitHub. It’s a port of the ao486 MiSTer PC core to the Sipeed Tang Console 138K FPGA. I’ve been trying to get an x86 core running on the Tang for a while. As far as I know, this is the first time ao486 has been ported to a non-Altera FPGA. Here’s a short write‑up of the project.486Tang ArchitectureEvery FPGA board is a little different. Porting a core means moving pieces around and rewiring things to fit. Here are the major components in 486Tang:Compared to ao486 on MiSTer, there are a few major differences:Switching to SDRAM for main memory. The MiSTer core uses DDR3 as main memory. Obviously, at the time of the 80486, DDR didn’t exist, so SDRAM is a natural fit. I also wanted to dedicate DDR3 to the framebuffer; time‑multiplexing it would have been complicated. So SDRAM became the main memory and DDR3 the framebuffer. The SDRAM on Tang is 16‑bit wide while ao486 expects 32‑bit accesses, which would normally mean one 32‑bit word every two cycles. I mitigated this by running the SDRAM logic at 2× the system clock so a 32‑bit word can be read or written every CPU cycle (“double‑pumping” the memory).SD‑backed IDE. On MiSTer, the core forwards IDE requests to the ARM HPS over a fast HPS‑FPGA link; the HPS then accesses a VHD image. Tang doesn’t have a comparable high‑speed MCU‑to‑FPGA interface—only a feeble UART—so I moved disk storage into the SD card and let the FPGA access it directly.Boot‑loading module. A PC needs several things to boot: BIOS, VGA BIOS, CMOS settings, and IDE IDENTIFY data (512 bytes). Since I didn’t rely on an MCU for disk data, I stored all of these in the first 128 KB of the SD card. A small boot loader module reads them into main memory and IDE, and then releases the CPU when everything is ready.System bring-up with the help of a whole-system simulatorAfter restructuring the system, the main challenge was bringing it up to a DOS prompt. A 486 PC is complex—CPU and peripherals—more so than the game consoles I’ve worked on. The ao486 CPU alone is >25K lines of Verilog, versus a few K for older cores like M68K. Debugging on hardware was painful: GAO builds took 10+ minutes and there were many more signals to probe. Without a good plan, it would be unmanageable and bugs could take days to isolate—not viable for a hobby project.My solution was Verilator for subsystem and whole‑system simulation. The codebase is relatively mature, so I skipped per‑module unit tests and focused on simulating subsystems like VGA and a full boot to DOS. Verilator is fast enough to reach a DOS prompt in a few minutes—an order of magnitude better if you factor in the complete waveforms you get in simulation. The trick, then, is surfacing useful progress and error signals. A few simple instrumentation hooks were enough for me:Bochs BIOS can print debug strings to port 0x8888 in debug builds. I intercept and print these (the yellow messages in the simulator). The same path exists on hardware—the CPU forwards them over UART—so BIOS issues show up immediately without waiting for a GAO build.Subsystem‑scoped tracing. For Sound Blaster, IDE, etc., I added --sound, --ide flags to trace I/O operations and key state changes. This is much faster than editing Verilog or using GAO.Bochs BIOS assembly listings are invaluable. I initially used a manual disassembly—old console habits—without symbols, which was painful. Rebuilding Bochs and using the official listings solved that.A lot of the bugs were in the new glue I added, as expected. ao486 itself is mature. Still, a few issues only showed up on this toolchain/hardware, mostly due to toolchain behavior differences. In one case a variable meant to be static behaved like an automatic variable and didn’t retain state across invocations, so a CE pulse never occurred. Buried deep, it took a while to find.Here’s a simulation session. On the left the simulated 486 screen. On the right is the simulator terminal output. You can see the green VGA output and yellow debug output, along with other events like INT 15h and video VSYNCs.Performance optimizationsWith simulation help, the core ran on Tang Console—just not fast. The Gowin GW5A isn’t a particularly fast FPGA. Initial benchmarks put it around a 25 MHz 80386.The main obstacle to clock speed is long combinational paths. When you find a critical path, you either shorten it or pipeline it by inserting registers—both risks bugs. A solid test suite is essential; I used test386.asm to validate changes.Here are a few concrete wins:Reset tree and fan-out reduction. Gowin’s tools didn’t replicate resets aggressively enough (even with “Place → Replicate Resources”). One reset net had >5,000 fan-out, which ballooned delays. Manually replicating the reset and a few other high‑fan-out nets helped a lot.Instruction fetch optimization. A long combinational chain sat in the decode/fetch interface. In decoder_regs.v, the number of bytes the fetcher may accept was computed using the last decoded instruction’s length:reg [3:0] decoder_count;
assign acceptable_1     = 4'd12 - decoder_count + consume_count;
always @(posedge clk) begin
  ...
  decoder_count <= after_consume_count + accepted;
end
Here, 12 is the buffer size, decoder_count is the current occupancy, and consume_count is the length of the outgoing instruction. Reasonable—but computing consume_count (opcode, ModR/M, etc.) was on the Fmax‑limiting path. By the way, this is one of several well-known problems of the x86 - variable length instructions complicating decoding, another is complex address modes and “effective address” calculation.The fix was to drop the dependency on consume_count:assign acceptable_1    = 4'd12 - decoder_count;
This may cause the fetcher to “under‑fetch” for one cycle because the outgoing instruction’s space isn’t reclaimed immediately. But decoder_count updates next cycle, reclaiming the space. With a 12‑byte buffer, the CPI impact was negligible and Fmax improved measurably on this board.TLB optimization. The Translation Lookaside Buffer (TLB) is a small cache that translates virtual to physical addresses. ao486 uses a 32‑entry fully‑associative TLB with a purely combinational read path—zero extra cycles, but a long path on every memory access (code and data).DOS workloads barely stress the TLB; even many 386 extenders use a flat model. As a first step I converted the TLB to 4‑way set‑associative. That’s simpler and already slightly faster than fully‑associative for these workloads. There’s room to optimize further since the long combinational path rarely helps.A rough v0.1 end‑to‑end result: about +35% per Landmark 6 benchmarks, reaching roughly 486SX‑20 territory.ReflectionsHere are a few reflections after the port:Clock speed scaling. I appreciate the lure of the megahertz race now. Scaling the whole system clock was the most effective lever—more so than extra caches or deeper pipelines at this stage. Up to ~200–300 MHz, CPU, memory, and I/O can often scale together. After that, memory latency dominates, caches grow deeper, and once clock speeds stop increasing, multiprocessing takes over—the story of the 2000s.x86 vs. ARM. Working with ao486 deepened my respect for x86’s complexity. John Crawford’s 1990 paper “The i486 CPU: Executing Instructions in One Clock Cycle” is a great read; it argues convincingly against scrapping x86 for a new RISC ISA given the software base (10K+ apps then). Compatibility was the right bet, but the baggage is real. By contrast, last year’s ARM7‑based GBATang felt refreshingly simple: fixed‑length 32‑bit instructions, saner addressing, and competitive performance. You can’t have your cake and eat it.So there you have it—that’s 486Tang in v0.1. Thanks for reading, and see you next time.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Four-year wedding crasher mystery solved]]></title>
            <link>https://www.theguardian.com/uk-news/2025/sep/12/wedding-crasher-mystery-solved-four-years-bride-scotland</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232562</guid>
            <description><![CDATA[Bride finally tracks down awkward-looking stranger she and husband noticed only when looking through photos]]></description>
            <content:encoded><![CDATA[A baffled bride has solved the mystery of the awkward-looking stranger who crashed her wedding four years ago.Michelle Wylie and her husband, John, registered the presence of their unidentifiable guest only as they looked through photographs of their wedding in the days after the happy occasion.Who was the tall man in a dark suit, distinguished by the look of quiet mortification on his face? But their family and friends could offer no explanation, nor could hotel staff at the Carlton hotel in Prestwick, where the event took place in November 2021. An appeal on Facebook likewise yielded no clues.Eventually, with the mystery still niggling, Wylie asked the popular Scottish content creator Dazza to cast the online net wider – and a sheepish Andrew Hillhouse finally stepped forward.In his explanatory post on Facebook, Hillhouse admitted that he had been “cutting it fine, as I’m known to do” when he pulled up at the wedding venue with five minutes to spare. Spotting a piper and other guests, he followed them into the hotel – “I remember thinking to myself: ‘Cool, this is obviously the right place’” – unaware that he had the address completely wrong and was supposed to be at a ceremony 2 miles away in Ayr.Michelle and John enjoy their wedding, unaware of the crasher. Photograph: Courtesy Michelle Wylie/SWNSHe was initially unperturbed to find himself surrounded by strangers as the ceremony began – at the marriage he was due to attend, the only person he knew was the bride, Michaela, while his partner, Andrew, was part of the wedding party. It was when an entirely different bride came walking down the aisle that he realised: “OMG that’s not Michaela … I was at the wrong wedding!”Hillhouse said: “You can’t exactly stand up and walk out of a wedding mid-ceremony, so I just had to commit to this act and spent the next 20 minutes awkwardly sitting there trying to be as inconspicuous as my 6ft 2 ass could be.”At the end of the ceremony, Hillhouse, who is from Troon, was hoping to make a discreet exit, only to be waylaid by the wedding photographer, who insisted he join other guests for a group shot. He can be spotted looming uncomfortably at the very back of the crowd.skip past newsletter promotionafter newsletter promotionHis post continued: “Rushed outside, made some phone calls and made my way to the correct wedding, where I was almost as popular as the actual bride and groom, and spent most of the night retelling that story to people.”For Michelle Wylie, this amiable resolution brings to a close years of speculation.Hillhouse said the wedding photographer insisted he join other guests for a group shot. Photograph: Courtesy Michelle Wylie/SWNSShe told BBC Scotland: “It would come into my head and I’d be like: ‘Someone must know who this guy is.’ I said a few times to my husband: ‘Are you sure you don’t know this guy, is he maybe from your work?’ We wondered if he was a mad stalker.”She is now Facebook friends with Hillhouse and the pair have met in person to cement their coincidental bond.“I could not stop laughing,” said Wylie. “We can’t believe we’ve found out who he is after almost four years.”]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: CLAVIER-36 – A programming environment for generative music]]></title>
            <link>https://clavier36.com/p/LtZDdcRP3haTWHErgvdM</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232299</guid>
        </item>
        <item>
            <title><![CDATA[Japan sets record of nearly 100k people aged over 100]]></title>
            <link>https://www.bbc.com/news/articles/cd07nljlyv0o</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45232052</guid>
            <description><![CDATA[The number of Japanese centenarians rose to 99,763 in September, with women making up 88% of the total.]]></description>
            <content:encoded><![CDATA[2 days agoJessica Rawnsley andStephanie HogartyPopulation correspondentThe number of people in Japan aged 100 or older has risen to a record high of nearly 100,000, its government has announced.Setting a new record for the 55th year in a row, the number of centenarians in Japan was 99,763 as of September, the health ministry said on Friday. Of that total, women accounted for an overwhelming 88%.Japan has the world's longest life expectancy, and is known for often being home to the world's oldest living person - though some studies contest the actual number of centenarians worldwide.It is also one of the fastest ageing societies, with residents often having a healthier diet but a low birth rate.The oldest person in Japan is 114-year-old Shigeko Kagawa, a woman from Yamatokoriyama, a suburb of the city Nara. Meanwhile, the oldest man is Kiyotaka Mizuno, 111, from the coastal city of Iwata.Health minister Takamaro Fukoka congratulated the 87,784 female and 11,979 male centenarians on their longevity and expressed his "gratitude for their many years of contributions to the development of society".The figures were released ahead of Japan's Elderly Day on 15 September, a national holiday where new centenarians receive a congratulatory letter and silver cup from the prime minister. This year, 52,310 individuals were eligible, the health ministry said.In the 1960s, Japan's population had the lowest proportion of people aged over 100 of any G7 country - but that has changed remarkably in the decades since.When its government began the centenarian survey in 1963, there were 153 people aged 100 or over. That figure rose to 1,000 in 1981 and stood at 10,000 by 1998.The higher life expectancy is mainly attributed to fewer deaths from heart disease and common forms of cancer, in particular breast and prostate cancer.Japan has low rates of obesity, a major contributing factor to both diseases, thanks to diets low in red meat and high in fish and vegetables.The obesity rate is particularly low for women, which could go some way to explaining why Japanese women have a much higher life expectancy than their male counterparts.As increased quantities of sugar and salt crept into diets in the rest of the world, Japan went in the other direction - with public health messaging successfully convincing people to reduce their salt consumption.But it's not just diet. Japanese people tend to stay active into later life, walking and using public transport more than elderly people in the US and Europe.Radio Taiso, a daily group exercise, has been a part of Japanese culture since 1928, established to encourage a sense of community as well as public health. The three-minute routine is broadcast on television and practised in small community groups across the country.However, several studies have cast doubt on the validity of global centenarian numbers, suggesting data errors, unreliable public records and missing birth certificates may account for elevated figures.A government audit of family registries in Japan in 2010 uncovered more than 230,000 people listed as being aged 100 or older who were unaccounted for, some having in fact died decades previously.The miscounting was attributed to patchy record-keeping and suspicions that some families may have tried to hide the deaths of elderly relatives in order to claim their pensions.The national inquiry was launched after the remains of Sogen Koto, believed to be the oldest man in Tokyo at 111, were found in his family home 32 years after his death.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My first impressions of gleam]]></title>
            <link>https://mtlynch.io/notes/gleam-first-impressions/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231852</guid>
            <description><![CDATA[What I've learned in my first few hours using Gleam for a small project.]]></description>
            <content:encoded><![CDATA[I’m looking for a new programming language to learn this year, and Gleam looks like the most fun. It’s an Elixir-like language that supports static typing.I read the language tour, and it made sense to me, but I need to build something before I can judge a programming language well.I’m sharing some notes on my first few hours using Gleam in case they’re helpful to others learning Gleam or to the team developing the language.My project: Parsing old AIM logs 🔗︎I used AOL Instant Messenger from about 1999 to 2007. For most of that time, I used AIM clients that logged my conversations, but they varied in formats. Most of the log formats are XML or HTML, which make re-reading those logs a pain.The simplest AIM logs are the plaintext logs, which look like this:Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
Every decade or so, I try writing a universal AIM log parser to get all of my old logs into a consistent, readable format. Unfortunately, I always get bored and give up partway through. My last attempt was seven years ago, when I tried doing it in Python 2.7.Parsing logs is a great match for Gleam because some parts of the project are easy (e.g., parsing the plaintext logs), so I can do the easy parts while I get the hang of Gleam as a language and gradually build up to the harder log formats and adding a web frontend.I’ve also heard that functional languages lend themselves especially well to parsing tasks, and I’ve never understood why, so it’s a good opportunity to learn.My background in programming languages 🔗︎I’ve been a programmer for 20 years, but I’m no language design connoisseur. I’m sharing things about Gleam I find unintuitive or difficult to work with, but they’re not language critiques, just candid reactions.I’ve never worked in a langauge that’s designed for functional programming. The closest would be JavaScript. The languages I know best are Go and Python.How do I parse command-line args? 🔗︎The first thing I wanted to do was figure out how to parse a command-line argument so I could call my app like this:./log-parser ~/logs/aim/plaintext
But there’s no Gleam standard library module for reading command-line arguments. I found glint, and it felt super complicated for just reading one command-line argument. Then, I realized there’s a simpler third-party library called argv.I can parse the command-line argument like this:pub fn main() {
  case argv.load().arguments {
    [path] -> io.println("command-line arg is " <> path)
    _ -> io.println("Usage: gleam run <directory_path>")
  }
}
$ gleam run ~/whatever
   Compiled in 0.01s
    Running log_parser.main
command-line arg is /home/mike/whatever
Cool, easy enough!What does gleam build do? 🔗︎I got my program to run with gleam run, but I was curious if I could compile an executable like go build or zig build does.$ gleam build
   Compiled in 0.01s
Hmm, compiled what? I couldn’t see a binary anywhere.The documentation for gleam build just says “Build the project” but doesn’t explain what it builds or where it stores the build artifact.There’s a build directory, but it doesn’t produce an obvious executable.$ rm -rf build && gleam build
Downloading packages
 Downloaded 5 packages in 0.00s
  Compiling argv
  Compiling gleam_stdlib
  Compiling filepath
  Compiling gleeunit
  Compiling simplifile
  Compiling log_parser
   Compiled in 0.52s

$ ls -1 build/
dev
gleam-dev-erlang.lock
gleam-dev-javascript.lock
gleam-lsp-erlang.lock
gleam-lsp-javascript.lock
gleam-prod-erlang.lock
gleam-prod-javascript.lock
packages
From poking around, I think the executables are under build/dev/erlang/log_parser/ebin/:$ ls -1 build/dev/erlang/log_parser/ebin/
log_parser.app
log_parser.beam
log_parser@@main.beam
log_parser_test.beam
plaintext_logs.beam
plaintext_logs_test.beam
Those appear to be BEAM bytecode, so I can’t execute them directly. I assume I could get run the BEAM VM manually and execute those files somehow, but that doesn’t sound appealing.So, I’ll stick to gleam run to run my app, but I wish gleam build had a better explanation of what it produced and what the developer can do with it.Let me implement the simplest possible parser 🔗︎To start, I decided to write a function that does basic parsing of plaintext logs.So, I wrote a test with what I wanted.pub fn parse_simple_plaintext_log_test() {
  "
Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005
[18:44] Jane: hi
[18:55] Me: hey whats up
Session Close (Jane): Mon Sep 12 18:56:02 2005
"
  |> string.trim
  |> plaintext_logs.parse
  |> should.equal(["hi", "hey whats up"])
}
Eventually, I want to parse all the metadata in the conversation, including names, timestamps, and session information. But as a first step, all my function has to do is read an AIM chat log as a string and emit a list of the chat messages as separate strings.That meant my actual function would look like this:pub fn parse(contents: String) -> List(String) {
  // Note: todo is a Gleam language keyword to indicate unfinished code.
  todo
}
Just to get it compiling, I add in a dummy implementation:pub fn parse(contents: String) -> List(String) {
  ["fake", "data"]
}
And I can test it like this:$ gleam test
  Compiling log_parser
warning: Unused variable
  ┌─ /home/mike/code/gleam-log-parser2/src/plaintext_logs.gleam:1:14
  │
1 │ pub fn parse(contents: String) -> List(String) {
  │              ^^^^^^^^^^^^^^^^ This variable is never used

Hint: You can ignore it with an underscore: `_contents`.

   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["fake", "data"]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
Cool, that’s what I expected. The test is failing because it’s returning hardcoded dummy results that don’t match my test.Adjusting my brain to a functional language 🔗︎Okay, now it’s time to implement the parsing for real. I need to implement this function:pub fn parse(contents: String) -> List(String) {
  todo
}
At this point, I kind of froze up. It struck me that Gleam excludes so many of the tools I’m used to in other languages:There are no if statementsThere are no loopsThere’s no return keywordThere are no list index accessorse.g., you can’t access the n-th element of a ListWhat do I even do? Split the string into tokens and then do something with that?Eventually, I realized for a simple implementation, I wanted to just split the string into lines, so I want to do this:pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
}
If I test again, I get this:$ gleam test
  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["Session Start (DumbAIMScreenName:Jane): Mon Sep 12 18:44:17 2005", "[18:44] Jane: hi", "[18:55] Me: hey whats up", "Session Close (Jane): Mon Sep 12 18:56:02 2005"]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
Okay, now I’m a little closer.How do I iterate over a list in a language with no loops? 🔗︎I turned my logs into a list of lines, but that’s where I got stuck again.I’m so used to for loops that my brain kept thinking, “How do I do a for loop to iterate over the elements?”I realized I needed to call list.map. I need to define a function that acts on each element of the list.import gleam/list
import gleam/string

fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> line
  }
}

pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
}
This is my first time using pattern matching in any language, and it’s neat, though it’s still so unfamiliar that I find it hard to recognize when to use it.Zooming in a bit on the pattern matching, it’s here:  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> line
  }
It evaluates the line variable and matches it to one of the subsequent patterns within the braces. If the line starts with "Session Start" (the <> means the preceding string is a prefix), then Gleam executes the code after the ->, which in this case is just the empty string. Same for "Session Close".If the line doesn’t match the "Session Start" or "Session Close" patterns, Gleam executes the last line in the case which just matches any string. In that case, it evaluates to the same string. Meaning "hi" would evaluate to just "hi".This is where it struck me how strange it feels to not have a return keyword. In every other language I know, you have to explicitly return a value from a function with a return keyword, but in Gleam, the return value is just the value from the last line that Gleam executes in the function.If I run my test, I get this:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "[18:44] Jane: hi", "[18:55] Me: hey whats up", ""]
     output:

Finished in 0.009 seconds
1 tests, 1 failures
Again, this is what I expected, and I’m a bit closer to my goal.I’ve converted the "Session Start" and "Session End" lines to empty strings, and the middle two elements of the list are the lines that have AIM messages in them.The remaining work is:Strip out the time and sender parts of the log lines.Filter out empty strings.Scraping an AIM message from a line 🔗︎At this point, I have a string like this:[18:55] Me: hey whats up
And I need to extract just the portion after the sender’s name to this:hey whats up
My instinct is to use a string split function and split on the : character. I see that there’s string.split which returns List(String).There’s also a string.split_once function, which should work because I can split once on : (note the trailing space after the colon).The problem is that split_once returns Result(#(String, String), Nil), a type that feels scarier to me. It’s a two-tuple wrapped in a Result, which means that the function can return an error on failure. It’s confusing that split_once can fail whereas split cannot, so for simplicity, I’ll go with split.fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
If I run my test, I get this:$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


   Compiled in 0.01s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
Good. That’s doing what I want. I’m successfully isolating the "hi" part, so now I just have to return it.How do I access the last element of a list? 🔗︎At this point, I feel close to victory. I’ve converted the line to a list of strings, and I know the string I want is the last element of the list, but how do I grab it?In most other languages, I’d just say line_parts[1], but Gleam’s lists have no accessors by index.Looking at the gleam/list module, I see a list.last function, so I try that:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       string.split(line, on: ": ")
       |> list.last
       |> echo
       |> todo
    }
  }
}
If I run that, I get:$ gleam test
  Compiling log_parser
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:12:11
   │
12 │        |> todo
   │           ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `fn(Result(String, Nil)) -> String`.


   Compiled in 0.24s
    Running log_parser_test.main
src/plaintext_logs.gleam:11
Ok("hi")
A bit closer! I’ve extracted the last element of the list to find "hi", but now it’s wrapped in a Result type.I can unwrap it with result.unwrapfn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       string.split(line, on: ": ")
       |> list.last
       |> result.unwrap("")
    }
  }
}
Re-running gleam test yields:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
F
Failures:

  1) plaintext_logs_test.parse_simple_plaintext_log_test: module 'plaintext_logs_test'
     Values were not equal
     expected: ["hi", "hey whats up"]
          got: ["", "hi", "hey whats up", ""]
     output:

Finished in 0.008 seconds
1 tests, 1 failures
Great! That did what I wanted. I reduced the messages lines to just the contents of the messages.Filtering out empty strings 🔗︎The only thing that’s left is to filter the empty strings out of the list, which is straightforward enough with list.filter:pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
  |> list.filter(fn(s) { !string.is_empty(s) })
}
And I re-run the tests:$ gleam test
  Compiling log_parser
   Compiled in 0.22s
    Running log_parser_test.main
.
Finished in 0.007 seconds
1 tests, 0 failures
Voilà! The tests now pass!Tidying up string splitting 🔗︎My tests are now passing, so theoretically, I’ve achieved my initial goal.I could declare victory and call it a day. Or, I could refactor!I’ll refactor.I feel somewhat ashamed of my string splitting logic, as it didn’t feel like idiomatic Gleam. Can I do it without getting into result unwrapping?Re-reading it, I realize I can solve it with this newfangled pattern matching thing. I know that the string will split into a list with two elements, so I can create a pattern for a two-element list:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       case string.split(line, on: ": ") {
          [_, message] -> message
          _ -> ""
       }
    }
  }
}
That feels a little more elegant than calling result.last.Can I tidy this up further? I avoided string.split_once because the type was too confusing, but it’s probably the better option if I expect only one split, so what does that look like?fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       echo string.split_once(line, on: ": ")
       todo
    }
  }
}
To inspect the data, I run my test again:$ gleam test
[...]
src/plaintext_logs.gleam:9
Ok(#("[18:44] Jane", "hi"))
Okay, that doesn’t look as scary as I thought. Even though my first instinct is to unwrap the error and access the last element in the tuple (which actually is easy for tuples, just not lists), I know at this point that there’s probably a pattern-matchy way. And there is:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -> message
        _ -> ""
       }
    }
  }
}
The Ok(#(_, message)) pattern will match a successful result from split_once, which is a two-tuple of String wrapped in an Ok result. The other case option is the catchall that returns an empty string.Getting rid of the empty string hack 🔗︎One of the compelling features of Gleam for me is its static typing, so it feels hacky that I’m abusing the empty string to represent a lack of message on a particular line. Can I use the type system instead of using empty strings as sentinel values?The pattern in Gleam for indicating that something might fail but the failure isn’t necessarily an error is Result(<type>, Nil), so let me try to rewrite it that way:import gleam/list
import gleam/result
import gleam/string

fn parse_line(line: String) -> Result(String, Nil) {
  case line {
    "Session Start" <> _ -> Error(Nil)
    "Session Close" <> _ -> Error(Nil)
    line -> {
       case string.split_once(line, on: ": ") {
        Ok(#(_, message)) -> Ok(message)
        _ -> Error(Nil)
       }
    }
  }
}

pub fn parse(contents: String) -> List(String) {
  string.split(contents, on: "\n")
  |> list.map(parse_line)
  |> result.values
}
Great! I like being more explicit that the lines without messages return Error(Nil) rather than an empty string. Also, result.values is more succinct for filtering empty lines than the previous list.filter(fn(s) { !string.is_empty(s) }).Overall reflections 🔗︎After spending a few hours with Gleam, I’m enjoying it. It pushes me out of my comfort zone the right amount where I feel like I’m learning new ways of thinking about programming but not so much that I’m too overwhelmed to learn anything.The biggest downside I’m finding with Gleam is that it’s a young language with a relatively small team. It just turned six years old, but it looks like the founder was working on it solo until a year ago. There are now a handful of core maintainers, but I don’t know if any of them work on Gleam full-time, so the ecosystem is a bit limited. I’m looking ahead to parsing other log formats that are in HTML and XML, and there are Gleam HTML and XML parsers, but they don’t seem widely used, so I’m not sure how well they’ll work.Love: Pipelines 🔗︎I love love love Gleam’s pipeline syntax. You can see me using it in the test with the |> characters: "..."
  |> string.trim
  |> plaintext_logs.parse
  |> should.equal(["hi", "hey whats up"])
The non-pipeline equivalent of the test would look like this:pub fn parse_simple_plaintext_log_test() {
  let input = "..."
  let trimmed = string.trim(input)
  let parsed = plaintext_logs.parse(trimmed)

  should.equal(parsed, ["hi", "hey whats up"])
}
It looks like wet garbage by comparison.Now that I’ve seen pipelines, they feel so obvious and conspicuously missing in every other programming language I use.I’ve enjoyed pipelining in bash, but it never occurred to me how strange it is that other programming languages never adopted it.Like: Example-centric documentation 🔗︎The Gleam documentation is a bit terse, but I like that it’s so example-heavy.I learn best by reading examples, so I appreciate that so much of the Gleam standard library is documented with examples showing simple usage of each API function.Like: Built-in unused symbol warnings 🔗︎I like that the Gleam compiler natively warns about unused functions, variables, and imports. And I like that these are warnings rather than errors.In Go, I get frustrated during debugging when I temporarily comment something out and then the compiler stubbornly refuses to do anything until I fix the stupid import, which I then have to un-fix when I finish whatever I was debugging.Like: todo keyword 🔗︎One of my favorite dumb programming jokes happened at my first programming job about 15 years ago. On a group email thread with several C++ developers, my friend shared a hot tip about C++ development.He said that if we were ever got fed up with arcane C++ compilation errors, we could just add a special line to our source code, and then even invalid C++ code would compile successfully:#pragma always_compile
Spoiler alert: it’s not a real C++ preprocessor directive.But I’ve found myself occasionally wishing languages had something like this when I’m in the middle of development and don’t care about whatever bugs the compiler is trying to protect me from.Gleam’s todo is almost like a #pragma always_compile. Even if your code is invalid, the Gleam compiler just says, “Okay, fine. I’ll run it anyway.”You can see this when I was in the middle of implementing parse_line:fn parse_line(line: String) -> String {
  case line {
    "Session Start" <> _ -> ""
    "Session Close" <> _ -> ""
    line -> {
      echo string.split(line, on: ": ")
      todo
    }
  }
}
If I take out the todo, Gleam refuses to run the code at all:$ gleam test
  Compiling log_parser
error: Type mismatch
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:8:5
   │
 8 │ ╭     line -> {
 9 │ │       echo string.split(line, on: ": ")
10 │ │     }
   │ ╰─────^

This case clause was found to return a different type than the previous
one, but all case clauses must return the same type.

Expected type:

    String

Found type:

    List(String)
Right, I’m returning an incorrect type, so why would the compiler cooperate with me?But adding todo lets me run the function anyway, which helps me understand what the code is doing even though I haven’t finished implementing it:$ gleam test
warning: Todo found
   ┌─ /home/mike/code/gleam-log-parser/src/plaintext_logs.gleam:10:7
   │
10 │       todo
   │       ^^^^ This code is incomplete

This code will crash if it is run. Be sure to finish it before
running your program.

Hint: I think its type is `String`.


  Compiling log_parser
   Compiled in 0.21s
    Running log_parser_test.main
src/plaintext_logs.gleam:9
["[18:44] Jane", "hi"]
F
[...]
Finished in 0.007 seconds
1 tests, 1 failures
Like: Pattern matching 🔗︎I find pattern matching elegant and concise, though it’s the part of Gleam I find hardest to adjust to. It feels so different from procedural style of programming I’m accustomed to in other languages I know.The downside is that I have a hard time recognizing when pattern matching is the right tool, and I also find pattern matching harder to read. But I think that’s just inexperience, and I think with more practice, I’ll be able to think in pattern matching.Dislike: Error handling 🔗︎I find Gleam’s error handling pretty awkward, especially because errors ruin the beauty of nice, tidy pipelines.For example, if I had a string processing pipeline like this:string.split(line, on: "-")
|> list.last
|> result.unwrap("") // Ugly!
|> string.uppercase
That result.unwrap line feels so ugly and out of place to me. I wish the syntax was like this:string.split(line, on: ": ")
|> try list.last
|> string.uppercase
|> Ok
Where try causes the function to return an error, kind of like in Zig.Dislike: Small core language 🔗︎I don’t know if this is a long-term design choice or if it’s just small for now because it’s an indie-developed language, but the first thing about Gleam that stood out to me is how few built-in features there are.For example, there’s no built-in feature for iterating over the elements of a List type, and the type itself doesn’t expose a function to iterate it, so you have to use the gleam/list module in the standard library.Similarly, if a function can fail, it returns a Result type, and there are no built-in functions for handling a Result, so you have to use the gleam/result module to check if the function succeeded.To me, that functionality feels so core to the language that it would be part of the language itself, not the standard library.Dislike: Limited standard library 🔗︎In addition to the language feeling small, the standard library feels pretty limited as well.There are currently only 19 modules in the Gleam standard library. Conspicuously absent are modules for working with the filesystem (the de facto standard seems to be the third-party simplifile module).For comparison, the standard libraries for Python and Go each have about 250 modules. Although, in fairness, those languages have about 1000x the resources as Gleam.Source code 🔗︎The source code for this project is available on Codeberg:https://codeberg.org/mtlynch/gleam-chat-log-parserCommit 291e6d is the version that matches this blog post.Thanks to Isaac Harris-Holt for helpful feedback on this post.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: A store that generates products from anything you type in search]]></title>
            <link>https://anycrap.shop/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45231378</guid>
        </item>
    </channel>
</rss>