<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 31 Aug 2025 08:35:49 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[My Foray into Vlang]]></title>
            <link>https://kristun.dev/posts/my-foray-into-vlang/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080808</guid>
            <description><![CDATA[Summary of the current state of Vlang and my experience writing an svg generator server]]></description>
            <content:encoded><![CDATA[ Table of contents
Open Table of contents

A little bit about Go
So, wtf is Vlang?

Maps
Struct-licious
WithOption pattern
Enums??? In this economy?
Lambda; the best kind of lamb


Some issues I‚Äôve encountered

net.http
veb
More complex build system
Concurrency


</Thoughts>
Links


A little bit about Go
I like Go. I actually don‚Äôt mind writing err != nil that much. Just set up a snippet and you‚Äôre good to Go. Although, I never really felt like I had a honeymoon period with Go. I learned the language, learned about channels, wrote a bunch of CRUDs and parsers and CLIs. It always felt strictly business. I thought it was because of where I am in my career. But I was wrong.
Go is vanilla. It just werks. You build it, you ship it. The language is simple and you don‚Äôt need to try hard to make it performant.
But sometimes you just want a little spiceüå∂Ô∏èü•µ
Do you ever wonder what else is out there? Hobby programming is a great meme. But I feel like we‚Äôre under too much pressure to produce the new unicorn SaaS with 10 million monthly active users.

You don‚Äôt have to pick a tool then find the right job for it. You can just grab a hammer and start smashing stuff. The same nails you‚Äôve smashed before might feel different if you smash it with another hammer. Pick a Rusty hammer and you might end up obsessed with how important health and safety is.
So, wtf is Vlang?
I might have shot myself in the foot with the hammer analogy there, so let‚Äôs talk about ice cream. Ok so here‚Äôs the gist: vanilla, drizzle some chocolate on top, peanuts? sure why not. You know this taste, you like it, it comes with more stuff on top. If you like vanilla then you might like vanilla++.
That how I see the current state of V. The syntax is similar to Go. It has extra features. The core of it is similar, you can cross compile, you have concurrency (which is also parallelism). Channels and message passing. Oh and defer as well. All my bros love using defer.
Anyway, let‚Äôs see some cool stuff.

  External Link Globe
  
  
  
  
vlang
Maps
// So simple!
simple_languages := {"elixir": {"score": 100, "width": 30}}

// Alternatively
mut languages := map[string]map[string]int{"elixir": {"score": 100, "width": 30}}
languages["elixir"] = {"score": 100}
languages["elixir"]["width"] = 30
Pretty cool! Much like Go, the maps require a fixed type, dynamic objects like JSON or JavaScript requires either a DTO or a type switch.
Ok, but what about the error handling?
elixir_score := languages["elixir"]["score"] or { -1 }

if racket := languages['racket'] {
  println('racket score ${racket['score']}')
  racket_width := racket['width'] or { 0 }
  println('racket width ${racket_width}')
}

// Another way to skin the cat
if 'haskell' in languages {
  if 'score' !in languages['haskell'] {
    println('where is my haskell score??')
  }
}

// Zeroth value
languages['this_dont_exist'] // {}
languages['this_dont_exist']['score'] // 0
Don‚Äôt you miss destructuring?
languages_with_racket_ocaml := {
  ...languages
  'racket': {'score': 99}
  'ocaml': {'score': 98}
}

  External Link Globe
  
  
  
  
vlang/maps
Struct-licious
module main

struct Language {
pub mut:
	score int = -1
	name  string @[required]
}

fn (lr []Language) total() int {
	mut total := 0
	for l in lr {
      if l.score > 0 {
        total += l.score
      }
	}

	return total
}

fn (lr []Language) average() int {
	return lr.total() / lr.len
}

fn main() {
	racket := Language{98, 'racket'}
    // Simple arrays too!
	langs_arr := [racket, Language{102, 'ocaml'}]
	println(langs_arr)
	println(langs_arr.total())
	println(langs_arr.average())
}
Isn‚Äôt that cool? We can have receiver methods on array types. Wait - did you see that? We had a required tag on the struct, that means the program won‚Äôt compile if you don‚Äôt initialise it. That‚Äôs another cool thing I wish Go has. Not to mention, the initialiser value, Go‚Äôs struct is quite predictable in how the value turns out. However, V‚Äôs struct allows you to be explicit. This came in very handy for my case!
@[xdoc: 'Server for GitHub language statistics']
@[name: 'v-gh-stats']
struct Config {
mut:
	show_help bool   @[long: help; short: h; xdoc: 'Show this help message']
	user      string = os.getenv('GH_USER')           @[long: user; short: u; xdoc: 'GitHub username env \$GH_USER']
	token     string = os.getenv('GH_TOKEN')          @[long: token; short: t; xdoc: 'GitHub personal access token env \$GH_TOKEN']
	debug     bool   = os.getenv('DEBUG') == 'true'   @[long: debug; short: d; xdoc: 'Enable debug mode env \$DEBUG']
	cache     bool   = os.getenv('CACHE') == 'true'   @[long: cache; short: c; xdoc: 'Enable caching env \$CACHE']
}
This example contains flags for running my SVG generation server, it allows you to define the flags yourself but if not, use the environmental value. Neato!

  External Link Globe
  
  
  
  
vlang/structs
WithOption pattern
Ahh yes, another thing I had to put up with. TBH, I did end up liking the pattern quite a bit. In Go, no default variables are allowed, you have to use variadics. You end up with an Option struct with zeroth value passing around a few functions to finally one last giant private receiver function that creates the struct, fills the value then finally build and check. Imagine a SQL repository pattern where you want to perform a List operation but optionally join or ensure some field is present in a query. Let‚Äôs see how we can cook this.
module main

import time

@[params]
struct ListOption {
pub mut:
	created_since time.Time
}

@[params]
struct HeroListOption {
	ListOption
pub mut:
	universe string
	name     ?string
}

struct Hero {}

struct Repo[T] {}

struct Villain {}

fn (r Repo[T]) list(o ListOption) ![]T {
	$if T is Villain {
		return error('whoops you found Villain some how but its not implemented yet')
	}

	return error('whoops not implemented for ${T.name} use one of (Hero, ...)')
}

fn (r Repo[Hero]) list(o HeroListOption) ![]Hero {
	mut query := orm.build_query()

	if o.universe != '' {
		query.eq('universe', o.universe)
	}

	if o.created_since.unix() > 0 {
		query.gt('created_since', o.created_since)
	}

	if name := o.name {
		query.eq('name', name)
	}

	return r.psql(query.do()!)!
}

fn main() {
	r := Repo[Villain]{}
	r.list() or { println(err) }

	hero_repo := Repo[Hero]{}
	hero_repo.list()!
	hero_repo.list(name: 'bruce')!
	hero_repo.list(name: 'bruce', universe: 'dc')!
	hero_repo.list(name: 'bruce', universe: 'marvel')!
	hero_repo.list(created_since: time.Time{year: 1996})!
}
There‚Äôs a lot to unpack here. Let‚Äôs start with @[params] which tells the V compiler that the struct as a whole can be omitted entirely so you can write the empty function and it will still works. Secondly, since generics are a compile time thing we can use reflection to check for the name of the type itself. See link below to see what is possible. You can reflect and check for field existence and field types as well as attributes (remember @[required]?).
Alright, we keep seeing this bang (!) everywhere. So what is it? Short answer: Result type. Medium answer: (int, err) -> !int. You don‚Äôt need the long answer. The bang can propagate although you must remember to handle this somewhere or it will cause a panic eventually. Finally, the optional type. I purposedly only use it for one of the field to show that it can be done, you can decide how you want to write your optionals. But damn! It feels great!

  External Link Globe
  
  
  
  
go-uber/functional-options

  External Link Globe
  
  
  
  
vlang/trailing-struct-args

  External Link Globe
  
  
  
  
vlang/compile-time-reflection

  External Link Globe
  
  
  
  
vlang/optional-and-result-type
Enums??? In this economy?
Enums are so back baby. We can totally replace the previous section‚Äôs universe field as such.
enum Universe {
  dc
  marvel
  nil
}

fn (u Universe) str() ?string {
	return match u {
      // V knows the enum there's no need to type Universe.dc
      .dc { 'dc' }
      .marvel { 'marvel' }
      else {''}
	}
}

@[params]
struct HeroListOption {
	ListOption
pub mut:
	universe Universe = .nil
	name     ?string
}

fn (r Repo[Hero]) list(o HeroListOption) ![]Hero {
	...

	if o.universe != .nil {
		query.eq('universe', o.universe.str())
	}

	...
}

fn main() {
	hero_repo := Repo[Hero]{}
	hero_repo.list(name: 'bruce', universe: .dc)!

	// functions not expecting enum requires the full path
	// auto str() conversion here - see Go fmt.Stringer() or your __str__, __toString()
	println('${Universe.dc}')
}
Optional type might be better here. I‚Äôm okay with this though. There is backed enum as well but you can only have integer backed enums. Did you also notice? Receiver method on the backed enum baby.

  External Link Globe
  
  
  
  
vlang/enums
Lambda; the best kind of lamb
The array stucts have a set of methods you can use like the basic filter, map - there is a stdlib module called arrays as well that you need to import. It provides more complex methods like fold and the likes. I don‚Äôt know about you but I am chuffed this exists.
import math

fn example() {
	// type hinting here to skip typing Universe.*
	mut universes := []Universe{}
	universes = [.dc, .marvel, .nil, .dc]
	dcs_or_marvel := universes.filter(it != .nil)
	nils := universes.filter(|u| u == .nil)

	// sorting in place
	[5, 2, 1, 3, 4].sort(a < b)
	sorted := [5, 2, 1, 3, 4].sorted(a < b)
}

struct XY {
	x int
	y int
}

fn (xy XY) dist_from_origin() f64 {
	return math.sqrt((xy.x * xy.x) + (xy.y * xy.y))
}

fn example2() {
	xys := [XY{1, 2}, XY{10, 20}, XY{-1, -69}]
	xys.sort(a.dist_from_origin() < b.dist_from_origin())
	y_asc := xys.sorted(a.y < b.y)
}
There‚Äôs a few caveats here. You gotta make sure the function you‚Äôre using actually allow for it or a < b expression, but lambda expression will work anywhere a function is accepted as an argument. However, you can‚Äôt use lambda as a variable like x_asc := |a, b| a.x < b.x. Still, neat. Use the LSP to check what is accepted.

  External Link Globe
  
  
  
  
vlang/lambdas

  External Link Globe
  
  
  
  
vlang/array

  External Link Globe
  
  
  
  
vlang/arrays
Some issues I‚Äôve encountered
As fun as it has been learning the language and building an 
  External Link Globe
  
  
  
  
svg service - it is not without problems. The language is on the immature side of things. It has had some time to cook since I last tried it in 2023 and I like it even more. Let‚Äôs discuss some of the problems I‚Äôve personally encountered.
net.http
When I was trying to call the GraphQL endpoint using the net.http module, I ran into issue where it would instantly timeout. This 
  External Link Globe
  
  
  
  
network issue described what is happening in my case precisely, adding the flag -d use_openssl completely fixed my problem. This seems to be the case when building for Ubuntu 22.4 - when building the exe for my Windows 11 I did not need this flag.
If you are wondering what the -d flag is about, it is a flag for compile-time code branching. See 
  External Link Globe
  
  
  
  
vlang/compiletime-code for more.

  External Link Globe
  
  
  
  
vlang/net.http
veb
Another weird quirk I‚Äôve had when working with the veb HTTP server is refusing to build when trying to use gzip. Take a look at this build error message.
/root/.local/v/vlib/veb/middleware.v:129:11: error: field `Ctx.return_type` is not public
127 |         handler: fn [T](mut ctx T) bool {
128 |             // TODO: compress file in streaming manner, or precompress them?
129 |             if ctx.return_type == .file {
    |                    ~~~~~~~~~~~
130 |                 return true
131 |             }
What do you think the issue could be? Maybe my version of the language is incorrect or my build was faulty? I purged the local V install and got a fresh version straight from master branch. Yet the issue still persists. Another -d flag perhaps?
Luckily for me somebody already posted about this issue in GitHub, unluckily for me, I didn‚Äôt search the error message first (whoops). Well, I can‚Äôt really tell you what the issue is since I haven‚Äôt delved into V‚Äôs codebase itself. But I can tell you the resolution.
In my main.v, since I was messing around with servers and running main with arguments I needed to import both modules. This was the head -n5 of my errorneous file.
module main

import os
import veb
The suggested fix?
module main

import veb
import os
Wow! The code now compiles! From a fresher‚Äôs perspective I have no clue why the order of import would affect code in different modules. Namespace should be sacred and completely independent of each other. The order of import should not matter at all. Both packages seems to be unrelated so wtf happened?

  External Link Globe
  
  
  
  
vlang/veb

  External Link Globe
  
  
  
  
vlang/gzip-issue
More complex build system
I had alluded to this earlier, there is a cost to using V over Go. V‚Äôs main backend compiles to C and this comes with complexity. There are a bunch of performance optimisations you can do when building the binary itself. You can even build non-static binaries if you wish (in fact this is the default). This is a double-edged sword, with Go, you get what you got. With V, I got what I got but I wonder if what I got can be gotten differently.
This might also complicate cross-compilation, the Go team has done a lot of work to ensure things werk across different architectures and OSes. I‚Äôve only tried compiling to Windows and Linux using the static flag. Here‚Äôs my build command:
v -prod -compress -d use_openssl -cflags '-static -Os -flto' -o main .
The -d flag would have to be optional here depending on where I am trying to target as well, I‚Äôd probably have to spend time learning what‚Äôs possible for Macs as well. I know those platforms are definitely supported since their GitHub actions page contains the CI pipelines for these, but I would personally need to check if my specific implemntation, order of imports as well as -d flags need to be there for those systems or not.
This is the one big point I have to give to Go. They really have the just werks philosophy down.

  External Link Globe
  
  
  
  
vlang/ci

  External Link Globe
  
  
  
  
vlang/performance-optimisation
Concurrency
I wondered how the performance of the concurrency is compared to Go. The model is almost identical (which is good) but surely the implementation details are different. Luckily, there is a programming benchmark that exists already that answers my questions.


  External Link Globe
  
  
  
  
benchmark/coro-sieve-v-vs-go
Since I brought up concurrency let‚Äôs take a look at the code to see the implementation.
module main

import os
import strconv

fn main() {
	mut n := 100
	if os.args.len > 1 {
		n = strconv.atoi(os.args[1]) or { n }
	}

	mut ch := chan int{cap: 1}
	spawn generate(ch)
	for _ in 0 .. n {
		prime := <-ch
		println(prime)
		ch_next := chan int{cap: 1}
		spawn filter(ch, ch_next, prime)
		ch = ch_next
	}
}

fn generate(ch chan int) {
	mut i := 2
	for {
		ch <- i++
	}
}

fn filter(chin chan int, chout chan int, prime int) {
	for {
		i := <-chin
		if i % prime != 0 {
			chout <- i
		}
	}
}

  External Link Globe
  
  
  
  
benchmark/sieve.go

  External Link Globe
  
  
  
  
benchmark/sieve.v
tldr; it‚Äôs finding prime numbers by computing a running channel of previous prime numbers to feed into n to check if n is divisible by any previous primes.
It seems weird to me that V‚Äôs version is timing out even though both implementation looks almost identical. So I ran the benchmark on my local machine. Here‚Äôs my justfile to run the benchmark using all I know so far about optimising V.
default:
    v -prod -gc boehm_full_opt -cc clang -cflags "-march=broadwell" -stats -showcc -no-rsp -o main_v 1.v
    go build -o main_go ./main.go
    hyperfine './main_v 100' './main_go 100' -N
And the result:
Benchmark 1: ./main_v 100
  Time (mean ¬± œÉ):      32.1 ms ¬±   2.9 ms    [User: 42.6 ms, System: 166.4 ms]
  Range (min ‚Ä¶ max):    22.1 ms ‚Ä¶  40.7 ms    99 runs

Benchmark 2: ./main_go 100
  Time (mean ¬± œÉ):       1.8 ms ¬±   0.2 ms    [User: 2.3 ms, System: 0.3 ms]
  Range (min ‚Ä¶ max):     1.2 ms ‚Ä¶   3.1 ms    1471 runs

Summary
  './main_go 100' ran
   18.18 ¬± 2.81 times faster than './main_v 100'
This is exacerbated further when we run N=1000
Benchmark 1: ./main_v 1000
  Time (mean ¬± œÉ):      1.189 s ¬±  0.340 s    [User: 4.410 s, System: 8.144 s]
  Range (min ‚Ä¶ max):    0.806 s ‚Ä¶  1.830 s    10 runs

Benchmark 2: ./main_go 1000
  Time (mean ¬± œÉ):      13.4 ms ¬±   2.4 ms    [User: 132.5 ms, System: 12.3 ms]
  Range (min ‚Ä¶ max):     8.6 ms ‚Ä¶  21.2 ms    182 runs

Summary
  './main_go 1000' ran
   88.54 ¬± 29.90 times faster than './main_v 1000'
Taking a look at the N=100 profiling we can see what happened exactly
‚ûú cat prof.txt | sort --key 2n -n | tail -n 10
           202          0.256ms         -1.819ms           1267ns sync__new_spin_lock
           404          0.064ms         -2.664ms            158ns sync__Semaphore_init
          4387      10644.653ms        540.655ms        2426408ns sync__Semaphore_wait
          8128       5572.567ms        739.231ms         685601ns sync__Channel_try_push_priv
          8172       9062.871ms        941.089ms        1109015ns sync__Channel_try_pop_priv
         15959        406.167ms         87.435ms          25451ns sync__Semaphore_post
         16160          6.993ms        -38.159ms            433ns sync__SpinLock_lock
         16174          3.412ms          0.754ms            211ns sync__SpinLock_unlock
       1766049        380.257ms       -434.470ms            215ns sync__Semaphore_try_wait
There is a ton of calls going to Semaphore_try_wait with the actual Sempahore_wait execution itself taking over 10_000 ms in total.
This suggests to me that while the concurrency is there, it exists and work similarly to the end user. Though in the current state, it‚Äôs no where near Go‚Äôs maturity and optimisation.
</Thoughts>
I like V a lot. The abstraction over the syntax is so nice that made me enjoy writing the syntax as a whole. It makes me wish that Go could do more with what they have, but you and I know that Go would never. V isn‚Äôt without it‚Äôs problems though, the ecosystem is still quite immature, compiler flags need grokking over even if you‚Äôre not a performance maximalist. IMO, the issue comes down to maturity, given enough time and contributor I believe the language will bloom beautifully. The syntax conveniences already had me sold. I know AI can write boilerplate but it feels good to not need it at all and write everything myself.
V has come a lot further than when I tried it in 2023. I‚Äôll be actively using it from now on since my main job in Go leaves me wishing for more from time to time. If you enjoy Go anyway it‚Äôs worth checking out. Life it too short to mainline one language. Oh and check out my SVG service 
  External Link Globe
  
  
  
  
ktunprasert/v-github-stats

Links
vlang - 
  External Link Globe
  
  
  
  
https://vlang.io
vlang/maps - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/v-types.html#maps
vlang/structs - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/structs.html
go-uber/functional-options - 
  External Link Globe
  
  
  
  
https://github.com/uber-go/guide/blob/master/style.md#functional-options
vlang/trailing-struct-args - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/structs.html#trailing-struct-literal-arguments
vlang/compile-time-reflection - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/conditional-compilation.html#compile-time-reflection
vlang/optional-and-result-type - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/type-declarations.html#optionresult-types-and-error-handling
vlang/enums - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/type-declarations.html#enums
vlang/lambdas - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/functions-2.html#lambda-expressions
vlang/array - 
  External Link Globe
  
  
  
  
https://modules.vlang.io/builtin.html#array
vlang/arrays - 
  External Link Globe
  
  
  
  
https://modules.vlang.io/arrays.html
svg service - 
  External Link Globe
  
  
  
  
https://github.com/ktunprasert/v-github-stats
network issue - 
  External Link Globe
  
  
  
  
https://github.com/vlang/v/issues/23717
vlang/compiletime-code - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/conditional-compilation.html#compile-time-code
vlang/net.http - 
  External Link Globe
  
  
  
  
https://modules.vlang.io/net.http.html
vlang/veb - 
  External Link Globe
  
  
  
  
https://modules.vlang.io/veb.html
vlang/gzip-issue - 
  External Link Globe
  
  
  
  
https://github.com/vlang/v/issues/20865#issuecomment-1955101657
vlang/ci - 
  External Link Globe
  
  
  
  
https://github.com/vlang/v/actions
vlang/performance-optimisation - 
  External Link Globe
  
  
  
  
https://docs.vlang.io/performance-tuning.html
benchmark/coro-sieve-v-vs-go - 
  External Link Globe
  
  
  
  
https://programming-language-benchmarks.vercel.app/v-vs-go
benchmark/sieve.go - 
  External Link Globe
  
  
  
  
https://github.com/hanabi1224/Programming-Language-Benchmarks/blob/main/bench/algorithm/coro-prime-sieve/1.go
benchmark/sieve.v - 
  External Link Globe
  
  
  
  
https://github.com/hanabi1224/Programming-Language-Benchmarks/blob/main/bench/algorithm/coro-prime-sieve/1.v ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Git Diagramming "The Weave"]]></title>
            <link>https://daverupert.com/2025/08/git-diagramming-the-weave/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080720</guid>
            <description><![CDATA[We all know the current US President is one hell of an orator and often assures us that he has ‚Äúthe best words‚Äù:]]></description>
            <content:encoded><![CDATA[
    

    
      
  

  
    

    We all know the current US President is one hell of an orator and often assures us that he has ‚Äúthe best words‚Äù:

I went to an Ivy League school. I‚Äôm very highly educated. I know words. I have the best words.

The man knows words. Says so right there. While some might view his non-sequitur ramblings as the nascent stages of dementia or an unfiltered ADHD brain launching into successive short (at times racist) bullet-point diatribes based on the last word or phrase he said like a cursed game of word association, the President asserts this is not the case:

You know, I do the weave. You know what the weave is? I‚Äôll talk about, like, nine different things, and they all come back brilliantly together. And it‚Äôs like - and friends of mine that are, like, English professors - they say, it‚Äôs the most brilliant thing I‚Äôve ever seen.

‚ÄúThe Weave‚Äù re-entered my consciousness this week after I watched a quick snippet of an Oval Office event where Trump says the (‚Äúradical left-wing‚Äù) CBO projects tariffs will reduce the deficit by $4 trillion USD. I was skeptical ‚Äìand for good reason‚Äì but I tuned in. What shocked me was not the complete lack of specifics about the CBO projection, but rather the actual reason for the Oval Office meeting: a FIFA event? Wow.

For awhile now, I‚Äôve clued into the cyclical pattern of his speeches, little snippets of ‚Äúthe best words‚Äù and talking points assembled like a ransom note cut from a magazine. I often wondered if it‚Äôs possible to diagram ‚Äúthe weave‚Äù. The ‚Äúbranching‚Äù narratives Trump uses made me think a git-graph-style visualization was apropos. So I grabbed a transcript and got to work.
For my first attempt, I used Mermaid.js‚Äô GitGraph Diagram which worked well but only supports horizontal charts. As I sat with it I realized I wanted a chronological list of statements that read like a transcript. I repurposed the Mermaid‚Äôs GitGraph DSL and made a web component called <git-graph> to help me visualize and document Trump‚Äôs derailing trains of thought from the above event.

  I would recommend viewing this on my site with JavaScript enabled.
  
branch tarrifs
checkout tarrifs
commit id: "I was very happy that today, as you saw, the uh group that does this [the CBO], a government group,"
branch radical-left
checkout radical-left
commit id: "a radical left group, announced that Trump was right"
checkout tarrifs
merge radical-left
commit id: "took in $4 trillion worth of tariffs"
commit id: "The $4 trillion they're going to reduce the deficit by numbers far greater than they ever expected or heard of."
branch stock-market
checkout stock-market
commit id: "And by the way, the stock market went up a thousand points. That was as of 10 minutes ago."
commit id: "I can't tell you what happened. A lot of things happened, but the stock market's up almost a thousand points."
cherry-pick id: "I was very happy that today, as you saw, the uh group that does this [the CBO], a government group,"
commit id: "It's basically on the news that uh the release that just came out from government that uh the tariffs that everybody was talking about that"
branch world-respect
checkout world-respect
commit id: "the whole world respects us for because of what we did"
cherry-pick id: "took in $4 trillion worth of tariffs"
commit id: "The tariffs are going to be at $4 trillion." 
cherry-pick id: "The $4 trillion they're going to reduce the deficit by numbers far greater than they ever expected or heard of."
commit id: "They're going to reduce the deficit by $4 trillion."
branch ask-jd
checkout ask-jd
commit id: "[Seeks validation from JD Vance]"
checkout stock-market
merge ask-jd
cherry-pick id: "And by the way, the stock market went up a thousand points. That was as of 10 minutes ago."
commit id: "It's had a huge impact and the stock market is way up."
commit id: "But this will drive more than $30 billion in US economy"
commit id: "and create 185,000 American jobs."
branch fifa-event
checkout fifa-event
commit id: "No sporting event attracts more attention or more fans or anything else"
commmit id: "And I just look forward to the draw."
commit id: "So we're going to have the draw essentially, Gianni, at the Kennedy Center"
branch kennedy-center-remodel
checkout kennedy-center-remodel
commit id: "and by that time it'll be in even better shape. We're working on it."
commit id: "It's about a year project to make it."
commit id: "It'll be great. It'll be fantastic."
branch oval-office-remodel
checkout oval-office-remodel
commit id: "You see the way [the oval office] is looking?"
commit id: "Looks nice."
commit id: "I can't tell you how much that gold costs, a lot of money."
commit id: "There's nothing like gold and there's nothing like solid gold."
commit id: "But this beautiful office needed it."
commit id: "It had to be representative when we took it over."
commit id: "It was dirty, not clean."
commit id: "I immediately changed the chair and had the this beautiful desk renovated, brought out by the White House."
commit id: "People that do this, they did a great job."
commit id: "They sent it out. We have a craftsman who's great."
commit id: "But this was not appropriate for the Oval Office when I took over."
commit id: "And now you look at all those paintings [instructs to look at paintings]"
branch painting-vault
checkout painting-vault
commit id: "All of these are great presidents and they were all in the vaults."
commit id: "They were in vaults for in some cases much more than a hundred years."
commit id: "And now they're proudly hanging on the oval office walls and I can't imagine anybody changing it."
commit id: "But they were they were buried in vaults for over a hundred years, many of them."
checkout oval-office-remodel
merge painting-vault
commit id: "So it's very exciting. People come in, they really love it."
branch self-congratulations
checkout self-congratulations
commit id: "They love what we're doing here."
commit id: "They love what we're doing in DC [deploying the national guard against American citizens]"
commit id: "and they love what we're doing most importantly in the country in the world. [citation needed]"
checkout fifa-event
merge self-congratulations
commit id: "I'd like to ask Gianni to say a few words ... [flattery] ... he's got the biggest event in the world coming right here to the United States."
commit id: "We did a little for Canada,"
commit id: "we did a little for Mexico."
checkout self-congratulations
merge fifa-event
commit id: "We thought, see, I'm a good citizen. I said, let them have a little piece."
cherry-pick id: "We did a little for Canada,"
commit id: "So, we gave a little to Canada."
commit id: "See how nice I am."
cherry-pick id: "we did a little for Mexico."
commit id: "And we gave a little bit to Mexico."
checkout fifa-event
merge self-congratulations
commit id: "Gianni, please say a few words."


By my count in that four-minute address there were ten distinct themes or ‚Äúbranches‚Äù, most of which are common grievances or rhetorical themes found in nearly all Trump speeches. I used ‚Äúcherry-picking‚Äù to model callbacks to a previous statement (e.g. ‚Äúthe stock market is up‚Äù) that seem to reverberate into later trains of thought, a quintessential feature of the weave.
Four minutes of the weave was about all I could handle but I think this visualization models what I experience when trying to follow along to the President‚Äôs speeches. One or two phrases on a topic, then jumping to a new topic, weaving in a (sometimes unrelated) point from a previous topic to make the thought appear more cohesive and linear, then driving deep down an inconsequential topic. When he talks so long about ten different topics at a time, I‚Äôd forgive you for thinking he said something salient. But when you break it down you see it for what it is: a mishmash of talking points.

  

  




    

    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rick Beato is right to rant about music copyright strikes]]></title>
            <link>https://savingcountrymusic.com/rick-beato-is-right-to-rant-about-music-copyright-strikes/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080618</guid>
            <description><![CDATA[Music labels have been leaving major opportunities to promote their catalogs and performers on the table with their punitive copyright claims that make it impossible to feature music on music podcasts.]]></description>
            <content:encoded><![CDATA[

		
	
    			
		

	
	

	

					August 20, 2025		
					

				Trigger
				Radio/Media
				
									
								
			
			
				
		

		
			





Go Beato, go! For 15+ years, Saving Country Music has been on the warpath against the completely ludicrous intellectual property regime that disallows even a small snippet of music to be featured in a podcast without draconious repercussions, including removing episodes, and deleting entire accounts, while not offering any reasonable alternative solutions to the issue. On YouTube videos, creators can freely filch copyrighted photos and other people‚Äôs videos virtually free of ramifications. You can take an entire 2 1/2 hour film, impose it over a background, and upload it to YouTube, and usually avoid any problems. But feature a barely audible 8 1/2-second clip of music underneath audio dialogue, and you could have your entire podcast career evaporate overnight. Music labels have been leaving major opportunities to promote their catalogs and performers on the table with their punitive copyright claims that make it impossible to feature music on music podcasts and other platforms. And instead of trying to build a system where perhaps there‚Äôs more reasonable revenue sharing or other opportunities for artists and songwriters through these podcast platforms to promote their music, it‚Äôs the punitive measures of record labels that eliminate these opportunities in lose/lose scenarios. Music video podcaster Rick Beato with his massive 5 million-plus subscriber base finally got fed up with it, and posted a rant on Tuesday, August 19th about this, and, it‚Äôs a thing of beauty. ‚ÄúI hate making these videos, but I really need to because it seems the only thing that ever gets done is when you talk about this stuff,‚Äù Beato starts off. But one of the nauseating things about this issue is that we‚Äôve been talking about it for going on two decades, and still nothing is getting done about it. Saving Country Music posted about this issue in 2024, and in 2020, and as far back as 2010, with no real movement on the issue. Luckily though, Rick Beato has a much bigger bullhorn, though he has brought up the issue before to no avail, though not in such a dedicated and forceful manner. Beato‚Äôs specific beef is with Universal Music Group, who is the most notorious actor for bringing these heavy handed claims against song clip uses that clearly fall under the fair use clauses of American copyright law, let alone are being brought against a guy who operates a massive music platform that promotes artists. But since there‚Äôs rarely humans making any of these decisions and it‚Äôs automated by bots, they don‚Äôt understand these claims are against Universal Music‚Äôs best interests. ‚ÄúI‚Äôm doing interviews with people and playing the music that they either wrote or recorded, or they produced,‚Äù Beato explains. ‚ÄúYou need to play people music to talk about it. That is the definition of fair use. These are interviews with people about their careers. Why are these record labels wanting to take down content about artist that have records on their label? What sense does that make?‚Äù ‚ÄúGive it a rest,‚Äù he continues. ‚ÄúMy God. If I didn‚Äôt fight these three claims, my channel would get taken down with my 2,000 videos. Is that ridiculous? It‚Äôs ridiculous to me. And they‚Äôre for interviews.‚Äù







People continue to ask, ‚ÄúWhy doesn‚Äôt Saving Country Music has a podcast?‚Äù Because what‚Äôs the point of having a music podcast when you can‚Äôt feature music? In fact, after over a decade of refusing to start one, I finally did, music free. What happened? About a dozen episodes in, someone took out a claim, and not only were all the episodes deleted, so was the entire account, even though no music even appeared on any of the episodes. I was given absolutely no recourse to fight whatever false claim had been made. For the record, Saving Country Music‚Äôs Country History X podcast remains live. But it‚Äôs been difficult to pursue it, knowing it could summarily disappear at any moment. People have lost thousands of episodes and videos over this issue. Meanwhile, you can use extended song clips on Instagram, TikTok, and Facebook. It has become an excellent way to promote songs and artists. These networks facilitate this music sharing, and credit the artists for their work. The music industry continues to so colossal fail the artists and catalogs they represent, and the fans they‚Äôre supposed to serve with this current system of how podcasts are handled. If everything changes today thanks to the Rick Beato rant, it would still be 15 years too late. But at least it would happen. The powers that be in the music industry need to recognize that the power base in media is moving to podcasting, and they need to make music available to podcasters in a way that‚Äôs fair to all parties. In fact in some respects we‚Äôre moving backwards on this issue. A couple of years ago, Spotify did offer the ability for podcasters to embed full songs in episodes. But they‚Äôve since discontinued that program. Let‚Äôs do this. It‚Äôs well beyond the time to solve this problem. You aren‚Äôt screwing podcasters. You‚Äôre screwing artists who could be using podcasts to help promote their music. Hopefully Rick Beato‚Äôs rant finally reaches the right ears. ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì



If you found this article valuable, consider leaving¬†Saving Country Music A TIP.
¬© 2025 Saving Country Music		
	
		
				
		 Rick Beato		
				
		

		
	
	
	



				
		
		
		
		

		


	



			    
		
		
	    
	            
			
	
	
            
    
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sheafification ‚Äì The optimal path to mathematical mastery]]></title>
            <link>https://sheafification.com/the-fast-track/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080388</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Q.js ‚Äì Smaller than React/Vue, yet more powerful (40KB gzipped)]]></title>
            <link>https://github.com/Qbix/Q.js</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080222</guid>
            <description><![CDATA[All-In-One Front-End Web Framework from Qbix, alternative to jQuery, Angular, Vue, Ember etc. - Qbix/Q.js]]></description>
            <content:encoded><![CDATA[Q.js
All-In-One Front-End Web Framework from Qbix, alternative to jQuery, Angular, Vue etc.
Size: ~40KB (Minified + GZipped), compare to other frameworks
How to use: copy contents of dist into your project, and then include it like this:



File Type
Code to Use




.html files
<script type="module">import { Q } from './Q.js</script>


.js or .ts files
import Q from './Q.js';



Full documentation here: https://qbix.com/platform/guide/javascript



This is part of the much larger full-stack Qbix Platform that contains many pre-built reusable tools, plugins, and requires PHP and Node.js on the back-end. If you want to build an entire full-stack social network like Facebook you're well-advised to go with that. But if you just want to use the lightweight front-end core, with your own back-end and other frameworks, then start with this framework here.
üåü Advantages of Q.minimal.js vs other frameworks
1. No build step
React/Vue/Angular need bundlers, transpilers, tree-shakers, and hydration layers. Q.js works with plain .html and .js ‚Äî drop it in and it runs.
2. Tiny footprint (~40KB gzipped)
Smaller than React (without ReactDOM), Vue runtime, and far smaller than Angular. Yet it packs components, events, routing, caching, batching, i18n, animations, IndexedDB, service workers, and more ‚Äî all in core.
3. Direct DOM updates (no Virtual DOM overhead)
No diffing, no reconciler. Q.js uses requestAnimationFrame and .rendering() hooks for high-performance, granular updates.
4. Autoloading methods & tools
Any method, template, or tool can be defined in its own file and is only loaded when first used. No code-splitting configs, no manual lazy-loading hacks ‚Äî it‚Äôs automatic.
5. Tools = components + behaviors
Like React components or Vue directives, but attachable as behaviors to any DOM element. Multiple tools can live on the same element, making composition more flexible than ‚Äúone component per node.‚Äù
6. Built-in batching & caching
APIs like Q.getter() and Q.batcher() deduplicate, throttle, and combine calls automatically. If multiple parts of the UI request the same object, it‚Äôs fetched once and everyone gets the result.
7. Templates that Just Work‚Ñ¢
Use Handlebars, .html <template> files (Vue-style), or JS-defined templates. Designers can work in HTML, devs can work in JS ‚Äî both paths integrate seamlessly.
8. First-class events & lifecycle
Events are observable, chainable (a.and(b), a.or(b), a.until(b)), and auto-clean up when a tool or page unloads. No more memory leaks or dangling listeners.
9. Progressive enhancement & SEO-friendly
Pages can render server-side HTML and Q.js simply activates tools as needed. No ‚Äúblank page until hydration‚Äù problem ‚Äî works with or without JS.
10. Unified, full-stack philosophy
It‚Äôs not just a front-end library. Q.js is part of the larger Qbix platform, which powers real social apps (with accounts, feeds, groups, payments, etc.). You can start small with Q.minimal.js, and later plug into the full stack without rewriting.
Q.js is smaller than jQuery, faster than React, easier than Vue, and more complete than Angular.
üîç Features
Despite its size, Q.js implements many features not found in other front-end frameworks.
Here is an overview of the main ones:



Class Name
Description




Q.Tool
reusable components, activate with Q.activate(element)


Q.Page
HTML pages, for your SPA, routes, browser history


Q.Event
events and handlers, loaded and unloaded automatically


Q.Template
for rendering templates, integrates with Handlebars


Q.Text
for loading internationalized translations and text for templates


Q.Method
defines methods loading JS files asynchronously as needed


Q.Visual
managing the front end interface, standard hints, as well as Q.Mask


Q.Animation
for animating using native Javascript animation


Q.Audio
speaking, loading and playing audio, etc.


Q.IndexedDB
for easy interaction with the built-in IndexedDB


Q.ServiceWorker
to manage service workers in a standard way


Requests
Q.request(), Q.handle(), Q.loadUrl(), Q.addScript(), Q.addStylesheet()


Flow
Q.chain(), Q.getter(), Q.batcher(), Q.promisify(), Q.debounce()


Helpers
Q.find(), Q.activate(), Q.cookie(), Q.handle()



‚öñÔ∏è Comparison with React, Vue, Angular, Svelte



Feature üèÜ
Q.js ‚ö° (40KB)
React üèóÔ∏è
Vue üé®
Angular üèõÔ∏è
Svelte üî•




Bundle Size üì¶
~40KB gzipped (core + tools + events + routing)
42KB + ReactDOM (120KB)
~60KB runtime
140KB+
~50KB compiler/runtime


Build Step üõ†Ô∏è
None (drop-in, works with .html + .js)
Required
Required
Required
Required


Rendering Approach üé®
Direct DOM Updates (No Diffing, No Virtual DOM)
Virtual DOM diffing
Virtual DOM diffing
Change detection via zones
Precompiled updates (no VDOM, still re-renders)


Performance ‚ö°
Ultra-Fast (Only Updates What‚Äôs Needed, No Extra Work)
Good, but reconciliation overhead
Good, but reconciliation overhead
Heavy watchers/zones
Fast, but dependencies rerender


Memory Usage üß†
Low (No Virtual DOM, Minimal Garbage Collection)
Higher (VDOM objects + GC)
Higher (VDOM overhead)
Higher (framework runtime)
Lower than React, some overhead


State Management üì¶
Q.Streams + Events (lightweight, no reconciliation)
React state / Redux / Context
Vuex / Pinia
Services
Reactive stores


SSR & Hydration üåç
Pre-renders HTML + activates Tools dynamically
Hydrates VDOM (slower)
Hydrates VDOM
Hydrates Angular components
Needs hydration after precompile


Component Model üß©
Q.Tools (behaviors on any DOM element)
JSX + Hooks
Directives + templates
Components + decorators
Compiled components


Interactivity & Events üé≠
Direct event binding (auto-cleans on removal)
Event handlers in JSX (hook dependencies)
Event handlers in templates
Angular event bindings
Reactive bindings


Batch Updates üöÄ
Efficient (requestAnimationFrame + .rendering())
setState batching
NextTick batching
Zone-based batching
Dependency-based, no explicit batching


Lazy Loading üí§
Built-in (images, tools, components auto-lazyload)
Needs 3rd party libs
Needs 3rd party libs
Built-in, but heavy
Manual setup


Internationalization üåê
Built-in (Q.Text)
3rd party
3rd party
i18n module
3rd party


Incremental Adoption üîå
Yes (drop-in, enhance existing HTML without rewrite)
No
No
No
No


SEO & Progressive Enhancement üîç
Works with static HTML (enhances dynamically)
Needs JS hydration
Needs JS hydration
Needs JS hydration
Needs JS for interactivity


Ecosystem Dependence üåê
All-in-one (routing, templates, events, batching, caching built-in)
Needs Router, Redux, i18n, etc.
Needs Vuex, Router, i18n
Huge framework but still many extra libs
Needs Kit/Sapper + libs


Learning Curve üìö
Simple (declarative, minimal magic)
Medium-high (hooks, context, JSX)
Medium (directives, reactivity caveats)
High (decorators, DI, RxJS)
Medium


Best For ‚úÖ
High-performance apps, real-time dashboards, low-latency UI, social platforms
Full-scale apps, large component hierarchies
Small-to-medium apps, good DX
Enterprise-scale apps
Small-to-medium apps, hobby projects



Overview
üìù Templates
You can dynamically create elements in a React-like way, such as this:
Q.element('div', {id: "foo", "class": "bar baz"}, [
   Q.element('img', {src: "foo.png"}),
   Q.element('img', {src: "bar.png"})
]); 
Instead, you can define Handlebars templates like this:
Q.Template.set("Namespace/some/name", `put your template here`);
And then render them later like this:
Q.Template.render("Namespace/some/name").then(html => Q.replace(element, html));
Rendering templates that haven't been set yet causes requests to autoload from inside Q.Template.load.options.dir
üõ†Ô∏è Tools
Tools represent re-usable components in Q.js -- just like in other front-end libraries, except Tools are only one part of a unified framework!
Here's how to define new types of Tools. Normally you'd define each one in its own file:
File: Namespace/js/tools/cool/name.js:
Q.Tool.define("Namespace/cool/name", constructor, defaultOptions, methods);
File: Namespace/js/tools/another.js:
Q.Tool.define("Namespace/another", function (options) {
  this.refresh(); // call method of tool
}, {
  x: 1,
  y: 2
}, {
  refresh: function () {
    this.state; // copy of options
    this.element; // the element it was activated on
    this.renderTemplate('Namespace/another/view', this.state,
      function (html, elements, tools) {
        // now this.x and this.y point to elements from
        // the template that was rendered, while
        this.element.forEachTool('Namespace/cool/name', function () {
            // run whenever a child tool activates
        });
      }, {
        some: options,
        'Namespace_cool_name_tool': {
            some: childToolOptions
        }
    });
    // this is how we handle in-place updates if x or y changes:
    this.rendering([x, y], (changed, previous, timestamp) => {
      Q.replace(this.elements.x, x); // very quick
      Q.replace(this.elements.y, y); // very quick
      this.element.addClass('updated_flash'); // some CSS effect
    });
    // to trigger these, anyone can simply call tool.stateChanged('x')
  },
  Q: {
    onInit: function () {
      // all child tools have been initialized
    }
    beforeRemove: function () {
      // cleanup, but see Events section!
    }
  }
});

// define a template with a child tool
Q.Template.set("Namespace/another/view",
   `<span class="Namespace_another_x">{{x}}</span>
    <span class="Namespace_another_y">{{y}}</span>
    {{{tool "Namespace/cool/name" "some-child-id" x=x y=3 z="foo"}}}`,
   {
      "elements": {
         "x": ".Namespace_another_x",
         "y": ".Namespace_another_y"
      }
   }
);
Optionally, you can also define tools in HTML files, Vue-style, which may result in nicer
syntax highlighting for both the HTML templates and the Javascript:
<template id="Namespace/another/view">
   <span class="Namespace_another_x">{{x}}</span>
</template>
<script>
Q.Tool.define("Namespace/another/x", constructor, defaultOptions, methods);
// you can Q.Template.render("Namespace/another/view") as above
</script>
How to include tools in HTML:
<div class="Q_tool Namespace_tool_name Namespace-another-tool"
  data-namespace-cool-name='{
    "some": { "options": "go"}, "here": 2
  }'
  data-namespace-another='{
    "some": [ "other", options", here" ]
  }'
>
   <p>optionally might give the tool initial content</p>
</div>
How to prepare tools in JS:
// new component:
Q.Tool.prepare('div', toolName, options); // new div
// or add behaviors to existing element:
Q.Tool.prepare(element, toolName, options);
Q.Tool.prepare(element, anotherTool, otherOptions);
Multiple Tools can be defined on the same element.
The Q.activate() function uses Q.find() to recursively find all elements with Q_tool CSS class, and then
activates the tools in the order they've been defined.
Tools are activated parent->child->grandchild, and then initialized grandchild->child->parent.
You can pass options at activation-time, too, targeting tools by .classname or #id:
Q.activate(element, {
  '#specific_id': {
    "override": { "some": "options" },
  },
  '.Namespace_another_tool': {
     "some": ["more", "options" ]
  }
}); // finds and activates tools, then initializes them
Whenever you call Q.activate(document.body) it will traverse the whole document body, so it's slightly faster to call
Q.activate() on containers where you've recently replaced HTML, rather than the whole document body.
You can remove tools manually before removing elements:
Q.Tool.clear(container); // child elements only
Q.Tool.remove(element); // also on element itself
but you usually don't need to, because Q.replace(element, source) does this for you automatically.
The source can be an element, document fragment, or some HTML string.
Some tools might have data-q-retain attributes, causing them to be retained, and not removed and re-activated, if the incoming
HTML has the same tool IDs. When a tool is retained, its tool.Q.onRetain event is triggered.
However, incoming tools with a data-q-replace attributes replace even tools that had data-q-retain set.
Q.js even overrides $.fn.html() and $.fn.activate() in libraries like jQuery and $cash so that you can call the following
to automatically cause old tools to be removed and new tools to be activated inside the element:
$(element).html(html).activate();
When tools are removed, all associated event handlers are removede automaitically! (See Events section below).
More information: https://qbix.com/platform/guide/tools
üìÑ Pages
While Tools are reusable components, the concept of Pages is tied to HTTP resources and URLs in browsers.
Q.Page class can be used to manage pages and browser page history, loading and unloading their contents, stylesheets, etc.
This happens automatically when you call Q.handle(url) or Q.loadUrl(url).
The server-side can send a JSON payload with keys like scripts, stylesheets, etc. and the framework will
call Q.addScript() and Q.addStylesheet() to add any new scripts and stylesheets, and Q.removeStylesheet() for any
stylesheets that are not in the new page.
Pages can be divided into named slots (e.g. "navigation", "content") so that only parts of a page are requested from the server.
The server typically responds with JSON containing slots, which is a map of {slotName: html} pairs, containing new HTML content.
Then Q.replace() is used on slots that should be replaced, and finally Q.activate() is called to activate any new tools.
Here is how you add code to run when a page loads and before it unloads:
Q.page('Namespace/action', function () {
  // runs when page is loaded
  return function () {
    // runs before page is unloaded
  };
});

Q.Page.onLoad and Q.Page.onUnload are events that occur when pages are loaded an unloaded.
Q.Page.push(), Q.Page.pop() and Q.Page.currentUrl works with browser history.
More information: https://qbix.com/platform/guide/pages
‚è∞ Events
Add onFoo: new Q.Event() as properties on any object. By convention, the properties are called onFoo, and optionally beforeFoo (before the event occurs).
To trigger an event, call Q.handle(event, context, arguments). This will call all the handlers set on the event. If any of the handlers returns false then all subsequent handlers are skipped and Q.handle returns false. While being handled, event.occurring = true, and afterwards, event.occured = true.
Use onFoo.set(handler, key) to set a handler on an event, or onFoo.add(handler, key) to set it but also call it, if the event already .occurred === true.
You can call onFoo.remove(key) to remove handlers previously set for that key. Or call onFoo.removeAllHandlers() to remove all handlers for that event.
Authors of new Tools and Pages use Q.Tool.define(name, constructor) and Q.page(name, constructor).  During the lifetime of a tool or page, outside code may add handlers to some events associated with those tools or pages. Normally, they'd need to follow it up with code to manually remove those events when the tool is removed or page is unloaded, i.e. during tool.Q.beforeRemove and Q.Page.beforeUnload events, respectively.
However, Q.js has a great way to automate the removal of events. Simply pass the tool or true instead of the key, as follows:



Call Type
Description




event.set(handler, string)
need to manually call event.remove(key)


event.set(handler, tool)
automatically removed when tool is removed


event.set(handler, true)
automatically removed when current page is unloaded


$cash.on(event, tool, handler)
automatically calls .off() when tool is removed


$cash.on(event, true, handler)
automatically removed when current page is unloaded



Calling set() or add() again with the same String key replaces previous handlers set with that same key. But if the key is not a String (i.e. it's a Q.Tool or true) then the handlers are added to the existing ones, while event.remove(tool) event.remove(true) removes everything added for that tool or current page.
You can define your own event factory very easily using the Q.Event.factory() function. The Event Factory pattern is used in order to create events on demand. For example, here we call an event factory to produce an event and add a handler to be run whenever a tool is activated
Q.Tool.onActivate(toolType) // retrieves or creates new Q.Event
   .add(handler, "MyModule"); // adds the handler with a key
Q.Event also supports Observable Streams / Reactive Events, through methods like



method
the new event




a.and(b)
occurs when both occurred`


a.or(b)
occurs when either occurred


a.stop()
indicates event won't be occurring anymore


a.until(b)
occurs when a occurs, until b starts occurring


a.then()
occurs only after a stops occurring


a.filter(test)
occurrs only if test returns true


a.map(transform)
occurrs after transforming parameters


a.debounce(ms)
occurs only during a pause in a being fired


a.throttle(ms)
occurs when a occurs, but at most once every ms


a.queue(ms)
queues up occurrences to happen every ms



More information: https://qbix.com/platform/guide/eventsClient
‚öôÔ∏è Methods
There are multiple ways to autoload external tools on demand.
// Analogues of node.js modules
Q.exports(function () { ... }); // in a file
Q.require(src, callback); // loading the file
Q.import(src).then(...); // wrapper around native import
But also, any asynchronous methods that utilize a callback or promise can be autoloaded:
Q.Data = Q.Method.define({
  all: function (a, b) {
    // regular method
  },
  digest: new Q.Method(),
  compress: new Q.Method(),
  decompress: new Q.Method(),
  sign: new Q.Method(),
  verify: new Q.Method(),
}, "Q/Data", function() {
  // pass variables in a closure
  return [Q, something];
}));
and then you can define the methods in files like Q/Data/digest.js:
Q.exports(function (Q, something) { // receive closures from main file
    return function Q_Data_digest(algorithm, payload, callback) {
        // here you have access to both the parameters and the closures!!
        return doStuff.then(function (result) {
            callback && callback(null, result); // callback interface
            return result; // promise interface
        });
    };
});
You should consider using this extensively, to organize your front-end code and load only as needed:
Q.Data.sign(algorithm, payload) // autoload method's code on demand
.then(...) // continue after promise resolves
üìÇ Objects
Use Q.copy(object, fields, levels) to copy an object. Sub-objects may expose custom .copy() methods to be used.
Do Q.extend(target, levels, obj1, obj2) method is used to modify target, adding from extension.
Do result = Q.extend({}, defaults, levels, obj1, obj2) to overrides some defaults with specific values.
You can extend Q.Event objects, Q.extend(target.onFoo, {"bar": handler)) will call target.onFoo.set(handler, bar).
This allows you to pass custom event handlers that will be added to a copy of an event:
Q.Tool.define("Foo/bar", function (options) {
   this.state // this is Q.extend({}, defaults, options);
}, {
  foo: ['a', 'b'],
  bar: ['a', 'b'],
  onFoo: new Event(function () {
     // default handler here
  }, "Foo/bar"), // with sensible key name
  childToolOptions: {
    onBaz: new Event() // empty event
  }
});
Every instance of the Foo/bar tool actually makes a copy of the default options, and then extends them, saving the result
in tool.state, ready to use. So suppose you instantiated the tool as follows:
const element = Q.Tool.prepare('div', 'Foo/bar', {
   // override some options when making element
   foo: ['c', 'd'],
   bar: {replace: ['c', 'd']},
   onFoo: {
     "Foo/bar": function () {
       // override default handler, but only for this instance
     }
   }
});
document.body.appendChild(element);
Q.activate(element, {
  // override more options at activation time
  childToolOptions: {
    onBaz: {
      "myCoolPage": function () {
         // add this handler, but only for this instance
      }
    }
  }
});
Other tool instances won't have these handlers added. They'll have a copy of the default options, including the event objects, that wasn't extended with these.
Also notice what Q.extend() does with arrays. The foo above would become ['a', 'b', 'c', 'd'] while the bar would become ['c', 'd'], because the array was being replaced by an object with a key "replace", so it replaces the array with the given value (another array).
‚ûï Functions
By convention, methods that take options have default options defined in the options property on the method itself.
When called, a copy of the default options extended with any options passed to the method. Example:
A.method = function (a, b, options) {
   const o = Q.extend({}, A.method.options, 10, options);
}
A.method.options = {
   some: {
     "default": options
   },
   many: "levels"
}

You've already seen this in action above with tools, but you're highly encouraged to use this pattern with any functions
or methods that take an object of options. It's a unified way to override default values, including events.
üöÄ Putting It All Together
üì¶ Main Module
When you write an app or a plugin, you'll probably want to have Javascript file that acts as your main module.
In it, you will define the tools, methods, and other things. Here is an example:
/**
 * Streams plugin's front end code
 *
 * @module Streams
 * @class Streams
 */
"use strict";
(function(Q, $) { // $ can be jQuery, $Cash, in any case it's optional

  // defaults are in english, but you can override with Q.Text.get below
  // by convention, modules usually store all user-facing text in Q.text:
  Q.text.Streams = {
    onboarding: {
      prompt: "Fill our your basic information to complete your signup.",
      title: "Basic Information"
    }
  };

  // set default text file for tools and templates
  // with names that start with "Streams/"
  Q.Text.addFor(
    ['Q.Tool.define', 'Q.Template.set'],
    'Streams/', ["Streams/content"]
  );

  Q.Tool.define({
    // specify js and css
    "Streams/chat": {
      js: "{{Streams}}/js/tools/chat.js",
      css: "{{Streams}}/css/tools/chat.css"
    },
    // or define tools as html file (Vue-style)
    "Streams/comments": {
      html: "Streams/html/tools/comments.html"
    }
    // override another module's tool:
    "Users/avatar": "{{Streams}}/js/tools/avatar.js",
  });

  Q.onInit.add(function _Streams_onInit() {
    Q.Text.get('Streams/content').then(text => {
      Q.extend(Q.text.Streams, 10, text);
    });
  });
})(Q, jQuery);
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Red: A programming language inspired by REBOL]]></title>
            <link>https://github.com/red/red</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45080051</guid>
            <description><![CDATA[Red is a next-generation programming language strongly inspired by Rebol, but with a broader field of usage thanks to its native-code compiler, from system programming to high-level scripting and c...]]></description>
            <content:encoded><![CDATA[



Red Programming Language

  

Red is a programming language strongly inspired by Rebol, but with a broader field of usage thanks to its native-code compiler, from system programming to high-level scripting, while providing modern support for concurrency and multi-core CPUs.
Red tackles the software building complexity using a DSL-oriented approach (we call them dialects) . The following dialects are built-in:

Red/System: a C-level system programming language compiled to native code
Parse: a powerful PEG parser
VID: a simple GUI layout creation dialect
Draw: a vector 2D drawing dialect
Rich-text: a rich-text description dialect

Red has its own complete cross-platform toolchain, featuring an encapper, a native compiler, an interpreter, and a linker, not depending on any third-party library, except for a Rebol2 interpreter, required during the alpha stage. Once 1.0 is reached, Red will be self-hosted. Currently, Red is still at alpha stage and 32-bit only.
Red's main features are:

Human-friendly syntax
Homoiconic (Red is its own meta-language and own data-format)
Functional, imperative, reactive and symbolic programming
Prototype-based object support
Multi-typing
Powerful pattern-matching Macros system
Rich set of built-in datatypes (50+)
Both statically and JIT-compiled(*) to native code
Cross-compilation done right
Produces executables of less than 1MB, with no dependencies
Concurrency and parallelism strong support (actors, parallel collections)(*)
Low-level system programming abilities through the built-in Red/System DSL
Powerful PEG parser DSL built-in
Fast and compacting Garbage Collector
Instrumentation built-in for the interpreter, lexer and parser.
Cross-platform native GUI system, with a UI layout DSL and a drawing DSL
Bridging to the JVM
High-level scripting and REPL GUI and CLI consoles included
Visual Studio Code plugin, with many helpful features
Highly embeddable
Low memory footprint
Single-file (~1MB) contains whole toolchain, full standard library and REPL (**)
No install, no setup
Fun guaranteed!

(*) Not implemented yet.
(**) Temporarily split in two binaries
More information at red-lang.org.
Running the Red REPL
Download a GUI or CLI console binary suitable for your operating system, rename it at your convenience, then run it from shell or by double-clicking on it (Windows). You should see the following output:
    ---== Red 0.6.5 ==--
    Type HELP for starting information.

    >>

A simple Hello World would look like:
    >> print "Hello World!"
    Hello World!

If you are on the GUI console, a GUI Hello World (prompt omitted):
    view [text "Hello World!"]


  

A more sophisticated example that retrieves the last commits from this repo and displays their log messages in a scrollable list:
    view [
        text-list data collect [
            foreach event load https://api.github.com/repos/red/red/commits [
                keep event/commit/message
            ]
        ]
    ]


  

Note: check also the following improved version allowing you to click on a given commit log and open the commit page on github.
You can now head to see and try some showcasing scripts here and there. You can run those examples from the console directly using Github's "raw" link. E.g.:
    >> do https://raw.githubusercontent.com/red/code/master/Showcase/calculator.red

Note: If you are using the Wine emulator, it has some issues with the GUI-Console. Install the Consolas font to fix the problem.
Generating a standalone executable
The Red toolchain comes as a single executable file that you can download for the big-3 platforms (32-bit only for now). Rename the file to redc (or redc.exe under Windows).


Put the downloaded redc binary in the working folder.


In a code or text editor, write the following Hello World program:
 Red [
     Title: "Simple hello world script"
 ]

 print "Hello World!"



Save it under the name: hello.red


Generate a compiled executable from that program: (first run will pre-compile libRedRT library)
 $ redc -c hello.red
 $ ./hello



Want to generate a compiled executable from that program with no dependencies?
 $ redc -r hello.red
 $ ./hello



Want to cross-compile to another supported platform?
 $ redc -t Windows hello.red
 $ redc -t Darwin hello.red
 $ redc -t Linux-ARM hello.red



The full command-line syntax is:
redc [command] [options] [file]

[file] any Red or Red/System source file.

The -c, -r and -u options are mutually exclusive.

[options]
-c, --compile                  : Generate an executable in the working
                                 folder, using libRedRT. (development mode)

-d, --debug, --debug-stabs     : Compile source file in debug mode. STABS
                                 is supported for Linux targets.

-dlib, --dynamic-lib           : Generate a shared library from the source
                                 file.

-e, --encap                    : Compile in encap mode, so code is interpreted
                                 at runtime. Avoids compiler issues. Required
                                 for some dynamic code.

-h, --help                     : Output this help text.

-o <file>, --output <file>     : Specify a non-default [path/][name] for
                                 the generated binary file.

-r, --release                  : Compile in release mode, linking everything
                                 together (default: development mode).

-s, --show-expanded            : Output result of Red source code expansion by
                                 the preprocessor.

-t <ID>, --target <ID>         : Cross-compile to a different platform
                                 target than the current one (see targets
                                 table below).

-u, --update-libRedRT          : Rebuild libRedRT and compile the input script
                                  (only for Red scripts with R/S code).

-v <level>, --verbose <level>  : Set compilation verbosity level, 1-3 for
                                 Red, 4-11 for Red/System.

-V, --version                  : Output Red's executable version in x.y.z
                                 format.

--config [...]                 : Provides compilation settings as a block
                                 of `name: value` pairs.

--no-compress                  : Omit Redbin format compression.

--no-runtime                   : Do not include runtime during Red/System
                                 source compilation.

--no-view                      : Do not include VIEW module in the CLI console
                                 and the libRedRT.

--view <engine>                : Select the VIEW engine (native, terminal, GTK, test)

--red-only                     : Stop just after Red-level compilation.
                                 Use higher verbose level to see compiler
                                 output. (internal debugging purpose)

--show-func-map                : Output an address/name map of Red/System
                                 functions, for debugging purposes.

[command]
build libRed [stdcall]         : Builds libRed library and unpacks the
                                 libRed/ folder locally.

clear [<path>]                 : Delete all temporary files from current
                                 or target <path> folder.

Cross-compilation targets:
MSDOS        : Windows, x86, console (+ GUI) applications
Windows      : Windows, x86, GUI applications
WindowsXP    : Windows, x86, GUI applications, no touch API
Linux        : GNU/Linux, x86, console (+ GUI) applications
Linux-GTK    : GNU/Linux, x86, GUI only applications
Linux-musl   : GNU/Linux, x86, musl libc
Linux-ARM    : GNU/Linux, ARMv5, armel (soft-float)
RPi          : GNU/Linux, ARMv7, armhf (hard-float)
RPi-GTK      : GNU/Linux, ARMv7, armhf (hard-float), GUI only applications
Pico         : GNU/Linux, ARMv7, armhf (hard-float), uClibc
Darwin       : macOS Intel, console-only applications
macOS        : macOS Intel, applications bundles
Syllable     : Syllable OS, x86
FreeBSD      : FreeBSD, x86
NetBSD       : NetBSD, x86
Android      : Android, ARMv5
Android-x86  : Android, x86

Note: The toolchain executable (redc.exe) relies on Rebol encapper which does not support being run from a location specified in PATH environment variable and you get PROGRAM ERROR: Invalid encapsulated data error. If you are on Windows try using PowerShell instead of CMD. You can also provide the full path to the executable, put a copy of it in your working folder or wrap a shell script (see relevant tickets: #543 and #1547).
Running Red from the sources (for contributors)
The compiler and linker are currently written in Rebol. Please follow the instructions for installing the compiler toolchain in order to run it from sources:


Clone this git repository or download an archive (ZIP button above or from tagged packages).


Download a Rebol interpreter suitable for your OS: Windows, Linux (or Linux), Mac OS X, FreeBSD, OpenBSD, Solaris.


Extract the rebol binary, put it in the root folder, that's all!


Let's test it: run ./rebol, you'll see a >> prompt appear. Windows users need to double-click on the rebol.exe file to run it.


From the REBOL console type:
 >> do/args %red.r "%tests/hello.red"



The compilation process should finish with a ...output file size message. The resulting binary is in the working folder. Windows users need to open a DOS console and run hello.exe from there.
You can compile the Red console from source:
    >> do/args %red.r "-r %environment/console/CLI/console.red"

To compile the Windows GUI console from source:
    >> do/args %red.r "-r -t Windows %environment/console/GUI/gui-console.red"

Note: the -c argument is not necessary when launching the Red toolchain from sources, as the default action is to compile the input script (the toolchain in binary form default action is to run the input script through the interpreter).
The -r argument is needed when compiling the Red console to make additional runtime functions available.
Note: The red git repository does not include a .gitignore file. If you run the automated tests, several files will be created that are not stored in the repository. Installing and renaming a copy of .git/.gitignore-sample file will ignore these generated files.
Contributing
If you want to contribute code to the Red project be sure to read the guidelines first.
It is usually a good idea to inform the Red team about what changes you are going to make in order to ensure that someone is not already working on the same thing. You can reach us through our chat room.
Satisfied with the results of your change and want to issue a pull request on Github?
Make sure the changes pass all the existing tests, add relevant tests to the test-suite, and please test on as many platforms as you can. You can run all the tests using (from Rebol console, at repository root):
    >> do %run-all-tests.r

Git integration with console built from sources
If you want git version included in your Red console built from sources, use this command:
call/show ""                                              ;-- patch call bug on Windows
save %build/git.r do %build/git-version.r                 ;-- lookup git version if available
do/args %red.r "-r %environment/console/CLI/console.red"  ;-- build Console
write %build/git.r "none^/"                               ;-- restore git repo status
Anti-virus false positive
Some anti-virus programs are a bit too sensitive and can wrongly report an alert on some binaries generated by Red (see here for the details). If that happens to you, please report it to your anti-virus vendor as a false positive.
License
Both Red and Red/System are published under BSD license, runtime is under BSL license. BSL is a bit more permissive license than BSD, more suitable for the runtime parts.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My phone is an ereader now]]></title>
            <link>https://www.davepagurek.com/blog/minimal-phone/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45079962</guid>
            <description><![CDATA[August 30, 2025]]></description>
            <content:encoded><![CDATA[


  



  August 30, 2025

  


  I got a Kobo in 2016 after borrowing my mom's old one for a year before that. It probably is responsible for getting me reading again after high school. I used to be an avid reader, the sort of kid who would have to be told to put down the book and go to sleep, and who would then creep slowly to the bookshelf to pick it up again without arousing suspicion after the light had been turned out. I think I slowed my reading for fun as the work load of school increased, and stopped when moving every four months for internships in university. Having something small and portable that I could load books onto changed that and got my momentum going again. I now loosely grade how much I'm thriving by how much I'm reading, as an indirect indicator of how not burnt out I am.It fared me quite well, but I had a few issues with it. Library books would randomly not work on it, even if they would show up on, for example, the Libby app on my phone. It also came with a way to sync articles to it via Pocket, but it always required a little too much forethought for me: I had to remember to find and save articles beforehand in order to then read them later. There were some services to do this automatically via RSS but the syncing process itself was just slow enough that I found myself rarely doing it. Also, Mozilla has now killed Pocket as a service. In the middle of writing this, they announced support for Instapaper instead, but it has the same workflow issues for me. My partner reads on her phone, but something about reading on a screen grates on me after a while, and makes it too easy to jump to something else.So I was intrigued when I heard about the Minimal Phone, an Android phone with an epaper display. It wasn't the first epaper Android device I'd seen‚ÄîI've seen reviews saying the Boox Palma is actually pretty great‚Äîbut it was the idea of this being an actual phone that can take a sim card that really get me interested. What if I could read the news and blogs on what looks like paper while commuting, without having the forethought of downloading or syncing something? I might otherwise spend that time staring into space or looking at nothing on Bluesky. I'd probably rather be reading a bit of a book, or other longer-form writing. So I ordered one as a gift to myself.The MP01I figured I might be a good fit for this device. I don't really watch videos on my phone. I send messages a bit, but not urgently. Most of the time I'm at or near a full keyboard anyway. I take some photos, but not that many any more. I feel like the photo winds changed for me sometime in university and I now feel weird posting Nice Photos to social media. Who are those for, really? I now send quick photos directly to friends mostly, and they don't have to be print quality or anything. They just have to be visible.With that in mind, I went in treating it like an experiment. I still have my Kobo that has its annoyances but works. I still have a fully functional Pixel 8 phone. I don't need this to work. At worst, this could just be an alternate ereader for me. So when it arrived mid-July, I started testing it full-time to see how it'd go, with my normal phone in my bag just in case.Overall, I actually really like it! I absolutely would not recommend this device to everyone‚ÄîI'll get into why later‚Äîbut it's been working pretty well for me. How the Minimal Phone worksThis phone is around the same size of my Pixel 8. It's just a tad shorter and just a tad wider. I don't really feel the shortness, but I do feel the wideness a bit, which makes it more comfortable to read on. The bottom third of the height is taken up by a physical keyboard, and the top two thirds are an epaper display.It's just Android under there, with a black-and-white epaper display. It comes with a few launchers, and I use one that works like a pretty traditional launcher, but comes with some built in icon choices that look sharp on the display.There's a side button between the phone's volume keys that you can tap to flash the display to clear ghosting. I don't find myself doing this often‚Äîghosting is not that bad‚Äîbut if you press and hold it, it opens the display settings. This is something I do all the time.The quick display settings screen, which you get to by pressing and holding the button between the volume keys.From the settings screen, you can turn on and off the light on the display and on the keyboard, and also change the display light's colour temperature. I mostly leave those off; I only need those if I'm outside after dark, and the controls are big enough that I can turn them on easily enough in low light.The most important setting is the refresh rate at the bottom. The slowest setting has the slowest refresh rate, but the highest quality visuals: always showing nice shades of grey, and with less ghosting but more flashing as it updates. The fastest setting (which, to be clear, is still not very fast) has much less flashing, a little more ghosting, and dithers pure black and white rather than showing any shades of grey. The middle setting, "hybrid" mode, is a combination of the two: it uses the faster setting while things are moving onscreen, and then updates to the slower, higher quality render when movement stops. I generally keep the phone in this hybrid mode, except for a few specific cases.The keyboard feels pretty good, and it's a comfortable size to type on with two thumbs. I can't really one-handed type on this phone; it's a tad too wide for that, but the width is worth it for easier reading. I really appreciate them including the keyboard here, as the display looks great but is definitely not all that responsive, so typing would be a lot more frustrating without this. The great partsThis thing is so nice to read on.I hate reading on screens. Something about dark mode especially messes with my eyes, but even without that, I've never enjoyed reading articles on my phone. Too easy to get distracted, the minor eye strain... This device though, the epaper display looks great. It's not especially high resolution or anything, but I could spend a long time reading on this without issue. I just spent two flights (Toronto to Vancouver and back again) just reading books on this, and I'd do it again. It's really crisp and visible in the sun too.A page of a book in the Libby app.It's super easy to queue up library book holds and read them all from the phone. I have had zero issues with that. Being able to add new things on-the-go has also made it really easy to grab another book on the spot once I finish one. I definitely have found myself reading more books this past month and a half.I also now am more likely to read people's blogs on an RSS reader than scroll through social media. I wasn't setting out to fully purge social media or anything, but I certainly feel a little more fulfilled after reading something that someone has clearly put time and effort into.Possibly as a consequence of the display technology, I also generally get 2 days of usage out of a charge. Most days I finish with 70% battery remaining, letting me go another day with some buffer room. On some really low usage days, I could maybe even go more, but already this is great. On a high usage day, I'll maybe end with 50%, which is still fine by me.This is secondary, by far, but I also feel now that I can fully turn off autocorrect, as this phone has a physical keyboard. Most of the time (with important caveats), I don't make typos. So I no longer have to suffer through autocorrect changing programming terms (which I still type a lot of), changing my capitalization, or doing its own insane capitalization (why would it format "city Hall" with just one capital? Commit to capitals or no capitals, don't do this awkward mix!)An article on The Verge. Hey, it was the top post in the feed when I took the photo!As another minor note, the fingerprint reader is actually quite fast. When it remembers my fingerprints, it's super reliable. ("What do you mean, when it remembers?" I'll get into it later, there's a pretty bad bug here. But in regular usage, it really does work well.)Everything else this phone does, it does a little worse than a normal phone, but not so much worse that it's a problem. I assume it would be a lot worse at watching videos but I never really did that much on my old phone anyway. So on the whole, this phone works really well where I want it to, and generally gets out of my way for the usual stuff. I keep using it without really worrying about it.The camera, once set up properly, is pretty passable. Well, the selfie camera is in a super awkward spot, but I don't really find myself using it anyway. But other photos look decent enough that I'm not embarrassed to send them to people!  My cat Pigeon looking out the window.  Toronto in the summer.  The selfie camera is a little sketchy, I wouldn't rely on it.  Phil Wizard breaking on Kits Beach in Vancouver. Growing painsEven though I do really like this thing, and am continuing to use it as my primary device, there are a lot of rough edges. This device is made by, primarily, two people (although they've been adding more developers in the past few weeks), so naturally there will be a lot of rough edges. You have to be willing to accept that if you're going to use this phone. They do make updates, but the pace is slow, and they are definitely bogged down by customer support and shipping/manufacturing logistics, so you need to not bank on fixes happening quickly.There's a double-tap-to-wake feature that you can't turn off, and it takes a sec once locked to stop responding to inputs. Consequently, I now put this in my pocket with the display facing out, which is opposite of what I used to do, in order to prevent accidentally disturbing it in my pocket. Doing that, I haven't had issues, but it's an adjustment you have to make for this phone right now.There are a few things you'll probably need to do to the device to make it work well for you. One of them involves the camera. By default, the camera super aggressively denoises its photos, resulting in images that look like they came off of my flip phone from 2008. However, if you use the Open Camera app, switch it to use the Camera 2 API, it then lets you turn off noise reduction in the settings. The resulting images look much crisper, and do have noise, but a tasteful‚Äîdare I say aesthetic?‚Äîamount of noise. There is no Pixel-style HDR in these photos, but now that that look is everywhere, the resulting photos are... kind of refreshing.Taking a photo of fast a moving subject is quite hard on this thing due to the refresh rate of the screen. But then again, doesn't a photo like this capture the moment better? This is my aunt's cat Lexi.The phone also uses something called Duraspeed to aggressively turn off background apps. This works well in general, but it also can stifle some notifications that you do want, and also can affect background audio. I know some people fully turn Duraspeed off, but I've just turned it off for my messaging apps and my music/podcast apps. I've had no notification or background process related issues since doing so.I also found that the backlight was way too bright, and I didn't really want any lights on most of the time anyway. I found that when opening the display settings, it'd turn all the lights back on. But if I save a preset, then it'd stick. You can do that by changing the settings, and then pressing and holding on the wrench icon to save it to your custom preset.Finally, the hybrid refresh mode needs things to stop moving in order to lock in on a higher quality render. That means animated ads are somehow even more annoying than they normally are. Thankfully, Firefox for Android lets you install addons, such as uBlock Origin, to deal with that. BugsThe most annoying bug is that this phone will occasionally restart and forget your fingerprint, forcing you to enter your PIN. I don't know why this happens. I can go for a few weeks with it working fine, and then it'll just forget. I can still get in with the PIN, so it's not locking me out, but there's really never a good time to re-set up a fingerprint, and typing a PIN on the onscreen display is slow and cumbersome. This is the bug I hope gets fixed the most.Another bug has something to do with the screen refresh rate, and something to do with responding to keyboard input. If you're on a slower refresh rate and are typing quickly, sometimes it misses keypresses, and you have to go back and fix things. This is also quite annoying, but doesn't seem to happen on the highest screen refresh rate. As a workaround, when I'm sending messages, I switch to the fastest refresh setting. This one-or-the-other approach isn't great though (I still want photos sent to me in messaging apps to look nice!), so I'd love to see that improved over time.I also have to use the phone in the lowest refresh rate for Google Maps in order to see the streets on the map. The color scheme is just too low contrast for the high refresh rate's dithering. The hybrid setting doesn't work either: your location on the map is always slightly moving and so it never locks in and renders a higher-quality image. Arguably, this is a problem with Google Maps because they don't have a high contrast mode. Surely that would have accessibility benefits beyond just this weird device!Google Maps when in hybrid or fast mode. Where did the streets go??There's a software update that the Minimal team has been working on for almost two months that will apparently address the fingerprint forgetting issue, make double-tap-to-wake optional, significantly increase the refresh rate on the fast refresh mode, and let you save per-app refresh rate settings. That'll address some of my problems for sure! But it also hasn't shipped yet. To use this device is an exercise in patience, and being accepting of imperfections. Feature RequestsNone of these are dealbreakers for me, but here's what I'm hoping to see in the future:I feel like the vibration on the phone is a tad aggressive. Not every vibration is, though‚ÄîFacebook Messenger notifications feel like the right level. I'd love to be able to adjust the cap for vibration intensity!I would love emoji search in the keyboard. But I also don't use that many different emoji or symbols, and by now the ones I do use are in the recents list, so it's fine. But the one time I need to use a weird one, it'd be nice to have!I wish the hybrid refresh mode would work well with camera apps. As it is, I think too much of the screen is updating at once, so it flashes a large part of the display every frame, making it really hard to see. If I put the phone in fast mode, there's no flashing, which is great! But then when I take a photo and tap on the thumbnail to see it, I have to switch back to hybrid or slow mode to see a clearer, non-dithered version. This is a little annoying, and I feel could be improved, but then again I'm not really using this to take a lot of photos anyway. Concluding the experimentIt's been more than a month, and despite not everything being perfect, I'm going to continue using this phone. I do occasionally switch to my Pixel 8 though. I use my Pixel 8 for running for its better waterproofing. When I needed to get actual good, postable photos from SIGGRAPH two weeks ago, I just used my Pixel 8. When seeing LCD Soundsystem last weekend, rather than worry about weirdness with the Ticketmaster app, I just took my Pixel 8. But I've used normal boarding passes for airplanes on my MP01, and I regularly go out without a backup phone. I do mostly rely on my partner to do Google Maps navigation since that's a little bit smoother, although in a pinch I can still use it myself (and the Transit app is a little better in hybrid mode.)Basically, I use the right tool for the job, and this phone doesn't have to be that tool for all jobs. But it turns out I don't need my phone to do all that many jobs, and it's maybe a good thing for it to be doing less of them.There are enough quirks that I wouldn't automatically recommend this experience. But if you know what you're getting into and have the right expectations, this is a really great little device!
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Six months into tariffs, businesses have no idea how to price anything]]></title>
            <link>https://www.wsj.com/business/retail/trump-tariff-business-price-impact-37b630c8</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077937</guid>
        </item>
        <item>
            <title><![CDATA[Why did books start being divided into chapters? A new history]]></title>
            <link>https://sydneyreviewofbooks.com/reviews/just-a-little-longer</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077735</guid>
            <description><![CDATA[Why did books start being divided into chapters? Joshua Barnes reviews Nicholas Dames‚Äô history of literary segmentation, a study that slices through and pauses over what chapters have always told us about the times we live in.]]></description>
            <content:encoded><![CDATA[Why did books start being divided into chapters? Joshua Barnes reviews Nicholas Dames‚Äô history of literary segmentation, a study that slices through and pauses over what chapters have always told us about the times we live in.I often return to an essay by Lydia Davis about an unusual experiment in translation. Better known for her work on French writers like Gustave Flaubert, Marcel Proust, and Maurice Blanchot, Davis had in this case tried to translate a literary text, not from French but rather from English into English. The text in question was Laurence Sterne‚Äôs unfinished 1768 novel, A Sentimental Journey through France and Italy by Mr. Yorick. Even a glance at its first page suggests why the book might require translation. Here is how it begins:¬†¬†‚Äî‚ÄîThey order, said I, this matter better in France‚Äî¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ÄîYou have been in France? said my gentleman, turning quick upon me with the most civil triumph in the world.‚ÄîStrange! quoth I, debating the matter with myself, That one and twenty miles sailing, for ‚Äôtis absolutely no further from Dover to Calais, should give a man these rights‚ÄîI‚Äôll look into them: so giving up the argument‚ÄîI went straight to my lodgings, put up half a dozen shirts and a black pair of silk breeches‚Äî‚Äòthe coat I have on, said I, looking at the sleeve, will do‚Äô‚Äîtook a place in the Dover stage[.]¬†Unusual English, to say the least. Davis sought to do two things: to modernise the novel by translating its sui generis language into contemporary English; and to figure out, in the process, what exactly makes it so unusual in the first place. Guiding her translation, however, is a deeper question: Why is it that visual art from ‚Äòthe eighteenth century and further back, to the beginning of discovered painting, is readily available, at least in reproduction, and enjoyed by the general public, not just scholars or specialists‚Äô ‚Äì but not literature ‚Äòbefore, say, Jane Austen‚Äô? Literature from before 1800, Davis notes, is ‚Äòmostly unread, even by writers‚Äô, and while many English speakers will learn foreign languages, they do not also try to ‚Äòcross the barrier to James Boswell‚Äôs English, or John Donne‚Äôs, or further back to Chaucer‚Äôs or Beowulf‚Äôs‚Äô.¬†The results of this experiment are perhaps less significant than the theoretical speculation it occasions. Translating across the gulf of historical difference ‚Äì what we could call ‚Äòtemporal translation‚Äô ‚Äì might actually be difficult because ‚Äòthe barrier is something other than the language‚Äô: ‚Äòmaybe it is the sensibility or the worldview that changes too much, as we travel back in time, for us to understand it, or, if we understand it, to feel any sympathy for it‚Äô. Sterne‚Äôs novel occupies the unusual and contradictory position of being at once proto-modern (or proto-modernist) and somehow, by this very stylistic prolepsis, archaic or antiquated. It seems to shatter our so-called modern conventions before they were even created.A Sentimental Journey appears to begin in the middle of a conversation, but it is not exactly clear who is speaking or to whom they speak: dialogue is not clearly set out in quotation marks; dashes of different lengths are used expressively; and, finally, it is not organised according to legible chapters. Flipping through the first fifteen or twenty pages one sees instead repeated chapter titles: ‚ÄòCalais‚Äô; ‚ÄòThe Monk | Calais‚Äô; ‚ÄòThe Monk | Calais‚Äô (again); ‚ÄòThe Monk | Calais‚Äô (once more); ‚ÄòThe Desobligeant | Calais‚Äô; finally and somewhat belatedly, a ‚ÄòPreface | In the Desobligeant‚Äô; then, three pages later, ‚ÄòCalais‚Äô (again). Such irregular chaptering produces an irregular experience of time, hence the comedy of the moment when Sterne‚Äôs hero, Yorick, is found rocking his horse-drawn carriage by the ‚Äòagitation of writing a preface‚Äô ‚Äì a few chapters in.Sterne‚Äôs indifference to normal chaptering only throws into relief the ordinary and invisible work that chapters do as literary infrastructure. As is so often the case, one only notices a convention when it‚Äôs violated. But it is equally true of conventions that they are made; they come from somewhere. Why is it that novels have chapters at all? This is the inquiry of The Chapter: A Segmented History from Antiquity to the Twenty-First Century by Nicholas Dames, a professor of English at Columbia University who specialises in Victorian literature and culture.¬†A book on chapters! I know. Stay with me. For this apparently technical question transforms into a historical phenomenology of literary time. In this sense The Chapter continues the inquiry Dames has been carrying out for the past quarter century in his scholarly work ‚Äì the exploration of what he called, in Amnesiac Selves (2000), the ‚Äòlinguistic organization of temporal experience‚Äô, borrowing the phrase from the great German historian Reinhart Koselleck. In The Chapter, however, the scope has been radically widened, in part ‚Äì one suspects ‚Äì to make sense of the novel‚Äôs present fortunes. Dames declared in his previous book, The Physiology of the Novel (2007), the necessity of developing ‚Äònuanced and even-handed accounts of what I might call the social norms of cognition of given historical moments‚Äô, norms that are reproduced in large measure by the norms of writing. Enter the chapter. One of the basic structures of the book, the chapter is a ‚Äòbox of time‚Äô that shapes the reader‚Äôs experience of temporality. As such, changes in chaptering present one way of exploring changes in the experience of time in literary history. How did time feel in late antiquity, or in fifteenth-century Burgundy, or to a former slave at the end of the eighteenth century? Studying the chapter might also tell us something about our experience of time now, in ‚Äòthe present‚Äô ‚Äì whatever that is ‚Äì and the historical distance between our time and that of times past.¬†Sterne comes up a lot in The Chapter, partly because his experiments in self-consciousness draw attention to the chapter‚Äôs conventionality, if only in the breach. In Sterne‚Äôs better-known novel The Life and Opinions of Tristram Shandy, Gentleman (1759-67), for instance, you might catch yourself in volume four thinking you had missed something as chapter twenty-three gives way to chapter twenty-five:¬†¬†‚ÄîNo doubt, Sir,‚Äîthere is a whole chapter wanting here‚Äîand a chasm of ten pages made in the book by it‚Äîbut the book-binder is neither a fool, or a knave, or a puppy‚Äînor is the book a jot more imperfect, (at least upon that score)‚Äîbut, on the contrary, the book is more perfect and complete by wanting the chapter, than having it.¬†If the book truly is ‚Äòmore perfect and complete by wanting the chapter‚Äô, that is because Tristram Shandy is a book about failure and errancy, where experiments with form and time are manifold. Notoriously it begins with Tristram‚Äôs attempt to narrate his life, but he prevaricates so long that he fails to get to any of the key points of his personal history. The preface again arrives late, in volume two; his birth only occurs in volume four; Tristram‚Äôs very name is an error, the intended birth name being Trismegistus. And what better expression of errancy than a gap in the novelistic edifice itself? Sterne also breaks off chapter nine of volume four, as Tristram‚Äôs father walks down a flight of stairs, and asks: ‚ÄòIs it not a shame to make two chapters out of what passed in going down one pair of stairs?‚Äô There begins a ‚Äòchapter upon chapters‚Äô, which Sterne calls ‚Äòthe best chapter in my whole work; and take my word, whoever reads it, is as well employed as in picking straws‚Äô. Picking straws: the very image of contingency. But with Dames‚Äô theory of the chapter in mind, all this meddling with chapterisation, or capitulation (from the Latin capitulum, meaning ‚Äòlittle head‚Äô), is not simply literary estrangement or satire. Rather it reflects, arguably even theorises, the function of the chapter ‚Äì the presentation of time as an experience of unified discontinuity.Dames illuminates Sterne‚Äôs eighteenth-century moment as one in which the novel chapter has lost ‚Äòmuch of its original function without as yet having acquired a new one‚Äô, which makes it properly experimental. But the place of Sterne‚Äôs experiments in the history of the chapter belies the fact that Dames‚Äô history is really an attempt to describe the genesis and function of a convention in its very conventionality ‚Äì not the exceptions, but rather the rule. His attention is directed instead towards the ‚Äòusual chapter and its almost unthinking repetitions of technique‚Äô. This is a more ambitious task than it might seem. A chapter is a ubiquitous part of novelistic architecture ‚Äì so easily overlooked, as Dames notes, that it is difficult even to conceptualise as an object of inquiry. And although the chapter finds its most distinctive uses in the novel, which has the ‚Äòunique ability to [‚Ä¶] articulate how the experience of time is the experience of time‚Äôs segmentations‚Äô, it does not originate there.If the enormous scope of this book invites comparisons with Erich Auerbach, then so too does its method, which similarly offers densely suggestive examples rather than an exhaustive historical inventory. Unlike Auerbach, however, Dames‚Äô organisation of his material tends towards the taxonomic and schematising. He offers eight views of the chapter performing different functions at different historical moments: there is the ‚Äòthreshold‚Äô of the classical heading, the ‚Äòabstract syncopation‚Äô of the Gospels, the ‚Äòcut‚Äô and ‚Äòfade‚Äô of medieval prose narratives ‚Äì and so on, down to the ‚Äòpost chapter‚Äô present. This is a longstanding ‚Äòtaxonomical urge‚Äô, as Dames termed it at the beginning of Amnesiac Selves, a habit that he picked up from his objects of study. Victorian theories of mind such as phrenology, for all their notorious problems, nonetheless ‚Äòprovided [‚Ä¶] a useful interpretive model‚Äô ‚Äì in permitting its division ‚Äòinto distinct parts‚Äô, they rendered a newly ‚Äòspatialized‚Äô and ‚Äòdiagrammatic‚Äô mind that was more susceptible of analysis. But in The Chapter there is perhaps a tension between the comprehensive ambitions of this ‚Äòtaxonomic urge‚Äô, and the suggestive but partial moments of Auerbachian literary history. You could say that the book makes a methodological wager that the nearly scientific goal of taxonomy ‚Äì to encompass everything ‚Äì can effectively be grafted onto an historicist hermeneutics constantly shifting its focus from part to whole and back again.We begin in the second century BCE, with a tablet, known today as the Tabula Bembina, upon which are inscribed some Roman anti-corruption laws from the time of the Gracchi. This was a ‚Äòpublic, technical matter, by no means literary‚Äô, but for Dames it captures the chapter‚Äôs characteristic early function ‚Äì as a technology not of narrative, but of reference ‚Äì that would eventually be imported into the codex. On the tablet, ablative Latin phrases designate the topics covered in the relevant sections (de nomine deferundo iduibusque legundeis or ‚Äòconcerning prosecution and the choosing of juries,‚Äô for instance). At once ‚Äòvisual and analytic‚Äô, these create a sort of resting place for the eye, and they organise the information presented into a logical and navigable form.But we are still very much in the realm of the heading; the tablet is a source of information. Jumping forward three centuries to the second century CE and to the work of the grammarian Aulus Gellius, Dames observes a new breadth in the headings of Gellius‚Äôs miscellany, Attic Nights, ranging from brief summary to something more authorial than a legal finding-aid: ‚ÄòHow Publius Nigidius with great cleverness showed that words are not arbitrary, but natural.‚Äô Yet a text like Attic Nights is still only something to be consulted partially, and on occasion, rather than read and absorbed line-by-line: ‚ÄòThe text is not an experience‚Äô, but rather a ‚Äòstorage place from which information is extracted; the condensed summary is not only possible, but desirable‚Äô. However, one can begin to see the line of transmission; those ablative phrases of the Tabula Bembina are a precursor to the summative chapter headings of a novel like Charlotte Lennox‚Äôs Female Quixote (1752): ‚ÄòIn which will be found one of the former Mistakes pursued, and another cleared up, to the great Satisfaction of Two Persons; among whom, the Reader, we expect, will make a Third.‚Äô And so, from the tablet, the chapter begins the migration it will be Dames‚Äô project to track: out of its originary informational context and, slowly but surely, into the temporality of the novel.Before the migration is complete, though, we have a centuries-long period of terminological and conceptual confusion as various terms, referring to both the textual unit and its title, are used: capitulum, kephalaia (‚Äòhead‚Äô), titlos (‚Äòtitle‚Äô), argumentum and breviculus (summaries used to aid the inspection of a text) ‚Äì these terms are all tangled together. One of the most inspired interpretations of this conceptual history is Dames‚Äô rereading of the Confessions by the fourth-century theologian Augustine. Amid a spiritual crisis, Augustine overhears some nearby children crying ‚Äòtake it and read‚Äô, and, turning to a random section of the Bible, resolves to read ‚Äòthe first chapter [capitulum] I might find‚Äô. This is pivotal in converting Augustine to Christianity, for his eyes fall upon what a modern reader of the Bible might know as Romans 13:13-14, a caution against revelry that urges one away from ‚Äòrioting and drunkenness‚Äô, exhorting instead that its reader ‚Äòput ye on the Lord Jesus Christ, and make not provision for the flesh, to fulfil the lusts thereof‚Äô. Or at least, that is how it reads in the Oxford edition of the King James Bible I have just taken down from my shelf (slightly hungover, I confess, and thus moved and gently interpellated by its message), navigating with relative ease to Romans 13. But this was not Augustine‚Äôs experience, for his Bible had no chapters; his capitulum refers to the general ‚Äòhead‚Äô or topic of the passage. The meaningful unit discloses itself ‚Äòout of an unmarked stream‚Äô.¬†The organisation of the Bible into ‚Äòchapter and verse‚Äô dates from well after Augustine‚Äôs time ‚Äì chapters in the thirteenth century, and verses in the sixteenth ‚Äì and though this format has to a large degree been naturalised by convention, it was not for this reason free of controversy. Early modern intellectuals like Robert Boyle and John Locke would even rail against Biblical chaptering: Boyle complained of its ‚Äòinconvenient Distinction‚Äô, which ‚Äòhath sometimes Sever‚Äôd Matters that should have been left United‚Äô; Locke for his part despaired that the system of chapter-and-verse left scripture ‚Äòso chop‚Äôd and minc‚Äôd [‚Ä¶] so broken and divided‚Äô that not only do the ‚ÄòCommon People take the Verses usually for distinct aphorisms‚Äô, but even the educated have their powers of memory enfeebled. Yet not even the complaints of Boyle and Locke could overturn the chapter‚Äôs ‚Äòembeddedness in biblical textual tradition specifically and literate culture generally‚Äô. (To this ‚Äòantichapter‚Äô tradition we might add Donald Trump, who, when asked during an interview in 2015 to name a favourite bit of scripture, replied: ‚ÄòThe Bible means a lot to me, but I don‚Äôt want to get into specifics.‚Äô)Christian scripture is a key site of this transformation of the classical heading, indexing discrete topics in a text, into something whose purpose is story-driven and temporal. The Gospels are, after all, narratives, demanding ‚Äòa new method‚Äô for their organisation. Surveying six competing divisions of the Gospels across ten centuries, Dames describes a project of ‚Äòcontainerization‚Äô in which the chapter becomes capable of holding a wider variety of topics without being ‚Äòtailored‚Äô to the shape of its content. But it is in the late twelfth century that the ‚Äòmodern‚Äô system of Biblical chaptering is inaugurated. Usually associated with the medieval Paris Bible, the origins of this chapter system have long been tied to the work of the English theologian Stephen Langton (c. 1150-1228), who sought more accurate methods of citation for the university classroom. Despite the fact that not all that much evidence binds Langton to the creation of this chaptering technique, this historical account has long been the dominant one: ‚Äòa creative and practical-minded English churchman, steeped in the chaotic environment of a cosmopolitan academy, takes on the chaptering of the Bible‚Äô to improve his pedagogy. This story, however, has been challenged by the discovery of the earlier Saint Albans Bible (1180), named for the Hertfordshire abbey where it was produced, which contains Hebrew calligraphy and thus suggests the possibility not only of a Jewish scribe, but also perhaps an immersion in medieval rabbinical practices. In this view, the objective of the chapter was not scholarly and citational but monastic and oriented towards ‚Äòcommunal reading tied to a ritualized calendar‚Äô. Whether first conceived for the ‚Äòclassroom or the chapel‚Äô, what Dames calls ‚Äì in the spirit of historiographic compromise ‚Äì the ‚ÄòLangton-Saint Albans model‚Äô of chaptering affords a new experience: the ‚Äòprivate continuous reading of narrative texts,‚Äô the glorious fact of silent reading.Of course, a whole host of other transformations were needed to make such reading possible: the scroll is first divided into the codex; ancient continuous script is split into discrete words, which are themselves separated uniformly into paragraphs only in the early modern period. Transformations like these are usually treated by book historians as a ‚ÄòBabel allegory‚Äô, as Dames put it in The Physiology of the Novel, where the historical development of the book as a technology is told as the story of its fragmentation into smaller and smaller parts (which is often a narrative of progress, too: smaller units make reading more accessible and democratic). The Chapter takes this story of fragmentation one step further: part of Dames‚Äô interest is motivated by the chapter‚Äôs final dematerialisation and its lingering power as metaphor. The chapter has ‚Äòbecome a metalanguage‚Äô that describes the different rhythms of social life, from clock time to the lived cadences of the body. One speaks of a new chapter in one‚Äôs life ‚Äì not a new paragraph or a new sentence or, indeed, a new clause. But: ‚ÄòIf it still works for us this way,‚Äô Dames asks, ‚Äòfor how much longer?‚Äô Here, we might be prompted to ask: who in fact is left in this us? Viewed in the less generous glare of media history, and from the perspective of a present less and less oriented towards reading of any kind, the answer is doubtful. If the members of an increasingly postliterate society still measure out their lives in chapters, this may only be a matter of mere habit or convention ‚Äì in the way that a car‚Äôs engine capacity continues to be measured in horsepower.Perhaps it is the inevitable fate of any convention, but literary history does not, it turns out, have many examples of people appreciating great chaptering. In The History of English Prose Rhythm (1912) ‚Äì one of the sources for James Joyce‚Äôs virtuosic-or-unreadable parodies of the evolution of English prose in Ulysses ‚Äì George Saintsbury remarks on Thomas Malory‚Äôs decision to insert a chapter break at a decisive moment in his fifteenth-century Morte d‚ÄôArthur. At the end of chapter ten of the Morte, Lancelot rides into a castle, having slayed its gatekeeper, only to hear from the castle‚Äôs residents ‚Äòin doors and windows that said ‚ÄúFair Knight: thou art unhappy.‚Äù‚Äô Saintsbury praises Malory‚Äôs sense of timing here. The chapter break introduces a pause, leaving those words, as Dames puts it, ‚Äòhovering in the air‚Äô. The next chapter begins with Lancelot successfully freeing captives from the prison; as such, the chapter has served to elongate the narrative incident and heighten the tension.The only problem is that this was not Malory‚Äôs division, but rather one added by the printer William Caxton (c.1422-92). This fact was only discovered in 1934 when an edition of the Morte predating Caxton was discovered at Winchester College. As it turns out, the Winchester version had no chapters. The modulations of time are the work of Caxton‚Äôs specific ‚Äòremediation‚Äô. He creates an ‚Äòartful segmentation, a resonant silence, in the printed volume‚Äôs visual patterning‚Äô. Caxton is paired in this chapter of The Chapter with the anonymous fifteenth-century remediators who transformed Chr√©tien de Troyes‚Äôs great twelfth-century Arthurian verse into prose. Unlike Caxton‚Äôs their results are not acclaimed; like the authors of movie novelisations today, they are vulgarisers, profaning the sacred bonds between form and content. In their hands, Chr√©tien‚Äôs flowing verse ‚Äì praised in Mimesis by Auerbach as ‚Äòlight and almost easy‚Äô ‚Äì is not only segmented with red ink, but also crowded with insistent explanations in the register of narrative history (‚ÄòHow the king kissed Enide‚Äô). Again, in the manner of movie novelisers, moments of introspection are reduced while battle sequences are dilated with a vigour that may equally be judged ‚Äòclumsy technique‚Äô or ‚Äòdaring maneuver‚Äô. More charitably, we might say these remediators practise what Dames calls, after Roman Jakobson, ‚Äòintralingual translation‚Äô ‚Äì a phrase that calls back to mind (there it is again!) Davis‚Äô experiment with Sterne. Like Davis, the remediators are working across an historical gap between time-feelings, transforming the internal temporality of Chr√©tien‚Äôs verse to fit their own prosaic times. Dames speculates on the reasons for this transformation. Could it be that the new and uncertain ruling clique in Burgundy ‚Äì ‚Äòfreshly arrived at what would be its historical apex‚Äô ‚Äì preferred these ‚Äòmodes of intense now-time‚Äô to the subtle continuities of Chr√©tien‚Äôs verse? Admitting the possibility of such an ‚Äòideological effect‚Äô, Dames also notes that it is equally likely that these ‚Äònew temporalities‚Äô were simply an ‚Äòaccident‚Äô.¬†Here one notices a difference between Dames‚Äô previous books and The Chapter, whose broader subject matter perhaps helped it to become a finalist for the National Book Critics Circle Award. As brilliant works of literary history, Amnesiac Selves and The Physiology of the Novel both have the density of specialist knowledge and the sensitivity of immersive textual studies. Each book reconstructs a forgotten discourse: the first book reassembles the understanding of memory in the Victorian period, as explored through close readings of key Victorian novelists and scientific writers; the second builds on this interest by turning to the forgotten paradigm of ‚Äòphysiological‚Äô novel theory and its exemplars, the philosopher-scientist-critics GH Lewes, ES Dallas, and Alexander Bain, who explored the embodied rhythms of reading. The physiological basis for a literary theory of form was ultimately swept away by more abstract formalisms espoused, on the one hand, by Henry James and his acolyte Percy Lubbock, and, on the other, by the practico-critical poetics of IA Richards (who effectively banished the novel from the classroom). Though Dames‚Äô close readings in The Chapter are no less attentive and sinuous than in these earlier books, they are perforce more limited by the widened scope. I don‚Äôt intend to downplay the brilliance of Dames as a reader of individual texts or as a literary historian. However, as the study twists and turns, the density of historical detail together with the vast scope can at times induce a kind of mental torsion, with the dual impulses to historicise and taxonomise pulling in different directions.In any case, the taxonomic conclusion Dames draws from the Burgundian remediators of Chr√©tien is that while their clumsy cuts are just that ‚Äì cuts in a continuous weave ‚Äì Caxton‚Äôs interventions are more like the ‚Äòfade‚Äô, offering ‚Äòaeration‚Äô to the narrative text. In this respect Caxton‚Äôs edits are oriented not towards reference, but ‚Äònarrative progression and rhythm‚Äô. The paradoxical outcome of this intervention is to unify Malory‚Äôs text precisely by dividing it; the Morte now comprises ‚Äòsemi-discrete moments in a single process, rather than entirely different moments‚Äô. Unification-through-division of this sort highlights two logics of narrative time: discontinuous and immersive reading. Chaptering itself comes to generate a ‚Äòfeeling of presentness‚Äô by adding white space, a species of visual fermata between narrative actions ‚Äì ‚Äòemptiness [with] a temporal intensity.‚ÄôIn their evocation of ‚Äòpresentness‚Äô, blank intensities of this kind recall a much longer-running theological dispute ‚Äì between Augustine and the great English theologian Bede ‚Äì on the divisibility of time. Where, after all, is the present? For Augustine, it is impossible to isolate something like ‚Äòpresentness‚Äô, for it is composed ‚Äì as he put it in the Confessions ‚Äì of ‚Äòfugitive moments‚Äô, suspended in the future or always being sucked away into the past. The present is thus not measurable by a distinctive unit. Bede, in his eighth-century work The Reckoning of Time, argued to the contrary that there is a ‚Äòminimal‚Äô or ‚Äòatomic‚Äô unit of time. He made his case through a thought experiment. Say you are just about to be punched in the face. As a reflex, you flinch and close your eyes. Between these two moments ‚Äì that ‚Äòtiniest interval of time in which the lids of our eyes move when a blow is launched‚Äô ‚Äì is where ‚ÄòBede‚Äôs present‚Äô may be found: the atomic unit of presentness. Dames‚Äô point is not that this theological argument directly influenced Caxton and the Burgundian prosateurs, but rather that the disagreement between the two great theologians reflects different investments in literary forms and their relationships to subjectivity in time. For Augustine a poem ‚Äòheld entire‚Äô in the mind of a reciter approximates divine omniscience; for Bede, meanwhile, the atomic present is best accessed via a ‚Äòpunctuated continuity and directionality‚Äô that might just be the hallmark of well-divided prose ‚Äì consequently it is ‚Äòseriality, not the transcendence of seriality, [that] is our access to the divine‚Äô. It is only in interrupting the present that we are able to perceive it.But it is left to the early novel (as an historian of the form, Dames is candid about this bias) to develop fully the space between Augustine‚Äôs durationless void and Bede‚Äôs serial present. Leaping forward another two hundred-odd years, then, Dames shows this binary of discontinuous and immersive reading exploding into an array of conceptual possibilities. ‚ÄòThe eighteenth-century synthesis‚Äô, as Dames calls it, spans the period from the picaresque to the first flourishing of the English novel in the middle of the eighteenth century, with the antics of Sterne and Henry Fielding. Functions inherited from older reference-based chapters are here experimentally set in tension with the narrative innovations first explored in the fifteenth-century remediations: the eighteenth-century chapter struggles with the relationship between the strange and the commonplace, the ‚Äòstriking and singular‚Äô and the ‚Äòcategorizable‚Äô. Hence the initial distinction between discontinuous and immersive reading turns out to contain other oppositions that structure it in turn: between space and time; and between the time narrated and the time it takes to narrate or read.Figuring all this is that moment on the staircase from the middle of Tristram Shandy, a kind of novelistic freezeframe, in which Sterne fixes Walter Shandy in place to reflect upon chaptering. In Dames‚Äô account, this metachapter makes explicit the chapter‚Äôs full conceptual field: it has a direct address; it narrates both an incident and an interruption. What stands out as the real ‚Äòheart‚Äô of the metachapter is the staircase itself, which serves as a kind of symbolic definition of the chapter‚Äôs function. The staircase ‚Äòcaptures the chapter‚Äôs double chronometry, that tension expressed by the simultaneous binaries of space versus time and narrated versus narrating times‚Äô. Fielding famously compared his chapters to inns along the road of a long journey, where the reader may ‚Äòstop and take a glass‚Äô, but Dames thinks the staircase a better figure. Fielding‚Äôs coach trip is merely ‚Äòlinear, starting and stopping‚Äô; Sterne‚Äôs staircase, on the other hand, ‚Äòunpacks two complementary but opposed dimensions‚Äô. Walter and Toby head down the stairs, troping narrative progress, while at the same time the sequence of steps and landings displays the segmentation of linearity ‚Äòinto discrete stages‚Äô. Sterne‚Äôs novel is a kind of ‚Äòfunhouse mirror‚Äô of temporality: instead of proceeding steadily along a horizontal axis, our temporal schema is thrown down the stairs.Later, in what JGA Pocock once called the ‚Äòsecond eighteenth century‚Äô, the so-called Age of Revolutions, the chapter mutates again. Now ‚Äòelongated‚Äô, the chapter is studied in two works that each seem in different ways to dissolve its earlier functions. In The Interesting Narrative of Olaudah Equiano (1789), the famous autobiography of a Nigerian slave who eventually regained his freedom and lived in Britain, Dames observes a mismatch between the protocols of chaptering and the life that these protocols divide up. Equiano‚Äôs chapters offer extensive summaries in the manner of a picaresque novel, but seem at the same time to show the inefficacy of that paratextual structure for capturing the experience of domination and eventual manumission. ‚ÄòHow then to describe the chapter in Equiano, or more bluntly, why bother to do so?‚Äô It is perhaps relevant precisely because the apparent orderliness of chaptering ‚Äì its ability meaningfully to sculpt time ‚Äì is shown, against the absolute alienation of slavery, to be unfit for its usual purpose of segmentation. Thus, the intensively expository chapter summaries of the Narrative not only fail to coordinate with the abbreviated summaries in the table of contents, but they also introduce chapters of far greater length (on average, Dames tells us, these are 6,500 words: up to four times longer than is typical for this period). So, then, what is the meaning of this technical decision? ‚ÄòTo say,‚Äô Dames writes, coming perilously close to ventriloquising Equiano, ‚Äòa life cannot be measured this way, not this kind of life.‚ÄôAs the self-testimony of a former slave, published in the same year as the storming of the Bastille, Equiano‚Äôs Narrative is certainly a sign of the times. It is perhaps as iconic a testament to the ‚Äònew epoch‚Äô of the nineteenth century as Girodet‚Äôs portrait of Jean-Baptiste Belley, a former slave from Saint-Domingue who would eventually be elected to the French National Convention. ‚ÄòNew epoch‚Äô: this is the legendary, and perhaps apocryphal, phrase of Goethe, uttered in response to the defeat of the Prussians at Valmy in 1792. ‚ÄòFrom this place and from this day a new epoch in world history begins and you can say you were there to see it.‚Äô We might observe that he, for one, did not reach here for the metaphor of the chapter ‚Äì too ‚Äòpartial, fleeting, unhistorical‚Äô, according to Dames, to register this period‚Äôs epochal shifts. In Goethe‚Äôs Wilhem Meister‚Äôs Apprenticeship (1795-96), for instance, the chapter becomes even more elongated (one of them is 20,000 words!), doubling in size in the novel‚Äôs second half, which was composed after Valmy. Wilhem Meister‚Äôs Apprenticeship is a ‚Äòtriple turning point‚Äô, tying together ‚Äòa world-historical transition, a maturational transition‚Äô, and a ‚Äòcareer transition‚Äô as Goethe, now older and on the other side of the revolution, has to produce fresh material rather than merely revising old writing. It is the very incongruity and ‚Äòdilation‚Äô of the chapter that ‚Äòitself is historical‚Äô. Jane Austen‚Äôs career is also adduced as an example of the eighteenth century‚Äôs passing into the nineteenth, with the three youthful novels drafted in the 1790s averaging chapter lengths of around 2,000 words, while the ‚Äòmature‚Äô novels of the 1810s are nearer 3,500.However sceptical we might like to be about periodisation, and nasty but inevitable grand narratives, it‚Äôs observable that history has, well, happened; historical experience makes ‚Äònorms‚Äô normal, and it is potentially why ‚Äì to return to Davis‚Äô question with which I began ‚Äì more people still read Austen for pleasure than Smollett, Fielding, Defoe, or, um, John Bunyan. Not unrelatedly, I recently invited some students to read paragraphs from the fourteenth, fifteenth, sixteenth, seventeenth, and eighteenth centuries (respectively, Margery Kempe, Edmund Spenser, Margaret Cavendish, Eliza Haywood, and Sterne: I welcome criticisms of my selections) and one of them said, in so many words, ‚ÄòPerhaps some things are forgotten for a reason.‚Äô Perhaps. But we might also wonder: to what extent do novels instruct their readers in how to think, feel, and act?¬†This has been one of the questions that Dames has posed most insistently across his career, with a special emphasis on the contributions of the Victorian novel to readerly subjectivity. At the end of Amnesiac Selves, he speculates on the way that Victorian fiction inculcates a special kind of nostalgia ‚Äì its warm selective memory is the flipside of the alienating nausea of the historical difference that makes you want to throw a book out the window (or, in homage to Sterne, down the stairs). Yet, as Dames noted then, the cultural prestige of Victorian fiction is ‚Äòincreasingly seen in an elegiac manner, as a strange fact that, as the twenty-first century begins, will not last much longer‚Äô. As the nineteenth century disappears further and further from view, ‚Äòthe Victorians will eventually, if belatedly, make Victorian fiction stranger and less attractive‚Äô. Since the publication of Amnesiac Selves in 2000, the Victorians have only receded further away from us in time.It was the Victorian novel that made the chapter seem natural. Key to the reality effects of nineteenth-century British fiction is its synchronisation of novel time with the natural rhythms of life. As a result, novelistic chapters lose their theatrics, their posturing and posing, even those unstable amalgamations surveyed in Equiano and Goethe, and instead become regular and ‚Äòtacit‚Äô, receding into the background. It is this very tacitness that secures the permanence of the chapter as a blank, unmarked, and ordinary vehicle for reflection. Surveying Tolstoy‚Äôs War and Peace (1867), Dames distils the repertoire of the chapter into another taxonomy of five key functions: the signal or incantation; the crossing of a threshold; the ‚Äòsuspended revelation‚Äô; the ‚Äòtense use‚Äô, which adjusts the temporal frame of narration; and the modulation of point of view. Together with Elizabeth Gaskell ‚Äì whose Wives and Daughters (1864-66) is shown virtuosically to assemble all five of these ‚Äòtacit‚Äô operations, in a careful and naturalistic counterpart to the brazen theatrics of Sterne ‚Äì the chaptering of Tolstoy presents a study in indistinctness.Perhaps the most ‚Äònatural‚Äô scheme for novelistic time is that of the day itself, which is what Dames shows to be at work in Charles Dickens and George Eliot, calling this the ‚Äòsuturing of story world and reader [‚Ä¶] an alignment of times, a synchronization of light‚Äô. Epic heroes lived in a time supercharged with meaning ‚Äì the time of kairos, or propitious instant of action, not the dun-coloured chronos, the everyday time of housework, care work, and all the other kinds of work. In contrast with epic, then, the diurnal frame of novelistic realism appears definitively chronological and quotidian ‚Äì but it is, of course, a complex literary artefact, one that Dames explores using some old-fashioned counting. There are 146 narrated days in Middlemarch, though the novel covers some 1,000 days. That means around 15% of the total ‚Äòdays of our lives‚Äô are narrated; of these, only 18 ‚Äòpeak days‚Äô are extended over two chapters. If, in the time of Goethe and Equiano, the coordinates of day, chapter, and epoch fell helplessly out of joint ‚Äì the chapter form desynchronised from life by historical forms of dislocation ‚Äì it is the innovation of Eliot‚Äôs realism to realign life with text: ‚ÄòNeither wholly impersonal and public like the ‚Äúday‚Äù nor intimately personal like the epoch, chapter time is, perhaps, something like an image of weak collective time‚Äô.¬†Weakness is an important term, capturing the chapter‚Äôs ignorable yet undeniable presence ‚Äì just like time itself ‚Äì which is nonetheless experienced collectively. It also calls to my mind Walter Benjamin‚Äôs famous evocation of the ‚Äòweak messianic power‚Äô, the spark of redemption glowing however faintly in the present. There is something of that melancholia in this history too. As Dames wonders, ‚ÄòWhen you share time, what is it you share?‚Äô A book? A memory? A moment? Or perhaps one shares nothing, for the whole point of fiction is that it is invented, nonactual, negative. The reader of a novel, as Benjamin put it, ‚Äòis isolated, more so than any other reader‚Äô. Reading of fictional lives becomes a way of experiencing death before it happens to you: the characters in a novel make its reader ‚Äòunderstand that death is already waiting for them‚Äô. In novelistic time, therefore, one feels in the fate of fictional beings the ‚Äòwarmth which we never draw from our own fate‚Äô.¬†If chapters become, by the twentieth century, simply ‚Äòembarrassing‚Äô, subject to two equal but opposing modernist processes ‚Äì autonomisation (√† la Joyce‚Äôs almost freestanding stylistic excursions) or decimation (as in Samuel Beckett where it is obviated entirely) ‚Äì these new formal strategies do not get around the fundamental matter of finitude that is immanent to the chapter as a vehicle of time. A key transitional figure here is the Brazilian novelist Joachim Maria Machado de Assis, whose experimental fiction of the late nineteenth century seems to repudiate the tacit chaptering of realism in favour of something more akin to Sterne. Yet in Machado‚Äôs hands, the ‚ÄòShandean chapter‚Äô is no longer free-wheeling and free-associative, but decisively bound: if Tristram struggled to bring forth the story of his birth, the eponymous narrator of The Posthumous Memoirs of Br√°s Cubas (1881) is already dead. Speaking from beyond the grave in radically attenuated chapters, Br√°s Cubas adds a new note of disillusionment and pessimism that Dames calls ‚Äòantique-diminutive‚Äô.The diminution ‚Äì decline? ‚Äì of the chapter continues in twentieth-century avant-garde fiction and film: The Unfortunates (1969), by the British novelist BS Johnson, is not bound in a codex but rather packaged up as so many loose sheets in a box, becoming as a result a literal ‚Äòbox of time‚Äô. Consider, too, the ‚Äòantique‚Äô and self-conscious quality of the onscreen chapters in Agn√®s Varda‚Äôs film Cl√©o from 5 to 7 (1962), tracking its protagonist minute by minute as she awaits the results of a cancer test. For all their apparently lively experimentalism, all three of these cases finally return to the negativity that attends the ‚Äòlinguistic organization of temporality‚Äô. Machado‚Äôs novel is narrated by a dead man; Johnson‚Äôs book in a box is about a dead man; Varda‚Äôs Cl√©o receives omens of death: these are texts ‚Äòby, for and about the dead or dying‚Äô. Dames refers to these as expressing the ‚Äòpoignancy of sequence‚Äô, a term that names ‚Äòthe sensation of an end indefinitely, but only temporarily, held off‚Äô. This finally is ‚Äòa melancholy purpose: to keep something going ‚Äì a life, a form, a moment ‚Äì just a little longer‚Äô. The chapter, then, not as inns on a journey, but halting steps towards the end.Right at the beginning of the book, Dames recalls the remark of a ‚Äògifted analyst‚Äô some years ago: ‚ÄòYou‚Äôre starting a new chapter.‚Äô Dames writes that this comment made him feel ‚Äì quoting the psychoanalyst Donald Winnicott ‚Äì ‚Äòheld‚Äô. It also spurred the research and writing of this book. Although his analyst‚Äôs offhand ‚Äònovelization‚Äô of his life seemed perfectly to capture the subjective experience of temporal passage, Dames could not explain why it had this comforting effect on him. Despite being a ‚Äònovel reader,‚Äô he ‚Äòhad no idea why chapters existed ‚Äì a historical question ‚Äì nor what exactly they did to our sense of time, a theoretical question‚Äô. Guided by these questions, his journey backwards in time terminates in the continuous present with the novels of Uwe Johnson, Jennifer Egan and L√°szl√≥ Krasznahorkai. There is a chapter in Egan‚Äôs novel A Visit From the Goon Squad (2010) that is presented in the form of a PowerPoint presentation on ‚ÄòGreat Rock and Roll Pauses‚Äô. This is a ‚Äòchapter on chapters,‚Äô Dames notes, in the manner of Sterne but relocated into a wholly changed technological environment. These slides ‚Äì presented in the novel by a twelve-year-old girl named Alison about her family ‚Äì represent for Dames an effort to ‚Äòunderstand the feeling of time passing, a feeling that is shaped by media‚Äô. If the project of The Chapter has been to coordinate the feeling of time passing with the changing mediations of that feeling, then it is perhaps unsurprising that one of the prevailing feelings in this study in turn is its melancholia, its very nearly depressive turns towards the experience of temporal passage.It is also significant, I think, that a history concerned with the objective features of literary history has an important but just-visible subjective dimension ‚Äì significant, that is, that the book began on the couch in analysis. I was struck, reading The Chapter, by its minimal but insistent evocations of finitude. This is a history of the novel that is partly a history of its death. In this respect it complements Dames‚Äô other books that have told this story from a different angle, as when, in The Physiology of the Novel, he writes of George Gissing‚Äôs ‚Äòdepressive‚Äô and ‚Äòambivalent‚Äô relation to the novel form in an era of speed reading. ‚ÄòIn many ways,‚Äô Dames adds, ‚Äòthat depressive position has lingered for readers, writers, and critics of novels, to our own day.‚Äô It has ‚Äì and it perhaps accounts for the alternatingly depressive and wistful tenor of The Chapter, which reconstructs its object from the position of its catastrophic obsolescence. But Dames is not moralising about the decline of the novel or of the reading public. The conclusion of the study refuses any of what he has termed ‚Äòthe morality of attention‚Äô, remarking that even if the chapter is dispersed across ‚Äòdifferent media that weave in and out of the format of the book, [it] can express the disjuncture of time itself‚Äô ‚Äì the ‚Äòdisjuncture‚Äô, that is, between ‚Äòour‚Äô lives and any of the ‚Äòrhythms ‚Äì biological, cultural-economic, political, planetary ‚Äì we live among but cannot manipulate‚Äô.Yet I think this argument must be evaluated in light of the earlier claim in The Physiology of the Novel that one of the vulnerabilities of Victorian physiological theory was its transformation of readers and texts into technologies: ‚ÄòThe more its findings turned both novel and reader into machines, the less necessary (or, for that matter, interesting) its procedures seemed, and the more ancillary to other technologies the novel became ‚Äì a melancholy conclusion that cut short some of the theoretical innovations that the theory had promised.‚Äô If in The Physiology of the Novel Dames was ‚Äòimplicitly arguing for the viability of an updated, historically aware version‚Äô of nineteenth-century physiological theories of reading, then The Chapter strikes me as a now-explicit attempt to realise such a theory. In it, Victorian novels stand as the apex of a kind of felt and intuitive ‚Äòchronocommunity‚Äô in which picking up a triple-decker was a reliable way to plug into the interface of temporality that everyone shared. That now is perhaps lost, and the historian‚Äôs effort at understanding the genesis of a technical object like the novelistic chapter could be seen as some small recompense ‚Äì for the lost ‚Äòweak collectivity‚Äô of an earlier period, but maybe, too, for the fearful lack at the centre of all reading. It is difficult not to think of this in Dames‚Äô closing evocation of the chapter‚Äôs dispersal across the mediascape. That is, the ‚Äòtechnical‚Äô question of The Chapter serves to absorb a more basic anxiety: not only about the demise of the novel, but rather about the emptiness at the heart of reading itself.I think here of Maurice Blanchot‚Äôs essay ‚ÄòLiterature and the Right to Death‚Äô (1949) ‚Äì translated, as it happens, by Davis, albeit in a more traditional manner ‚Äì in which Blanchot writes of finitude as the inescapable meaning of literature. Language kills, and literary language most of all: ‚ÄòLanguage can only begin with the void,‚Äô he writes, ‚Äòno fullness, no certainty can ever speak; something essential is lacking in anyone who expresses himself.‚Äô Questions like ‚ÄòWhat is literature?‚Äô have received ‚Äòonly meaningless answers‚Äô insofar as they fail directly to confront this negativity. ‚ÄòPeople can and do ask, ‚ÄúWhat is poetry?‚Äù ‚ÄúWhat is art? And even ‚ÄúWhat is the novel?‚Äù‚Äô ‚Äì but for Blanchot it is in the ineradicable ‚Äòemptiness present in all these serious things‚Äô that the impossible centre of literature consists, that empty heart ‚Äòto which reflection, with its own gravity, cannot direct itself without losing its seriousness‚Äô.¬†Another way of saying this is that The Chapter seems to me most fully to grasp its subject not when it considers the historical or technical question of chaptering, but when it turns from this to the lived experience of time, in which the collapse of literature as such figures the collapse of everything else. Perhaps the function of ‚Äòthe chapter‚Äô in The Chapter is not so much a ‚Äòpoint of departure‚Äô as Auerbach once imagined it ‚Äì in his phrase, a ‚Äòhandle [‚Ä¶] by which the subject can be seized‚Äô ‚Äì but rather a point of return: an obsession, an id√©e fixe, or, in a more Freudian vocabulary, a reaction formation to the anxiety attending time‚Äôs ceaseless passage. Viewed in this gloomy half-light, it is possible to see how the most moving parts of The Chapter are those rarer moments in the subjective register that look right into the void at the centre of literary experience. It is, after all, to escape this emptiness that we write in the first place, even as the act of doing so can only return us to it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Are we decentralized yet?]]></title>
            <link>https://arewedecentralizedyet.online/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077291</guid>
            <description><![CDATA[A site with statistics regarding the decentralization status of various web services]]></description>
            <content:encoded><![CDATA[
      How Concentrated Is User Data On The:
      
        Fediverse
        Mastodon, Pixelfed, etc.
        
        Shannon Index: 
        
            Servers
            Biggest(%)
            Rest(%)
        
      
      
        Atmosphere
        Bluesky, WhiteWind, etc.
        
        Shannon Index: 
        
            Servers
            Biggest(%)
            Rest(%)
        
      
      Data last updated: 
    
      
        This page measures the concentration of user data on the Fediverse and the Atmosphere according to the
        Herfindahl‚ÄìHirschman
        Index (HHI) and the Shannon Index.
      

      
        HHI is an indicator from economics used to measure competition between firms in
        an industry.
        Mathematically, HHI is the sum of the squares of market shares of all servers.
        Values close to zero indicate perfectly competitive markets (eg. many servers, with users
        spread evenly), while values close to 10000 indicate highly concentrated monopolies (eg.
        most users on a single server). In economics, values below 100 are considered
        "Highly Competitive", below 1500 is "Unconcentrated", and above 2500 is
        considered "Highly Concentrated".
      

      
        The Shannon Index is an entropy-based measure used in ecological studies.
        It is computed the same as Shannon entropy using the natural log: the negative sum over all servers of the "market
        share" times the log of the market share. Lower values indicate lower entropy (a high concentration of one species),
        while higher values indicate a more even population. In this context,
        the maximum value is the number of servers, which would mean that all servers have equal population.
      

      
        This site currently measures the concentration of user data for active users: in the
        Fediverse, this data is on servers (also known as instances);
        in the Atmosphere, it is on the
        PDSes
        that host users' data repos.
        All PDSes run by the company Bluesky Social PBC are aggregated in this
        dataset, since they are under the control of a single entity. Similarly,
        mastodon.social and mastodon.online are combined as they are run by the
        same company.
      

      
        The location of user data is not the only interesting measure of 
        centralization. On a technical level, there is the network
        structure (peer to peer, relays, etc.), identity management, the
        infrastructure on which it is hosted, etc. On a legal level, there are
        issues regarding the jurisdictions where servers are located, companies
        are located, etc. On a social level, there are issues around where
        human power is concentrated in and on the platform, and whether that
        power is disproportionately held by certain groups. If you would like
        to help contribute other measures of decentralization, get in touch.
      

      
        Code and data are available on
          GitHub.
        Comments and pull requests, including other metrics for measuring
        distribution and resiliency, are welcome!
      

      
        By Rob Ricci: @ricci@discuss.systems / 
            @ricci.io 
      

          
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anduril's product engineering machine]]></title>
            <link>https://joincolossus.com/article/the-amusement-park-for-engineers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077209</guid>
            <description><![CDATA[An exclusive peek behind the curtain of Anduril's product engineering machine]]></description>
            <content:encoded><![CDATA[
                        
This article features first-ever photos taken from inside Anduril‚Äôs R&D facilities in Costa Mesa, California. All photos by Ryan Young.







On a Saturday afternoon in April 2024, I was on the rooftop pool deck of a Marriott hotel, setting up radar equipment aimed above the Hollywood Hills in Burbank, California. My five-year-old son, still damp from swimming, darted around as I calibrated the system.



‚ÄúWhat are you doing?‚Äù he asked, touching the electronics with wet hands.



‚ÄúTracking ‚Ä¶ flying objects,‚Äù I said, carefully moving his hands away from the sensitive equipment. ‚ÄúIt‚Äôs a special radar that will help our drones find targets better.‚Äù



Working on a thousand-dollar radar that could potentially transform a landmark missile platform during a father-son weekend was fairly typical in those days. The technology that my son wanted to touch, and which other poolside guests gawked at, was a throwback to the AGM-114 Hellfire missile system from the 1960s‚Äîa simple direction finder that could be guided by a ground system that paints targets with radio frequency (RF) instead of lasers. If we could get this to work, we could reduce the cost of our Roadrunner system‚Äîa reusable, twin-turbojet, vertical-takeoff-and-landing microfighter‚Äîby 30x.



Between trips to the pool and Chick-fil-A, I eventually managed to collect enough data to prove the concept worked: We could detect aircraft at 10 kilometers with a thousand-dollar sensor. It was the kind of breakthrough that could change how we approached reusable weapons and low-cost solutions for air defense‚Äîan ongoing R&D project I remain consumed by.



It wasn‚Äôt company-sanctioned work. I was officially on family time, having left Anduril as SVP of Engineering the month before to start a robotics company, Physical Intelligence (PI). Yet Anduril was never just a job; it was part of my identity. My badge still worked, I continued on in an emeritus role, and I still spent around 15 hours a week working with the engineering team I‚Äôd helped build.



When I joined Anduril in the fall of 2018, I was employee #20, the company was valued at $250 million, and we had lofty, but hypothetical, ambitions of reinventing the defense ecosystem. Less than six years later, the 4,000-person, $28 billion company has deployed 30-plus products with thousands of fielded systems, and changed the arc of American defense technology. It‚Äôs worth looking back now at those years of explosive growth, in order to give other founders, engineers, investors, operators, and everyone else a glimpse of what zero-to-one at Anduril was actually like.



        
            
            

            
                            Team member welding in Anduril‚Äôs R&D prototyping shop.
                    

    



    




I‚Äôve always been drawn to the kind of science that translates into strategic impact, and to problems too dangerous to ignore. After studying at MIT, I worked on flood disaster relief in Pakistan, then became a founding engineer at a biotech startup developing affordable genome sequencing technology. When the company was acquired, I left for Tesla, where I worked on projects from the Falcon wing doors in the Model X to electromechanical architectures, autopilot sensors and advanced technologies enabling future vehicle platforms. It was cutting-edge work with one of the most innovative companies in the world, and I was genuinely happy there.



A coffee in July 2018 with Anduril‚Äôs founder Palmer Luckey changed everything. What was supposed to be a quick 30-minute chat turned into a six-hour conversation that made it impossible for me to go back to Tesla. These were the days when supposedly bleeding-edge work in Silicon Valley was still largely dominated by consumer apps and services. By contrast, the picture of the world that Palmer drew kept me up at night.



While the threat from China wasn‚Äôt yet front-page news, Palmer and his team had already recognized the need for better defense technology to deter a great power conflict and to maintain American hegemony. They understood that America‚Äôs technological edge in defense was eroding, and that traditional defense contractors were too glacial and bureaucratic to meet the challenge‚Äîa culture I‚Äôd previously witnessed firsthand (and had forever sworn off) during an internship with an aerospace manufacturer.



I was struck by the Anduril team‚Äôs strategy of developing defense products on its own dime and selling them off-the-shelf, turning the traditional business model of defense contractors on its head. I was also impressed by their choice to build the company in Southern California, deliberately removed from Silicon Valley. My coffee with Palmer came only a few weeks after Google canceled Project Maven, which would have assisted the Department of Defense with AI-based drone-footage analysis. When I eventually told colleagues at Tesla that I was leaving to join a 20-person defense technology startup working out of a hangar near Santa Ana airport, they looked at me like I was insane.



        
            
            

            
                    

    



    




Click here¬†to subscribe to print for your office or home.



    




When I joined the company in September 2018, we worked out of a small building at 3000 Airway, then expanded into hangar B8, which was adjacent to a dog kennel at the Santa Ana airport. There was no heating or air conditioning, just incessant barking. I claimed a closet that received a little heat through proximity to another part of the building. That became my lab.



On my first day, I found myself back at MIT, on stage beside Palmer, explaining our vision to skeptical engineering students confused why anyone would work in the defense sector. By the end of that same week, I was on the southwest border installing one of our pilot systems, a surveillance tower. It wasn‚Äôt the polished product defense contractors typically wait to unveil‚Äîin fact, our first tower was literally a telephone pole with a gaming PC housed in a weatherproof box, a pan-tilt unit normally used as stage lighting, with spikes on it to prevent bird shit from blocking the sensors. A lot of it came from Home Depot.



That makeshift tower, which we built on our own dime to prove what was possible, helped intercept nearly 1,000 pounds of marijuana and led to dozens of drug trafficking arrests‚Äîultimately earning us a pilot program with Customs and Border Protection. It was primitive, but it worked, and reflected our approach: get to a minimum viable demonstrator, something that creates end-to-end capability, then iterate ruthlessly. By then I understood that Anduril would be the fastest, most intense environment I‚Äôd ever experienced.



A few days each week, we‚Äôd pile into vehicles and drive to our test site in Apple Valley‚Äîa remote California desert location where temperatures reached 110 degrees on summer days, then dropped to 30 at night. We stayed in the cheapest hostel-like accommodations we could find and worked 16- to 18-hour days in complete isolation from distractions. We operated out of dusty trailers with minimal equipment. If something broke, we couldn‚Äôt just order a replacement part‚Äîsomeone had to drive 200 miles back to Santa Ana, rebuild the component, then drive 200 miles back.



Brian Schimpf, Anduril‚Äôs co-founder and CEO, functioned as our chief engineer, with an intuitive understanding of how every component fit together. Brian shaped our strategy and had a remarkable knack for pulling together engineering pieces and connecting them to business outcomes. When obstacles appeared, the other founders would come up with a strategy to unblock the engineering so Brian could focus on solving technical challenges‚Äîlike the time co-founder and COO Matt Grimm chartered a plane in order to fly oversized batteries across the country for a critical demo.




Our first tower was literally a telephone pole with a gaming PC housed in a weatherproof box, a pan-tilt unit normally used as stage lighting, with spikes on it to prevent bird shit from blocking the sensors. A lot of it came from Home Depot.




Even in those early days, the company was single-minded and self-selecting. No one cared about meetings or performance management or building a well-rounded company. We lived and died by our ability to quickly fire a ‚Äútracer bullet‚Äù through the heart of each problem, illuminating a clear path to the full solution.



It was a bad day when we‚Äôd be testing a quadcopter drone and it would crash a couple of kilometers from the takeoff zone. That seemed to happen most often at night or on weekends at the Capistrano Test Site (CTS), an environmentally protected zone of beautiful rolling hills, where we had to recover every scattered bolt by hand. At 2am, we‚Äôd be searching in the middle of the cactus brush for a drone that had fallen out of the sky.



We came up with creative solutions, like gluing glow sticks to the ‚Äúbirds‚Äù (our word for drones) so we could see them in the dark, or putting beepers on them so we could hear where they landed.



        
            
            

            
                            Anduril‚Äôs first surveillance tower, with spikes to prevent birds from relieving themselves on the sensors and gaming PC.
                    

    



        
            
            

            
                            Early radio frequency (RF) chamber, purchased on eBay.
                    

    



    




Like diamonds, all great products are born from heat and pressure. Consider the Tesla Model 3: The battery engineers pushed for maximum energy density; the chassis team insisted on minimal weight and thickness; and the safety team required uncompromising crash resilience. Each group had conflicting demands, yet this friction ultimately yielded an exceptional battery pack‚Äîpowerful, efficient, and safe. No stakeholder was completely satisfied, but through that creative rigor and tension, something extraordinary emerged.



The same was true at Anduril, where an additional layer of pressure came from the international political reality. America‚Äôs adversaries evolve tactics in weeks, and the company had to operate with that same urgency. We couldn‚Äôt deliver solutions in years‚Äîwe needed to prototype, test, and deliver in months or weeks. Each product therefore had to embody our chief working principles: move fast with purpose; question everything; take ownership; keep it simple; hold high standards; and design with deployment in mind.



One of the first products I worked on was our counter-drone interceptor, Anvil. This was important because the US government had spent billions of dollars on counter-drone technology with limited success‚Äîin some cases, they were literally training falcons (real birds in this case) to take down drones, or using Patriot missiles costing millions of dollars to destroy $500 quadcopters. The inefficiency was absurd, but the problem was serious. In 2018, a rogue drone shut down London‚Äôs Gatwick Airport for two days, and they were being used on the battlefield more and more.



We had a simple idea: What if we just used a quadcopter to crash into another quadcopter? Our first prototype used an Intel RealSense camera to look up and fly into targets. The approach was primitive‚Äîif the target moved, we‚Äôd miss‚Äîbut at a bake-off at White Sands Missile Range, our system successfully intercepted targets about 40% of the time, while competitors had single-digit success rates.



The government customer was in shock, but to us, a 40% hit rate was nothing to dine out on. They quickly deployed our solution as a stopgap overseas, which created pressure to improve, because the system could fall short in the field: the cameras and sensors couldn‚Äôt reliably detect targets due to glare or clouds, and our guidance system was too basic.



We had three months to stop the bleeding, and I spent my paternity leave developing a 3D radar system. I realized we could leverage the same radar technology used in self-driving cars, build an RF frontend with a non-uniform antenna, and create our own algorithms for terminal guidance instead of collision avoidance.



In three months, we went from a 40% kill probability to knocking out 35 of 35 targets. There wasn‚Äôt a quadcopter you could throw at our mechanical bird that it couldn‚Äôt take out. We even hired one of the top First Person View (FPV) drone pilots to try to evade our system. Anvil caught the drone every time.



As Elon would often say at Tesla, ‚ÄúIf the schedule is long, it‚Äôs wrong; if it‚Äôs tight, it‚Äôs right.‚Äù Speed was our weapon. Even our recruitment process reflected this: We‚Äôd openly talk to candidates from defense contractors and show them around our facilities, confident that we were moving faster.



        
            
            

            
                            ‚ÄúOur first prototype used an Intel RealSense camera to look up and fly into targets. The approach was primitive‚Äîif the target moved, we‚Äôd miss‚Äîbut at a bake-off at White Sands Missile Range, our system successfully intercepted targets about 40% of the time.‚Äù Anvil V1.
                    

    



        
            
            

            
                            Repairing the bistatic radar seeker with a soldering iron in the electrical engineering R&D lab.
                    

    



    




We paired speed with another key principle: question everything. This meant engineering from first principles‚Äîbreaking down every problem into physics, math, and operational reality, then building solutions from there.



In 2019, the US Air Force wanted to explore new solutions for detecting low-altitude cruise missiles‚Äîa critical capability with threats from Russia and China that could penetrate our borders. The official requirements for the Advanced Battle Management System (ABMS) program called for a radar with high azimuth and elevation accuracy and full hemispherical coverage, which would mean multimillion-dollar systems.



But no air defense radar manufacturer wanted to sell to us‚Äîsome because we were a small no-name company, others because they wanted to capture the full value of delivering their own systems, still others because they saw Anduril as a potential future competitor. We needed to create our own solution, but traditional radar development takes years.



Instead of taking the Air Force‚Äôs brief at face value, we asked: ‚ÄúWhy do they need hemispherical coverage? What‚Äôs the actual threat?‚Äù The primary concern was low-altitude cruise missiles coming across unprotected territory, which meant we only needed to cover the first few degrees above the horizon, not the entire sky.



We modified a $5,000 commercial boating radar‚Äîthe spinning ‚Äúcandy bar‚Äù type you see on fishing vessels. Boat radars are designed to detect small objects far away on water, but not in the air. By modifying the waveguide assembly to create a narrower beam, we concentrated more energy in a specific direction and extended the range by about 10x.



When we showed up to the bake-off with our cheap modified boat radar mounted on a rickety welded truck, competing against traditional defense contractors with multimillion-dollar systems the size of a shipping container optimized for 360-degree hemisphere coverage, the other attendees laughed.



Yet we won. We understood what the customer needed to accomplish, but mostly ignored what they thought they wanted in their requirements. Our system could be scaled along any border as a true cruise missile detection network at a cost that was orders of magnitude lower than traditional solutions.



        
            
            

            
                            Inner guts of a tower assembly.
                    

    



        
            
            

            
                            The author flying the Anvil V3.9.
                    

    



    




Every project at Anduril had a directly responsible individual (DRI)‚Äîa single owner accountable for the outcome from end to end. Sometimes ownership meant taking extraordinary measures when the stakes were high.



One of the most dramatic examples arose at a critical test for a US defense customer of our V2P interceptor drone‚Äîa quadcopter that could fly at 150 miles per hour and intercept other drones with impressive accuracy. This was a billion-dollar contract opportunity that could transform Anduril from a border protection startup into a serious defense company.



V2P was an evolution of our Anvil system. After successfully developing Anvil to intercept small quadcopters, we discovered a much bigger threat emerging: larger Group Two and Group Three drones. The former are often used for intelligence reconnaissance and surveillance missions, and the latter with larger payloads and longer ranges. Countries like Iran were developing the Shahed series‚Äîmassive drones that could carry substantial explosive payloads and fly kamikaze-style into bases.



Anvil was already successful, but it wasn‚Äôt designed for these larger targets. It needed a complete overhaul: more speed, better guidance, and enhanced durability.




We lived and died by our ability to quickly fire a ‚Äútracer bullet‚Äù through the heart of each problem, illuminating a clear path to the full solution.




Over three and a half months, we developed the V2P. We redesigned the propellers, motors, and the entire power architecture, which pushed the boundaries of what a small drone could do.



The result was unheard-of: a 5.2 kilogram vehicle that could achieve a speed of 147 miles per hour. (The world-record quadcopter at the time weighed 800 grams and did 173 mph). More impressively, it maintained sub-degree accuracy on pitch, roll, and yaw, even at steep angles of attack, where quadrotor dynamics become extremely challenging.



As a US defense customer‚Äôs Systems Integration Partner (SIP) competition approached, we needed to build 50 drones. One of our lead engineers from the test team and his co-worker came in on a weekend and built 28 complete vehicles in a single day‚Äîa feat that would have taken weeks through conventional processes. In total, they built 53 vehicles in 14 days, test-flying each one three times while I analyzed all the flight logs in real time. When we arrived at the competition, our competitors showed up with elaborate, expensive systems that had been years in development. Our V2P interceptor dominated the event. It destroyed 30 targets with extra interceptors to spare.



The highlight came on the evening before the last day of the bake-off, when US government officials asked if we could take down a far bigger Group Three aircraft‚Äîmuch larger than our system was designed for. With our existing approach, the V2P would simply bounce off such a large target. But we had a potential solution: radar firmware that could identify propellers through micro-Doppler signature and target them specifically. The night before the upcoming test round, we needed to finish writing the updated firmware and flash all of our drones with it.



At 3am, the same lead engineer who built 53 vehicles in two weeks went down to the hotel room where the drones were stored and proceeded to disassemble 18 birds. He took apart each radar, separated the processor and RF boards, hooked them up to his computer, flashed them with the new firmware, verified the changes, and reassembled everything. At 7am, he casually walked out as if he‚Äôd just woken up like everyone else, as the competing teams came down the elevators.



On the last day of the bake-off, our modified V2P took out the Group Three target on the first attempt, hitting it directly through the propeller. The entire room erupted in celebration.



That win was the cherry on top of a 200-person effort that ultimately secured a billion-dollar program of record for Anduril, transforming the company‚Äôs trajectory. It also exemplified the level of ownership we cultivated in our engineers‚Äîpeople who felt in their bones that they owned the outcome, and therefore cared so deeply about it that they didn‚Äôt blink at doing the kind of dirty work that their counterparts in other companies might consider drudgery.



This kind of approach is never without risk. But in our environment, we gave people agency and trusted their judgment. When a single engineer saw a 50/50 chance of success versus the near-zero probability with the original firmware, he made a call that changed Anduril‚Äôs future.



        
            
            

            
                            Team member building a Bolt for development testing.
                    

    



        
            
            

            
                    

    



    




Click here¬†to subscribe to print for your office or home.



    




In engineering, simplicity is strength. At Anduril, we continuously asked what we could eliminate or simplify.



Consider the challenge of defending vast territories against cruise missiles. Conventional systems, like Patriot PAC-3 and NASAMS batteries, typically cost millions of dollars per installation. So we asked ourselves a simple question: What if we could create a forcefield of low-cost drones to intercept cruise missiles worth millions?



The concept seemed absurd at first, even to our team‚Äîthe overmatch appeared too extreme. But we stripped the problem again to its fundamentals. Cruise missiles are fast, but they follow predictable flight paths. If we could accurately determine that flight path using two ground-based IR passive sensors (what we called Wide-Area Infrared System for Persistent Surveillance, or WISPs), we wouldn‚Äôt need expensive targeting systems on the interceptor itself.



We modified our Anvil drone to carry no sensors at all‚Äîthe drone would simply position itself in the projected path of the incoming missile, aligning with where the missile would pierce our virtual ‚Äúforce field.‚Äù Despite the initial skepticism, we demonstrated the concept successfully, destroying a target that could fly an order of magnitude faster than our interceptor.



The beauty of this solution wasn‚Äôt just its low cost, but its elegance. We didn‚Äôt need to match speed with speed or complexity with complexity. We found the simplest possible point at which to intervene and disrupt the threat.



We applied this thinking to all our products, and constantly fought against feature creep. Most product managers naturally want to add capabilities‚Äîone after the next after the next. But we ruthlessly focused on the 20% of features that delivered 80% of the value, and made those exceptional.



        
            
            

            
                            A prototype compressor for Roadrunner turbojet development.
                    

    



        
            
            

            
                            Team member utilizing a 5-Axis DMG machine to make challenging parts.
                    

    







    




Yet there‚Äôs a line between scrappy and crappy. At an all-hands meeting a while back, one of our team members asked, ‚ÄúWhy don‚Äôt we just build perfect products?‚Äù The answer reflected our core philosophy: We had an ethical obligation to get the best solutions into warfighters‚Äô hands quickly. We could build one or two gold-plated systems over the course of years, or we could deliver 10 near-perfect solutions that actually make it to the field in a battle-relevant timeframe.



We were scrappy to the core, but we also had a very clear understanding of what ‚Äúdeployability‚Äù meant for each system. We were uncompromising about those standards while tolerating rough edges elsewhere. A disciplined approach to trade-offs allowed us to deliver capabilities that competitors with 10 times our resources couldn‚Äôt match. The key was attention to detail. Teams without a painfully clear understanding of what‚Äôs important have a bias toward frills, whereas we went after the aspects that delivered the most value to the warfighter‚Äîavoiding the classic mistake that sales-led organizations often make by building pretty products that fall short in functionality or usability.



To take one example, when we learned the Marine Corps was seeking a new loitering munition, we initially didn‚Äôt want to compete. The thinking was that a different company specializing in thermal imaging cameras and sensors had been working on this project for five years, and Anduril would simply waste millions of dollars trying to beat them.



Yet the opportunity for a competitive edge remained. Quadcopters usually flew slowly into targets horizontally and could be seen coming from far away. What we needed was a drone that could dive with such blistering speed that by the time you heard its scream, it was already too late ‚Ä¶ If we could come up with a game-changing new capability for top-down kills that would be much harder to anticipate, we could win.



I asked Raichelle Aniceto, my chief of staff, to procure the competitor‚Äôs drone, and within days, we had one completely disassembled in our lab. We hot-glued the components onto a trifold board using her wedding invitation kit. What appeared to be a science fair display nevertheless clearly demonstrated how each component was not only suboptimal, but dependent on multi-tiered foreign supply chains‚Äîand that we could build something lighter, faster, easier to manufacture, and more reliable, while de-risking tangled Chinese supply chains.




Like diamonds, all great products are born from heat and pressure.




But there was a catch‚Äîthe Marines expected a product, not a proposal, and we had nothing more than a concept. We went a week without sleep to create high-performance renders, building life-like mockups, and drafting a technical proposal for a drone that didn‚Äôt yet exist.



The proposal was submitted 60 seconds before the deadline. We won the first phase, but now we had to actually build what we‚Äôd promised.



This became Bolt‚Äîa loitering munition that could precisely target ground vehicles. I saw it as a weekend project at first: take our existing drone platform, retrofit it with a vision seeker, and have it crash into ground targets instead of aerial ones. We created a quick demonstration video for a proposal and were one of the few companies down-selected.



As the project progressed and the stakes escalated, we learned that what worked for aerial targets wouldn‚Äôt work for ground targets. The team tried to apply the same guidance approach that worked for Anvil, but when diving at steep angles toward the ground, the drones kept missing by several meters.



During a critical customer demonstration, our bird completely missed its target. The room went silent. We knew we were at risk of losing a half-billion-dollar program milestone if we couldn‚Äôt fix Bolt quickly.



The engineering team tried applying more aggressive corrections, but the misses only grew wider. It turned out there was a fundamental issue: When a quadcopter dives downward faster than its propellers can ‚Äúbite‚Äù through the air, the propellers act as air brakes, inverting the effect of guidance commands. We needed to completely reverse their guidance commands when crossing this threshold.



That weekend, I developed an entirely new guidance approach we called ‚ÄúDive,‚Äù which allowed the drone to fall along a target vector with precise lateral corrections. Along with optimizing the propellers for inflow velocity with dynamic throttle margin, the problem was solved.



Later on, when preparing for a demonstration with a four-star general, another issue emerged: The airframe was vibrating and we were losing attitude control. The team drove the drone (with no lethal payload on it, of course) from Southern California to my home in Los Altos in the middle of the night. The next day was the baby shower for my second child, and we spent the hours and minutes before the guests arrived tuning the drone in my backyard.



The ferocious commitment from our team paid off again: The Marine Corps down-selected Bolt for its Organic Precision Fires-Light (OPF-L) program.



        
            
            

            
                            ‚ÄúThe ferocious commitment from our team paid off again: The Marine Corps down-selected Bolt for its Organic Precision Fires-Light (OPF-L) program.‚Äù
                    

    



    




It was never enough to create solutions that worked in the lab. Each one had to work reliably in the field, at scale, and at a cost that made sense.



This was part of Anduril‚Äôs secret sauce, and antithetical to how traditional defense contractors operate. The defense primes typically optimize for high-margin, low-volume production with expensive maintenance contracts. Anduril brought Silicon Valley‚Äôs mindset of scalable technology to defense‚Äîsolutions that could be mass-produced and widely deployed.



When designing hardware, we broke the product development process into three distinct stages. In the conceptual phase, the most important metric was lead time‚Äîhow quickly we could get the components needed to build a prototype. In the new product introduction phase, when building 10 to 100 units, we focused on ramp time‚Äîhow quickly we could work with vendors to reach the rates required for a pilot. In the third stage, full-rate production, the focus shifted to scrap rate and cycle time.



Traditional defense programs often fail because they create exotic systems that are too hard to produce or too expensive to deploy at scale. We were determined not to make that mistake.



By designing with scale in mind from day one, we aimed to create a virtuous cycle: our products could be deployed more widely because they were affordable, which generated more data and experience, which improved the next generation of products.



These core principles guided our product development. But principles alone aren‚Äôt enough. To apply them consistently across hundreds of engineers and dozens of products, we needed to design an organization that could sustain this approach at scale.



        
            
            

            
                            Electrical test rack with a new board.
                    

    



    




As we approached 60 employees, it became impossible for everyone to report to Brian. What began as my leadership of a handful of electrical engineers quickly expanded to 75 people, then to all hardware engineering, and ultimately to all product engineering‚Äîelectrical, mechanical, and embedded systems combined. When I became SVP of Engineering in June 2022, I had 164 people in my department. By the time I left in March 2024, it had grown to 550 engineers working on 30 products across 15 different families.



Building a high-performance organization was as important as solving technical problems. Throughout my time, I had to maintain this dual identity‚Äîan engineer on the frontlines driving design and development, and also a leader responsible for creating the organizational structure that would enable others to do the same. I needed to build a leadership team that could own full lifecycle product development and deliver world-class systems at the pace and precision demanded by our mission. Every hire was made with this blueprint in mind.



The first priority was to anchor the organization with deep technical credibility. ‚ÄúBadass engineers want to work for badass engineers,‚Äù as the saying goes‚Äîthe best will only work for leaders they can learn from and respect technically. We needed to avoid the common mistake made by organizations which fail by promoting or hiring managers without the technical skills to understand problems, build strong teams, or avoid making flawed engineering decisions.



For the electrical team, we wanted to position ourselves at the leading edge of avionics design. I recruited Shaun Donovan, a veteran of General Atomics and an early employee at Anduril, who had been involved in nearly every major electrical design to date. His mandate was clear: take hard-won lessons from legacy systems, and build the next generation of electrical architecture from first principles. On the mechanical side, we needed a leader deeply experienced in rapid prototyping and fabrication; someone who could translate ambitious concepts into functional hardware quickly and effectively. That was Matt Zipfel, whose career at SpaceX was defined by turning bold mechanical ideas into working prototypes under intense timelines.



With the core engineering leadership in place, the next critical step was embedding product thinking across the organization and developing a scalable product platform. We needed product leaders who could unify technical execution with the production rigor needed to scale. I brought over Danish Tejani, Anduril‚Äôs first hardware product manager and a former NPI lead at Tesla, to help build our product development function.



As the team matured and the product portfolio expanded, it became clear that maintaining alignment with the defense customer was both a strategic imperative and a growing risk. Many of our new hires came from non-defense industries, and while that brought valuable innovation, it also introduced a potential gap in mission understanding. To close that gap, we hired Joe Bayer, a former GA program executive and F18 pilot. His deep domain knowledge and firsthand understanding of defense customers allowed us to stay laser-focused on delivering solutions that mattered.



The final piece of the leadership architecture was operational scalability. By this point, we had grown to hundreds of engineers across dozens of teams. We needed a chief of staff who could partner with me to build and run the engineering organization, without losing the technical context that made us successful. It‚Äôs common practice to hire Jared from Silicon Valley types, but I needed someone who could communicate with engineers on their level and play the role of a technical leader. That was Raichelle Aniceto‚Äîan MIT-trained aerospace engineer who led Relativity Space‚Äôs ambitious Terran R rocket program.



        
            
            

            
                            Test racks for electrical assemblies in the R&D building‚Äôs electrical engineering lab.
                    

    



        
            
            

            
                            Dev space in the R&D building‚Äôs electrical engineering lab.
                    

    







    




Anduril is ‚Äúan amusement park for engineers,‚Äù as I once remarked, because we worked on such diverse and challenging technologies. But behind the thrill and excitement had to be a carefully designed system that could turn ambitious ideas into deployable products. As we grew from a handful of engineers to over 500, maintaining our speed and innovation became increasingly challenging. Traditional organizational models wouldn‚Äôt work‚Äîdedicated teams for each product would have required thousands of engineers and created silos that slowed innovation. We needed something different.



We rebranded our engineering organization as ‚ÄúProduct Engineering‚Äù to make our purpose clear: delivering products that meet customer needs. We then consolidated scattered teams into a cohesive group with three clear frameworks: products, core technologies, and key capabilities.



Products were our mission-focused integrated systems. Core technologies were our standardized building blocks‚Äîour LEGO pieces‚Äîthat could be rapidly assembled into new products. Instead of starting each drone from scratch, we created reusable components like flight computers and propulsion systems. Key capabilities were our internal engineering services, like a machine shop that could transform digital concepts into physical prototypes within hours, or teams that could ‚Äúshake, bake, and shock‚Äù components to ensure reliability.



What made this work was our matrix organization. Instead of creating dedicated teams for each product, we built functional organizations (across electrical, mechanical, and embedded systems) with deep expertise that could surge resources toward critical projects when needed. When we began developing Roadrunner, we pulled engineers from electrical and mechanical pools for intensive development, then shifted them to other projects when those phases ended.



The results were unprecedented in hardware: In early 2023, fewer than 200 people were responsible for over 25 different products, some of which were deployed across the world in the order of thousands.



We maintained integrated product teams where specialized expertise was required, like our Electronic Warfare group led by Sam El-Akkad, with deep RF and signal processing expertise. Or the Imaging team led by Bill Ross, with expertise in developing sensors down to the pixel-level silicon design. We also established product architects who were responsible for trade studies and system-level decisions‚Äîengineers who had proven themselves technically and could now lead cross-functional efforts.



With this foundation, we maintained small, focused teams while leveraging the broader ecosystem around them. But structure alone wasn‚Äôt enough‚Äîachieving this level of performance required recruiting the right people and building strong leadership capable of operating in a dynamic environment.



        
            
            

            
                            The author setting up a Pulsar, Anduril‚Äôs electronic warfare product, in a RF anechoic chamber.
                    

    



    




The leadership approach at Anduril centered on a few core principles.



First, we prioritized relentlessly through a daily red-light/green-light system based on the Objectives and Key Results (OKRs) we set. Every product and project had clear metrics that we reviewed constantly. When something showed red, we‚Äôd immediately assemble the team to identify root causes. This consumed 60‚Äì70% of my time‚Äîfiguring out the biggest obstacles and eliminating them alongside the team. I was notorious for being chronically late to scheduled meetings because I wouldn‚Äôt cut short work on critical problems.



Second, we maintained technical credibility through hands-on involvement. By day, I‚Äôd handle the corporate aspects‚Äîmeeting with leaders across the company to drive product and technology development, and continuing a constant discussion about what was slowing us down, what was blocked, and what was broken. But after 5pm, my engineering work began. I‚Äôd return to the building at 7:30pm after dinner and wander the labs until the early hours, sitting with teams debugging problems. I‚Äôd whiteboard calculations, write Python scripts, and sometimes even solve structural dynamics questions. Engineers knew leadership understood their challenges at a fundamental level because we were there doing the work with them.



Third, many of us deliberately stayed out of the spotlight. If you look through photos of our major victories in those days, you won‚Äôt find me in them. This wasn‚Äôt false modesty; it was strategic. I wanted teams to own their achievements completely. By giving them full credit, they grew more confident and capable for the next challenge.




The results were unprecedented in hardware: In early 2023, fewer than 200 people were responsible for over 25 different products, some of which were deployed across the world in the order of thousands.




Underlying these principles was a fundamental belief: I work for my team. These 550 engineers were dedicating the best years of their careers to Anduril, and I felt responsible for making those years meaningful. My job was to harness the unique superpowers of each individual and create an environment where they could thrive.



This philosophy has to start at the top, and the impact of this approach, exemplified by Brian, cascaded through our organization. When leaders instead prioritize expanding their scope and influence over developing their teams, they create a vacuum that attracts similar self-interested leaders. The result is inevitable: layers of political fiefdoms that don‚Äôt value winning at an outcome level and don‚Äôt care about their people.



We elevated our approach into the management structure by promoting engineers who had earned respect through technical excellence, and creating a ‚Äúscaffolding‚Äù system to support these technical leaders as they developed management skills.



        
            
            

            
                            The latest Anvil, equipped with energetics and Launch Box.
                    

    



        
            
            

            
                            Backside of the bistatic seeker.
                    

    



    




When it came to recruitment, we obsessed over what I called ‚Äúthe highest density and intensity of talent.‚Äù While talent density is a common concept, we focused equally on how to leverage a team‚Äôs capabilities through the environment we created. As Steve Jobs illustrated with his rock tumbler story, ordinary rocks become polished gems through friction against each other‚Äîjust as talented teams polish each other through productive conflict to create something exceptional.



Our hiring approach was uncompromising. We installed ‚Äúbar raisers‚Äù on interview panels‚Äîpeople who would reject candidates unless they were exceptional. I personally spent about 20% of my time on recruiting.



Even when scaling to 30‚Äì40 hires monthly, I interviewed most engineering candidates. I built Python dashboards to analyze our talent sources and maintained a LinkedIn Recruiter account. We expanded geographically, opening offices strategically to access new talent pools when my personal network was completely depleted.



We sought engineers who combined technical excellence with passion for our defense mission. We particularly valued former startup founders for their demonstrated agency and self-motivation.



Our performance management philosophy sat between two extremes: Elon‚Äôs approach of firing anyone on the left side of the performance curve versus Jensen Huang‚Äôs philosophy of trying to uplevel everyone. Elon believed that if you had five apples and one might be rotten, you have to throw out the whole batch. It‚Äôs a ruthless but logical view‚Äîbad apples can spoil the bunch. My approach aimed to be pragmatic‚Äîwe maintained annual 10% attrition targets, but focused on clear expectations and respectful transitions.



We created a quarterly performance coaching program that identified the bottom 10% who needed support. We established clear OKRs so everyone understood their metrics for success. With transparent expectations, underperforming engineers often recognized on their own when they weren‚Äôt the right fit, leading to more respectful separations. This avoided the toxic alternative, common in other companies, where managers undermine struggling employees behind their backs rather than addressing issues directly.



Preventing organizational bloat proved equally critical. B-players who don‚Äôt understand product requirements tend to inflate headcount needs. When business lines claimed they needed 50 people for a project, we evaluated the engineering requirements and often found half were unnecessary. Our hiring estimates typically ran at half of what stakeholders requested and a third of industry standards. I reviewed every engineering hiring proposal personally, rejecting many while we still grew rapidly.



Building this culture was like coding our organizational DNA‚Äîyou need the right sequences at the start. The legendary stories from Anduril‚Äôs early days became the foundational code, continuously retold and refreshed with new chapters, embedding traits that transformed new engineers into problem-solvers capable of the impossible. When I became responsible for new teams, engineers would approach me saying, ‚ÄúWe‚Äôve heard so many stories about what it was like at the beginning.‚Äù These stories weren‚Äôt just entertainment; they transmitted our values and showed what was possible. As our team grew by over 2,500%, these legends ensured that each new hire understood what made us different.



        
            
            

            
                            Thermal imager with a 900mm lens, in the R&D building‚Äôs dev test lab.
                    

    



    




Throughout my career, I‚Äôd worked on cutting-edge hardware systems‚Äîdrones, missiles, autonomous vehicles‚Äîthat could theoretically perform any task, but in practice were often constrained by limited autonomy and human-guided control. Despite extraordinary advances in robotic hardware, the foundational intelligence to power these systems lags far behind. Most autonomous robots remain essentially puppets on strings, awaiting human instruction or executing elaborately choreographed behaviors.



In February 2024, I‚Äôd been approached about advising a robotics startup and called Brian to discuss it. We talked about physical intelligence and how, while AI companies everywhere were chasing language models and reasoning, half of the world‚Äôs GDP was generated by physical labor‚Äîyet no one had cracked the foundational model for robots to function effectively in the real world.



Brian listened carefully, then said something that will stay with me forever: ‚ÄúI‚Äôve always seen you as a founder.‚Äù



For me, this wasn‚Äôt just encouragement‚Äîit was perspective from someone who had followed the same path, moving from leading engineering at Palantir to co-founding Anduril. Brian ultimately supported my growing belief that building a universal intelligence to unlock the limitless capabilities of robotic hardware‚Äîand fundamentally reshape humanity‚Äôs experience of physical labor‚Äîwas an opportunity I couldn‚Äôt pass up.



I thought then of my son, and the innumerable times he accompanied me, often at strange hours and with curious equipment in tow, to desert test sites, landing strips, and hotel rooftops. The best problems, I‚Äôve always told him, are the ones everyone else avoids, until they become impossible to ignore. When he tells me he wants to build things like me when he grows up, I feel the weight of the work I did at Anduril, and what I still have left to do.¬†¬†



In May 2024, I left the company as a full-time employee to co-found Physical Intelligence (PI). PI is now building the universal intelligence model that can finally close the gap: a single, powerful ‚Äúbrain‚Äù capable of bringing genuine autonomy to every physically actuated device, from drones and industrial robots to household appliances. Solving this problem means more than a technological breakthrough: it‚Äôs about fundamentally redefining humanity‚Äôs relationship with physical labor, enabling a productivity revolution on the scale of industrialization itself.



The point is that Anduril‚Äôs own mission is so impactful, and meant so much to me, that I couldn‚Äôt have left it for any other mission short of PI‚Äôs. Still, the decision to leave wasn‚Äôt easy. I‚Äôd helped build a team of 550 extraordinary engineers who had become family. But the culture of ownership we‚Äôd created meant they didn‚Äôt need me anymore.



        
            
            

            
                    

    



    




Adnan Esmail is the co-founder of Physical Intelligence and Emeritus SVP Engineering at Anduril.



    

                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Default Trap: Why Anthropic's Data Policy Change Matters]]></title>
            <link>https://natesnewsletter.substack.com/p/the-default-trap-why-anthropics-data</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45076274</guid>
        </item>
        <item>
            <title><![CDATA[New research reveals longevity gains slowing, life expectancy of 100 unlikely]]></title>
            <link>https://lafollette.wisc.edu/news/new-research-reveals-longevity-gains-slowing-life-expectancy-of-100-unlikely/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075813</guid>
            <description><![CDATA[A new study co-authored by a University of Wisconsin-Madison professor finds that life expectancy gains made by high-income countries in the first half of the 20th century have slowed significantly, and that none of the ‚Ä¶]]></description>
            <content:encoded><![CDATA[
	
		
			
			
			

	
		A new study co-authored by a University of Wisconsin-Madison professor finds that life expectancy gains made by high-income countries in the first half of the 20th century have slowed significantly, and that none of the generations born after 1939 will reach 100 years of age on average.
Published in the journal Proceedings of the National Academy of Sciences, the study by H√©ctor Pifarr√© i Arolas of the La Follette School of Public Affairs, Jos√© Andrade of the Max Planck Institute for Demographic Research, and Carlo Giovanni Camarda of the Institut national d‚Äô√©tudes d√©mographiques analyzed life expectancy for 23 high-income and low-mortality countries using data from the Human Mortality Database and six different mortality forecasting methods.
Assistant Professor H√©ctor Pifarr√© i Arolas
‚ÄúThe unprecedented increase in life expectancy we achieved in the first half of the 20th century appears to be a phenomenon we are unlikely to achieve again in the foreseeable future,‚Äù according to Pifarr√© i Arolas. ‚ÄúIn the absence of any major breakthroughs that significantly extend human life, life expectancy would still not match the rapid increases seen in the early 20th century even if adult survival improved twice as fast as we predict.‚Äù
From 1900 to 1938, life expectancy rose by about five and a half months with each new generation. The life expectancy for an individual born in a high-income country in 1900 was an average of 62 years. For someone born just 38 years later in similar conditions, life expectancy had jumped to 80 years on average.
For those born between 1939 and 2000, the increase slowed to roughly two and a half to three and a half months per generation, depending on the forecasting method. Mortality forecasting methods are statistical techniques that make informed predictions about future lifespans based on past and current mortality information. These models enabled the research team to estimate how life expectancy will develop under a variety of plausible future scenarios.
Doctoral student Jos√© Andrade of the Max Planck Institute for Demographic Research
‚ÄúWe forecast that those born in 1980 will not live to be 100 on average, and none of the cohorts in our study will reach this milestone. This decline is largely due to the fact that past surges in longevity were driven by remarkable improvements in survival at very young ages,‚Äù according to corresponding author Andrade.
At the beginning of the 20th century, infant mortality fell rapidly due to medical advances and other improvements in quality of life for high-income countries. This contributed significantly to the rapid increase in life expectancy. However, infant and child mortality are now so low that the forecasted improvements in mortality in older age groups will not be enough to sustain the previous pace of longevity gains.
While mortality forecasts can never be certain as the future may unfold in unexpected ways ‚Äì by way of pandemics, new medical treatments or other unforeseen societal changes ‚Äì this study provides critical insight for governments looking to anticipate the needs of their healthcare systems, pension planning and social policies.
Although a population-level analysis, this research also has implications for individuals, as life expectancy influences personal decisions about saving, retirement and long-term planning. If life expectancy increases more slowly as this study shows is likely, both governments and individuals may need to recalibrate their expectations for the future.
Share on: 	

	


	
		Post navigation
		
	
	

	


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LandChad, a site dedicated to turning internet peasants into Internet Landlords]]></title>
            <link>https://landchad.net</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075384</guid>
            <description><![CDATA[This is LandChad.net, a site dedicated to turning internet peasants into Internet Landlords by showing them how to setup websites, email servers, chat servers and everything in between.
Starting a website is something that can be done in a lazy afternoon and costs pocket change.
Most of the internet‚Äôs problems could be solved if more people had their own personal platforms, so the objective of this site is to guide any normal person through the process of installing a website.]]></description>
            <content:encoded><![CDATA[
This is LandChad.net, a site dedicated to turning internet peasants into Internet Landlords by showing them how to setup websites, email servers, chat servers and everything in between.
Starting a website is something that can be done in a lazy afternoon and costs pocket change.
Most of the internet‚Äôs problems could be solved if more people had their own personal platforms, so the objective of this site is to guide any normal person through the process of installing a website.
Start a website

‚ÄúBuild your own platform!‚Äù


Host your own services, social media and more.
Setup an Email Server

Maintaining a Server
Tips and articles on mastering your server and learning about GNU/Linux systems administration.

Certbot on Standalone Domains and Subdomains[server]
Cronjobs[server]
GeminiA minimalist alternative to HTTP with a modern twist.[server]
Log on with SSH Keys[server]
Maintaining a Server[server]
OpenAlias[server]
Page Quality[server]
Requiring Passwords for Webpages (HTTP Authentication)[server]
Rsync: Upload and Sync Files and Websites[server]
Self hosting[server]
Server-Side Scripting with CGI[server]
SSH - Advanced Usage[server]
Using UFW as a Firewall[server]



Support LandChad.net

BTC: bc1q9f3tmkhnxj8gduytdktlcw8yrnx3g028nzzsc5
XMR: 84RXmrsE7ffCe1ADprxLMHRpmyhZuWYScDR4YghE8pFRFSyLtiZFYwD6EPijVzD3aZiEpg57MfHEr1pGJNPXyJgENMnWrSh

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Have to Feel It]]></title>
            <link>https://mitchellh.com/writing/feel-it</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075048</guid>
            <description><![CDATA[You see a series of checkboxes checked. Schedules met.
Requirements satisfied. Demos delivered.
It's a good day. Good job, you, good job! A promotion is in sight.]]></description>
            <content:encoded><![CDATA[You see a series of checkboxes checked. Schedules met.
Requirements satisfied. Demos delivered.
It's a good day. Good job, you, good job! A promotion is in sight.
But you didn't feel it. You didn't feel it.
We, as people, feel something with every interaction. Frustration, joy, relief,
confidence. A feeling. A person interacts with our work. Our work evokes
a feeling. The feeling matters. The feeling is part of the work. The
desired feeling is part of the requirements.
When you feel it, you know. The feature makes you smile when you use it.
It fits right in, like it was always meant to be there. You want to
use it again. You want to tell people about it.
This is the difference. This is what metrics, specifications, and demos
miss. They don't capture the feeling. For the people who will use and live
in the work, the feeling is part of their daily experience. Which means
you can't stop at checking the boxes on paper. You have to sit with it,
use it, live with it.
You have to feel it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Condor's Cuzco RISC-V Core at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074895</guid>
            <description><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive.]]></description>
            <content:encoded><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor‚Äôs formation with a few RISC-V cores under their belt from years past.Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It‚Äôs in the same segment as high performance RISC-V designs like SiFive‚Äôs P870 and Veyron‚Äôs V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD‚Äôs C910 and SiFive‚Äôs P550.Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a ‚Äútime-based‚Äù scheduling scheme. I‚Äôll cover more on this later, but it‚Äôs important to note that this is purely an implementation detail. It doesn‚Äôt require ISA modifications or special treatment from the compiler for optimal performance.Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC‚Äôs 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core‚Äôs pipeline length relative to its competitors.As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.Cuzco‚Äôs frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn‚Äôt work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.AMD‚Äôs Zen 2, Ampere‚Äôs AmpereOne, and Qualcomm‚Äôs Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco‚Äôs TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco‚Äôs part, they‚Äôve disclosed that the TAGE predictor‚Äôs base component uses a 16K entry table of bimodal counters.Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor‚Äôs slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.Cuzco‚Äôs instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.Much of the action in Condor‚Äôs presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco‚Äôs renamer goes a step further and predicts instruction schedules as well.One parallel to this is Nvidia‚Äôs static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren‚Äôt standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction‚Äôs dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can‚Äôt find free resources in the search window, the instruction will be stalled at the ID2 stage.Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.Compared to a hypothetical ‚Äúgreedy‚Äù setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the ‚Äúgreedy‚Äù figure may not even be possible. A conventional out-of-order core wouldn‚Äôt have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face ‚Äúfalse‚Äù delays even though free execution units may be available on other scheduling queues.Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn‚Äôt attempt to predict memory dependencies like Intel‚Äôs Core 2, and also doesn‚Äôt try to predict whether a load will miss cache.Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it‚Äôs sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via ‚Äúpoison‚Äù bits. The erroneously executed instruction‚Äôs result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn‚Äôt a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core‚Äôs supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can‚Äôt issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There‚Äôs generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There‚Äôs one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel‚Äôs Golden Cove, albeit at much lower clocks.Cuzco‚Äôs load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD‚Äôs Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco‚Äôs core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There‚Äôs no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn‚Äôt have staying power. Intel‚Äôs Itanium sought to use an ISA-based approach, but failed to unseat the company‚Äôs own x86 cores that used out-of-order execution. Nvidia‚Äôs Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That‚Äôs driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn‚Äôt rely on a compiled microcode cache like Denver, so it doesn‚Äôt end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache missesIf you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI models need a virtual machine]]></title>
            <link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074467</guid>
        </item>
        <item>
            <title><![CDATA[Bcachefs Goes to "Externally Maintained"]]></title>
            <link>https://lwn.net/Articles/1035736/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074312</guid>
            <description><![CDATA[Linus Torvalds has quietly changed the maintainer status of bcachefs to 'externally maintained' [...]]]></description>
            <content:encoded><![CDATA[
[Posted August 29, 2025 by corbet]
               

Linus Torvalds has quietly changed
the maintainer status of bcachefs to "externally maintained",
indicating that further changes are unlikely to enter the mainline anytime
soon.  This change also suggests, though, that the immediate removal of
bcachefs from the mainline kernel is not in the cards.
             

      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 17:30 UTC (Fri)
                               by intelfx (subscriber, #130118)
                              [Link] (42 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 17:42 UTC (Fri)
                               by zdzichu (subscriber, #17118)
                              [Link] (41 responses)
      
      
      
I guess another maintainer, easier to work with, can continue the development of bcachefs. Subsystems with bus factor of one are frowned upon.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 19:21 UTC (Fri)
                               by NYKevin (subscriber, #129325)
                              [Link] (40 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 22:11 UTC (Fri)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (39 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 4:57 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [Link] (4 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 16:30 UTC (Sat)
                               by ttuttle (subscriber, #51118)
                              [Link] 
      
      
      
Thank you for posting such a compassionate response. This thread could easily turn into an unkind discussion or an all-out flame war, but you went out of your way to be kind instead.


      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 20:48 UTC (Sat)
                               by linuxrocks123 (subscriber, #34648)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 23:41 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] 
      
      
      

      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 8:10 UTC (Sun)
                               by NYKevin (subscriber, #129325)
                              [Link] 
      
      
      

      
          
        
     



    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 7:57 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (30 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 11:44 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (13 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 14:25 UTC (Sat)
                               by ma4ris8 (subscriber, #170509)
                              [Link] (12 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:21 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (11 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:00 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (10 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:48 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (9 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:09 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (6 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:43 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (5 responses)
      
      
      
Debian has processes for obtaining carvouts/exceptions for critical system packages, naturally with more review. E2fsprogs used them and that's what should have been done here; there was no need to rush packing bcachefs-tools for Debian.


      
          
        
     
      Debian
       Posted Aug 31, 2025 1:29 UTC (Sun)
                               by comex (subscriber, #71521)
                              [Link] (4 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 1:49 UTC (Sun)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (3 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 2:11 UTC (Sun)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 3:40 UTC (Sun)
                               by jmalcolm (subscriber, #8876)
                              [Link] 
      
      
      

      
          
        
     


    
      Debian
       Posted Aug 31, 2025 3:47 UTC (Sun)
                               by jmalcolm (subscriber, #8876)
                              [Link] 
      
      
      

      
          
        
     





    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:10 UTC (Sat)
                               by lordsutch (guest, #53)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 23:42 UTC (Sat)
                               by pizza (subscriber, #46)
                              [Link] 
      
      
      

      
          
        
     







    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 12:24 UTC (Sat)
                               by muase (subscriber, #178466)
                              [Link] (13 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:27 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (12 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 19:20 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (11 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:17 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (9 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:29 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (8 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:35 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (4 responses)
      
      
      
You'll be waiting a long time if you have no desire to engage in a fashion other than "are you ready to agree I was right yet".


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:41 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] 
      
      
      
And to be clear, 1) I am not "you Debian folks" here, and 2) I am not commenting on whether I agree or disagree with Debian's policies on bundling *because that's not the point here*.


      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:48 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 0:40 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:37 UTC (Sun)
                               by ben0x539 (guest, #119600)
                              [Link] 
      
      
      
Did just removing the package ever come up? Surely not shipping a piece of code is preferable for all parties over shipping a known broken configuration.


      
          
        
     




    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:39 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] 
      
      
      

      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:51 UTC (Sat)
                               by sheepdestroyer (guest, #54968)
                              [Link] (1 responses)
      
      
      
Just to understand what is discussed here, was there ever any technical reason mentioned in public from Debian for why Kent's recommendation to not unbundle rust dependencies could and should not be followed?


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 6:42 UTC (Sun)
                               by zejn (guest, #116440)
                              [Link] 
      
      
      

      
          
        
     




    
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 8:20 UTC (Sun)
                               by zejn (guest, #116440)
                              [Link] 
      
      
      

      
          
        
     




    
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:34 UTC (Sun)
                               by jmalcolm (subscriber, #8876)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:41 UTC (Sun)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] 
      
      
      

      
          
        
     



    
      A few suggestions (which you don‚Äôt have to follow)
       Posted Aug 30, 2025 17:12 UTC (Sat)
                               by DemiMarie (subscriber, #164188)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      A few suggestions (which you don‚Äôt have to follow)
       Posted Aug 30, 2025 22:24 UTC (Sat)
                               by koverstreet (‚ú≠ supporter ‚ú≠, #4296)
                              [Link] 
      
      
      

      
          
        
     


    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:53 UTC (Sat)
                               by ATLief (subscriber, #166135)
                              [Link] 
      
      
      

      
          
        
     





      A broken link?
       Posted Aug 29, 2025 17:38 UTC (Fri)
                               by ahippo (subscriber, #154692)
                              [Link] (5 responses)
      
      
      

      
          
        
     
      A broken link?
       Posted Aug 29, 2025 17:50 UTC (Fri)
                               by Poliorcetics (subscriber, #165001)
                              [Link] (3 responses)
      
      
      

      
          
        
     
      A broken link?
       Posted Aug 29, 2025 18:39 UTC (Fri)
                               by ewen (subscriber, #4772)
                              [Link] 
      
      
      

      
          
        
     

    
      A broken link?
       Posted Aug 29, 2025 19:03 UTC (Fri)
                               by ahippo (subscriber, #154692)
                              [Link] (1 responses)
      
      
      
Ah, yeah, that must be it!
Thank you for pointing me to that blog post!
My phone indeed has an odd number of cores.


      
          
        
     
      A broken link?
       Posted Aug 29, 2025 21:03 UTC (Fri)
                               by alspnost (guest, #2763)
                              [Link] 
      
      
      Fascinating - this is a whole new thing to me, but I guess I'm also "vulnerable", since I have a Pixel 8 Pro with 9 cores!


      
          
        
     



    
      A broken link?
       Posted Aug 30, 2025 13:17 UTC (Sat)
                               by Baughn (subscriber, #124425)
                              [Link] 
      
      
      

      
          
        
     


      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 29, 2025 17:39 UTC (Fri)
                               by JMB (guest, #74439)
                              [Link] (4 responses)
      
      
      

      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 29, 2025 17:47 UTC (Fri)
                               by zdzichu (subscriber, #17118)
                              [Link] (3 responses)
      
      
      
Typo? Looks like one bit flip: 'v' ‚Üí 01110110; 'b' ‚Üí 01100010. Linus is not using a filesystem with checksums‚ÄΩ


      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 30, 2025 0:28 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (2 responses)
      
      
      
Much more likely to have been a typo; b and v are next to each other on a qwerty keyboard.


      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 30, 2025 5:17 UTC (Sat)
                               by awilfox (guest, #124923)
                              [Link] 
      
      
      
Why was this typo made in the first place, though?  Are you telling me Linus types his email all the time instead of using `git commit -s`?  That seems.. really odd to me.


      
          
        
     

    
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 31, 2025 0:58 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] 
      
      
      
Could be either. I'd rate the relative probabilities of Linus writing using a keyboard and just outputting raw bits as roughly even.


      
          
        
     




      Why not removed?
       Posted Aug 29, 2025 18:13 UTC (Fri)
                               by mb (subscriber, #50428)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      Why not removed?
       Posted Aug 29, 2025 18:49 UTC (Fri)
                               by tux3 (subscriber, #101245)
                              [Link] 
      
      
      

      
          
        
     

    
      Why not removed?
       Posted Aug 29, 2025 22:47 UTC (Fri)
                               by hailfinger (subscriber, #76962)
                              [Link] 
      
      
      
There are quite a few advantages of keeping the code in the tree:
1. Minimal result: Users can continue to use bcachefs with newer kernels without having to patch the kernel, they just won't get bug fixes, but there will be no functional regression
2. Improvement with some effort by users: Users willing to patch the kernel can still apply any patches provided by Kent
3. Optimal result: A unicorn with the ability to work with Linus and Kent at the same time may appear, resulting in fixes from Kent being merged with the timing and criteria wanted by Linus


      
          
        
     


      Not so bad
       Posted Aug 29, 2025 21:36 UTC (Fri)
                               by birdie (guest, #114905)
                              [Link] 
      
      
      
I thought Linus would eject it from the kernel, but this change leaves the door open for continued development ‚Äî exactly what I asked of Linus in my private exchange with him and Kent. Maybe he listened in the end.


      
          
        
     

      Winning the battles, losing the war
       Posted Aug 30, 2025 22:01 UTC (Sat)
                               by julian67 (guest, #99845)
                              [Link] (1 responses)
      
      
      
View from a mildly interested end user:  if you alienate the people you depend on it doesn't matter if you're right or wrong on any particular technical issue. You lose. The people you claim to care for or represent? Congratulations, you lost it for them too.  There is a 30+ year old kernel project with well established protocols, doctrine, structure, methods - all in the public space, totally discoverable, explicitly described, well understood.  But you know better, so much better that you repeatedly ignore every precedent, explicitly oppose and argue every admonition, refuse all advice, bad mouth the people who have been there before and who you need......guess what?  You needed them but they don't need you. Your talent is not unique.  You are not Moses, Jesus or Buddha or Mohammed.  Other people can make file systems too. And they did and they do.  And they work.  And they get a lot of development.  That's why they work, they are in.  And you are out.  Because you are not a voice of truth in the wilderness.


      
          
        
     
      Winning the battles, losing the war
       Posted Aug 30, 2025 23:44 UTC (Sat)
                               by pizza (subscriber, #46)
                              [Link] 
      
      
      

      
          
        
     


      Mediation?
       Posted Aug 30, 2025 22:33 UTC (Sat)
                               by sheepdestroyer (guest, #54968)
                              [Link] (1 responses)
      
      
      
Couldn't some big weights interested in bcachefs (at Valve & others maybe?) just nicely ask Linus to let this one go, or find a positive solution to keep bcachefs upstream?


      
          
        
     
      Mediation?
       Posted Aug 31, 2025 0:41 UTC (Sun)
                               by willy (subscriber, #9762)
                              [Link] 
      
      
      
I love it that you think this hasn't already been attempted and failed.


      
          
        
     


      Are Linus' patches posted to the mailing lists?
       Posted Aug 31, 2025 1:01 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] 
      
      
      
What's the process nowadays with all these git pulls etc. Do all patches, including those by Linus, still end up on some mailing list? I didn't find this one in any, but maybe just not indexed yet.


      
          
        
     
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cognitive load is what matters]]></title>
            <link>https://github.com/zakirullin/cognitive-load</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074248</guid>
            <description><![CDATA[üß† Cognitive Load is what matters. Contribute to zakirullin/cognitive-load development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Cognitive Load is what matters
Readable version | Chinese translation | Korean translation | Turkish translation
It is a living document, last update: August 2025. Your contributions are welcome!
Introduction
There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.
Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.
Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.
Cognitive load

Cognitive load is how much a developer needs to think in order to complete a task.

When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.
Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.


We should reduce the cognitive load in our projects as much as possible.

  Cognitive load and interruptions
  

Types of cognitive load
Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.
Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.


Let's jump straight to the concrete practical examples of extraneous cognitive load.

We will refer to the level of cognitive load as follows:
üß†: fresh working memory, zero cognitive load
üß†++: two facts in our working memory, cognitive load increased
ü§Ø: cognitive overload, more than 4 facts

Our brain is much more complex and unexplored, but we can go with this simplistic model.

Complex conditionals
if val > someConstant // üß†+
    && (condition2 || condition3) // üß†+++, prev cond should be true, one of c2 or c3 has be true
    && (condition4 && !condition5) { // ü§Ø, we are messed up by this point
    ...
}
Introduce intermediate variables with meaningful names:
isValid = val > someConstant
isAllowed = condition2 || condition3
isSecure = condition4 && !condition5 
// üß†, we don't need to remember the conditions, there are descriptive variables
if isValid && isAllowed && isSecure {
    ...
}
Nested ifs
if isValid { // üß†+, okay nested code applies to valid input only
    if isSecure { // üß†++, we do stuff for valid and secure input only
        stuff // üß†+++
    }
} 
Compare it with the early returns:
if !isValid
    return
 
if !isSecure
    return

// üß†, we don't really care about earlier returns, if we are here then all good

stuff // üß†+
We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.
Inheritance nightmare
We are asked to change a few things for our admin users: üß†
AdminController extends UserController extends GuestController extends BaseController
Ohh, part of the functionality is in BaseController, let's have a look: üß†+
Basic role mechanics got introduced in GuestController: üß†++
Things got partially altered in UserController: üß†+++
Finally we are here, AdminController, let's code stuff! üß†++++
Oh, wait, there's SuperuserController which extends AdminController. By modifying AdminController we can break things in the inherited class, so let's dive in SuperuserController first: ü§Ø
Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.
Too many small methods, classes or modules

Method, class and module are interchangeable in this context

Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.
Deep module - simple interface, complex functionality
Shallow module - interface is relatively complex to the small functionality it provides


Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.

Information hiding is paramount, and we don't hide as much complexity in shallow modules.

I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.
Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.

The best components are those that provide powerful functionality yet have a simple interface.
John K. Ousterhout

The interface of the UNIX I/O is very simple. It has only five basic calls:
open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)
A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.

This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.

P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.
Responsible for one thing
All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.
We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.

A module should be responsible to one, and only one, user or stakeholder.

This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.
But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.
Too many shallow microservices
This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.
I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. It took an enormous amount of time to reproduce and debug an issue in such a distributed system. Both time to market and cognitive load were unacceptably high. ü§Ø
Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.
The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.
A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.
Feature-rich languages
We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.
If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!
You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. ü§Ø
These statements are made by none other than Rob Pike.

Reduce cognitive load by limiting the number of choices.

Language features are OK, as long as they are orthogonal to each other.

  Thoughts from an engineer with 20 years of C++ experience ‚≠êÔ∏è
  
  I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!
  I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.
  Like, can you imagine, the token || has a different meaning in requires ((!P<T> || !Q<T>)) and in requires (!(P<T> || Q<T>)). The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.
  You can't allocate space for a trivial type and just memcpy a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.
  Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. ü§Ø
  There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then... ü§Ø
  This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).
  I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.
  By no means I am trying to blame C++. I love the language. It's just that I am tired now.Thanks to 0xd34df00d for writing.

Business logic and HTTP status codes
On the backend we return:
401 for expired jwt token
403 for not enough access
418 for banned users
The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:
401 is for expired jwt token // üß†+, ok just temporary remember it
403 is for not enough access // üß†++
418 is for banned users // üß†+++
Frontend developers would (hopefully) introduce some kind numeric status -> meaning dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.
Then QA engineers come into play:
"Hey, I got 403 status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.
Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:
{
    "code": "jwt_has_expired"
}
Cognitive load on the frontend side: üß† (fresh, no facts are held in mind)
Cognitive load on the QA side: üß†
The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.

People spend time arguing between 401 and 403, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.

P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.
Abusing DRY principle
Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.
Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. ü§Ø
In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.
Rob Pike once said:

A little copying is better than a little dependency.

We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.
All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.
Tight coupling with a framework
There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.
Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. ü§Ø
By no means do we advocate to invent everything from scratch!
We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.

Why I Hate Frameworks

Layered architecture
There is a certain engineering excitement about all this stuff.
I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. ü§Ø
Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture‚Äôs layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. ü§Ø
This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.

  Coding principles and experience
  
  @flaviocopes

If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.

With a sufficient number of users of an API,
it does not matter what you promise in the contract:
all observable behaviors of your system
will be depended on by somebody.

We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.
So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.
These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.
Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.
Layers of abstraction aren't free of charge, they are to be held in our limited working memory.


Domain-driven design
Domain-driven design has some great points, although it is often misinterpreted. People say, "We write code in DDD", which is a bit strange, because DDD is more about the problem space rather than the solution space.
Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.
Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. ü§Ø
Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.
Cognitive load in familiar projects

The problem is that familiarity is not the same as simplicity. They feel the same ‚Äî that same ease of moving through a space without much mental effort ‚Äî but for very different reasons. Every ‚Äúclever‚Äù (read: ‚Äúself-indulgent‚Äù) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get ‚Äúthe new kid‚Äù to critique the code before they get too institutionalised!
It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.
In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: ‚ÄúWhen there are only 2 or 3 conditionals, adding another one doesn‚Äôt make any difference. By the time there are 20 or 30 conditionals, adding another one doesn‚Äôt make any difference!‚Äù
There is no ‚Äúsimplifying force‚Äù acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.
Thanks to Dan North for his comment.

If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.


The more mental models there are to learn, the longer it takes for a new developer to deliver value.
Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.
If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.
Examples

Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres
How Instagram scaled to 14 million users with only 3 engineers
The companies where we were like ‚Äùwoah, these folks are smart as hell‚Äù for the most part failed
One function that wires up the entire system. If you want to know how the system works - go read it

These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.
Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.
Maintaining software is hard, we would need every bit of mental effort we can save.
Conclusion
Imagine for a moment that what we inferred in the second chapter isn‚Äôt actually true. If that‚Äôs the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. ü§Ø
Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.


We should reduce any cognitive load above and beyond what is intrinsic to the work we do.

LinkedIn, X, GitHub
Readable version

    Comments
    
    Rob PikeNice article.
    Andrej Karpathy (ChatGPT, Tesla)Nice post on software engineering. Probably the most true, least practiced viewpoint.
    Elon MuskTrue.
    Addy Osmani (Chrome, the most complex software system in the world)I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.
    The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."
    What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.
    And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.
    Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.
    One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.
    Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. 
    Sometimes the simplest solution is the best one, even in a complex system.
    antirez (Redis)Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.
    A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.
    Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).
    These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D
    A developer from the internetYou would not hire me... I sell myself on my track record of released enterprise projects.
    I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.
    I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.
    I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.
    Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.
    Comments on Hacker News

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Client Protocol (ACP)]]></title>
            <link>https://agentclientprotocol.com/overview/introduction</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074147</guid>
            <description><![CDATA[Get started with the Agent Client Protocol (ACP)]]></description>
            <content:encoded><![CDATA[The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).
The protocol is still under development, but it should be complete enough to build interesting user experiences using it.Why ACP?
AI coding agents and editors are tightly coupled but interoperability isn‚Äôt the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.
This creates several problems:
Integration overhead: Every new agent-editor combination requires custom work
Limited compatibility: Agents work with only a subset of available editors
Developer lock-in: Choosing an agent often means accepting their available interfaces

ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.
Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.
This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.Overview
ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.
Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.
The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.Supported Editors

Zed
neovim through the CodeCompanion plugin

Supported Agents

Gemini
‚Ä¶ more coming soon ;)
]]></content:encoded>
        </item>
    </channel>
</rss>