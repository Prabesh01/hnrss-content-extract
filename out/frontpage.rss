<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 29 Aug 2025 12:57:44 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Deepnote (YC S19) is hiring engineers to build a better Jupyter notebook]]></title>
            <link>https://deepnote.com/join-us</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062914</guid>
            <description><![CDATA[We need powerful data science tools that help us explore, reason, and collaborate. These tools do not exist yet. Help us invent them.]]></description>
            <content:encoded><![CDATA[We build tools for explorersWe are here to revolutionize how data teams work together.We started Deepnote to help data teams solve the hardest problems. We don’t just need better algorithms, bigger data sets, and more computing power. We need tools that help us explore, collaborate, and share. These tools don’t exist yet. We need to invent them first.Data work is as much a scientific and creative process as it is an engineering one. It involves working together, failing, learning, and going back to the drawing board. Data professionals are explorers. To make projects successful, we need tools that are both powerful and easy to use. Tools that help us collaborate and share our work in an engaging way. Tools that make working with data fun again.That’s why we’re building the new standard in data tooling: a notebook that brings teams together to code, query, visualize, organize, and share — all in one place.We are building tools for explorers. Join us.Build the future with usWe’re building a collaborative notebook that beautifully integrates analytics and data science into every workflow and decision. But it’s not just about designing, shipping, and selling. It’s about the people who power it — and that means you.Get ready to do your best workTransforming how people work with data isn't easy. But we built a culture that allows us to do precisely that.We move with urgencyWe are a small, passionate team revolutionizing how data teams work. We give everyone the tools they need and enable them to take action.We keep learningWe are knowledge-seekers. We invest in continuous learning across every role and encourage a culture of proactive feedback.We take ownershipWe are makers. We expect everyone to be a decision-maker — no politics or walls to get in the way.We collaborateWe are partners. We work in a fully transparent environment and put open, effective communication above all else.Backed by the best in the businessWe’re backed by industry leaders — and they’re as excited about reimagining the future as we are.Y CombinatorIndex VenturesAccelGreg BrockmanCTO at OpenAIElad GilAngel InvestorNaval RavikantAngel InvestorElena VernaAngel InvestorExplore open positionsThousands of data professionals already use Deepnote — but we’re only scratching the surface of what’s possible. We’re building out our core team, and we want kind, curious explorers to join and grow with us.Senior Business Development Executive (B2B SaaS)Remote→Go to jobHead of Data / Solutions EngineerPrague→Go to job]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Meta might be secretly scanning your phone's camera roll]]></title>
            <link>https://www.zdnet.com/article/meta-might-be-secretly-scanning-your-phones-camera-roll-how-to-check-and-turn-it-off/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062910</guid>
            <description><![CDATA[Some Facebook users have noticed new settings that let Meta analyze and retain your phone's photos. Yes, you read that right.]]></description>
            <content:encoded><![CDATA[   Why is Facebook cloud-processing my device's camera roll?   Meta is uploading and analyzing your camera roll photos and videos, even ones you haven't posted, in its cloud in order to generate AI-powered suggestions like collages, monthly recaps, themed albums, or AI-restyled versions of your images.      Where is this feature being tested?   Meta has confirmed the feature is a test, saying, "We're exploring ways to make content sharing easier for people on Facebook by testing suggestions of ready-to-share and curated content from a person's camera roll."   The test is currently available in the US and Canada, but it's not available in Illinois or Texas due to those states' privacy laws.      Did Facebook ask for my consent before turning this on?   Meta is showing a pop-up asking users if they want to enable cloud processing, but some users claim they haven't seen it. Instead, they found the toggles in their settings already switched on by default, raising questions about whether clear consent was given.      Elyse Betters Picaro / ZDNET Can I remove my photos once they've been uploaded? ZDNET's sister site, CNET, reports that Meta pulls from your newer pictures (roughly the last 30 days) and if you disable the feature, your uploaded photos will be deleted after 30 days. The only way to confirm is by downloading your Facebook account data.   Why is this a potential privacy issue?   It expands Meta's reach beyond the content you've chosen to upload and share online -- into your private, unposted photos and videos. For many, that's a major red flag and a line they're not comfortable crossing, understandably so.   Also: What Zuckerberg's 'personal superintelligence' sales pitch leaves outEven if Meta is asking for consent to access your camera roll in order to analyze your phone's photos and provide AI-powered suggestions, the company could have done a better job of being clear and explicit about what it's trying to do.   How many users, like me, simply dismissed the consent pop-up without fully realizing what they'd just agreed to?   Editor's note: This article was updated on Aug. 24, 2025 to clarify that Meta's camera roll sharing suggestions are not turned on by default and are entirely opt-in. Still, some users say they never knowingly agreed and are finding the features enabled in their settings.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[If you have a Claude account, they're going to train on your data moving forward]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1n2ubjx/if_you_have_a_claude_personal_account_they_are/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062738</guid>
        </item>
        <item>
            <title><![CDATA[Anthropic reverses privacy stance, will train on Claude chats]]></title>
            <link>https://www.perplexity.ai/page/anthropic-reverses-privacy-sta-xH4KWU9nS3KH4Aj9F12dvQ</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062683</guid>
        </item>
        <item>
            <title><![CDATA[Tesla said it didn't have key data in a fatal crash. Then a hacker found it]]></title>
            <link>https://www.washingtonpost.com/technology/2025/08/29/tesla-autopilot-crashes-evidence-testimony-wrongful-death/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062614</guid>
            <description><![CDATA[The key evidence was presented last month to a jury, which found the company partially liable for the 2019 crash in Key Largo, Florida.]]></description>
            <content:encoded><![CDATA[Years after a Tesla driver using Autopilot plowed into a young Florida couple in 2019, crucial electronic data detailing how the fatal wreck unfolded was missing. The information was key for a wrongful death case the survivor and the victim’s family were building against Tesla, but the company said it didn’t have the data.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What the interns have wrought, 2025]]></title>
            <link>https://blog.janestreet.com/wrought-2025/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45062274</guid>
            <description><![CDATA[Yet again, we’re at the end of our internship season, and so it’s time to summarize whatthe interns were up to!]]></description>
            <content:encoded><![CDATA[
            Yet again, we’re at the end of our internship season, and so it’s time to summarize what
the interns were up to!

This year, I was recommended a real bumper crop of exciting projects to include. It’s kind
of crazy how many great intern projects are out there. To mention a few that I’m not
going to have time to cover in detail:


  Annie Hu spent a big chunk of her summer investigating, implementing, and optimizing
different neural net sequence models, trying out a variety of compilation techniques and
toolchains.
  Aster Oransoy added build priorities to our build systems’ shared action execution
service, so you can ensure low-priority builds don’t slow down high-priority ones.
  Allen Pei wrote a quickcheck-like
system for creating automated tests of trading systems by generating randomized
sequences of market events, along with shrinking heuristics for creating minimal test
cases.
  Evan Thompson wrote an LSP for our inline CSS syntax extension which includes a CSS
validator that found tons of instances of invalid CSS in our applications.
  Zhibo Chen added a generic form of optional arguments to OCaml, so that it can use other
types than the traditional OCaml option type (including more efficient
representations) for optional values.
  Conor Kennedy added predicate
pushdown to our internal data
warehouse system to do filtration before it gets to the full query engine, and even
wrote a mini query planner for analyzing filter expressions to derive narrower key
ranges.
  Joe Cutler worked on using JIT-ing to make our
HardCaml simulator fast enough to be
competitive with Verilator, but with much better
start-up times.


And those are just the ones I felt like I could explain in a handful of words each!

As usual, I picked just three projects to go into in more detail. In particular:


  Leo Gagnon wrote a (sometimes dramatically) more efficient evaluator for JSQL, our
internal SQL dialect that we use for lots of different user-facing tools.
  Aryan Khatri built a new version of our OCaml torch
bindings that leverage OxCaml’s new features for
controlling memory management to build bindings that clean up tensors safely and
deterministically.
  Anthony Li wrote a library for managing memory across processes within our trading
systems via ref-counting, making it possible to more efficiently and safely ship data
across the process boundary.


Let’s dive in!

Faster (J)SQL evaluation

We use a lot of SQL at Jane Street, both in standard Postgres (or similar) databases
floating around, and for accessing our own homegrown analytics-oriented data warehouse
software.

Over time, we came to realize that SQL was sufficiently well-known internally that we
wanted to use it beyond the context of databases, as a general language for filtering and
transforming tabular data.  This could be useful in all sorts of contexts: web UIs, data
visualization tools, trading-systems configuration tools, etc.

The problem with this idea is… which version of SQL should you use?  Every database you
look at has its own charmingly unique SQL dialect that’s almost but not quite the same as
all the others.

We decided to deal with this by (I
know, I know) building our own dialect of SQL called JSQL.  We’ve
built a bunch of tools for using JSQL, including parsers, translators to other SQL
dialects, web-UI components, and a collection of different in-memory evaluators for
computing the results of a JSQL expression without invoking a traditional database at all.

Our evaluators started out very simple, doing little more than walking though a collection
of rows and one-by-one evaluating whether they passed or failed a WHERE clause. Over time,
we’ve built multiple evaluators with different performance properties, including
incremental evaluators.

That said, none of our evaluators were all that sophisticated, and in particular, none of
them made use of indexing. Leo Gagnon’s project was to change that!

The idea was that when presented with data that’s in an indexed container, like a Map.t
or Hashtbl.t, to be able to use that indexing to more efficiently filter down to the
data you need.  So, if you have a SELECT statement where the WHERE clause contains:

author = "Dijkstra" AND publication_year > 1980



and the underlying data is contained in, say, a Paper.t list String.Map.t (a map from author names to 
lists of their papers), Leo’s evaluator would have to:


  determine that we only care about things under the key "Dijkstra",
  use an O(log n) Map.find to get the resulting Paper.t list,
  use List.filter on the resulting much smaller list to select the papers with
publication_year > 1980


Which is way more efficient than walking over the entire map.

Getting this done involved a bunch of steps!


  
    Building a selection type that represented the possible over-approximations of the
range of keys that would be needed to satisfy a given query.
  
  
    Writing code to extract and optimize the selection for a given query.
  
  
    Writing code to specialize the execution of the selection to the backing store for
the data.  For example, the selection type tracks when ranges of queries are in scope.
The Map.t type supports efficient range queries, but the Hashtbl.t type doesn’t, so
you need different execution strategies depending on which you use to store your data.
  
  
    Supporting multi-index data-structures, like our Immutable_indexable_bag.  This
involved building selection heuristics that help us pick the most efficient index to
use.
  


And, of course, benchmarking.

The results of that benchmarking were pretty promising.  We ran some sample queries over
3.8 million rows of test data, comparing a linear scan over an array versus an
index-optimized scan over a Map.t.

This first query shows a ~700x speedup, since it lets us zoom in on just the MSFT trades,
ignoring everything else.

SELECT * WHERE und = "MSFT US" AND event_date > "2025-01-01"::date
+----------------------------------+--------------------+
| aggregator_name                  | average_time       |
+----------------------------------+--------------------+
| jsql-aggregations-eval           | 15.844514478s      |
| jsql-indexed-aggregations-eval   | 21.939788ms        |
+----------------------------------+--------------------+



This second query is more complicated, in that it requires us to do a scan over a range of
values, but we still get a ~30x speedup here.

SELECT * WHERE
  (und = "MSFT US" OR (und >= "AAPL US" AND und < "AMZN US"))
  AND event_date > "2025-01-01"::date
+--------------------------------+--------------------+
| aggregator_name                | average_time       |
+--------------------------------+--------------------+
| jsql-aggregations-eval         | 37.056874003s      |
| jsql-indexed-aggregations-eval | 1.324532585s       |
+--------------------------------+--------------------+



Despite this being a pretty algorithmic and performance-oriented project, a lot of the
challenges turned out to be about API design, and getting all of this work done with a
codebase that was simple and readable, and presented a convenient API to users.

Better Torch bindings

We use PyTorch a lot as part of our machine learning efforts, and as you might expect,
most of that work is done in Python. But sometimes, we want to drive PyTorch from OCaml,
which we do using ocamltorch, originally written by
Laurent Mazare some years back.

But OCaml is in some ways an awkward match for PyTorch, because OCaml manages memory using
a tracing GC, in contrast to Python, which uses a refcounting GC.

A lot of ink
has
been
spilled on the
tradeoffs between refcounting and tracing, but one clear difference is around the
determinism of collection.  With a tracing GC, it’s hard to know when the memory you’ve
allocated will be reclaimed. With refcounting, your object will be collected the moment
you drop your last reference to it.

This determinism comes in handy when you’re using your collector for managing things other
than main memory, like precious GPU memory.  This is a plot of GPU memory usage over time
doing one forward and backward pass on a batch, then some sampling, then 3 more batches,
written naively with ocamltorch.



This behavior is pretty awful! We’re holding on to tensors we just don’t need anymore,
which is basically intolerable.

You’d deal with this in ocamltorch by carefully calling Gc.full_major () after each
batch and each token sampled, to force the GC to recognize that the memory is unused and
reclaim it. That gives you the desired memory behavior:



but it’s a poor solution, since the calls to the GC are expensive, and there’s no
discipline to help you make sure you put them in the right place.

Aryan’s project was to build a better API for ocamltorch that provided a safe and
efficient discipline for managing tensor memory, leveraging some of the new features of
OxCaml, a set of extensions to OCaml that have been developed at Jane
Street.

The basic idea is to introduce a way of marking a scope of allocation for a tensor, using
this with_rc_scope function, where “rc” is short for “reference count”:

val with_rc_scope : (unit -> 'a) @ local -> 'a



The idea is that the body of the closure passed to this function acts as a scope, and that
any tensors allocated within it will have their refcounts decremented when the function
ends.

To make this all work, we use OxCaml’s local
mode to make sure that tensors can’t
escape their scope.  In particular, any function that allocates a tensor will allocate it
as a local value:

val ( + ) : t @ local -> t @ local -> t @ local



This prevents the allocated value from being returned from the closure passed to
with_rc_scope.

Here’s a worked example of how you might use this in practice.

    let vs = Var_store.create ~name:"vs" () in
    let opt = Optimizer.sgd vs ~learning_rate:1e-3 in
    let model = Model.init vs in
    for index = 1 to 100 do
      Tensor.with_rc_scope (fun () ->
        Optimizer.zero_grad opt;
        let ys_ = Model.forward model xs in
        let loss = Tensor.(mean (square (ys - ys_))) in
        Tensor.backward loss;
        Optimizer.step opt)
    done;



The full API is a bit more complicated than just that.  The system has support for nested
scopes, which is needed to support many of the idioms that are used in practice for both
training and inference workflows on GPUs.  As part of that, there is some special support
for returning tensors from an inner scope to an outer scope in a controlled way that
doesn’t violate the reference counting rules.

The project itself involved a lot of experimentation at the API level, to design
an API that was easy to use and understand and that also captured the memory-use patterns
we run into in practice. The project also had an interesting performance-engineering
aspect to it: removing all of the now-unnecessary GC invocations made it easier to understand
and identify further inefficiencies (like unnecessary synchronizations between the CPU and
GPU) that were harder to see amongst the performance mess created by the full_major
invocations.

We have more ideas about how to extend and improve these interfaces, but we already expect
the new APIs to be quite useful in their current form.  This is part of our open-source
code, so once the new code is released, you’ll be able to find it
here.



At Jane Street, we have lots of performance-sensitive trading systems that gather complex
information over the course of their execution, and then periodically serialize pieces of
that data over a shared-memory channel to another process.

This is generally a pretty good approach, but it has its limitations.  Serialization
always has a cost, but here it’s made worse by the fact that the data we want to send is
complex nested data with shared structure between messages. As a result, serializing the
data can involve serializing the same sub-structures over and over.

Anthony Li’s project was to build a library supporting a very different – and much more
efficient – approach.

The idea is to get rid of the serialization and deserialization altogether, and to just
pass pointers to the values in question instead. This requires that the space of objects
in question is visible to both processes, so it means we need to allocate those objects
within a shared memory segment.

We already have support for managing pools of objects in a shared memory segment, so this
sounds easy enough at first glance.  But the tricky bit is figuring out when you can
recycle one of your pooled objects.

We can’t rely on OCaml’s ordinary GC for this because the data resides in a shared-memory
segment between two processes, each with their own GC.  And anyway, we don’t want to be
churning the garbage collector in a latency-sensitive trading system.

Instead, Anthony’s project was to use a tried-and-true technique for this: reference
counting.

Safer refcounting through modes

Reference counting is tricky to integrate into a language like OCaml that doesn’t have it
designed in from the start.  There are really three invariants you need to get right for
this system to work:


  There are no data-races on the refcounts or the objects themselves
  Refcounts are incremented every time a new reference is created
  Refcounts are decremented every time a reference is destroyed


But how do we ensure that these rules are followed when they’re not natively enforced by
the runtime?  This is a bit like the problem Aryan ran into with reference counting in
PyTorch, and again, the solution is to leverage the system of modes to ensure the
necessary invariants.

We’ll need different modes at different times, so in order to manage this, we’re going to
have a special handle object o Handle.t that guards access to the underlying object (of
type o).  We can both use modes to protect the use of the handle itself, and the handle
can release the object o with specific modal types under specific circumstances.

That’s all a bit abstract, so let’s talk about the details:

Eliminating data races

There are really two data-race questions to handle here: one is about the refcounts, and
the other is about the actual objects being managed.  For the refcounts, an atomic
compare-and-set operation can be used to manage them in a safe way, so that’s pretty
simple, and doesn’t require anything from the mode system.

The mutability of the objects is more complicated, because the rules are different at
different times.  The objects must be mutable on initialization, since they have to be
filled in at that point.  But once you have multiple readers of the object, you really
need them to not change.  It turns out we can leverage OxCaml’s
visibility mode axis, which include
immutable, read, and read_write modes.

Specifically:


  
    During initialization, we expose the value under the read_write mode (which is the
default), so the data in the object can be set.  Notably, at this point, we’re
guaranteed there’s only one reference to the object in question.
  
  
    When reading, we expose objects under the read mode.  This way, multiple readers (even
across processes) can access the same object without fear of a race.
  


Notably, once an object’s reference count goes back to zero, it can again be the subject
of an initialization, so it can again be exposed read_write.

Another interesting aspect of this is that when we release the underlying values, we do so
under the local mode, to prevent the value from escaping its intended scope. As such,
what we’re implementing is analogous to borrow-checking in Rust.

Managing increments and decrements

The key invariant here is that people don’t just go about duplicating handles without
incrementing the associated reference count.  To ensure this, each Handle.t is created
under the unique mode, and all operations that use handles require that they be provided
uniquely.

This guarantees that all handles that are used are held uniquely, and so if you want to
refer to the handle in multiple places, an explicit copy function must be called.  And,
critically, that copy function increments the reference count.

There’s also a free operation that consumes a handle and decrements the reference count.
And a way of sending a handle to another process, at which point the sending handle is
consumed, and a receiving handle is created, without changing the reference count.

Anthony’s library is complete, and the team is now working it into our production systems.
We hope that this will be a library that’s useful to multiple teams across the firm.

Join us!

If this sounds like a fun way to spend your summer, you should apply to our internship
program.  Jane Street interns
get a chance to solve fun and challenging problems that have real impact.  I hope this
post gives you a sense of that!

        Yaron Minsky joined Jane Street back in 2002, and claims the dubious honor
of having convinced the firm to start using OCaml.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Probability of typing a wrong Bitcoin address]]></title>
            <link>https://www.johndcook.com/blog/2025/08/28/wrong-address/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45061980</guid>
            <description><![CDATA[How likely is it that a mistyped Bitcoin address is a valid address? How likely is it that some address is a plausible typo of another?]]></description>
            <content:encoded><![CDATA[
		I heard someone say that Bitcoin is dangerous because you could easily make a typo when entering an address, sending money to the wrong person, and have no recourse. There are dangers associated with Bitcoin, such as losing a private key, but address typos are not a major concern.
Checksums
There are several kinds of Bitcoin addresses. Each is at least 20 bytes (160 bits) long, with at least 4 bytes (32 bits) of checksum. The chances of a typo resulting in a valid checksum are about 1 in 232.
Used addresses
Let’s ignore the checksum for this section.
Because addresses are formed by cryptographic hash functions, we can assume the values are essentially randomly distributed in the space of possible addresses. The addresses are deterministic, but for modeling purposes, random is as random does.
This means a typo of an actual address is no more or less likely to be another actual address than an address typed at random. This is unlike, say, English words: a mistyped English word is more likely to be another English word than random keystrokes would be.
There have been on the order of a billion Bitcoin addresses used, in a space of 2160 possibilities. (Actually more since some addresses have more than 160 bits.) There’s about a 1 in 1039 chance that a random 160-bit sequence corresponds to an address somewhere on the Bitcoin blockchain.
Addresses close in edit distance
Someone with the Caesarean handle Veni Vidi Vici on X asked
What about the odds that out of those 1B addresses, two of them are one character swap away from each other?
That’s an interesting question. Let’s assume the addresses are Base58-encoded strings of length 26. Addresses could be longer, but assuming the minimum length increases the probability of addresses being close.
How many addresses are within one or two character swaps of another? I addressed a similar question here a couple weeks ago. If all the characters were unique, the number of strings within k swaps of each other would be
|S1(26, 26 − k)|
where S1 denotes Stirling numbers of the first kind. For k = 1 this would be 325 and for k = 2 this would be 50,050. This assumes all the characters are unique; I haven’t thought through the case where characters are repeated.
For round numbers, let’s say there are a billion addresses, and for each address there are a million other addresses that are close in some sense, plausible typos of the address. That would be 1012 addresses and typos, spread out in a space of ≈1045 (i.e. 5826) possible addresses.
Now there’s an implicit Birthday Problem here. No particular address is likely to collide with another, even when you allow typos, but what about the likelihood that some address collides?
Say we partition our space of 1045 addresses into N = 1029 addresses with a million possible typos for each address. Then as a rule of thumb, you’d need around √N random draws before you have a 50-50 chance of seeing a collision. Since 109 is a lot less than 1014.5, it’s unlikely that any two addresses collide, even when you consider each address along with a million associated typos.
			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Synology End Game]]></title>
            <link>https://lowendbox.com/blog/they-used-to-be-good-but-now-theyve-turned-to-evil-the-synology-end-game/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060920</guid>
            <description><![CDATA[Synology has recently implemented some pretty user-hostile policies, which means my long-term love affair with them is going to end in heartbreak.]]></description>
            <content:encoded><![CDATA[I’ve been a Synology fan for many years.  I used to roll my own NAS servers for home, but eventually decided that quieter, more energy-friendly dedicated NAS solutions were a better path forward.  I don’t use a lot of their on-board apps, just basic file storage.Right now I’ve got a DS920, a DS418, and a DS1522…but I probably won’t be buying another Synology again.Why?Their abusive, customer-hostile policies.Samba LimitsI started getting queasy when I read earlier this year that on some models, they limit how many concurrent connections you can make.  I though this was just something setup by default in smb.conf, but in fact Synology has a proprietary wrapper around the daemon that artificially limits it.Whiskey.  Tango.  Foxtrot.You Must Buy Your Hard Drives From UsFor a long time, Synology has only officially supported certain hard drives.  I don’t have a problem with this, for three reasons.  First, it was a pretty extensive list and included all the major players (WD, Seagate, etc.).  Second, it’s unreasonable to expect Synology to certify every single hard drive from every maker on the planet.  And finally, it was just a support limit.  In other words, you could use whatever hard drives you wanted, but if there was a problem, they wouldn’t be able to support you if the drive wasn’t on their list.I could live with that.  What I can’t live with is the new policy, implemented this year, where you must buy your drives from Synology.  This only affects new models from this year forward.  Details still seem sketchy, but rumor is that it’s going to be along the lines of “we don’t recognize your WD Black hard drive, therefore we won’t use it.”And by the way, Synology’s hard drives aren’t all that great.  My WD Blacks come with a 5 year warranty.  Synology’s only come with 3 years.Golf.  Foxtrot.  Yankee.Where to Now?I could go back to building my own, with TrueNAS.  In the past, my home-build NAS boxes were hand-me-down gaming PCs (because they were big enough towers) but I have to imagine one can find a case that allows tons of drives and is still powered by something modest.Or I may look at UGREEN.  Or Buffalo.  Or someone else.Raindog308 is a longtime LowEndTalk community administrator, technical writer, and self-described techno polymath. With deep roots in the *nix world, he has a passion for systems both modern and vintage, ranging from Unix, Perl, Python, and Golang to shell scripting and mainframe-era operating systems like MVS. He’s equally comfortable with relational database systems, having spent years working with Oracle, PostgreSQL, and MySQL.As an avid user of LowEndBox providers, Raindog runs an empire of LEBs, from tiny boxes for VPNs, to mid-sized instances for application hosting, and heavyweight servers for data storage and complex databases. He brings both technical rigor and real-world experience to every piece he writes.Beyond the command line, Raindog is a lover of German Shepherds, high-quality knives, target shooting, theology, tabletop RPGs, and hiking in deep, quiet forests.His goal with every article is to help users, from beginners to seasoned sysadmins, get more value, performance, and enjoyment out of their infrastructure.You can find him daily in the forums at LowEndTalk under the handle @raindog308.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Strange CW Keys]]></title>
            <link>https://sites.google.com/site/oh6dccw/strangecwkeys</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060161</guid>
            <description><![CDATA[Made by OH6DC
You can also use the Text-only index page (divided into useful categories).]]></description>
            <content:encoded><![CDATA[HomeStrange CW KeysMade by OH6DCYou can also use the Text-only index page (divided into useful categories).Lever arch file CW key Lambic pedals Valentine's day lollipop CW paddle Rubber stamp CW key Letter scale CW key Clamp cootie Code book Pepper mill CW key Lightsaber CW key Nutcracker CW key Straight(ener) key Smoke alarm CW key Teletubbygraph key Soap dispenser CW key Vinyl record player CW key Moomin triangle CW key Antiperspirant roll-on CW key Dual banana CW paddle Power twister CW key Power twister CW key Handsaw CW key Hole punch CW key Watering can CW key Toilet brush CW key CW glove Remote control CW key Tea bag CW key Eyebrow-raising CW key with optical transmitter Back scratcher CW key Whisk CW key Pliers CW key Liver casserole CW key Licorice pipe CW key Chocolate CW key Ski-W key Power drill CW keyer Six megapixel CW key Suspenders CW key Spirit bottle cap CW key Speed skate CW key Flower CW key Knee pad sideswiper CW key for portable operation QRP transmitter powered by a CW key Alarm clock CW key Hammer CW key CW gun Nail clipper CW key Ballpoint pen CW key Rotary dial CW key Hammock CW key Joystick CW key Rowing boat CW key Guitar CW key Wallet CW key Radio controlled CW key Amaryllis telegraphiensis Multi-function knife with CW key Toilet paper roll CW key Table ice hockey CW key Big toe CW key Waffle iron CW key Lego straight key Lego bug Pogo stick CW key Crutch CW key Smoke signal CW key CCW key Necktie CW key Toothbrush CW key Bench press CW key Handshake CW key Chopsticks CW key Trailer hitch CW key Typewriter CW keyboard Refrigerator CW key Mobile phone CW key Paper cup iambic paddles Morsetrap CW key Fingertips CW key Vacuum cleaner semi-automatic CW key Banana CW key Rolling pin CW key Toaster CW key Cheese slicer CW key Rocking chair CW key QLF pedal for left foot CW Cross-country ski shoe CW key CW insoles QRQ paddles Onion chopper CW key Beer can CW key Egg slicer CW key Stapler CW key Bicycle pump CW key Iron bar CW key Homebrew semi-automatic bug Hacksaw blade sideswiper CW key Plywood CW key Home  |  Homebrew QRP  Page updated Google SitesReport abuse]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PSA: Libxslt is unmaintained and has 5 unpatched security bugs]]></title>
            <link>https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060004</guid>
            <description><![CDATA[Alan Coopersmith reports:]]></description>
            <content:encoded><![CDATA[
Alan Coopersmith reports:

	  On 6/16/25 15:12, Alan Coopersmith wrote:
	  
	    BTW, users of libxml2 may also be using its sibling project, libxslt,
	    which currently has no active maintainer, but has three unfixed security issues
	    reported against it according to
	    
		https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt
	  
	  2 of the 3 have now been disclosed:
	  (CVE-2025-7424) libxslt: Type confusion in xmlNode.psvi between stylesheet and source nodes
	    https://gitlab.gnome.org/GNOME/libxslt/-/issues/139
	    https://project-zero.issues.chromium.org/issues/409761909
	  (CVE-2025-7425) libxslt: heap-use-after-free in xmlFreeID caused by `atype` corruption
	    https://gitlab.gnome.org/GNOME/libxslt/-/issues/140https://project-zero.issues.chromium.org/issues/410569369
	  Engineers from Apple & Google have proposed patches in the GNOME gitlab issues,
	  but neither has had a fix applied to the git repo since there is currently no
	    maintainer for libxslt.
	

Note that a fourth vulnerability was reported on June 18, 2025, which remains undisclosed to date (GNOME libxslt issue 148, link below), see
	  
	    https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt
	
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lucky 13: a look at Debian trixie]]></title>
            <link>https://lwn.net/Articles/1033474/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45059160</guid>
            <description><![CDATA[After more than two years of development, the Debian Project has released its new stable versio [...]]]></description>
            <content:encoded><![CDATA[

LWN.net needs you!

Without subscribers, LWN would simply not exist.  Please consider
       signing up for a subscription and helping
       to keep LWN publishing.


After more than two years of development, the Debian Project has released its new stable version, Debian 13 ("trixie"). The release comes with the usual bounty of
upgraded packages and more than 14,000 new packages; it also debuts Advanced Package Tool
(APT) 3.0 as the default package manager and makes 64-bit
RISC-V a supported architecture. There are few surprises with trixie,
which is exactly what many Linux users are hoping for—a free
operating system that just works as expected.

Debian's stable
releases are aptly named; the project prioritizes stability over
shipping the latest software. The freeze
schedule for trixie called for a soft freeze in April, which meant
that (for example) the KDE Plasma 6.4
release in June was too late to make the cut—even though trixie
was not released until August. Users who prefer to live on the edge
will want to run another distribution or follow Debian development by
running the testing release
that previews the next stable version—Debian 14 ("forky"). Truly
adventurous users may take their chances with the unstable ("sid")
release.

That said, trixie is up-to-date enough for many folks; it includes
GNOME 48, KDE Plasma 6.3, Xfce 4.20, GNU
Emacs 30.1, GnuPG 2.4.7, LibreOffice 25.2, and
more. Under the hood, it includes the most recent Linux LTS kernel
(6.12.41), GNU Compiler Collection (GCC) 14.2, GNU C Library (glibc)
2.41, LLVM/Clang 19, Python 3.13, Rust 1.85, and
systemd 257. The release notes have a section
for well-known software that compares the version in Debian 12
against Debian 13. While some of the versions lag a bit behind the
upstream, they are not woefully outdated.

The project now supports
six major hardware architectures: x86-64/amd64, 32-bit Arm with a
hardware FPU (armhf), 64-bit Arm (arm64), IBM POWER8 or newer
(ppc64el), IBM S/390 (s390x), and 64-bit RISC-V. The i386 architecture
is not supported for trixie, though the project continues to
build some i386 packages to run on 64-bit systems; users with i386 systems cannot upgrade to
trixie. The MIPS
architectures (mipsel and mis64el) have also been removed in trixie.

The Arm EABI
(armel) port that targets older 32-bit Arm devices prior to Arm v7 is
still supported with trixie, but this release is the end of the
line. There is no installation media for armel systems, but users who
have bookworm installed can upgrade to trixie if they have supported
hardware: the Raspberry Pi 1, Zero, and Zero W are the
only devices mentioned in
the release notes.

Upgrades from bookworm are supported, of course. The release
notes suggest that users convert APT source files to the DEB822 format
before the upgrade. APT 3.0
includes an "apt modernize-sources" command to convert APT data
source files to DEB822, but that is not available in bookworm. Users are
also expected to remove
all third-party packages prior to running the upgrade. I tested
the upgrade on one of my servers, after taking a snapshot to roll back
to if needed, and all went smoothly. Users who are considering an
upgrade should read the release notes carefully before forging ahead;
in particular, users should be aware that it's possible (but not
certain) for network interface names to change on upgrade.

Installation

For users who want to start fresh, Debian offers a
variety of installer images and download methods; users can choose
a 64MB minimal ISO image with the netboot
installer, all the way up to a set of Blu-ray images. The project
recommends using BitTorrent or Jigsaw
Download (jigdo) for the largest images. BitTorrent probably needs
no introduction, but jigdo is not as well-known. Jigdo is a method of
downloading all of the individual packages for an image from multiple
mirrors and then assembling them into an ISO image on the user's
machine. It was a bit fiddly to use jigdo to download an image, but
not overly so—and the speed of the whole process was comparable
to simply downloading an ISO of the same size.

Debian's network
install ("netinst") image is probably the best option for server
installations and for experienced Linux users; it includes the
packages required for a base install and then fetches the remaining
software from Debian mirrors. Unlike the tiny netboot image, it
includes the option of using either the graphical installer or the
text-based installer.

The installer is a bit of a throwback to an earlier era when users
were expected to know a lot more about the workings of a Linux system. 
Users who have only worked with distributions like Fedora and Ubuntu
will notice that installing Debian requires many more steps than other
popular distributions. For example, many desktop distributions have
eliminated the step of setting a password for the root
user—instead, it is generally assumed that the primary user will
also be the system administrator, so the default is to give the
primary user sudo privileges instead. Debian does not take that
approach; in fact, there is no way to give a user sudo privileges
during installation. Setting up sudo has to be done manually after
the installation is completed Update: Users can skip creation of a root account and the installer will then set up the regular user as an administrator with sudo permissions. Apologies for the error.

For some folks, installing Debian will be a bit of a chore and may
even be confusing for users who are new to Linux. For example, the
text-mode installer requires users to specify the device for GRUB boot
loader installation, without providing a default. If one chooses an
invalid partition, the installer tells the user that the operation has
failed and drops back to a menu listing all the installation
steps. Presumably if one picks the wrong partition it will
happily install GRUB to that and render the system unbootable. This is
not insurmountable for experienced Linux users, but it would no doubt
be a hurdle for many users.

More experienced Linux users are likely to appreciate the
amount of control offered by the installer. For example, Fedora's
recent web-based installer makes it difficult to even find the option to
perform custom partitioning. Debian has a guided partitioning option
for those who do not want to fuss with it, but the option to create
custom partitions is not hidden from the user.

Debian has a better installation option for newer Linux users,
though it is easy to miss: the live install images, which
use the Calamares installer. Its
workflow is more akin to the installation process one finds with
Fedora and Ubuntu; it also sets up the primary user with sudo
privileges rather than creating a root password. Unfortunately,
the live images are not listed on the main page for installer
images—though they are mentioned, briefly, in the release
notes.







The Debian installer also has the option of using a Braille display
and/or speech synthesizer voice for the installation. I have not tried
these options, but they are available for users who need them.

X.org

Many distributions are in the process of phasing out X.org support
for GNOME and KDE as the upstream projects have started doing so.
For example, Fedora will remove X.org session support
for GNOME in Fedora 43, and the plan is for Ubuntu to do the same
in its upcoming 25.10 release. GNOME will be completely removing X.org
support in GNOME 49, which is planned for September.

Much has already been said about this, of course, and there is
likely little new left to be said or that needs to be
said. However, for users who still need or want X.org support,
Debian 13 includes X.org sessions for GNOME and KDE. In testing
trixie, I've spent some time in the GNOME and KDE X.org sessions as
well as the Wayland sessions; if there are any gotchas or horrible
bugs, I haven't encountered them (yet). This might be a compelling
reason for some folks to switch to (or stick with) Debian.

Trying trixie

I use Debian for my personal web site and blogs, but it has been
quite some time since I used it as my primary desktop operating
system. Debian (and Ubuntu) derivatives, such as Linux Mint and Pop!_OS, yes—but it's been
several years since I've used vanilla Debian on the desktop for
more than casual tinkering.

The Debian release announcement boasts about the number of packages
included in trixie: 64,419 packages total, with 14,100 added and more
than 6,000 removed as obsolete
since bookworm. That is quite a few packages, but falls short of some
other distributions. For example, "dnf repoquery --repo=fedora
--available" shows more than 76,000 packages available for
Fedora 42.

After installing Debian, I went to install some of my preferred
software, such as aerc,
Ghostty, niri, and Speech Note. The aerc
packages in trixie are current, but Ghostty and niri are not packaged
for Debian at all. Ghostty is written in Zig, which is also not
available, so users who want to build it from source will need to
install Zig separately and then build Ghostty. Speech Note is packaged
as a Flatpak, but Debian does not enable Flatpaks or Flathub in the
GNOME Software Store by default. Users who want Flatpaks on Debian via
Flathub will need to install the flatpak package and manually
add the Flathub repo:

    flatpak remote-add --if-not-exists flathub \
      https://dl.flathub.org/repo/flathub.flatpakrepo


Users will need to add the gnome-software-plugin-flatpak
package for Flatpak support in GNOME Software, and
plasma-discover-backend-flatpak to add it to
KDE Discover.

Trixie ships with the Firefox extended-support release (ESR) by
default: Firefox
128, which was released in July 2024. Happily,
Mozilla offers a Debian
repository for those who want to run more current versions. Even
better, there is a little-advertised utility called extrepo that
has a curated list of external repositories users might want to enable
for Debian. To enable the Mozilla repository, for example, a user only
needs to install extrepo, run
"extrepo enable mozilla" as root (or with
sudo), update the package cache, and look for the regular
Firefox package. In all, extrepo includes more than 160 external
repositories for applications like Docker CE, Signal, and Syncthing. Unfortunately, the
extrepo utility does not have a separate "list" command to show the
available repositories, though running "extrepo search"
with no search parameter will return all of its DEB822-formatted
repository entries. Some of the software is
in an external repository due to a non-free license, other software (like
Firefox) just has a development cycle that outpaces Debian's.

As one might expect, the Debian desktop experience is not
dramatically different from other distributions; GNOME 48 on
Debian is little different than GNOME 48 on Fedora, and the same
is true for KDE, Xfce, etc. The primary difference is that users can
expect more or less the same desktop experience running Debian stable
in two years that they have today, which is not necessarily true for
other distributions.

Miscellaneous

One of the features in Debian 13 is something that most users
won't notice or appreciate at all: a transition to
64-bit time_t on 32-bit architectures, to avoid the Year 2038 problem. The
short version is that 32-bit integers cannot hold a Unix epoch
timestamp for dates after January 19, 2038. That may seem
like a distant concern, even irrelevant for Debian trixie; after all,
Debian 13 is only supported by the project until 2030. However,
the project expects that some 32-bit embedded systems will still be running
trixie in 2038, so Debian developers did the heavy lifting to complete
the transition to 64-bit time_t now. LWN covered the early planning
for this in 2023.

By now, most users have retired their DSA
SSH keys; if not, now is the time to do so. DSA keys were disabled by
default with OpenSSH in 2015, and they are entirely disabled now with
the openssh-client and openssh-server packages in
trixie. If there is a device that can, for some reason, only be
connected to with DSA, users can install the
openssh-client-ssh1 package and use ssh1 to make the
connection.

As we covered in
June 2024, Debian 13 has switched to using a tmpfs
filesystem for the /tmp directory. By default, Debian
allocates up to 50% of memory to /tmp, but this can be
changed by following the instructions
in the release notes. Note that this also applies to systems that
are upgraded to trixie from bookworm.

Forward to forky

Debian Project Leader (DPL) Andreas Tille recently
announced "Debian's 100000th birthday", so clearly the project has a
bit of experience with putting out solid releases. Granted, he was
reporting the number in binary, but even when converted to decimal 
numbers (32 years), it's an impressive track record.

While testing, I installed trixie on a couple of systems, including
a new Framework 12-inch laptop. My original intent was to just see
whether Debian had any problems with the new hardware (it didn't), but
now I'm leaning toward sticking with Debian on this system for a while
to see if stability suits me.

With trixie out the door, the Debian Project has already turned its
attention to working on forky, which has no release date set. Debian has
stuck to a loose schedule of a new stable release roughly every two
years. Most likely we will see Debian 14 sometime in 2027. After
the forky release, trixie will still receive updates from Debian's
security team through 2028, and then from its LTS team through 2030.

As of yet, there are no major new features or changes announced for
forky; it seems likely that those will be coming to light in the
coming months now that the project has trixie out the door. LWN will,
of course, be reporting on those developments as they happen.


            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Claude Sonnet will ship in Xcode]]></title>
            <link>https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058688</guid>
        </item>
        <item>
            <title><![CDATA[Rupert's Property]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/08/28/a-polyhedron-without-ruperts-property/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057561</guid>
            <description><![CDATA[You can cut a hole in a cube that’s big enough to slide an identical cube through that hole! Think about that for a minute—it’s kind of weird. Amazingly, nobody could prove any co…]]></description>
            <content:encoded><![CDATA[
				



You can cut a hole in a cube that’s big enough to slide an identical cube through that hole!   Think about that for a minute—it’s kind of weird.
Amazingly, nobody could prove any convex polyhedron doesn’t have this property!  It’s called ‘Rupert’s property’.
Until this week.
This week Steininger and Yurkevich proved there is a convex polyhedron that you can’t cut a hole in big enough to slide the entire polyhedron through the hole.  It has 90 vertices, and apparently 240 edges and 152 faces.




To prove that no such hole is possible, they had to do a computer search of 18 million different holes, plus use a lot of extra math to make sure they’d checked enough possibilities:
• Jakob Steininger and Sergey Yurkevich, A convex polyhedron without Rupert’s property.
To celebrate their discovery, they gave this polyhedron a silly name.  Since this polyhedron lacks Rupert’s property, they called it a ‘noperthedron’.
Why is this property called ‘Rupert’s property’?  Wikipedia explains:

In geometry, Prince Rupert’s cube is the largest cube that can pass through a hole cut through a unit cube without splitting it into separate pieces. Its side length is approximately 1.06, 6% larger than the side length 1 of the unit cube through which it passes. The problem of finding the largest square that lies entirely within a unit cube is closely related, and has the same solution.
Prince Rupert’s cube is named after Prince Rupert of the Rhine, who asked whether a cube could be passed through a hole made in another cube of the same size without splitting the cube into two pieces. A positive answer was given by John Wallis. Approximately 100 years later, Pieter Nieuwland found the largest possible cube that can pass through a hole in a unit cube.

Here Greg Egan shows how Rupert’s property works for the cube:


Here he shows how it works for the regular octahedron:


And finally, here’s a video by David Renshaw showing 26 polyhedra with Rupert’s property… and 5 polyhedra that might lack it:




The triakis tetrahedron is an extremely close call, but it does have Rupert’s property:




				
				
					
					This entry was posted  on Thursday, August 28th, 2025 at 7:50 am and is filed under mathematics.					You can follow any responses to this entry through the RSS 2.0 feed.
											You can leave a response, or trackback from your own site.
					
					
				

				
					Post navigation
					« Previous Post
					
				

			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Space Shuttle Columbia disaster and the over-reliance on PowerPoint (2019)]]></title>
            <link>https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057404</guid>
            <description><![CDATA[We’ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We’re so used to it we expect it.  We accept it.  We even consider it ‘learning’. As an educator I push against ‘death by PowerPoint’ and I'm fascinated with how we]]></description>
            <content:encoded><![CDATA[

      

      

      
        
          
            
              
            
          
        
      


      
      
      

      
        
        

  
  

    

    

      

      
        
          
        
        

        
          
            
          
        

        
          
          
            The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)
          
        
      
        
      

    
  We’ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We’re so used to it we expect it.  We accept it.  We even consider it ‘learning’. As an educator I push against ‘death by PowerPoint’ and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audience’s inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Whereas other shuttle crews had focused on work to the Hubble Space Telescope or to the International Space Station this mission was one of pure scientific research.  The launch proceeded as normal.  The crew settled into their mission.  They would spend 16 days in orbit, completing 80 experiments.  One day into their mission it was clear to those back on Earth that something had gone wrong.  As a matter of protocol NASA staff reviewed footage from an external camera mounted to the fuel tank.  At eighty-two seconds into the launch a piece of spray on foam insulation (SOFI) fell from one of the ramps that attached the shuttle to its external fuel tank.  As the crew rose at 28,968 kilometres per hour the piece of foam collided with one of the tiles on the outer edge of the shuttle’s left wing.  


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Frame of NASA launch footage showing the moment the foam struck the shuttle’s left wing (Creative Commons)
          
        
      
        
      

    
  It was impossible to tell from Earth how much damage this foam, falling nine times faster than a fired bullet, would have caused when it collided with the wing.   Foam falling during launch was nothing new.  It had happened on four previous missions and was one of the reasons why the camera was there in the first place.  But the tile the foam had struck was on the edge of the wing designed to protect the shuttle from the heat of Earth’s atmosphere during launch and re-entry.  In space the shuttle was safe but NASA didn’t know how it would respond to re-entry.  There were a number of options.  The astronauts could perform a spacewalk and visually inspect the hull.  NASA could launch another Space Shuttle to pick the crew up.  Or they could risk re-entry.  NASA officials sat down with Boeing Corporation engineers who took them through three reports; a total of 28 slides.    The salient point was whilst there was data showing that the tiles on the shuttle wing could tolerate being hit by the foam this was based on test conditions using foam more than 600 times smaller than that that had struck Columbia.  This is the slide the engineers chose to illustrate this point:

  NASA managers listened to the engineers and their PowerPoint.  The engineers felt they had communicated the potential risks.  NASA felt the engineers didn’t know what would happen but that all data pointed to there not being enough damage to put the lives of the crew in danger.  They rejected the other options and pushed ahead with Columbia re-entering Earth’s atmosphere as normal.  Columbia was scheduled to land at 0916 (EST) on February 1st 2003.  Just before 0900, 61,170 metres above Dallas at 18 times the speed of sound, temperature readings on the shuttle’s left wing were abnormally high and then were lost.  Tyre pressures on the left side were soon lost as was communication with the crew.  At 0912, as Columbia should have been approaching the runway, ground control heard reports from residents near Dallas that the shuttle had been seen disintegrating.  Columbia was lost and with it her crew of seven.  The oldest crew member was 48.  The shuttle programme was on lock down, grounded for two years as the investigation began.  The cause of the accident became clear: a hole in a tile on the left wing caused by the foam let the wing dangerously overheat until the shuttle disintegrated.  The questions to answer included a very simple one: Why, given that the foam strike had occurred at a force massively out of test conditions had NASA proceeded with re-entry?  Edward Tufte, a Professor at Yale University and expert in communication reviewed the slideshow the Boeing engineers had given NASA, in particular the above slide.  His findings were tragically profound.


 Firstly, the slide had a misleadingly reassuring title claiming that test data pointed to the tile being able to withstand the foam strike.  This was not the case but the presence of the title, centred in the largest font makes this seem the salient, summary point of this slide.  This helped Boeing’s message be lost almost immediately.























  Secondly, the slide contains four different bullet points with no explanation of what they mean.  This means that interpretation is left up to the reader.  Is number 1 the main bullet point?  Do the bullet points become less important or more?  It’s not helped that there’s a change in font sizes as well.  In all with bullet points and indents six levels of hierarchy were created.  This allowed NASA managers to imply a hierarchy of importance in their head: the writing lower down and in smaller font was ignored.  Actually, this had been where the contradictory (and most important) information was placed.  


Thirdly, there is a huge amount of text, more than 100 words or figures on one screen.   Two words, ‘SOFI’ and ‘ramp’ both mean the same thing: the foam.  Vague terms are used.  Sufficient is used once, significant or significantly, five times with little or no quantifiable data.  As a result this left a lot open to audience interpretation.  How much is significant?  Is it statistical significance you mean or something else?  
























Finally the single most important fact, that the foam strike had occurred at forces massively out of test conditions, is hidden at the very bottom.  Twelve little words which the audience would have had to wade through more than 100 to get to.  If they even managed to keep reading to that point.  In the middle it does say that it is possible for the foam to damage the tile.  This is in the smallest font, lost. 























  NASA’s subsequent report criticised technical aspects along with human factors.  Their report mentioned an over-reliance on PowerPoint: “The Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA.”  Edward Tufte’s full report makes for fascinating reading. Since being released in 1987 PowerPoint has grown exponentially to the point where it is now estimated than thirty million PowerPoint presentations are made every day.  Yet, PowerPoint is blamed by academics for killing critical thought.  Amazon’s CEO Jeff Bezos has banned it from meetings.   Typing text on a screen and reading it out loud does not count as teaching.  An audience reading text off the screen does not count as learning.  Imagine if the engineers had put up a slide with just: “foam strike more than 600 times bigger than test data.”  Maybe NASA would have listened.  Maybe they wouldn’t have attempted re-entry.  Next time you’re asked to give a talk remember Columbia. Don’t just jump to your laptop and write out slides of text.  Think about your message.  Don’t let that message be lost amongst text.  Death by PowerPoint is a real thing.  Sometimes literally.Thanks for reading - Jamie 


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Columbia’s final crew (from https://www.space.com/19436-columbia-disaster.html)
          
        
      
        
      

    

    

    

  

  

  

  
  
  


        
        
      

      

      

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Expert: LSP for Elixir]]></title>
            <link>https://github.com/elixir-lang/expert</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057322</guid>
            <description><![CDATA[Official Elixir Language Server Protocol implementation - elixir-lang/expert]]></description>
            <content:encoded><![CDATA[Expert
Expert is the official language server implementation for the Elixir programming language.
Installation
You can download Expert from the releases page for your
operating system and architecture. Put the executable somewhere on your $PATH, like ~/.local/bin/expert
For editor specific installation instructions, please refer to the Installation Instructions
Nightly Builds
If you want to try out the latest features, you can download a nightly build.
Using the GH CLI, you can run the following command to download the latest nightly build:
gh release download nightly --pattern 'expert_linux_amd64' --repo elixir-lang/expert
Then point your editor to the downloaded binary.
Building from source
To build Expert from source, you need Zig 0.14.1 installed on your system.
Then you can run the following command or follow the instructions in the Installation Instructions:
just release-local
This will build the Expert binary and place it in the apps/expert/burrito_out directory. You can then point your
editor to this binary.
Sponsorship
Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.






Corporate
For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at danj@erlef.org.
Individual
Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.
Other resources

Architecture
Development Guide
Glossary
Installation Instructions

LICENSE
Expert source code is released under Apache License 2.0.
Check LICENSE file for more information.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fuck up my site – Turn any website into beautiful chaos]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid>
            <description><![CDATA[PARODY/ENTERTAINMENT ONLY: Transform any website into pure chaos. Add burning cursors, Comic Sans everything, fake cursors, and more chaos features to any site. A humorous parody tool - not for real use. Some people just want to watch the web burn.]]></description>
            <content:encoded><![CDATA[This tool is for parody and entertainment purposes only. It temporarily applies visual chaos effects to websites for comedic effect. We do not store, collect, or transmit any personal information.NEVER enter passwords, credit card details, or any sensitive information while using this tool. The proxied sites are not secure and should not be used for any real transactions or logins.By using this tool, you acknowledge that it's purely for entertainment and you will not enter any sensitive data. Banking, financial, healthcare, and government sites are blocked for safety.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[I’m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

                ❄                ❄                ❄                ❄

I’ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren’t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn’t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that’s going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

                ❄                ❄                ❄                ❄

I’m often asked, “what is the future of programming?” Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it’s too late? My answer to all these questions is “I haven’t the foggiest”. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

                ❄                ❄               ❇                ❄

I’m also asked: “is AI a bubble”? To which my answer is “OF COURSE IT’S A BUBBLE”. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don’t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan… but it did not kill Amazon.

                ❄                ❄                ❄                ❄

I retired from public speaking a couple of years ago. But while I don’t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I’m looking forward to catching up with many of them at GOTO Copenhagen. I’ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

                ✢                ❄                ❄                ❄

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren’t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it’s just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn’t ask an LLM to calculate an answer than we can calculate deterministically (yes, I’ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

                ❄                ❄                ❄                ❄

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can’t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

                ❄                ❄                ❄                ❄

I’ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say “all tests green”, yet when I run them, there are failures. If that was a junior engineer’s behavior, how long would it be before H.R. was involved?

                ❄                ❄                ❄                ❄

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (“exfiltration”). That “untrusted content” can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attacker’s web page, and it could trick the agent to go to your bank account in another tab and “buy you a present” by transferring your balance to the kind attacker. Willison’s view is that “the entire concept of an agentic browser extension is fatally flawed and cannot be built safely”.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison’s LLM CLI, you’ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren’t just chatbots or
      autocomplete tools - they’re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that’s exactly what we did, and in
      this article I’ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You’ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they’re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we’re
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (I’ve
      shortened ours for the article). Now something magical happened. I could
      say “X isn’t working” and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes “fixed” failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al’s default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned “Bedrock is timing out”
      or “the model responses are getting truncated,” the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the iceberg—the AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentation—recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask “Why is
      this API call failing intermittently?” and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation—especially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capable—it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say “The authentication tests are failing, please fix the issue”
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestions—it was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit—like having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development—from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourself—even a simple version—gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates aren’t exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesn’t exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know what’s wrong with people?
                They’re too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence — however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say “it depends” all the time.
                Much more fun getting to say
                “let’s ship it and iterate” as Staff or
                “that won’t scale” as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? 🤨
}

              GPS coordinates aren’t exact.
                They’re noisy. They’re approximate. They’re probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              you’re probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesn’t capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach that’s both
              mathematically rigorous and surprisingly practical.
            
            As you’d expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2σ confidence 🤓
}

            When you compare two Uncertain values,
              you don’t get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/m³
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // m²
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed —
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesn’t exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              let’s take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or “fruit machine” for our UK readers 🇬🇧):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "◻️", "◻️", "◻️",  // blanks
            "🍒", "🍋", "🍊", "🍇", "💎"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("💎", "💎", "💎"): return 100  // Jackpot!
        case ("🍒", "🍒", "🍒"): return 10
        case ("🍇", "🍇", "🍇"): return 5
        case ("🍊", "🍊", "🍊"): return 3
        case ("🍋", "🍋", "🍋"): return 2
        case ("🍒", _, _), // Any cherry
             (_, "🍒", _),
             (_, _, "🍒"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically —
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: ≈ $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23°C
let tempSpread = temperature.standardDeviation() // about 1°C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)°C and \(upper)°C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp ≤ 25°C) < 20%
    print("Unlikely to be 25°C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users don’t notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is ±1000 meters,
            that’s a bad look 🤡
          So where do we go from here?
            Let’s channel our Senior+ memes from before for guidance.
          That Staff engineer saying “let’s ship it and iterate”
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineer’s warning of “that won’t scale”.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isn’t to eliminate uncertainty —
            it’s to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
    </channel>
</rss>