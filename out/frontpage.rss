<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 16:09:57 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Cloudflare Search Engine Market Share 2025Q2]]></title>
            <link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid>
        </item>
        <item>
            <title><![CDATA[Isolated(any)]]></title>
            <link>https://nshipster.com/isolated-any/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093590</guid>
            <description><![CDATA[There are cases where just a little more visibility and control over how to schedule asynchronous work can make all the difference.]]></description>
            <content:encoded><![CDATA[
              Ahh, @isolated(any).
                Itâ€™s an attribute of contradictions.
                You might see it a lot, but itâ€™s ok to ignore it.
                You donâ€™t need to use it, but I think it should be used more.
                It must always take an argument, but that argument cannot vary.
              Confusing? Definitely.
                But weâ€™ll get to it all.
              To understand why @isolated(any) was introduced,
                we need to take a look at async functions.
              let respondToEmergency: () async -> Void

              This is about as simple a function type as we can get.
                But, things start to get a little more interesting
                when we look at how a function like this is used.
                A variable with this type must always be invoked with await.
              await respondToEmergency()

              This, of course, makes sense.
                All async functions must be called with await.
                But! Consider this:
              let sendAmbulance: @MainActor () -> Void = {
    print("ğŸš‘ WEE-OOO WEE-OOO!")
}

let respondToEmergency: () async -> Void = sendAmbulance

await respondToEmergency()

              The explicit types are there to help make whatâ€™s going on clear.
                We first define a synchronous function that must run on the MainActor.
              And then we assign that to a plain old,
              non-MainActor async function.
            Weâ€™ve changed so much that you might find it surprising this even compiles.
          Remember what await actually does. It allows the current task to suspend. That doesnâ€™t just let the task wait for future work to complete. It also is an opportunity to change isolation. This makes async functions very flexible!
          Just like a dispatcher doesnâ€™t sit there doing nothing while waiting for the ambulance to arrive, a suspended task doesnâ€™t block its thread. When the dispatcher puts you on hold to coordinate with the ambulance team, thatâ€™s the isolation switch - theyâ€™re transferring your request to a different department that specializes in that type of work.
          
            But change to where, exactly?
          Ok, so we know that async functions, because they must always be awaited, gain a lot of flexibility. We are close, but have to go just a little further to find the motivation for this attribute.
          func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

await dispatchResponder {
    // no explicit isolation => nonisolated
    print("ğŸš’ HONK HOOOOONK!")
    await airSupport()
    print("ğŸš SOI SOI SOI SOI SOI!")
}

await dispatchResponder { @MainActor in
    print("ğŸš‘ WEE-OOO WEE-OOO!")
}

          We now have a function that accepts other functions as arguments. Itâ€™s possible to pass in lots of different kinds of functions to dispatchResponder. They could be async functions themselves, or even be synchronous. And they can be isolated to any actor. All thanks to the power of await.
        Except thereâ€™s a little problem now.
          Have a look at dispatchResponder on its own:
      func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

      The type of responder fully describes everything about this function,
        except for one thing.
        We have no way to know its isolation.
        That information is only available at callsites.
        The isolation is still present,
        so the right thing happens at runtime.
        Itâ€™s just not possible to inspect it statically or even programmatically.
        If youâ€™ve encountered type erasure before,
        this should seem familiar.
        The flexibility of async has come with a price -
        a loss of information.
      This is where @isolated(any) comes in.
      
        Using @isolated(any)
      
      We can change the definition of dispatchResponder to fix this.
    func dispatchResponder(_ responder: @isolated(any) () async -> Void) async {
    print("responder isolation:", responder.isolation)

    await responder()
}

    When you apply @isolated(any) to a function type, it does two things. Most importantly, it gives you access to a special isolation property. You can use this property to inspect the isolation of the function. The isolation could be an actor. Or it could be non-isolated. This is expressible in the type system with (any Actor)?.
    Functions with properties felt really strange to me at first.
      But, after thinking for a minute,
      it became quite natural.
      Why not?
      Itâ€™s just a type like any other.
      In fact, we can simulate how this all works with another feature:
      callAsFunction.
struct IsolatedAnyFunction<T> {
    let isolation: (any Actor)?
    let body: () async -> T

    func callAsFunction() async -> T {
        await body()
    }
}

let value = IsolatedAnyFunction(isolation: MainActor.shared, body: {
    // isolated work goes here
})

await value()

This analogy is certainly not perfect,
  but itâ€™s close enough that it might help.
There is one other subtle change that @isolated(any) makes to a function
  that you should be aware of.
  Its whole purpose is to capture the isolation of a function.
  Since that could be anything,
  callsites need an opportunity to switch.
  And that means an @isolated(any) function must be called with an await â€”
  even if it isnâ€™t itself explicitly async.
func dispatchResponder(_ responder: @isolated(any) () -> Void) async {
    await responder() // note the function is synchronous
}

This makes synchronous functions marked with @isolated(any) a little strange.
  They still must be called with await,
  yet they arenâ€™t allowed to suspend internally?
As it turns out, there are some valid (if rare) situations
  where such an arrangement can make sense.
  But adding this kind of constraint to your API
  should at least merit some extra documentation.

  How @isolated(any) Affects Callers
All of the task creation APIs â€”
  Task initializers and TaskGroup â€”
make use of @isolated(any).
These are used a lot
and are usually encountered very early on when learning about concurrency.
So, itâ€™s completely natural to run into this attribute and think:
â€œUgh another thing to understand!â€
Itâ€™s reasonable because
  the components of a function type dictate how it can be used.
  They are all essential qualities for API consumers.
  They are the interface.

  Parameters
  Return value
  Does it throw?
  Is it async?

This is not an exhaustive list,
  but whatâ€™s important is all of these are things callers must care about.
  Except for @isolated(any), which is the opposite.
  It doesnâ€™t affect callers at all.
This, I think, is the root of a lot of confusion around @isolated(any).
  Unlike other qualities of a function,
  this attribute is used to capture information for the API producer.
Iâ€™m so close to saying â€œyou can and should just ignore @isolated(any)â€œ.
  But I just cannot quite go that far,
  because there is one situation you should be aware of.

  Scheduling
To help understand when you should be thinking about using @isolated(any),
  Iâ€™m going to quote
  the proposal:

  This allows the API to make more intelligent scheduling decisions about the function.

Iâ€™ve highlighted â€œintelligent schedulingâ€,
  because this is the key component of @isolated(any).
  The attribute gives you access to the isolation of a function argument.
  But what would you use that for?
Did you know that, before Swift 6.0, the ordering of the following code was undefined?
@MainActor
func threeAlarmFire() {
    Task { print("ğŸš’ Truck A reporting!") }
    Task { print("ğŸš’ Truck B checking in!") }
    Task { print("ğŸš’ Truck C on the case!") }
}

Ordering turns out to be a very tricky topic when working with unstructured tasks.
  And while it will always require care, Swift 6.0 did improve the situation.
  We now have some stronger guarantees about scheduling work on the MainActor,
and @isolated(any) was needed to make that possible.

Take a look at this:
@MainActor
func sendAmbulance() {
    print("ğŸš‘ WEE-OOO WEE-OOO!")
}

nonisolated func dispatchResponders() {
    // synchronously enqueued
    Task { @MainActor in
        sendAmbulance()
    }

    // synchronously enqueued
    Task(operation: sendAmbulance)

    // not synchronously enqueued!
    Task {
        await sendAmbulance()
    }
}

These are three ways to achieve the same goal.
  But, there is a subtle difference in how the last form is scheduled.
  Task takes an @isolated(any) function
  so it can look at its isolation
  and synchronously submit it to an actor.
  This is how ordering can be preserved!
  But, it cannot do that in the last case.
  That closure passed into Task isnâ€™t actually itself MainActor â€”
it has inherited nonisolated from the enclosing function.
I think it might help to translate this into
  GCD.
func dispatchResponders() {
    // synchronously enqueued
    DispatchQueue.main.async {
        sendAmbulance()
    }

    // synchronously enqueued
    DispatchQueue.main.async(execute: sendAmbulance)

    // not synchronously enqueued!
    DispatchQueue.global().async {
        DispatchQueue.main.async {
            sendAmbulance()
        }
    }
}

Look really closely at that last one!
  What we are doing there is introducing a new async closure
  that then calls our MainActor function.
There are two steps.
This doesnâ€™t always matter,
but it certainly could.
And if you need to precisely schedule asynchronous work,
@isolated(any) can help.

  isolated(all)
All this talk about @isolated(any) got me thinkingâ€¦
Itâ€™s kinda strange that only some functions get to have this isolation property.
  It would certainly feel more consistent to me if all functions had it.
  In fact, I think we can go further.
  I can imagine a future where an explicit @isolated(any)
  isnâ€™t even necessary for async functions.
  As far as I can tell, there is no downside.
And a little less syntactic noise would be nice.
  Perhaps one day!

  isolated(some)
We do have to talk about that any.
  Itâ€™s surprising that this attribute requires an argument,
  yet permits only one possible value.
  The reason here comes down to future considerations.
The concrete actor type that this isolation property returns
  is always (any Actor)?.
  This is the most generic type for isolation and matches the #isolation macro.
  Today, there is no way to constrain a function to only specific actor types,
  such as @isolated(MyActor).
The any keyword here was chosen to mirror how protocols handle this.
But accepting an argument leaves the door open
to more sophisticated features in the future.
And that really fits the spirit of @isolated(any).
  Doing a little work now in exchange for flexibility down the road.
Because youâ€™ll see it in many foundational concurrency APIs,
  itâ€™s very natural to feel like you must understand @isolated(any).
  Iâ€™m 100% behind technical curiosity!
  In this case, however, it is not required.
  For the most part, you can just ignore this attribute.
  You will rarely, if ever, need to use it yourself.
But if you ever find yourself capturing isolated functions
  and passing them along to other APIs that use @isolated(any),
  you should consider adopting it.
  It could prove useful.
  Itâ€™s even a source-compatible change
  to add or remove this attribute from an async function.

So there you have it.
As with many parts of the concurrency system,
  thereâ€™s a surprising depth to @isolated(any).
  Thankfully, from a practical perspective,
  we can enjoy the ordering guarantees of task creation
  that it enables without needing to master it.
  And one less thing on this journey is most welcome.
Isolated maybe, but never alone.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid>
        </item>
        <item>
            <title><![CDATA[Effective learning: Twenty rules of formulating knowledge (1999)]]></title>
            <link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid>
            <description><![CDATA[This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge.]]></description>
            <content:encoded><![CDATA[Dr Piotr Wozniak, February, 1999 (updated)This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledgeThe speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!Â Â The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as inÂ SuperMemo).The 20 rules of formulating knowledge in learning1) Do not learn if you do not understandTrying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such â€œblind learningâ€ is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe â€œit will be useful some dayâ€. Â 2) Learn before you memorizeBefore you proceed with memorizing individual facts and rules, you need toÂ build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned inÂ Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g.Â the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g.Â What moves the pistons in the internal combustion engine?), etc.3) Build upon the basicsThe picture of the learned whole (as discussed inÂ Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!4) Stick to theÂ minimum information principleThe material you learn must be formulated in as simple way as it isSimple is easyBy definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex materialRepetitions of simple items are easier to scheduleI assume you will make repetitions of the learned material using optimum inter-repetition intervals (as inÂ SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split intoÂ ten or moreÂ simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!Here is a striking example:Ill-formulated knowledge â€“ Complex and wordyQ: What are the characteristics of the Dead Sea?A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earthâ€™s surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline watersWell-formulated knowledge â€“ Simple and specificQ: Where is the Dead Sea located?A:Â on the border between Israel and JordanQ: What is the lowest point on the Earthâ€™s surface?A:Â The Dead Sea shorelineQ: What is the average level on which the Dead Sea is located?A:Â 400 metersÂ (below sea level)Q: How long is the Dead Sea?A:Â 70 kmQ: How much saltier is the Dead Sea than the oceans?A:Â 7 timesQ: What is the volume content of salt in the Dead Sea?A:Â 30%Q: Why can the Dead Sea keep swimmers afloat?A:Â due to high salt contentQ: Why is the Dead Sea called Dead?A:Â because only simple organisms can live in itQ: Why only simple organisms can live in the Dead Sea?A:Â because of high salt contentYou might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e.Â the longer the time you need to remember knowledge, the more you benefit from simplifying your items!Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition!Â We want answer to be as short as imaginably possible!You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted toÂ enumerations)5) Cloze deletionÂ is easy and effectiveCloze deletionÂ is a sentence with its parts missing and replaced by three dots.Â Cloze deletion exerciseÂ is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example,Â Bill â€¦[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique calledÂ incremental reading.Ill-formulated knowledge â€“ Complex and wordyQ: What was the history of the Kaleida company?A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleidaâ€™s mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.Well-formulated knowledge â€“ Simple cloze deletionQ: Kaleida was funded to the tune of â€¦(amount) by Apple Computer and IBM in 1991A: $40 millionQ: Kaleida was funded to the tune of $40 million by â€¦(companies) in 1991A: Apple and IBMQ: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in â€¦ (year)A: 1991Q: â€¦(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three yearsA: Kaleidaâ€™sQ: Kaleidaâ€™s mission was to create a â€¦ It finally produced one, called Script X. But it took three yearsA: multimedia programming languageQ: Kaleidaâ€™s mission was to create a multimedia programming language. It finally produced one, called â€¦ But it took three yearsA: Script XQ: Kaleidaâ€™s mission was to create a multimedia programming language. It finally produced one, called Script X. But it took â€¦(time)A: three yearsQ: Kaleidaâ€™s mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as â€¦ had snapped up all the businessA: Macromedia/AsymetrixQ: Kaleidaâ€™s mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in â€¦(year)A: 1995Optional: SuperMemo Recipe:SuperMemo 2002SuperMemo 2000SuperMemo 98/99CreatingÂ cloze deletionsÂ in new SuperMemos:select the keyword that is to be replaced with tree dots and pressÂ Alt+ZGenerating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:1. PressÂ Ctrl+Alt+NÂ to paste the text to SuperMemoÂ 2. Select the part that is to be replaced with three dots 3. Right-click to open theÂ component menuÂ and selectÂ Reading : Remember clozeÂ (or click one of cloze icons on the reading toolbar)Â Cloze deletions in SuperMemo 98/99:1. PressÂ Ctrl+AÂ to add a standard question-and-answer item2. Paste the text into the question field. This will create the outline of your items3. PressÂ Ctrl+Alt+UÂ toÂ DuplicateÂ the element4. Select the part that is to be replaced with three dots5. Cut the selection to the clipboard (e.g. withÂ Shift+Del)6. Type in three dots (optionally, add the explanation in parentheses as in above examples)7. PressÂ Ctrl+TÂ to save the question field and move to the answer field8. Paste the text cut in Step 5 (e.g. withÂ Shift+InsÂ orÂ Ctrl+V). Your first item is ready9. PressÂ PgUpÂ to go back to the outline item created in Step 210. Goto Step 3 and continue adding new items6) Use imageryVisual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we sayÂ one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzanâ€™s mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.Less beneficial formulationQ: What African country is located between Kenya, Zambia and Mozambique?A: TanzaniaWell-formulated knowledge â€“ Simple cloze deletionQ: What African country is marked white on the map?A: Tanzania7) UseÂ mnemonic techniquesMnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will needÂ SuperMemoÂ and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as:Â mind maps, peg lists,Â mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!Exemplary mind map:(Six StepsÂ mind map generated inÂ Mind Manager 3.5, imported to SuperMemo 2004, courtesy of John England,Â TeamLink Australia)8) Graphic deletionÂ is as good as cloze deletionGraphic deletion works likeÂ cloze deletionÂ but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The studentâ€™s job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:SuperMemo 2000/2002SuperMemo 99This is how you can quickly generate graphic deletion using a picture from the clipboard:1. PressÂ Shift+InsÂ to paste the picture to SuperMemo2. PressÂ Ctrl+Shift+MÂ and chooseÂ OcclusionÂ template to apply graphic deletion template3. SuperMemo 2000 only: ChooseÂ Ctrl+Shift+F2Â to impose and detach theÂ OcclusionÂ template4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (useÂ Alt+clickÂ twice to set the rectangle in the dragging mode)In SuperMemo 99 you will need a few more steps:1.Create an item containing the following components:â€“ question text: What is the name of the area covered with the red rectangle?â€“ empty answer text (click Answer on the component menu)â€“ your illustration (use Import file on the image component menu)â€“ red rectangle component (choose red color with Color on the rectangle component menu)2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)3. Ctrl+click the rectangle component twice to place it in the dragging mode4. Drag and size the red rectangle to cover the area in question5. Type in the answer in the answer field6. Press PgUp to go back to the original element created in Step 17. Go to Step 2 to add generate more graphic deletionsNote that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from >SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above techniqueA more detailed recipe for creating occlusion tests is presented in:Â Flow of knowledge9) Avoid setsAÂ setÂ is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example:Â What countries belong to the European Union?Â You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them intoÂ enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of theÂ minimum information principle:Â you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason:Â Avoid sets!Â If you need them badly, convert them into enumerations and useÂ techniques for dealing with enumerationsIll-formulated knowledge â€“ Sets are unacceptable!Q:Â What countries belong to the European UnionÂ (2002)?A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.Well-formulated knowledge â€“ Converting a set into a meaningful listingQ: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?A: FranceQ: Which countries apart from France joined the European Coal and Steel Community in 1952?A: Germany, Italy and the BeneluxQ: What countries make up the Benelux?A: Belgium, Luxembourg, and the NetherlandsQ: Whose membership did Charles de Gaulle oppose in the 1960s?A: that of UKQ: Which countries joined the EEC along the UK in 1973?A: Ireland and DenmarkQ: Which country joined the EEC in 1981?A: GreeceQ: Which countries joined the EEC in 1986?A: Spain and PortugalQ: Which countries joined the EU in 1995?A: Austria, Sweden and FinlandQ: What was the historic course of expansion of the European Union membership?A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and FinlandNote that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with theÂ minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.10) Avoid enumerationsEnumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them usingÂ cloze deletionsÂ (overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:Hard to learn itemQ: What is the sequence of letters in the alphabet?A: abcdefghijklmnopqrstuvwxyzEasy to learn itemsQ: What three letters does the alphabet begin with?A: ABCQ: Fill out the missing letters of the alphabet A â€¦ â€¦ â€¦ EA: B, C, DQ: Fill out the missing letters of the alphabet B â€¦ â€¦ â€¦ FA: C, D, EQ: Fill out the missing letters of the alphabet C â€¦ â€¦ â€¦ GA: D, E, FThe above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabetÂ afterÂ making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict theÂ minimum information principleÂ becauseÂ the extra information is added in extra items.You can also deal with enumerations by using grouping like in the case of sets (see theÂ European Union example) but cloze deletions should be simpler and should suffice in most cases.Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurableA poem that is hard to rememberQ: The credit belongs â€¦ (Teddy Roosevelt)A: The credit belongs to the man whoâ€™s actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeatÂ A poem split into easy itemsQ: The credit belongs â€¦ (Teddy Roosevelt)A: to the man whoâ€™s actually in the arenaQ: The credit belongs to the man whoâ€™s actually in the arena â€¦A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)Q: whose face is marred by dust and sweat â€¦ (The credit belongs)A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)Q: a man who knows the great enthusiasm and the great devotions â€¦ (The credit belongs)A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)Q: who spends himself in a worthy cause â€¦ (The credit belongs)A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!11) CombatÂ interferenceWhen you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the wordsÂ historicÂ andÂ historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case ofÂ memory interference. You can often remember an item for years with straight excellent grades until â€¦ you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is toÂ detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and â€¦ aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:make items as unambiguous as possiblestick to theÂ minimum information principleÂ (many of the remaining rules in this text are based on avoiding interference!)eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the wordÂ ineptÂ you think â€œI know the meanings ofÂ ineptÂ andÂ inaptÂ but I will never know which is which!â€)in SuperMemo useÂ ViewÂ :Â Other browsers :Â Leeches(Shift+F3) to regularly review andÂ eliminateÂ most difficult itemsread more:Â Memory interference12) Optimize wordingThe wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based â€¦ blew past. PageMaker, now owned by Adobe, remains No. 2Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based â€¦ blew past. PageMaker, now owned by Adobe, remains No. 2A: QuarkBetter item: fewer words will speed up learningQ: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then â€¦ blew past (PageMaker remains No. 2)A: QuarkOr better:Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by â€¦A: QuarkOr better:Q: PageMaker failed to improve and was outdistanced by â€¦A: QuarkOr better:Q: PageMaker lost ground to â€¦A: QuarkNote that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name:Â Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!13) Refer to other memoriesReferring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the wordsÂ humbleÂ andÂ supplicantÂ helps the student focus on the wordÂ shamelesslyÂ and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the wordsÂ humbleÂ andÂ supplicantÂ makes it possible to avoid interference ofÂ cringingÂ with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rulesÂ basics-to-detailsÂ andÂ do not learn what you do not understandÂ require that the wordsÂ humbleÂ andÂ supplicantÂ be learned beforehand (or at least at the same time)Item subject to strong interferenceQ: derog: adj: shamelessly conscious of oneâ€™s failings and asking in a begging wayA: cringingItem that uses interfering memories to amplify the correct meaningQ: derog: adj: shamelessly humble and supplicantA: cringing14) Personalize and provide examplesOne of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the questionItem subject to strong interferenceQ: What is the name of a soft bed without arms or back?A: divanItem that uses interfering memories to amplify the correct meaningQ: What is the name of a soft bed without arms or back? (like the one at Robertâ€™s parents)A: divanIf you remember exactly what kind of soft bed can be found in Robertâ€™s parentsâ€™ apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant toÂ interferenceÂ and can greatly reduce your learning time15) Rely on emotional statesIf you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situationHarder itemQ: a light and joking conversationA: banterEasier itemQ: a light and joking conversation (e.g. Mandela and de Klerk in 1992)A: banterIf you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such asÂ badinageÂ or evenÂ chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent toÂ 25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted onÂ simplicityÂ and against theÂ interference16) Context cuesÂ simplify wordingYou can useÂ categoriesÂ inÂ SuperMemo 2000/2002, provide different branches of knowledge with a different look (differentÂ template), use reference labels (Title, Author, Date,Â etc.) and clearly label subcategories (e.g. with strings such asÂ chemÂ forÂ chemistry,Â mathÂ forÂ mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefixÂ bioch:Â saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the labelÂ biochÂ which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and â€¦ what is worse â€¦ you will light up the wrong areas of your brain that will now perhaps be prone to interference!Wordy item can cause accidental lapses through interferenceQ: What does GRE stand for in biochemistry?A: glucocorticoid response elementContext-labeled items increase success rateQ: bioch: GREA: glucocorticoid response element17) RedundancyÂ does not contradict minimum information principleRedundancyÂ in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict theÂ minimum information principleÂ and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate thatÂ minimum information principleÂ cannot be understood asÂ minimum number of characters or bits in your collections or even items:passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such asÂ phone-telefono, language-lingvo, hope-esperanto,Â etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail withÂ telefono-phone, lingvo-language,Â orÂ esperanto-hope.Â Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themInÂ SuperMemo 2000/2002, you can quickly generate swapped word-pair items withÂ DuplicateÂ (Ctrl+Alt+D) andÂ SwapÂ (Ctrl+Shift+S)reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of justÂ memorizingÂ the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) andÂ generateÂ the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitionsderivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read:Â Roots of genius and creativity, as well as more specific:Â Derivation, reasoning and intelligencemultiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with theÂ forgetting index)!flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g.Â a mark made by ink spilt on sthÂ =Â blot/blob/blotch)more18) ProvideÂ sourcesExcept for well-tested and proven knowledge (such asÂ 2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g.Â Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).19) ProvideÂ date stampingKnowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.20) PrioritizeYou will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see:Â learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:Prioritizing sourcesÂ â€“ there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of othersExtracting knowledgeÂ â€“ unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to useÂ incremental readingÂ toolsTransferring knowledge to SuperMemoÂ â€“ you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate itFormulating itemsÂ â€“ make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its contextUsing forgetting indexÂ â€“ you can use theÂ forgetting indexÂ to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priorityLearningÂ â€“ the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:RememberÂ (Ctrl+M) â€“ re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you chooseÂ Ctrl+MÂ you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantlyRescheduleÂ (Ctrl+J) â€“ manually schedule the date of the next repetitionExecute repetitionÂ (Ctrl+Shift+R) â€“ manually execute a repetition even before the repetitionâ€™s due date (e.g. when reviewing particularly important material)ForgetÂ (Ctrl+R)- remove the current item from the learning process and place it at the end of theÂ pending queueDismissÂ (Ctrl+D)Â â€“ ignore the current item in the learning process altogetherDeleteÂ (Ctrl+Shift+Del) â€“ remove the current item from your collectionChange the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)SummaryHere again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:Do not learn if you do not understandLearn before you memorizeÂ â€“ build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!Build upon the basicsÂ â€“ never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit inStick to the minimum information principleÂ â€“ if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)Cloze deletion is easy and effectiveÂ â€“ completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginnersUse imageryÂ â€“ a picture is worth a thousand wordsUse mnemonic techniquesÂ â€“ read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You wonâ€™t have problems with phone numbers and complex figuresGraphic deletion is as good as cloze deletionÂ â€“ obstructing parts of a picture is great for learning anatomy, geography and moreAvoid setsÂ â€“ larger sets are virtually un-memorizable unless you convert them into enumerations!Avoid enumerationsÂ â€“ enumerations are also hard to remember but can be dealt with using cloze deletionCombat interferenceÂ â€“ even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal lifeOptimize wordingÂ â€“ like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maximsRefer to other memoriesÂ â€“ building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gapsPersonalize and provide examplesÂ â€“ personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memoriesRely on emotional statesÂ â€“ emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in rememberingContext cues simplify wordingÂ â€“ providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interferenceRedundancy does not contradict minimum information principleÂ â€“ some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!Provide sourcesÂ â€“ sources help you manage the learning process, updating your knowledge, judging its reliability, or importanceProvide date stampingÂ â€“ time stamping is useful for volatile knowledge that changes in timePrioritizeÂ â€“ effective learning is all about prioritizing. InÂ incrementalÂ readingÂ you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also:Â Incremental reading,Â Devouring knowledge,Â Flow of knowledge,Â Using tasklists]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Turns out Google made up an elaborate story about me]]></title>
            <link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid>
        </item>
        <item>
            <title><![CDATA[Git for Music â€“ Using Version Control for Music Production (2023)]]></title>
            <link>https://grechin.org/2023/05/06/git-and-reaper.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid>
            <description><![CDATA[last updated on 6 Apr 2024]]></description>
            <content:encoded><![CDATA[
        

  

  
    
  last updated on 6 Apr 2024


Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.

Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.

Okay, and now letâ€™s get to the point andâ€¦



Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?

my-cool-song-new-vocals-brighter-mix-4.rpp

Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?

This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as â€œgitâ€, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).

For music production, I use Reaper, and instead of creating dozens of copies of my project file (my-cool-song.rpp), such as my-cool-song-new-vocals-brighter-mix-4.rpp, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the â€œhomeâ€ for managing the version of our music project.

By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.


  


My git-based music production workflow

Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install git-bash, and have all the git functionality at your fingertips through a command-line interface.

First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$


in the example above:

  I first navigated to the directory with my project with cd command
  initialized a repository with git init .
  on the last, third line, my command prompt starts having a little (master) thing, which is the default â€œbranchâ€ in my repository that Git has created for me


I also create a .gitignore file and that this is this particular project file that I want to track, and not any other, such as media or peak files:

*

!in_your_eyes_remix_git_managed.rpp


Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. â€œbass vst settings adjustedâ€.

Then I can see all the versions of my project in git gui tool.




  side note: you can use any git frontend, not only git gui.


Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.

The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.

Managing other files (WAVs etc.)

Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.

About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.

This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.

collaborating with GIT? Not sureâ€¦

GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I donâ€™t see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.

And letâ€™s not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.

Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we canâ€™t really consider it a real backup - because of missing media.

Tracking TODO items for your music project in GitHub

Interesting use-case Iâ€™m currently testing is to have a â€œtodo listâ€, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:

fix panning issues in chorus TODO
add one more synth layer TODO


Once itâ€™s in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (donâ€™t forget to pull your update, though, once you are back to your DAW PC).

In conclusion, when we inspect this idea of â€œgit for musicâ€ a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!

Thanks for reading.

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI enters the grant game, picking winners]]></title>
            <link>https://www.science.org/content/article/ai-enters-grant-game-picking-winners</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092880</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Simple modenized .NET NuGet server reached RC]]></title>
            <link>https://github.com/kekyo/nuget-server</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid>
            <description><![CDATA[Simple modenized NuGet server ğŸ“¦. Contribute to kekyo/nuget-server development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[nuget-server
Simple modenized NuGet server implementation.






(æ—¥æœ¬èªã¯ã“ã¡ã‚‰)
What is this?
A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.
Compatible with dotnet restore and standard NuGet clients for package publishing, querying, and manually downloading.
A modern browser-based UI is also provided:

You can refer to registered packages. You can check various package attributes.
You can download packages by version.
You can also publish (upload) packages.
You can manage user accounts.

Browse package list:

Publishing packages:

User account managements:

Key Features

Easy setup, run NuGet server in 10 seconds!
NuGet V3 API compatibility: Support for modern NuGet client operations
No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements
Package publish: Flexible client to upload .nupkg files via HTTP POST using cURL and others
Basic authentication: Setup authentication for publish and general access when you want it
Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution
Modern Web UI with enhanced features:

Multiple package upload: Drag & drop multiple .nupkg files at once
User account management: Add/delete users, reset passwords (admin only)
API password regeneration: Self-service API password updates
Password change: Users can change their own passwords


Package importer: Included package importer from existing NuGet server
Docker image available


Installation
npm install -g nuget-server
For using Docker images, refer to a separate chapter.
Usage
# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json
The NuGet V3 API is served on the /v3 path.

Default nuget-server served URL (Show UI): http://localhost:5963
Actual NuGet V3 API endpoint: http://localhost:5963/v3/index.json

The default URL provided by nuget-server can be changed using the --base-url option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.
Configure the NuGet client
nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.
If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.
Add as package source:
For HTTP endpoints:
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections
For HTTPS endpoints:
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Or specify in nuget.config:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
</configuration>
Publish packages
Upload packages by HTTP POST method, using cURL or any HTTP client with /api/publish endpoint:
# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
You may be dissatisfied with publishing using this method. The dotnet command includes dotnet nuget push, which is the standard approach.
However, in my experience, this protocol uses multipart/form-data for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.
Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately.
You might still feel issue with managing read operations and publish operation with the same key,
but in that case, you can simply separate the users.
For authentication feature, please refer to below chapter.

Package storage configuration
Storage location
By default, packages are stored in the ./packages directory relative to where you run nuget-server.
You can customize this location using the --package-dir option:
# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location
Package storage layout
Packages are stored in the filesystem using the following structure:
packages/
â”œâ”€â”€ PackageName/
â”‚   â”œâ”€â”€ 1.0.0/
â”‚   â”‚   â”œâ”€â”€ PackageName.1.0.0.nupkg
â”‚   â”‚   â”œâ”€â”€ PackageName.nuspec
â”‚   â”‚   â””â”€â”€ icon.png            # Package icon (if present)
â”‚   â””â”€â”€ 2.0.0/
â”‚       â”œâ”€â”€ PackageName.2.0.0.nupkg
â”‚       â”œâ”€â”€ PackageName.nuspec
â”‚       â””â”€â”€ icon.jpg            # Package icon (if present)
â””â”€â”€ AnotherPackage/
    â””â”€â”€ 1.5.0/
        â”œâ”€â”€ AnotherPackage.1.5.0.nupkg
        â”œâ”€â”€ AnotherPackage.nuspec
        â””â”€â”€ icon.png            # Package icon (if present)

Backup and restore
You can backup the package directory using simply tar or other achiver:
cd /your/server/base/dir
tar -cf - ./packages | lz4 > backup-packages.tar.lz4
Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.

Configuration
nuget-server supports configuration through command-line options, environment variables, and JSON file.
Settings are applied in the following order (highest to lowest priority):

Command-line options
Environment variables
config.json
Default values

Configuration file structure
You can specify a custom configuration file:
# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server
If not specified, nuget-server looks for ./config.json in the current directory.
config.json structure
Create a config.json file:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "<your-secret-here>",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}
All fields are optional. Only include the settings you want to override.
Both packageDir and usersFile paths can be absolute or relative. If relative, they are resolved from the directory containing the config.json file.

Authentication
nuget-server also supports authentication.



Authentication Mode
Details
Auth Initialization




none
Default. No authentication required
Not required


publish
Authentication required only for package publishing
Required


full
Authentication required for all operations (must login first)
Required



To enable authentication on the NuGet server, first register an initial user using the --auth-init option.
Initialize
Create an initial admin user interactively:
nuget-server --auth-init
This command will:

Prompt for admin username (default: admin)
Prompt for password (with strength checking, masked input)
Create users.json
Exit after initialization (server does not start)

When enabling authentication using a Docker image, use this option to generate the initial user.
Example session
Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================

User Management
Users added with --auth-init automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.

While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.
Using the API password
The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server.
The password used by NuGet clients when accessing the server is called the "API password,"
and access is granted using the combination of the user and the API password.
Please log in by displaying the UI in the browser.
Select the â€œAPI passwordâ€ menu from the UI menu to generate an API password.
Using this API password will enable access from the NuGet client.

Here is an example of using the API password:
# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections
Or specify nuget.config with credentials:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
  <packageSourceCredentials>
    <local>
      <add key="Username" value="reader" />
      <add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" />
    </local>
  </packageSourceCredentials>
</configuration>
For package publishing:
# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
When publishing a package, you can send the package by setting Basic authentication in the Authorization header.
Password strength requirements
nuget-server uses the zxcvbn library to enforce strong password requirements:

Evaluates password strength on a scale of 0-4 (Weak to Very Strong)
Default minimum score: 2 (Good)
Checks against common passwords, dictionary words, and patterns
Provides real-time feedback during password creation

Configure password requirements in config.json:
{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}
The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved.
However, if you do not use HTTPS (TLS), be aware that the Authorization header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.

Import packages from another NuGet server
Import all packages from another NuGet server to your local nuget-server instance.
This feature can be used when migrating the foreign NuGet server to nuget-server.
Package import from another NuGet server
Import packages interactively in CLI:
nuget-server --import-packages --package-dir ./packages
This command will:

Prompt for source NuGet server URL
Ask if authentication is required
If needed, prompt for username and password (masked input)
Discover all packages from the source server
Download and import all packages to local storage
Display progress for each package (1% intervals)
Exit after import (server does not start)

Import behavior

Existing packages with the same version will be overwritten
Failed imports are logged with error details
Progress is reported at 1% intervals to reduce log noise
Package icons are preserved during import

Parallel downloads are not done. This is to avoid making a large number of requests to the repository.
This feature is a type of downloader.
Therefore, it does not need to be run on the actual host where it will operate.
You can perform the import process in advance on a separate host and then move the packages directory as-is.
Example session
Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================


Reverse proxy interoperability
The server supports running behind a reverse proxy.
For example, when you have a public URL like https://nuget.example.com and run nuget-server on a host within your internal network via a gateway.
In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.
URL resolving
The server resolves URLs using the following priority order:

Fixed base URL (highest priority): When --base-url option is specified, it always takes precedence
Trusted proxy headers: When trusted proxies are configured with --trusted-proxies:

HTTP Forwarded header (proto, host, port)
Traditional X-Forwarded-* headers (X-Forwarded-Proto, X-Forwarded-Host, X-Forwarded-Port)


Standard request information (fallback): Uses Host header when proxy headers are not available

For example --base-url option:

nuget-server served public base URL: https://packages.example.com
Actual NuGet V3 API endpoint: https://packages.example.com/v3/index.json

# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Another option, you can configure with trusted proxy addresses:
# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"
Environment variables are also supported:
export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here

Docker usage
Docker images are available for multiple architectures:

linux/amd64 (x86_64)
linux/arm64 (aarch64)

When pulling the image, Docker automatically selects the appropriate architecture for your platform.
Quick start
Suppose you have configured the following directory structure for persistence (recommended):
docker-instance/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ user.json
â””â”€â”€ packages/
    â””â”€â”€ (package files)

Execute as follows:
# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat > docker-compose.yml << EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d
Your NuGet server is now available at:

Web UI: http://localhost:5963
NuGet V3 API: http://localhost:5963/v3/index.json

Permission requirements
The Docker container runs as the nugetserver user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.
Set proper permissions for mounted directories:
# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages
Important: Without proper permissions, you may encounter 500 Permission Denied errors when:

Creating or updating user accounts
Publishing packages
Writing configuration files

Basic usage
# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest
You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use config.json.
Since the Docker image has mount points configured, you can mount /data and /packages as shown in the example above and place /data/config.json there to flexibly configure settings. Below is an example of config.json:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}
When initializing credentials or importing packages, configure config.json and perform the operation via the CLI before launching the Docker image:
# Initialize authentication
nuget-server -c ./data/config.json --auth-init
Volume mounts and configuration

/data: Default data directory for config.json, users.json and other persistent data
/packages: Default package storage directory (mounted to persist packages)

Default behavior: The Docker image runs with --users-file /data/users.json --package-dir /packages by default.
Configuration priority (highest to lowest):

Custom command line arguments (when overriding CMD)
Environment variables (e.g., NUGET_SERVER_PACKAGE_DIR)
config.json file (if explicitly specified)
Default command line arguments in Dockerfile

Example of Automatic Startup Using systemd
Various methods exist for automatically starting containers with systemd.
Below is a simple example of configuring a systemd service using Podman.
This is a simple service unit file used before quadlets were introduced to Podman.
By placing this file and having systemd recognize it, you can automatically start the nuget-server:
/etc/systemd/system/container-nuget-server.service:
# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target

Building the Docker image (Advanced)
The build of the nuget-server Docker image uses Podman.
Multi-platform build with Podman (recommended)
Use the provided multi-platform build script that uses Podman to build for all supported architectures:
# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect
Important: For cross-platform builds, QEMU emulation must be configured first:
# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64
Without QEMU, you can only build for your native architecture.

Note
Non-interactive mode (CI/CD)
The --auth-init and --import-packages options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:
export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json
This allows initialization in CI/CD pipelines without user interaction.
Session Security
For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:
export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server
(Or use config.json.)
If not set, a random secret is generated (warning will be logged).
Supported NuGet V3 API endpoints
The server implements a subset of the NuGet V3 API protocol:

Service index: /v3/index.json
Package content: /v3/package/{id}/index.json
Package downloads: /v3/package/{id}/{version}/{filename}
Registration index: /v3/registrations/{id}/index.json


License
Under MIT.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems]]></title>
            <link>https://github.com/gargakshit/zfsbackrest</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid>
            <description><![CDATA[pgbackrest style encrypted backups for ZFS filesystems - gargakshit/zfsbackrest]]></description>
            <content:encoded><![CDATA[zfsbackrest

âš ï¸ Experimental:
Do not use it as your only way for backups. This is something I wrote over a
weekend. There's a lot of things that need work here.

pgbackrest style encrypted backups for ZFS
filesystems.
Getting Started
Installing
You need age installed to generate
encryption keys. Encryption is NOT optional.
$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest
Configuring
Create /etc/zfsbackrest.toml.
debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4
Creating a repository
$ zfsbackrest init --age-recipient-public-key="<your age public key>"
Backing up
$ zfsbackrest backup --type <full | diff | incr>
full backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.
diff backups are sent incrementally from the latest full backup. They depend
on the parent full backup to be present in the repository to restore.
incr backups are send incrementally from the latest diff backup. They depend
on the parent diff backup to restore.
Viewing the repository
$ zfsbackrest detail
It shows a list of backups, orphans and all.
Cleaning up the repository
Sometimes, orphaned backups are left as an artefact of incomplete or cancelled
backups. You can clean those by running
$ zfsbackrest cleanup --orphans --dry-run=false
You can clean up expired backups by running
$ zfsbackrest cleanup --expired --dru-run=false
Restoring
To restore the backups, you'll need your age identity file (private key).
zfsbackrest restore -i <path-to-age-identity-file> \
  -s <name of the dataset to restore from> \
  -b <optionally, the backup ID to restore from, leave empty to restore the latest> \
  -d <name of the dataset to restore to> # Restoring to a dataset that already exists on your local FS will fail.
Safety
zfsbackrest doesn't write or modify actual zfs datasets. It makes extensive
use of snapshots. List of zfs operations used by zfsbackrest are


backup

zfs snapshot - Creating a zfs snapshot for zfsbackrest
zfs hold - Creating a reference to that snapshot to prevent removal
zfs send - Sending the snapshot incrementally



cleanup / force-destroy

zfs release - Release the held snapshot
zfs destroy - Destroy the snapshot



restore

zfs recv - Receiving the remote snapshot



Model
TODO
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tetris is NP-hard even with O(1) rows or columns [pdf]]]></title>
            <link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid>
        </item>
        <item>
            <title><![CDATA[Ask HN: Do custom ROMs exist for electric cars, for example Teslas?]]></title>
            <link>https://news.ycombinator.com/item?id=45092204</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092204</guid>
            <description><![CDATA[I think I know what you are asking but it is complicated.]]></description>
            <content:encoded><![CDATA[
I think I know what you are asking but it is complicated.For safety, regulator, historical and frankly common sense reasons, a car is not one system.  It is a system of system that communicate via a CAN BUS, https://en.wikipedia.org/wiki/CAN_bus.  This is still true for electric cars.  Can this be hacked?  Like everything else, yes.Can you side load a new ROM like an android device?  Not that know of and hope that never becomes a reality because your phone crashing is different than you car crashing (figuratively and literally).  Can you enable/disable features?  Yes, usually through ECU hacking.  On my P3 Volvo, I bought a cheap stripped down Chinese clone of Volvo's diagnostic tool called DiCE.  Once the ECU is decrypted, which is done through brute force, you can use something like https://d5t5.com/article/vdash-volvo-diagnostic or P3Tool to change level settings like the theme of LED dash or engine tuning.You may be interested in https://github.com/jaredthecoder/awesome-vehicle-security#re...
Not really. You might want to look at what Rivian has been sharing about their vehicle hardware and software architecture. Sandy Munro did a few on site visits with their team.I think you are underestimating how complex EVs are, how much software goes into them, and what goes into coming up with an alternative software stack. Also, I doubt that the likes of Rivian, Tesla, etc. are going to just let people boot whatever on their cars. Why would they?But at the lower levels, hacking things like battery management systems is definitely a thing that is done and somewhat supported. A lot of retrofits where ICE engines are swapped out for an electrical drive train end up repurposing drive trains from EVs.
Mazdas, kind of.  https://mazdatweaks.com/There are of course after market ECU tweaks and parts that, for example, will change your throttle response with a physical piece of hardwareâ€”Pedal Commander is a simple example.
No. Aftermarket ECUs absolutely exist for almost all internal combustion engines. Other aftermarket modules are rare. Integration of them into a complete system even more so.
Not ROMs but OrBit is a "OrBit is PC software for diagnostics, configuration,  and software flashing for newer Volvo and Polestar vehicles".American Polestars can, for example, enable their adaptive headlights using OrBit.https://spaycetech.com/
I've been reluctant to try this with my P2 for fear of voiding warranties etc.But it seems Polestar is doing their best to make my warranty useless by closing local dealers and offering shitty service anyways.I should give this a whirl.
Does "infotainment system" include the cluster displays (speedometer etc) in vehicles where that is entirely a digital display (must be most at this point). I really hate the (very non-traditional) way my current vehicle displays this info. Whats funny is that both the highest trim level of this model and the one-step-cheaper model both do it in a more traditional, far superior way.
There is a lot of potential liability for anyone who creates something that targets anything other than the infotainment system.
I believe these systems are quite coupled with the hardware itself, making it quite difficult to port any custom ROM or such on them. I am not aware of any projects with the goals of creating an open-source Android ROM for a car. Even Phone ROMs are slowly dying off, with the exceptions of Lineage and GrapheneOS.
I believe law environment need to change to make possible digital  custom car's ROM. Now everything can be closed in same of safety, security, user convience...
Security (for vendor) from obscurity. AFAIK most of car owners cannot just buy the replace electronics for his car on used market so most of owners afraid of messing with proprietary computers in the car.
The only parts where that's true are for things like FCC certification. The US does not have an affirmative certification process for automotive software, including safety critical systems. NHTSA instead puts out a set of rules called FMVSS that manufacturers and aftermarket parts must comply with. Manufacturers then self-certify that they meet FMVSS and produce a bunch of documentation demonstrating that if NHTSA asks.Note that FMVSS has almost nothing to say on the topic of software. The industry broadly follows industry standards like ISO 26262 and the less universal 21448, but these don't have firm legal weight outside their status as standards of practice, nor do they preclude installing your own software.The situation in Europe is different and an affirmative certification process does exist there.
"For off-road use" is the the magic phrase that lets people sell any random garbage as "car parts" or modifications in the USA.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CocoaPods Is Deprecated]]></title>
            <link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid>
            <description><![CDATA[The Dependency Manager for iOS & Mac projects]]></description>
            <content:encoded><![CDATA[
        TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.

Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:


We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.


I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.

Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)

May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the prepare_command field in a Podspec. Any existing Pods using prepare_command are hard-coded to bypass this check.

Timeline

My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.

May 2025

We are stopping new CocoaPods from being added which use the prepare_command field

Mid-late 2025

I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.

September-October 2026

I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.

November 1-7th 2026

I will trigger a test run, giving automation a chance to break early

December 2nd 2026

I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.



These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.

If you have questions, you can contact the team via [emailÂ protected], me personally at [emailÂ protected] or reach out to me via Bluesky: @orta.io.

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[De-Googling TOTP Authenticator Codes]]></title>
            <link>https://imrannazar.com/articles/degoogle-otp</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091202</guid>
            <description><![CDATA[I've been slowly removing Google apps from my life, and one of the last ones left is Authenticator. In this post I look at migrating codes out of Authenticator to a command-line OTP tool, and the steps involved.]]></description>
            <content:encoded><![CDATA[
   
Back to Articles
1st Sep 2025
In the ongoing effort to extricate myself from Google's services, I've been paring down my usage of their apps on my (admittedly Android) phone. I'm now down to two Google apps I use regularly: Maps (for traffic data) and Authenticator (for TOTP[A]Time-based One Time Password codes).
Now, I spend most of my time in a terminal window on MacOS or connected to a Linux machine; it'd be nice if I could get TOTPs on the command-line, and it turns out there's a utility called oathtool that allows for TOTP generation on the CLI. However, that would mean switching my OTP provider, which usually involves:

 Logging into each service that has an OTP registered in the app;
 Disabling two-factor authentication (2FA);
 Re-enabling 2FA and using the "manual entry" code as input to oathtool;
 Doing it all again for the next website or service.

Fortunately, Google's Authenticator provides a way to migrate codes between instances of the app based on scanning QR codes, and we can use this to migrate them away from Google into a TOTP handler of our choosing. It's another four-step process:

 Generating a QR code in Google Authenticator for the codes you want to export;
 Decoding the QR somewhere off-device, into a URL;
 Decoding the URL into its constituent services and secret values;
 Setting up oathtool to use the secrets.

Note that the below steps are presented just as I went through them, you may be able to find efficiencies or you may run into troubles that I didn't (especially if you're trying this exclusively on Windows); "your mileage may vary" is apt here.
Going from Authenticator to a migration URL
The first step is getting the code out of Authenticator, through the Transfer Codes menu option in the app. Picking the services you'd like to extract leads you to a code like this:

 
 Figure 1: QR code exported from Google AuthenticatorUnrivalled padding between the QR and Next button

You may have an app on your phone that decodes QRs, but I don't; instead, I transferred the file to my MacOS machine over Tailscale, and used a command-line tool called qrtool to get the QR content:

 Decoding the migration QR
 $ brew install qrtool
$ qrtool decode Screenshot_20250901_062719_Authenticator.jpg
otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D

Decoding the URL into secrets
So we have our migration URL, with a Base64-encoded data block. Unfortunately, if we were to simply decode the data, we'd end up with some binary gibberish:

 Trying to decode the URL directly
 $ php -r 'var_dump(base64_decode("CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D"));'
string(69) "
<

i*H??h??Ã½

It turns out that this is a Protobuf-encoded data string, and we need to use Google's Protobuf library to get the data out. It turns out Tim Brooks has already done this with a short piece of Python at: https://github.com/brookst/otpauth_migrate
I decided to install this on a Linux machine I tend to be connected to (entirely unrelated to my Python installation being broken on Mac...):

 Extracting the data via otpauth_migrate
 $ git clone https://github.com/brookst/otpauth_migrate
$ cd otpauth_migrate
$ ./otpauth_migrate.py otpauth-migration://offline?data=CjwKC2kqSJnNaAyKkw6jEhJUaGUgUmlja3JvbGwgU3RvcmUgASgBMAJCEzg4Yzg5ZTE3NTY3MDQzOTE0MzkQAhgBIAA%3D
secret: "i*H\231\315h\014\212\223\016\243"
name: "The Rickroll Store"
algorithm: ALGORITHM_SHA1
digits: DIGIT_COUNT_SIX
type: OTP_TYPE_TOTP

Secret code = NEVERGONNAGIVEYOUM======

This tool is intelligent enough to extract any number of names and secrets from a migration URL, so you can export all your codes from Authenticator into one giant QR without needing to do each separately.
Using oathtool to generate OTPs
The final step is to use this secret code with oathtool, which takes the secret directly as a parameter. If you instead want to refer to the service by name, Michael Bushey[1]"CLI 2-Factor Authentication", Michael Bushey, 2023 has a quick wrapper script which extracts the secrets from a locally-stored file:

 Wrapper script to generate OTPs: /usr/local/bin/otp
 #!/bin/bash
OTPKEY="$(sed -n "s/${1}=//p" ~/.otpkeys)"
if [ -z "$OTPKEY" ]; then
   echo "$(basename $0): Bad Service Name '$1'"
   exit
fi
date
oathtool --totp -b "$OTPKEY"
 OTP key store: ~/.otpkeys
 rickroll=NEVERGONNAGIVEYOUM======

With this in place, you won't need to use your Authenticator app again. The tool outputs the current date and time, so you can double-check that your code won't expire (at :00 seconds) before you get a chance to type it in:

 $ otp rickroll
Mon Sep  1 07:10:42 AM UTC 2025
200213

Future expansion
There's a security issue here, of course, which is the exposed secret key sitting in a file on-disk. I'm happy to sit with that and not require a password to generate OTPs every time, but if you're interested in adapting the wrapper script to use symmetric encryption to secure the keys, Vivek Gite[2]"Use oathtool Linux command line for 2 step verification (2FA)", Vivek Gite, updated Feb 2025 has a set of scripts which employ gpg for the job.
Now I just need to find a way to get traffic data into a maps App that doesn't involve Google's servers... Thoughts welcome.
  
  
 
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UK's largest battery storage facility at Tilbury substation]]></title>
            <link>https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091119</guid>
            <description><![CDATA[The 300MWÂ Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.]]></description>
            <content:encoded><![CDATA[    
            National Grid has connected the UKâ€™s largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.
        The 300MWÂ Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.Our Tilbury substation once served a coal plant, and with battery connections like this, itâ€™s today helping to power a more sustainable future for the region and the country.National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the batteryâ€™s significant additional load, with new protection and control systems installed to ensure a robust connection.The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.John Twomey, director of customer and network development at National Grid Electricity Transmission, said:â€œBattery storage plays a vital role in Britainâ€™s clean energy transition. Connecting Thurrock Storage, the UKâ€™s biggest battery, to our transmission network marks a significant step on that journey.â€œOur Tilbury substation once served a coal plant, and with battery connections like this, itâ€™s today helping to power a more sustainable future for the region and the country.â€Tom Vernon, Statera Energy CEO and founder, said:â€œWe are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever itâ€™s needed.â€National Grid is continuing work at Tilbury substation to connect the 450MWÂ Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.The connection of the UKâ€™s biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent â€“ the largest solar plant in the country â€“ which National Grid connected to its adjacent Cleve Hill substation.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Telli (YC F24) is hiring engineers, designers, and interns (on-site in Berlin)]]></title>
            <link>https://hi.telli.com/join-us</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45090216</guid>
        </item>
        <item>
            <title><![CDATA[Preserving Order in Concurrent Go Apps: Three Approaches Compared]]></title>
            <link>https://destel.dev/blog/preserving-order-in-concurrent-go</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45089938</guid>
            <description><![CDATA[Concurrency breaks ordering by design, but sometimes we need both. Explore three methods to preserve order in concurrent Go applications, from standard ReplyTo channels to sophisticated permission passing, with benchmarks and real-world trade-offs.]]></description>
            <content:encoded><![CDATA[ Concurrency is one of Goâ€™s greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine â€“ unordered processing is enough, itâ€™s faster and simpler.
But sometimes, order matters.
When Order Matters
Here are three real-world scenarios where preserving order becomes critical:
Real-time Log Enrichment: Youâ€™re processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing canâ€™t keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.
Finding the First Match in a File List: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order â€“ the 50th file might finish before the 5th file, so you canâ€™t simply return the first match you find without knowing if an earlier file also contains the string.
Time Series Data Processing: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.
The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isnâ€™t just a theoretical problem â€“ itâ€™s a practical constraint that affects real systems at scale.
In this article, weâ€™ll explore three approaches Iâ€™ve developed and used in production Go applications. Weâ€™ll build a concurrent OrderedMap function that transforms a channel of inputs into a channel of outputs while preserving order. Through benchmarks of each approach, weâ€™ll understand their trade-offs and discover surprising performance insights along the way.
The Problem: Why Concurrency Breaks Order

Letâ€™s quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason â€“ we canâ€™t predict how exactly goroutines will be scheduled by the Go runtime.
For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.
If you want to see this in action, hereâ€™s a quick demo the Go playground.
Design Philosophy: Backpressure vs Buffering
The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but itâ€™s too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.
In such designs buffers can typically grow without bound. This happens when:

The input is skewed â€“ early items take longer to process than later items
Downstream consumers are slow

The algorithms presented below are backpressure-first. If a worker canâ€™t yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.

Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger â€œbufferâ€ in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.

Establishing a Performance Baseline
To understand the true cost of ordering, we first need a baseline to measure against.
Letâ€™s implement and benchmark a basic concurrent Map function that doesnâ€™t preserve order â€“ this will show us exactly what overhead the ordering approaches add.
Our Map function transforms an input channel into an output channel using a user-supplied function f. Itâ€™s built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.
// Map transforms items from the input channel using n goroutines, and the
// provided function f. Returns a new channel with transformed items.
func Map[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	out := make(chan B)
	Loop(in, n, out, func(a A) {
		out <- f(a)
	})
	return out
}

// Loop is a worker pool implementation. It calls function f for each 
// item from the input channel using n goroutines. This is a non-blocking function 
// that signals completion by closing the done channel when all work is finished.
func Loop[A, B any](in <-chan A, n int, done chan<- B, f func(A)) {
	var wg sync.WaitGroup

	for i := 0; i < n; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for a := range in {
				f(a)
			}
		}()
	}

	go func() {
		wg.Wait()
		if done != nil {
			close(done)
		}
	}()
}

// Discard is a non-blocking function that consumes and discards
// all items from the input channel
func Discard[A any](in <-chan A) {
	go func() {
		for range in {
			// Discard the value
		}
	}()
}

func BenchmarkMap(b *testing.B) {
	for _, n := range []int{1, 2, 4, 8, 12, 50} {
		b.Run(fmt.Sprint("n=", n), func(b *testing.B) {
			in := make(chan int)
			defer close(in)
			out := Map(in, n, func(a int) int {
				//time.Sleep(50 * time.Microsecond)
				return a // no-op: just return the original value
			})
			Discard(out)

			b.ReportAllocs()
			b.ResetTimer()

			for i := 0; i < b.N; i++ {
				in <- 10 // write something to the in chan
			}
		})
	}
}
As you can see, Map uses Loop to create a worker pool that processes items concurrently, while Loop itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.
What exactly are we measuring here? Weâ€™re measuring throughput â€“ how fast we can push items through the entire pipeline. Since the Map function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Letâ€™s run the benchmark (I used Apple M2 Max laptop to run it):



































GoroutinesTime /opAllocs/op2408.6ns04445.1ns08546.4ns012600.2ns0501053ns0
You might wonder: â€œShouldnâ€™t higher concurrency increase throughput?â€ In real applications, absolutely â€“ but only when thereâ€™s actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.
Weâ€™ll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, letâ€™s run one more benchmark with some work simulated (50Î¼s sleep):















































GoroutinesTime /opSpeedupAllocs/op161656ns1.0x0230429ns2.0x0415207ns4.1x087524ns8.2x0125034ns12.2x0501277ns48.3x0
Perfect! Here we see the dramatic benefits of concurrency when thereâ€™s real work to be done. With 50Î¼s of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.
Weâ€™re now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.
Approach 1: ReplyTo Channels
This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my batching article), but somehow this was the hardest approach for me to explain clearly.
Hereâ€™s how it works:

A packer goroutine creates jobs by attaching a unique replyTo channel to every input item.
Workers process jobs concurrently, and send results through those replyTo channels.
An unpacker goroutine unpacks the values sent via replyTo channels and writes them to the output.

The following diagram illustrates how this pattern in more detail:

The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the replyTo channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.
func OrderedMap1[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job struct {
		Item    A
		ReplyTo chan B
	}

	// Packer goroutine.
	// `jobs` chan will be processed by the pool
	// `replies` chan will be consumed by unpacker goroutine
	jobs := make(chan Job)
	replies := make(chan chan B, n)
	go func() {
		for item := range in {
			replyTo := make(chan B)
			jobs <- Job{Item: item, ReplyTo: replyTo}
			replies <- replyTo
		}
		close(jobs)
		close(replies)
	}()

	// Worker pool of n goroutines.
	// Sends results back via replyTo channels
	Loop[Job, any](jobs, n, nil, func(job Job) {
		job.ReplyTo <- f(job.Item) // Calculate the result and send it back
		close(job.ReplyTo)
	})

	// Unpacker goroutine.
	// Unpacks replyTo channels in order and sends results to the `out` channel
	out := make(chan B)
	go func() {
		defer close(out)
		for replyTo := range replies {
			result := <-replyTo
			out <- result
		}
	}()
	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2818.7ns+410ns14808.9ns+364ns18826.8ns+280ns112825.6ns+225ns150772.3ns-281ns1
This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new replyTo channel for every item. Unfortunately, we canâ€™t use a package level sync.Pool to mitigate this because our function is generic â€“ channels for different types canâ€™t share the same pool.
Whatâ€™s also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens â€“ OrderedMap1 becomes faster than Map (-281ns at 50 goroutines).
I havenâ€™t investigated this phenomenon deeply. I believe it canâ€™t be caused by inefficiencies inside Map since itâ€™s already based on the simplest possible channel-based worker pool. One guess that I have is that in Map we have 50 goroutines competing to write into a single output channel. On the contrary, in OrderedMap, despite additional moving parts, only one goroutine is writing to the output.
Letâ€™s now move on to the next approach.
Approach 2: sync.Cond for Turn-Taking
This was the first algorithm I implemented when I needed ordered concurrency, and itâ€™s much easier to explain than the ReplyTo approach.
Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.
This conditional waiting is implemented using a shared currentIndex variable protected by sync.Cond, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.
Hereâ€™s how the turn-taking mechanism works:

Here, after each write, all workers wake up (using broadcast) and recheck â€œis it my turn?â€ condition
func OrderedMap2[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job struct {
		Item  A
		Index int
	}

	// Indexer goroutine.
	// Assign an index to each item from the input channel
	jobs := make(chan Job)
	go func() {
		i := 0
		for item := range in {
			jobs <- Job{Item: item, Index: i}
			i++
		}
		close(jobs)
	}()

	// Shared state.
	// Index of the next result that must be written to the output channel.
	nextIndex := 0
	cond := sync.NewCond(new(sync.Mutex))

	// Worker pool of n goroutines.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job) {
		result := f(job.Item) // Calculate the result

		// Cond must be used with a locked mutex (see stdlib docs)
		cond.L.Lock()

		// wait until it's our turn to write the result
		for job.Index != nextIndex {
			cond.Wait()
		}

		// Write the result
		out <- result

		// Increment the index and notify all other workers
		nextIndex++
		cond.Broadcast()

		cond.L.Unlock()
	})

	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2867.7ns+459ns041094ns+649ns081801ns+1255ns0122987ns+2387ns05016074ns+15021ns0
The results are telling â€“ no more per-item allocations, which is excellent for memory efficiency. But thereâ€™s a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the â€œthundering herdâ€ problem: after each write, all goroutines wake up via cond.Broadcast(), but only one will do useful work.
This inefficiency led me to think: â€œHow can I wake only the goroutine that should write next?â€ And this is how the 3rd approach was born.
Approach 3: Permission Passing Chain
Hereâ€™s the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.
In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: canWrite channel to receive the permission, and nextCanWrite channel to pass the permission to the next job.

This chain structure makes the worker logic remarkably simple:

Calculate: Process the job using the provided function
Wait: Receive the permission from canWrite channel
Write: Send the result to the output channel
Pass: Send the permission to the next job via nextCanWrite channel

Hereâ€™s the diagram that illustrates the whole flow:

The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the â€œthundering herdâ€ problem entirely â€“ each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.
Letâ€™s see how this translates to code. The implementation has two parts: a â€œlinkerâ€ goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:
func OrderedMap3[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = make(chan struct{}, 1)
		close(nextCanWrite) // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, make(chan struct{}, 1)
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		<-job.CanWrite          // Wait for the write permission
		out <- result           // Write to the output channel
		close(job.NextCanWrite) // Pass the permission to the next job
	})

	return out
}
Performance Results:









































GoroutinesTime /opvs BaselineAllocs/op2927.2ns+519ns14939.8ns+495ns18860.7ns+314ns112823.8ns+224ns150609.8ns-443ns1
Here the result is very similar to what weâ€™ve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But thereâ€™s one differenceâ€¦
Unlike approach 1, here weâ€™re allocating a non-generic chan struct{}. This means we can use a package level sync.Pool to eliminate those allocations â€“ letâ€™s explore that next.
Approach 3a: Zero-Allocation Permission Passing Chain
Letâ€™s create a pool for canWrite channels. Implementation is straightforward â€“ the pool itself and make/release functions.
// Package-level pool for canWrite channels
type chainedItem[A any] struct {
	Value        A
	CanWrite     chan struct{}
	NextCanWrite chan struct{} // canWrite channel for the next item
}

var canWritePool sync.Pool

func makeCanWriteChan() chan struct{} {
	ch := canWritePool.Get()
	if ch == nil {
		return make(chan struct{}, 1)
	}
	return ch.(chan struct{})
}

func releaseCanWriteChan(ch chan struct{}) {
	canWritePool.Put(ch)
}
Now letâ€™s use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.
func OrderedMap3a[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite <- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		<-job.CanWrite                    // Wait for the write permission
		out <- result                     // Write to the output channel
		releaseCanWriteChan(job.CanWrite) // Release our canWrite channel to the pool
		job.NextCanWrite <- struct{}{}    // Pass the permission to the next job
	})

	return out
}
Performance Results with Pooling:









































GoroutinesTime /opvs BaselineAllocs/op2891.0ns+482ns04916.5ns+471ns08879.5ns+333ns012872.6ns+272ns050657.6ns-395ns0
Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeveâ€¦
One more thing: Building Reusable Abstractions
The permission passing approach has another significant advantage over the ReplyTo method: it controls when to write rather than where to write.
Iâ€™ll admit it â€“ sometimes I get a bit obsessed with building clean abstractions. When working on rill, I really wanted to extract this ordering logic into something reusable and testable. This â€œwhen vs whereâ€ distinction was an AHA moment for me.
Since the algorithm doesnâ€™t care where the outputs are written, itâ€™s easy to abstract it into a separate function â€“ OrderedLoop. The API is very similar to the Loop function we used before, but here the user function receives two arguments â€“ an item and a canWrite channel. Itâ€™s important that the user function must read from the canWrite channel exactly once to avoid deadlocks or undefined behavior.
func OrderedLoop[A, B any](in <-chan A, done chan<- B, n int, f func(a A, canWrite <-chan struct{})) {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite <- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs <- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	Loop(jobs, n, done, func(job Job[A]) {
		f(job.Item, job.CanWrite) // Do the work

		releaseCanWriteChan(job.CanWrite) // Release item's canWrite channel to the pool
		job.NextCanWrite <- struct{}{}    // Pass the permission to the next job
	})
}
The typical usage looks like:
OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
	// [Do processing here]
	
	// Everything above this line is executed concurrently,
	// everything below it is executed sequentially and in order
	<-canWrite
	
	// [Write results somewhere]
})

With this abstraction in hand itâ€™s remarkably simple to build any ordered operations. For example OrderedMap becomes just 7 lines of code:
func OrderedMap3b[A, B any](in <-chan A, n int, f func(A) B) <-chan B {
	out := make(chan B)
	OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
		result := f(a)
		<-canWrite
		out <- result
	})
	return out
}
We can also easily build an OrderedFilter that conditionally writes outputs:
func OrderedFilter[A any](in <-chan A, n int, predicate func(A) bool) <-chan A {
	out := make(chan A)
	OrderedLoop(in, out, n, func(a A, canWrite <-chan struct{}) {
		keep := predicate(a)
		<-canWrite
		if keep {
			out <- a
		}
	})
	return out
}
Or even an OrderedSplit that distributes items to two channels based on a predicate:
func OrderedSplit[A any](in <-chan A, n int, predicate func(A) bool) (<-chan A, <-chan A) {
	outTrue := make(chan A)
	outFalse := make(chan A)
	done := make(chan struct{})
	
	OrderedLoop(in, done, n, func(a A, canWrite <-chan struct{}) {
		shouldGoToTrue := predicate(a)
		<-canWrite
		if shouldGoToTrue {
			outTrue <- a
		} else {
			outFalse <- a
		}
	})
	
	go func() {
		<-done
		close(outTrue)
		close(outFalse)
	}()
	
	return outTrue, outFalse
}
Simply put, this abstraction makes building ordered operations trivial.
Performance Comparison
Hereâ€™s how all approaches perform across different concurrency levels:





























































ConcurrencyBaselineApproach 1(ReplyTo)Approach 2(sync.Cond)Approach 3(Permission)Approach 3a(+ Pool)2408.6ns818.7ns867.7ns927.2ns891.0ns4445.1ns808.9ns1094ns939.8ns916.5ns8546.4ns826.8ns1801ns860.7ns879.5ns12600.2ns825.6ns2987ns823.8ns872.6ns501053ns772.3ns16074ns609.8ns657.6nsZero allocsâœ…âŒâœ…âŒâœ…
Key Takeaways


sync.Cond is a no-go for ordered concurrency â€“ While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.


ReplyTo is a strong contender â€“ it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.


Permission Passing emerges as the clear winner â€“ It has it all:

Good performance: at most ~500ns of overhead compared to the baseline
Zero allocations: Less GC pressure for long running tasks
Clean abstraction: Core synchronization logic can be abstracted away and used to build various concurrent operations.
Maintainability: Separation of concerns and the intuitive â€œcalculate â†’ wait â†’ write â†’ passâ€ pattern make code easy to support and reason about



This exploration shows that ordered concurrency doesnâ€™t have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Goâ€™s channels can be used creatively to solve complex coordination problems.
Finally, these patterns have been battle-tested in production through rill concurrency toolkit (1.7k ğŸŒŸ on GitHub). It implements Map, OrderedMap, and many other concurrent operations. Rill focuses on composability â€“ operations chain together into larger pipelines â€“ while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.
Playground Links:

Code from this article
Finding the First Match in a File List example
 ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lewis and Clark marked their trail with laxatives]]></title>
            <link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid>
            <description><![CDATA[AS LEWIS AND CLARKâ€™S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably werenâ€™t thinking much about their place in history. So they werenâ€™t taking any particular pains to document their every movement.

There were, however, some particular pains they were experiencing, as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.

Luckily, they had something that helped with that â€” a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called â€œthunder-clappers,â€ which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.

And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in â€œthunder-clappersâ€ was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground â€” which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in â€” and take samples of the dirt in them. 

If it comes up with an off-the-charts reading for mercury, well, thatâ€™s a Corps of Discovery pit toilet â€” and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
												   
(Astoria, Clatsop County; 1800s) --  #ofor #oregonHistory #ORhistory -- 26 Jan 2025 -- By Finn J.D. John]]></description>
            <content:encoded><![CDATA[
			
		
				
		
		
		
        
			ASTORIA, CLATSOP COUNTY; 1800s: 
			
     
    
			  
			  			  
				   Audio version is not yet available
				  
            


		              By Finn J.D. John
			                January 26, 2025
                            
                        
		              
		              AS LEWIS AND CLARKâ€™S Corps of Discovery made its way across the continent to Oregon, the men (and woman) of the party probably werenâ€™t thinking much about their place in history. So they werenâ€™t taking any particular pains to document their every movement.
            There were, however, some particular pains they were experiencing with every movement, so to speak ... as a result of a relentlessly low-fiber diet: Everyone was constipated, all the time.
            Luckily, they had something that helped with that â€” a lot. The Corps of Discovery left on its journey with a trove of 600 giant pills that the men called â€œthunder-clappers,â€ which the soldiers and travelers used to jump-start things when they got bound up. And everyone used them pretty regularly.
            
               
                  The reproduction of Fort Clatsop, built at or near the site of the Corps of Expedition's original buildings. Dr. Rush's Bilious Pills have not been particularly helpful in locating the original Fort Clatsop, long since rotted away â€” either because it hasnâ€™t been found yet, or because the site of the old pit latrine has been disturbed by farming or logging activities in the years since. (Image: National Parks Service)
                
              
            
            And, strange as it seems, that fact is why we know several of their campsites along the way. The main active ingredient in â€œthunder-clappersâ€ was a mercury salt, which is a pretty stable compound. Archaeologists simply have to search for dimples in the ground â€” which is what old latrine pits often end up looking like, hundreds of years later, after Nature has partly filled them in â€” and take samples of the dirt in them. 
            If it comes up with an off-the-charts reading for mercury, well, thatâ€™s a Corps of Discovery pit toilet â€” and the layout of the rest of the campsite can be extrapolated with considerable precision by consulting the military manuals they used to lay out their camps.
            
              THESE PILLS WERE the pride and joy of Dr. Benjamin Rush, one of the Founding Fathers and a signer of the Declaration of Independence. Rush was also the man President Thomas Jefferson considered the finest physician in the republic. 
            In that opinion, Jefferson was probably alone, or at least in a small minority. Dr. Rushâ€™s style of â€œheroic medicineâ€ had caused his star to fall quite a bit by this time â€” especially after the Philadelphia yellow fever epidemic of 1793, when his patients died at a noticeably higher rate than untreated sufferers. 
            At the time, of course, very little was known about how the human body worked. Physicians were basically theorists, who made educated guesses and did their best. 
            The problem was, the education on which those educated guesses were based varied pretty wildly depending on what school you came from. Homeopathic physicians theorized that giving patients a tiny amount of something that mimicked their symptoms would stimulate the body to cure itself. Eclectic physicians sought cures from herbs and folk remedies. Hydropathic physicians believed hot and cold water, applied externally or internally, was all that was needed. 
            Dr. Rush wasnâ€™t from one of these schools. He was from the school of mainstream medicine â€” also known as allopathic medicine (although that term is a perjorative today).
            Allopathic medical theory, in the early 1800s, dated from the second century A.D., courtesy of a Roman doctor named Galen. 
            Galen theorized that the human body ran on four different fluids, which he called â€œhumoursâ€: Blood, phlegm, yellow bile, and black bile. All disease, he claimed, stemmed from an imbalance in these humours.
            Thus, too much blood caused inflammation and fever; the solution was to let a pint or two out. Too much bile caused problems like constipation; the solution was to administer a purgative and let the patient blow out some black bile into a handy chamber-pot, or vomit up some yellow bile â€” or both.
            These interventions sometimes helped, but most of the time they had little or no good effect. So by Rushâ€™s time, a number of physicians were going on the theory that what was needed was a doubling-down on their theory â€” in a style of practice that they called â€œheroic medicine.â€
            If a sensible dose of a purgative didnâ€™t get a patientâ€™s bile back in balance, a â€œheroicâ€ dose might. If a cup or two of blood didnâ€™t get the fever down, four or five surely would.          
          
            Â 
            
              [EDITOR'S NOTE: In "reader view" some phone browsers truncate the story here, algorithmically "assuming" that the second column is advertising. (Most browsers do not recognize this page as mobile-device-friendly; it is designed to be browsed on any device without reflowing, by taking advantage of the "double-tap-to-zoom" function.) If the story ends here on your device, you may have to exit "reader view" (sometimes labeled "Make This Page Mobile Friendly Mode") to continue reading. We apologize for the inconvenience.]
            
            â€”(Jump to top of next column)â€”
    

        
           
            A sketch of Fort Clatsop as it would have appeared in 1805. (Image: Oregon Historical Society)
          
        
        
          You can imagine what the result of this philosophy was, when applied to an actual sick person.
        â€œSome people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,â€ says physician and historian David Peck. â€œI think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.â€
        In lieu of a trained physician, the Corps of Discoveryâ€™s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing â€œheat,â€ opium products for relieving pain and inducing sleep â€” and purgatives.
        Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called â€œDr. Rushâ€™s Bilious Pills.â€ They contained about 10 grains of calomel and 10 to 15 grains of jalap.
        
           
              This recipe for a milder version of Rush's Bilious Pills comes from the National Formulary in 1945. This image appears in the Lewis and Clark Fort Mandan Foundation's Web site, at which there's a lot more information about the ingredients in this compound. Mercury was still being used as an internal medicine in the 1960s and as a topical antiseptic (chiefly as Mercurochrome) into the 1990s.
            
          
        
        Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power. 
        And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they donâ€™t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance. 
        Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis â€œsporting houseâ€ before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.
        When symptoms broke out, the patient would be dosed with â€œthunder clappersâ€ and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself. 
        And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative â€œon the regularâ€ (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.
        And this low-fiber diet had predictable results.
        It had another result, too, which was less predictable â€” although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given â€œbilious pillâ€ gets blown out post-haste in the ensuing â€œpurge.â€
        Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.
        So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way â€” a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.        
        
          
            (Sources: Class lecture in History of American Medicine, October 2009, Univ. of Oregon, by Dr. James Mohr; Or Perish in the Attempt: Wilderness Medicine in the Lewis and Clark Expedition, a book by David J. Peck published in 2002 by Farcountry Press; â€œFollowing Lewis and Clarkâ€™s Trail of Mercurial Laxatives,â€ an article by Marisa Sloan published in the Jan. 29, 2022, issue of Discover Magazine.)
          TAGS: #Archaeology #HeroicMedicine #DavidPeck #Jalap #Syphilis #CorpsOfDiscovery #BenjaminRush #Humours #Medicine #FrontierDoctors #Galen #FortClatsop #Calomel #MercuryPoisoning #Thunderclappers #Constipation #DrJamesMohr #OregonTrail #DrRush's #BiliousPills #Bile #COLUMBIAgorge #CLATSOPcounty
        

		  
          

          
          

      

     
    
		
		    Background image is a postcard, a hand-tinted photograph of Crown Point and the Columbia Gorge Scenic Highway. Here is a link to the Offbeat Oregon article about it, from 2024.
		    Scroll sideways to move the article aside for a better view.
		    
		    Looking for more?
            On our Sortable Master Directory you can search by keywords, locations, or historical timeframes. Hover your mouse over the headlines to read the first few paragraphs (or a summary of the story) in a pop-up box.
            ... or ...		    
		    Home
		    
	      

    
    
  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Linux version of the Procmon Sysinternals tool]]></title>
            <link>https://github.com/microsoft/ProcMon-for-Linux</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid>
            <description><![CDATA[A Linux version of the Procmon Sysinternals tool. Contribute to microsoft/ProcMon-for-Linux development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Process Monitor for Linux (Preview) 
Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows.  Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.

Installation & Usage
Requirements

OS: Ubuntu 18.04 lts
cmake >= 3.14 (build-time only)
libsqlite3-dev >= 3.22 (build-time only)

Install Procmon
Please see installation instructions here.
Build Procmon
Please see build instructions here.
Usage
Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file
Examples
The following traces all processes and syscalls on the system:
sudo procmon
The following traces processes with process id 10 and 20:
sudo procmon -p 10,20
The following traces process 20 only syscalls read, write and open at:
sudo procmon -p 20 -e read,write,openat
The following traces process 35 and opens Procmon in headless mode to output all captured events to file procmon.db:
sudo procmon -p 35 -c procmon.db
The following opens a Procmon tracefile, procmon.db, within the Procmon TUI:
sudo procmon -f procmon.db
Feedback

Ask a question on Stack Overflow (tag with ProcmonForLinux)
Request a new feature on GitHub
Vote for popular feature requests
File a bug in GitHub Issues

Contributing
If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:

How to build and run from the source
The development workflow, including debugging and running tests
Coding Guidelines
Submitting pull requests

Please see also our Code of Conduct.
License
Copyright (c) Microsoft Corporation. All rights reserved.
Licensed under the MIT License.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We should have the ability to run any code we want on hardware we own]]></title>
            <link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid>
            <description><![CDATA[Refuting the common and flawed argument of]]></description>
            <content:encoded><![CDATA[
  
  Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: â€œI should be able to run whatever code I want on hardware I ownâ€. I agree entirely with this point, but within the context of this discussion itâ€™s moot.


  â€œI should be able to run whatever code I want on hardware I ownâ€


When Google restricts your ability to install certain applications they arenâ€™t constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. Itâ€™s through this control of the operating system that Google is exerting control, not at the hardware layer. You often donâ€™t have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Appleâ€™s success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.

You shouldnâ€™t take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldnâ€™t be of the restrictions in place in the operating systems they provide â€“ rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sonyâ€™s restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.


  
    

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Eternal Struggle]]></title>
            <link>https://yoavg.github.io/eternal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid>
            <description><![CDATA[change background]]></description>
            <content:encoded><![CDATA[
  
  
  change background



]]></content:encoded>
        </item>
    </channel>
</rss>