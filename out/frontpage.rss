<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 17:31:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Optery (YC W22) Is Hiring in Engineering, Legal, Sales, Marketing (U.S., Latam)]]></title>
            <link>https://www.optery.com/careers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094471</guid>
            <description><![CDATA[Now is a great time to join Optery. Optery is profitable, and we 3x-ed sales last year. Our product was awarded “Editors’ Choice” by PCMag as the most outstanding in the personal data removal category for the 4th year in a row (2022 - 2025), Optery was ranked the #1 most effective of all personal data removal services tested in 2024 study by Consumer Reports, Optery was named one of Business Insider’s Top 30 Future Unicorns of 2025, and we’re changing the game in the world of consumer data in a way that puts individuals in control. Optery is automated opt out software, and we serve individuals, families and businesses. Our mission is to empower people to take control of their personal data, and we have a vision for a safer world through data privacy. Hundreds of thousands of people rely on Optery to prevent attacks and keep their personal information off the Internet. Optery has raised $9M+ in funding from world-class investors such as Y Combinator, Alumni Ventures, Bayhouse Capital, Flex Capital, Global Founders Capital, Goodwater Capital, Pioneer Fund, Soma Capital, TRAC, Tribe Capital, and Uncorrelated Ventures. Optery is headquartered in the San Francisco Bay Area, but operates as a fully-remote global team.]]></description>
            <content:encoded><![CDATA[
      Use promo code:  Xi8TJRBw  at checkout for 20% Off 🎉 with Optery’s Labor Day Sale! 🎇
    

          
        Ready to safeguard your personal data?
      
    
          
        Join the movement of people strengthening their privacy      
    
          
        Sign Up Free      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[93% of GPT-4 performance at 1/4 cost: LLM routing with weak bandit feedback]]></title>
            <link>https://arxiv.org/abs/2508.21141</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094421</guid>
            <description><![CDATA[Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.
    

    
    
              
          Comments:
          Accepted at EMNLP 2025 (findings)
        

          Subjects:
          
            Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21141 [cs.LG]
        
        
           
          (or 
              arXiv:2508.21141v1 [cs.LG] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.21141
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Pranoy Panda [view email]          [v1]
        Thu, 28 Aug 2025 18:18:19 UTC (1,560 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The car is not the future: On the myth of motorized freedom]]></title>
            <link>https://blog.scaramuzza.me/articles/the_car_is_not_the_future.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094351</guid>
            <description><![CDATA[Pol's website. The Car Is Not the Future: On the Myth of Motorized Freedom]]></description>
            <content:encoded><![CDATA[
	
I’ve always felt uneasy about the idea that owning a car is somehow a default requirement for adult life. It’s not just the cost or the maintenance, it’s the deeper realization that cities, economies, and even social expectations are built around the assumption that you’ll drive. Growing up, we were sold the idea that cars meant freedom. Freedom to move, to explore, to escape. But in practice, the promise feels more like a trap, paved with traffic lights, gas bills, and endless parking lots. But it wasn’t always like this. What if the car's dominance was a choice, not a necessity? In the early 20th century, streets were shared spaces: places where people walked, gathered, played. Automakers reshaped public perception with propaganda, blaming pedestrians for getting hit. It worked. Streets became something to fear, not share. As explored in The Forgotten History of How Automakers Invented the Crime of Jaywalking, the word jaywalking itself was a propaganda tool and streets eventually, surrendered to cars entirely. Automakers, faced with public backlash from pedestrian deaths, needed a scapegoat. So they blamed the walkers, not the drivers. The sidewalk became your lane; the street belonged to the car. This redefinition of public space went hand in hand with policy and infrastructure changes. Highways sliced through neighborhoods, parking lots consumed city centers, and transit funding was gutted. What’s more, this car-centric worldview made its way into our laws, our habits, and even our sense of freedom. Today, to live without a car in many places is to opt out of participation in basic civic life. Cities that have experimented with removing traffic signals, as shown in What Happens If You Turn Off the Traffic Lights?, found that people actually behave more cautiously and accidents often decline. You see something similar in one of the boldest infrastructure experiments of the 20th century: Sweden’s switch from driving on the left to the right, known as Dagen H. On September 3rd, 1967, at precisely 5:00 AM, the entire country changed the side of the road people drove on. The preparation was immense: over 360,000 road signs were replaced overnight, and a nationwide traffic ban was enforced during the switch. And yet it worked. According to The Guardian, not only was the transition smooth, but traffic accidents actually dropped immediately afterward. People drove slower, more carefully. For a brief period, the streets were safer. Not because of more control, but because of collective attention and uncertainty. Both cases suggest the same underlying truth: when we stop overengineering our environment for efficiency and control, people tend to act more responsibly. The absence of strict systems forces drivers to be more aware and respectful. It shows that a different kind of city is possible: one that isn’t built around cars, but around people. But how did we get to a place where reclaiming the street feels radical? We tell ourselves it’s about convenience, but is it really? If the price of gasoline truly reflected its environmental, health, and geopolitical costs, some economists argue it would exceed $10 per gallon (the article below is from 2016 so nowadays the cost will be even higher). That’s one of the main arguments in The Absurd Primacy of the Automobile in American Life, which lays out just how deeply society subsidizes car culture, socializing the harms while privatizing the benefits.Subsidies, tax breaks, and unaccounted externalities keep this illusion affordable. But it’s like getting a loan with hidden interest rates: eventually, someone pays. Perhaps the most profound critique comes not from an economist or engineer, but from philosopher André Gorz in his 1973 essay The Social Ideology of the Motorcar Gorz argued that the car, sold as a symbol of personal freedom, ends up enslaving its owner through debt, isolation, and dependence. Fifty years later, it still rings uncomfortably true. He wrote: "never make transportation an issue by itself. Always connect it to the problem of the city, of the social division of labour, and to the way this compartmentalizes the many dimensions of life. One place for work, another for “living,” a third for shopping, a fourth for learning, a fifth for entertainment. The way our space is arranged carries on the disintegration of people that begins with the division of labour in the factory. It cuts a person into slices, it cuts our time, our life, into separate slices so that in each one you are a passive consumer at the mercy of the merchants, so that it never occurs to you that work, culture, communication, pleasure, satisfaction of needs, and personal life can and should be one and the same thing: a unified life, sustained by the social fabric of the community". But, there's a glimmer of hope. Recent data from a study titled Are Generation Z Less Car‑Centric Than Millennials? shows a clear generational shift. In the early 2000s, sitting for a driver’s license was seen as a near-universal rite of passage. By 2020, Generation Z teens were significantly less likely to get their licenses and later, when they do, it's driven by necessity rather than aspiration. While the study focuses on the U.S., the trend appears to be global. The peak car theory reinforces this perspective: in many developed regions, young people are driving less, acquiring licenses later, and treating cars more as tools than status symbols. Policy is beginning to catch up with this shift. Cities like New York and states like California have recently moved to decriminalize jaywalking, recognizing that the law was never about safety alone but about enforcing car dominance on streets. Lawmakers cited how enforcement disproportionately targeted Black and Latino pedestrians and wasted police resources while doing little to prevent accidents (AP News, CBS News). All of this leads to a simple question: why do we keep designing our world around machines that isolate us, pollute the air, and bankrupt our cities?  We think this is normal. But it’s not. It’s the result of decades of policy, ideology, and inertia. Maybe it’s time to take our foot off the gas. 
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Unique, High-Tech (Family) Computer]]></title>
            <link>https://nicole.express/2025/a-computer-in-your-home.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093956</guid>
            <description><![CDATA[There’s a concept that many people have tried, with varying effects: the “educational computer”, a device that a parent can buy for their children to learn t...]]></description>
            <content:encoded><![CDATA[
        

  

  
    There’s a concept that many people have tried, with varying effects: the “educational computer”, a device that a parent can buy for their children to learn the basics of the computer, which everyone will need to know in the future, and can also play games, so the children will actually want to use it. These have ranged from plasticky VTech toys with little more than an electronic organizer, to the Wonder Computer of the 1980’s, the Commodore VIC-20, which was a full computer. This is a prime market fit for an aging 8-bit platform, so of course, the Famicom has been wedged into it too… but not by Nintendo.

Unique, High-Tech, What more could you want?



This is it: a unique, high-tech computer. As we can see, it’s also advertising Contra on the box, along with “8 Bit” games, so immediately, you know that this is a Famiclone, and it’s got a Famicom cartridge slot underneath the cartridge flap. There’s been more than a few of these out there; they’re unique to me because they rarely show up in the United States (I bought this from Goodwill.com), but I would bet to many of the readers of this blog they won’t see this as unique at all.

What’s in the box?



In addition to the computer, you can see a whole selection of peripherals: two controllers, a mouse, a light-gun. And a power supply with a Europlug; further evidence that this is definitely not for the US market. Thankfully, it’s just 9V center-negative, so any plug you can use to power a Famicom should work here as well.



The sticker on the bottom of the system doesn’t match the sticker on the front of the box, but it does give us a release year for this model of the product: 2003. By 2003, the Famicom hardware was definitely old hat; in fact, that’s the same year Nintendo of Japan officially discontinued the system. You can definitely tell this sticker is trying to get you thinking this is relevant to the Windows XP world.



The sticker in the top left corner is long gone. Underneath is interesting, though; you can see three holes that look to the world like the Caps Lock, Num Lock, and Scroll Lock lights you’d see in the corner of a standard Windows keyboard of the era. Was this top case also used for standard keyboards? And if so, what did they do with the cartridge slot?



More evidence of plastics reuse is on the back, which shows a blanking plate covering nothing, and a speaker grille with no speaker behind it.



The actual ports you get are paltry; the common DB-9 ports you see for Famiclones, a power plug, and three RCA jacks. Think that’s stereo audio? (Something we have discussed as a Famicom mod on this blog before) Look closer!



The white RCA port is actually the RF modulator! Audio is the red jack. I’m guessing white, yellow, and red triplets of RCA ports were just extremely cheap at the time of this computer’s manufacture, so why not use them?



This is held together by screws, not plastic clips, which actually surprised me. But inside is just a standard keyboard membrane and a few small PCBs.



The keyboard mechanism is self-contained in the top plastic, and is actually a bit more elaborate than I expected; this is a “slider over membrane” design, where pressing a key causes a tiny point-like piece of plastic to connect the membrane. It works fairly well; you could definitely learn to type on this. Assuming it didn’t bind as much when it was new and clean, anyway.



Where’s the Famiclone itself? It’s just underneath the cartridge port, of course!
And also of course, it’s an epoxy blob.



On the epoxy blob was a small piece of masking tape, which I removed for the earlier screenshot. I can’t quite make it out as the ink has unfortunately bled a lot; the first letter seems to be a “V”. A major series of Famiclone chips from V.R. Technology has serial numbers beginning with “VT”, which could be related.



One thing about that controller. You might notice that on a real NES controller, the A button is on the outside edge, and the B button closer to the center. This is labeled in the opposite way– and this is how the buttons are arranged, too. Why did they swap the button positions? I don’t know, perhaps they just don’t like games being playable. The X and Y buttons are turbo buttons, as is commonly the case on four-button controllers being used for the Famicom.

Built-in hardware

This Famiclone has no built-in software or games. That seems to be pretty standard for models with cartridge slots; everything that makes this an educational computer is on the “48 in 1” cartridge. 48 is a much more achievable goal than many multicarts claim.



What’s inside?



An epoxy blob, of course, and 32kiB of SRAM. It’s a shame this is an epoxy blob, because I’m actually quite curious how that SRAM is wired. The NES memory map does not have room for 32kiB of cartridge PRG-ROM (usual amount of area mapped to the ROM) and 32kiB of cartridge RAM, so my assumption is that some sort of banking much be going on here.

Turn it on



But let’s boot the damn thing up already! Worth noting that this is a PAL 50Hz console; that should’ve been evident from the Europlug. I don’t think anywhere uses the Europlug and 60Hz NTSC; though possibly parts of Latin America?





The UI is clearly inspired by Microsoft Windows, though not the Windows XP that the sticker on the console tries to hint at. It’s actually pretty adorable, though having to move the cursor to the top corner is annoying. (Protip: use the page up and page down keys on the keyboard) The cursor can be moved with the mouse, or the controller. What is Super Hero?

You don't have a video tag support or something, so long story short: it's a rhythm game

It’s a rhythm game of some sort. I can’t recognize the track, and I don’t know how to play the game either; it doesn’t seem like controller inputs are what it’s looking for, or the arrow keys on the keyboard? So I’ll just let it be for now.

UPDATE: Thanks to The_Opponent for finding the track: Boys by smile.dk. Still not sure why it’s called “Super Hero”, though.

This actually has a lot of unique elements. For example, like any good version of Microsoft Windows, it has Solitaire.



And like any good multicart, it pads things out. Not only does it break up Duck Hunt (remember that gun in the package?) into multiple games…



And yes, it is Nintendo’s Duck Hunt. What else did you expect?



The most extreme case is Konami’s Track & Field. It’s here, sure.



But it’s been broken up into so many individual options for individual events that an entire page of the menu is taken up by it.



Also, you know what Konami game is not present on this multicart? Contra. Which was advertised on the box.



There are some educational games. Not really worth noting too much; mostly focused on typing, though it can also sing “Happy Birthday to You”. Since the keyboard is pretty decent, that’s probably actually the best usecase, but making games focused on typing is always a bit limiting. Here’s a classic “press the key listed” game, with a “My First Missile Command” theme.



But we were promised an Electronic Organ. So what does it have for a “MUSIC BOARD”?





That’s right; it’s MUSIC BOARD, from Nintendo and Hudson’s Family BASIC. Just separated into its own option on the menu, just like they did for Duck Hunt and Track and Field. Family BASIC MUSIC BOARD is fine, though I wouldn’t call it an electronic organ. I feel robbed.

But if Family BASIC’s MUSIC BOARD is here…



Then Family BASIC’s GAME BASIC should be here too. And it is! Or at least, I assume this is Family BASIC. (V3, judging by the version number) 32kiB of RAM is much more than it usually has access to, but is likely the purpose of the extra RAM on the cartridge. Very nice.



Unfortunately, this has some severe downsides compared to the real Family BASIC, despite the extra RAM. The biggest being that there is no way to save your work between sessions; neither battery-backed RAM nor a way to interface with a cassette tape. This pretty much relegates G BASIC to a novelty, though it always was one anyways.



Part of the FAMILY?



One thing I wondered here was, if it has Family BASIC on board, would the original one work?

Well… unfortunately, Family BASIC has a very annoying UI where you have to talk to the computer using text. And so I learned that while the keyboard is compatible in the sense that pressing keys makes letters appear, the keyboard matrix has been remapped.



I didn’t even make it to the actual BASIC.



Computers for the whole family

As I noted, the educational computer market has a lot of entries. Many had features like printers; I wonder if that was what the blanking plate was for. This one is very bare-bones. But let’s face it; it was mostly a way for kids to get their parents to get something into the house which could play games like Super Mario Bros. 3, albiet at a PAL 50Hz slowdown.

You don't have a video tag support or something, so long story short: it's SMB3 but slow

Still, I think it’s a pretty cool bit of computing history, especially important outside the wealthier countries whose markets I usually look at. I hope you enjoyed!

  

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The time picker on the iPhone's alarm app isn't circular, it's just a long list]]></title>
            <link>https://old.reddit.com/r/interestingasfuck/comments/1n5lztw/the_time_picker_on_the_iphones_alarm_app_isnt/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093765</guid>
        </item>
        <item>
            <title><![CDATA[Search engine referral report for 2025 Q2]]></title>
            <link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid>
        </item>
        <item>
            <title><![CDATA[Isolated(any)]]></title>
            <link>https://nshipster.com/isolated-any/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093590</guid>
            <description><![CDATA[There are cases where just a little more visibility and control over how to schedule asynchronous work can make all the difference.]]></description>
            <content:encoded><![CDATA[
              Ahh, @isolated(any).
                It’s an attribute of contradictions.
                You might see it a lot, but it’s ok to ignore it.
                You don’t need to use it, but I think it should be used more.
                It must always take an argument, but that argument cannot vary.
              Confusing? Definitely.
                But we’ll get to it all.
              To understand why @isolated(any) was introduced,
                we need to take a look at async functions.
              let respondToEmergency: () async -> Void

              This is about as simple a function type as we can get.
                But, things start to get a little more interesting
                when we look at how a function like this is used.
                A variable with this type must always be invoked with await.
              await respondToEmergency()

              This, of course, makes sense.
                All async functions must be called with await.
                But! Consider this:
              let sendAmbulance: @MainActor () -> Void = {
    print("🚑 WEE-OOO WEE-OOO!")
}

let respondToEmergency: () async -> Void = sendAmbulance

await respondToEmergency()

              The explicit types are there to help make what’s going on clear.
                We first define a synchronous function that must run on the MainActor.
              And then we assign that to a plain old,
              non-MainActor async function.
            We’ve changed so much that you might find it surprising this even compiles.
          Remember what await actually does. It allows the current task to suspend. That doesn’t just let the task wait for future work to complete. It also is an opportunity to change isolation. This makes async functions very flexible!
          Just like a dispatcher doesn’t sit there doing nothing while waiting for the ambulance to arrive, a suspended task doesn’t block its thread. When the dispatcher puts you on hold to coordinate with the ambulance team, that’s the isolation switch - they’re transferring your request to a different department that specializes in that type of work.
          
            But change to where, exactly?
          Ok, so we know that async functions, because they must always be awaited, gain a lot of flexibility. We are close, but have to go just a little further to find the motivation for this attribute.
          func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

await dispatchResponder {
    // no explicit isolation => nonisolated
    print("🚒 HONK HOOOOONK!")
    await airSupport()
    print("🚁 SOI SOI SOI SOI SOI!")
}

await dispatchResponder { @MainActor in
    print("🚑 WEE-OOO WEE-OOO!")
}

          We now have a function that accepts other functions as arguments. It’s possible to pass in lots of different kinds of functions to dispatchResponder. They could be async functions themselves, or even be synchronous. And they can be isolated to any actor. All thanks to the power of await.
        Except there’s a little problem now.
          Have a look at dispatchResponder on its own:
      func dispatchResponder(_ responder: () async -> Void) async {
    await responder()
}

      The type of responder fully describes everything about this function,
        except for one thing.
        We have no way to know its isolation.
        That information is only available at callsites.
        The isolation is still present,
        so the right thing happens at runtime.
        It’s just not possible to inspect it statically or even programmatically.
        If you’ve encountered type erasure before,
        this should seem familiar.
        The flexibility of async has come with a price -
        a loss of information.
      This is where @isolated(any) comes in.
      
        Using @isolated(any)
      
      We can change the definition of dispatchResponder to fix this.
    func dispatchResponder(_ responder: @isolated(any) () async -> Void) async {
    print("responder isolation:", responder.isolation)

    await responder()
}

    When you apply @isolated(any) to a function type, it does two things. Most importantly, it gives you access to a special isolation property. You can use this property to inspect the isolation of the function. The isolation could be an actor. Or it could be non-isolated. This is expressible in the type system with (any Actor)?.
    Functions with properties felt really strange to me at first.
      But, after thinking for a minute,
      it became quite natural.
      Why not?
      It’s just a type like any other.
      In fact, we can simulate how this all works with another feature:
      callAsFunction.
struct IsolatedAnyFunction<T> {
    let isolation: (any Actor)?
    let body: () async -> T

    func callAsFunction() async -> T {
        await body()
    }
}

let value = IsolatedAnyFunction(isolation: MainActor.shared, body: {
    // isolated work goes here
})

await value()

This analogy is certainly not perfect,
  but it’s close enough that it might help.
There is one other subtle change that @isolated(any) makes to a function
  that you should be aware of.
  Its whole purpose is to capture the isolation of a function.
  Since that could be anything,
  callsites need an opportunity to switch.
  And that means an @isolated(any) function must be called with an await —
  even if it isn’t itself explicitly async.
func dispatchResponder(_ responder: @isolated(any) () -> Void) async {
    await responder() // note the function is synchronous
}

This makes synchronous functions marked with @isolated(any) a little strange.
  They still must be called with await,
  yet they aren’t allowed to suspend internally?
As it turns out, there are some valid (if rare) situations
  where such an arrangement can make sense.
  But adding this kind of constraint to your API
  should at least merit some extra documentation.

  How @isolated(any) Affects Callers
All of the task creation APIs —
  Task initializers and TaskGroup —
make use of @isolated(any).
These are used a lot
and are usually encountered very early on when learning about concurrency.
So, it’s completely natural to run into this attribute and think:
“Ugh another thing to understand!”
It’s reasonable because
  the components of a function type dictate how it can be used.
  They are all essential qualities for API consumers.
  They are the interface.

  Parameters
  Return value
  Does it throw?
  Is it async?

This is not an exhaustive list,
  but what’s important is all of these are things callers must care about.
  Except for @isolated(any), which is the opposite.
  It doesn’t affect callers at all.
This, I think, is the root of a lot of confusion around @isolated(any).
  Unlike other qualities of a function,
  this attribute is used to capture information for the API producer.
I’m so close to saying “you can and should just ignore @isolated(any)“.
  But I just cannot quite go that far,
  because there is one situation you should be aware of.

  Scheduling
To help understand when you should be thinking about using @isolated(any),
  I’m going to quote
  the proposal:

  This allows the API to make more intelligent scheduling decisions about the function.

I’ve highlighted “intelligent scheduling”,
  because this is the key component of @isolated(any).
  The attribute gives you access to the isolation of a function argument.
  But what would you use that for?
Did you know that, before Swift 6.0, the ordering of the following code was undefined?
@MainActor
func threeAlarmFire() {
    Task { print("🚒 Truck A reporting!") }
    Task { print("🚒 Truck B checking in!") }
    Task { print("🚒 Truck C on the case!") }
}

Ordering turns out to be a very tricky topic when working with unstructured tasks.
  And while it will always require care, Swift 6.0 did improve the situation.
  We now have some stronger guarantees about scheduling work on the MainActor,
and @isolated(any) was needed to make that possible.

Take a look at this:
@MainActor
func sendAmbulance() {
    print("🚑 WEE-OOO WEE-OOO!")
}

nonisolated func dispatchResponders() {
    // synchronously enqueued
    Task { @MainActor in
        sendAmbulance()
    }

    // synchronously enqueued
    Task(operation: sendAmbulance)

    // not synchronously enqueued!
    Task {
        await sendAmbulance()
    }
}

These are three ways to achieve the same goal.
  But, there is a subtle difference in how the last form is scheduled.
  Task takes an @isolated(any) function
  so it can look at its isolation
  and synchronously submit it to an actor.
  This is how ordering can be preserved!
  But, it cannot do that in the last case.
  That closure passed into Task isn’t actually itself MainActor —
it has inherited nonisolated from the enclosing function.
I think it might help to translate this into
  GCD.
func dispatchResponders() {
    // synchronously enqueued
    DispatchQueue.main.async {
        sendAmbulance()
    }

    // synchronously enqueued
    DispatchQueue.main.async(execute: sendAmbulance)

    // not synchronously enqueued!
    DispatchQueue.global().async {
        DispatchQueue.main.async {
            sendAmbulance()
        }
    }
}

Look really closely at that last one!
  What we are doing there is introducing a new async closure
  that then calls our MainActor function.
There are two steps.
This doesn’t always matter,
but it certainly could.
And if you need to precisely schedule asynchronous work,
@isolated(any) can help.

  isolated(all)
All this talk about @isolated(any) got me thinking…
It’s kinda strange that only some functions get to have this isolation property.
  It would certainly feel more consistent to me if all functions had it.
  In fact, I think we can go further.
  I can imagine a future where an explicit @isolated(any)
  isn’t even necessary for async functions.
  As far as I can tell, there is no downside.
And a little less syntactic noise would be nice.
  Perhaps one day!

  isolated(some)
We do have to talk about that any.
  It’s surprising that this attribute requires an argument,
  yet permits only one possible value.
  The reason here comes down to future considerations.
The concrete actor type that this isolation property returns
  is always (any Actor)?.
  This is the most generic type for isolation and matches the #isolation macro.
  Today, there is no way to constrain a function to only specific actor types,
  such as @isolated(MyActor).
The any keyword here was chosen to mirror how protocols handle this.
But accepting an argument leaves the door open
to more sophisticated features in the future.
And that really fits the spirit of @isolated(any).
  Doing a little work now in exchange for flexibility down the road.
Because you’ll see it in many foundational concurrency APIs,
  it’s very natural to feel like you must understand @isolated(any).
  I’m 100% behind technical curiosity!
  In this case, however, it is not required.
  For the most part, you can just ignore this attribute.
  You will rarely, if ever, need to use it yourself.
But if you ever find yourself capturing isolated functions
  and passing them along to other APIs that use @isolated(any),
  you should consider adopting it.
  It could prove useful.
  It’s even a source-compatible change
  to add or remove this attribute from an async function.

So there you have it.
As with many parts of the concurrency system,
  there’s a surprising depth to @isolated(any).
  Thankfully, from a practical perspective,
  we can enjoy the ordering guarantees of task creation
  that it enables without needing to master it.
  And one less thing on this journey is most welcome.
Isolated maybe, but never alone.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who is hiring? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093192</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093192</guid>
            <description><![CDATA[Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.]]></description>
            <content:encoded><![CDATA[Ask HN: Who is hiring? (September 2025)48 points by whoishiring 1 hour ago  | hide | past | favorite | 51 commentsPlease state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.Please only post if you are actively filling a position and are committed
to responding to applicants.Commenters: please don't reply to job posts to complain about
something. It's off topic here.Readers: please only email if you are personally interested in the job.Searchers: try https://dheerajck.github.io/hnwhoishiring/,
https://amber-williams.github.io/hackernews-whos-hiring/,
http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com,
https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/,
https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension:
https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....Don't miss these other fine threads:Who wants to be hired? https://news.ycombinator.com/item?id=45093190Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45093191
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who wants to be hired? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093190</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093190</guid>
            <description><![CDATA[Share your information if you are looking for work. Please use this format:]]></description>
            <content:encoded><![CDATA[Ask HN: Who wants to be hired? (September 2025)20 points by whoishiring 1 hour ago  | hide | past | favorite | 50 commentsShare your information if you are looking for work. Please use this format:  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:

Please only post if you are personally looking for work. Agencies, recruiters, job boards,
and so on, are off topic here.Readers: please only email these addresses to discuss work opportunities.There's a site for searching these posts at https://www.wantstobehired.com.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid>
        </item>
        <item>
            <title><![CDATA[Effective learning: Rules of formulating knowledge (1999)]]></title>
            <link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid>
            <description><![CDATA[This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge.]]></description>
            <content:encoded><![CDATA[Dr Piotr Wozniak, February, 1999 (updated)This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledgeThe speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!  The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as in SuperMemo).The 20 rules of formulating knowledge in learning1) Do not learn if you do not understandTrying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such “blind learning” is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe “it will be useful some day”.  2) Learn before you memorizeBefore you proceed with memorizing individual facts and rules, you need to build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned in Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g. the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g. What moves the pistons in the internal combustion engine?), etc.3) Build upon the basicsThe picture of the learned whole (as discussed in Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!4) Stick to the minimum information principleThe material you learn must be formulated in as simple way as it isSimple is easyBy definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex materialRepetitions of simple items are easier to scheduleI assume you will make repetitions of the learned material using optimum inter-repetition intervals (as in SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split into ten or more simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!Here is a striking example:Ill-formulated knowledge – Complex and wordyQ: What are the characteristics of the Dead Sea?A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth’s surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline watersWell-formulated knowledge – Simple and specificQ: Where is the Dead Sea located?A: on the border between Israel and JordanQ: What is the lowest point on the Earth’s surface?A: The Dead Sea shorelineQ: What is the average level on which the Dead Sea is located?A: 400 meters (below sea level)Q: How long is the Dead Sea?A: 70 kmQ: How much saltier is the Dead Sea than the oceans?A: 7 timesQ: What is the volume content of salt in the Dead Sea?A: 30%Q: Why can the Dead Sea keep swimmers afloat?A: due to high salt contentQ: Why is the Dead Sea called Dead?A: because only simple organisms can live in itQ: Why only simple organisms can live in the Dead Sea?A: because of high salt contentYou might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e. the longer the time you need to remember knowledge, the more you benefit from simplifying your items!Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition! We want answer to be as short as imaginably possible!You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted to enumerations)5) Cloze deletion is easy and effectiveCloze deletion is a sentence with its parts missing and replaced by three dots. Cloze deletion exercise is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example, Bill …[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique called incremental reading.Ill-formulated knowledge – Complex and wordyQ: What was the history of the Kaleida company?A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleida’s mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.Well-formulated knowledge – Simple cloze deletionQ: Kaleida was funded to the tune of …(amount) by Apple Computer and IBM in 1991A: $40 millionQ: Kaleida was funded to the tune of $40 million by …(companies) in 1991A: Apple and IBMQ: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in … (year)A: 1991Q: …(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three yearsA: Kaleida’sQ: Kaleida’s mission was to create a … It finally produced one, called Script X. But it took three yearsA: multimedia programming languageQ: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called … But it took three yearsA: Script XQ: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called Script X. But it took …(time)A: three yearsQ: Kaleida’s mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as … had snapped up all the businessA: Macromedia/AsymetrixQ: Kaleida’s mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in …(year)A: 1995Optional: SuperMemo Recipe:SuperMemo 2002SuperMemo 2000SuperMemo 98/99Creating cloze deletions in new SuperMemos:select the keyword that is to be replaced with tree dots and press Alt+ZGenerating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:1. Press Ctrl+Alt+N to paste the text to SuperMemo 2. Select the part that is to be replaced with three dots 3. Right-click to open the component menu and select Reading : Remember cloze (or click one of cloze icons on the reading toolbar) Cloze deletions in SuperMemo 98/99:1. Press Ctrl+A to add a standard question-and-answer item2. Paste the text into the question field. This will create the outline of your items3. Press Ctrl+Alt+U to Duplicate the element4. Select the part that is to be replaced with three dots5. Cut the selection to the clipboard (e.g. with Shift+Del)6. Type in three dots (optionally, add the explanation in parentheses as in above examples)7. Press Ctrl+T to save the question field and move to the answer field8. Paste the text cut in Step 5 (e.g. with Shift+Ins or Ctrl+V). Your first item is ready9. Press PgUp to go back to the outline item created in Step 210. Goto Step 3 and continue adding new items6) Use imageryVisual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we say one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzan’s mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.Less beneficial formulationQ: What African country is located between Kenya, Zambia and Mozambique?A: TanzaniaWell-formulated knowledge – Simple cloze deletionQ: What African country is marked white on the map?A: Tanzania7) Use mnemonic techniquesMnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will need SuperMemo and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as: mind maps, peg lists, mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!Exemplary mind map:(Six Steps mind map generated in Mind Manager 3.5, imported to SuperMemo 2004, courtesy of John England, TeamLink Australia)8) Graphic deletion is as good as cloze deletionGraphic deletion works like cloze deletion but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The student’s job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:SuperMemo 2000/2002SuperMemo 99This is how you can quickly generate graphic deletion using a picture from the clipboard:1. Press Shift+Ins to paste the picture to SuperMemo2. Press Ctrl+Shift+M and choose Occlusion template to apply graphic deletion template3. SuperMemo 2000 only: Choose Ctrl+Shift+F2 to impose and detach the Occlusion template4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (use Alt+click twice to set the rectangle in the dragging mode)In SuperMemo 99 you will need a few more steps:1.Create an item containing the following components:– question text: What is the name of the area covered with the red rectangle?– empty answer text (click Answer on the component menu)– your illustration (use Import file on the image component menu)– red rectangle component (choose red color with Color on the rectangle component menu)2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)3. Ctrl+click the rectangle component twice to place it in the dragging mode4. Drag and size the red rectangle to cover the area in question5. Type in the answer in the answer field6. Press PgUp to go back to the original element created in Step 17. Go to Step 2 to add generate more graphic deletionsNote that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from >SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above techniqueA more detailed recipe for creating occlusion tests is presented in: Flow of knowledge9) Avoid setsA set is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example: What countries belong to the European Union? You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them into enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of the minimum information principle: you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason: Avoid sets! If you need them badly, convert them into enumerations and use techniques for dealing with enumerationsIll-formulated knowledge – Sets are unacceptable!Q: What countries belong to the European Union (2002)?A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.Well-formulated knowledge – Converting a set into a meaningful listingQ: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?A: FranceQ: Which countries apart from France joined the European Coal and Steel Community in 1952?A: Germany, Italy and the BeneluxQ: What countries make up the Benelux?A: Belgium, Luxembourg, and the NetherlandsQ: Whose membership did Charles de Gaulle oppose in the 1960s?A: that of UKQ: Which countries joined the EEC along the UK in 1973?A: Ireland and DenmarkQ: Which country joined the EEC in 1981?A: GreeceQ: Which countries joined the EEC in 1986?A: Spain and PortugalQ: Which countries joined the EU in 1995?A: Austria, Sweden and FinlandQ: What was the historic course of expansion of the European Union membership?A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and FinlandNote that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with the minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.10) Avoid enumerationsEnumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them using cloze deletions (overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:Hard to learn itemQ: What is the sequence of letters in the alphabet?A: abcdefghijklmnopqrstuvwxyzEasy to learn itemsQ: What three letters does the alphabet begin with?A: ABCQ: Fill out the missing letters of the alphabet A … … … EA: B, C, DQ: Fill out the missing letters of the alphabet B … … … FA: C, D, EQ: Fill out the missing letters of the alphabet C … … … GA: D, E, FThe above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabet after making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict the minimum information principle because the extra information is added in extra items.You can also deal with enumerations by using grouping like in the case of sets (see the European Union example) but cloze deletions should be simpler and should suffice in most cases.Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurableA poem that is hard to rememberQ: The credit belongs … (Teddy Roosevelt)A: The credit belongs to the man who’s actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeat A poem split into easy itemsQ: The credit belongs … (Teddy Roosevelt)A: to the man who’s actually in the arenaQ: The credit belongs to the man who’s actually in the arena …A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)Q: whose face is marred by dust and sweat … (The credit belongs)A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)Q: a man who knows the great enthusiasm and the great devotions … (The credit belongs)A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)Q: who spends himself in a worthy cause … (The credit belongs)A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!11) Combat interferenceWhen you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the words historic and historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case of memory interference. You can often remember an item for years with straight excellent grades until … you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is to detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and … aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:make items as unambiguous as possiblestick to the minimum information principle (many of the remaining rules in this text are based on avoiding interference!)eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the word inept you think “I know the meanings of inept and inapt but I will never know which is which!”)in SuperMemo use View : Other browsers : Leeches(Shift+F3) to regularly review and eliminate most difficult itemsread more: Memory interference12) Optimize wordingThe wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2A: QuarkBetter item: fewer words will speed up learningQ: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then … blew past (PageMaker remains No. 2)A: QuarkOr better:Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by …A: QuarkOr better:Q: PageMaker failed to improve and was outdistanced by …A: QuarkOr better:Q: PageMaker lost ground to …A: QuarkNote that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name: Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!13) Refer to other memoriesReferring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the words humble and supplicant helps the student focus on the word shamelessly and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the words humble and supplicant makes it possible to avoid interference of cringing with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rules basics-to-details and do not learn what you do not understand require that the words humble and supplicant be learned beforehand (or at least at the same time)Item subject to strong interferenceQ: derog: adj: shamelessly conscious of one’s failings and asking in a begging wayA: cringingItem that uses interfering memories to amplify the correct meaningQ: derog: adj: shamelessly humble and supplicantA: cringing14) Personalize and provide examplesOne of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the questionItem subject to strong interferenceQ: What is the name of a soft bed without arms or back?A: divanItem that uses interfering memories to amplify the correct meaningQ: What is the name of a soft bed without arms or back? (like the one at Robert’s parents)A: divanIf you remember exactly what kind of soft bed can be found in Robert’s parents’ apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant to interference and can greatly reduce your learning time15) Rely on emotional statesIf you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situationHarder itemQ: a light and joking conversationA: banterEasier itemQ: a light and joking conversation (e.g. Mandela and de Klerk in 1992)A: banterIf you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such as badinage or even chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent to 25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted on simplicity and against the interference16) Context cues simplify wordingYou can use categories in SuperMemo 2000/2002, provide different branches of knowledge with a different look (different template), use reference labels (Title, Author, Date, etc.) and clearly label subcategories (e.g. with strings such as chem for chemistry, math for mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefix bioch: saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the label bioch which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and … what is worse … you will light up the wrong areas of your brain that will now perhaps be prone to interference!Wordy item can cause accidental lapses through interferenceQ: What does GRE stand for in biochemistry?A: glucocorticoid response elementContext-labeled items increase success rateQ: bioch: GREA: glucocorticoid response element17) Redundancy does not contradict minimum information principleRedundancy in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict the minimum information principle and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate that minimum information principle cannot be understood as minimum number of characters or bits in your collections or even items:passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such as phone-telefono, language-lingvo, hope-esperanto, etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail with telefono-phone, lingvo-language, or esperanto-hope. Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themIn SuperMemo 2000/2002, you can quickly generate swapped word-pair items with Duplicate (Ctrl+Alt+D) and Swap (Ctrl+Shift+S)reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of just memorizing the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) and generate the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitionsderivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read: Roots of genius and creativity, as well as more specific: Derivation, reasoning and intelligencemultiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with the forgetting index)!flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g. a mark made by ink spilt on sth = blot/blob/blotch)more18) Provide sourcesExcept for well-tested and proven knowledge (such as 2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g. Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).19) Provide date stampingKnowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.20) PrioritizeYou will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see: learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:Prioritizing sources – there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of othersExtracting knowledge – unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to use incremental reading toolsTransferring knowledge to SuperMemo – you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate itFormulating items – make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its contextUsing forgetting index – you can use the forgetting index to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priorityLearning – the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:Remember (Ctrl+M) – re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you choose Ctrl+M you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantlyReschedule (Ctrl+J) – manually schedule the date of the next repetitionExecute repetition (Ctrl+Shift+R) – manually execute a repetition even before the repetition’s due date (e.g. when reviewing particularly important material)Forget (Ctrl+R)- remove the current item from the learning process and place it at the end of the pending queueDismiss (Ctrl+D) – ignore the current item in the learning process altogetherDelete (Ctrl+Shift+Del) – remove the current item from your collectionChange the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)SummaryHere again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:Do not learn if you do not understandLearn before you memorize – build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!Build upon the basics – never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit inStick to the minimum information principle – if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)Cloze deletion is easy and effective – completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginnersUse imagery – a picture is worth a thousand wordsUse mnemonic techniques – read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You won’t have problems with phone numbers and complex figuresGraphic deletion is as good as cloze deletion – obstructing parts of a picture is great for learning anatomy, geography and moreAvoid sets – larger sets are virtually un-memorizable unless you convert them into enumerations!Avoid enumerations – enumerations are also hard to remember but can be dealt with using cloze deletionCombat interference – even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal lifeOptimize wording – like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maximsRefer to other memories – building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gapsPersonalize and provide examples – personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memoriesRely on emotional states – emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in rememberingContext cues simplify wording – providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interferenceRedundancy does not contradict minimum information principle – some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!Provide sources – sources help you manage the learning process, updating your knowledge, judging its reliability, or importanceProvide date stamping – time stamping is useful for volatile knowledge that changes in timePrioritize – effective learning is all about prioritizing. In incremental reading you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also: Incremental reading, Devouring knowledge, Flow of knowledge, Using tasklists]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google AI Overview made up an elaborate story about me]]></title>
            <link>https://bsky.app/profile/bennjordan.bsky.social/post/3lxojrbessk2z</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092925</guid>
        </item>
        <item>
            <title><![CDATA[Git for Music – Using Version Control for Music Production (2023)]]></title>
            <link>https://grechin.org/2023/05/06/git-and-reaper.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092895</guid>
            <description><![CDATA[last updated on 6 Apr 2024]]></description>
            <content:encoded><![CDATA[
        

  

  
    
  last updated on 6 Apr 2024


Being both a musician and a software engineer, I always felt that these two areas are almost completely separated. My developer skill-set seemed to have little to no use for my work as a musician. Which is a pity considering how cool it would be if there was some kind of a sinergy across these two sides of my life.

Recently, though, I have found a useful possibility to utilize something I previously used solely for my development work, namely, git, the version control tool, for my music production.

Okay, and now let’s get to the point and…



Did you notice yourself creating a dozen of versions of your project? Are the names like this familiar to you?

my-cool-song-new-vocals-brighter-mix-4.rpp

Did you ever feel frustrated about unmanageability of all this and how sloppy you project directory ends up looking?

This version nightmare problem for software people has a solid and well-recognized solution: version control systems. Such as “git”, which is not only the most widely used one in the industry, but also completely free, open source and cross platform (that is working flawlessly on Win/Mac/Linux).

For music production, I use Reaper, and instead of creating dozens of copies of my project file (my-cool-song.rpp), such as my-cool-song-new-vocals-brighter-mix-4.rpp, I simply initialize a git repository in the project folder and put the file under version control. This git repository will be the “home” for managing the version of our music project.

By the way, a good supplementary for this reading could be this video of me going through an example. If you are not fan of watching videos, feel free to read on.


  


My git-based music production workflow

Although, when wearing a developer hat, I am normally in linux, for the music production stuff, due to the better availability of plugins and such, Windows is a better option. For Windows, you can install git-bash, and have all the git functionality at your fingertips through a command-line interface.

First, I initialize a repository in the project directory. For me, it is most convenient to use a git bash command line terminal:

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music
$ cd test_git_project/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project
$ git init .
Initialized empty Git repository in C:/home/music/test_git_project/.git/

Acer@DESKTOP-NRN84IB MINGW64 /c/home/music/test_git_project (master)
$


in the example above:

  I first navigated to the directory with my project with cd command
  initialized a repository with git init .
  on the last, third line, my command prompt starts having a little (master) thing, which is the default “branch” in my repository that Git has created for me


I also create a .gitignore file and that this is this particular project file that I want to track, and not any other, such as media or peak files:

*

!in_your_eyes_remix_git_managed.rpp


Then I am free to work with the project in my DAW as usual. When I am done working on a specific version, I make a commit and give it a descriptive name, e.g. “bass vst settings adjusted”.

Then I can see all the versions of my project in git gui tool.




  side note: you can use any git frontend, not only git gui.


Not only that, but I can also open any historical version of the project, create branches and so on. In other words, I can fully benefit from the version control system! If you are already using git, you know what I mean.

The days of versioned files mess in my project folder are finally gone! I wonder, though, if Reaper developers will be willing to incorporate that into their product one day.

Managing other files (WAVs etc.)

Git is not super suited for managing big binary files (such as WAV samples and stems), but this is not a problem for me since I only manage the main project file.

About other files I do not care. Why? Becase I never remove them. The media files are either WAVs related to this project (and which are therefore kept in the project folder) or samples from my library. In both cases, these files are normally (at least withing the lifespan of the project under construction) never deleted.

This approach, which, I guess, I share with most producers, makes it easy to return to any historical version of the project and rely on the media files to be found.

collaborating with GIT? Not sure…

GIT is not only about versioning, but also about collaboration, with remote repositories and so on. Frankly, I don’t see it feasible for collaboration over music projects since the project files are normally opaque and we should not expect git or any other version control system to be able to merge/diff them.

And let’s not forget that to be able to work on your project, the collaborator needs to have very close set up: the DAW, the plugins and all the media files.

Another note of the remote repositories: I do find it useful that I can push my music project to github and this kind of a backup that will outlive my current PC. This is nice, but we can’t really consider it a real backup - because of missing media.

Tracking TODO items for your music project in GitHub

Interesting use-case I’m currently testing is to have a “todo list”, think of an small per-project issue tracker with a list of things you plan to do later. Just a version-tracked text file of the format similar to this:

fix panning issues in chorus TODO
add one more synth layer TODO


Once it’s in Github, you can update it from anywhere (GitHub allows you to edit files right in the browser), so, basically, you project gets its own, private, read/write website. On the go and got a cool idea? Now you know where to record it (don’t forget to pull your update, though, once you are back to your DAW PC).

In conclusion, when we inspect this idea of “git for music” a bit closer, we can see that it does have a few viable applications. Yes, this tool is not magical, but still pretty useful!

Thanks for reading.

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI enters the grant game, picking winners]]></title>
            <link>https://www.science.org/content/article/ai-enters-grant-game-picking-winners</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092880</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Simple modenized .NET NuGet server reached RC]]></title>
            <link>https://github.com/kekyo/nuget-server</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092734</guid>
            <description><![CDATA[Simple modenized NuGet server 📦. Contribute to kekyo/nuget-server development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[nuget-server
Simple modenized NuGet server implementation.






(日本語はこちら)
What is this?
A simple NuGet server implementation built on Node.js that provides essential NuGet v3 API endpoints.
Compatible with dotnet restore and standard NuGet clients for package publishing, querying, and manually downloading.
A modern browser-based UI is also provided:

You can refer to registered packages. You can check various package attributes.
You can download packages by version.
You can also publish (upload) packages.
You can manage user accounts.

Browse package list:

Publishing packages:

User account managements:

Key Features

Easy setup, run NuGet server in 10 seconds!
NuGet V3 API compatibility: Support for modern NuGet client operations
No need database management: Store package file and nuspecs into filesystem directly, feel free any database managements
Package publish: Flexible client to upload .nupkg files via HTTP POST using cURL and others
Basic authentication: Setup authentication for publish and general access when you want it
Reverse proxy support: Configurable trusted reverse proxy handling for proper URL resolution
Modern Web UI with enhanced features:

Multiple package upload: Drag & drop multiple .nupkg files at once
User account management: Add/delete users, reset passwords (admin only)
API password regeneration: Self-service API password updates
Password change: Users can change their own passwords


Package importer: Included package importer from existing NuGet server
Docker image available


Installation
npm install -g nuget-server
For using Docker images, refer to a separate chapter.
Usage
# Start server on default port 5963
nuget-server

# Custom port
nuget-server --port 3000

# Multiple options
nuget-server --port 3000 --config-file config/config.json --users-file config/users.json
The NuGet V3 API is served on the /v3 path.

Default nuget-server served URL (Show UI): http://localhost:5963
Actual NuGet V3 API endpoint: http://localhost:5963/v3/index.json

The default URL provided by nuget-server can be changed using the --base-url option.
This is particularly necessary when public endpoint service using a reverse proxy. For details, refer to below chapter.
Configure the NuGet client
nuget-server only supports the NuGet V3 API. Therefore, NuGet clients must always access it using the V3 API.
If you do not explicitly specify to use the V3 API, some implementations may fall back to the V3 API while others may not, potentially causing unstable behavior. Therefore, you must always specify it. Example below.
Add as package source:
For HTTP endpoints:
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" --protocol-version 3 --allow-insecure-connections
For HTTPS endpoints:
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Or specify in nuget.config:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
</configuration>
Publish packages
Upload packages by HTTP POST method, using cURL or any HTTP client with /api/publish endpoint:
# Upload "MyPackage.1.0.0.nupkg" file
curl -X POST http://localhost:5963/api/publish \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
You may be dissatisfied with publishing using this method. The dotnet command includes dotnet nuget push, which is the standard approach.
However, in my experience, this protocol uses multipart/form-data for transmission, which has caused issues with gateway services, reverse proxies, load balancers, and similar components.
Therefore, the current nuget-server does not implement this method and instead uses the simplest binary transmission procedure.
Another advantage is that when authentication is enabled, you don't need to manage Basic authentication and V3 API keys separately.
You might still feel issue with managing read operations and publish operation with the same key,
but in that case, you can simply separate the users.
For authentication feature, please refer to below chapter.

Package storage configuration
Storage location
By default, packages are stored in the ./packages directory relative to where you run nuget-server.
You can customize this location using the --package-dir option:
# Use default ./packages directory
nuget-server

# Use custom directory (relative or absolute path)
nuget-server --package-dir /another/package/location
Package storage layout
Packages are stored in the filesystem using the following structure:
packages/
├── PackageName/
│   ├── 1.0.0/
│   │   ├── PackageName.1.0.0.nupkg
│   │   ├── PackageName.nuspec
│   │   └── icon.png            # Package icon (if present)
│   └── 2.0.0/
│       ├── PackageName.2.0.0.nupkg
│       ├── PackageName.nuspec
│       └── icon.jpg            # Package icon (if present)
└── AnotherPackage/
    └── 1.5.0/
        ├── AnotherPackage.1.5.0.nupkg
        ├── AnotherPackage.nuspec
        └── icon.png            # Package icon (if present)

Backup and restore
You can backup the package directory using simply tar or other achiver:
cd /your/server/base/dir
tar -cf - ./packages | lz4 > backup-packages.tar.lz4
Restore is simply extract it and re-run nuget-server with the same package directory configuration, because nuget-server does not use any specialized storage such as databases.

Configuration
nuget-server supports configuration through command-line options, environment variables, and JSON file.
Settings are applied in the following order (highest to lowest priority):

Command-line options
Environment variables
config.json
Default values

Configuration file structure
You can specify a custom configuration file:
# Using command line option
nuget-server --config-file /path/to/config.json
# or short alias
nuget-server -c /path/to/config.json

# Using environment variable
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
nuget-server
If not specified, nuget-server looks for ./config.json in the current directory.
config.json structure
Create a config.json file:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "packageDir": "./packages",
  "usersFile": "./users.json",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "trustedProxies": ["127.0.0.1", "::1"],
  "authMode": "none",
  "sessionSecret": "<your-secret-here>",
  "passwordMinScore": 2,
  "passwordStrengthCheck": true
}
All fields are optional. Only include the settings you want to override.
Both packageDir and usersFile paths can be absolute or relative. If relative, they are resolved from the directory containing the config.json file.

Authentication
nuget-server also supports authentication.



Authentication Mode
Details
Auth Initialization




none
Default. No authentication required
Not required


publish
Authentication required only for package publishing
Required


full
Authentication required for all operations (must login first)
Required



To enable authentication on the NuGet server, first register an initial user using the --auth-init option.
Initialize
Create an initial admin user interactively:
nuget-server --auth-init
This command will:

Prompt for admin username (default: admin)
Prompt for password (with strength checking, masked input)
Create users.json
Exit after initialization (server does not start)

When enabling authentication using a Docker image, use this option to generate the initial user.
Example session
Initializing authentication...
Enter admin username [admin]:
Enter password: ********
Confirm password: ********

============================================================
Admin user created successfully!
============================================================
Username: admin
Password: *********************
============================================================

User Management
Users added with --auth-init automatically become administrator users.
Administrator users can add or remove other users via the UI. They can also reset user passwords.

While administrator users can also be assigned API passwords (described later), we recommend separating users for management whenever possible.
Using the API password
The NuGet server distinguishes between the password used to log in to the UI and the password used by NuGet clients when accessing the server.
The password used by NuGet clients when accessing the server is called the "API password,"
and access is granted using the combination of the user and the API password.
Please log in by displaying the UI in the browser.
Select the “API password” menu from the UI menu to generate an API password.
Using this API password will enable access from the NuGet client.

Here is an example of using the API password:
# Add source with API password
dotnet nuget add source http://localhost:5963/v3/index.json \
  -n "local" \
  -u admin \
  -p xxxxxxxxxxxxxxxxxxxxxx \
  --protocol-version 3 --store-password-in-clear-text --allow-insecure-connections
Or specify nuget.config with credentials:
<?xml version="1.0" encoding="utf-8"?>
<configuration>
  <packageSources>
    <add key="local" value="http://localhost:5963/v3/index.json"
      protocolVersion="3" allowInsecureConnections="true" />
  </packageSources>
  <packageSourceCredentials>
    <local>
      <add key="Username" value="reader" />
      <add key="ClearTextPassword" value="xxxxxxxxxxxxxxxxxxxxxx" />
    </local>
  </packageSourceCredentials>
</configuration>
For package publishing:
# Publish packages with API password
curl -X POST http://localhost:5963/api/publish \
  -u admin:xxxxxxxxxxxxxxxxxxxxxx \
  --data-binary @MyPackage.1.0.0.nupkg \
  -H "Content-Type: application/octet-stream"
When publishing a package, you can send the package by setting Basic authentication in the Authorization header.
Password strength requirements
nuget-server uses the zxcvbn library to enforce strong password requirements:

Evaluates password strength on a scale of 0-4 (Weak to Very Strong)
Default minimum score: 2 (Good)
Checks against common passwords, dictionary words, and patterns
Provides real-time feedback during password creation

Configure password requirements in config.json:
{
  "passwordMinScore": 2, // 0-4, default: 2 (Good)
  "passwordStrengthCheck": true // default: true
}
The NuGet server stores both "password" and "API password" as SALT hashed information, so no plaintext passwords are ever saved.
However, if you do not use HTTPS (TLS), be aware that the Authorization header will contain the plaintext password, making it vulnerable to sniffing.
When makes public endpoint, protect communications using HTTPS.

Import packages from another NuGet server
Import all packages from another NuGet server to your local nuget-server instance.
This feature can be used when migrating the foreign NuGet server to nuget-server.
Package import from another NuGet server
Import packages interactively in CLI:
nuget-server --import-packages --package-dir ./packages
This command will:

Prompt for source NuGet server URL
Ask if authentication is required
If needed, prompt for username and password (masked input)
Discover all packages from the source server
Download and import all packages to local storage
Display progress for each package (1% intervals)
Exit after import (server does not start)

Import behavior

Existing packages with the same version will be overwritten
Failed imports are logged with error details
Progress is reported at 1% intervals to reduce log noise
Package icons are preserved during import

Parallel downloads are not done. This is to avoid making a large number of requests to the repository.
This feature is a type of downloader.
Therefore, it does not need to be run on the actual host where it will operate.
You can perform the import process in advance on a separate host and then move the packages directory as-is.
Example session
Starting package import...
Enter source NuGet server URL [http://host.example.com/repository/nuget/]: https://nexus.example.com/repository/nuget/
Does the server require authentication? [y/N]: y
Enter username: reader
Enter password: **********

============================================================
Import Configuration:
Source: https://nexus.example.com/repository/nuget/
Target: ./packages
Authentication: reader (password hidden)
============================================================

Start importing packages? (existing packages will be overwritten) [y/N]: y

Discovering packages from source server...
Found 125 packages with 563 versions total.
Starting package import...
Progress: 100/563 packages (17%) - MyPackage.Core@1.2.3
Progress: 563/563 packages (100%) - AnotherPackage@2.0.0

============================================================
Import Complete!
============================================================
Total packages: 125
Total versions: 563
Successfully imported: 563
Failed: 0
Time elapsed: 125.3 seconds
============================================================


Reverse proxy interoperability
The server supports running behind a reverse proxy.
For example, when you have a public URL like https://nuget.example.com and run nuget-server on a host within your internal network via a gateway.
In such cases, you MUST specify the base URL of the public URL to ensure the NuGet V3 API can provide the correct sub-endpoint address.
URL resolving
The server resolves URLs using the following priority order:

Fixed base URL (highest priority): When --base-url option is specified, it always takes precedence
Trusted proxy headers: When trusted proxies are configured with --trusted-proxies:

HTTP Forwarded header (proto, host, port)
Traditional X-Forwarded-* headers (X-Forwarded-Proto, X-Forwarded-Host, X-Forwarded-Port)


Standard request information (fallback): Uses Host header when proxy headers are not available

For example --base-url option:

nuget-server served public base URL: https://packages.example.com
Actual NuGet V3 API endpoint: https://packages.example.com/v3/index.json

# Configure served base URL (do not include /v3 path)
nuget-server --base-url https://packages.example.com

# Add as NuGet source (HTTPS - no --allow-insecure-connections needed)
dotnet nuget add source https://packages.example.com/v3/index.json \
  -n "packages" --protocol-version 3
Another option, you can configure with trusted proxy addresses:
# Configure trusted proxies for proper host header handling
nuget-server --trusted-proxies "10.0.0.1,192.168.1.100"
Environment variables are also supported:
export NUGET_SERVER_BASE_URL=https://packages.example.com
export NUGET_SERVER_TRUSTED_PROXIES=10.0.0.1,192.168.1.100
export NUGET_SERVER_CONFIG_FILE=/path/to/config.json
export NUGET_SERVER_USERS_FILE=/path/to/users.json
export NUGET_SERVER_SESSION_SECRET=your-secret-key-here

Docker usage
Docker images are available for multiple architectures:

linux/amd64 (x86_64)
linux/arm64 (aarch64)

When pulling the image, Docker automatically selects the appropriate architecture for your platform.
Quick start
Suppose you have configured the following directory structure for persistence (recommended):
docker-instance/
├── data/
│   ├── config.json
│   └── user.json
└── packages/
    └── (package files)

Execute as follows:
# Pull and run the latest version
docker run -d -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# Or with Docker Compose
cat > docker-compose.yml << EOF
version: '3'
services:
  nuget-server:
    image: kekyo/nuget-server:latest
    ports:
      - "5963:5963"
    volumes:
      - ./data:/data
      - ./packages:/packages
    environment:
      - NUGET_SERVER_AUTH_MODE=publish
EOF

docker-compose up -d
Your NuGet server is now available at:

Web UI: http://localhost:5963
NuGet V3 API: http://localhost:5963/v3/index.json

Permission requirements
The Docker container runs as the nugetserver user (UID 1001) for security reasons. You need to ensure that the mounted directories have the appropriate permissions for this user to write files.
Set proper permissions for mounted directories:
# Create directories if they don't exist
mkdir -p ./data ./packages

# Set ownership to UID 1001 (matches the container's nugetserver user)
sudo chown -R 1001:1001 ./data ./packages
Important: Without proper permissions, you may encounter 500 Permission Denied errors when:

Creating or updating user accounts
Publishing packages
Writing configuration files

Basic usage
# Run with default settings (port 5963, packages and data stored in mounted volumes)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  kekyo/nuget-server:latest

# With authentication (users.json will be created in /data)
docker run -p 5963:5963 \
  -v $(pwd)/data:/data \
  -v $(pwd)/packages:/packages \
  -e NUGET_SERVER_AUTH_MODE=publish \
  kekyo/nuget-server:latest
You can also change settings using environment variables or command-line options, but the easiest way to configure settings is to use config.json.
Since the Docker image has mount points configured, you can mount /data and /packages as shown in the example above and place /data/config.json there to flexibly configure settings. Below is an example of config.json:
{
  "port": 5963,
  "baseUrl": "http://localhost:5963",
  "realm": "Awsome nuget-server",
  "logLevel": "info",
  "authMode": "publish"
}
When initializing credentials or importing packages, configure config.json and perform the operation via the CLI before launching the Docker image:
# Initialize authentication
nuget-server -c ./data/config.json --auth-init
Volume mounts and configuration

/data: Default data directory for config.json, users.json and other persistent data
/packages: Default package storage directory (mounted to persist packages)

Default behavior: The Docker image runs with --users-file /data/users.json --package-dir /packages by default.
Configuration priority (highest to lowest):

Custom command line arguments (when overriding CMD)
Environment variables (e.g., NUGET_SERVER_PACKAGE_DIR)
config.json file (if explicitly specified)
Default command line arguments in Dockerfile

Example of Automatic Startup Using systemd
Various methods exist for automatically starting containers with systemd.
Below is a simple example of configuring a systemd service using Podman.
This is a simple service unit file used before quadlets were introduced to Podman.
By placing this file and having systemd recognize it, you can automatically start the nuget-server:
/etc/systemd/system/container-nuget-server.service:
# container-nuget-server.service

[Unit]
Description=Podman container-nuget-server.service
Documentation=man:podman-generate-systemd(1)
Wants=network-online.target
After=network-online.target
RequiresMountsFor=%t/containers

[Service]
Environment=PODMAN_SYSTEMD_UNIT=%n
Restart=always
RestartSec=30
TimeoutStopSec=70
ExecStart=/usr/bin/podman run \
        --cidfile=%t/%n.ctr-id \
        --cgroups=no-conmon \
        --rm \
        --sdnotify=conmon \
        --replace \
        -d \
        -p 5963:5963 \
        --name nuget_server \
        -v /export/data:/data -v /export/packages:/packages docker.io/kekyo/nuget-server:latest
ExecStop=/usr/bin/podman stop \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
ExecStopPost=/usr/bin/podman rm \
        -f \
        --ignore -t 10 \
        --cidfile=%t/%n.ctr-id
Type=notify
NotifyAccess=all

[Install]
WantedBy=default.target

Building the Docker image (Advanced)
The build of the nuget-server Docker image uses Podman.
Multi-platform build with Podman (recommended)
Use the provided multi-platform build script that uses Podman to build for all supported architectures:
# Build for all platforms (local only, no push)
./build-docker-multiplatform.sh

# Build and push to Docker Hub
./build-docker-multiplatform.sh --push

# Build for specific platforms only
./build-docker-multiplatform.sh --platforms linux/amd64,linux/arm64

# Push with custom Docker Hub username
OCI_SERVER_USER=yourusername ./build-docker-multiplatform.sh --push

# Inspect existing manifest
./build-docker-multiplatform.sh --inspect
Important: For cross-platform builds, QEMU emulation must be configured first:
# Option 1: Use QEMU container (recommended)
sudo podman run --rm --privileged docker.io/multiarch/qemu-user-static --reset -p yes

# Option 2: Install system packages
# Ubuntu/Debian:
sudo apt-get update && sudo apt-get install -y qemu-user-static
# Fedora/RHEL:
sudo dnf install -y qemu-user-static

# Verify QEMU is working:
podman run --rm --platform linux/arm64 alpine:latest uname -m
# Should output: aarch64
Without QEMU, you can only build for your native architecture.

Note
Non-interactive mode (CI/CD)
The --auth-init and --import-packages options require interactive responses from the operator.
Therefore, attempting to automate these may not work properly.
In such cases, you can provide credentials via environment variables:
export NUGET_SERVER_ADMIN_USERNAME=admin
export NUGET_SERVER_ADMIN_PASSWORD=MySecurePassword123!
nuget-server --auth-init --config-file ./config.json
This allows initialization in CI/CD pipelines without user interaction.
Session Security
For special configurations (or to support persistent sessions), you can set a fixed session secret. Specify a sufficiently long value for the secret:
export NUGET_SERVER_SESSION_SECRET=$(openssl rand -base64 32)
nuget-server
(Or use config.json.)
If not set, a random secret is generated (warning will be logged).
Supported NuGet V3 API endpoints
The server implements a subset of the NuGet V3 API protocol:

Service index: /v3/index.json
Package content: /v3/package/{id}/index.json
Package downloads: /v3/package/{id}/{version}/{filename}
Registration index: /v3/registrations/{id}/index.json


License
Under MIT.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zfsbackrest: Pgbackrest style encrypted backups for ZFS filesystems]]></title>
            <link>https://github.com/gargakshit/zfsbackrest</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092605</guid>
            <description><![CDATA[pgbackrest style encrypted backups for ZFS filesystems - gargakshit/zfsbackrest]]></description>
            <content:encoded><![CDATA[zfsbackrest

⚠️ Experimental:
Do not use it as your only way for backups. This is something I wrote over a
weekend. There's a lot of things that need work here.

pgbackrest style encrypted backups for ZFS
filesystems.
Getting Started
Installing
You need age installed to generate
encryption keys. Encryption is NOT optional.
$ go install github.com/gargakshit/zfsbackrest/cmd/zfsbackrest@latest
Configuring
Create /etc/zfsbackrest.toml.
debug = true # warning, may log sensitive data

[repository]
# zfsbackrest does not support changing the list of datasets after a repository
# is initialized YET. That's one feature I need.
included_datasets = ["storage/*"] # Glob is supported

[repository.s3]
# zfsbackrest does NOT support non-secure S3 endpoints.
endpoint = "todo"
bucket = "todo"
key = "todo"
secret = "todo"
region = "todo"

[repository.expiry]
# Child backups expire if the parent expires. See the model below for a better
# explanation.
full = "336h" # 14 days
diff = "120h" # 5 days
incr = "24h" # 1 day

[upload_concurrency]
full = 2
diff = 4
incr = 4
Creating a repository
$ zfsbackrest init --age-recipient-public-key="<your age public key>"
Backing up
$ zfsbackrest backup --type <full | diff | incr>
full backups are standalone. They do not depend on any other backups. They are
also huge in size because of that.
diff backups are sent incrementally from the latest full backup. They depend
on the parent full backup to be present in the repository to restore.
incr backups are send incrementally from the latest diff backup. They depend
on the parent diff backup to restore.
Viewing the repository
$ zfsbackrest detail
It shows a list of backups, orphans and all.
Cleaning up the repository
Sometimes, orphaned backups are left as an artefact of incomplete or cancelled
backups. You can clean those by running
$ zfsbackrest cleanup --orphans --dry-run=false
You can clean up expired backups by running
$ zfsbackrest cleanup --expired --dru-run=false
Restoring
To restore the backups, you'll need your age identity file (private key).
zfsbackrest restore -i <path-to-age-identity-file> \
  -s <name of the dataset to restore from> \
  -b <optionally, the backup ID to restore from, leave empty to restore the latest> \
  -d <name of the dataset to restore to> # Restoring to a dataset that already exists on your local FS will fail.
Safety
zfsbackrest doesn't write or modify actual zfs datasets. It makes extensive
use of snapshots. List of zfs operations used by zfsbackrest are


backup

zfs snapshot - Creating a zfs snapshot for zfsbackrest
zfs hold - Creating a reference to that snapshot to prevent removal
zfs send - Sending the snapshot incrementally



cleanup / force-destroy

zfs release - Release the held snapshot
zfs destroy - Destroy the snapshot



restore

zfs recv - Receiving the remote snapshot



Model
TODO
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bear is now source-available]]></title>
            <link>https://herman.bearblog.dev/license/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092490</guid>
            <description><![CDATA[Updates to the Bear license]]></description>
            <content:encoded><![CDATA[
  
  
    
      
        ᕕ( ᐛ )ᕗ Herman's blog
      
    
    
      Home Now Projects Blog

    
  
  
    

    
        
    

    
        

        
            
                
                    01 Sep, 2025
                
            
        
    

    When I started building Bear I made the code available under an MIT license. I didn't give it much thought at the time, but knew that I wanted the code to be available for people to learn from, and to make it easily auditable so users could validate claims I have made about the privacy and security of the platform.
Unfortunately over the years there have been cases of people forking the project in the attempt to set up a competing service. And it hurts. It hurts to see something you've worked so hard on for so long get copied and distributed with only a few hours of modification. It hurts to have poured so much love into a piece of software to see it turned against you and threaten your livelihood. It hurts to believe in open-source and then be bitten by it.
After the last instance of this I have come to the difficult decision to change Bear's license from MIT to a version of copyleft called the Elastic License—created by the Elastic Search people.
This license is almost identical to the MIT license but with the stipulation that the software cannot be provided as a hosted or managed service. You can view the specific wording here.
After spending time researching how other projects are handling this, I realise I'm not alone. Many other open-source projects have updated their licenses to prevent "free-ride competition" in the past few years.123456
We're entering a new age of AI powered coding, where creating a competing product only involves typing "Create a fork of this repo and change its name to something cool and deploy it on an EC2 instance".
While Bear's code is good, what makes the platform special is the people who use it, and the commitment to longevity.
I will ensure the platform is taken care of, even if it means backtracking on what people can do with the code itself.


    

    
        

        
            


        
    


  
  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tetris is NP-hard even with O(1) rows or columns (2020) [pdf]]]></title>
            <link>https://martindemaine.org/papers/ThinTetris_JIP/paper.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45092324</guid>
        </item>
        <item>
            <title><![CDATA[CocoaPods trunk read-only plan]]></title>
            <link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid>
            <description><![CDATA[The Dependency Manager for iOS & Mac projects]]></description>
            <content:encoded><![CDATA[
        TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.

Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:


We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.


I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.

Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)

May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the prepare_command field in a Podspec. Any existing Pods using prepare_command are hard-coded to bypass this check.

Timeline

My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.

May 2025

We are stopping new CocoaPods from being added which use the prepare_command field

Mid-late 2025

I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.

September-October 2026

I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.

November 1-7th 2026

I will trigger a test run, giving automation a chance to break early

December 2nd 2026

I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.



These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.

If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.

      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UK's largest battery storage facility at Tilbury substation]]></title>
            <link>https://www.nationalgrid.com/national-grid-connects-uks-largest-battery-storage-facility-tilbury-substation</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45091119</guid>
            <description><![CDATA[The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.]]></description>
            <content:encoded><![CDATA[    
            National Grid has connected the UK’s largest battery energy storage system (BESS) to its transmission network at Tilbury substation in Essex.
        The 300MW Thurrock Storage project, developed by Statera Energy, is now energised and delivering electricity flexibly to the network across London and the south east.With a total capacity of 600MWh, Thurrock Storage is capable of powering up to 680,000 homes, and can help to balance supply and demand by soaking up surplus clean electricity and discharging it instantaneously when the grid needs it.Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.National Grid reinforced its Tilbury substation to ensure the network in the region could safely carry the battery’s significant additional load, with new protection and control systems installed to ensure a robust connection.The substation previously served the coal-fired Tilbury A and B power stations on adjacent land prior to their demolition, so the connection of the Thurrock Storage facility marks a symbolic transition from coal to clean electricity at the site.John Twomey, director of customer and network development at National Grid Electricity Transmission, said:“Battery storage plays a vital role in Britain’s clean energy transition. Connecting Thurrock Storage, the UK’s biggest battery, to our transmission network marks a significant step on that journey.“Our Tilbury substation once served a coal plant, and with battery connections like this, it’s today helping to power a more sustainable future for the region and the country.”Tom Vernon, Statera Energy CEO and founder, said:“We are delighted that Thurrock Storage is now energised, following its successful connection to the grid by National Grid Electricity Transmission. Increasing BESS capacity is essential for supporting the grid when renewable generation, such as solar and wind, is low or changes quickly. It ensures that energy can be stored efficiently and returned to the grid whenever it’s needed.”National Grid is continuing work at Tilbury substation to connect the 450MW Thurrock Flexible Generation facility, another Statera project that is set to support the energy needs of the region.The connection of the UK’s biggest battery follows energisation in July of the 373MW Cleve Hill Solar Park in Kent – the largest solar plant in the country – which National Grid connected to its adjacent Cleve Hill substation.]]></content:encoded>
        </item>
    </channel>
</rss>