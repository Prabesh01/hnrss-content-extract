<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Mon, 01 Sep 2025 21:07:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[The Physics of Sales]]></title>
            <link>https://howtogrow.substack.com/p/the-physics-of-sales</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45096254</guid>
        </item>
        <item>
            <title><![CDATA[Towards Memory Specialization: A Case for Long-Term and Short-Term RAM]]></title>
            <link>https://arxiv.org/abs/2508.02992</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45096140</guid>
            <description><![CDATA[Both SRAM and DRAM have stopped scaling: there is no technical roadmap to reduce their cost (per byte/GB). As a result, memory now dominates system cost. This paper argues for a paradigm shift from today's simple memory hierarchy toward specialized memory architectures that exploit application-specific access patterns. Rather than relying solely on traditional off-chip DRAM and on-chip SRAM, we envisage memory systems equipped with additional types of memory whose performance trade-offs benefit workloads through non-hierarchical optimization. We propose two new memory classes deserving explicit OS support: long-term RAM (LtRAM) optimized for read-intensive data with long lifetimes, and short-term RAM (StRAM) designed for transient, frequently-accessed data with short lifetimes. We explore underlying device technologies that could implement these classes, including their evolution and their potential integration into current system designs given emerging workload requirements. We identify critical research challenges to realize what we believe is a necessary evolution toward more efficient and scalable computing systems capable of meeting future demands.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
            Abstract:Both SRAM and DRAM have stopped scaling: there is no technical roadmap to reduce their cost (per byte/GB). As a result, memory now dominates system cost. This paper argues for a paradigm shift from today's simple memory hierarchy toward specialized memory architectures that exploit application-specific access patterns. Rather than relying solely on traditional off-chip DRAM and on-chip SRAM, we envisage memory systems equipped with additional types of memory whose performance trade-offs benefit workloads through non-hierarchical optimization. We propose two new memory classes deserving explicit OS support: long-term RAM (LtRAM) optimized for read-intensive data with long lifetimes, and short-term RAM (StRAM) designed for transient, frequently-accessed data with short lifetimes. We explore underlying device technologies that could implement these classes, including their evolution and their potential integration into current system designs given emerging workload requirements. We identify critical research challenges to realize what we believe is a necessary evolution toward more efficient and scalable computing systems capable of meeting future demands.
    

    
    
              
          Comments:
          9 pages, 3 figures
        

          Subjects:
          
            Hardware Architecture (cs.AR); Emerging Technologies (cs.ET)
        
          Cite as:
          arXiv:2508.02992 [cs.AR]
        
        
           
          (or 
              arXiv:2508.02992v1 [cs.AR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.02992
              
                                arXiv-issued DOI via DataCite
            
          
        
    
  
      Submission history From: Peijing Li [view email]          [v1]
        Tue, 5 Aug 2025 01:39:37 UTC (405 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Patrick Winston: How to Speak (2018) [video]]]></title>
            <link>https://www.youtube.com/watch?v=Unzc731iCUY</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095849</guid>
        </item>
        <item>
            <title><![CDATA[SparseLoCo: Communication-Efficient LLM Training]]></title>
            <link>https://arxiv.org/abs/2508.15706</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095763</guid>
            <description><![CDATA[Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models (LLMs) in bandwidth-constrained settings, such as across data centers and over the internet. Despite reducing communication frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a communication bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While quantization and error feedback are often applied to reduce the pseudo-gradient's size, in the context of LLM pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited quantization. In this work, we introduce SparseLoCo, a communication-efficient training algorithm for LLMs that effectively leverages Top-k sparsification and quantization to reach extreme compression ratios of up to 1-3% sparsity and 2-bit quantization while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback combined with aggressive sparsity and that sparse aggregation can actually improve model performance. We empirically demonstrate in a range of communication-constrained LLM training settings that SparseLoCo provides significant benefits in both performance and communication cost.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Communication-efficient distributed training algorithms have received considerable interest recently due to their benefits for training Large Language Models (LLMs) in bandwidth-constrained settings, such as across data centers and over the internet. Despite reducing communication frequency, these methods still typically require communicating a full copy of the model's gradients-resulting in a communication bottleneck even for cross-datacenter links. Furthermore, they can slightly degrade performance compared to a naive AdamW DDP baseline. While quantization and error feedback are often applied to reduce the pseudo-gradient's size, in the context of LLM pre-training, existing approaches have been unable to additionally leverage sparsification and have obtained limited quantization. In this work, we introduce SparseLoCo, a communication-efficient training algorithm for LLMs that effectively leverages Top-k sparsification and quantization to reach extreme compression ratios of up to 1-3% sparsity and 2-bit quantization while outperforming full-precision DiLoCo. Our key observations are that outer momentum can be locally approximated by an error feedback combined with aggressive sparsity and that sparse aggregation can actually improve model performance. We empirically demonstrate in a range of communication-constrained LLM training settings that SparseLoCo provides significant benefits in both performance and communication cost.
    

    
    
              
          Comments:
          15 pages, 9 tables, 2 figures
        

          Subjects:
          
            Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.15706 [cs.LG]
        
        
           
          (or 
              arXiv:2508.15706v1 [cs.LG] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.15706
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Amirmohammad Sarfi [view email]          [v1]
        Thu, 21 Aug 2025 16:48:19 UTC (85 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Thoughts on (Amazonian) leadership]]></title>
            <link>https://www.daemonology.net/blog/2025-09-01-Thoughts-on-Amazonian-Leadership.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095545</guid>
            <description><![CDATA[Amazon's
Leadership
Principles are famous, not just within Amazon but also in the tech
world at large.  While they're frequently mocked — including by
Amazonians — they're also generally sensible rules by which to run
a company.  I've been an Amazon customer for over 25 years and an AWS
customer for almost 20 years, and also an
AWS Hero
for 6 years, and while I've never worked for Amazon I feel that I've seen
behind the curtain enough to offer some commentary on a few of these
principles.]]></description>
            <content:encoded><![CDATA[

Amazon's
Leadership
Principles are famous, not just within Amazon but also in the tech
world at large.  While they're frequently mocked — including by
Amazonians — they're also generally sensible rules by which to run
a company.  I've been an Amazon customer for over 25 years and an AWS
customer for almost 20 years, and also an
AWS Hero
for 6 years, and while I've never worked for Amazon I feel that I've seen
behind the curtain enough to offer some commentary on a few of these
principles.

Customer Obsession: Leaders start with the customer and work backwards.
They work vigorously to earn and keep customer trust. Although leaders pay
attention to competitors, they obsess over customers.

Customer Obsession is great, but I often see Amazonians taking this too
simplistically: "Start with the customer" doesn't have to mean "ask customers
what they want and then give them faster horses".  In the early days of AWS I
saw a lot of what I call "cool engineering driven" products: When EC2
launched, it wasn't really clear what people would do with it, but it was
very cool and it was clear that it could be a big deal in some form, sooner
or later.  Some time around 2012, the culture in AWS seemed to shift from
"provide cool building blocks" to "build what customers are asking for" and
in my view this was a step in the wrong direction (mind you, not nearly as
much as the ca. 2020 shift to "build what analysts are asking for in quarterly
earnings calls").

This tension of what customers are asking for vs what customers really need
shows up in areas like resilience.  Amazon's "Well-Architected Framework"
strongly exhorts customers to avoid building production workloads in a single
Availability Zone — but Amazon's cross-AZ bandwidth pricing is painful,
and Amazon doesn't provide useful tools for building durable multi-AZ
applications.  Most customers are not going to implement Paxos, and very few
customers — certainly not executives who are removed from actual
development processes — are going to ask Amazon for Paxos-as-a-service;
but if Amazonians sat down and asked themselves "what do customers need in
order to design their applications well" they could probably come up with
several services which Amazon already has internally.  AWS should
return to its roots and release important building blocks — the things
customers will need, not necessarily what they're asking for.

Ownership: Leaders are owners. They think long term and don't sacrifice
long-term value for short-term results. They act on behalf of the entire
company, beyond just their own team. They never say "that's not my job."

This principle is both too narrow, and not being fulfilled, in my view.  It's
not enough to simply act on behalf of the entire company: It's important to
act on behalf of the entire technological ecosystem.  Some Amazonians are great
at this — I recently commited patches to FreeBSD's bhyve because an
Amazonian was putting together a standard for interrupt handling in large VMs,
and even though Amazon doesn't make any use of bhyve (at least, I don't think
it does!)  he understood the importance of getting standards widely accepted
across the entire virtualization space rather than narrowly in the code Amazon
relied upon.  There's a saying in computer security, that anything which makes
one of us less secure makes all of us less secure: Attackers will leverage an
exploit against one system to allow them to attack another system.  While the
same does not directly apply in other fields, working with others to
produce the best results for everyone will be much better in the long-term
than focusing solely on what Amazon needs right now.

But in general Amazon doesn't even live up to its stated (narrow) promise of
having leaders acting on behalf of the entire company — it's simply too
siloed.  Amazon is famously secretive, and this applies internally as well as
externally: When AWS launches two similar services, it's often because two
teams didn't know what each other was working on.  How can leaders act across
the entire company if nobody knows what's happening outside of their team?
They can't; and if Amazon wants to allow its best people to be true Owners,
Amazon needs to start breaking down walls.

Bias for Action: Speed matters in business. Many decisions and actions
are reversible and do not need extensive study. We value calculated risk
taking.

Amazonians talk about "one-way doors" and "two-way doors", and it is quite
true that many decisions are can be reversed... but that doesn't always
mean that there is no cost associated with reversing a decision.  There
is a clear and widely recognized tension between "Bias for Action" and another
principle, "Insist on the Highest Standards"; but there is also a tension
between this and earning and keeping customer trust.  When AWS ships a service
which is half-baked, it diminishes customer trust in AWS as a whole; even if
the problems in that service ultimately get corrected (either by fixing them
or in some cases by simply getting rid of a service which should never have
existed in the first place) the memory of a failed launch will live on in
customers' minds for years to come.

During my seven-year tenure as FreeBSD Security Officer, people knew me as
the guy sending out security advisories; but the most important thing I did
was not to ship Security Advisories — that is, it was to stop the
train and say "no, we are not going to send this out yet".  I knew that for all
the importance of getting patches into people's hands in a timely manner, it
was even more important to establish trust: If I gave people a broken patch, even
once, they would be much slower to install security updates in the future.  My
team became familiar with the phrase "convince me that this is correct", and
I'd like to see more of that at senior levels of Amazon: Principal and
Distinguished Engineers need to step in with a bias for inaction, and
use the respect they have earned to stop projects which do not meet the
highest standards before they undermine trust.  Amazon's hiring process
famously includes "bar raisers" who can veto hiring decisions; they should
also have service bar raisers who can veto launches.

Werner Vogels famously said in his 2024 re:Invent keynote, "Listen to the AWS
Heroes".  I think he was talking about technical advice, and perhaps speaking
mainly to AWS customers; but I like to think that Amazon might also benefit
listening to some of what I've said here.  We criticize because we care. 




blog comments powered by Disqus
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The future of 32-bit support in the kernel]]></title>
            <link>https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095475</guid>
            <description><![CDATA[Arnd Bergmann started his Open Source Summit Europe 2025 talk with a clear statement of positio [...]]]></description>
            <content:encoded><![CDATA[

Arnd Bergmann started his Open
Source Summit Europe 2025 talk with a clear statement of position: 32-bit
systems are obsolete when it comes to use in any sort of new products.  The
only reason to work with them at this point is when there is existing
hardware and software to support.  Since Bergmann is the overall maintainer
for architecture support in the kernel, he is frequently asked whether
32-bit support can be removed.  So, he concluded, the time has come to talk
more about that possibility.

Nobody covers the Linux kernel like LWN; be in the know with
a one-month trial subscription, no credit card needed.


People naturally think about desktop machines first, he continued.  If you
were running Linux in the 1990s, you had a 32-bit, desktop system.  Unix
systems, though, moved to 64-bit platforms around 30 years ago, and
the Linux desktop made that move about 20 years ago.  Even phones and
related devices have been 64-bit for the last decade.  If those systems
were all that Linux had to support, 32-bit support would have long since
been removed from the kernel.  He summarized the situation with this slide,
showing how the non-embedded architectures have transitioned to either
64-bit or nonexistence over time:




The world is not all desktops — or servers — though; embedded Linux exists
as well.  About 90% of those systems are running on Arm processors.  The
kernel has accumulated a lot of devicetree files describing those systems
over the years; only in this last year has the number of devicetrees for
armv8 (64-bit) systems exceeded the number for armv7 (32-bit) systems.

For Arm processors with pre-armv7 architectures, there are only three for
which it is still possible to buy hardware, but a number are still
supported by the kernel community:






Many other pre-armv7 CPUs are out of production,
but the kernel still has support for them.  Of those, he said, there are
about ten that could be removed now.  It would be nice to be able to say
that support for the others will be removed after a fixed period, ten years
perhaps, but hardware support does not work that way.  Instead, one has to
think in terms of half lives; every so often, it becomes possible to remove
support for half of the platforms.  It all depends on whether there are
users for the processors in question.

The kernel is still adding support for some 32-bit boards, he said, but at
least ten new 64-bit boards gain support for each 32-bit one.

There are a number of non-Arm 32-bit architectures that still have support
in the kernel; these include arc, microblaze, nios2, openrisc, rv32,
sparc/leon, and xtensa.  All of them are being replaced by RISC-V
processors in new products.  RISC-V is what you use if you don't care about
Arm compatibility, he said.

Then, there is the dusty corner where nommu (processors without a
memory-management unit) live; these include armv7-m, m68k, superh, and
xtensa.  Nobody is building anything with this kind of hardware now, and
the only people who are working on them in any way are those who have to
support existing systems.  "Or to prove that it can be done."

There are still some people who need to run 32-bit applications that cannot
be updated; the solution he has been pushing people toward is to run a
32-bit user space on a 64-bit kernel.  This is a good solution for
memory-constrained systems; switching to 32-bit halves the memory usage of
the system.  Since, on most systems, almost all memory is used by user
space, running a 64-bit kernel has a relatively small cost.  Please, he
asked, do not run 32-bit kernels on 64-bit processors.




There are some definite pain points that come with maintaining 32-bit
support; most of the complaints, he said, come from developers in the
memory-management subsystem.  The biggest problem there is the need to
support high memory; it is complex, and requires support throughout the
kernel.  High memory is needed when the kernel lacks the address space to
map all of the installed physical memory; that tends to be at about 800MB
on 32-bit systems. (See this article for
more information about high memory).

Currently the kernel is able to support 32-bit systems with up to 16GB of
installed memory.  Such systems are exceedingly rare, though, and support
for them will be going away soon.  There are a few 4GB systems out there,
including some Chromebooks.  Systems with 2GB are a bit more common.  Even
these systems, he said, are "a bit silly" since the memory costs
more than the CPU does.  There are some use cases for such systems, though.
Most 32-bit systems now have less than 1GB of installed memory.  The
kernel, soon, will not try to support systems with more than 4GB.

There are some ideas out there for how to support the larger-memory 32-bit
systems without needing the high-memory abstraction.  Linus Walleij is
working on entirely separating the kernel and user-space address spaces,
giving each 4GB to work with; this is a variant on the "4G/4G" approach
that has occasionally been tried for many years.  It is difficult to make
such a system work efficiently, so this effort may never succeed, Bergmann
said.

Another approach is the proposed "densemem" memory model, which does some
fancy remapping to close holes in the physical address space.  Densemem can
support up to 2GB and is needed to replace the
SPARSEMEM memory model, the removal of which which will eventually be
necessary in any case.  This work has to be completed before high memory
can be removed; Bergmann said that he would be interested in hearing from
potential users of the densemem approach.

One other possibility is to drop high memory, but allow the extra physical
memory to be used as a zram swap
device.  That would not be as efficient as accessing the memory directly,
but it is relatively simple and would make it possible to drop the
complexity of high memory.

Then, there is the year-2038 problem, which
he spent several years working on.  The kernel-side work was finished in
2020; the musl C library was updated that same year, and the GNU C
Library followed the year after.  Some distributors have been faster than
others to incorporate this work; Debian and Ubuntu have only become
year-2038-safe this year.

The year-2038 problem is not yet completely solved, though; there are a lot
of packages that have unfixed bugs in this area.  Anything using futex(),
he said, has about a 50% chance of getting time handling right.  The legacy
32-bit system calls, which are not year-2038 safe, are still enabled in the
kernel, but they will go away at some point, exposing more bugs.  There are
languages, including Python and Rust, that have a lot of broken language
bindings.  Overall, he said, he does not expect any 32-bit desktop system to
survive the year-2038 apocalypse.

A related problem is big-endian support, which is also 32-bit only, and
also obsolete.  Its removal is blocked because IBM is still supporting
big-endian mainframe and PowerPC systems; as long as that support
continues, big-endian support will stay in the kernel.

A number of other types of support are under discussion.  There were once
32-bit systems with more than eight CPUs, but nobody is using those
machines anymore, so support could be removed.  Support for armv4
processors, such as the DEC StrongARM CPU,
should be removed.  Support for early armv6 CPUs, including the omap2 and
i.mx31, "complicates everything"; he would like to remove it, even
though there are still some Nokia
770 systems in the wild.  The time is coming for the removal of all
non-devicetree board files.  Removal of support for Cortex M CPUs,
which are nommu systems, is coming in a couple of years.  Developers are
eyeing i486 CPU support, but that will not come out yet.  Bergmann has sent
patches to remove support for KVM on 32-bit CPUs, but there is still
"one PowerPC user", so that support will be kept for now.

To summarize, he said, the kernel will have to retain support for armv7
systems for at least another ten years.  Boards are still being produced
with these CPUs, so even ten years may be optimistic for removal.
Everything else, he said, will probably fade away sooner than that.  The
removal of high-memory support has been penciled in for sometime around
2027, and nommu support around 2028.  There will, naturally, need to be
more discussion before these removals can happen.

An audience member asked how developers know whether a processor is still
in use or not; Bergmann acknowledged that it can be a hard question.  For
x86 support, he looked at a lot of old web pages to make a list of which
systems existed, then showed that each of those systems was already broken
in current kernels for other reasons; the lack of complaints showed that
there were no users.  For others, it is necessary to dig through the Git
history, see what kinds of changes are being made, and ask the developers
who have worked on the code; they are the ones who will know who is using
that support.

Another person asked about whether the kernel would support big-endian
RISC-V systems.  Bergmann answered that those systems are not supported
now, and he hoped that it would stay that way.  "With RISC-V, anybody
can do anything, so they do, but it is not always a good idea".  The
final question was about support for nommu esp32 CPUs; he answered that
patches for those CPUs exist, but have not been sent upstream.  Those
processors are "a cool toy", but he does not see any practical
application for them.

The slides
for this talk are available.  The curious may also want to look at Bergmann's 2020 take on this topic.


[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           KernelArchitectures
            ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Implementing a Foil Sticker Effect]]></title>
            <link>https://www.4rknova.com/blog/2025/08/30/foil-sticker</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095460</guid>
            <description><![CDATA[A breakdown of how to build a custom Three.js shader that recreates the iridescent, sparkling look of foil stickers using vertex deformation, angle-based color shifts, and procedural flakes.]]></description>
            <content:encoded><![CDATA[In this post, I’ll walk you through how to create a custom shader in Three.js that simulates the look of a foil sticker, complete with angle-dependent iridescence and sparkling metallic flakes. The goal is to capture that premium, holographic effect you see on collectible stickers, trading cards, and high-end packaging, but to render it in real time directly in the browser.IridescenceIf you’ve ever tilted a holographic sticker or watched sunlight catch on a soap bubble, you’ve seen iridescence in action. In the real world, this rainbow shimmer comes from thin-film interference. When light waves bounce between layers of a surface, some wavelengths are reinforced while others cancel out, causing colors to shift depending on your viewing angle.In real-time computer graphics, we don’t need to simulate the exact physics. Instead, we can approximate this by mapping view angle to hue, as the surface tilts relative to the camera, its color smoothly shifts through a spectrum. This gives that dynamic, “alive” quality you expect from foil stickers.Foil FlakesAlongside the shifting colors, there’s another key detail: foil flakes. Real metallic foils have tiny reflective particles embedded in them, creating hundreds of bright, sharp highlights that twinkle as you move. These aren’t smooth reflections but randomized sparkles, giving the surface its tactile, premium feel.To replicate this in a shader, we’ll introduce procedural noise to generate small random patches of brightness across the surface. When combined with lighting, they look like metallic specks catching the light. Together, angular hue shifts and flake sparkles create a convincing illusion of printed holographic foil without expensive rendering tricks.ImplementationThis implementation simulates a peeling, iridescent sticker with foil flakes using Three.js. While I will borrow concepts such as metalness, roughness, and Fresnel from Physically Based Rendering (PBR), this shader is not physically based. The goal is to create a visually plausible, artistic effect.Below is a live demo of the shader, where you can modify its parameters and experiment with different configurations. Use your mouse to rotate the sticker around and see how the material reacts to the lighting.Vertex ShaderThe vertex shader handles the peel geometry and passes useful information to the fragment shader.Uniform / VaryingTypePurposeuPeelAmountfloatOverall peel strength (0 = flat, 1 = fully peeled).uPeelAnglefloatPeel direction in degrees.vUvvec2UV coordinates for texture mapping.vWorldPosvec3Vertex position in world space.vNormalvec3Transformed normal for lighting.vAOIntensityfloatDistance moved by vertex, used to darken lifted areas.The shader goes through the following simple steps:Compute vector from hinge to current vertex.Calculate the peel factor and angle.Define the rotation axis and apply Rodrigues’ rotation formula to rotate the vertex around that axis.Apply the same rotation to the normal.Calculate a fake ambient occlusion term.Here’s the full vertex shader code:uniform float uPeelAmount;  // Strength of peel (0.0 → no peel, 1.0 → full peel)
uniform float uPeelAngle;   // Peel angle in degrees (converted to radians in shader)
varying vec2  vUv;          // UV coordinates
varying vec3  vWorldPos;    // Vertex position in world space
varying vec3  vNormal;      // Transformed vertex normal
varying float vAOIntensity; // Ambient occlusion or peel intensity factor

void main() {
    vUv = vec2(uv.x, 1.0 - uv.y);
    vec3 pos = position;

    // Define hinge point for peel
    vec3 hinge = vec3(0.0, 0.0, 0.0);

    // Vector from hinge to current vertex
    vec3 toVertex = pos - hinge;

    // Peel factor calculation
    // Interpolates peel strength diagonally
    // (bottom-left → top-right)
    float peelFactor = (uv.x + uv.y) * 0.5;

    // Convert peel angle to radians
    // Final angle is scaled by peelAmount
    // and per-vertex peelFactor
    float radAngle = radians(uPeelAngle);
    float angle = radAngle * uPeelAmount * peelFactor;

    // Define rotation axis for peel
    // Diagonal axis pointing from top-left 
    // to bottom-right
    vec3 axis = normalize(vec3(-1.0, 1.0, 0.0));
    float cosA = cos(angle);
    float sinA = sin(angle);

    // Apply Rodrigues' rotation formula
    // Rotates the vertex around the diagonal axis
    vec3 rotated = toVertex * cosA +
                   cross(axis, toVertex) * sinA +
                   axis * dot(axis, toVertex) * (1.0 - cosA);

    // Update vertex position after rotation
    pos = hinge + rotated;

    // Rotate vertex normal the same way to
    // ensure lighting matches the peeled
    // geometry
    vec3 rotatedNormal = normal * cosA +
                         cross(axis, normal) * sinA +
                         axis * dot(axis, normal) * (1.0 - cosA);

    // Transform normal into view space
    vNormal = normalize(normalMatrix * rotatedNormal);

    // Transform vertex to world space
    vec4 worldPos = modelMatrix * vec4(pos, 1.0);
    vWorldPos = worldPos.xyz;

    // Ambient Occlusion term based on distance moved
    // from original vertex position
    vAOIntensity = length(toVertex - rotated);

    // Final projection
    gl_Position = projectionMatrix * viewMatrix * worldPos;
}
Fragment ShaderThe fragment shader handles all lighting, reflections, iridescence, and foil flakes. It layers procedural effects to create a rich, dynamic look.UniformTypePurposemapsampler2DSticker albedo + alpha.envMap2Dsampler2DEnvironment map for reflections.uCameraPosvec3Camera position for view vector.uAlphaCutofffloatDiscard pixels below this alpha.uFlakesEnabledfloatToggle foil flakes.uFlakeSizefloatSize of flakes.uFlakeReductionfloatRandomness threshold for flakes.uFlakeThresholdfloatBrightness threshold to show flakes.uFlakeBrightnessfloatBase brightness of flakes.uMetalnessfloatPBR-like metal reflectivity control.uRoughnessfloatControls reflection sharpness.uEnvIntensityfloatScales environment contribution.uMetalmaskfloatMask controlling metallic regions.uIridescencefloatStrength of angle-dependent rainbow effect.uIriMin, uIriRangefloatRange for simulated film thickness.uPeelAmount, uPeelAnglefloatPeel geometry info for shading.This is how this works:Alpha cutoff to discard transparent pixels early.Back-face shading to render the rear surface as plain white or darkened, depending on peel.Foil flakes are computed using procedural noise. Normals are perturbed slightly to create sparkle variation. The environment map is sampled to get an iridescent tint.Iridescence (thin-film approximation) is calculated using sine-based waves to shift hue by view angle.Environment reflections are modulated by Fresnel.Final shading combines diffuse base, reflections, iridescence, and flakes.Here’s the full vertex shader code:precision highp float;

#define PI  3.14159265

varying vec2 vUv;
varying vec3 vNormal;
varying vec3 vWorldPos;
varying float vAOIntensity;

uniform sampler2D map;      // sticker albedo + alpha
uniform sampler2D envMap2D; // LDR equirectangular environment

uniform vec3  uCameraPos;
uniform float uAlphaCutoff;
uniform float uMaxMip;
uniform float uFlakesEnabled;
uniform float uFlakeSize;
uniform float uFlakeReduction;
uniform float uFlakeThreshold;
uniform float uFlakeBrightness;
uniform float uPeelAmount;
uniform float uPeelAngle;
uniform float uMetalness;
uniform float uRoughness;
uniform float uEnvIntensity;
uniform float uMetalmask;
uniform float uIridescence;
uniform float uIriMin;
uniform float uIriRange;

float hash(vec2 p) {
    return fract(sin(dot(p, vec2(127.1, 311.7))) * 43758.5453123);
}

// Map 3D dir to 2D equirect UV
vec2 dirToEquirectUv(vec3 dir) {
    dir = normalize(dir);
    float phi = atan(dir.z, dir.x);
    float theta = acos(clamp(dir.y, -1.0, 1.0));
    return vec2((phi + 3.14159265) / (2.0 * 3.14159265), theta / 3.14159265);
}

vec3 sampleEnvRough(vec3 R, float roughness) {
    vec2 uv = dirToEquirectUv(R);

    // Map roughness to LOD level
    float lod = roughness * uMaxMip;
    vec3 color = texture2DLodEXT(envMap2D, uv, lod).rgb;

    return color;
}

// Iridescence / thin-film color
vec3 iridescenceColor(float cosTheta) {
    float thickness = uIriMin + uIriRange * (1.0 - cosTheta);
    float phase = 6.28318 * thickness * 0.01; // scaled for visuals
    vec3 rainbow = 0.5 + 0.5 * vec3(sin(phase), sin(phase + 2.094), sin(phase + 4.188));
    return mix(vec3(1.0), rainbow, uIridescence);
}

// Convert RGB to perceived luminance (Rec.709)
float luminance(vec3 color) {
    return dot(color, vec3(0.2126, 0.7152, 0.0722));
}

void main() {

    vec4 base = texture2D(map, vUv);
    if(base.a < uAlphaCutoff)
        discard;

    if(!gl_FrontFacing) {
        float col = 1.0;
        if(uPeelAngle > 0.0) {
            col = mix(1.0, 0.2, vAOIntensity);
        }
        // Render back side as white
        gl_FragColor = vec4(vec3(col), base.a);
        return;
    }

    vec3 N = normalize(vNormal);
    vec3 V = normalize(uCameraPos - vWorldPos);
    vec3 R = reflect(-V, N);

    // Ambient occlusion / peel shadow
    float peelShadow = 0.0;

    if(uPeelAngle < 0.0) {
        peelShadow = smoothstep(0.0, 0.3, vAOIntensity);
        base.rgb *= mix(1.0, 0.3, peelShadow);
    }

    // Flakes
    float flakeIntensity = 0.0;
    vec3 flakeEnv = vec3(0.0);

    float brightness = luminance(base.rgb);

    if(uFlakesEnabled > 0.5) {
        // Procedural flake mask
        float flake = hash(floor(vUv * uFlakeSize));
        float flakeMask = smoothstep(uFlakeReduction, 1.0, flake);

        // Base brightness influence
        float flakeBoost = smoothstep(uFlakeThreshold, 1.0, brightness);

        // Perturbed flake normal
        float angleOffset = (hash(vec2(flake, flake + 3.0)) - 0.5) * 0.25;
        vec3 perturbedNormal = normalize(N + vec3(angleOffset, 0.0, angleOffset));

        // Reflection for sparkle
        vec3 PR = reflect(-V, perturbedNormal);

        // Dynamic flicker factor (only brightens, never darkens)
        float flakePhase = hash(floor(vUv * uFlakeSize) + floor(PR.xy * 15.0));
        float phaseMod = mix(1.0, 1.8, flakePhase);
        
        // Core sparkle factor (glimmer preserved)
        float flakeSpec = pow(clamp(dot(perturbedNormal, V) * 0.5 + 0.5, 0.0, 1.0), 8.0);
        flakeSpec = max(flakeSpec, 0.15); // always visible

        // Environment tint (never too dark, controlled by uniform)
        float flakeRough = clamp(uRoughness * 0.4, 0.0, 1.0);
        flakeEnv = sampleEnvRough(PR, flakeRough) * mix(0.9, 1.2, brightness);
        flakeEnv = max(flakeEnv, vec3(uFlakeBrightness));

        vec3 flakeIri = iridescenceColor(dot(perturbedNormal, V));
        flakeEnv *= mix(vec3(1.0), flakeIri, 0.9);

        // Final intensity
        flakeIntensity = flakeMask * flakeBoost * flakeSpec * phaseMod * 18.0;
        flakeIntensity = clamp(flakeIntensity, 0.0, 1.0);
    }

    // Final roughness modulation
    float finalRough = clamp(mix(uRoughness, 1.0, flakeIntensity), 0.0, 1.0);

    // Environment reflection
    vec3 env = sampleEnvRough(R, finalRough) * uEnvIntensity;

    // Blend in flake environment contribution
    env = mix(env, flakeEnv, clamp(flakeIntensity, 0.0, 1.0));

    // Fresnel term
    float cosTheta = clamp(dot(N, V), 0.0, 1.0);
    float F0 = mix(0.04, 1.0, uMetalness);
    float fres = F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);

    // Iridescence
    float metalicMask = mix(uMetalmask, 1.0, brightness);
    vec3 iriCol = iridescenceColor(cosTheta) * metalicMask;

    // Final color
    vec3 diffuse = base.rgb * (1.0 - uMetalness);
    vec3 spec = env * fres * iriCol * (1.0 - finalRough * 0.85);
    vec3 color = diffuse + spec;

    gl_FragColor = vec4(color, base.a);
}
LicensingThe code in this page is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). Feel free to share and adapt the code for non-commercial purposes with proper attribution. If you wish to use the code commercially, please contact me for a separate license agreement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MAGA declares war on the property tax]]></title>
            <link>https://www.urbanproxima.com/p/maga-declares-war-on-the-property</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095240</guid>
            <description><![CDATA[Republican politicians declare war on the property tax, following in the footsteps of that famous bastion of American conservatism - California]]></description>
            <content:encoded><![CDATA[In a recent Twitter post, Representative Marjorie Taylor Greene called for an end to property taxes. Meanwhile in Florida, Governor Ron DeSantis has been advocating for the elimination of property taxes in his state. Vivek Ramaswamy (remember him?) called for the same during a campaign speech in Ohio.So, MAGA is all-in on nixing the property tax and, in an American context, this makes a lot of political sense. From a policy perspective, it’s one of the worst ideas imaginable. And, ironically, Republicans need look no further than California to see exactly why.The United States is a nation of homeowners. This is the result of a concerted effort on the part of the federal government to subsidize homeownership following World War II. Easy credit and car-centric infrastructure changed where (as well as how) Americans live and gave rise to the modern-day U.S. suburb.1Once Americans became homeowners, we became increasingly sensitive to property taxes, for several reasons.Historically, homeownership was pitched as The Path to middle-class prosperity.2  The idea was that, after paying off the subsidized loan used to buy a house, homeowners could expect their property to continually appreciate in value, and use those gains to fund their retirement. Property taxes are thus a tax on what Americans have been told for generations is supposed to be their main vehicle for savings.The problem with this is that suburban homeowners are overwhelmingly owner-occupiers. The houses they live in—though they might increase in value in a hypothetical, speculative sense—don’t generate a stream of revenue that can be used to pay off taxes. If the value of a property goes through the roof (no pun intended), an owner’s ability to pay their taxes doesn’t necessarily increase with it.And on top of all that, the property tax is what we’d call high salience. It’s a large, once a year bill that feels jarring and impactful in a way that the slow bleed of monthly income taxes withheld from an employer's paycheck do not.So, Americans hate property taxes. And when enough Americans hate the same thing all at the same time, we tend to revolt.In 1978, an angry old man named Howard Jarvis succeeded in changing California’s property tax system in a truly profound way. He successfully floated a statewide ballot initiative to limit property taxes: the infamous Proposition 13.3 The proposition’s purpose was to limit California’s ability to collect property taxes which it accomplished by capping them at 1% of assessed value at the time of purchase.4 For tax purposes, reassessment would only occur if the property changed ownership.5 This means that if you bought a house in San Francisco in 1978, you’re basically paying 1% on the inflation-adjusted value of what the property was worth when Jimmy Carter was President.Howard Jarvis in "The Last Angry Man." Credit: Bread and Butter FilmsProp 13’s immediate impact was to reduce the state’s budget, precipitating spending cuts that have hampered public education ever since. More broadly, this has forced the state to rely on income and capital gains taxes to fund public expenditure. Not only is this bad for overall economic growth, it also makes the state budget super sensitive to the boom and bust cycle of the stock market. When the stock market tanks, California goes broke.At the local level, this has also encouraged the growth of fee culture wherein municipalities desperately nickel-and-dime things like new housing — not because it’s good policy, but because it’s a legally and politically feasible way to raise revenue.More subtly, Prop 13 has also acted as a multiplier on California’s baseline level of NIMBYism. No matter how expensive housing becomes, anti-housing homeowners remain completely insulated from runaway prices, even as they block new construction.To give the devil a fair shake, reasonable people can have reasonable complaints about property taxes. If the local economy booms, and property values boom with it, retirees on fixed incomes may have a hard time keeping up with their taxes. That’s a problem with a solution, though. As the name suggests, a property tax deferral program allows seniors to defer paying their property taxes until death. At that point, accumulated taxes are paid by the estate (which can presumably cover the bill by selling the property).On a more emotional level, property taxes often evoke a sense of unfairness. As Governor DeSantis put it, “Property taxes effectively require homeowners to pay rent to the government”. The unfortunate reality is that there’s an entire constellation of municipal services and infrastructure required for even suburban development. These things cost money and the best way to raise that money is to charge the people who benefit from the value said infrastructure provides.If MAGA lawmakers insist on following in California’s footsteps, they’ll end up in at least as bad a place...though I’d wager they’ll manage to land somewhere even worse.Despite the fact that getting rid of property taxes is Not a Good Idea, that doesn’t mean that existing property tax regimes are necessarily optimal. Property taxes, after all, punish development because they factor in the value of the house or whatever other structure happens to be built on top.6 Instead of abolishing property taxes, shifting to land value taxation (LVT) would constitute an actual improvement.In brief, an LVT works like a property tax, but only applies to the value of a parcel’s land (i.e. it doesn’t tax the value of any buildings or infrastructure on top). So, if you own a $350,000 property where the land component is worth $50,000 (and the building is valued at $300,000) an LVT would only tax you based on the $50,000 value of your land. Land value taxes encourage development of valuable land by making it expensive to keep vacantAll else equal, a vacant lot and a parcel with a building get the same treatment. In an area where land has become valuable, it incentivizes denser development and disincentivizes keeping expensive land out of use.Importantly, an LVT can be structured such that it’s somewhere between a tax break and net neutral for homeowners.As an example,  recently did the math on his own single-family home in Texas. Going from a 1.7% property tax to a much higher (in percentage terms) LVT constituted a break in his bill. Lars calculated that:Even with a massive 6% LVT rate, I still save about two thousand bucks, as would other median Texas homeowners like me.Anything that constitutes a tax break will be popular with homeowners, but it’s important to remember that this would probably increase tax receipts overall. In this hypothetical, vacant lots and underutilized surface parking start to pay much higher taxes and probably get put to better use over time — that means more housing and commercial space, and, consequently, more economic activity. On top of that, existing homeowners would no longer be financially punished for building an ADU or otherwise improving their property. The more you develop a parcel, the higher property taxes becomeObviously, all that assumes local land use rules allow that kind of redevelopment, but that’s a different work stream and the underlying logic still stands.Whether the proponents of property tax abolition would ever embrace such a policy…well, I’m not holding my breath.On one level, we’re not exactly dealing with the greatest policy minds to ever hold sway in the American Republic. On a deeper level, the individuals in question are simply not selling a vision of widespread prosperity. Gone is even the pretense of a rising tide that lifts all ships and in its place is a politics of grievance and privilege. Downstream of that, all policy looks like rent seeking. We’ve already seen where that road leads in California: broken budgets and a dysfunctional political-economy. If MAGA remains intent on walking the same path, it’ll succeed only in hamstringing state and local budgets, all while mortgaging the country’s future for nothing more than a few extra points in polling. Share1Note that subsidies weren’t shared out equally. American minorities not only didn’t have access to the cheap loans that vaulted other folks into the landowning class, they faced prejudicial terms in what credit they were able to access along with a host of legal barriers to acquiring housing even in situations where money was not an issue. 2Arguably, the American penchant for land ownership goes back to the Jeffersonian conceptions of the yeoman citizen. Later in the country’s history, government organs like the Department of Labor launched pro-home ownership campaigns as part of the government’s anti-communism efforts. This continued after WWII with the creation of quasi-government agencies Fannie Mae and Freddie Mac, the institutions that buy mortgages from originating banks, thereby ensuring a constant supply of credit. 3In California, after gathering enough signatures, anyone can propose a new law which the entire state then votes on in a general plebiscite.4There are some additional details in the original ballot text plus several years of follow-on legislation to deal with some of the knock-on effects of prop 13, so, like most things, it’s slightly more complicated.5The definition of “ownership” does a lot of work here. Savvy property owners put properties under the ownership of transferable trusts. When an owner wants to sell a property, they just transfer control of the trust. This way, the ownership of the property never changes — it’s still under the ownership of the trust — so reassessment never occurs.6And if you take the Strong Towns growth Ponzi Scheme thesis seriously, cities and states may be grossly undercharging property owners for the value of the infrastructure they provide anyway...but that’s a whole separate can of worms.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Territorial Markings as a Predictor of Driver Aggression and Road Rage (2008)]]></title>
            <link>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1559-1816.2008.00364.x?prevSearch=allfield%3A%28szlemko%29</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45095079</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: woomarks, transfer your Pocket links to this app or self-host it]]></title>
            <link>https://woomarks.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094936</guid>
            <description><![CDATA[woomarks
        FAQ]]></description>
            <content:encoded><![CDATA[
    
        woomarks
        FAQ
      

    

    
    Made with woomarks.

    
      
        Title
          
        

        URL
          
        

        Tags (comma-separated)
          
        

        

        
          
            Bulk Transfer
          
          
        
      
    
  
  
  

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[One of Britain's largest stocks of second-hand books ever amassed]]></title>
            <link>https://www.worldofinteriors.com/story/richard-axe-second-hand-books-yorkshire</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094692</guid>
            <description><![CDATA[Bookseller Richard Axe has amassed 150,000 second-hand books and housed them in a vast youth hostel in the Yorkshire Dales. Now a buyer is sought]]></description>
            <content:encoded><![CDATA[Richard Axe is celebrated in the bookselling trade for owning one of the largest collections of antiquarian and second-hand volumes ever assembled. Since 2005, his stock has been arrayed in a huge former youth hostel in the Yorkshire Dales, but now his career is approaching its coda30 August 2025An American-themed section faces the viewer in a room devoted to Art and DesignOne of Britain’s largest stocks of second-hand books ever amassed can be found in the unlikeliest of locations: a vast former youth hostel in a pretty corner of the Yorkshire Dales. Meticulously sorted into subject areas, from naval history to 19th-century literature, architecture to zoology, over 150,000 volumes fill some 25 high-ceilinged rooms spread over four floors. To withstand the sheer weight of all those hardbacks, the building, which began life as a prep school in c1878, must surely be as strong as a Romanesque church.Certainly the collection has been assembled with an almost religious zeal by sole trader Richard Axe, a spry 70-something who spoke to me from the Philippines, where he lives with his wife roughly half the year. Unlike the more commercially oriented of his peers, he has sold books primarily so that he could acquire more for himself. Of the Harrogate shop he owned prior to his move here he says: ‘Its main purpose was not to sell at all, but rather to buy and increase my buying profile.’ I had fondly imagined that Richard chose Aysgarth in order to lure customers from the nearby falls, a daytrippers’ magnet. But in fact, he says he’s never advertised, nor had more than four visitors per week, and all of them were by appointment. Yet very rarely did a week pass by without him shifting at least £1,000 worth of books.Stacked from floor to ceiling with second-hand books, the 25-room former youth hostel in Aysgarth, North Yorkshire, has been occupied by bookseller Richard Axe for two decades
So how did his business, which has been highly successful, operate? Most professionals in the field swim in the waters of either ‘good quality but relatively ordinary second-hand books’ or ‘very specialist, expensive antiquarian books’; Richard has ended up being a big fish in both streams. In his heyday, he would reckon to drive some 25,000 miles a year, attending auctions from Plymouth to Glasgow. There he’d bid for large lots, sometimes whole libraries, subsequently selling a handful of important titles to private customers and international dealers, while creaming off books for his own collection. Hence, in part, the appeal of North Yorkshire. Far from being ‘the back of beyond’, the A1, M1 and East Coast mainline railway offer superb connections. ‘I like the idea of being in the middle of the country.’The custom-built wooden shelves stretch to a combined length of over a mile
Art and design books, recently purchased, await shelving. Prominent in the pile is a book by the late artist and postcard aficionado Tom Phillips, whose house was featured in WoI Sept 2025
An armchair with access to plenty of light creates a reading nook between two stacks
For the eagle-eyed expert, the condition of a book is critical. Richard gives the example of Dickens – ‘incredibly popular in his own lifetime, and so a first edition in reasonably good nick might fetch £50. But a really fine copy could go for ten times that.’ As is the case with all antiques, the internet has had a polarising effect. In the past, you might have traipsed round bookshops for a lifetime not finding the missing piece in your authorial jigsaw, but often such editions previously thought rare can now be quite easily unearthed via online search engines, and so their value has slumped. ‘But what’s transpired is that things that are genuinely uncommon have shot up.’A run of spines from Collins’s ‘New Naturalist’ series offers a burst of colour in a top-floor room devoted to zoology, topography and horticulture
The classic case is JK Rowling’s Harry Potter and the Philosopher’s Stone. Given that it was the first in the sequence, Bloomsbury, with no inkling of the monster the brand would become, published the smallest viable number of copies – 500 – in its first edition. It is thus truly rare, says Richard, much more so than a Jane Austen equivalent. As a consequence: ‘A fine copy is worth, certainly, £50,000.’ Similarly, photograph albums and manuscripts, by definition one-off items, have seen skyrocketing interest in cyberspace – and among a new, younger breed of collector to boot. Indeed, the most expensive item my interlocutor has ever sold was not a book at all, but a scrapbook owned by Cecil Beaton, which included the photographer’s own drawing of Mick Jagger, a personal friend, and sketches by Jean Cocteau.A map of York reflects the rolling dales in a room devoted to leather-bound antiquarian books
Richard Axe has deployed every little sliver of real estate to house his 150,000-plus books. A staircase makes the ideal spot for shelving titles about mountaineering and hiking
When Richard moved to his property in 2005, consolidating a warehouse, shop and large house in Harrogate, little structural work was required to the sturdily built edifice apart from new guttering, though he had to strip out rows of urinals and a few municipal-style kitchens. Inevitably, however, two men were employed full time for a year just building wooden shelves, which now stretch to a combined length of over a mile. The original plan was to carve out some domestic space in this behemoth of a building, but Richard states that mixing residence and business had potential VAT implications. And then there was the small issue of his ever-encroaching library… So he moved into the two-bedroom modern cottage previously inhabited by the youth-hostel manager. Presumably you had a no-book policy here, I ask Richard. ‘Well, if I did, it didn’t last,’ he replies ruefully. ‘Let’s just say it was restricted.’This two-bedroom cottage, built in the 1980s, is where the youth-hostel manager used to live. Now Richard relaxes here after a hard day humping boxes, cataloguing and shelving
Philosophical follies, such as doorknobs surreally attached to tree trunks, dot the unusual sculpture garden that Richard has proudly created out of a capacious paddock that’s part of the estate. In the middle of the pond sits an inaccessible table and chairs overhung with a metal fruit basket you also cannot reach. ‘It’s all about, you know, the unattainable.’ This modern take on the Tantalus myth strikes me as the perfect symbol for the avid collector, feverishly hunting down the final piece of a set, mourning the rarities that slipped through one’s fingers. The psychological make-up of the type emerged early on. Even as a Dulwich College scholarship boy, ‘I collected stamps rather more avidly than most.’ Later, at Bristol University, ‘I bought books initially to read, but the physical possession of having quite a lot, of having a substantial range of bookshelves, became important as a manifestation of knowledge and understanding and culture.’ Even now, he can find himself transfixed when, say, a history of signposts tumbles into his lap.Richard’s modern version of the Tantalus myth is a table and chairs in the middle of a pond, overhung with an unreachable fruit bowl. In the background lies the Grade I-listed St Andrew’s church, known as the ‘Cathedral of the Dales’
The vast former hostel and its contents are now on the market. Richard is loath to break up his collection
The physical demands of his job, combined with declining health, mean Richard ‘must face the facts of age’. With great reluctance, he is now selling his estate and collection, lock, stock and barrel, for around £1.5 million. His greatest fear is that the land, library and buildings get sold off bit by bit, ‘my life’s work just disappearing’. He is even prepared to play the role of éminence grise to the putative lucky buyer, sharing his contacts and experience.A room devoted to British history. Richard employed two men full-time for a year to construct bookshelves
But what about the keepers? Surely, there are a few titles he’d like to hold on to? Well, there’s his extensive collection of antiquarian books on Yorkshire; those on another great love, football (‘though the Philippines is pretty much the only country in the world that doesn’t have an interest’); books on the fluctuatingly fashionable Ruskin, as well as 18th- and 19th-century folding maps that the chin-stroking aesthete might have consulted himself while hiking in the Lakes. Oh, and let’s not forget the collection of 1750s–1870s books in their original drab boards or cloth. ‘Cheaper to buy at the time, because they were unbound, they are now valuable because of their fragility.’ The bookseller’s eyes sparkle.To enquire about purchasing the property plus stock, ring Elaine Williams-Bird on 07798 818651 or email nellybirdpress@gmail.comIn your inbox: get our bi-weekly newsletter, featuring letters from the editor and exclusive featuresInterested in writing for The World of Interiors? Find out howRead More]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optery (YC W22) Is Hiring in Engineering, Legal, Sales, Marketing (U.S., Latam)]]></title>
            <link>https://www.optery.com/careers/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094471</guid>
            <description><![CDATA[Now is a great time to join Optery. Optery is profitable, and we 3x-ed sales last year. Our product was awarded “Editors’ Choice” by PCMag as the most outstanding in the personal data removal category for the 4th year in a row (2022 - 2025), Optery was ranked the #1 most effective of all personal data removal services tested in 2024 study by Consumer Reports, Optery was named one of Business Insider’s Top 30 Future Unicorns of 2025, and we’re changing the game in the world of consumer data in a way that puts individuals in control. Optery is automated opt out software, and we serve individuals, families and businesses. Our mission is to empower people to take control of their personal data, and we have a vision for a safer world through data privacy. Hundreds of thousands of people rely on Optery to prevent attacks and keep their personal information off the Internet. Optery has raised $9M+ in funding from world-class investors such as Y Combinator, Alumni Ventures, Bayhouse Capital, Flex Capital, Global Founders Capital, Goodwater Capital, Pioneer Fund, Soma Capital, TRAC, Tribe Capital, and Uncorrelated Ventures. Optery is headquartered in the San Francisco Bay Area, but operates as a fully-remote global team.]]></description>
            <content:encoded><![CDATA[
      Use promo code:  Xi8TJRBw  at checkout for 20% Off 🎉 with Optery’s Labor Day Sale! 🎇
    

          
        Ready to safeguard your personal data?
      
    
          
        Join the movement of people strengthening their privacy      
    
          
        Sign Up Free      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Adaptive LLM routing under budget constraints]]></title>
            <link>https://arxiv.org/abs/2508.21141</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45094421</guid>
            <description><![CDATA[Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.
    

    
    
              
          Comments:
          Accepted at EMNLP 2025 (findings)
        

          Subjects:
          
            Machine Learning (cs.LG)
        
          Cite as:
          arXiv:2508.21141 [cs.LG]
        
        
           
          (or 
              arXiv:2508.21141v1 [cs.LG] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.21141
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Pranoy Panda [view email]          [v1]
        Thu, 28 Aug 2025 18:18:19 UTC (1,560 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A Unique, High-Tech (Family) Computer]]></title>
            <link>https://nicole.express/2025/a-computer-in-your-home.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093956</guid>
            <description><![CDATA[There’s a concept that many people have tried, with varying effects: the “educational computer”, a device that a parent can buy for their children to learn t...]]></description>
            <content:encoded><![CDATA[
        

  

  
    There’s a concept that many people have tried, with varying effects: the “educational computer”, a device that a parent can buy for their children to learn the basics of the computer, which everyone will need to know in the future, and can also play games, so the children will actually want to use it. These have ranged from plasticky VTech toys with little more than an electronic organizer, to the Wonder Computer of the 1980’s, the Commodore VIC-20, which was a full computer. This is a prime market fit for an aging 8-bit platform, so of course, the Famicom has been wedged into it too… but not by Nintendo.

Unique, High-Tech, What more could you want?



This is it: a unique, high-tech computer. As we can see, it’s also advertising Contra on the box, along with “8 Bit” games, so immediately, you know that this is a Famiclone, and it’s got a Famicom cartridge slot underneath the cartridge flap. There’s been more than a few of these out there; they’re unique to me because they rarely show up in the United States (I bought this from Goodwill.com), but I would bet to many of the readers of this blog they won’t see this as unique at all.

What’s in the box?



In addition to the computer, you can see a whole selection of peripherals: two controllers, a mouse, a light-gun. And a power supply with a Europlug; further evidence that this is definitely not for the US market. Thankfully, it’s just 9V center-negative, so any plug you can use to power a Famicom should work here as well.



The sticker on the bottom of the system doesn’t match the sticker on the front of the box, but it does give us a release year for this model of the product: 2003. By 2003, the Famicom hardware was definitely old hat; in fact, that’s the same year Nintendo of Japan officially discontinued the system. You can definitely tell this sticker is trying to get you thinking this is relevant to the Windows XP world.



The sticker in the top left corner is long gone. Underneath is interesting, though; you can see three holes that look to the world like the Caps Lock, Num Lock, and Scroll Lock lights you’d see in the corner of a standard Windows keyboard of the era. Was this top case also used for standard keyboards? And if so, what did they do with the cartridge slot?



More evidence of plastics reuse is on the back, which shows a blanking plate covering nothing, and a speaker grille with no speaker behind it.



The actual ports you get are paltry; the common DB-9 ports you see for Famiclones, a power plug, and three RCA jacks. Think that’s stereo audio? (Something we have discussed as a Famicom mod on this blog before) Look closer!



The white RCA port is actually the RF modulator! Audio is the red jack. I’m guessing white, yellow, and red triplets of RCA ports were just extremely cheap at the time of this computer’s manufacture, so why not use them?



This is held together by screws, not plastic clips, which actually surprised me. But inside is just a standard keyboard membrane and a few small PCBs.



The keyboard mechanism is self-contained in the top plastic, and is actually a bit more elaborate than I expected; this is a “slider over membrane” design, where pressing a key causes a tiny point-like piece of plastic to connect the membrane. It works fairly well; you could definitely learn to type on this. Assuming it didn’t bind as much when it was new and clean, anyway.



Where’s the Famiclone itself? It’s just underneath the cartridge port, of course!
And also of course, it’s an epoxy blob.



On the epoxy blob was a small piece of masking tape, which I removed for the earlier screenshot. I can’t quite make it out as the ink has unfortunately bled a lot; the first letter seems to be a “V”. A major series of Famiclone chips from V.R. Technology has serial numbers beginning with “VT”, which could be related.



One thing about that controller. You might notice that on a real NES controller, the A button is on the outside edge, and the B button closer to the center. This is labeled in the opposite way– and this is how the buttons are arranged, too. Why did they swap the button positions? I don’t know, perhaps they just don’t like games being playable. The X and Y buttons are turbo buttons, as is commonly the case on four-button controllers being used for the Famicom.

Built-in hardware

This Famiclone has no built-in software or games. That seems to be pretty standard for models with cartridge slots; everything that makes this an educational computer is on the “48 in 1” cartridge. 48 is a much more achievable goal than many multicarts claim.



What’s inside?



An epoxy blob, of course, and 32kiB of SRAM. It’s a shame this is an epoxy blob, because I’m actually quite curious how that SRAM is wired. The NES memory map does not have room for 32kiB of cartridge PRG-ROM (usual amount of area mapped to the ROM) and 32kiB of cartridge RAM, so my assumption is that some sort of banking much be going on here.

Turn it on



But let’s boot the damn thing up already! Worth noting that this is a PAL 50Hz console; that should’ve been evident from the Europlug. I don’t think anywhere uses the Europlug and 60Hz NTSC; though possibly parts of Latin America?





The UI is clearly inspired by Microsoft Windows, though not the Windows XP that the sticker on the console tries to hint at. It’s actually pretty adorable, though having to move the cursor to the top corner is annoying. (Protip: use the page up and page down keys on the keyboard) The cursor can be moved with the mouse, or the controller. What is Super Hero?

You don't have a video tag support or something, so long story short: it's a rhythm game

It’s a rhythm game of some sort. I can’t recognize the track, and I don’t know how to play the game either; it doesn’t seem like controller inputs are what it’s looking for, or the arrow keys on the keyboard? So I’ll just let it be for now.

UPDATE: Thanks to The_Opponent for finding the track: Boys by smile.dk. Still not sure why it’s called “Super Hero”, though.

This actually has a lot of unique elements. For example, like any good version of Microsoft Windows, it has Solitaire.



And like any good multicart, it pads things out. Not only does it break up Duck Hunt (remember that gun in the package?) into multiple games…



And yes, it is Nintendo’s Duck Hunt. What else did you expect?



The most extreme case is Konami’s Track & Field. It’s here, sure.



But it’s been broken up into so many individual options for individual events that an entire page of the menu is taken up by it.



Also, you know what Konami game is not present on this multicart? Contra. Which was advertised on the box.



There are some educational games. Not really worth noting too much; mostly focused on typing, though it can also sing “Happy Birthday to You”. Since the keyboard is pretty decent, that’s probably actually the best usecase, but making games focused on typing is always a bit limiting. Here’s a classic “press the key listed” game, with a “My First Missile Command” theme.



But we were promised an Electronic Organ. So what does it have for a “MUSIC BOARD”?





That’s right; it’s MUSIC BOARD, from Nintendo and Hudson’s Family BASIC. Just separated into its own option on the menu, just like they did for Duck Hunt and Track and Field. Family BASIC MUSIC BOARD is fine, though I wouldn’t call it an electronic organ. I feel robbed.

But if Family BASIC’s MUSIC BOARD is here…



Then Family BASIC’s GAME BASIC should be here too. And it is! Or at least, I assume this is Family BASIC. (V3, judging by the version number) 32kiB of RAM is much more than it usually has access to, but is likely the purpose of the extra RAM on the cartridge. Very nice.



Unfortunately, this has some severe downsides compared to the real Family BASIC, despite the extra RAM. The biggest being that there is no way to save your work between sessions; neither battery-backed RAM nor a way to interface with a cassette tape. This pretty much relegates G BASIC to a novelty, though it always was one anyways.



Part of the FAMILY?



One thing I wondered here was, if it has Family BASIC on board, would the original one work?

Well… unfortunately, Family BASIC has a very annoying UI where you have to talk to the computer using text. And so I learned that while the keyboard is compatible in the sense that pressing keys makes letters appear, the keyboard matrix has been remapped.



I didn’t even make it to the actual BASIC.



Computers for the whole family

As I noted, the educational computer market has a lot of entries. Many had features like printers; I wonder if that was what the blanking plate was for. This one is very bare-bones. But let’s face it; it was mostly a way for kids to get their parents to get something into the house which could play games like Super Mario Bros. 3, albiet at a PAL 50Hz slowdown.

You don't have a video tag support or something, so long story short: it's SMB3 but slow

Still, I think it’s a pretty cool bit of computing history, especially important outside the wealthier countries whose markets I usually look at. I hope you enjoyed!

  

  


      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The time picker on the iPhone's alarm app isn't circular, it's just a long list]]></title>
            <link>https://old.reddit.com/r/interestingasfuck/comments/1n5lztw/the_time_picker_on_the_iphones_alarm_app_isnt/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093765</guid>
        </item>
        <item>
            <title><![CDATA[Search engine referral report for 2025 Q2]]></title>
            <link>https://radar.cloudflare.com/reports/search-engine-market-share-2025-q2</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093693</guid>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who is hiring? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093192</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093192</guid>
            <description><![CDATA[Please state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.]]></description>
            <content:encoded><![CDATA[Ask HN: Who is hiring? (September 2025)113 points by whoishiring 5 hours ago  | hide | past | favorite | 106 commentsPlease state the location and include REMOTE for remote work, REMOTE (US)
or similar if the country is restricted, and ONSITE when remote work is not an option.Please only post if you personally are part of the hiring company—no
recruiting firms or job boards. One post per company. If it isn't a household name,
explain what your company does.Please only post if you are actively filling a position and are committed
to responding to applicants.Commenters: please don't reply to job posts to complain about
something. It's off topic here.Readers: please only email if you are personally interested in the job.Searchers: try https://dheerajck.github.io/hnwhoishiring/,
https://amber-williams.github.io/hackernews-whos-hiring/,
http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com,
https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/,
https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension:
https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....Don't miss these other fine threads:Who wants to be hired? https://news.ycombinator.com/item?id=45093190Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45093191
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: Who wants to be hired? (September 2025)]]></title>
            <link>https://news.ycombinator.com/item?id=45093190</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093190</guid>
            <description><![CDATA[Share your information if you are looking for work. Please use this format:]]></description>
            <content:encoded><![CDATA[Ask HN: Who wants to be hired? (September 2025)54 points by whoishiring 4 hours ago  | hide | past | favorite | 133 commentsShare your information if you are looking for work. Please use this format:  Location:
  Remote:
  Willing to relocate:
  Technologies:
  Résumé/CV:
  Email:

Please only post if you are personally looking for work. Agencies, recruiters, job boards,
and so on, are off topic here.Readers: please only email these addresses to discuss work opportunities.There's a site for searching these posts at https://www.wantstobehired.com.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloudflare Radar: AI Insights]]></title>
            <link>https://radar.cloudflare.com/ai-insights</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid>
        </item>
        <item>
            <title><![CDATA[Effective learning: Rules of formulating knowledge (1999)]]></title>
            <link>https://www.supermemo.com/en/blog/twenty-rules-of-formulating-knowledge</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45093022</guid>
            <description><![CDATA[This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledge.]]></description>
            <content:encoded><![CDATA[Dr Piotr Wozniak, February, 1999 (updated)This article will help you overcome one of the greatest difficulties you will face when trying to accelerate learning: formulating knowledgeThe speed of learning will depend on the way you formulate the material. The same material can be learned many times faster if well formulated! The difference in speed can be stunning!  The rules are listed in the order of importance. Those listed first are most often violated or bring most benefit if complied with!There is an underlying assumption that you will proceed with learning using spaced repetition, i.e. you will not just learn once but you will repeat the material optimally (as in SuperMemo).The 20 rules of formulating knowledge in learning1) Do not learn if you do not understandTrying to learn things you do not understand may seem like an utmost nonsense. Still, an amazing proportion of students commit the offence of learning without comprehension. Very often they have no other choice! The quality of many textbooks or lecture scripts is deplorable while examination deadlines are unmovable.If you are not a speaker of German, it is still possible to learn a history textbook in German. The book can be crammed word for word. However, the time needed for such “blind learning” is astronomical. Even more important: The value of such knowledge is negligible. If you cram a German book on history, you will still know nothing of history.The German history book example is an extreme. However, the materials you learn may often seem well structured and you may tend to blame yourself for lack of comprehension. Soon you may pollute your learning process with a great deal of useless material that treacherously makes you believe “it will be useful some day”.  2) Learn before you memorizeBefore you proceed with memorizing individual facts and rules, you need to build an overall picture of the learned knowledge. Only when individual pieces fit to build a single coherent structure, will you be able to dramatically reduce the learning time. This is closely related to the problem comprehension mentioned in Rule 1: Do not learn if you do not understand. A single separated piece of your picture is like a single German word in the textbook of history.Do not start from memorizing loosely related facts! First read a chapter in your book that puts them together (e.g. the principles of the internal combustion engine). Only then proceed with learning using individual questions and answers (e.g. What moves the pistons in the internal combustion engine?), etc.3) Build upon the basicsThe picture of the learned whole (as discussed in Rule 2: Learn before you memorize) does not have to be complete to the last detail. Just the opposite, the simpler the picture the better. The shorter the initial chapter of your book the better. Simple models are easier to comprehend and encompass. You can always build upon them later on.Do not neglect the basics. Memorizing seemingly obvious things is not a waste of time! Basics may also appear volatile and the cost of memorizing easy things is little. Better err on the safe side. Remember that usually you spend 50% of your time repeating just 3-5% of the learned material! Basics are usually easy to retain and take a microscopic proportion of your time. However, each memory lapse on basics can cost you dearly!4) Stick to the minimum information principleThe material you learn must be formulated in as simple way as it isSimple is easyBy definition, simple material is easy to remember. This comes from the fact that its simplicity makes is easy for the brain to process it always in the same way. Imagine a labyrinth. When making a repetition of a piece of material, your brain is running through a labyrinth (you can view a neural network as a tangle of paths). While running through the labyrinth, the brain leaves a track on the walls. If it can run in only one unique way, the path is continuous and easy to follow. If there are many combinations, each run may leave a different trace that will interfere with other traces making it difficult to find the exit. The same happens on the cellular level with different synaptic connections being activated at each repetition of complex materialRepetitions of simple items are easier to scheduleI assume you will make repetitions of the learned material using optimum inter-repetition intervals (as in SuperMemo). If you consider an item that is composed of two sub-items, you will need to make repetitions that are frequent enough to keep the more difficult item in memory. If you split the complex item into sub-items, each can be repeated at its own pace saving your time. Very often, inexperienced students create items that could easily be split into ten or more simpler sub-items! Although the number of items increases, the number of repetitions of each item will usually be small enough to greatly outweigh the cost of (1) forgetting the complex item again and again, (2) repeating it in excessively short intervals or (3) actually remembering it only in part!Here is a striking example:Ill-formulated knowledge – Complex and wordyQ: What are the characteristics of the Dead Sea?A: Salt lake located on the border between Israel and Jordan. Its shoreline is the lowest point on the Earth’s surface, averaging 396 m below sea level. It is 74 km long. It is seven times as salty (30% by volume) as the ocean. Its density keeps swimmers afloat. Only simple organisms can live in its saline watersWell-formulated knowledge – Simple and specificQ: Where is the Dead Sea located?A: on the border between Israel and JordanQ: What is the lowest point on the Earth’s surface?A: The Dead Sea shorelineQ: What is the average level on which the Dead Sea is located?A: 400 meters (below sea level)Q: How long is the Dead Sea?A: 70 kmQ: How much saltier is the Dead Sea than the oceans?A: 7 timesQ: What is the volume content of salt in the Dead Sea?A: 30%Q: Why can the Dead Sea keep swimmers afloat?A: due to high salt contentQ: Why is the Dead Sea called Dead?A: because only simple organisms can live in itQ: Why only simple organisms can live in the Dead Sea?A: because of high salt contentYou might want to experiment and try to learn two subjects using the two above approaches and see for yourself what advantage is brought by minimum information principle. This is particularly visible in the long perspective, i.e. the longer the time you need to remember knowledge, the more you benefit from simplifying your items!Note in the example above how short the questions are. Note also that the answers are even shorter! We want a minimum amount of information to be retrieved from memory in a single repetition! We want answer to be as short as imaginably possible!You will notice that the knowledge learned in the ill-structured example is not entirely equivalent to the well-structured formulation. For example, although you will remember why the Dead Sea can keep swimmers afloat, you may forget that it at all has such a characteristic in the first place! Additionally, rounding 396 to 400 and 74 to 70 produces some loss of information. These can be remedied by adding more questions or making the present ones more precise.You will also lose the ability to fluently recite the description of the Dead Sea when called up to the blackboard by your teachers. I bet, however, that shining in front of the class is not your ultimate goal in learning. To see how to cope with recitations and poems, read further (section devoted to enumerations)5) Cloze deletion is easy and effectiveCloze deletion is a sentence with its parts missing and replaced by three dots. Cloze deletion exercise is an exercise that uses cloze deletion to ask the student to fill in the gaps marked with the three dots. For example, Bill …[name] was the second US president to go through impeachment.If you are a beginner and if you find it difficult to stick to the minimum information principle, use cloze deletion! If you are an advanced user, you will also like cloze deletion. It is a quick and effective method of converting textbook knowledge into knowledge that can be subject to learning based on spaced repetition. Cloze deletion makes the core of the fast reading and learning technique called incremental reading.Ill-formulated knowledge – Complex and wordyQ: What was the history of the Kaleida company?A: Kaleida, funded to the tune of $40 million by Apple Computer and IBM in 1991. Hyped as a red-hot startup, Kaleida’s mission was to create a multimedia programming language It finally produced one, called Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in 1995.Well-formulated knowledge – Simple cloze deletionQ: Kaleida was funded to the tune of …(amount) by Apple Computer and IBM in 1991A: $40 millionQ: Kaleida was funded to the tune of $40 million by …(companies) in 1991A: Apple and IBMQ: Kaleida was funded to the tune of $40 million by Apple Computer and IBM in … (year)A: 1991Q: …(company) mission was to create a multimedia programming language. It finally produced one, called Script X. But it took three yearsA: Kaleida’sQ: Kaleida’s mission was to create a … It finally produced one, called Script X. But it took three yearsA: multimedia programming languageQ: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called … But it took three yearsA: Script XQ: Kaleida’s mission was to create a multimedia programming language. It finally produced one, called Script X. But it took …(time)A: three yearsQ: Kaleida’s mission was to create a multimedia programming language: Script X. But it took three years. Meanwhile, companies such as … had snapped up all the businessA: Macromedia/AsymetrixQ: Kaleida’s mission was to create Script X. But it took three years. Meanwhile, companies such as Macromedia and Asymetrix had snapped up all the business. Kaleida closed in …(year)A: 1995Optional: SuperMemo Recipe:SuperMemo 2002SuperMemo 2000SuperMemo 98/99Creating cloze deletions in new SuperMemos:select the keyword that is to be replaced with tree dots and press Alt+ZGenerating a cloze deletions from texts placed in the clipboard in SuperMemo 2000:1. Press Ctrl+Alt+N to paste the text to SuperMemo 2. Select the part that is to be replaced with three dots 3. Right-click to open the component menu and select Reading : Remember cloze (or click one of cloze icons on the reading toolbar) Cloze deletions in SuperMemo 98/99:1. Press Ctrl+A to add a standard question-and-answer item2. Paste the text into the question field. This will create the outline of your items3. Press Ctrl+Alt+U to Duplicate the element4. Select the part that is to be replaced with three dots5. Cut the selection to the clipboard (e.g. with Shift+Del)6. Type in three dots (optionally, add the explanation in parentheses as in above examples)7. Press Ctrl+T to save the question field and move to the answer field8. Paste the text cut in Step 5 (e.g. with Shift+Ins or Ctrl+V). Your first item is ready9. Press PgUp to go back to the outline item created in Step 210. Goto Step 3 and continue adding new items6) Use imageryVisual cortex is that part of the brain in which visual stimuli are interpreted. It has been very well developed in the course of evolution and that is why we say one picture is worth a thousand words. Indeed if you look at the number of details kept in a picture and the easiness with which your memory can retain them, you will notice that our verbal processing power is greatly inferior as compared with the visual processing power. The same refers to memory. A graphic representation of information is usually far less volatile.Usually it takes much less time to formulate a simple question-and-answer pair than to find or produce a neat graphic image. This is why you will probably always have to weigh up cost and profits in using graphics in your learning material. Well-employed images will greatly reduce your learning time in areas such as anatomy, geography, geometry, chemistry, history, and many more.The power of imagery explains why the concept of Tony Buzan’s mind maps is so popular. A mind map is an abstract picture in which connections between its components reflect the logical connections between individual concepts.Less beneficial formulationQ: What African country is located between Kenya, Zambia and Mozambique?A: TanzaniaWell-formulated knowledge – Simple cloze deletionQ: What African country is marked white on the map?A: Tanzania7) Use mnemonic techniquesMnemonic techniques are various techniques that make remembering easier. They are often amazingly effective. For most students, a picture of a 10-year-old memorizing a sequence of 50 playing cards verges on discovering a young genius. It is very surprising then to find out how easy it is to learn the techniques that make it possible with a dose of training. These techniques are available to everyone and do not require any special skills!Before you start believing that mastering such techniques will provide you with an eternal solution to the problem of forgetting, be warned that the true bottleneck towards long-lasting and useful memories is not in quickly memorizing knowledge! This is indeed the easier part. The bottleneck lies in retaining memories for months, years or for lifetime! To accomplish the latter you will need SuperMemo and the compliance with the 20 rules presented herein.There have been dozens of books written about mnemonic techniques. Probably those written by Tony Buzan are most popular and respected. You can search the web for keywords such as: mind maps, peg lists, mnemonic techniques, etc.Experience shows that with a dose of training you will need to consciously apply mnemonic techniques in only 1-5% of your items. With time, using mnemonic techniques will become automatic!Exemplary mind map:(Six Steps mind map generated in Mind Manager 3.5, imported to SuperMemo 2004, courtesy of John England, TeamLink Australia)8) Graphic deletion is as good as cloze deletionGraphic deletion works like cloze deletion but instead of a missing phrase it uses a missing image component. For example, when learning anatomy, you might present a complex illustration. Only a small part of it would be missing. The student’s job is to name the missing area. The same illustration can be used to formulate 10-20 items! Each item can ask about a specific subcomponent of the image. Graphic deletion works great in learning geography!Exemplary graphic deletion:SuperMemo 2000/2002SuperMemo 99This is how you can quickly generate graphic deletion using a picture from the clipboard:1. Press Shift+Ins to paste the picture to SuperMemo2. Press Ctrl+Shift+M and choose Occlusion template to apply graphic deletion template3. SuperMemo 2000 only: Choose Ctrl+Shift+F2 to impose and detach the Occlusion template4. Fill out the fields and place the occlusion rectangle to cover the appropriate part of the picture (use Alt+click twice to set the rectangle in the dragging mode)In SuperMemo 99 you will need a few more steps:1.Create an item containing the following components:– question text: What is the name of the area covered with the red rectangle?– empty answer text (click Answer on the component menu)– your illustration (use Import file on the image component menu)– red rectangle component (choose red color with Color on the rectangle component menu)2. Choose Duplicate on the element menu (e.g. by pressing Ctrl+Alt+U)3. Ctrl+click the rectangle component twice to place it in the dragging mode4. Drag and size the red rectangle to cover the area in question5. Type in the answer in the answer field6. Press PgUp to go back to the original element created in Step 17. Go to Step 2 to add generate more graphic deletionsNote that you could also paint covering rectangles or circles on the original image but this would greatly increase the size of your collection. The above method makes sure that you reuse the same image many times in all items of the same template. For example, the collection Brain Anatomy available from >SuperMemo Library and on SuperMemo MegaMix CD-ROM uses the above techniqueA more detailed recipe for creating occlusion tests is presented in: Flow of knowledge9) Avoid setsA set is a collection of objects. For example, a set of fruits might be an apple, a pear and a peach. A classic example of an item that is difficult to learn is an item that asks for the list of the members of a set. For example: What countries belong to the European Union? You should avoid such items whenever possible due to the high cost of retaining memories based on sets. If sets are absolutely necessary, you should always try to convert them into enumerations. Enumerations are ordered lists of members (for example, the alphabetical list of the members of the EU). Enumerations are also hard to remember and should be avoided. However, the great advantage of enumerations over sets is that they are ordered and they force the brain to list them always in the same order. An ordered list of countries contains more information than the set of countries that can be listed in any order. Paradoxically, despite containing more information, enumerations are easier to remember. The reason for this has been discussed earlier in the context of the minimum information principle: you should always try to make sure your brain works in the exactly same way at each repetition. In the case of sets, listing members in varying order at each repetition has a disastrous effect on memory. It is nearly impossible to memorize sets containing more than five members without the use of mnemonic techniques, enumeration, grouping, etc. Despite this claim, you will often succeed due to subconsciously mastered techniques that help you go around this problem. Those techniques, however, will fail you all too often. For that reason: Avoid sets! If you need them badly, convert them into enumerations and use techniques for dealing with enumerationsIll-formulated knowledge – Sets are unacceptable!Q: What countries belong to the European Union (2002)?A: Austria, Belgium, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, Sweden, and the United Kingdom.Well-formulated knowledge – Converting a set into a meaningful listingQ: Which country hosted a meeting to consider the creation of a European Community of Defence in 1951?A: FranceQ: Which countries apart from France joined the European Coal and Steel Community in 1952?A: Germany, Italy and the BeneluxQ: What countries make up the Benelux?A: Belgium, Luxembourg, and the NetherlandsQ: Whose membership did Charles de Gaulle oppose in the 1960s?A: that of UKQ: Which countries joined the EEC along the UK in 1973?A: Ireland and DenmarkQ: Which country joined the EEC in 1981?A: GreeceQ: Which countries joined the EEC in 1986?A: Spain and PortugalQ: Which countries joined the EU in 1995?A: Austria, Sweden and FinlandQ: What was the historic course of expansion of the European Union membership?A: (1) France and (2) Germany, Italy and the Benelux, (3) UK and (4) Ireland and Denmark, (5) Greece, (6) Spain and Portugal and (7) Austria, Sweden and FinlandNote that in the example above, we converted a 15-member set into 9 items, five of which are 2-3 member sets, and one is a six member enumeration. Put it to your SuperMemo, and see how easy it is to generate the list of the European Union members using the historic timeline! Note the tricks used with France and the UK. They joined the union in the company of others but have been listed as separate items to simplify the learning process. Note also that the sum of information included in this well-formulated approach is far greater than that of the original set. Thus along simplicity, we gained some useful knowledge. All individual items effectively comply with the minimum information principle! You could go further by trying to split the Germany-Italy-Benelux set or using mnemonic techniques to memorize the final seven-member enumeration (i.e. the last of the questions above). However, you should take those steps only if you have any problems with retaining the proposed set in memory.10) Avoid enumerationsEnumerations are also an example of classic items that are hard to learn. They are still far more acceptable than sets. Avoid enumerations wherever you can. If you cannot avoid them, deal with them using cloze deletions (overlapping cloze deletions if possible). Learning the alphabet can be a good example of an overlapping cloze deletion:Hard to learn itemQ: What is the sequence of letters in the alphabet?A: abcdefghijklmnopqrstuvwxyzEasy to learn itemsQ: What three letters does the alphabet begin with?A: ABCQ: Fill out the missing letters of the alphabet A … … … EA: B, C, DQ: Fill out the missing letters of the alphabet B … … … FA: C, D, EQ: Fill out the missing letters of the alphabet C … … … GA: D, E, FThe above items will make learning the alphabet much faster. The greatest advantage of the above approach is that is it easier for psychological reasons: the student does not have to stop repetitions to recite the whole sequence and can only focus on a small part of the learned material. Still it is recommended that he recite the whole alphabet after making the repetition. However, once all individual pieces are well remembered, reciting the whole should be a pleasant and speedy action that produces little frustration.The cloze deletion used above is an overlapping cloze deletion, i.e. the same parts of the enumeration are strengthened in memory using different items (for example, the sequence C-D will be needed to recall the second and the third item). This redundancy does not contradict the minimum information principle because the extra information is added in extra items.You can also deal with enumerations by using grouping like in the case of sets (see the European Union example) but cloze deletions should be simpler and should suffice in most cases.Learning poems is an example of learning enumerations (all words and sentences have to be uttered in a predefined sequence); however, due to strong semantic connections, the rhyme and the rhythm, it may often be possible to effectively remember poems without using cloze deletion and without the frustration of forgetting small subcomponents again and again. However, once you notice you stumble with your poem, you should dismember it using cloze deletion and thus make sure that the learning is fast, easy, effective and pleasurableA poem that is hard to rememberQ: The credit belongs … (Teddy Roosevelt)A: The credit belongs to the man who’s actually in the arena, whose face is marred by dust and sweat; a man who knows the great enthusiasm and the great devotions, who spends himself in a worthy cause, who in the end knows the triumph of high achievement, so that his place shall never be with those cold and timid souls who know neither victory nor defeat A poem split into easy itemsQ: The credit belongs … (Teddy Roosevelt)A: to the man who’s actually in the arenaQ: The credit belongs to the man who’s actually in the arena …A: whose face is marred by dust and sweat (a man who knows the great enthusiasm)Q: whose face is marred by dust and sweat … (The credit belongs)A: a man who knows the great enthusiasm and the great devotions (who spends himself in a worthy cause)Q: a man who knows the great enthusiasm and the great devotions … (The credit belongs)A: who spends himself in a worthy cause (who in the end knows the triumph of high achievement)Q: who spends himself in a worthy cause … (The credit belongs)A: who in the end knows the triumph of high achievement (so that his place shall never be), etc. etc.Does it all sound artificial? It does! But you will never know how effective this approach is until you try it by yourself!11) Combat interferenceWhen you learn about similar things you often confuse them. For example, you may have problems distinguishing between the meanings of the words historic and historical. This will even be more visible if you memorize lots of numbers, e.g. optimum dosages of drugs in pharmacotherapy. If knowledge of one item makes it harder to remember another item, we have a case of memory interference. You can often remember an item for years with straight excellent grades until … you memorize another item that makes it nearly impossible to remember either! For example, if you learn geography and you memorize that the country located between Venezuela, Suriname and Brazil is Guyana, you are likely to easily recall this fact for years with just a couple of repetitions. However, once you add similar items asking about the location of all these countries, and French Guyana, and Colombia and more, you will suddenly notice strong memory interference and you may experience unexpected forgetting. In simple terms: you will get confused about what is what.Interference is probably the single greatest cause of forgetting in collections of an experienced user of SuperMemo. You can never be sure when it strikes, and the only hermetic procedure against it is to detect and eliminate. In other words, in many cases it may be impossible to predict interference at the moment of formulating knowledge. Interference can also occur between remotely related items like Guyana, Guyard and Guyenne, as well as Guyana, kayman and … aspirin. It may work differently for you and for your colleague. It very hard to predict.Still you should do your best to prevent interference before it takes its toll. This will make your learning process less stressful and mentally bearable. Here are some tips:make items as unambiguous as possiblestick to the minimum information principle (many of the remaining rules in this text are based on avoiding interference!)eliminate interference as soon as you spot it, i.e. before it becomes your obsession (e.g. as soon as you see the word inept you think “I know the meanings of inept and inapt but I will never know which is which!”)in SuperMemo use View : Other browsers : Leeches(Shift+F3) to regularly review and eliminate most difficult itemsread more: Memory interference12) Optimize wordingThe wording of your items must be optimized to make sure that in minimum time the right bulb in your brain lights up. This will reduce error rates, increase specificity, reduce response time, and help your concentration.Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2Less optimum item: cloze deletion that is too wordyQ: Aldus invented desktop publishing in 1985 with PageMaker. Aldus had little competition for years, and so failed to improve. Then Denver-based … blew past. PageMaker, now owned by Adobe, remains No. 2A: QuarkBetter item: fewer words will speed up learningQ: Aldus invented desktop publishing in 1985 with PageMaker but failed to improve. Then … blew past (PageMaker remains No. 2)A: QuarkOr better:Q: Aldus invented desktop publishing with PageMaker but failed to improve. It was soon outdistanced by …A: QuarkOr better:Q: PageMaker failed to improve and was outdistanced by …A: QuarkOr better:Q: PageMaker lost ground to …A: QuarkNote that the loss of information content in this item is inconsequential. During repetition you are only supposed to learn the name: Quark. You should not hope that the trailing messages on the ownership of PageMaker and the year of its development will somehow trickle to your memory as a side effect. You should decide if the other pieces of information are important to you and if so, store them in separate items (perhaps reusing the above text, employing cloze deletion again and optimizing the wording in a new way). Otherwise the redundant information will only slow down your learning process!13) Refer to other memoriesReferring to other memories can place your item in a better context, simplify wording, and reduce interference. In the example below, using the words humble and supplicant helps the student focus on the word shamelessly and thus strengthen the correct semantics. Better focus helps eliminating interference. Secondly, the use of the words humble and supplicant makes it possible to avoid interference of cringing with these words themselves. Finally, the proposed wording is shorter and more specific. Naturally, the rules basics-to-details and do not learn what you do not understand require that the words humble and supplicant be learned beforehand (or at least at the same time)Item subject to strong interferenceQ: derog: adj: shamelessly conscious of one’s failings and asking in a begging wayA: cringingItem that uses interfering memories to amplify the correct meaningQ: derog: adj: shamelessly humble and supplicantA: cringing14) Personalize and provide examplesOne of the most effective ways of enhancing memories is to provide them with a link to your personal life. In the example below you will save time if you use a personal reference rather than trying to paint a picture that would aptly illustrate the questionItem subject to strong interferenceQ: What is the name of a soft bed without arms or back?A: divanItem that uses interfering memories to amplify the correct meaningQ: What is the name of a soft bed without arms or back? (like the one at Robert’s parents)A: divanIf you remember exactly what kind of soft bed can be found in Robert’s parents’ apartment you will save time by not having to dig exactly into the semantics of the definition and/or looking for an appropriate graphic illustration for the piece of furniture in question. Personalized examples are very resistant to interference and can greatly reduce your learning time15) Rely on emotional statesIf you can illustrate your items with examples that are vivid or even shocking, you are likely to enhance retrieval (as long as you do not overuse same tools and fall victim of interference!). Your items may assume bizarre form; however, as long as they are produced for your private consumption, the end justifies the means. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, Linda Tripp, Nelson Mandela, etc. It is well known that emotional states can facilitate recall; however, you should make sure that you are not deprived of the said emotional clues at the moment when you need to retrieve a given memory in a real-life situationHarder itemQ: a light and joking conversationA: banterEasier itemQ: a light and joking conversation (e.g. Mandela and de Klerk in 1992)A: banterIf you have vivid and positive memories related to the meetings between Nelson Mandela and F.W. de Klerk, you are likely to quickly grasp the meaning of the definition of banter. Without the example you might struggle with interference from words such as badinage or even chat. There is no risk of irrelevant emotional state in this example as the state helps to define the semantics of the learned concept! A well-thought example can often reduce your learning time several times! I have recorded examples in which an item without an example was forgotten 20 times within one year, while the same item with a subtle interference-busting example was not forgotten even once in ten repetitions spread over five years. This is roughly equivalent to 25-fold saving in time in the period of 20 years! Such examples are not rare! They are most effectively handled with the all the preceding rules targeted on simplicity and against the interference16) Context cues simplify wordingYou can use categories in SuperMemo 2000/2002, provide different branches of knowledge with a different look (different template), use reference labels (Title, Author, Date, etc.) and clearly label subcategories (e.g. with strings such as chem for chemistry, math for mathematics, etc.). This will help you simplify the wording of your items as you will be relieved from the need to specify the context of your question. In the example below, the well-defined prefix bioch: saves you a lot of typing and a lot of reading while still making sure you do not confuse the abbreviation GRE with Graduate Record Examination. Note that in the recommended case, you process the item starting from the label bioch which puts your brain immediately in the right context. While processing the lesser optimum case, you will waste precious milliseconds on flashing the standard meaning of GRE and … what is worse … you will light up the wrong areas of your brain that will now perhaps be prone to interference!Wordy item can cause accidental lapses through interferenceQ: What does GRE stand for in biochemistry?A: glucocorticoid response elementContext-labeled items increase success rateQ: bioch: GREA: glucocorticoid response element17) Redundancy does not contradict minimum information principleRedundancy in simple terms is more information than needed or duplicate information, etc. Redundancy does not have to contradict the minimum information principle and may even be welcome. The problem of redundancy is too wide for this short text. Here are some examples that are only to illustrate that minimum information principle cannot be understood as minimum number of characters or bits in your collections or even items:passive and active approach: if you learn a foreign language, e.g. Esperanto, you will often build word pairs such as phone-telefono, language-lingvo, hope-esperanto, etc. These pairs require active recall of the foreign word. Active recall does not, however, guarantee passive recognition and you may fail with telefono-phone, lingvo-language, or esperanto-hope. Adding new elements with swapped questions and answers may in some cases be redundant but it does not contradict the minimum information principle! Your items are still as simple as possible. You just get more of themIn SuperMemo 2000/2002, you can quickly generate swapped word-pair items with Duplicate (Ctrl+Alt+D) and Swap (Ctrl+Shift+S)reasoning cues: you will often want to boost your reasoning ability by asking about a solution to the problem. Instead of just memorizing the answer you would like to quickly follow the reasoning steps (e.g. solve a simple mathematical equation) and generate the answer. In such a case, providing the hint on the reasoning steps in the answer will only serve helping you always follow the right path at repetitionsderivation steps: in more complex problems to solve, memorizing individual derivation steps is always highly recommended (e.g. solving complex mathematical problems). It is not cramming! It is making sure that the brain can always follow the fastest path while solving the problem. For more on boosting creativity and intelligence read: Roots of genius and creativity, as well as more specific: Derivation, reasoning and intelligencemultiple semantic representation: very often the same knowledge can be represented and viewed from different angles. Memorizing different representations of the same fact or rule is recommended in cases where a given memory is of high value. This will increase the expected recall rate (beyond that specified with the forgetting index)!flexible repetition: if there are many valid responses to the same question make sure that your representation makes it possible to identify the equivalence and reward you with good grades by providing just one of the equivalent choices. For example, if you learn a language, it rarely make sense to learn all synonyms that meet a definition of a concept. It is more adequate to consider a single synonym as the sufficient answer (e.g. a mark made by ink spilt on sth = blot/blob/blotch)more18) Provide sourcesExcept for well-tested and proven knowledge (such as 2+2=4), it is highly recommended that you include sources from which you have gathered your knowledge. In real-life situation you will often be confronted with challenges to your knowledge. Sources can come to your rescue. You will also find that facts and figures differ depending on the source. You can really be surprised how frivolously reputable information agencies publish figures that are drastically different from other equally reputable sources. Without SuperMemo, those discrepancies are often difficult to notice: before you encounter the new fact, the old one is often long forgotten. With sources provided, you will be able to make more educated choices on which pieces of information are more reliable. Adding reliability labels may also be helpful (e.g. Watch out!, Other sources differ!, etc.). Sources should accompany your items but should not be part of the learned knowledge (unless it is critical for you to be able to recall the source whenever asked).19) Provide date stampingKnowledge can be relatively stable (basic math, anatomy, taxonomy, physical geography, etc.) and highly volatile (economic indicators, high-tech knowledge, personal statistics, etc.). It is important that you provide your items with time stamping or other tags indicating the degree of obsolescence. In case of statistical figures, you might stamp them with the year they have been collected. When learning software applications, it is enough you stamp the item with the software version. Once you have newer figures you can update your items. Unfortunately, in most cases you will have to re-memorize knowledge that became outdated. Date stamping is useful in editing and verifying your knowledge; however, you will rarely want to memorize stamping itself. If you would like to remember the changes of a given figure in time (e.g. GNP figures over a number of years), the date stamping becomes the learned knowledge itself.20) PrioritizeYou will always face far more knowledge that you will be able to master. That is why prioritizing is critical for building quality knowledge in the long-term. The way you prioritize will affect the way your knowledge slots in. This will also affect the speed of learning (e.g. see: learn basics first). There are many stages at which prioritizing will take place; only few are relevant to knowledge representation, but all are important:Prioritizing sources – there will always be a number of sources of your knowledge. If you are still at student years: these will most likely be books and notes pertaining to different subjects. Otherwise you will probably rely more on journals, Internet, TV, newspapers, encyclopedias, dictionaries, etc. It is always worth being aware what is the optimum proportion of time devoted to those varied sources. As you progress with learning, you will quickly develop a good sense of which learning slots bring better results and which might be extended at the cost of othersExtracting knowledge – unless you are about to pass an important exam, it nearly never makes sense to memorize whole books or whole articles. You will need to extract those parts that are most likely to impact the quality of your knowledge. You can do it by (1) marking paragraphs in a book or journal, (2) pasting relevant web pages to SuperMemo, (3) pasting relevant passages to SuperMemo, (4) typing facts and figures directly to SuperMemo notes, etc. You will need some experience before you can accurately measure how much knowledge you can indeed transfer to your brain and what degree of detail you can feasibly master. Your best way to prioritize the flow of knowledge into your memory is to use incremental reading toolsTransferring knowledge to SuperMemo – you may try to stick with the 20 rules of formulating knowledge at the moment of introducing your material to SuperMemo. However, you can also literally transfer your notes or import whole files and later use the mechanisms provided by SuperMemo to determine the order of processing the imported material. Probably the best criterion for choosing between formulating or just importing is the time needed for accurately formulating the item or items. If formulation requires more knowledge, more time, comparing with other sources, etc. you can just import. Otherwise, if you believe that formulating an accurate item is a matter of seconds, formulate itFormulating items – make sure that explanatory or optional components of the answer are placed in the parentheses so that your attention is focused on the most important part of the item. The parts in the parentheses can be read after the repetition to strengthen the memory in its contextUsing forgetting index – you can use the forgetting index to prioritize pending items. The sequence of repetitions will naturally be determined by SuperMemo; however, you can request higher retention level for items that are more important and lower retention level for items of lower priorityLearning – the process of prioritizing does not end with the onset of repetitions. Here are the tools you can use to continue setting your priorities while the learning process is under way:Remember (Ctrl+M) – re-memorize items of high priority that have changed or which are extremely important to your knowledge at a given moment. If you choose Ctrl+M you will be able to determine the next interval for the currently reviewed item (its repetition counter will be reset to zero). It is recommended that you always re-memorize items whose content has changed significantlyReschedule (Ctrl+J) – manually schedule the date of the next repetitionExecute repetition (Ctrl+Shift+R) – manually execute a repetition even before the repetition’s due date (e.g. when reviewing particularly important material)Forget (Ctrl+R)- remove the current item from the learning process and place it at the end of the pending queueDismiss (Ctrl+D) – ignore the current item in the learning process altogetherDelete (Ctrl+Shift+Del) – remove the current item from your collectionChange the forgetting index of memorized items or change the ordinal of pending items (Ctrl+Shift+P)SummaryHere again are the twenty rules of formulating knowledge. You will notice that the first 16 rules revolve around making memories simple! Some of the rules strongly overlap. For example: do not learn if you do not understand is a form of applying the minimum information principle which again is a way of making things simple:Do not learn if you do not understandLearn before you memorize – build the picture of the whole before you dismember it into simple items in SuperMemo. If the whole shows holes, review it again!Build upon the basics – never jump both feet into a complex manual because you may never see the end. Well remembered basics will help the remaining knowledge easily fit inStick to the minimum information principle – if you continue forgetting an item, try to make it as simple as possible. If it does not help, see the remaining rules (cloze deletion, graphics, mnemonic techniques, converting sets into enumerations, etc.)Cloze deletion is easy and effective – completing a deleted word or phrase is not only an effective way of learning. Most of all, it greatly speeds up formulating knowledge and is highly recommended for beginnersUse imagery – a picture is worth a thousand wordsUse mnemonic techniques – read about peg lists and mind maps. Study the books by Tony Buzan. Learn how to convert memories into funny pictures. You won’t have problems with phone numbers and complex figuresGraphic deletion is as good as cloze deletion – obstructing parts of a picture is great for learning anatomy, geography and moreAvoid sets – larger sets are virtually un-memorizable unless you convert them into enumerations!Avoid enumerations – enumerations are also hard to remember but can be dealt with using cloze deletionCombat interference – even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal lifeOptimize wording – like you reduce mathematical equations, you can reduce complex sentences into smart, compact and enjoyable maximsRefer to other memories – building memories on other memories generates a coherent and hermetic structure that forgetting is less likely to affect. Build upon the basics and use planned redundancy to fill in the gapsPersonalize and provide examples – personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memoriesRely on emotional states – emotions are related to memories. If you learn a fact in the sate of sadness, you are more likely to recall it if when you are sad. Some memories can induce emotions and help you employ this property of the brain in rememberingContext cues simplify wording – providing context is a way of simplifying memories, building upon earlier knowledge and avoiding interferenceRedundancy does not contradict minimum information principle – some forms of redundancy are welcome. There is little harm in memorizing the same fact as viewed from different angles. Passive and active approach is particularly practicable in learning word-pairs. Memorizing derivation steps in problem solving is a way towards boosting your intellectual powers!Provide sources – sources help you manage the learning process, updating your knowledge, judging its reliability, or importanceProvide date stamping – time stamping is useful for volatile knowledge that changes in timePrioritize – effective learning is all about prioritizing. In incremental reading you can start from badly formulated knowledge and improve its shape as you proceed with learning (in proportion to the cost of inappropriate formulation). If need be, you can review pieces of knowledge again, split it into parts, reformulate, reprioritize, or delete. See also: Incremental reading, Devouring knowledge, Flow of knowledge, Using tasklists]]></content:encoded>
        </item>
    </channel>
</rss>