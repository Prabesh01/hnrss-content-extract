<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 28 Aug 2025 20:09:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Engineers send quantum signals with standard Internet Protocol]]></title>
            <link>https://phys.org/news/2025-08-quantum-standard-internet-protocol.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056079</guid>
            <description><![CDATA[In a first-of-its-kind experiment, engineers at the University of Pennsylvania brought quantum networking out of the lab and onto commercial fiber-optic cables using the same Internet Protocol (IP) that powers today's web.]]></description>
            <content:encoded><![CDATA[
										

        
            
             
                Yichi Zhang, a doctoral student in Materials Science and Engineering, with the equipment used to generate and send the quantum signal over Verizon fiber optic cables. Credit: Sylvia Zhang
                    
    In a first-of-its-kind experiment, engineers at the University of Pennsylvania brought quantum networking out of the lab and onto commercial fiber-optic cables using the same Internet Protocol (IP) that powers today's web.


										      
																																	Reported in Science, the work shows that fragile quantum signals can run on the same infrastructure that carries everyday online traffic. The team tested their approach on Verizon's campus fiber-optic network.
The Penn team's tiny "Q-chip" coordinates quantum and classical data and, crucially, speaks the same language as the modern web. That approach could pave the way for a future "quantum internet," which scientists believe may one day be as transformative as the dawn of the online era.
Quantum signals rely on pairs of "entangled" particles, so closely linked that changing one instantly affects the other. Harnessing that property could allow quantum computers to link up and pool their processing power, enabling advances like faster, more energy-efficient AI or designing new drugs and materials beyond the reach of today's supercomputers.
Penn's work shows, for the first time on live commercial fiber, that a chip can not only send quantum signals but also automatically correct for noise, bundle quantum and classical data into standard internet-style packets, and route them using the same addressing system and management tools that connect everyday devices online.
"By showing an integrated chip can manage quantum signals on a live commercial network like Verizon's, and do so using the same protocols that run the classical internet, we've taken a key step toward larger-scale experiments and a practical quantum internet," says Liang Feng, Professor in Materials Science and Engineering (MSE) and in Electrical and Systems Engineering (ESE), and the Science paper's senior author.

        
            
             
                Part of the equipment used to create a node of the quantum network, roughly one kilometer's worth of Verizon commercial fiber optic cable away from its source. Credit: Sylvia Zhang
                    
    


																																						
																																			The challenges of scaling the quantum internet
Erwin Schrodinger, who coined the term "quantum entanglement," famously related the concept to a cat hidden in a box. If the lid is closed, and the box also contains radioactive material, the cat could be alive or dead. One way to interpret the situation is that the cat is both alive and dead. Only opening the box confirms the cat's state.
That paradox is roughly analogous to the unique nature of quantum particles. Once measured, they lose their unusual properties, which makes scaling a quantum network extremely difficult.
"Normal networks measure data to guide it towards the ultimate destination," says Robert Broberg, a doctoral student in ESE and co-author of the paper. "With purely quantum networks, you can't do that, because measuring the particles destroys the quantum state."

        
            
             
                From left: Liang Feng, Professor in Materials Science and Engineering, and Robert Broberg, a doctoral student in Electrical and Systems Engineering. The wires behind them include a Verizon fiber optic cable that carried the quantum signal. Credit: Sylvia Zhang
                    
    

Coordinating classical and quantum signals
To get around this obstacle, the team developed the "Q-Chip" (short for "Quantum-Classical Hybrid Internet by Photonics") to coordinate "classical" signals, made of regular streams of light, and quantum particles.
"The classical signal travels just ahead of the quantum signal," says Yichi Zhang, a doctoral student in MSE and the paper's first author. "That allows us to measure the classical signal for routing, while leaving the quantum signal intact."
In essence, the new system works like a railway, pairing regular light locomotives with quantum cargo. "The classical 'header' acts like the train's engine, while the quantum information rides behind in sealed containers," says Zhang.
"You can't open the containers without destroying what's inside, but the engine ensures the whole train gets where it needs to go."
Because the classical header can be measured, the entire system can follow the same "IP" or "Internet Protocol" that governs today's internet traffic.
"By embedding quantum information in the familiar IP framework, we showed that a quantum internet could literally speak the same language as the classical one," says Zhang. "That compatibility is key to scaling using existing infrastructure."

        
            
             
                A node of the quantum network, roughly one kilometer's worth of Verizon fiber optic cable away from the quantum signal's source. Credit: Sylvia Zhang
                    
    


																									
																																			Adapting quantum technology to the real world
One of the greatest challenges to transmitting quantum particles on commercial infrastructure is the variability of real-world transmission lines. Unlike laboratory environments, which can maintain ideal conditions, commercial networks frequently encounter changes in temperature, thanks to weather, as well as vibrations from human activities like construction and transportation, not to mention seismic activity.
To counteract this, the researchers developed an error-correction method that takes advantage of the fact that interference to the classical header will affect the quantum signal in a similar fashion.
"Because we can measure the classical signal without damaging the quantum one," says Feng, "we can infer what corrections need to be made to the quantum signal without ever measuring it, preserving the quantum state."
In testing, the system maintained transmission fidelities above 97%, showing that it could overcome the noise and instability that usually destroy quantum signals outside the lab. And because the chip is made of silicon and fabricated using established techniques, it could be mass-produced, making the new approach easy to scale.
"Our network has just one server and one node, connecting two buildings, with about a kilometer of fiber-optic cable installed by Verizon between them," says Feng. "But all you need to do to expand the network is fabricate more chips and connect them to Philadelphia's existing fiber-optic cables."

        
            
             
                Yichi Zhang, a doctoral student in Materials Science and Engineering, inspects the source of the quantum signal. Credit: Sylvia Zhang
                    
    


																																						
																																			The future of the quantum internet
The main barrier to scaling quantum networks beyond a metro area is that quantum signals cannot yet be amplified without destroying their entanglement.
While some teams have shown that "quantum keys," special codes for ultra-secure communication, can travel long distances over ordinary fiber, those systems use weak coherent light to generate random numbers that cannot be copied, a technique that is highly effective for security applications but not sufficient to link actual quantum processors.
Overcoming this challenge will require new devices, but the Penn study provides an important early step: showing how a chip can run quantum signals over existing commercial fiber using internet-style packet routing, dynamic switching and on-chip error mitigation that work with the same protocols that manage today's networks.
"This feels like the early days of the classical internet in the 1990s, when universities first connected their networks," says Broberg. "That opened the door to transformations no one could have predicted. A quantum internet has the same potential."

																																	
																														
																				
																						More information:
												Yichi Zhang et al, Classical-decisive quantum internet by integrated photonics, Science (2025). DOI: 10.1126/science.adx6176. www.science.org/doi/10.1126/science.adx6176
																						
																						
																					
                               														
																					
                              													                                        
										
										
											 Citation:
												Engineers send quantum signals with standard Internet Protocol (2025, August 28)
												retrieved 28 August 2025
												from https://phys.org/news/2025-08-quantum-standard-internet-protocol.html
											 
											 
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 
										
                                        
									]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[I’m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

                ❄                ❄                ❄                ❄

I’ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren’t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn’t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that’s going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

                ❄                ❄                ❄                ❄

I’m often asked, “what is the future of programming?” Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it’s too late? My answer to all these questions is “I haven’t the foggiest”. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

                ❄                ❄               ❇                ❄

I’m also asked: “is AI a bubble”? To which my answer is “OF COURSE IT’S A BUBBLE”. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don’t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan… but it did not kill Amazon.

                ❄                ❄                ❄                ❄

I retired from public speaking a couple of years ago. But while I don’t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I’m looking forward to catching up with many of them at GOTO Copenhagen. I’ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

                ✢                ❄                ❄                ❄

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren’t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it’s just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn’t ask an LLM to calculate an answer than we can calculate deterministically (yes, I’ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

                ❄                ❄                ❄                ❄

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can’t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

                ❄                ❄                ❄                ❄

I’ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say “all tests green”, yet when I run them, there are failures. If that was a junior engineer’s behavior, how long would it be before H.R. was involved?

                ❄                ❄                ❄                ❄

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (“exfiltration”). That “untrusted content” can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attacker’s web page, and it could trick the agent to go to your bank account in another tab and “buy you a present” by transferring your balance to the kind attacker. Willison’s view is that “the entire concept of an agentic browser extension is fatally flawed and cannot be built safely”.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison’s LLM CLI, you’ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren’t just chatbots or
      autocomplete tools - they’re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that’s exactly what we did, and in
      this article I’ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You’ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they’re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we’re
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (I’ve
      shortened ours for the article). Now something magical happened. I could
      say “X isn’t working” and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes “fixed” failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al’s default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned “Bedrock is timing out”
      or “the model responses are getting truncated,” the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the iceberg—the AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentation—recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask “Why is
      this API call failing intermittently?” and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation—especially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capable—it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say “The authentication tests are failing, please fix the issue”
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestions—it was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit—like having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development—from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourself—even a simple version—gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VLT observations of interstellar comet 3I/ATLAS II]]></title>
            <link>https://arxiv.org/abs/2508.18382</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055335</guid>
            <description><![CDATA[We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Å). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3σ$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.]]></description>
            <content:encoded><![CDATA[
    
    
    Authors:Rohan Rahatgaonkar, Juan Pablo Carvajal, Thomas H. Puzia, Baltasar Luco, Emmanuel Jehin, Damien Hutsemékers, Cyrielle Opitom, Jean Manfroid, Michaël Marsset, Bin Yang, Laura Buchanan, Wesley C. Fraser, John Forbes, Michele Bannister, Dennis Bodewits, Bryce T. Bolin, Matthew Belyakov, Matthew M. Knight, Colin Snodgrass, Erica Bufanda, Rosemary Dorsey, Léa Ferellec, Fiorangela La Forgia, Manuela Lippi, Brian Murphy, Prasanta K. Nayak, Mathieu Vander Donckt            
    View PDF
    HTML (experimental)
            Abstract:We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000Å). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3\sigma$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.
    

    
    
              
          Comments:
          12 pages, 3 figures, 2 tables, submitted to ApJL
        

          Subjects:
          
            Solar and Stellar Astrophysics (astro-ph.SR); Earth and Planetary Astrophysics (astro-ph.EP)
        
          Cite as:
          arXiv:2508.18382 [astro-ph.SR]
        
        
           
          (or 
              arXiv:2508.18382v1 [astro-ph.SR] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2508.18382
              
                                arXiv-issued DOI via DataCite
            
          
        
    
  
      Submission history From: Thomas H. Puzia [view email]          [v1]
        Mon, 25 Aug 2025 18:15:44 UTC (2,178 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates aren’t exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesn’t exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know what’s wrong with people?
                They’re too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence — however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say “it depends” all the time.
                Much more fun getting to say
                “let’s ship it and iterate” as Staff or
                “that won’t scale” as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? 🤨
}

              GPS coordinates aren’t exact.
                They’re noisy. They’re approximate. They’re probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              you’re probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesn’t capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach that’s both
              mathematically rigorous and surprisingly practical.
            
            As you’d expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2σ confidence 🤓
}

            When you compare two Uncertain values,
              you don’t get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/m³
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // m²
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed —
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesn’t exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              let’s take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or “fruit machine” for our UK readers 🇬🇧):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "◻️", "◻️", "◻️",  // blanks
            "🍒", "🍋", "🍊", "🍇", "💎"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("💎", "💎", "💎"): return 100  // Jackpot!
        case ("🍒", "🍒", "🍒"): return 10
        case ("🍇", "🍇", "🍇"): return 5
        case ("🍊", "🍊", "🍊"): return 3
        case ("🍋", "🍋", "🍋"): return 2
        case ("🍒", _, _), // Any cherry
             (_, "🍒", _),
             (_, _, "🍒"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically —
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: ≈ $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23°C
let tempSpread = temperature.standardDeviation() // about 1°C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)°C and \(upper)°C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp ≤ 25°C) < 20%
    print("Unlikely to be 25°C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users don’t notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is ±1000 meters,
            that’s a bad look 🤡
          So where do we go from here?
            Let’s channel our Senior+ memes from before for guidance.
          That Staff engineer saying “let’s ship it and iterate”
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineer’s warning of “that won’t scale”.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isn’t to eliminate uncertainty —
            it’s to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: The government of my country blocked VPN access. What should I use?]]></title>
            <link>https://news.ycombinator.com/item?id=45054260</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054260</guid>
            <description><![CDATA[Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).]]></description>
            <content:encoded><![CDATA[
Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).- First things first, you have to get your hands on actual VPN software and configs. Many providers who are aware of VPN censorship and cater to these locales distribute their VPNs through hard-to-block channels and in obfuscated packages. S3 is a popular option but by no means the only one, and some VPN providers partner with local orgs who can figure out the safest and most efficient ways to distribute a VPN package in countries at risk of censorship or undergoing censorship.- Once you've got the software, you should try to use it with an obfuscation layer.Obfs4proxy is a popular tool here, and relies on a pre-shared key to make traffic look like nothing special. IIRC it also hides the VPN handshake. This isn't a perfectly secure model, but it's good enough to defeat most DPI setups.Another option is Shapeshifter, from Operator (https://github.com/OperatorFoundation). Or, in general, anything that uses pluggable transports. While it's a niche technology, it's quite useful in your case.In both cases, the VPN provider must provide support for these protocols.- The toughest step long term is not getting caught using a VPN. By its nature, long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a state actor). I don't know the situation on the ground in Indonesia, so I won't speculate about what the best way to avoid this would be, long-term.I will endorse Mullvad as a trustworthy and technically competent VPN provider in this niche (n.b., I do not work for them, nor have I worked for them; they were a competitor to my employer and we always respected their approach to the space).
Thank you very much for a detailed answer. Might I rudely ask -- as you're knowledgeable in this space, what do you think of Mullvad's DAITA, which specifically aims to defeat traffic analysis by moving to a more pulsed constant bandwidth model?
DAITA was introduced after my time in the industry, but this isn't a new idea (though as far as I know, it's the first time this kind of thing's been commercialized).It's clever. It tries to defeat attacks against one of the tougher parts of VPN connections to reliably obfuscate, and the effort's commendable, but I'll stop short of saying it's a good solution for one big reason: with VPNs and censorship circumvention, the data often speaks for itself.A VPN provider working in this space will often have aggregate (and obviously anonymized, if they're working in good faith) stats about success rates and failure classes encountered from clients connecting to their nodes. Where I worked, we didn't publish this information. I'm not sure where Mullvad stands on this right now.In any case -- some VPN providers deploying new technology like this will partner with the research community (because there's a small, but passionate formal research community in this space!) and publish papers, studies, and other digests of their findings. Keep an eye out for this sort of stuff. UMD's Breakerspace in the US in particular had some extremely clever people working on this stuff when I was involved in the industry.
This makes me wonder: are there "cloud drive virtual sneakernet" systems that will communicate e.g. by a client uploading URL request(s) as documents via OneDrive/SharePoint/Google Drive/Baidu etc., a server reacting to this via webhook and uploading (say) a PDF version of the rendered site, then allowing the client to download that PDF? You effectively use the CDN of that service as a (very slow) proxy.Of course, https://xkcd.com/538/ applies in full force, and I don't have any background in the space to make this a recommendation!
It doesn't apply imo as OP is probably not a high value target of the govt, he just wants to bypass his govt restrictions and I doubt the situation is so bad that the govt will send people physically to deal with people circumventing the block.Your solution could technically work over any kind of open connection / data transfer protocol that isn't blocked by the provider but it would be an absolute pain to browse the web that way and there are probably better solutions out there.
I wonder if it can be embedded in a video stream, like a video of a lava lamp that you always have open, but the lsb of ever byte is meaningful.
That's an interesting idea, and probably something you might be able to achieve with a tool like h26forge.It's also probably more useful to just have a connection be fully dedicated to a VPN, and have the traffic volume over time mimic what you'd see in a video, rather than embedding it in a video -- thanks to letsencrypt, much of the web's served over TLS these days (asterisks for countries like KZ and TM which force the use of a state-sponsored CA), so going to great lengths to embed your VPN in a video isn't really practical.
I’m curious about what makes it difficult to block a vpn provider long term. You said getting the software is difficult, but can a country not block known vpn ingress points?
A country can and absolutely will block known VPN ingress points. There are two tricks that we can use to circumvent this:- Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc). Bonus points if you can leverage something like ECH (ex-ESNI) to make it harder to identify a single bucket or subdomain.- Keep spawning new domains and subdomains to distribute your binaries.There are complications with both approaches. Some countries block ECH outright. Some have no problem shutting the internet down wholesale for a little bit. The domain-hopping approach presents challenges w/r/t establishing trust (though not insurmountable ones, much of the time).These are thing that have to be judged and balanced on a case-by-case basis, and having partners on the ground in these places really helps reduce risk to users trying to connect from these places, but then you have to be very careful talking to then since they could themselves get in trouble for trying to organize a VPN distribution network with you. It's layers on layers, and at some point it helps to just have someone on the team with a background in working with people in vulnerable sectors and someone else from a global affairs and policy background to try and keep things as safe as they can be for people living under these regimes.
I've heard of domain fronting, where you host something on a subdomain of a large provider like Azure or Amazon. Is this what you're talking about when you say> - Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc).How can one bounce VPN traffic through S3? Or are you just talking about hosting client software, ingress IP address lists, etc?
That's generally for distribution, but yeah, it's a form of domain fronting.There are some more niche techniques that are _really_ cool but haven't gained widespread adoption, too, like refractive routing. The logistics of getting that working are particularly challenging since you need a willing partner who'll undermine some of their trustworthiness with some actors to support (what is, normally, to them) your project.
You've come to a wrong place to ask. Most people here (judging by recommendations of own VPN instances, Tor, Tailscale/other Wireguard-based VPNs, and Mullvad) don't have any experience with censorship circumvention.Just look for any VPNs that are advertised specifically for China, Russia, or Iran. These are the cutting edge tech, they may not be so privacy-friendly as Mullvad, but they will certainly work.
> Just look for any VPNs that are advertised specifically for China, Russia, or Iran.If I was working for a secret service for these countries, I would set up many "VPNs that are advertised specifically for x" as honeypots to gather data about any dissidents.
It doesn't matter, he should look into the open source protocols that these services use. He doesn't have to use them.VLESS / v2ray works in Russia, as far as I know.
Mr. Kafka, suspicion is healthy. However, abstraction provides no way forward when faced with practicalities instead of theory. Creates a Kafka-esque situation - anything suitable is by definition unsuitable. Better to focus on practical technical advice.
Hmm. People who recommend widely used approaches, and well-known, well-established providers, "don't have any experience with cenorship circumvention".So the solution is no-name providers using random ad-hoc hackery, chosen according to a criterion more or less custom designed to lead you into watering hole attacks.Right.
None of the things I listed are "widely used approaches, and well-known, well-established providers" in the parts of the world where it does matter.Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.
Furthermore, you can always run another VPN on top of that if you don’t trust the outer one with the actual plaintext traffic.
VPNs that are advertised are for-profit products, which means:1. They are in most cases run by national spy agencies.2. They will at least appear to work, i.e., they will provide you with access to websites that are blocked by the country you are in.  Depending on which country's spies run the system, they may actually work in the sense of hiding your traffic from that country's spies, or they may mark you as a specific target and save all your traffic for later analysis.My inclination is to prefer free (open-source) software that isn't controlled by a company which can use that control against its users.
Well, you have to host your free open-source VPN software somewhere. And then, (N. B.: technical and usability stuff aside, I'm talking only about privacy bits here) everything boils down to two equally nightmarish options.First, you use well-known cloud or dedicated hoster. All your traffic is now tied to the single IP address of that hoster. It may be linked to you by visiting two different sites from the same IP address. Furthermore, this hoster is legally required to do anything with your VPN machine on demand of corresponding state actors (this is not a speculative scenario; i. e. Linode literally silently MitMed one of their customers on German request). Going ever further, residential and company IPs have quite different rules when it comes to law enforcement. Seeding Linux ISOs from your residential IP will be overlooked almost everywhere (sorry, Germany again), but seeding Linux ISOs from AWS can easily be a criminal offense.Second, you use some shady abuse-proof hosting company, which keeps no logs (or at least says that) and accepts payments in XMR. Now you're logging in to your bank account from an IP address that is used to seedbox pirate content or something even more illegal, and you still don't know if anyone meddles with your VPN instance looking for crypto wallet keys in your traffic.VPN services have a lot of "good" customers for a small amount of IP addresses, so even if they have some "bad" actors, their IPs as a whole remain "good enough". And, as the number of customers is big, each IP cannot be reliably tied to a specific customer without access logs.
Tor is a third option, at least as one layer, and seeding Linux ISOs is not, to my knowledge, a criminal offense in any jurisdiction, not even in China.  I don't know where you got that idea.
It is absolutely self-evident that VPNs are considered high-value targets and that all spy agencies invest a chunk of resources to go after high-value targets.
I would invite you to read again the two claims made, and consider whether your statement actually addresses the veracity of either.To be a little trite: we all agree that chickens like grain, but it does not follow that a majority of grain producers are secretly controlled by a cabal of poultry.
You can always do v2ray -> Mullvad in a docker container routed with gluetun for censorship avoidance and privacy
^ this comment is right on. The cutting edge of VPN circumvention is the one marketed to people in China. Last I poked at this there were a lot of options.
I have a little, maybe enough to be dangerous. SSH won’t be sufficient to avoid all traffic analysis. Everyone can see how much traffic and the pattern of that traffic, which can leak info about the sort of things you’re doing.If you’re worried about ending up on a list, using things that look like VPNs while the VPNs are locked down is likely to do so.Also… your neighbors in Myanmar didn’t do a lockdown during the genocide and things got pretty fucking dire as a result. People have taken different lessons from this. I’m not sure what the right answer is, and which is the greater evil. Deplatforming and arresting people for inciting riots and hate speech is probably the best you can do to maintain life and liberty for the most people.
OP: look into VLESS (and similar).  And read up on ntc.party (through Google translate).  There are certain VPN providers that offer the protocol.
Mullvad worked OK in China for me recently. Sometimes I'd have to try a few different endpoints before it worked. Something built specifically to work in those places would probably be better, but it wasn't too much trouble. Not necessarily a recommendation, just sharing one data point.
I remember always needing obfuscation enabled in Mullvad, but it would work in the end (as you said, after trying a few endpoints).
- Tor. Pros: Reasonably user friendly and easy to get online, strong anonymity, free. Cons: a common target for censorship, not very fast, exit nodes are basically universally distrusted by websites.- Tailscale with Mullvad exit nodes. Pros: little setup but not more than installing and configuring a program, faster than Got, very versatile. Cons: deep packet inspection can probably identify your traffic is using Mullvad, costs some money.- Your own VPSs with Wireguard/Tailscale. Pros: max control, you control how fast you want it, you can share with people you care about (and are willing to support). Cons: the admin effort isn't huge but requires some skill, cost is flexible but probably 20-30$ per month minimum in hosting.
> - Tailscale with Mullvad exit nodesTailscale is completely unnecessary here, unless OP can't connect to Mullvad.net in the first place to sign up. But if the Indonesian government blocks Mullvad nodes, they'll be out of luck either way.> - Your own VPSs with Wireguard/TailscaleKeep in mind that from the POV of any websites you visit, you will be easily identifiable due to your static IP.My suggestion would be to rent a VPS outside Indonesia, set up Mullvad or Tor on the VPS and route all traffic through that VPS (and thereby through Mullvad/Tor). The fastest way to set up the latter across devices is probably to use the VPS as Tailscale exit node.
Tailscale + Mullvad does have a privacy advantage over either one by itself: the party that could potentially spy on the VPN traffic (Mullvad) doesn’t know whose traffic it is beyond that it’s a Tailscale customer. Any government who wanted to trace specific traffic back to OP would need to get the cooperation of both Mullvad and Tailscale, which is a lot less likely than even the quite unlikely event of getting Mullvad to cooperate.
I mean multiple VPSs for redundancy. Contabo is maybe the cheapest I've seen and it's like 3$ mtl for the smallest?
And using another VPN like NordVPN or ProtonVPN is probably in the same category as Mullvad, but worth being cautious. If it's free, you are the product. If you pay, you're still sending your traffic to a publicly (usually) known server of a VPN. That metadata alone in some jurisdictions can still put you in danger.Stay safe
This is good overview, I just wanted to add that a VPS IP is not a residential IP. You will encounter roadblocks when you try to access services if you appear to be coming from a VPS. Not that I had a better solution, just to clarify what you can expect.
Wireguard is not censorship-resistant, and most VPN-averse countries block cross-border Wireguard. Why reply a practical question in an area in which you have no experience?
Because Indonesia is new to the game and might still be catching up. They’re probably playing whackamole with the most common public VPN providers and might not be doing deep packet inspection yet. I worked with someone getting traffic out of Hong Kong a year ago and there was a lot trial and error figuring out what was blocked and what was not. Wireguard was one that worked.
Tor also has anti-censorship mechanisms (snowflakes, ...). Depending on how aggressive the blocking is, Tor might be the most effective solution.
IMO most people should have a VPS even if you don't need it for tunneling. Living without having a place to just leave services/files is very hard and often "free" services will hold your data hostage to manipulate your behavior which is annoying on a good day.
Yeah they can be cheap, but I would definitely recommend having at least 3 for redundancy. If one get shut down or it's IP blacklisted you still hopefully have a backup line to create a replacement.
No, unless you pay month to month. If you wait till BF you can find some really good deals on sites like lowendspirit
> cost is flexible but probably 20-30$ per month minimum in hosting.$4/month VPS from DigitalOcean is more than enough to handle a few users as per my experience. I have a Wireguard setup like this for more than a year. Didn't notice any issues.
Australia and UK might soon go down this path.Something quite depressing is if we (HN crowd) find workarounds, most regular folks won't have the budget/expertise to do so, so citizen journalism will have been successfully muted by government / big media.
I would have laughed in your face if you wrote this comment merely 6 months ago. Now I'm just depressed. (UK)
Don't worry, you shouldn't underestimate the capability of society.I grew up in a pretty deprived area of the UK, and we all knew "a guy" who could get you access to free cable, or shim your electric line to bypass the meter, or get you pirated CD's and VHS' and whatever.There will always be "that guy down the pub" selling raspberry pi's with some deranged outdated firmware that runs a proxy for everything in the house or whatever. To be honest with you, I might end up being that guy for a bunch of people once I'm laid off from tech like the rest. :)
Normally I would agree with you, but the ability to pull this kind of thing off hinges on there being enough shadows that the Eye doesn't look at for prolonged periods of time. And the overall trajectory of technological advance lately is such that those shadows are rapidly shrinking. First it was the street cameras (and UK is already one of the most enthusiastic adopters in the world). And now comes AI which can automatically sift through all the mined data, performing sentiment analysis etc. I feel that the time will come pretty soon when "a guy" will need to be so adept at concealing the tracks in order to avoid detection that most people wouldn't have access to one.
I wouldn’t worry about it.They can barely handle wolf-whistlers let alone pedophile rape gangs consisting of the lowest IQ dregs of our society.I know it’s only painfully stupid people who think the law is stupid, but dodgy Dave down the way tends to fly under the radar. Otherwise there wouldn’t be so many of them.
Don't worry, you shouldn't underestimate the capability of society.You should be worried. Don't underestimate the capabilities of the government bureaucrats. That "guys down the pub" will quickly disappear once they start getting jail time for their activities.
I think you really overestimate the capability of the UK to enforce laws. Yes, they can write them and yes they can fine large corporations, that's basically it.They cannot enforce laws against such "petty" crimes, the reason society mostly functions in the UK is because most people don't try to break the law.Pretty sure the local punters would kick the cops out if they came for one of their own, especially if he got them their porn back.
It's not just about UK abilities to enforce laws, but also about other factors. The described activities are extremely unattractive as criminal: small market, small margin, the need for planning, preparation and qualification.There is no need for special efforts to enforce the law. Put a few people in jail - and everyone else will quickly find safer and more legal ways to spend their time. No one will do something like that unless they are confident of their impunity.
Yes, it's also dystopian to pin one's future on such hopes. People need to stick it to the government and demand their freedoms. Far too many things are being forced on us in the West that go against fundamental values that have been established for centuries.Somehow, things that could be unifying protests where the working class of every political stripe are able to overlook their differences and push back against government never seem to happen. It is always polarized so that it's only ever one side at a time, and the other side is against them. How does that work?
Reflex. People's opinion on a subject changes if you tell them which political group supports it, sometimes even if they get asked twice in a row. Tribal identity determines ideology more than the other way around for a lot of people.So as soon as Labour comes out for something, Cons are inclined to be against it and so on. The only way to have neutral protests is if no one visibly backs them and they don't become associated with a side, but then how do they get support and organization?
90% of “citizen journalism” is nothing of the sort. Just like “citizen science” researching vaccines.
> 90% of “citizen journalism” is (trash)You're right. But compared to what?I guess 99% of mainstream "journalism" is irrelevant and/or inaccurate, hence citizen journalism is a 10x improvement in accuracy and relevancy! Not 10% better, 900% better! This makes a huge difference to our society as a whole and in our daily lives!But this misses the most important point which is that the user should have the right to choose for themselves what they say and read. Making citizen journalism unduly burdensome deprives everyone of that choice.
Preach comrade!Those citizen journalists with their primary sources, disgusting.Thats nothing but propaganda.Remember it doesnt matter what the video shows, it only matters who showed it to you.
>Remember it doesnt matter what the video shows, it only matters who showed it to youIn an age of mass media (where there's a video for anything) or now one step further synthetic media knowing who makes something is much more important than the content, given that what's being shown can be created on demand. Propaganda in the modern world is taking something that actually happened, and then framing it as an authentic piece of information found "on the street", twisting its context."what's in the video" is now largely pointless, and anyone who isn't gullible will obviously always focus on where the promoter of any material wants to direct the audiences attention to, or what they want to deflect from.
I am just waiting for red states in the US to try this too since their current laws requiring ID verification for porn sites aren’t effective.
> red statesWell you'd be surprised to find out that this stupid policy (and many more) have been brought forward by Labour (Left).
At this point, anyone who has been watching politics for a few decades understands that the left/right dichotomy is primarily one designed to keep the majority of people within a certain set of bounds. We see it revealed when politicians and ideologies that should be in opposition to one another still cooperate on the same strategies, like this one.The goal right now is to make online anonymity impossible. Adult content is the wedge issue being used to make defending it unpalatable for any elected official, but nobody actually has it as a goal to prevent teenagers from looking at porn - if they did, they would be using more direct and efficient strategies.  No, it's very clear that anonymous online commentary is hurting politicians and they are striking back against it.
It has been my impression that in UK, both parties are strongly authoritarian, with the sole difference being what kinds of speech and expression, precisely, they want to police.
Both the major Australian parties (Liberal and Labor) seem as spineless as each other.They're being pushed by media conglomerates News Corp and Nine Entertainment [0] to crush competition (social media apps). With the soon-to-be-introduced 'internet licence' (euphemism: 'age verification'), and it's working. If they ban VPN's, it will make social media apps even more burdensome to access and use.[0] News Corp and Nine Entertainment together own 90% of Australian print media, and are hugely influential in radio, digital and paid and free-to-air TV. They have a lot to gain by removing access to social media apps, where many (especially young) people get their information now days.
I'm currently traveling in Uzbekistan and am surprised that wireguard as a protocol is just blocked. I use wireguard with my own server, because usually governments just block well known VPN providers and a small individual server is fine.It's the first time I've encountered where the entire protocol is just blocked. Worth checking what is blocked and how before deciding which VPN provider to use.
WireGuard by itself has a pretty noticeable network pattern and I don't think they make obfuscating it a goal.There are some solutions that mimic the traffic and, say, route it through 443/TCP.
Wow, kinda crazy to think about a government blocking a protocol that just simply lets two computers talk securely over a tunnel.
Well, think about it - almost every other interaction you can have with an individual in another country is mediated by government. Physical interaction? You need to get through a border and customs. Phone call? Going through their exchanges, could be blocked, easy to spy on with wiretaps. Letter mail? Many cases historically of all letters being opened before being forwarded along.We lived through the golden age of the Internet where anyone was allowed to open a raw socket connection to anyone else, anywhere. That age is fading, now, and time may come where even sending an email to someone in Russia or China will be fraught with difficulty. Certainly encryption will be blocked.We're going to need steganographic tech that uses AI-hallucinated content as a carrier, or something.
Cloak + wireguard should work fine on the server side. The problem is that I didn't find any clients for Android and I doubt there are clients for iOs that can (a) open a cloak tunnel and then (b) allow wireguard to connect to localhost...
A year ago I was traveling through Uzbekistan while also partly working remotely. IKEv2 VPN was blocked but thankfully I was able to switch to SSL VPN which worked fine. I didn't expect that, everything else (people, culture) in the country seemed quite open.
> surprised that wireguard as a protocol is just blocked.Honestly this is the route I'm sure the UK will decide upon in the not too distant future.The job of us hackers is going to become even more important...
XRay protocol based VPN worked for me in Uzbekistan when I were travelling there.Wireguard is indeed blocked.
how can they detect it is wireguard, I thought the traffic is encrypted?how does it differ from regular TLS 1.3 traffic?
It's UDP, not TCP (like TLS) and has a distinguishable handshake. Wireguard is not designed as a censorship prevention tool, it's purely a networking solution.The tunnel itself is encrypted, but the tunnel creation and existence is not obfuscated.
There are many instances of Mastodon, and due to its federated nature, you can use any of them to access it, and even host your own.
Sure, but if you have an account on a different server, you can still see things posted on mastodon.social if you have followed someone there.
It would be easy to block on protocol level. Countries that block VPNs usually progress to that level pretty fast once they discover that simple IP blocks don't work.
Tunneling via SSH (ssh -D) is super easy to detect. The government doesn't need any sophisticated analysis to tell SSH connections for tunneling from SSH connections where a human is typing into a terminal.Countries like China have blocked SSH-based tunneling for years.It can also block sessions based on packet sizes: a typical web browsing session involves a short HTTP request and a long HTTP response, during which the receiving end sends TCP ACKs; but if the traffic traffic mimics the above except these "ACKs" are a few dozen bytes larger than a real ACK, it knows you are tunneling over a different protocol. This is how it detects the vast majority of VPNs.
One alternative would be to set up a VPS, run VNC on it, run your browser on that to access the various web sites, and connect over an SSH tunnel to the VNC instance. Then it actually is an interactive ssh session.
15 years ago, I was using EC2 at work, and realized it was surprisingly easy to SSH into it in a way where all my traffic went through EC2. I could watch local Netflix when traveling. It was a de facto VPN.Details are not at the top of my mind these years later, but you can probably rig something up yourself that looks like regular web dev shit and not a known commercial VPN. I think there was a preference in Firefox or something.
The issue these days is that all of the EC2 IP ranges are well known, and are usually not very high-reputation IPs, so a lot of services will block them, or at least aggressively require CAPTCHAs to prevent botting.Source: used to work for a shady SEO company that searched Google 6,000,000 times a day on a huge farm of IPs from every provider we could find
I watched a season of Doctor Who that way back when the BBC were being precious about it. But Digital Ocean, so $5.
The most effective solution is to use X-ray/V2ray with VLESS, or VMESS, or Trojan as a protocol.Another obfuscated solution is AmneziaIf you are not ready to set up your own VPN server and need any kind of connection right now, try Psiphon, but it's a proprietary centralized service and it's not the best solution.
Nations severing peoples connections to the world is awful. I'm so sorry for the chaos in general, and the state doing awful things both.Go on https://lowendbox.com and get a cheap cheap cheap VPS. Use ssh SOCKS proxy in your browser to send web traffic through it.Very unfancy, a 30+ year old solution, but uses such primitive internet basics that it will almost certainly never fail. Builtin to everything but Windows (which afaik doesn't have an ssh client built-in).Tailscale is also super fantastic.
>  uses such primitive internet basics that it will almost certainly never fail.It already fails in China and Russia. Simply tunneling HTTP through SSH is too easy to detect with DPI.> Windows (which afaik doesn't have an ssh client built-in)It has had both SSH client and SSH server built-in since Win10.
What is going on if you don’t mind my asking? Our local news does not mention anything. Nor does ddging help? Any sources?
Very possible, though many of our users are saying that in network environments where WireGuard is blocked they were able to use Obscura.
Hey, I went to take a look at Obscura and I like the ideas but I can't find the source code.You are making some bold claims but without the source I can't verify those claims.Any plans to open-source it?
Personally, I like Amnezia VPN, it has some ways to work around blocks: https://amnezia.org/en
You can very easily self-host it, their installer automatically works on major cloud platforms.Though if Indonesia has blocked VPNs only now, possibly they only block major providers and don't try to detect the VPN protocol itself, which would make self-hosting any VPN possible.
As someone based in China, it's a bit surprising that techniques used by Chinese people get very few mentions here, while I do think they are quite effective against access blocking, especially after coevolving with GFW for the past decade. While I do hope blocking in Indonesia won't get to GFW level, I will leave this here in case it helps.I found this article [0] summarizing the history of censorship and anti-censorship measures in China, and I think it might be of help to you if the national censorship ever gets worse. As is shown in the article, access blocking in China can be categorized into several kinds: (sorted by severity)1. DNS poisoning by intercepting DNS traffic. This can be easily mitigated by using a DOT/DOH DNS resolver.2. Keyword-based HTTP traffic resetting. You are safe as long as you use HTTPS.3. IP blocking/unencrypted SNI header checking. This will require the use of a VPN/proxy.4. VPN blocking by recognizing traffic signatures. (VPNs with identifiable signatures include OpenVPN and WireGuard (and Tor and SSH forwards if you count those as VPNs), or basically any VPN that was designed without obfuscation in mind.) This really levels up the blocking: if the government don't block VPN access, then maybe any VPN provider will do; but if they do, you will have a harder time finding providers and configuring things.5. Many other ways to detect and block obfuscated proxy traffic. It is the worse (that I'm aware of), but it will also cost the government a lot to pull off, so you probably don't need to worry about this. But if you do, maybe check out V2Ray, XRay, Trojan, Hysteria, NaiveProxy and many other obfuscated proxies.But anyways, bypassing techniques always coevolve with the blocking measures. And many suggestions here by non-Indonesian (including mine!) might not be of help. My personal suggestion is to find a local tech community and see what techniques they are using, which could suit you better.[0] https://danglingpointer.fun/posts/GFWHistory
WireGuard should still work. Tons of different providers. I trust Mullvad but ProtonVPN has a free tier. If they start blocking WireGuard, check out v2ray and xray-core. If those get blocked... that means somehow they're restricting all HTTPS traffic going out of the country
In case known VPN providers are blocked you can pick a small VPS from a hoster like Hetzner and setup your own VPN.
In this scenario, Chinese have very rich experience.
you need to use the advance proxy tool like clash ,v2ray, shadowsocks etc.
shadowsocks was the winner of the state of the art I had to do at work. It address the "long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a stat)" comment.
I live in Pakistan and two years back we had this exact same problem, (election interference) and frankly, you just try to scrape through solutions, but without an answerable government, there is little you can do.We tried things like Proton VPN and Windscribe VPN, as well as enabling MT proxy on Telegram, but soon govts find it easier to just mass ban internet access.Use Netblocks.org to analyse the level of internet blockage and try to react accordingly.
I'd recommend using Outline - it's a one click setup that lets you provision your own VPN on a cloud provider (or your own hardware).Since you get to pick where the hardware is located and it is just you (or you and a small group of friends & family) using the VPN, blocking is more difficult.If you don't want the hassle of using your own hardware you can rent a Digital Ocean droplet for <$5 per month.https://getoutline.org/
I’ve set this up for friends in fairly heavily censored countries before, it has been working well so far, but as others have said, this is a cat and mouse game
Your first option until you get settled is to use an SSH reverse proxy:    ssh -D 9999 user@my.server

Then configure your browser to use local port 9999 for your SOCKS5 proxy.This gets you a temporarily usable system and if you can tunnel this way successfully installing some WireGuard or OpenVPN stuff will likely work.EDIT: Thanks it's -D not -R
“Some demonstrators on Monday were seen on television footage carrying a flag from the Japanese manga series One Piece, which has become a symbol of protest against government policies in the country.”
there is a major protest currently happening due to the legislative body representative just giving themselves a monthly domicile stipend of ~$3300 on top of their salaries (yes, multiple), while the average people earned ~$330 monthly. the information about the protest are not broadcasted on local TVs, so the only spread of information is through social media. i guess since a lot of people went around it using VPN, the gov decided to block it too.
The official word is to counter gambling. Lately the government is not really popular after some decisions that could be interpreted as authoritative, and as citizens have spoken out about it online, causing more voices to join and protests erupting..So well, my guess is they're trying to control it.
AmneziaWG is a decent option for censorship resistance, and it can be installed as a container on your own server.
Launch an EC2 instance in the US region (Ubuntu, open ports 22 and 1194), then connect via SSH and run the OpenVPN install script. Generate the .ovpn profile with the script and download it to your local machine. Finally, import the file into the OpenVPN client and connect to route traffic through the US server.
Usually when countries block websites they don't block major cloud providers, like AWS and Google Cloud. Because most websites are hosted on them. So you can get a cheap VPS from AWS or GCP (always free VM is available) and host OpenVPN on it.
Use the Tor browser window in Brave. It's nowhere near as anonymous as the Tor browser, but the built in ad blocking makes browsing via Tor usable. And that's what you and your compatriots are interested in.Prepare to fill in Cloudflare captchas all day, but that's what it takes to have a bit of privacy nowadays.
You could rent a cheapo instance at a cloud provider and tunnel https over ssh.That’s basically undetectable. Long lived ssh connection? Totally normal. Lots of throughput? Also normal. Bursts throughput? Same.Not sure how to do this on mobile.Tailscale might be an option too (they have a free account for individuals and an exit node out of country nearly bypasses your problem) It uses wireguard which might not be blocked and which comes with some plausible deniability. It’s a secure network overlay not a VPN. It just connects my machines, honest officer.
I'd recommend Obscura because it uses Wireguard over QUIC and it pretty good at avoiding these blocks. It's also open source.
What I'm worried most are that most people are not even aware of what is DNS and how to change it.I can't imagine those who are caught in the chaos with only their phone and unable to access information that could help them to be safe.
Generally speaking, the general population that wants to use blocked services will develop enough technical know-how to circumvent it. The biggest risk is that there are bad actors giving malicious advice and to such learners, looking to defraud or otherwise exploit them.
Set up a VM on AWS/azure/gcp/... in the desired cell, install a VPN server and done. Once you have automation in place it takes ~2 minutes to start, you can run it on demand so you can pay per minute.
In this case the blockage will probably just be up for a few days, until the protests calmed down.Other than that: tor
I was wondering something like this but in a different capacity.What with certain countries (they know who they are) and their hatred for encryption, it got me wondering how people would communicate securely if - for example - Signal/WhatsApp/etc. pulled out and the country wound up disconnecting the submarine cables to "keep $MORAL_PANIC_OF_THE_DAY safe."How would people communicate securely and privately in a domestic situation like that?
In person or not at all.At that point you've essentially lost.You either hope another country sees value in spreading you some democracy, or you rise up and hope others join you.Or not and you accept the protection the state is graciously providing to you.
It is not a real URI... lolThe point was to include something clowns can't filter without incurring collateral costs, and wrapping the ssh protocol in standard web traffic. =3
All the various proxy solutions offered are good (although the simplest ones - like squid - haven't been mentioned yet). You can also use a remote desktop or even just ssh -Y me@remote-server "firefox"
Depending on the circumstances, maybe ditch the landline local ISP for a satellite connection with a foreign ISP?
Make your own VPN using a VPS and something like openvpn.Not every website will allow it, but it should get you access to more than you have now.
Aren't there local (online or print) newspapers to get news from, as an alternative to Discord? Hope I'm not asking a dumb question
In countries where it comes to government blocking/censoring internet traffic, traditional media is cleared of all dissent and fully controlled long before. Last stages of that are happening in my country, Serbia, currently.
Right, that makes sense. Did some looking up and nonfree press seems to be indeed the case for Indonesia: https://rsf.org/en/country/indonesiaIt's a mixed bag apparently, free press is technically legal since 1998 but selective prosecution and harassment of those actually uncovering issues (mainly becomes clear in the last section, "Safety")Tried looking up Serbia next on that website but got a cloudflare block. I'm a robot now...
It's not a dumb question at all. Level on hn really got down lately if you're getting downvoted.Think about it Aachen. If the government has enough power to censor internet traffic, that what was the first thing it censored? Which media is traditionally known for being censored or just speaking propaganda? That's the classical newspapers. It's not uncommon in authoritarian countries for editors to need state to sign off on the day's paper. And if not that, articles are signed and publishers are known. They will auto-censor to avoid problems. Just like creators on YouTube don't comment on this one country's treatment of civilians to avoid problems.
A proxy service like shadow socks works. There are thousands of providers for $X/month for a decent amount of traffic
OP, you can rent a VPS from a reputable and cheap provider within the NA region - OVH, Vultr, Linode etc. are decent. Also check out lowendtalk.comThen, setup Tailscale on the server. You can VPN into it and essentially browse the internet as someone from NA.
From some of the comments here I get why you are downvoted. But tbh I would also have gone that route. So are we just inexperienced? I read here indeed that wireguard is very easily blocked. It was at the company I worked for but then I just set port 23 (who uses ftp anyways??). And it worked. But why is this still bad then?Obviously I have 0 real experience with this.
Android doesn't come with system wide socks proxy support, and i couldn't find an open source app for it either. Is anyone aware of one?Nonetheless this is a surprisingly simple and bullet proof solution: SSH, that's not vpn boss, i need it for work.
Outline is an open source shadowsocks client, and you provision your own server to act as the proxy.  You can use it against any Shadowsocks server you want, and the protocol makes it look like regular https traffic.https://github.com/Jigsaw-Code/outline-appsAndroid & iOS & Linux & Mac & Windowstheir server installer will help set up a proxy for users that aren't familiar with shadowsocks, too
Tor should be pretty good even for environments where they crack down on VPNs, although it can be a bit slow, at least it works.
Yeah, sucks, but really should find better places for people to gather regardless, if you're in that sort of environment.
How is this practical advice in a thread where someone mentions that the clampdown happened without notice?The "shoulda done..." advice isn't useful in the slightest, and I'd argue is malicious with how often it's done simply to satiate a poster's ego.
I can relate to this because my country has an election soon and I'm sure we wont have internet for 3 - 5 days then.
Just please be safe and necessarily paranoidOne way they tend to "solve" workarounds is making examples of people
Use an Actual Private Network? Radio links that you control. Peer with someone who owns a Starlink terminal. Rent instances in GCP's Jakarta datacenter.
Blocking Twitter is a good start, now Facebook, Instagram, Whatsup and TikTok.This is a good start but more should be blocked. Then force ISP to block ads.Not just for Indonesia but all countries. But we still have a lot more to do to fix the web.
The issue with that is where do they draw the line. Next thing you know each country becomes North Korea.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Dedalus Labs (YC S25) – Vercel for Agents]]></title>
            <link>https://news.ycombinator.com/item?id=45054040</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054040</guid>
            <description><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.]]></description>
            <content:encoded><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools – local or hosted by us. No Dockerfiles or YAML configs required.Here’s a demo: https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&t=11Last October, I was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and let’s just say it was super annoying to build… I was thinking to myself the entire time “Why can’t I just pass in `tools=code_execution` to the model and just have it…work?Even with MCP, you’re stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.The code you write looks something like this:  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=["author/server-1”, “author/server-2”],
    model=["openai/gpt-4.1”, “anthropic/claude-sonnet-4-20250514”],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too

Our docs start at https://docs.dedaluslabs.ai. Here’s a simple Hello World example: https://docs.dedaluslabs.ai/examples/01-hello-world. For basic tool execution, see https://docs.dedaluslabs.ai/examples/02-basic-tools. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: https://docs.dedaluslabs.ai/examples/use-case/weather-foreca....There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (we joke that the “S” in MCP stands for “security”). MCP servers right now are expected to act as both the authentication server and the resource server. That is too much to ask of server writers. People just want to expose a resource endpoint and be done.Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve them. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.We aren’t quite yet at the stateful code execution sandbox that I wanted last October, but we’re getting there! Shipping secure and stateful MCP servers is high on our priority list, and we’ll be launching our auth solution next month. We’re also working on an MCP marketplace, so people can monetize their tools, while we handle billing and rev-share.We’re big on open sourcing things and have these SDKs so far (MIT licensed):https://github.com/dedalus-labs/dedalus-sdk-pythonhttps://github.com/dedalus-labs/dedalus-sdk-typescripthttps://github.com/dedalus-labs/dedalus-sdk-gohttps://github.com/dedalus-labs/dedalus-openapiWe would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.Thanks HN!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Service members deserve the right to repair]]></title>
            <link>https://www.militarytimes.com/opinion/2025/07/11/why-service-members-deserve-the-right-to-repair/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054037</guid>
            <description><![CDATA[Opinion: Service members need the tools, parts and authority to make immediate fixes themselves, without having to navigate red tape]]></description>
            <content:encoded><![CDATA[“The generator is down, and we don’t have enough ice to continue icing the remains of soldiers killed in action. How much longer, ma’am?”That’s the message I received while deployed to Balad, Iraq, as an Air Force second lieutenant. I was overseeing generators in theater, and the one powering the mortuary facility had failed. The clock was ticking. I didn’t have HVAC expertise or the necessary parts. The only viable backup generator was on the other side of the country.I had two choices: initiate a long contracting process to hire a civilian technician, or send a convoy across Iraq for the backup unit — risking lives to get it there. We chose the convoy. We got lucky.But what if we hadn’t?That question — what if — has stuck with me. Because this wasn’t a theoretical delay. This was a real moment where a failure in repair readiness jeopardized our ability to care for the fallen with dignity and speed. And I know it wasn’t the only one.That’s why I support the right to repair.I’ve lived what it means when a piece of equipment fails at a critical time. I know how far away contractors can be when you need them most. And I know what our service members are capable of — if we trust them with the tools and training to do the job.Military right to repair means giving service members the ability to fix their own gear — on base, in the field or downrange — without having to wait on outside contractors. That includes access to the tools, parts and manuals they need to do the job. Right now, private companies can put restrictions on military equipment that block troops from making even basic repairs. That slows everything down, costs taxpayers more, and in the worst cases, puts lives at risk.This year, Congress has a chance to change that.Thanks to a growing, bipartisan push — including new legislation led by Senators Elizabeth Warren, D-Mass., and Tim Sheehy, R-Mont. — right-to-repair reforms are being considered in the National Defense Authorization Act (NDAA). These two senators, from opposite parties, have made it clear: The Pentagon is wasting billions, and service members are bearing the cost.Including the Warrior Right to Repair Act in the NDAA is a critical step forward for readiness, national security and the safety of warfighters downrange, and it’s taxpayer friendly. Mission readiness depends on the ability to make repairs in the field. Service members need the tools, parts and authority to make immediate fixes themselves, without having to navigate red tape.When our military relies too heavily on private contractors for basic maintenance, that dependence gives outside companies leverage over military operations, introducing profit motives into urgent repair decisions. Our armed forces should operate on military timelines — not corporate maintenance schedules.Warfighter safety is directly tied to the reliability of our gear. In high-risk environments, a delay in repairing a critical piece of equipment can mean the difference between life and death. Right to repair is smart, responsible fiscal policy. Sustainment costs can represent up to 70% of a weapons system’s lifetime expense. When troops are blocked from doing basic repairs, costs increase and transparency disappears. That’s a waste of taxpayer dollars. Finally, repairing our own equipment is critical to inculcating and maintaining a military culture of adaptability and self-reliance. We train service members to be problem solvers. Empowering them to repair their own gear isn’t just smart policy — it reflects the values we instill in every recruit. Right to repair honors that ethos and ensures we treat service members like professionals, not passive end-users.We wouldn’t tell a Marine they can’t clean their rifle without a manufacturer present. So why are we telling our soldiers, sailors, airmen and Marines they can’t fix a comms system or power supply?I’m grateful that, in that moment in Iraq, we had a backup option — even if it meant launching a risky cross-country convoy. But what if we hadn’t? What if that second generator didn’t exist or wasn’t reachable in time? Without immediate repair options, the remains of our fallen could have become unrecognizable. The families who entrusted us with their loved ones would have suffered consequences that no one should have to imagine.That’s what’s at stake when service members can’t repair the equipment they rely on. It’s not just about saving money or avoiding delays — it’s about honoring our dead, protecting the living and giving our troops the flexibility they need to do the right thing when it counts. Right to repair is about readiness. It’s about humanity. And it’s long overdue. That’s why I’m calling on Congress — and especially House Armed Services Committee leaders like Alabama Republican Rep. Mike Rogers — to include military right-to-repair provisions in this year’s NDAA by supporting the bipartisan Warrior Right to Repair Act.Retired Lt. Col. Cindy Serrano Roberts is a combat veteran after 21 years of service. As a community leader, she has served as a national council member of the United Nations Association (U.S.) and serves as a consultant to the United Nations Human Rights Council and Economic and Social Council. Cindy is a military family policy advocate and has led policy overhauls as an active duty service member and spouse within the Department of Defense. She is a Truman National Security Project political partner and leads their Women’s Affinity Group.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PinePhone Pro [GNU/Linux smartphone] has been discontinued]]></title>
            <link>https://social.treehouse.systems/@pine64/115027515081143369</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053872</guid>
        </item>
        <item>
            <title><![CDATA[China is eating the world]]></title>
            <link>https://apropos.substack.com/p/china-is-eating-the-world</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053771</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Grammit – Local-only AI grammar checker (Chrome extension)]]></title>
            <link>https://chromewebstore.google.com/detail/grammit-the-ai-grammar-ch/pkfmoknmnkbidlniedaloiijibdpjjmm</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053553</guid>
            <description><![CDATA[Check your grammar and refine your writing with local AI.]]></description>
            <content:encoded><![CDATA[OverviewCheck your grammar and refine your writing with local AI.✦ AI-Powered Corrections

Grammit's AI is great at correcting spelling and grammar mistakes. But it also catches other errors. Did you accidentally write "The theory of evolution was developed by Charles Dickens"? No worries, Grammit will correct that to "Charles Darwin".

✦ AI Rephrasing and Drafting

You can ask Grammit to help you with your writing tasks. Just ask it to rephrase your writing to make it more professional and it will do that. Or have it draft an email for you or help you brainstorm ideas.

✦ Privacy-Focused with AI Running on your Computer

Other grammar checkers (*cough* Grammarly, *cough* Quillbot, *cough* LanguageTool) might send your writing to their servers. Your work emails, personal messages, or private notes… yikes! Grammit checks your writing directly on your computer using a local, on-device LLM, It never sends your writing to external servers.

✦ Works Everywhere

Whether you're writing emails, social media posts, or chat messages, Grammit is there helping you write clearly and confidently.

~~~

Is this for me? Support and sales teams, teachers, healthcare professionals, HR, lawyers, students, writers, real estate agents, and more can benefit from Grammit.

If you're tired of mistakes slipping through or uncomfortable with companies reading your writing, it's time to switch to Grammit. It’s free!

Grammit’s on-device local AI is always private. All the grammar checking, rephrasing, and chats with Grammit happen entirely on your computer, using a local LLM without your writing ever being sent to external servers.

Try Grammit today to see your writing improve.DetailsVersion1.5.5UpdatedAugust 25, 2025Size422KiBLanguagesEnglishDeveloperBlaze Today Inc570 Puccini Dr.
Sunnyvale, CA 94087
US Website Email questions@blaze.today Phone +1 415-300-0495TraderThis developer has identified itself as a trader per the definition from the European Union and committed to only offer products or services that comply with EU laws.D-U-N-S111636927PrivacyThe developer has disclosed that it will not collect or use your data. To learn more, see the developer’s privacy policy.This developer declares that your data isNot being sold to third parties, outside of the approved use casesNot being used or transferred for purposes that are unrelated to the item's core functionalityNot being used or transferred to determine creditworthiness or for lending purposesSupportFor help with questions, suggestions, or problems, visit the developer's support site]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimising for maintainability – Gleam in production at Strand]]></title>
            <link>https://gleam.run/case-studies/strand/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053462</guid>
            <description><![CDATA[A case study of Gleam in production at Strand]]></description>
            <content:encoded><![CDATA[Strand is a marketing agency based in London,
UK. The company specialises in copywriting and content creation for many of the
world’s largest enterprise technology companies, running marketing programmes
that produce hundreds of white papers, case studies, blog posts and articles
every year.
Challenge
For many years, Strand has relied on a custom-built project management system
to support the operational aspects of its business—creating projects, tracking
activities and managing documents. However, managing the financial aspects of
project management had always been a more manual process, using spreadsheets to
ensure that billable work was assigned to the correct purchase orders and
invoices.
“Just before the pandemic, we decided to build a new financial management
system,” recalls Ed Kelly, Director of Technology at Strand. “It turned out to
be a very timely decision. When we had to pivot to remote working, the fact
that everyone could track their billable work in a centralised system helped us
keep the business on track.”
The new system quickly became an integral part of Strand’s daily workflow, and
users began requesting new features. As the application gradually grew larger
and more complex, the company’s small development team wanted to ensure that
the system would remain reliable, maintainable and scalable.
“Almost by accident, what we launched as a prototype became a business-critical
application,” says Ed Kelly. “Our development resources are limited, so our top
priority was to make sure the system would just run forever without needing
constant maintenance. At the same time, we also wanted to keep the codebase
simple and approachable, so it’s easy for developers to dive back into when
they need to make a change. The challenge for us was to build and maintain this
business-critical system cost-effectively with our lean development team.”
Solution
As a small business, Strand is not afraid to innovate. “We do have systems that
are written in mainstream programming languages like Python and JavaScript, but
our strategy is to pick the best tool for the job, not just the most popular,”
explains Ed Kelly. “Gleam was a good fit for our requirements.”
The features of Gleam that appealed to Strand were its robustness and
maintainability, its combination of modern language features with access to a
broad ecosystem of battle-tested, production-grade libraries, and its strong
focus on developer experience.
Safety and reliability
“Gleam is a safe language,” explains Ed Kelly. “Broadly speaking, if you write
a program in pure Gleam, it’s guaranteed not to crash. And in cases where you
need to interface with code written in other, less-safe languages, there is a
second layer of protection provided by Gleam’s runtime platform, the BEAM.”
The BEAM was developed by Ericsson in the 1980s as a fault-tolerant platform
for managing large telephone switches that need to handle thousands of calls
simultaneously and can never be taken offline for maintenance. The central idea
is that the platform is able to divide applications into thousands or even
millions of lightweight processes. Each process runs independently, and
processes can communicate by sending messages to each other. If an individual
process crashes, it can be restarted automatically without affecting any of the
other processes.
“The application that we’ve built is composed of several services that interact
with the outside world,” explains Ed Kelly. “For example, we have a service
that periodically downloads currency exchange rates from the UK government’s
website, and another that syncs data with our project management system. The
BEAM ensures that if there’s some unforeseen problem with any of these external
services, it won’t crash our application.”
Modernity and pragmatism
Gleam is designed to be a simple language that provides powerful features while
remaining resolutely practical. “It gives us access to features from more
academic programming languages, but it makes them approachable,” says Ed Kelly.
“The language is small—an experienced developer can learn it in an
afternoon—and there is a strong focus on only having one way to do things. That
means you can onboard new developers into a Gleam codebase quickly.”
Because Gleam code runs on the BEAM, developers also have easy access to
thousands of high-quality software libraries. “The Gleam library ecosystem is
growing rapidly year-on-year,” says Ed Kelly. “And when we need to, we can also
reach for 40 years’ worth of battle-tested libraries written in other BEAM
languages such as Erlang and Elixir. The language prioritises pragmatism over
purity, which helps us get things done.”
Developer experience
In Strand’s experience, Gleam’s developer tools are second to none. “When you
download Gleam, you get all the tooling in a single package,” says Ed Kelly.
“It integrates with your code editor to provide features like formatting,
suggestions and autocomplete. The error messages are really friendly and
helpful—when you make a mistake, Gleam will often tell you what you should have
written. And it’s really fast—the days of going for a coffee break while you
wait for your code to build are over.”
He adds: “We’re heading into a new age of AI-assisted coding, and right now,
it’s difficult to predict how that will play out. But if I had to place a bet,
I would say that in the long run, AIs are more likely to generate high-quality
code in a language like Gleam. Gleam makes it quick and easy for AIs to check
their code, get instant feedback, and iterate. That should be an advantage
compared to languages that are slow to build, have cryptic error messages, and
can’t catch mistakes at build-time.”
Incremental adoption
For Strand, introducing Gleam into its codebase was a low-risk, incremental
process. “We started with just one service—our integration with the UK
government’s currency exchange rate API,” says Ed Kelly. “We were so pleased
with how it turned out that we then rewrote some of our other services in
Gleam. And recently, we’ve decided to give Gleam an even more important role by
replacing the whole part of the backend that talks to our database. We’re very
confident that this will give us a safer and more maintainable codebase
overall.”
Results
As one of the first companies in the world to run Gleam in production, Strand
took a risk. Two years later, the development team is delighted with the
decision. “Since we started, the language has really matured and reached a
stable state,” says Ed Kelly. “The community has grown massively and there’s a
real buzz around the language. It’s even starting to be recognised by
mainstream industry analysts like Thoughtworks in their Technology
Radar. I
think today, Gleam is a safe and solid choice for companies to use in
production.”
Since go-live, the Gleam code within Strand’s application has been rock-solid.
“We’ve had zero Gleam-related crashes, and even when there have been issues
with other parts of the system, the BEAM has kept everything running,” says Ed
Kelly. “We’ve been able to fix problems without our users even noticing that
anything was wrong.”
The simplicity of the language and the sophistication of the development tools
also help to keep the codebase maintainable. “Even when we haven’t looked at
the codebase for a few weeks, it’s easy to get back into it,” says Ed Kelly.
“The language and tooling gently push you to use a consistent, idiomatic style,
and to write clearly and simply without trying to be too clever. So, we don’t
have to spend time puzzling out what our past selves were trying to do with the
code that we wrote six months ago.”
He concludes: “Adopting a new language is always a gamble, but Gleam has paid
off. The belt-and-braces approach to safety and fault-tolerance has given us a
system that just works, reliably, day in and day out, without constant
babysitting and maintenance. For a team like ours, with many other priorities
and projects we need to work on, the confidence that Gleam gives us is worth
its weight in gold.”
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Anything can be a message queue if you use it wrongly enough (2023)]]></title>
            <link>https://xeiaso.net/blog/anything-message-queue</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053234</guid>
            <description><![CDATA[Xe Iaso's personal website.]]></description>
            <content:encoded><![CDATA[ Loading...Please wait a moment while we ensure the security of your connection.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mosh Mobile Shell]]></title>
            <link>https://mosh.org</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053040</guid>
            <description><![CDATA[Mobile shell that supports roaming and intelligent local echo. Like SSH secure shell, but allows mobility and more responsive and robust.]]></description>
            <content:encoded><![CDATA[


  
    
      
        
        (mobile shell)
        Remote terminal application that
        allows roaming, supports intermittent connectivity, and provides intelligent local echo and line editing of user keystrokes.
        Mosh is a replacement for interactive SSH terminals. It's more robust and responsive, especially over Wi-Fi, cellular, and long-distance links.
        Mosh is free software, available for GNU/Linux, BSD, macOS, Solaris, Android, Chrome, and iOS.
      
      
      
    

    Getting Mosh »
    Tech Video »
  

  
  
      
        Change IP. Stay connected.
        Mosh automatically roams as you move between Internet
        connections. Use Wi-Fi on the train, Ethernet in a hotel,
        and LTE on a beach: you'll stay logged in. Most network
        programs lose their connections after roaming,
        including SSH and Web apps like Gmail. Mosh
        is different.
      
      
        Makes for sweet dreams.
        With Mosh, you can put your laptop to sleep and wake it
        up later, keeping your connection intact. If your
        Internet connection drops, Mosh will warn you — but
        the connection resumes when network service
        comes back.
      
      
        Get rid of network lag.
        SSH waits for the server's reply before showing you your
        own typing. That can make for a lousy user interface. Mosh
        is different: it gives an instant response to typing,
        deleting, and line editing. It does this adaptively and
        works even in full-screen programs like emacs and vim. On
        a bad connection, outstanding predictions are underlined
        so you won't be misled.
        
      
      
        No privileged code. No daemon.
        You don't need to be the superuser to install or run
        Mosh. The client and server are executables run by an
        ordinary user and last only for the life of the
        connection.
      
  
  
      
        Same login method.
        Mosh doesn't listen on network ports or authenticate
         users. The mosh client logs in to the server via
         SSH, and users present the same credentials (e.g.,
         password, public key) as before. Then Mosh runs the
         mosh-server remotely and connects to it over UDP.
      
      
        Runs inside your terminal, but better.
        Mosh is a command-line program, like ssh. You can use it
        inside xterm, gnome-terminal, urxvt, Terminal.app, iTerm,
        emacs, screen, or tmux. But mosh was designed from scratch
        and supports just one character set: UTF-8. It fixes Unicode
        bugs in other terminals and in SSH.
      
      
        Control-C works great.
        Unlike SSH, mosh's UDP-based protocol handles packet loss
          gracefully, and sets the frame rate based on network conditions. Mosh
          doesn't fill up network buffers, so Control-C always works
          to halt a runaway process.
      
  
  

      

  

  
    October 31, 2022: Mosh 1.4.0 released, with Alex Chernyakhovsky and Benjamin Barenblat as release managers, and major contributions
      from Wolfgang Sanyer, John Hood, Anders Kaseorg, and Andrew Chin. The release adds true-color support as well as other features, bug fixes, and fuzzing infrastructure.

    September 29, 2017: The
	macOS Homebrew
	collection renames its formula from “mobile-shell” to
	“mosh”.

    July 21, 2017: Mosh 1.3.2 released, with John Hood as
      release lead. The release includes improved tests, bug fixes,
      and improvements to IPv6 support on non-Linux systems.

    March 25,
2017: Mosh
1.3.0 released, with John Hood as release lead. The release
includes broader platform compatibility, robustness improvements,
better testing, and fixes for excess CPU consumption in some cases. We
have switched to semver.org-style versioning and will increment the
minor version number whenever we add new functionality. (In our
previous practice, this release would probably have been called
“1.2.7.”)

    September 20, 2016: Blink Shell: Mosh & SSH Terminal for iOS has its first gold release on the App Store. Free version is available on GitHub.
    August 10,
2016: Mosh
1.2.6 released, with John Hood as release lead. New features
include huge performance improvements, especially on large terminals,
the ability to set a timeout to end dormant sessions automatically,
and support for crypto libraries other than OpenSSL.

August 10, 2016: The Mosh website moves to https://mosh.org. We continue to be grateful for hosting provided
by the MIT Student Information Processing Board.

    June 15, 2016: Mosh for iOS (Blink) has its first alpha release.

    April 17, 2016: Termux (open source Linux environment for Android) adds a mosh 1.2.5 package.
    July 23, 2015: Mosh 1.2.5 released, with John Hood as release lead. New features include support for mouse modes and a reconfigurable escape character, and initial support for IPv6.
    May 31, 2015: Another team of Stanford students has reproduced some of the Mosh research paper's results.
    January 20, 2014: Mosh for Chrome, which brings Mosh to the Chrome browser and Chrome OS, is released. It can be installed here.
    August 9, 2013: JuiceSSH (SSH client for Android) adds official Mosh support — available on the Play Store
    April 14, 2013: Mosh has posted an Ideas List for interested contributors!
    March 27,
    2013: Mosh
    1.2.4 has been released. Changes largely include bug
    fixes, improved robustness, and added platform support (now
    on AIX and stock Solaris!). This version will be in Ubuntu
    13.04 (raring).
    March 24, 2013: The Debian Project Leader switches to Mosh. Welcome, Stefano! We're proud to have you.
    March 14, 2013: Two teams of Stanford students have
    reproduced parts of the Mosh research paper on Stanford's
    Reproducing Network Research blog. Kanthi Nagaraj and Emily McMilin tested SSP's resilience to packet loss, and Ahmed Aljunied and Anand Atreya evaluated Mosh's predictive local echo.
    March 12, 2013: Mosh celebrates its first anniversary of
1.0. Hard to believe it's already been a year. We could not have done
it without the hard work of many of you, especially Hari Balakrishnan,
Keegan McAllister, Anders Kaseorg, Quentin Smith, Richard Tibbetts,
Nelson Elhage, Christine Spang, Stefie Tellex, Joseph Sokol-Margolis,
Waseem Daher, Bill McCloskey, Austin Roach, Greg Hudson, Karl Ramm,
Alexander Chernyakhovsky, Peter Iannucci, Evan Broder, Neha Narula,
Katrina LaCurts, Ramesh Chandra, Peter Jeremy, Ed Schouten, Ryan
Steinmetz, Jay Freeman, Dave Täht, Larry Doolittle, Daniel Drown, Timo
Juhani Lindfors, Timo Sirainen, Ira Cooper, Felix Gröbert, Luke
Mewburn, Anton Lundin, Kevin Ballard, and Axel Beckert!
    November 2012: Mosh on the cover of Linux Magazine.
    Oct. 19,
    2012: Mosh
    1.2.3 has been released. Changes include more resilience to
evil NATs, power savings for mobile clients, switching to OpenSSL's AES
implementation, and a licensing exception to allow Mosh on Apple's app store.
This version will be in Debian 7.0 (wheezy).
    Aug. 22, 2012: Mosh (and its tolerance for high
packet loss) helps Iain Learmonth escape from an elevator.
  



  

  
  

  
  
    
      Windows
      
      There is no "native" mosh executable for Windows available at this time. The Chrome version of Mosh is the easiest way to use mosh on Windows.
    
  
    
      Cygwin
      C:\> setup.exe -q mosh
      Mosh on Cygwin uses OpenSSH and is suitable for Windows users with advanced SSH configurations.
	  
	  Mosh is not compatible with Cygwin's built-in Windows Console terminal emulation.  You will need to run Mosh from a full-featured terminal program such as mintty, rxvt, PuTTY, or an X11 terminal emulator.
    
  

  
  

  
  
  
  
      Ubuntu 12.04 and later
            $ sudo apt-get install mosh
      The ppa:keithw/mosh-dev PPA tracks the development version of Mosh.
      
    


  
  

  
  
      OpenCSW Solaris 10 Update 8 or later
      # pkgutil -i mosh
      
    

  Operating system logos are trademarks or registered trademarks and are displayed for identification
only. The vendors shown aren't affiliated with and haven't endorsed Mosh.

  

  
    
          Dependencies
	  debian/control (in Git) includes an authoritative list of build dependencies.
          
                  NameTypical package
                  Perl (5.14 or newer)perl
		  Protocol Buffersprotobuf-compiler, libprotobuf-dev
                  ncurseslibncurses5-dev
                  zlibzlib1g-dev
                  utempter (optional)libutempter-dev
            OpenSSLlibssl-dev
          
	  Additionally, pkg-config is a build-only dependency on most systems.
        
    
            Security on new operating systems
            
              Note that mosh-client receives an AES session key as an environment
              variable.  If you are porting Mosh to a new operating system, please make sure that a
              running process's environment variables are not readable by other users.  We have
              confirmed that this is the case on GNU/Linux, OS X, and FreeBSD.
            
      
  



  

  Replaces interactive SSH. Instant keystroke response, robust to roaming. But you'll need working UDP.

  
    
      Typical usage
      $ mosh chewbacca.norad.mil
      Mosh will log the user in via SSH, then start a connection on a UDP port between 60000 and 61000.
    

    
      Different username
      $ mosh potus@ackbar.bls.gov
    

    
      Server binary outside path
      $ mosh --server=/tmp/mosh-server r2d2
      The user can specify an alternate path for the mosh-server on the remote host. The server binary can even
        be installed in the user's home directory.
    
  

  

    
      Selecting Mosh UDP port
      $ mosh -p 1234 darth
      Useful when the server is behind a port-forwarder or NAT.
    

          
            Selecting SSH port
            $ mosh --ssh="ssh -p 2222" figrindan
          

          
            Other SSH options
            $ mosh --ssh="~/bin/ssh -i ./identity" fett
          

        

  

    
      Disable instant echo
      $ mosh --predict=never niennunb
      The -n switch is a synonym. By contrast,
      passing --predict=always or -a
      will enable instant local echo even on low-delay
      links.
    

    
      With a command
      $ mosh pello -- screen -dr
      This reattaches to a long-running screen session.
    
  

  Ending the connection

  Normally, logout or exit on the remote host will close
    the session. Mosh accepts the escape sequence Ctrl-^
    .  (typically typed with Control-Shift-6, then a
    period) to end the connection forcibly. To send a
    literal Ctrl-^, type Ctrl-^ ^.

  Manual

  More details can be found in
  the mosh(1), mosh-client(1),
  and mosh-server(1) manual pages.




  
  

  Papers

  
    
      The  Mosh research paper describes the
        design and evaluation of Mosh in more detail than you may
        want.
        The paper was presented at the
      2012 USENIX
        Annual Technical Conference, held June 13–15, 2012, in
        sunny Boston, Mass.

      In addition,
the  Mosh: A State-of-the-Art Good
Old-Fashioned Mobile Shell essay gives further information about
the design principles behind Mosh, including the "prophylactic
retransmission" technique. The essay was published
in USENIX
;login: magazine, August 2012.

      “ISO 2022 locking escape
    sequences oh flying spaghetti monster please kill me
    now.” — actual USENIX peer review from the
            Mosh paper.
        

        (Why you should trust Mosh with your remote terminal needs: we
        worry about details so obscure, even USENIX reviewers don't want to
            hear about them.)

      
    

    
      
    
  

  How Mosh works

  
  Remote-shell protocols traditionally work by conveying a
    byte-stream from the server to the client, to be interpreted
    by the client's terminal. (This includes TELNET, RLOGIN, and
    SSH.) Mosh works differently and at a different layer. With
      Mosh, the server and client both maintain a snapshot of
      the current screen state. The problem becomes one of
      state-synchronization: getting the client to the
      most recent server-side screen as efficiently as
      possible.

    This is accomplished using a new protocol called the
      State Synchronization Protocol, for which Mosh is the
      first application. SSP runs over UDP, synchronizing the
      state of any object from one host to another. Datagrams
      are encrypted and authenticated
      using AES-128
      in OCB3 mode. While SSP takes care of the networking
      protocol, it is the implementation of the object being
      synchronized that defines the ultimate semantics of the
      protocol.

    Roaming with SSP becomes easy: the client sends datagrams
      to the server with increasing sequence numbers, including
      a "heartbeat" at least once every three seconds. Every time
      the server receives an authentic packet from the client
      with a sequence number higher than any it has previously
      received, the IP source address of that packet becomes the
      server's new target for its outgoing packets. By doing
      roaming “statelessly” in this manner, roaming works in and
      out of NATs, even ones that may themselves be
      roaming. Roaming works even when the client is not aware
      that its Internet-visible IP address has changed. The
      heartbeats allow Mosh to inform the user when it hasn't
      heard from the server in a while (unlike SSH, where users
      may be unaware of a dropped connection until they try to
      type).

    Mosh runs two copies of SSP, one in each direction of the
      connection. The connection from client to server
      synchronizes an object that represents the keys typed by
      the user, and with TCP-like semantics. The connection from
      server to client synchronizes an object that represent the
      current screen state, and the goal is always to convey the
      client to the most recent server-side state, possibly
    skipping intermediate frames.

    Because SSP works at the object layer and can control the
      rate of synchronization (in other words, the frame rate),
      it does not need to send every byte it receives from the
      application. That means Mosh can regulate the frames so as
      not to fill up network buffers, retaining the
      responsiveness of the connection and making sure Control-C
      always works quickly. Protocols that must send every byte
      can't do this.

  Careful terminal emulation

    One benefit of working at the terminal layer
      was the opportunity to build a clean UTF-8 terminal
      emulator from scratch. Mosh fixes several Unicode bugs in
      existing terminals and in SSH, and was designed as a fresh
      start to try to be robust and correct even for
      pathological inputs.


  Tricky unicode

  Only Mosh and the OS X Terminal correctly handle a Unicode combining character in the first column.
    
      
        xterm: circumflex on wrong letter.
        GNOME Terminal: no circumflex at all.
      
      
        OS X Terminal.app gets it right.
        Mosh gets it right too.
      
    
  

  ISO 2022 locking escapes

  Only Mosh will never get stuck in hieroglyphs when a nasty program writes to the terminal. (See Markus Kuhn's discussion of the relationship between
ISO 2022 and UTF-8.)

    
      
        xterm
        GNOME Terminal
      
      
        OS X Terminal.app
        Mosh
      
    
  

  Evil escape sequences

  Only Mosh and GNOME Terminal have a defensible rendering when
Unicode mixes with an ECMA-48/ANSI escape sequence. The OS X Terminal
unwisely tries to normalize its input before the vt500 state machine,
causing it to misinterpret and become unusable after receiving the
following input!* (This also means the OS X Terminal's interpretation
of the incoming octet stream varies depending on how
the incoming octets are split across TCP segments, because the
normalization only looks ahead to available bytes.)

* We earlier wrote that this misbehaving sequence "crashes"
the OS X Terminal.app. This was mistaken—instead, Terminal.app
interprets the escape sequence as shutting off keyboard input, and
because of an unrelated bug in Terminal.app, it is not possible for
the user to restore keyboard input by resetting the terminal from the
menu.

    
      
        xterm: circumflex on wrong letter.
        GNOME Terminal's circumflex placement is defensible.
      
      
        OS X Terminal.app applies circumflex to part of escape sequence, then irretrievably shuts off keyboard input.
        Mosh gets this one right.
      
    
  

  Mosh sets IUTF8

  In the POSIX framework, the kernel needs to know whether
    the user is typing in an 8-bit character set or in UTF-8,
    because in canonical mode (i.e. "cooked" mode), the kernel
    needs to be able to delete a typed multibyte character
    sequence from an input buffer.  On OS X and Linux, this is
    done with the "IUTF8" termios flag.)
    (See diagnostic
        explaining the need for this flag.)

    Mosh sets the IUTF8 flag when possible and stubbornly refuses to start up unless the user has a
    UTF-8-clean environment. SSH does not set the IUTF8 flag, which can lead to garbage in input buffers.
  


  Instant local echo and line editing

    The other major benefit of working at the
      terminal-emulation layer is that the Mosh client is free
      to scribble on the local screen without lasting
      consequence. We use this to implement intelligent local
      echo. The client runs a predictive model in the background
      of the server's behavior, hypothesizing that each
      keystroke will be echoed at the cursor location and that
      the backspace and left- and right-arrow keys will have
      their traditional effect. But only when a prediction is
      confirmed by the server are these effects actually shown
      to the user. (In addition, by default predictions are only
      displayed on high-delay connections or during a network
      “glitch.”) Predictions are done in epochs: when the
      user does something that might alter the echo behavior
      — like hit ESC or carriage return or an up- or
      down-arrow — Mosh goes back into making background
      predictions until a prediction from the new batch can be
      confirmed as correct.

    Thus, unlike previous attempts at local echo with TELNET
      and RLOGIN, Mosh's local echo can be used everywhere, even
      in full-screen programs like emacs and vi.

  Real-world benefits

We evaluated Mosh using traces contributed by six users, covering
about 40 hours of real-world usage and including 9,986 total
keystrokes. These traces included the timing and contents of all
writes from the user to the host and vice versa. The users were asked
to contribute "typical, real-world sessions." In practice, the traces
include use of popular programs such as the bash shell and zsh shells,
the alpine and mutt e-mail clients, the emacs and vim text editors,
the irssi and barnowl chat clients, the links text-mode Web browser,
and several programs unique to each user.

To evaluate typical usage of a "mobile" terminal, we replayed the
traces over an otherwise unloaded Sprint commercial EV-DO (3G)
cellular Internet connection in Cambridge, Mass. A client-side process
played the user portion of the traces, and a server-side process
waited for the expected user input and then replied (in time) with the
prerecorded server output. We speeded up long periods with no
activity.  The average round-trip time on the link was about half a
second.

We replayed the traces over two different transports, SSH and Mosh,
and recorded the user interface response latency to each simulated
user keystroke. The Mosh predictive algorithm was frozen prior to
collecting the traces and was not adjusted in response to their
contents or results.

The results

  Cumulative distribution of keystroke response times with Sprint EV-DO (3G) Internet service


Mosh reduced the median keystroke response
time from 503 ms to nearly instant (because more than 70% of the
keystrokes could be immediately displayed), and reduced the mean
keystroke response time from 515 ms to 173 ms. Qualitatively, Mosh makes
remote servers "feel" more like the local machine!

    


      
  
  Q: Who wrote Mosh?

  Mosh was written by Keith Winstein, along with Anders Kaseorg, Quentin Smith, Richard Tibbetts, Keegan McAllister, and John Hood.

  Q: Why another remote-terminal protocol?

  Practical latency on the Internet is on the increase, with
    the rise of bufferbloat and sophisticated wireless links
    that optimize for throughput over delay. And roaming is more
    common than ever, now that laptops and handheld devices have
    largely displaced desktops. SSH is great, but frustrating to
    use when you want to change IP addresses or have a
    long-delay link or a dodgy connection.

  Moreover, TELNET had some good things going for it — a
    local-echo mode and a well-defined network virtual
    terminal. Even today, SSH doesn't properly support UTF-8
    end-to-end on a POSIX system.

  Q: Are the mosh principles relevant to other network applications?

  We think so. The design principles that Mosh stands for are
    conservative: warning the user if the state being displayed
    is out of date, serializing and checkpointing all
    transactions so that if there are no warnings, the user
    knows every prior transaction has succeeded, and handling expected events (like roaming from one
    WiFi network to another) gracefully.

  Those don't seem too controversial, but fancy apps like
  Gmail-in-Chromium or on Android still behave atrociously on
  dodgy connections or after switching IP addresses. (Have you
  ever had Gmail leave an e-mail message in "Sending..." for ten
  hours while merrily retrieving new mail and not indicating any
  kind of error? Us too.) We think there may be considerable
  room for improvement in many network user interfaces from the
  application of these values.

  

  Q: I'm getting "mosh requires a UTF-8 locale." How can I fix this?

        To diagnose the problem, run locale on the local
        terminal, and ssh remotehost locale. To use Mosh,
        both sides of the connection will need to show a UTF-8 locale, like
        LC_CTYPE="en_US.UTF-8".

        On many systems, SSH will transfer the locale-related
        environment variables, which are then inherited by
        mosh-server.  If this mechanism fails, Mosh (as of
        version 1.2) will pass the variables itself.  If neither
        mechanism is successful, you can do something like

        mosh remotehost --server="LANG=en_US.UTF-8 mosh-server"

        If en_US.UTF-8 does not exist on the remote server,
        you can replace this with a UTF-8 locale that does exist.  You
        may also need to set LANG locally for the benefit of
        mosh-client.  It is possible that the local and
        remote machines will need different locale names. See also this GitHub
        ticket.

  Q: What does the message "Nothing received from the server on UDP port 60003" mean?

  
    This means that mosh was able to start
    mosh-server successfully on the remote machine, but the client is
    not able to communicate with the server.  This generally means that
    some type of firewall is
    blocking the UDP packets between the client and the server.  If you
    had to forward TCP port 22 on a NAT for SSH, then you will have to
    forward UDP ports as well.  Mosh will use the first available
    UDP port, starting at 60001 and stopping at 60999.  If you are only
    going to have a small handful of concurrent sessions on a server, then you can
    forward a smaller range of ports (e.g., 60000 to 60010).
    

    
    Tools like netstat, netcat, socat, and tcpdump can be useful for debugging
    networking and firewall problems.
    

    This problem can also be the result of a bug in glibc 2.22 that
      affects programs that link with protobuf and utempter and use
      aggressive compiler hardening flags. (glibc bugtracker entry, as well as Mosh bugtracker entry.) The
      problem causes mosh-server to segfault immediately on startup. We
      believe we have worked around this problem in Mosh 1.2.6, but please
      report a bug if you find otherwise.
  

  Q: Why do you insist on UTF-8 everywhere?

  We're really not UTF-8 zealots. But it's a lot easier to
  correctly implement one terminal emulator
  than to try to do the right thing in a variety of difficult
  edge cases. (This is what GNU screen tries to do, and in our
  experience it leads to some very tricky-to-debug situations.)
  So mosh just won't start up until the user has everything
  configured for a UTF-8-clean pathway. It may be annoying, but
  it also probably reduces frustration down the
  road. (Unfortunately an 8-bit vt220 and a UTF-8 vt220 are
  different and incompatible terminal types; the UTF-8 goes
  in underneath the vt220 state machine.)

  Q: How do I use a different SSH port (not 22)?

        As of Mosh 1.2, you can pass arguments to ssh like so:

        mosh remotehost --ssh="ssh -p 2222"

        Or configure a host alias in ~/.ssh/config with a
        Port directive.  Mosh will respect that too.

  Q: I'm getting 'mosh-server not found'.

  Please make sure that mosh is installed on the client, and
  mosh (or at least mosh-server) is installed on the server you are
  trying to connect to.  Also, the server is expected to be available
  on your server's default login PATH, which is not
  usually true on OS X and BSD servers, or if you install mosh-server
  in your home directory.  In these cases please see the "Server
  binary outside path" instructions in the Usage section,
  above.

  Q: SSH authenticates using Kerberos tickets, but Mosh asks me for a password.
        In some configurations, SSH canonicalizes the hostname
        before passing it to the Kerberos GSSAPI plugin.  This breaks
        for Mosh, because the initial forward DNS lookup is done by
        the Mosh wrapper script.  To work around this, invoke Mosh as

        mosh remotehost --ssh="ssh -o GSSAPITrustDns=no"

        This will
        often fail on a round-robin DNS setup.  In that case it is probably
        best to pick a specific host from the round-robin pool.

  Q: Why is my terminal's scrollback buffer incomplete?

        Mosh synchronizes only the visible state of the terminal.  We
are tracking this issue; see this issue and the
        others which are linked from there.  For now, the workaround is to use
        screen or tmux on the remote side.

  Q: How do I get 256 colors?

  Make sure you are running mosh in a terminal that
  advertises itself as 256-color capable. (This generally means
  TERM will be xterm-256color or screen-256color-bce.)

  Q: How do I type C-^, Mosh's default escape character?

  On keyboards with the United States layout, this can be typed
  as Ctrl-Shift-6, or often as Ctrl-6 (this depends on your OS and
  terminal emulator).  On non-US keyboards, it is often hard to find
  the right key, and sometimes it's not available at all.  If your
  keyboard has a dead key with an accent-circumflex, this is not
  likely to be the right key.  Ctrl-6 sometimes works, though.  If you
  are unable to type this character, you will need to set
  the MOSH_ESCAPE_KEY variable; see the Mosh man page for
  details.

  Q: How can I make the server automatically clean up dormant sessions?

  Please see the entries for MOSH_SERVER_NETWORK_TMOUT
      and MOSH_SERVER_SIGNAL_TMOUT in the mosh-server(1) man page.

  Q: What is Mosh's security track record so far?

  Mosh 1.0 was released in March 2012. As of the release of Mosh
      1.4.0 in October 2022, as far as the developers are aware:

      
      
      In the last decade, no security vulnerabilities of
	any kind (major or minor) have been reported in Mosh.

      No major security vulnerabilities have ever been
      reported in Mosh. We define major security vulnerabilities to
      include privilege escalation, remote code execution,
      denial-of-service by a third party, etc.

      Two denial-of-service issues were discovered and fixed in
	releases in 2012. One issue allowed a mosh-server to cause the
	mosh-client to spend excess CPU (CVE-2012-2385, fixed in Mosh
	1.2.1, released May 2012). Another issue allowed the server
	host to cause the mosh-client to send UDP datagrams to an
	incorrect address, foiling its attempt to connect (fixed in
	Mosh 1.2.3, released October 2012).
      

      Q: How does Mosh's security compare with SSH's?
      
    We think that Mosh's conservative design means that its attack
      surface compares favorably with more-complicated systems like
      OpenSSL and OpenSSH. Mosh's track record has so
      far borne this out. Ultimately, however, only time will tell
      when the first serious security vulnerability is discovered in
      Mosh—either because it was there all along or because it
      was added inadvertently in development. OpenSSH and OpenSSL have
      had more vulnerabilities, but they have also been released
      longer and are more prevalent.

    In one concrete respect, the Mosh protocol is more secure than
      SSH's: SSH relies on unauthenticated TCP to carry the contents
      of the secure stream. That means that an attacker can end an SSH
      connection with a single phony "RST" segment. By contrast, Mosh
      applies its security at a different layer (authenticating every
      datagram), so an attacker cannot end a Mosh session unless the
      attacker can continuously prevent packets from reaching
      the other side. A transient attacker can cause only a transient
      user-visible outage; once the attacker goes away, Mosh will
      resume the session.

    However, in typical usage, Mosh relies on SSH to exchange keys
      at the beginning of a session, so Mosh will inherit the
      weaknesses of SSH—at least insofar as they affect the
      brief SSH session that is used to set up a long-running Mosh
      session.

  Q: Is Mosh affected by the 2018 attacks against the OCB2 cipher mode?

  Not that we know of—Mosh uses OCB3. The authors of the
paper write that the attack is not applicable to OCB3.

  Q: Why does mosh use AES-128 for a session-key, not AES-192 or AES-256?

  
    
      AES-128 is more than an adequate key length for a session key.

      The OCB FAQ recommends AES-128.

      AES-128 is a bit nicer and is not subject to the related-key attacks that afflict AES-192 and AES-256. (Schneier: "the key schedule for 256-bit version is pretty lousy -- something we pointed out in our 2000 paper -- but doesn't extend to AES with a 128-bit key." See this blog post.)
    
  

  Q: Does mosh work with Amazon EC2?

  Yes, it works great, but please remember to open up UDP ports 60000–61000 on the EC2 firewall.

  Q: How do I tell if mosh is working correctly?

  
      After you run mosh user@server, if successful you will be dropped into your login
      shell on the remote machine.

      If you want
      to check that mosh is being used instead of ssh, try typing Ctrl-^ Ctrl-Z
      to suspend the session (with mosh 1.2.4 or later on the client). Running fg will then return.
      
  

  Q: What's the difference between mosh, mosh-client, and mosh-server?   Which one do I use?

  
      The mosh command is a wrapper script that is designed to be the primary way that
      you use mosh.  In most cases, you can simply just replace "ssh" with "mosh" in your command line.
      Behind the scenes, the mosh wrapper script will SSH to the server, start up
      mosh-server, and then close the SSH connection.  Then it will start up the
      mosh-client executable on the client, passing it the necessary information for
      it to connect to the newly spawned mosh-server instance.
      

      In normal usage, mosh-client and
      mosh-server don't need to be run directly.
      
  

  Q: How do I run the mosh client and server separately?

  If the mosh wrapper script isn't working for you, you can try running
    the mosh-client and mosh-server programs separately to
    form a connection. This can be a useful debugging technique.

  1. Log in to the remote host, and run mosh-server.

  It will give output like:
$ mosh-server 

MOSH CONNECT 60004 4NeCCgvZFe2RnPgrcU1PQw

mosh-server (mosh 1.1.3)
Copyright 2012 Keith Winstein <[email protected]>
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

[mosh-server detached, pid = 30261]


  2. On the local host, run:
$ MOSH_KEY=key mosh-client remote-IP remote-PORT

where "key" is the 22-byte string printed by mosh-server (in this
example, "4NeCCgvZFe2RnPgrcU1PQw"), "remote-PORT" is the port number
given by the server (60004 in this case), and "remote-IP" is the IP address of the server. You can look up the
server's IP address with "host remotehost".

  3. If all goes well, you should have a working Mosh connection. Information about where the process fails can help us debug why Mosh isn't working for you.

  Q: With the mosh-server on FreeBSD or OS X, I sometimes get weird color problems. What's wrong?

  This bug is fixed in Mosh 1.2. Thanks to Ed Schouten and Peter Jeremy for tracking this down.

  Q: How do I contribute to mosh?

  We welcome your contribution! Please join us in #mosh channel on Libera Chat IRC, visit us on GitHub,
  or email [email protected].  To contribute to our code base, please fork the repository on GitHub and open a pull request there.

  Q: Who helped with mosh?

  We're very grateful for assistance and support from:

    

Hari Balakrishnan, who advised this work and came up with the name.

Paul Williams, whose reverse-engineered vt500 state diagram is the basis for the Mosh parser.

The anonymous users who contributed session logs for tuning and measuring Mosh's predictive echo.

Nickolai Zeldovich for helpful comments on the Mosh research paper.

Richard Stallman for helpful discussion about the capabilities of the SUPDUP Local Editing Protocol.

Nelson Elhage

Christine Spang

Stefie Tellex

Joseph Sokol-Margolis

Waseem Daher

Bill McCloskey

Austin Roach

Greg Hudson

Karl Ramm

Alexander Chernyakhovsky

Peter Iannucci

Evan Broder

Neha Narula

Katrina LaCurts

Ramesh Chandra

Peter Jeremy

Ed Schouten

Ryan Steinmetz

Jay Freeman

Dave Täht

Larry Doolittle

Daniel Drown

Timo Juhani Lindfors

Timo Sirainen

Ira Cooper

Felix Gröbert

Luke Mewburn

Anton Lundin

Philipp Haselwarter

Timo J. Rinne

Barosl Lee

Andrew Chin

Louis Kruger

Jérémie Courrèges-Anglas

Pasi Sjöholm

Richard Woodbury

Igor Bukanov

Geoffrey Thomas

Steve Dignam

HIGUCHI Yuta

Baruch Siach






  
      [email protected]
        Mosh development and discussion.

        Sign up or view archives at https://mailman.mit.edu/mailman/listinfo/mosh-devel.

      [email protected]
        Mosh user discussion and site best practices.

        Sign up or view archives at https://mailman.mit.edu/mailman/listinfo/mosh-users.

      #mosh channel on Libera IRC

              You can connect with a
              Web client, try an irc:// URL,
              or manually configure your client for irc.libera.chat.

      At the recommendation of the security community, confidential security-related matters may be sent to: [email protected]

        
    Messages may optionally be encrypted with Keith Winstein's public key:
pub   rsa4096 2012-02-05 [SC] [expires: 2025-02-27]
      B1A4 7069 121F 6642 BB3D  7F3E 20B7 283A FE25 4C69
        

      
    

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How to install TrueNAS on a Raspberry Pi]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/how-install-truenas-on-raspberry-pi</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052429</guid>
            <description><![CDATA[Now that Joel0 in the TrueNAS community has created a fork of TrueNAS that runs on Arm, I thought I'd give it a spin—on a Raspberry Pi.]]></description>
            <content:encoded><![CDATA[Now that Joel0 in the TrueNAS community has created a fork of TrueNAS that runs on Arm, I thought I'd give it a spin—on a Raspberry Pi.



I currently run an Ampere Arm server in my rack with Linux and ZFS as my primary storage server, and a Raspberry Pi with four SATA SSDs and ZFS as backup replica in my studio. My configuration for these Arm NASes is up on GitHub.

I've been looking forward to TrueNAS support on Arm for years, though it seems the sentiment in that community was 'Arm servers aren't powerful enough to run serious storage servers'—despite myself and many others doing so for many years... but that's besides the point.

On a Raspberry Pi?

Yes, in fact.

I've found numerous times, running modern applications on slower hardware is an excellent way to expose little configuration flaws and misconceptions that lead to learning how to run the applications much better on more capable machines.



From my Pi Dramble to my Petabyte Pi Project, running apps intended for much more powerful hardware taught me a lot. So maybe running TrueNAS, which demands 8 GB of RAM and 16 GB of primary storage, would be a fun learning exercise.

I've done it on x86 servers, but that's boring. It's easy. I don't learn much when a project goes off without a hitch, and I'm not forced to look closer at some of the configuration quirks.

You can watch the video for a full demo, or read on below:





On a Raspberry Pi, there's no UEFI

One glaring problem with the Raspberry Pi is no official support for UEFI, a standard way to boot computers and interface operating systems to device firmware. Raspberry Pi only officially supports device-tree-based Linux booting, which is much less standard. That means you can't just throw any old Linux distribution on the Pi, you have to have ones tailored to the Pi. There are good OSes for the Pi, like Raspberry Pi OS, based on Debian. But it's not the same as grabbing Windows on Arm and installing it on my Ampere workstation.

To get past this restriction, we have to rely on a community project, forked from Windows on Raspberry Pi. Specifically, I'm using NumberOneGit's rpi5-uefi fork.

To get your Pi 5 to support UEFI (CM5 process may be slightly different):


Update the EEPROM to the 2025-06-09 release (or later - check what version you're running in Pi OS with the command rpi-eeprom-update):
 a. Typically, you can upgrade using Raspberry Pi Imager, sudo apt full-upgrade -y, or sudo rpi-eeprom-update -a. However, at the time of this writing, those methods will get you to the latest stable release (2025-05-08), so until then, use one of these methods:
 b. Manually update the bootloader with usbboot from source.
 c. Switch to the beta bootloader release channel: sudo nano /etc/default/rpi-eeprom-update, then change latest to beta, and run sudo rpi-eeprom-update -a.
 d. Verify the bootloader version you're running with rpi-eeprom-update after a reboot.
Download the latest .zip file release from rpi5-uefi Releases.
Take a microSD card that's already formatted for the Pi (I just pulled the Pi OS card out of my Pi 5 that I just used for the EEPROM update), and clear out the contents of the FAT32 'bootfs' volume. Copy all the contents of the .zip file you downloaded into that folder (including RPI_EFI.fd).
Eject the microSD card, insert it into the Pi, and power it on with an HDMI display connected.
You should see a Raspberry Pi logo and the EDK2 bootloader screen appear. Unless you have NVMe or USB boot media installed, it will say "Press any key to enter the Boot Manager Menu."
Since I couldn't find the 'any' key, I pressed 'Enter', then I could navigate through a standard boot manager menu. In there you can configure SD card speeds, set the PCIe bus speed, etc.
After you've changed the settings to your liking (see some suggestions for Linux), save and reset.




TrueNAS on a Pi 5

Now that the Pi is booting into UEFI mode, you can install TrueNAS. To do that:


Download a TrueNAS on Arm ISO from https://truenas-releases.jmay.us (I chose 25.04.2).
Use a tool like Etcher to write the ISO to a USB drive.
After Etcher finishes, eject the USB drive and insert it into the Pi (I used a USB 3 thumb drive, so I inserted it into one of the blue USB 3 ports on the Pi for maximum speed).
If it doesn't automatically boot to the TrueNAS installer, select the external USB drive in the UEFI boot manager and boot into the TrueNAS installer.
Follow the TrueNAS installer's prompts to install TrueNAS on any device other than the installer drive or the microSD card (I used a second USB flash drive plugged into the other USB 3 port). Wait for installation to complete.
When prompted, reboot and remove the USB drive.


TrueNAS SCALE should boot up, and the first boot can take a while as many services need to generate files, configure services, and start them the first time.



In my case, on first boot, the ix-etc service failed to start (it timed out), and its purpose is to Generate TrueNAS /etc files. After booting, I chose to enter the Linux console, then ran systemctl start ix-etc, and rebooted.

After a reboot, TrueNAS seemed to launch all its services without issue, including the web UI. I visited the IP address printed on the console, logged in as the admin user I set up during install, and was greeted with the TrueNAS dashboard:



Current Limitations

Right now, most of the limitations are around missing features in UEFI mode; since Raspberry Pi hasn't pushed RP1 support into the Linux kernel, and nobody's yet reverse-engineered RP1 interfaces, you can't use:


Fan header PWM support (no fan control)
CSI/DSI connections for displays/cameras
GPIO
Built-in Ethernet


The Ethernet limitation is especially annoying, as you are forced to use an external USB Ethernet dongle, just like on most non-Qualcomm systems running Windows on Arm.

Andrea della Porta from SUSE is working on upstreaming RP1 support into Linux with some help from Raspberry Pi, but progress has been a bit slow.

What I've been wondering lately, more and more: why doesn't Raspberry Pi consider official UEFI support in the first place? With or without Microsoft's official blessing, being able to boot vanilla Windows 11 for Arm on the Pi would be neat. Not to mention, any regular Linux Arm distro (including TrueNAS SCALE) would boot too...

Next Steps

I recently received a new hardware project, the Homelabs Pi Storage server, which uses a custom CM5 SATA backplane and a 3D printable enclosure for a 6-bay NAS:



I got TrueNAS installed on a CM5 Lite (using the same process as above), but when I installed four SATA hard drives, they spun up, but were not recognized. Right now the Pi 5 UEFI support doesn't allow for more than one PCIe device, and the Homelabs Pi Storage server has a PCIe switch that branches off to 2.5 Gbps Ethernet and a 6-port SATA controller.

These devices all work perfectly out of the box on Raspberry Pi OS (and I was able to set up a ZFS array, getting 250 MB/s over the built-in 2.5G Ethernet—see below), but they aren't recognized currently when running under UEFI :(



I'm already running vanilla ZFS under Raspberry Pi OS on my other Raspberry Pi storage server, and that's running on four SSDs and no hard drives. It can sustain 200 MB/sec writes, and I presume TrueNAS would be able to do the same.

There are also NVMe-only boards, like the $50 GeeekPi N16 Quad-NVMe HAT, which provide a pretty small footprint all-flash server option. But again, since those boards use switch chips (because the Pi is limited to 1 PCIe lane), none of those drives would be accessible to TrueNAS as it stands today. Your best bet if you want to use TrueNAS instead of just managing ZFS on you own on a Pi would be to use a single-purpose HAT or SATA controller or HBA in IT mode, to connect disks directly to the Pi.

Because of the current UEFI limitations, I would still recommend running TrueNAS on higher-end Arm hardware (like Ampere servers). If you want to stick to an SBC, there's UEFI firmware for RK3588 platforms under active development. It may offer even more functionality for some boards, so check the compatibility list.

Or you could be boring and just install TrueNAS on x86, where it's fully supported ;)]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GPU Prefix Sums: A nearly complete collection]]></title>
            <link>https://github.com/b0nes164/GPUPrefixSums</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051542</guid>
            <description><![CDATA[A nearly complete collection of prefix sum algorithms implemented in CUDA, D3D12, Unity and WGPU. Theoretically portable to all wave/warp/subgroup sizes.  - GitHub - b0nes164/GPUPrefixSums: A nearl...]]></description>
            <content:encoded><![CDATA[GPU Prefix Sums

GPUPrefixSums aims to bring state-of-the-art GPU prefix sum techniques from CUDA and make them available in portable compute shaders. In addition to this, it contributes "Decoupled Fallback," a novel fallback technique for Chained Scan with Decoupled Lookback that should allow devices without forward thread progress guarantees to perform the scan without crashing. The D3D12 implementation includes an extensive survey of GPU prefix sums, ranging from the warp to the device level; all included algorithms utilize wave/warp/subgroup (referred to as "wave" hereon) level parallelism but are completely agnostic of wave size. As a measure of the quality of the code, GPUPrefixSums has also been implemented in CUDA and benchmarked against Nvidia's CUB library. Although GPUPrefixSums aims to be portable to any wave size supported by HLSL, [4, 128], due to hardware limitations, it has only been tested on wave sizes 4, 16, 32, and 64. You have been warned!
If you are interested in prefix sums for their use in radix sorting, check out GPUPrefixSum's sibling repository GPUSorting!
Decoupled Fallback
In Decoupled Fallback, a threadblock will spin for a set amount of cycles while waiting for the reduction of a preceding partition tile. If the maximum spin count is exceeded, the threadblock is free to perform a fallback operation. Multiple thread blocks are allowed to perform fallbacks on the same deadlocking tile, but through use of atomic compare and swap, only one thread block ends up broadcasting its reduction in device memory. Although this means potentially performing redundant calculations, the upside is that fallback performance is no longer limited by the latency of signal propagation between thread blocks.
As of writing this 9/22/2024, Decoupled Fallback shows promising results on Apple M GPU's. However the version included here are out of date, with the most up-to-date development occuring in Vello.
Survey

A prefix sum, also called a scan, is a running total of a sequence of numbers at the n-th element. If the prefix sum is inclusive the n-th element is included in that total, if it is exclusive, the n-th element is not included. The prefix sum is one of the most important algorithmic primitives in parallel computing, underpinning everything from sorting, to compression, to graph traversal.
Basic Scans


Kogge-Stone





Sklansky





Brent-Kung





Reduce Scan





Raking Reduce-Scan



Warp-Synchronized Scans


Warp-Sized-Radix Brent-Kung





Warp-Sized-Radix Brent-Kung with Fused Upsweep-Downsweep





Warp-Sized-Radix Sklansky





Warp-Sized-Radix Serial





Warp-Sized-Radix Raking Reduce-Scan



Block-Level Scan Pattern


First Partition





Second Partition and Onwards



Device Level Scan Pattern (Reduce-Then-Scan)


Reduce





Scan Along the Intermediate Reductions





Scan and Pass in Intermediate Values



Getting Started
GPUPrefixSumsD3D12
Headless implementation in D3D12, includes:

Reduce then Scan
Chained Scan with Decoupled Lookback
Chained Scan with Decoupled Lookback Decoupled Fallback

Requirements:

Visual Studio 2019 or greater
Windows SDK 10.0.20348.0 or greater

The repository folder contains a Visual Studio 2019 project and solution file. Upon building the solution, NuGet will download and link the following external dependencies:

DirectX 12 Agility SDK
DirectX Shader Compiler
Microsoft Windows Implementation Library

See the repository wiki for information on running tests.
GPUPrefixSumsCUDA
GPUPrefixSumsCUDA includes:

Reduce then Scan
Chained Scan with Decoupled Lookback

The purpose of this implementation is to benchmark the algorithms and demystify their implementation in the CUDA environment. It is not intended for production or use; instead, a proper implementation can be found in the CUB library.
Requirements:

Visual Studio 2019 or greater
Windows SDK 10.0.20348.0 or greater
CUDA Toolkit 12.3.2
Nvidia Graphics Card with Compute Capability 7.x or greater.

The repository folder contains a Visual Studio 2019 project and solution file; there are no external dependencies besides the CUDA toolkit. The use of sync primitives necessitates Compute Capability 7.x or greater. See the repository wiki for information on running tests.
GPUPrefixSumsUnity
Released as a Unity package includes:

Reduce then Scan
Chained Scan with Decoupled Lookback

Requirements:

Unity 2021.3.35f1 or greater

Within the Unity package manager, add a package from git URL and enter:
https://github.com/b0nes164/GPUPrefixSums.git?path=/GPUPrefixSumsUnity
See the repository wiki for information on running tests.
GPUPrefixSumsWGPU
WARNING: TESTING ONLY CURRENTLY, NOT FULLY PORTABLE
Barebones implementation--no vectorization, no wave intrinsics--to be used as a testbed.
Requirements:

wgpu 22.0
pollster 0.3
bytemuck 1.16.3

Interesting Reading and Bibliography
Duane Merrill and Michael Garland. “Single-pass Parallel Prefix Scan with De-coupled Lookback”. In: 2016.
url: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
Grimshaw, Andrew S. and Duane Merrill. “Parallel Scan for Stream Architectures.” (2012).
url: https://libraopen.lib.virginia.edu/downloads/6t053g00z
Matt Pettineo. GPU Memory Pools in D3D12. Jul. 2022.
url: https://therealmjp.github.io/posts/gpu-memory-pool/
Ralph Levien. Prefix sum on portable compute shaders. Nov. 2021.
url: https://raphlinus.github.io/gpu/2021/11/17/prefix-sum-portable.html
Tyler Sorensen, Hugues Evrard, and Alastair F. Donaldson. “GPU Schedulers: How Fair Is Fair Enoughl”. In: 29th International Conference on Concurrency Theory (CONCUR 2018). Ed. by Sven Schewe and Lijun Zhang. Vol. 118. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, 23:1–23:17. isbn: 978-3-95977-087-3. doi: 10.4230/LIPIcs.CONCUR.2018.23.
url: http://drops.dagstuhl.de/opus/volltexte/2018/9561.
Vasily Volkov. “Understanding Latency Hiding on GPUs”. PhD thesis. EECS Department, University of California, Berkeley, Aug. 2016.
url: http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.html
Zhe Jia et al. Dissecting the NVidia Turing T4 GPU via Microbenchmarking. 2019. arXiv: 1903.07486.
url: https://arxiv.org/abs/1903.07486
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Prosper AI (YC S23) Is Hiring Founding Account Executives (NYC)]]></title>
            <link>https://jobs.ashbyhq.com/prosper-ai/29684590-4cec-4af2-bb69-eb5c6d595fb8</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051096</guid>
        </item>
        <item>
            <title><![CDATA[Charting Form Ds to roughly see the state of venture capital “fund” raising]]></title>
            <link>https://tj401.com/blog/formd/index.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45051034</guid>
            <content:encoded><![CDATA[The startup bubble that no one is talking aboutAugust 28, 2025Figure 1**Above is a graph that displays the amount of Form Ds filed, where the entity (read: company/firm) name contains the phrases "fund I", "fund II", "fund III", and "fund IV". The x-axis is not the prettiest, but it is broken down by quarter. You can see that the line for "fund I" sees by far the greatest peak around quarter 3 of 2022, with a steep drop off immediately after. The other lines have a similar, but less pronounced trend.So, what is the significance of this? Companies and firms file Form Ds in compliance with Regulation D, which requires disclosure when raising funds under specific circumstances. I won't go into the details here, but the TLDR is that it isn't always required, but it's not uncommon either. Another piece of context is that venture capital firms (among other financial investment groups) label their individual funds, often by appending Fund [<fund roman numeral>] to describe where the fund falls in their sequence of funds. Here is a search query on the SEC filings database. You can see how the naming convention works. Each fund often constitutes its own entity. My hypothesis here is that, by charting the amount of Form Ds filed with "fund [#]" in the name, we can roughly see the state of venture capital “fund” raising. My Takeaways1. From this graph we can roughly see the ratio of venture firms that make it to a given fund cycle. That's a little hard to claim as the funds get higher in number, because once firms get large enough, they often stop creating sequential funds, instead raising in parallel and creating funds targeting specific industries/products etc. If you are planning to start a venture firm though, you may gain a bit of insight into your odds of longevity.2. Venture funding is about to drop off BIG time. During the early 2020s, everyone and their mom raised a VC fund. This is due to several factors The zero interest rate phenomenon. Because rates were so low, endowments, pensions, high networth individuals, and other institutional investors were looking for ways to deploy capital. VC was a compelling way to do that.It was easier than ever to start a venture firm. Another interesting graph is my estimation of the amount of Form Ds filed by “SPV as a service” companies, such as Angellist or Sydecar. These companies handle the back office administration for groups raising funds to invest in startups. If your firm isn’t big enough to hire its own staff, you can handle tasks like fund incorporation, and wire coordination through these groups. An abundance of LPs in tandem with these new companies meant that you only needed one or two well presenting individuals to raise a venture fund. In the graph below you can see that Form Ds filed by these groups also peaked around late 2021/2022. Figure 2Note that I estimated these values by looking at similarities matching entities listed on the Forms that were also associated with Angellist/Sydecar/other fund backend providers. These groups likely also contribute to the fluffing of the venture economy by increasing access to investments in startups*. As interest rates increased, and things settled down, the amount of venture firms raising new funds decrease, as evidenced by the decrease in Form Ds with “fund [#]” in the name after Q3 2022 shown in figure 2.Why we are going to see the effects of the bubble nowMaking the assumptions that a) most venture funds target a life span of ~10 years and b) the capital deployment stage of funds lasts 2-4 years (again, roughly), we are just passing the moment of peak fund availability.This has all coincided nicely with an immense increase in expectations put on startups focused on AI solutions. Investor interest in wrapper/agent/AI lab companies has seemed insatiable over the past 18 months, in alignment with the end of the funding deployment stage for funds that raised at the peak. This has led to more startups raising rounds at higher valuations (see another keyword search based graph below). My predictions, based on the above data, and my anecdotal experience is that the amount of venture funding available is about to decrease. This will lead to lower valuations as the supply of funds decreases, inducing relative scarcity. As a result many companies will be left “swimming naked as that tide goes out”. This along with other issues such as (very) newly decreasing expectations of the AI vertical as a whole could lead to a sizable contraction. The decrease in available funding will also put more pressure on companies to actually ** make money **. Unless compute becomes much more cost effective in the immediate future, foundational model providers will be required to raise their prices to supplement equity based funding for compute cost, likely causing wrappers and agent companies to do the same. Some users will be priced out, and likely, many companies will no longer be viable.All this to say: A future contraction may not be the exclusive result of changing sentiment in the AI industry. Sure that's part of it, but the availability of VC funds has been destined to decrease since firm fundraising peaked in 2022.                              Figure 3The offering amount represents the sum of all “total offering amounts” listed on all Form Ds containing “ ai” or “.ai” in the entity names for a given quarter. Note: 2025 sees a dip because only quarters 1 and 2 are accounted for.TJ Jeffersonhttps://tj401.comtj@shcove.com*As a sidenote I do think these services offer ways to anonymize funding sources, and allow potentially less savvy investors (God forbid, unaccredited) to be duped into investing at insanely high valuations.
**This is an updated graph from an the original where some values were double counted. This issue did not change any aspect of the argument laid out in this essay, and the depicted trends are nearly identical. Please email me if you have any questions.Figure 4]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Important machine learning equations]]></title>
            <link>https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45050931</guid>
            <description><![CDATA[Musings of a Deep Learning Enthusiast.]]></description>
            <content:encoded><![CDATA[
  Motivation
Machine learning (ML) is a powerful field driven by mathematics. Whether you’re building models, optimizing algorithms, or simply trying to understand how ML works under the hood, mastering the core equations is essential. This blog post is designed to be your go-to resource, covering the most critical and “mind-breaking” ML equations—enough to grasp most of the core math behind ML. Each section includes theoretical insights, the equations themselves, and practical implementations in Python, so you can see the math in action.

This guide is for anyone with a basic background in math and programming who wants to deepen their understanding of ML and is inspired by this tweet from @goyal__pramod. Let’s dive into the equations that power this fascinating field!



Table of Contents


  Introduction
  
    Probability and Information Theory

    
      Bayes Theorem
      Entropy
      Joint and Conditional Probability
      Kullback-Leibler Divergence (KLD)
      Cross-Entropy
    
  
  
    Linear Algebra

    
      Linear Transformation
      Eigenvalues and Eigenvectors
      Singular Value Decomposition (SVD)
    
  
  
    Optimization

    
      Gradient Descent
      Backpropagation
    
  
  
    Loss Functions

    
      Mean Squared Error (MSE)
      Cross-Entropy Loss
    
  
  
    Advanced ML Concepts

    
      Diffusion Process
      Convolution Operation
      Softmax Function
      Attention Mechanism
    
  
  Conclusion
  Further Reading




Introduction
Mathematics is the language of machine learning. From probability to linear algebra, optimization to advanced generative models, equations define how ML algorithms learn from data and make predictions. This blog post compiles the most essential equations, explains their significance, and provides practical examples using Python libraries like NumPy, scikit-learn, TensorFlow, and PyTorch. Whether you’re a beginner or an experienced practitioner, this guide will equip you with the tools to understand and apply ML math effectively.



Probability and Information Theory
Probability and information theory provide the foundation for reasoning about uncertainty and measuring differences between distributions.

Bayes’ Theorem

Equation:

\[P(A|B) = \frac{P(B|A) P(A)}{P(B)}\]

Explanation: Bayes’ Theorem describes how to update the probability of a hypothesis ($A$) given new evidence ($B$). It’s a cornerstone of probabilistic reasoning and is widely used in machine learning for tasks like classification and inference.

Practical Use: Applied in Naive Bayes classifiers, Bayesian networks, and Bayesian optimization.

Implementation:

def bayes_theorem(p_d, p_t_given_d, p_t_given_not_d):
    """
    Calculate P(D|T+) using Bayes' Theorem.
    
    Parameters:
    p_d: P(D), probability of having the disease
    p_t_given_d: P(T+|D), probability of testing positive given disease
    p_t_given_not_d: P(T+|D'), probability of testing positive given no disease
    
    Returns:
    P(D|T+), probability of having the disease given a positive test
    """
    p_not_d = 1 - p_d
    p_t = p_t_given_d * p_d + p_t_given_not_d * p_not_d
    p_d_given_t = (p_t_given_d * p_d) / p_t
    return p_d_given_t

# Example usage
p_d = 0.01  # 1% of population has the disease
p_t_given_d = 0.99  # Test is 99% sensitive
p_t_given_not_d = 0.02  # Test has 2% false positive rate
result = bayes_theorem(p_d, p_t_given_d, p_t_given_not_d) 
print(f"P(D|T+) = {result:.4f}")  # Output: P(D|T+) = 0.3333 


Entropy

Equation:

\[H(X) = -\sum_{x \in X} P(x) \log P(x)\]

Explanation: Entropy measures the uncertainty or randomness in a probability distribution. It quantifies the amount of information required to describe the distribution and is fundamental in understanding concepts like information gain and decision trees.

Practical Use: Used in decision trees, information gain calculations, and as a basis for other information-theoretic measures.

Implementation:

import numpy as np

def entropy(p):
    """
    Calculate entropy of a probability distribution.
    
    Parameters:
    p: Probability distribution array
    
    Returns:
    Entropy value
    """
    return -np.sum(p * np.log(p, where=p > 0))

# Example usage
fair_coin = np.array([0.5, 0.5])  # fair coin has the same probability of heads and tails
print(f"Entropy of fair coin: {entropy(fair_coin)}")  # Output: 0.6931471805599453 

biased_coin = np.array([0.9, 0.1])  # biased coin has a higher probability of heads
print(f"Entropy of biased coin: {entropy(biased_coin)}")  # Output: 0.4698716731013394 


Joint and Conditional Probability

Equations:


  
    Joint Probability:

\[P(A, B) = P(A|B) P(B) = P(B|A) P(A)\]
  
  
    Conditional Probability:

\[P(A|B) = \frac{P(A, B)}{P(B)}\]
  


Explanation: Joint probability describes the likelihood of two events occurring together, while conditional probability measures the probability of one event given another. These are the building blocks of Bayesian methods and probabilistic models.

Practical Use: Used in Naive Bayes classifiers and probabilistic graphical models.

Implementation:

from sklearn.naive_bayes import GaussianNB
import numpy as np

X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])
model = GaussianNB().fit(X, y)
print(model.predict([[2.5, 3.5]]))  # Output: [1]


Kullback-Leibler Divergence (KLD)

Equation:

\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]

Explanation: KLD measures how much one probability distribution $P$ diverges from another $Q$. It’s asymmetric and foundational in information theory and generative models.

Practical Use: Used in variational autoencoders (VAEs) and model evaluation.

Implementation:

import numpy as np

P = np.array([0.7, 0.3])
Q = np.array([0.5, 0.5])
kl_div = np.sum(P * np.log(P / Q))
print(f"KL Divergence: {kl_div}")  # Output: 0.08228287850505156


Cross-Entropy

Equation:

\[H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log Q(x)\]

Explanation: Cross-entropy quantifies the difference between the true distribution $P$ and the predicted distribution $Q$. It’s a widely used loss function in classification.

Practical Use: Drives training in logistic regression and neural networks.

Implementation:

import numpy as np

y_true = np.array([1, 0, 1])
y_pred = np.array([0.9, 0.1, 0.8])
cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f"Cross-Entropy: {cross_entropy}")  # Output: 0.164252033486018




Linear Algebra
Linear algebra powers the transformations and structures in ML models.

Linear Transformation

Equation:

\[y = Ax + b \quad \text{where } A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, y \in \mathbb{R}^m, b \in \mathbb{R}^m\]

Explanation: This equation represents a linear mapping of input $x$ to output $y$ via matrix $A$ and bias $b$. It’s the core operation in neural network layers.

Practical Use: Foundational for linear regression and neural networks.

Implementation:

import numpy as np

A = np.array([[2, 1], [1, 3]])
x = np.array([1, 2])
b = np.array([0, 1])
y = A @ x + b
print(y)  # Output: [4 7]


Eigenvalues and Eigenvectors

Equation:

\[Av = \lambda v \quad \text{where } \lambda \in \mathbb{R}, v \in \mathbb{R}^n, v \neq 0\]

Explanation: Eigenvalues $\lambda$ and eigenvectors $v$ describe how a matrix $A$ scales and rotates space, crucial for understanding data variance.

Practical Use: Used in Principal Component Analysis (PCA).

Implementation:

import numpy as np

A = np.array([[4, 2], [1, 3]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")


Singular Value Decomposition (SVD)

Equation:

\[A = U \Sigma V^T\]

Explanation: SVD breaks down a matrix $A$ into orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ of singular values. It reveals the intrinsic structure of data.

Practical Use: Applied in dimensionality reduction and recommendation systems.

Implementation:

import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])
U, S, Vt = np.linalg.svd(A)
print(f"U:\n{U}\nS: {S}\nVt:\n{Vt}")




Optimization
Optimization is how ML models learn from data.

Gradient Descent

Equation:

\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta)\]

Explanation: Gradient descent updates parameters $\theta$ by moving opposite to the gradient of the loss function $L$, scaled by learning rate $\eta$.

Practical Use: The backbone of training most ML models.

Implementation:

import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(epochs):
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta

X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([1, 2, 3])
theta = gradient_descent(X, y)
print(theta)  # Output: ~[0., 1.]


Backpropagation

Equation:

\[\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}\]

Explanation: Backpropagation applies the chain rule to compute gradients of the loss $L$ with respect to weights $w_{ij}$ in neural networks.

Practical Use: Enables efficient training of deep networks.

Implementation:

import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

X = torch.tensor([[0., 0.], [1., 1.]], dtype=torch.float32)
y = torch.tensor([[0.], [1.]], dtype=torch.float32)

optimizer.zero_grad()
output = model(X)
loss = loss_fn(output, y)
loss.backward()
optimizer.step()
print(f"Loss: {loss.item()}")




Loss Functions
Loss functions measure model performance and guide optimization.

Mean Squared Error (MSE)

Equation:

\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

Explanation: MSE calculates the average squared difference between true $y_i$ and predicted $\hat{y}_i$ values, penalizing larger errors more heavily.

Practical Use: Common in regression tasks.

Implementation:

import numpy as np

y_true = np.array([1, 2, 3])
y_pred = np.array([1.1, 1.9, 3.2])
mse = np.mean((y_true - y_pred)**2)
print(f"MSE: {mse}")  # Output: 0.01


Cross-Entropy Loss

(See Cross-Entropy above for details.)



Advanced ML Concepts
These equations power cutting-edge ML techniques.

Diffusion Process

Equation:

\[x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, I)\]

Explanation: This describes a forward diffusion process where data $x_0$ is gradually noised over time $t$, a key idea in diffusion models.

Practical Use: Used in generative AI like image synthesis.

Implementation:

import torch

x_0 = torch.tensor([1.0])
alpha_t = 0.9
noise = torch.randn_like(x_0)
x_t = torch.sqrt(torch.tensor(alpha_t)) * x_0 + torch.sqrt(torch.tensor(1 - alpha_t)) * noise
print(f"x_t: {x_t}")




Convolution Operation

Equation:

\[(f * g)(t) = \int f(\tau) g(t - \tau) \, d\tau\]

Explanation: Convolution combines two functions by sliding one over the other, extracting features in data like images.

Practical Use: Core to convolutional neural networks (CNNs).

Implementation:

import torch
import torch.nn as nn

conv = nn.Conv2d(1, 1, kernel_size=3)
image = torch.randn(1, 1, 28, 28)
output = conv(image)
print(output.shape)  # Output: torch.Size([1, 1, 26, 26])




Softmax Function

Equation:

\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]

Explanation: Softmax converts raw scores $z_i$ into probabilities, summing to 1, ideal for multi-class classification.

Practical Use: Used in neural network outputs.

Implementation:

import numpy as np

z = np.array([1.0, 2.0, 3.0])
softmax = np.exp(z) / np.sum(np.exp(z))
print(f"Softmax: {softmax}")  # Output: [0.09003057 0.24472847 0.66524096]




Attention Mechanism

Equation:

\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V\]

Explanation: Attention computes a weighted sum of values $V$ based on the similarity between queries $Q$ and keys $K$, scaled by $\sqrt{d_k}$.

Practical Use: Powers transformers in NLP and beyond.

Implementation:

import torch

def attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attn = torch.softmax(scores, dim=-1)
    return torch.matmul(attn, V)

Q = torch.tensor([[1., 0.], [0., 1.]])
K = torch.tensor([[1., 1.], [1., 0.]])
V = torch.tensor([[0., 1.], [1., 0.]])
output = attention(Q, K, V)
print(output)




Conclusion

This blog post has explored the most critical equations in machine learning, from foundational probability and linear algebra to advanced concepts like diffusion and attention. With theoretical explanations, practical implementations, and visualizations, you now have a comprehensive resource to understand and apply ML math. Point anyone asking about core ML math here—they’ll learn 95% of what they need in one place!



Further Reading

  Pattern Recognition and Machine Learning by Christopher Bishop
  Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
  Stanford CS229: Machine Learning
  PyTorch Tutorials


  ]]></content:encoded>
        </item>
    </channel>
</rss>