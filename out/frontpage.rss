<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 28 Aug 2025 23:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[RSS Is Awesome]]></title>
            <link>https://evanverma.com/rss-is-awesome</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058024</guid>
            <description><![CDATA[Evan briefly discusses rss]]></description>
            <content:encoded><![CDATA[ 
    
      NetNewsWire
    
  
    
       is my latest most-used iPhone app. It is a simple, free RSS reader. 
    
  
    
      RSS is an old technology that it seems most people have forgotten about. Here's how it works: you enter a link to an RSS "feed", and your app pulls data from this feed every few minutes or so. When there is a new post from your feed, that post is pulled directly to your app. 
    
  
    
      RSS is really simple, so it is still very well supported. Notably, all substack publications automatically have an RSS feed included at 
    
  https://{{substack-domain}}/feed
    
      . 
    
  
    
      Blogs are great but I don't enjoy reading posts in my email, having to remember the websites each one is hosted at, or reading from each publications' different typesetting opinions with varying pop-ups and advertisements. An RSS reader centralizes all content from your blogs into a single place for reading. 
    
  
    
      Since I started using this app I spend much more of my "mindless phone time" reading blog posts, which I think is somewhere between marginally good and good. 
    
   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rupert's Property]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/08/28/a-polyhedron-without-ruperts-property/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057561</guid>
            <description><![CDATA[You can cut a hole in a cube that‚Äôs big enough to slide an identical cube through that hole! Think about that for a minute‚Äîit‚Äôs kind of weird. Amazingly, nobody could prove any co‚Ä¶]]></description>
            <content:encoded><![CDATA[
				



You can cut a hole in a cube that‚Äôs big enough to slide an identical cube through that hole!   Think about that for a minute‚Äîit‚Äôs kind of weird.
Amazingly, nobody could prove any convex polyhedron doesn‚Äôt have this property!  It‚Äôs called ‚ÄòRupert‚Äôs property‚Äô.
Until this week.
This week Steininger and Yurkevich proved there is a convex polyhedron that you can‚Äôt cut a hole in big enough to slide the entire polyhedron through the hole.  It has 90 vertices, and apparently 240 edges and 152 faces.




To prove that no such hole is possible, they had to do a computer search of 18 million different holes, plus use a lot of extra math to make sure they‚Äôd checked enough possibilities:
‚Ä¢ Jakob Steininger and Sergey Yurkevich, A convex polyhedron without Rupert‚Äôs property.
To celebrate their discovery, they gave this polyhedron a silly name.  Since this polyhedron lacks Rupert‚Äôs property, they called it a ‚Äònoperthedron‚Äô.
Why is this property called ‚ÄòRupert‚Äôs property‚Äô?  Wikipedia explains:

In geometry, Prince Rupert‚Äôs cube is the largest cube that can pass through a hole cut through a unit cube without splitting it into separate pieces. Its side length is approximately 1.06, 6% larger than the side length 1 of the unit cube through which it passes. The problem of finding the largest square that lies entirely within a unit cube is closely related, and has the same solution.
Prince Rupert‚Äôs cube is named after Prince Rupert of the Rhine, who asked whether a cube could be passed through a hole made in another cube of the same size without splitting the cube into two pieces. A positive answer was given by John Wallis. Approximately 100 years later, Pieter Nieuwland found the largest possible cube that can pass through a hole in a unit cube.

Here Greg Egan shows how Rupert‚Äôs property works for the cube:


Here he shows how it works for the regular octahedron:


And finally, here‚Äôs a video by David Renshaw showing 26 polyhedra with Rupert‚Äôs property‚Ä¶ and 5 polyhedra that might lack it:



The triakis tetrahedron is an extremely close call, but it does have Rupert‚Äôs property:




				
				
					
					This entry was posted  on Thursday, August 28th, 2025 at 7:50 am and is filed under mathematics.					You can follow any responses to this entry through the RSS 2.0 feed.
											You can leave a response, or trackback from your own site.
					
					
				

				
					Post navigation
					¬´ Previous Post
					
				

			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Death by PowerPoint: the slide that killed seven people]]></title>
            <link>https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057404</guid>
            <description><![CDATA[We‚Äôve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We‚Äôre so used to it we expect it.  We accept it.  We even consider it ‚Äòlearning‚Äô. As an educator I push against ‚Äòdeath by PowerPoint‚Äô and I'm fascinated with how we]]></description>
            <content:encoded><![CDATA[

      

      

      
        
          
            
              
            
          
        
      


      
      
      

      
        
        

  
  

    

    

      

      
        
          
        
        

        
          
            
          
        

        
          
          
            The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)
          
        
      
        
      

    
  We‚Äôve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We‚Äôre so used to it we expect it.  We accept it.  We even consider it ‚Äòlearning‚Äô. As an educator I push against ‚Äòdeath by PowerPoint‚Äô and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audience‚Äôs inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Whereas other shuttle crews had focused on work to the Hubble Space Telescope or to the International Space Station this mission was one of pure scientific research.  The launch proceeded as normal.  The crew settled into their mission.  They would spend 16 days in orbit, completing 80 experiments.  One day into their mission it was clear to those back on Earth that something had gone wrong.  As a matter of protocol NASA staff reviewed footage from an external camera mounted to the fuel tank.  At eighty-two seconds into the launch a piece of spray on foam insulation (SOFI) fell from one of the ramps that attached the shuttle to its external fuel tank.  As the crew rose at 28,968 kilometres per hour the piece of foam collided with one of the tiles on the outer edge of the shuttle‚Äôs left wing.  


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Frame of NASA launch footage showing the moment the foam struck the shuttle‚Äôs left wing (Creative Commons)
          
        
      
        
      

    
  It was impossible to tell from Earth how much damage this foam, falling nine times faster than a fired bullet, would have caused when it collided with the wing.   Foam falling during launch was nothing new.  It had happened on four previous missions and was one of the reasons why the camera was there in the first place.  But the tile the foam had struck was on the edge of the wing designed to protect the shuttle from the heat of Earth‚Äôs atmosphere during launch and re-entry.  In space the shuttle was safe but NASA didn‚Äôt know how it would respond to re-entry.  There were a number of options.  The astronauts could perform a spacewalk and visually inspect the hull.  NASA could launch another Space Shuttle to pick the crew up.  Or they could risk re-entry.  NASA officials sat down with Boeing Corporation engineers who took them through three reports; a total of 28 slides.    The salient point was whilst there was data showing that the tiles on the shuttle wing could tolerate being hit by the foam this was based on test conditions using foam more than 600 times smaller than that that had struck Columbia.  This is the slide the engineers chose to illustrate this point:

  NASA managers listened to the engineers and their PowerPoint.  The engineers felt they had communicated the potential risks.  NASA felt the engineers didn‚Äôt know what would happen but that all data pointed to there not being enough damage to put the lives of the crew in danger.  They rejected the other options and pushed ahead with Columbia re-entering Earth‚Äôs atmosphere as normal.  Columbia was scheduled to land at 0916 (EST) on February 1st 2003.  Just before 0900, 61,170 metres above Dallas at 18 times the speed of sound, temperature readings on the shuttle‚Äôs left wing were abnormally high and then were lost.  Tyre pressures on the left side were soon lost as was communication with the crew.  At 0912, as Columbia should have been approaching the runway, ground control heard reports from residents near Dallas that the shuttle had been seen disintegrating.  Columbia was lost and with it her crew of seven.  The oldest crew member was 48.  The shuttle programme was on lock down, grounded for two years as the investigation began.  The cause of the accident became clear: a hole in a tile on the left wing caused by the foam let the wing dangerously overheat until the shuttle disintegrated.  The questions to answer included a very simple one: Why, given that the foam strike had occurred at a force massively out of test conditions had NASA proceeded with re-entry?  Edward Tufte, a Professor at Yale University and expert in communication reviewed the slideshow the Boeing engineers had given NASA, in particular the above slide.  His findings were tragically profound.


 Firstly, the slide had a misleadingly reassuring title claiming that test data pointed to the tile being able to withstand the foam strike.  This was not the case but the presence of the title, centred in the largest font makes this seem the salient, summary point of this slide.  This helped Boeing‚Äôs message be lost almost immediately.























  Secondly, the slide contains four different bullet points with no explanation of what they mean.  This means that interpretation is left up to the reader.  Is number 1 the main bullet point?  Do the bullet points become less important or more?  It‚Äôs not helped that there‚Äôs a change in font sizes as well.  In all with bullet points and indents six levels of hierarchy were created.  This allowed NASA managers to imply a hierarchy of importance in their head: the writing lower down and in smaller font was ignored.  Actually, this had been where the contradictory (and most important) information was placed.  


Thirdly, there is a huge amount of text, more than 100 words or figures on one screen.   Two words, ‚ÄòSOFI‚Äô and ‚Äòramp‚Äô both mean the same thing: the foam.  Vague terms are used.  Sufficient is used once, significant or significantly, five times with little or no quantifiable data.  As a result this left a lot open to audience interpretation.  How much is significant?  Is it statistical significance you mean or something else?  
























Finally the single most important fact, that the foam strike had occurred at forces massively out of test conditions, is hidden at the very bottom.  Twelve little words which the audience would have had to wade through more than 100 to get to.  If they even managed to keep reading to that point.  In the middle it does say that it is possible for the foam to damage the tile.  This is in the smallest font, lost. 























  NASA‚Äôs subsequent report criticised technical aspects along with human factors.  Their report mentioned an over-reliance on PowerPoint: ‚ÄúThe Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA.‚Äù  Edward Tufte‚Äôs full report makes for fascinating reading. Since being released in 1987 PowerPoint has grown exponentially to the point where it is now estimated than thirty million PowerPoint presentations are made every day.  Yet, PowerPoint is blamed by academics for killing critical thought.  Amazon‚Äôs CEO Jeff Bezos has banned it from meetings.   Typing text on a screen and reading it out loud does not count as teaching.  An audience reading text off the screen does not count as learning.  Imagine if the engineers had put up a slide with just: ‚Äúfoam strike more than 600 times bigger than test data.‚Äù  Maybe NASA would have listened.  Maybe they wouldn‚Äôt have attempted re-entry.  Next time you‚Äôre asked to give a talk remember Columbia. Don‚Äôt just jump to your laptop and write out slides of text.  Think about your message.  Don‚Äôt let that message be lost amongst text.  Death by PowerPoint is a real thing.  Sometimes literally.Thanks for reading - Jamie 


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Columbia‚Äôs final crew (from https://www.space.com/19436-columbia-disaster.html)
          
        
      
        
      

    

    

    

  

  

  

  
  
  


        
        
      

      

      

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sometimes CPU cores are odd ‚Äì Anubis]]></title>
            <link>https://anubis.techaro.lol/blog/2025/cpu-core-odd/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057346</guid>
            <description><![CDATA[TL;DR: all the assumptions you have about processor design are wrong and if you are unlucky you will never run into problems that users do through sheer chance.]]></description>
            <content:encoded><![CDATA[Protected by Anubis From Techaro. Made with ‚ù§Ô∏è in üá®üá¶.Mascot design by CELPHASE.This website is hosted by Techaro. If you have any complaints or notes about the service, please contact support@techaro.lol and we will assist you as soon as possible.
-- ImprintThis website is running Anubis version v1.22.0-pre1-8-g21c3e0c.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Expert LSP the official language server implementation for Elixir]]></title>
            <link>https://github.com/elixir-lang/expert</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057322</guid>
            <description><![CDATA[Official Elixir Language Server Protocol implementation - elixir-lang/expert]]></description>
            <content:encoded><![CDATA[Expert
Expert is the official language server implementation for the Elixir programming language.
Installation
You can download Expert from the releases page for your
operating system and architecture.
For editor specific installation instructions, please refer to the Installation Instructions
Nightly Builds
If you want to try out the latest features, you can download a nightly build.
Using the GH CLI, you can run the following command to download the latest nightly build:
gh release download nightly --pattern 'expert_linux_amd64'
Then point your editor to the downloaded binary.
Building from source
To build Expert from source, you need Zig 1.14.1 installed on your system.
Then you can run the following command:
just release-local
This will build the Expert binary and place it in the apps/expert/burrito_out directory. You can then point your
editor to this binary.
Sponsorship
Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.






Corporate
For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at danj@erlef.org.
Individual
Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.
Other resources

Architecture
Development Guide
Glossary
Installation Instructions

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Speed-coding for the 6502 ‚Äì a simple example]]></title>
            <link>https://www.colino.net/wordpress/en/archives/2025/08/28/speed-coding-for-the-6502-a-simple-example/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057209</guid>
            <description><![CDATA[A real-life example showing how to scale an image in either 10 seconds, or 0.2 seconds, using 6502 assembly.]]></description>
            <content:encoded><![CDATA[
	
		
		
			

				
				
				
					2025/08/28
						 - 
						About 5 minutes read
					
					
				

				
				
					
						
Usually clocked at 1MHz, with no hardware support for multiplication or division, and limited support for bit shifting, it is often important to take a step back from an algorithm in order to make it do the same thing, hundreds of times faster.



Note: this article doesn‚Äôt describe any technological breakthrough or extremely clever new way of doing things. It‚Äôs just a real-life example using a little part of an algorithm I made, and where at first, I stopped at step 2 instead of going all the way to the last step.



In this example, we want to scale down a 256√ó192 bitmap down to 192√ó144, which means the scaling factor is 3/4.



We‚Äôre going to have to apply this factor to every one of the 192 lines, and for each line, for every one of the 256 pixels composing the line. This means we‚Äôll have to apply our factor (192*256)+192 times, 49344 times in total. 



We can also note that the numbers we want to scale are 8 bits, but at first, it looks like we‚Äôll need to do 16 bits math, as 256*3 is greater than 256.



The first, naive way is to do scaled_value = value*3/4:



lda value       ; load the value
ldx #3          ; load the multiplicator
jsr mult_a_x    ; jump to the multiplication function
                ; which returns a 16-bit number in A and X
ldy #4          ; load the divisor
jsr div_ax_y    ; jump to the division function
sta scaled_value; we have our result!



Given that multiplication and division is made via software, and each one takes about 200 cycles (approximately), our image scaling will require about 20 million cycles, aka 20 seconds at 1MHz!



So let‚Äôs try to avoid multiplication and division. Notice our formula can also be written as scaled_value = (value*2 + value)/4, and as we know, multiplying by 2 is shifting left, and dividing by 4 is shifting right twice: scaled_value = (value<<1 + value) >> 2, which in assembly, translates to:



 lda value      ; load the value
 ldx #$00       ; init the high byte of the multiplication to zero
 stx tmp1_zp    ; store it in a variable, as the 6502 can't bit-
                ; shift the X or Y registers

 asl a          ; shift left, which sets the carry if the result
                ; is > 255.
 rol tmp1_zp    ; bring the carry into the high byte
                ; We have done *2

 adc value      ; Add the value to the temporary result
 bcc :+         ; Did we overflow again?
 inc tmp1_zp    ; if so, we need to increment the high byte.
:
                ; We now have value*3 in A and tmp1_zp. Let's 
                ; divide by four.

lsr tmp1_zp     ; Shift high byte right, leaving low bit in carry
ror a           ; Shift low byte taking carry in high bit
lsr tmp1_zp     ; and a second time.
ror a



We have made the same calculation using ‚Äúonly‚Äù 35 cycles at best/39 at worst. Taking an average of 37 cycles, we can now scale our image in a bit less than two seconds, a ten-fold improvement over the first algorithm. But we can do better!



Our formula can also be rewritten as scaled_value = value*1.5/2, that is, scaled_value = (value>>1 + value)>>1. It looks like it could lead to loss of precision, but it doesn‚Äôt. That will simplify greatly the previous algorithm, as we will be able to do 9-bits math (the carry being the 9th bit):



lda value      ; load our value
lsr a          ; divide it by 2 (sets the carry if low bit=1)
clc            ; clear the carry, we don't care about
               ; what the low bit was
adc value      ; add our value to its half (sets the carry if >255)
ror a          ; divide by two, bringing the carry to the high bit.



We‚Äôre now at a nice 12 cycles per operation! And we‚Äôre able to scale our image in only 0.6 seconds.



Should we stop there? We also know that ‚Äúthere is no faster computation than the one you already know the result of‚Äù, which, is 6502 assembly, translates to ‚Äúprovide lookup tables to spare computing to the computer‚Äù. It often comes trading size for speed: in this case, we‚Äôll need a table of 256 values containing pre-calculated results of X*3/4.



I didn‚Äôt really want to dedicate a full memory page to that little part of a much larger program where memory constraints exist‚Ä¶ But, this program already has a few buffers that are completely unused, and trashable, during this scaling operation. So, instead of providing a dedicated, calculated at build time table, I will just generate it before starting scaling, applying the formula only once for every possible value in the 0-255 range:



 ldy #$00       ; start at 0

:
 tya            ; transfer to A
 ....           ; the previous 12-cycles *1.5/2 algorithm goes here
 sta table,y    ; store the result
 iny
 bne :-         ; and loop 256 times.



Our table is now built at runtime, which has a cost of less than 6000 cycles, and scaling down a pixel is now just a matter of lda table,y: 4 cycles. We just went from needing 10 seconds to scale an image, to 0.2 seconds.
																
					
				
				
				
			
		
		
		
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fuck up my site ‚Äì Turn any website into beautiful chaos]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid>
            <description><![CDATA[Transform any website into pure chaos. Add burning cursors, Comic Sans everything, fake cursors, and more chaos features to any site. Some people just want to watch the web burn.]]></description>
            <content:encoded><![CDATA[fuckupmysiteSome people just want to watch the web burnTry:üòàChaos Settings3 of 6 agents of chaos enabledHeads up: Not every site plays nice with the chaos. Got feedback or discovered something broken? Let me know on Twitter]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You no longer need JavaScript: an overview of what makes modern CSS so awesome]]></title>
            <link>https://lyra.horse/blog/2025/08/you-dont-need-js/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056878</guid>
            <description><![CDATA[An overview of what makes modern CSS so awesome.]]></description>
            <content:encoded><![CDATA[
    
  
  2025-08-28  ¬¶ css


  So much of the web these days is ruined by the bloat that is modern JavaScript frameworks. React apps that take several seconds to load. NextJS sites that throw random hydration errors. The node_modules folder that takes up gigabytes on your hard drive.
It‚Äôs awful. And you don‚Äôt need it.




  
    
      Name
      Status
      Type
      Size
      Time
    
  
  
    app200document153.8 kB51 ms
    6920616d20612066-s.p.6f6e7421.woff2200font31.5 kB32 ms
    686579206d652074-s.p.6f6f2121.woff2200font28.5 kB116 ms
    77687920646f6573.css200stylesheet253 kB47 ms
    2074686520646566.js200script648 kB83 ms
    61756c74206e6578.js200script166 kB363 ms
    746a732074616b65.js200script83.3 kB46 ms
    turbopack-20757020302e354d.js200script38.0 kB95 ms
    423f207468617427.js200script414 B34 ms
    73206d6f72652074.js200script32.6 kB49 ms
    68616e206d792065.js200script15.1 kB71 ms
    6e7469726520626c.js200script143 kB48 ms
    6f6721 hey there!200script4.1 kB103 ms
  



The intro paragraph of this post is tongue-in-cheek. It‚Äôs there to get you to read the rest of the post. I suspect the megabytes of tracking scripts intertwined with bad code is far more likely to be the real culprit behind all the terrible sites out there. Web frameworks have their time and place. And despite my personal distaste for them, I know they are used by many teams to build awesome well-optimized apps.
Despite that, I think there‚Äôs some beauty in leaving it all behind. Not just the frameworks, but JavaScript altogether. Not every site needs JavaScript. Perhaps your e-commerce site needs it for its complex carts and data visualization dashboards, but is it really a necessity for most of what‚Äôs out there?
It‚Äôs actually pretty incredible what HTML and CSS alone can achieve.

So, what do you say?
My goal with this article is to share my perspectives on the web, as well as introduce many aspects of modern HTML/CSS you may not be familiar with. I‚Äôm not trying to make you give up JavaScript, I‚Äôm just trying to show you everything that‚Äôs possible, leaving it up to you to pick what works best for whatever you‚Äôre working on.
I think there‚Äôs a lot most web developers don‚Äôt know about CSS.
And I think JS is often used where better alternatives exist.
So, let me show you what‚Äôs out there.

‚ÄúBut CSS sucks‚Äù
I believe a lot of the negativity towards CSS stems from not really knowing how to use it. Many developers kind of just skip learning the CSS fundamentals in favor of the more interesting Java- and TypeScript, and then go on to complain about a styling language they don‚Äôt understand.
I suspect this is due to many treating CSS as this silly third wheel for adding borders and box-shadows to a webapp. It‚Äôs undervalued and often compared to glorified crayons, rather than what it really is - a powerful domain-specific programming language.
It‚Äôs telling when to this day the only CSS joke in the webdev circles is centering a div.


i am a div

body {  display: flex;  flex-direction: rowcolumnrow-reversecolumn-reverse;  flex-wrap: nowrapwrap;  align-content: centerflex-startflex-endspace-aroundspace-betweenstretch;  justify-content: centerflex-startflex-endspace-aroundspace-betweenspace-evenly;  align-items: centerflex-startflex-endstretchbaseline;}




Yes, the syntax isn‚Äôt the prettiest, but is it really that hard?
Besides, your devtools probably1 come with a fun little gadget that lets you fiddle with the flexbox by just clicking around. You don‚Äôt even need to remember the syntax.

I don‚Äôt think CSS is fundamentally any more difficult than JS, but if you skip the basics on one and only focus on the other, it‚Äôs no surprise it feels that way.
‚ÄúBut it‚Äôs painful to write‚Äù
Another source of disdain for CSS is how awful it has been to write in the past. This is very much true, and is probably why things like Sass and Tailwind2 exist.
But that‚Äôs the thing, it used to be bad.



ü¶ä


Rebane@rebane2001
btw u should write css like

cool-thing {
    display: flex;
    &[shadow] {
        box-shadow: 1px 1px #0007;
    }
    @media (width < 480px) {
        flex-direction: column;
    }
}

and html like

<cool-thing shadow>wow</cool-thing>

because it's allowed & modern & neat!
11:58 AM ¬∑ Apr 8, 2025

‚ù§Ô∏è 1.5K


(yes! the code above is standards compliant3)
In the past few years, CSS has received a ton of awesome quality-of-life additions, making it nice to do stuff that has historically required preprocessors or JavaScript.
Nesting is definitely one of my favorite additions!
In the past, you‚Äôve had to write code that looks like this:
:root {
  --like-color: #24A4F3;
  --like-color-hover: #54B8F5;
  --like-color-active: #0A6BA8;
}

.post {
  display: block;
  background: #EEE;
  color: #111;
}

.post .avatar {
  width: 48px;
  height: 48px;
}

.post > .buttons {
  display: flex;
}

.post > .buttons .label {
  font-size: 24px;
  padding: 8px;
}

.post > .buttons .like {
  cursor: pointer;
  color: var(--like-color);
}

.post > .buttons .like:hover {
  color: var(--like-color-hover);
}

.post > .buttons .like:active {
  color: var(--like-color-active);
}

@media screen (max-width: 800px) {
  .post > .buttons .label {
    font-size: 16px;
    padding: 4px;
  }
}

@media (prefers-color-scheme: dark) {
  .post {
    background: #222;
    color: #FFF;
  }
}


And yeah, that‚Äôs pretty awful to work with. For anything that involves multiple chained selectors, you kind of have to keep a mental map of how every parent selector relates to its children, and the more CSS you add the harder it gets.
But let‚Äôs try it with nesting:
:root {
  --like-color: #24A4F3;
  --like-color-hover: hsl(from var(--like-color) h s calc(l + 10));
  --like-color-active: hsl(from var(--like-color) h s calc(l - 20));
}

.post {
  display: block;
  background: #EEE;
  color: #111;
  @media (prefers-color-scheme: dark) {
    background: #222;
    color: #FFF;
  }
  .avatar {
    width: 48px;
    height: 48px;
  }
  & > .buttons {
    display: flex;
    .label {
      font-size: 24px;
      padding: 8px;
      @media (width <= 800px) {
        font-size: 16px;
        padding: 4px;
      }
    }
    .like {
      cursor: pointer;
      color: var(--like-color);
      &:hover { color: var(--like-color-hover); }
      &:active { color: var(--like-color-active); }
    }
  }
}

That is way nicer to read4! All the relevant parts are right next to each other, so it‚Äôs a lot easier to understand what‚Äôs going on. Seeing the &:hover and &:active right next to the .like button is especially nice imo.
And since you can sort of see the structure - the parent selectors ‚Äúguarding‚Äù the child ones - it also makes it a lot easier to get away with short and simple class names (or even referring to elements themselves).
You may have noticed that I‚Äôm also making use of relative colors in the second example. I think the MDN article has a lot of awesome examples, but the jist of it is that you can take an existing color, modify it in many different ways across multiple color spaces, and mix it with other colors using color-mix().
/* remove blue from a color */
rgb(from #123456 r g 0);
/* make a color transparent */
rgb(from #123456 r g b / 0.5);
/* make a color lighter */
hsl(from #123456 h s calc(l + 10));
/* change the hue in oklch color space */
oklch(from #123456 l c calc(h + 10));
/* mix two colors in oklab color space */
color-mix(in oklab, #8CFFDB, #04593B 25%);

These snippets are really useful for when you want something to be just ever so slightly darker or brighter, such as a button hover effect or a matching border color, and they‚Äôre way nicer to use than doing all those color conversions in JavaScript. If you‚Äôre feeling particularly adventurous, you could even go ahead and generate your entire color scheme in just CSS.

100200300400500600700800900
-40¬∞-20¬∞0¬∞+20¬∞+40¬∞
primarycomplimentarysecondary
successdangerwarninginfo

view-source


(yes! the color picker above is written in just css)
Safari is currently broken when handling of cqw/cqh units, therefore the demo above may not work correctly. If this happens, try using Firefox or Chrome instead.
There are so many cool new CSS features that make writing it just that little bit nicer. Things like letting you use (width <= 768px) instead of (max-width: 768px) in your @media query, the lh unit that matches the line-height, the scrollbar-gutter property that solves the little scrollbar-related layout shifts, or the ability to finally center stuff vertically without flex/grid.


Baseline
And all of this is brought together by the cherry on top that is Baseline. It‚Äôs a guarantee that a specific feature works in every major browser5, and it also lets you know since when - newly available features work in all the latest browsers, and widely available ones work in browsers up to 2.5 years old. Nesting, for example, has been fully supported in all browsers since December 2023, and thus will become widely available in June 2026. You can find the Baseline symbols in various places, such as the MDN docs6.
These are just a few examples of what makes modern CSS so much nicer to write than what we had even just 5 years ago. It almost feels like comparing ES37 to ECMAScript 2025 - and I wouldn‚Äôt blame your grudge if the former is what you‚Äôre used to.
Why bother?
Okay, so CSS has more quality-of-life stuff than before. Still, why would one choose to use it over something else? Doesn‚Äôt JavaScript already let us do everything just fine?
You need to disable JavaScript to run this app.
I think my reasons for using CSS fall into two main categories - because some users don‚Äôt want to use JavaScript, and because doing things in CSS can be genuinely better.
My blog, for example, focuses on infosec topics. Many security researchers (myself included) use a hardened browser configuration to protect themselves, which often means disabling JavaScript by default. I think it‚Äôs nice that they can fully experience my blog without changing their security settings or running a separate, sandboxed browser.
The same goes for privacy-conscious users, and it makes sense! As an experiment, I opened up a local Estonian news site in a web browser with JavaScript enabled. Can you guess how many js files it fetched? (answer in footnote8) That‚Äôs crazy! You do not want that running on your computer.
But surely, you are not one of the evil devs who loads a double-digit number of analytics scripts on your site - is there still any reason to reach for CSS?
Well, I think a lot of things are just plain nicer to make in HTML/CSS, both from the developer and end-user perspectives, be it for ease of use, accessibility, or performance.
Hover effects for your buttons? Toast animations? Input validation? All of these things just work in CSS, and you won‚Äôt have to reinvent the wheel, or throw kilobytes of someone else‚Äôs code at it. There will always be some cases where you do need that extra flexibility JavaScript often provides, but if you don‚Äôt need that, and doing it in CSS is easier, then why not save yourself the trouble?


  
  
  
  
  
  
  
  
  
  
  
  













And the performance of CSS is so much better! Every JavaScript interaction has to go through an event loop that wastes CPU cycles, eats some battery, and adds that tiny bit of stutter to everything.
Sure, in the grand scale of things it isn‚Äôt that bad, APIs like requestAnimationFrame are really good at keeping things smooth. But CSS animations run in the separate compositor thread, and aren‚Äôt affected by stutters and blocking in the event loop.
It makes quite a difference on low-end devices, but feels nice even on high-end ones. CSS animations on my 240hz monitor look amazing9 - JS can look pretty good too, but it has that tiny bit of stutter to it that keeps it from being perfect, especially if you plan on running other heavy code at the same time.
It also means you won‚Äôt have to worry as much about optimization, as the browser takes care of a lot more of the rendering side of things, and often runs your stuff on the GPU if possible.
Pro tip! Wanna trigger animations from JS anyways? Use the modern Web Animations API to easily play the smooth CSS animations from JS.
Transitioning

Speaking of which, I think it‚Äôs time I start showing you practical examples, and a good place to start showing the styles is well, @starting-style.
In the past it has been pretty annoying to add start animations (such as fade-ins) to elements. You‚Äôve had to either set up an entire CSS animation with a separate @keyframes block to go with it, or do a transition using JavaScript where you first add an element to the page, then wait a frame, and then add a class to the element.
.toast {
  transition: opacity 1s, translate 1s;
  opacity: 1;
  translate: 0 0;
  @starting-style {
    opacity: 0;
    translate: 0 10px;
  }
}
Success!

But this has all changed thanks to the new @starting-style at-rule!
Pretty much all you have to do is set your properties as usual, add the initial transition states to @starting-style, and add those properties to a transition. It‚Äôs pretty simple and it kind of just works without having to trigger the animation in any way.
Lunalover
Another good example of where CSS shines is theming. Many sites need separate light and dark modes, and modern CSS makes dealing with that pretty easy.
:root {
  color-scheme: light dark;
  --text: light-dark(#000, #FFF);
  --bg: light-dark(#EEE, #242936);
}
hi there!you are awesome!i am!

By setting the color-scheme property to light dark, you are telling the browser to automatically pick the theme according to the user preference, and you can then make use of that by setting color values with the light-dark() function.
Not only does it set your own colors, but also those of the native components, such as the default buttons, form elements, and scrollbars. It kind of just makes stuff work by default, and that‚Äôs nice!
:root {
  color-scheme: light dark;
  &:has(#theme-light:checked) {
    color-scheme: light;
  }
  &:has(#theme-dark:checked) {
    color-scheme: dark;
  }
}

    Auto
    Light
    Dark
  

You can then add some way of overriding the color-scheme property to let the user pick a theme different from their system setting. Here I am using radio buttons to accomplish that.
Pro tip! CSS can‚Äôt save the theme preference, but you can still do progressive enhancement. Make the themes work CSS-only, and then add the saving/loading of preference as an optional extra in JavaScript or server-side code.
Lyres and accordions
‚ÄúBut those don‚Äôt look like radio buttons‚Äù I hear you cry.
Input elements such as radio buttons and checkboxes are a great foundation to build other stuff on top of - the example above consists of labels for the buttons and invisible radio buttons that can be checked for with the :checked pseudo-class.
<radio-picker aria-label="Radio buttons example" role="radiogroup">
  <label><input type="radio" name="demo" id="veni" checked>veni</label>
  <label><input type="radio" name="demo" id="vidi">vidi</label>
  <label><input type="radio" name="demo" id="vici">vici</label>
</radio-picker>
<style>
  radio-picker {
    display: flex;
    label {
      &:has(input:checked) {
        box-shadow: inset 0px 0px 8px 0px #888;
      }
      &:has(input:focus-visible) {
        outline: 2px solid #000;
      }
      box-shadow: inset 0px 0px 1.2px 0px #000;
      padding: 10px;
      cursor: pointer;
      background: #0002;
      &:hover { background: #0004; }
      &:active { background: #0006; }
    }
    input {
      /* To allow screen reader to still access these. */
      opacity: 0;
      position: absolute;
      pointer-events: none;
    }
  }
</style>


    veni
    vidi
    vici
  

This is how I made the theme selector from the previous example. I‚Äôve made the radio buttons half-visible in the demo for clarity, but with the opacity: 0 they would not actually be visible.
There‚Äôs a whole lot going on here, so let‚Äôs break it down.
<radio-picker aria-label="Radio buttons example" role="radiogroup">

We start off with the radio-picker element - I just made it up, you can use a div instead if you‚Äôd prefer. We give it an aria-label to give the group an accessible name, and the aria role of radiogroup to make it work as a group for the radio buttons.
You could also use the fieldset element instead of doing the aria roles if that‚Äôd fit your use case better.
<label><input type="radio" name="demo" id="veni" checked>veni</label>
<label><input type="radio" name="demo" id="vidi">vidi</label>
<label><input type="radio" name="demo" id="vici">vici</label>

Next, we add the radio buttons with their respective labels - usually you‚Äôd have to use the for attribute on labels to define which element they‚Äôre referring to, but since we have the input inside the label we don‚Äôt have to do that.
All the type="radio" inputs should also have a name value set to the same thing so that they are grouped together (you still need10 the radiogroup though). And then you can give them values or ids however you want.
label {
  &:has(input:checked) {
    box-shadow: inset 0px 0px 8px 0px #888;
  }
  &:has(input:focus-visible) {
    outline: 2px solid #000;
  }
  box-shadow: inset 0px 0px 1.2px 0px #000;
  padding: 10px;
  cursor: pointer;
  background: #0002;
  &:hover { background: #0004; }
  &:active { background: #0006; }
}

We then style the labels as we wish - the :hover and :active pseudo-classes can be used to make the buttons more fun to click, the :has(input:checked) selector can be used to define the style of the selected button, and the :has(input:focus-visible) selector can be used to add an outline when someone tabs over to the button.
The difference between :focus and :focus-visible is that the former shows up even if you use your mouse, while the latter only shows up when you use keyboard navigation, so it‚Äôs often visually more clean to use the latter.
input {
  opacity: 0;
  position: absolute;
  pointer-events: none;
}

And last, we make the radio button input exist while not being visible. This is a bit hacky, but it‚Äôs how you can keep this control accessible to keyboard navigation and screen readers.
And that‚Äôs how we get the cool-looking radio buttons!
<radio-tabs>
  <div tabindex=0 id="tab-veni">veni...</div>
  <div tabindex=0 id="tab-vidi">vidi...</div>
  <div tabindex=0 id="tab-vici">vici...</div>
</radio-tabs>
<style>
  body:has(#veni:not(:checked)) #tab-veni,
  body:has(#vidi:not(:checked)) #tab-vidi,
  body:has(#vici:not(:checked)) #tab-vici {
    display: none;
  }
</style>

    veni
    vidi
    vici


  veni/Ààve…™ni/(intransitive) to come
  vidi/ÀàviÀêdi/(intransitive) to see
  vici/ÀàviÀêtÕ° Éi/(intransitive) to conquer


We can now use them in the CSS however we want by just seeing if they‚Äôre :checked. Here I made tabs with separate divs for the content by using a :has selector on a parent element to find out which radio button is currently selected.
The :has selector has to be on a parent element that contains both the radio button and the target element - you can simply use html or body if you want it to work across the entire page. You should never use something like :has(‚Ä¶) by itself as it‚Äôll run the selector for every element of the page, which can cause performance issues (body:has(‚Ä¶) is okay).
<div>
  <details name="deets">
    <summary>What's your name?</summary>
    My name is Lyra Rebane.
  </details>
  <details name="deets">
    ...
  </details>
</div>
<style>
  div {
    border: 1px solid #AAA;
    border-radius: 8px;
    /* based on the MDN example */
    summary {
      font-weight: bold;
      margin: -0.5em -0.5em 0;
      padding: 0.5em;
      cursor: pointer;
    }
    details {
      &:last-child { border: none }
      border-bottom: 1px solid #aaa;
      padding: 0.5em 0.5em 0;
      &[open] {
        padding: 0.5em;
        summary {
          border-bottom: 1px solid #aaa;
          margin-bottom: 0.5em;
        }
      }
    }
  }
</style>

  
    
    What's your name?My name is Lyra Rebane.
    Cool name!I know ^_^
    Where can I learn more?On my website, lyra.horse!
  


Finally, before we move on, I want to give you a quick introduction to the details element. It‚Äôs great for if you want an accordion-style menu, such as for a FAQ section. The details open and close independently of each other, but you can set their name attribute to the same value to have only one open at a time.
Using them is pretty easy, put your content and a summary tag inside a details tag, and put the title inside the summary tag. The example above is a bit more convoluted for the visual flair, but all you really need is the html part of it.
The details elements are pretty stylable! You can add animations depending on the [open] state, and you can also get rid of the arrow by setting list-style: none on the summary.
Also, ctrl+f works with it, which is a big win in my book!
Validation
And lastly, I want to show you the power of input validation in HTML and CSS.
<label for="usrname">Username</label>
<input type="text" id="usrname" pattern="\w{3,16}" required>
<small>3-16 letters, only alphanum and _.</small>
<style>
 input:valid {
   border: 1px solid green;
 }
 input:invalid {
   border: 1px solid red;
 }
</style>

  Username
    
    3-16 letters, only alphanum and _.
  


This is a simple example of how you can validate an input field with a regex pattern. If you set a pattern attribute like above, a form that contains the input cannot be submitted unless the field matches the pattern. If you‚Äôre submitting something like an e-mail address, a phone number, or a url, it might make sense to use the respective input types instead of writing your own regex.
Now, where CSS comes in is styling the input to show whether its value is valid. In the example above, I‚Äôm using :valid and :invalid to set a border color, but that comes with the downside of always having your input marked, even if the user hasn‚Äôt entered anything yet.
input {
  border: none;
  border-radius: 2px;
  outline: 1px solid #000;
  &:focus { outline-width: 2px; }
  &:user-valid { outline-color: green; }
  &:user-invalid { outline-color: red; }
}

  Username
    
    3-16 letters, only alphanum and _.
  


An easy win here is to instead use :user-valid and :user-invalid - these pseudo-classes only become active once you‚Äôve interacted with input field. I also made this example use an outline instead of a border, which I think looks a lot nicer.
It may sometimes even make sense to use a combination of :valid and :user-invalid.
And of course, you can use the :has selector to style other elements depending on the input too!


  Password
    
    The password must:
- be 8-16 characters
- contain at least ‚Ö∞ roman numeral
- not end with a letter

  


This one's just for fun ^_-! you win! yay!
I do want to mention that for some stuff, such as date pickers () or datalists (), there are built-in elements that do the job, but you may find them limited in one way or the other. If you‚Äôre making an input like that with specific requirements, you may still need to dip your feet in a bit of JavaScript.

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


Do not the vw/vh
This section is kind of random but I wanted to include it here because I think a lot of people are messing this one up and I want more people to know how to do this stuff right.
So CSS has vw/vh units that correspond to 1% of the viewport width and height respectively, which makes perfect sense for desktop browsers.





  CBSignal chatAre you feeling encrypted?
  MMaratit smells of onions in here...
  bmblackle moriwhat's the scoop in yer smacker, horseberry?
  RRhynoraterCSS go BRRRRR
  PPatTheHyrulerI just lost the game
  MMalkI can't wait to taste the sorbet!


üîílyra.horse/blog/‚Ä¢‚Ä¢‚Ä¢

lyra's epic blog posts tags
You no longer need JavaScript
yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap




Where it becomes a bit more nuanced is on mobile devices. For example, mobile versions of both Firefox and Chrome will hide the URL bar when scrolling down on a page.
This causes the vw/vh units to be a bit ambigous - do they represent the entire available screen, only the area that‚Äôs visible with the URL bar, or something in between?
If it‚Äôs the first option, you might end up with buttons or links off-screen11! If it‚Äôs the second, you may end up with a background div that doesn‚Äôt cover the entire background.




  ppingotuxcss spec so good i transitioned
  mmayahi wife!!
  ZZvitlol. lmao, sogar.
  !!!! HAND !!yap yap yap
  JJonesGlory to KuK
  SSpaxim goop
  eenscribeI need a job


  lvh
  dvh
  svh











üîílyra.horse/blog/‚Ä¢‚Ä¢‚Ä¢




  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh

Your values

  
    
      Unit
      Value
    
  
  
    
      vh
      px
    
    
      lvh
      px
    
    
      dvh
      px
    
    
      svh
      px
    
  

Above is a table of values your browser reports - if you're on mobile, try scrolling the blogpost up and down so that the URL bar hides and see how the numbers change.
The values are multiplied by 100 (eg 100vh is used instead of 1vh).




The solution to this is to use the new responsive viewport units: lvh, svh, and dvh.
lvh stands for largest viewport height, and thus is useful for things like backgrounds that you‚Äôd want to cover the entire screen with, and wouldn‚Äôt care about getting cut off.
svh stands for smallest viewport height, and should be used for things that must always fit on the screen, such as buttons and links.
And dvh stands for dynamic viewport height - this one will update to whatever the current viewport height is. It might seem like the obvious choice, but it should not be used for elements you don‚Äôt want resizing or moving around as the user scrolls the page, as it could become quite annoying and possibly even laggy otherwise.
Of course, the respective lvw, svw, and dvw units exist too :).
Keyboard cat
By default, the viewport units do not account for the keyboard overlaying the page.
There are two ways to deal with that: the interactive-widget attribute, and the VirtualKeyboard API.
The former option is widely supported across browsers, works without JS, and goes in the meta viewport tag. It makes it so that opening the keyboard will change all of the viewport units.
<meta name="viewport" content="width=device-width, interactive-widget=resizes-content">

The latter option is currently only supported in Chromium-based browsers, and requires a single line of JavaScript to use:
navigator.virtualKeyboard.overlaysContent = true;

The advantage of the second option is that it allows you to use environment variables in CSS to get the position and size of the keyboard, which is pretty cool.
floating-button {
  margin-bottom: env(keyboard-inset-height, 0px);
}

But considering the fact that it doesn‚Äôt work cross-browser, I‚Äôd avoid it.
CSS wishlist
Alright, so this is a little different from the rest of the post, but I wanted to bring up some things that I wish were in CSS. I haven‚Äôt fully fleshed out all of them, so some definitely wouldn‚Äôt fit the spec as-is, but maybe they can inspire some other stuff at least.
They are just fun ideas, don‚Äôt take them too seriously.
Reusable blocks
I wish it was possible to put classes in other classes in CSS, so that you could write something like:
.border {
  border: 2px solid;
  border-radius: 4px;
}

.button {
  @apply border;
}

.card {
  @apply border;
}

This is something that Tailwind already has, and that makes me jealous.

Combined @media selectors
We can currently do nested @media queries, and also multiple selectors at the same time:
div {
  &.foo, &.bar {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
  @media (width < 480px) {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

But we cannot combine the two into a single selector:
div {
  @media (width < 480px), &.foo {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

Which means if you want to do that you‚Äôll inevitably have to repeat code or do some silly variable hacks, neither of which is ideal.
n-th child variable
For many of the CSS crimes I like to commit, I often end up writing code like:
div {
  span:nth-child(1) { --nth: 1; }
  span:nth-child(2) { --nth: 2; }
  span:nth-child(3) { --nth: 3; }
  span:nth-child(4) { --nth: 4; }
  span:nth-child(5) { --nth: 5; }
  ...
  span {
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

And I think it would be a lot nicer if we could instead just do:
div {
  span {
    --nth: nth-child();
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

n-th letter targeting
CSS has the ability to style the ::first-letter of text. It‚Äôd be cool if were was also a ::nth-letter(‚Ä¶) selector, similar to :nth-child. I suspect the reason this isn‚Äôt a thing is because the ::first-letter selector is a pseudo-element, which would be a bit tricky to implement with the nth-letter idea.
/* not a real feature */
p::nth-letter(2) {
  color: red;
}

hi there~


Blackle suggested that combining the nth-child() variable with :nth-letter targeting would also be fun for certain effects, such as putting the value in the sin() function to create wavy text.
div {
  /* not a real feature */
  --nth: nth-child(nth-letter);
  will-change: transform;
  translate: 0 calc(sin(var(--nth) * 0.35 - var(--wave) * 3) * 5px);
  color: color-mix(in oklch, #58C8F2, #EDA4B2 calc(sin(var(--nth) * 0.5 - var(--wave)) * 50% + 50%));
}

  
    untucknowqueen
  
  (taphover to play animation)


Unit removal
I wish you could easily remove units from values, for example by dividing them.

div {
  /* Turns into:  (no unit) */
  --screen-width: calc(100vw / 1px);
  color: hsl(var(--screen-width) 100, 50);
}
This would allow you to use the size of the viewport or container as a numeric variable for things other than length. For example, the color picker from earlier uses it to convert the location of the color picker dot to a number to be used in a color value instead.
Uh, but wait? Does that mean this feature already exists?
Yeah, lol! We already have the ability to get unitless values in CSS, but it involves doing hacky stuff such as tan(atan2(var(--vw), 1px)) with a custom @property. It‚Äôd be nice to have this as just a division, for example.
Oh, and good news, this one we might actually be getting soon!
Also if you do something like calc(1px + sqrt(1px * 1px)) your browser will crash12.
A better image function
The image() function exists, but no browsers implement it. It‚Äôs similar to just using url(), but adds some really cool features such as a fallback color, and image fragments to crop a smaller section out of a bigger image (think spritesheets).
We can already do both fallbacks and spritesheets with the various background properties, but it‚Äôd be nice to have this pretty syntax. I‚Äôd honestly love this syntax even more for <img> tags than CSS.
style tags in body
I make heavy use of <style> tags in <body> for my projects. On my blog, for example, I write the relevant CSS close to their graphics so that you can start reading the blog before the entire page (or the entire CSS) has finished loading13. And it works great!
But what‚Äôs unfortunate is that despite browsers supporting this, and major sites using this, it‚Äôs not officially spec-compliant. I suspect it‚Äôs in the spec to avoid the FOUC footgun, but there are so many reasons you would want/need style in body that I don‚Äôt think it justifies it.
I think an HTML validator should warn for this, but not error.
The art
I want to end this article by saying that to me, web development is an art, and thus, CSS is too. I often have a hard time relating to people who do webdev solely to earn money or build a startup - web development is very different when you‚Äôre on a team and are given tasks from above instead of having free will over what you create for fun.
It‚Äôs probably most apparent with things like AI14, that for me take all the fun and creativity out of my work. But it also applies to build chain tooling such as linters and minifiers - the way I write my code is part of the art, and I don‚Äôt want a tool to erase that. I don‚Äôt even use an IDE15.
Among the practical reasons for sticking to CSS listed throughout this post, there‚Äôs a secret extra reason I like to do everything in CSS, and that‚Äôs expression and art. Art isn‚Äôt always practical, and using CSS isn‚Äôt either. But it‚Äôs how I like to express myself, and it‚Äôs why I do what I do.
I tried to keep this post approachable and practical for all web developers. But there is so much more to CSS that I‚Äôd like to talk about, so expect another post about the stuff that isn‚Äôt practical, and is instead just cool as fuck. I think CSS is a programming language, and I made a game to prove it.
But that‚Äôs a topic for another time.
afterword
it‚Äôs been almost a year since my last post, but i hope it‚Äôs been worth the wait ^_^
as usual, this post is a self-contained html file with no javascript, images, or other external resources - everything on the page is handwritten html/css, weighing in at around 49kB gzipped. it was really fun creating all the little interactive widgets and visuals this time around, i think i‚Äôve improved in css a lot since the last time i posted.
this entire post turned out to be a bit of a fun mess (as did i!), it‚Äôs almost like a chaotic gradient of tone throughout, i hope it was still interesting and enjoyable to read though.
i have a few new posts in the works: in addition to the second css one mentioned earlier, i also have one about a new web vulnerability subclass i discovered, and one about a trans topic. i‚Äôm not sure when these posts will come out, but we‚Äôll see! make sure to add me to your rss reader if that sounds fun.
i‚Äôll also be giving a talk at bsides tallinn in september! i‚Äôm hoping to also do css-related talks at the next ccc and disobey, but we‚Äôll have to see whether i get accepted and have the travel budget for those.
thank you so much for reading <3
you're awesome!! (i can tell because you checked that checkbox from earlier)

Discuss this post on: twitter, mastodon, lobsters





Chrome‚Äôs DevTools come with the cool flexbox widget. Firefox‚Äôs however don‚Äôt seem to for some reason? I find that weird because Firefox does have really good tools for flexbox and grid development, so this seems like an odd omission.¬†‚Ü©Ô∏é


While I think what I said is true, Tailwind does have more to its existence, the core of which can be found in this post by its creator.¬†‚Ü©Ô∏é


You are allowed to just make up elements as long as their names contain a hyphen. Apart from the 8 existing tags listed at the link, no HTML tags contain a hyphen and none ever will. The spec even has <math-Œ±> and <emotion-üòç> as examples of allowed names. You are allowed to make up attributes on an autonomous custom element, but for other elements (built-in or extended) you should only make up data-* attributes. I make heavy use of this on my blog to make writing HTML and CSS nicer and avoid meaningless div-soup.¬†‚Ü©Ô∏é


Still not nice to read for you? I‚Äôm personally not a fan of BEM, but I‚Äôd definitely recommend reading up on it too if you just don‚Äôt vibe with the way I‚Äôm writing my examples. Also, my example intentionally shows off a lot of the syntax at once, but in the real world it might make sense to structure things a little differently.¬†‚Ü©Ô∏é


Baseline browsers are Safari (macOS/iOS), Chrome (desktop/Android), Edge (desktop), and Firefox (desktop/Android).¬†‚Ü©Ô∏é


The MDN docs of course also list detailed browser compatibility, but the Baseline symbols are nice for just getting a quick ‚Äúyeah, we can use it and it‚Äôll work for everyone‚Äù type overview.¬†‚Ü©Ô∏é


ES3 (1999) is the last ‚Äúclassic‚Äù version of JavaScript. In 2009 we got the first major revision known as ES5, and a few years later we kicked off the yearly spec updates with ES2015. Also ES4 was abandoned which makes me feel sad :c.¬†‚Ü©Ô∏é


93 files!! Seems like they‚Äôre 1/3 functionality, 1/3 ads, and 1/3 analytics. The site works just fine with JavaScript disabled - only stuff like the comments section and ads won‚Äôt load. It‚Äôs no longer a laggy mess either for some reason.¬†‚Ü©Ô∏é


I think the x3ctf challenges page looks really smooth on my computer - the marquee text animation and clicking on the challenges is buttery. And it also runs pretty well on the low-end hardware I have. Note that some browser performance recording tools can act a bit weird with CSS animations, so make sure your tools are working as expected before using them. Unrelated, but I made some other cool x3ctf web stuff too - check out the archive.¬†‚Ü©Ô∏é


There‚Äôs a bug in Chrome that requires you to use a fieldset/radiogroup for the radio button index to work correctly in screenreaders. Eg if you have 3 radio buttons with the same name, selecting one of them should read ‚Äúradio button 1 of 3‚Äù, which is what Firefox does, but in Chrome it will instead read it as ‚Äúradio button 4 of 9‚Äù or whatever if you don‚Äôt have a fieldset/radiogroup because it kind of just combines all the radio buttons on the page into a single index.¬†‚Ü©Ô∏é


A certain HR platform I have to use puts its action buttons at the very bottom of a 100vh container, leading to them not being visible/interactable on my phone - not a headache you want to go through when requesting sick days. It‚Äôs a good example of how just using the wrong unit can cause a pretty bad real world accessibility problem.¬†‚Ü©Ô∏é


Well, probably not. This is a bug I found while writing this post that only affects Chrome, and it‚Äôll probably get fixed before it even manages to hit stable. Update: I took so long to get this blog post out that it has been fixed now. During the writing of this blog post I found another bug in Chrome though, which is pretty funny. Update 2: I found yet another Chrome bug while writing this post, this one is kinda weird, you should read it.¬†‚Ü©Ô∏é


This matters for people on slow connections, such as bad mobile data, satellite internet, tor, or iodine. While my blog posts are very small in size, the CSS alone can take up more than the first 14kB of a TCP round trip, so with blocking CSS in the head you might have to wait a few extra seconds (or minutes, in the case of iodine) just to start reading the first paragraph. Now, that 14kB number isn‚Äôt completely accurate in the modern world, but testing on my own server (HTTP/2, TLS 1.3), around ~16kB of the compressed html reaches the browser in the first batch of http data.¬†‚Ü©Ô∏é


By this I mean tools such as Copilot, Cursor, chatbots etc. I understand there is a huge difference between full-on vibe coding and just using the tab key, but I do not want to use or interact with any of those tools. Please respect that.¬†‚Ü©Ô∏é


I write all my code (and blogposts) in Sublime Text, which to me is just a glorified version of Notepad. The features over Notepad it gives me are syntax highlighting, multiple cursors, keyboard shortcuts, and a better visual design. It doesn‚Äôt do that much, and yet, it‚Äôs perfect. It‚Äôs so good I paid for it.¬†‚Ü©Ô∏é





  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RFC 8594: The Sunset HTTP Header Field (2019)]]></title>
            <link>https://datatracker.ietf.org/doc/html/rfc8594</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056142</guid>
            <description><![CDATA[This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future. It also defines a sunset link relation type that allows linking to resources providing information about an upcoming resource or service sunset.]]></description>
            <content:encoded><![CDATA[Internet Engineering Task Force (IETF)                          E. Wilde
Request for Comments: 8594                                      May 2019
Category: Informational
ISSN: 2070-1721


                      The Sunset HTTP Header Field

Abstract

   This specification defines the Sunset HTTP response header field,
   which indicates that a URI is likely to become unresponsive at a
   specified point in the future.  It also defines a sunset link
   relation type that allows linking to resources providing information
   about an upcoming resource or service sunset.

Status of This Memo

   This document is not an Internet Standards Track specification; it is
   published for informational purposes.

   This document is a product of the Internet Engineering Task Force
   (IETF).  It represents the consensus of the IETF community.  It has
   received public review and has been approved for publication by the
   Internet Engineering Steering Group (IESG).  Not all documents
   approved by the IESG are candidates for any level of Internet
   Standard; see Section¬†2 of RFC 7841.

   Information about the current status of this document, any errata,
   and how to provide feedback on it may be obtained at
   https://www.rfc-editor.org/info/rfc8594.

Copyright Notice

   Copyright (c) 2019 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (https://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.





Wilde                         Informational                     [Page 1]

RFC 8594                      Sunset Header                     May 2019


Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Temporary Resources . . . . . . . . . . . . . . . . . . .   3
     1.2.  Migration . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.3.  Retention . . . . . . . . . . . . . . . . . . . . . . . .   3
     1.4.  Deprecation . . . . . . . . . . . . . . . . . . . . . . .   3
   2.  Terminology . . . . . . . . . . . . . . . . . . . . . . . . .   4
   3.  The Sunset HTTP Response Header Field . . . . . . . . . . . .   4
   4.  Sunset and Caching  . . . . . . . . . . . . . . . . . . . . .   5
   5.  Sunset Scope  . . . . . . . . . . . . . . . . . . . . . . . .   6
   6.  The Sunset Link Relation Type . . . . . . . . . . . . . . . .   6
   7.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .   7
     7.1.  The Sunset Response Header Field  . . . . . . . . . . . .   7
     7.2.  The Sunset Link Relation Type . . . . . . . . . . . . . .   8
   8.  Security Considerations . . . . . . . . . . . . . . . . . . .   8
   9.  Example . . . . . . . . . . . . . . . . . . . . . . . . . . .   9
   10. References  . . . . . . . . . . . . . . . . . . . . . . . . .  10
     10.1.  Normative References . . . . . . . . . . . . . . . . . .  10
     10.2.  Informative References . . . . . . . . . . . . . . . . .  10
   Acknowledgements  . . . . . . . . . . . . . . . . . . . . . . . .  10
   Author's Address  . . . . . . . . . . . . . . . . . . . . . . . .  11

1.  Introduction

   As a general rule, URIs should be stable and persistent so that
   applications can use them as stable and persistent identifiers for
   resources.  However, there are many scenarios where, for a variety of
   reasons, URIs have a limited lifetime.  In some of these scenarios,
   this limited lifetime is known in advance.  In this case, it can be
   useful for clients if resources make this information about their
   limited lifetime known.  This specification defines the Sunset HTTP
   response header field, which indicates that a URI is likely to become
   unresponsive at a specified point in the future.

   This specification also defines a sunset link relation type that
   allows information to be provided about 1) the sunset policy of a
   resource or a service, and/or 2) upcoming sunsets, and/or 3) possible
   mitigation scenarios for resource/service users.  This specification
   does not place any constraints on the nature of the linked resource,
   which can be targeted to humans, machines, or both.

   Possible scenarios for known lifetimes of resources include, but are
   not limited to, the following scenarios.







Wilde                         Informational                     [Page 2]

RFC 8594                      Sunset Header                     May 2019


1.1.  Temporary Resources

   Some resources may have a limited lifetime by definition.  For
   example, a pending shopping order represented by a resource may
   already list all order details, but it may only exist for a limited
   time unless it is confirmed and only then becomes an acknowledged
   shopping order.  In such a case, the service managing the pending
   shopping order can make this limited lifetime explicit, allowing
   clients to understand that the pending order, unless confirmed, will
   disappear at some point in time.

1.2.  Migration

   If resources are changing identity because a service migrates them,
   then this may be known in advance.  While it may not yet be
   appropriate to use HTTP redirect status codes (3xx), it may be
   interesting for clients to learn about the service's plan to take
   down the original resource.

1.3.  Retention

   There are many cases where regulation or legislation require that
   resources are kept available for a certain amount of time.  However,
   in many cases there is also a requirement for those resources to be
   permanently deleted after some period of time.  Since the deletion of
   the resource in this scenario is governed by well-defined rules, it
   could be made explicit for clients interacting with the resource.

1.4.  Deprecation

   For Web APIs one standard scenario is that an API or specific subsets
   of an API may get deprecated.  Deprecation often happens in two
   stages: the first stage being that the API is not the preferred or
   recommended version anymore and the second stage being that the API
   or a specific version of the API gets decommissioned.

   For the first stage (the API is not the preferred or recommended
   version anymore), the Sunset header field is not appropriate: at this
   stage, the API remains operational and can still be used.  Other
   mechanisms can be used for signaling that first stage that might help
   with more visible deprecation management, but the Sunset header field
   does not aim to represent that information.

   For the second stage (the API or a specific version of the API gets
   decommissioned), the Sunset header field is appropriate: that is when
   the API or a version does become unresponsive.  From the Sunset
   header field's point of view, it does not matter that the API may not




Wilde                         Informational                     [Page 3]

RFC 8594                      Sunset Header                     May 2019


   have been the preferred or recommended version anymore.  The only
   thing that matters is that it will become unresponsive and that this
   time can be advertised using the Sunset header field.

   In this scenario, the announced sunset date typically affects all of
   the deprecated API or parts of it (i.e., just deprecated sets of
   resources), and not just a single resource.  In this case, it makes
   sense for the API to define rules about how an announced sunset on a
   specific resource (such as the API's home/start resource) implies the
   sunsetting of the whole API or parts of it (i.e., sets of resources),
   and not just the resource returning the sunset header field.
   Section 5 discusses how the scope of the Sunset header field may
   change because of how a resource is using it.

2.  Terminology

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in
   BCP 14 [RFC2119] [RFC8174] when, and only when, they appear in all
   capitals, as shown here.

3.  The Sunset HTTP Response Header Field

   The Sunset HTTP response header field allows a server to communicate
   the fact that a resource is expected to become unresponsive at a
   specific point in time.  It provides information for clients that
   they can use to control their usage of the resource.

   The Sunset header field contains a single timestamp that advertises
   the point in time when the resource is expected to become
   unresponsive.  The Sunset value is an HTTP-date timestamp, as defined
   in Section¬†7.1.1.1 of [RFC7231], and SHOULD be a timestamp in the
   future.

   It is safest to consider timestamps in the past mean the present
   time, meaning that the resource is expected to become unavailable at
   any time.

   Sunset = HTTP-date

   For example:

   Sunset: Sat, 31 Dec 2018 23:59:59 GMT







Wilde                         Informational                     [Page 4]

RFC 8594                      Sunset Header                     May 2019


   Clients SHOULD treat Sunset timestamps as hints: it is not guaranteed
   that the resource will, in fact, be available until that time and
   will not be available after that time.  However, since this
   information is provided by the resource itself, it does have some
   credibility.

   After the Sunset time has arrived, it is likely that interactions
   with the resource will result in client-side errors (HTTP 4xx status
   codes), redirect responses (HTTP 3xx status codes), or the client
   might not be able to interact with the resource at all.  The Sunset
   header field does not expose any information about which of those
   behaviors can be expected.

   Clients not interpreting an existing Sunset header field can operate
   as usual and simply may experience the resource becoming unavailable
   without recognizing any notification about it beforehand.

4.  Sunset and Caching

   It should be noted that the Sunset HTTP response header field serves
   a different purpose than HTTP caching [RFC7234].  HTTP caching is
   concerned with making resource representations (i.e., represented
   resource state) reusable so that they can be used more efficiently.
   This is achieved by using header fields that allow clients and
   intermediaries to better understand when a resource representation
   can be reused or when resource state (and, thus, the representation)
   may have changed.

   The Sunset header field is not concerned with resource state at all.
   It only signals that a resource is expected to become unavailable at
   a specific point in time.  There are no assumptions about if, when,
   or how often a resource may change state in the meantime.

   For these reasons, the Sunset header field and HTTP caching should be
   seen as complementary and not as overlapping in scope and
   functionality.

   This also means that applications acting as intermediaries, such as
   search engines or archives that make resources discoverable, should
   treat Sunset information differently from caching information.  These
   applications may use Sunset information for signaling to users that a
   resource may become unavailable.  But they still have to account for
   the fact that resource state can change in the meantime and that
   Sunset information is a hint and, thus, future resource availability
   may differ from the advertised timestamp.






Wilde                         Informational                     [Page 5]

RFC 8594                      Sunset Header                     May 2019


5.  Sunset Scope

   The Sunset header field applies to the resource that returns it,
   meaning that it announces the upcoming sunset of that specific
   resource.  However, as discussed in Section 1.4, there may be
   scenarios where the scope of the announced Sunset information is
   larger than just the single resource where it appears.

   Resources are free to define such an increased scope, and usually
   this scope will be documented by the resource so that consumers of
   the resource know about the increased scope and can behave
   accordingly.  However, it is important to take into account that such
   increased scoping is invisible for consumers who are unaware of the
   increased scoping rules.  This means that these consumers will not be
   aware of the increased scope, and they will not interpret Sunset
   information different from its standard meaning (i.e., it applies to
   the resource only).

   Using such an increased scope still may make sense, as Sunset
   information is only a hint anyway; thus, it is optional information
   that cannot be depended on, and clients should always be implemented
   in ways that allow them to function without Sunset information.
   Increased scope information may help clients to glean additional
   hints from resources (e.g., concluding that an API is being
   deprecated because its home/start resource announces a Sunset) and,
   thus, might allow them to implement behavior that allows them to make
   educated guesses about resources becoming unavailable.

6.  The Sunset Link Relation Type

   The Sunset HTTP header field indicates the upcoming retirement of a
   resource or a service.  In addition, a resource may want to make
   information available that provides additional information about how
   retirement will be handled for resources or services.  This
   information can be broadly described by the following three topics:

   Sunset policy:  The policy for which resources and in which way
         sunsets may occur may be published as part of service's
         description.  Sunsets may only/mostly affect a subset of a
         service's resources, and they may be exposed according to a
         certain policy (e.g., one week in advance).

   Upcoming sunset:  There may be additional information about an
         upcoming sunset, which can be published as a resource that can
         be consumed by those looking for this additional information.






Wilde                         Informational                     [Page 6]

RFC 8594                      Sunset Header                     May 2019


   Sunset mitigation:  There may be information about possible
         mitigation/migration strategies, such as possible ways how
         resource users can switch to alternative resources/services.

   Any information regarding the above issues (and possibly additional
   ones) can be made available through a URI that then can be linked to
   using the sunset link relation type.  This specification places no
   constraints on the scope or the type of the linked resource.  The
   scope can be for a resource or for a service.  The type is determined
   by the media type of the linked resource and can be targeted to
   humans, machines, or both.

   If the linked resource does provide machine-readable information,
   consumers should be careful before acting on this information.  Such
   information may, for example, instruct consumers to use a migration
   rule so that sunset resources can be accessed at new URIs.  However,
   this kind of information amounts to a possibly large-scale identity
   migration of resources, so it is crucial that the migration
   information is authentic and accurate.

7.  IANA Considerations

7.1.  The Sunset Response Header Field

   The Sunset response header field has been added to the "Permanent
   Message Header Field Names" registry (see [RFC3864]), taking into
   account the guidelines given by HTTP/1.1 [RFC7231].

      Header Field Name: Sunset

      Protocol: http

      Status: informational

      Author/Change controller: IETF

      Reference: RFC 8594














Wilde                         Informational                     [Page 7]

RFC 8594                      Sunset Header                     May 2019


7.2.  The Sunset Link Relation Type

   The sunset link relation type has been added to the permanent "Link
   Relation Types" registry according to Section¬†4.2 of [RFC8288]:

      Relation Name: sunset

      Description: Identifies a resource that provides information about
      the context's retirement policy.

      Reference: RFC 8594

8.  Security Considerations

   Generally speaking, information about upcoming sunsets can leak
   information that otherwise might not be available.  For example, a
   resource representing a registration can leak information about the
   expiration date when it exposes sunset information.  For this reason,
   any use of sunset information where the sunset represents an
   expiration or allows the calculation of another date (such as
   calculating a creation date because it is known that resources expire
   after one year) should be treated in the same way as if this
   information would be made available directly in the resource's
   representation.

   The Sunset header field SHOULD be treated as a resource hint, meaning
   that the resource is indicating (and not guaranteeing with certainty)
   its potential retirement.  The definitive test whether or not the
   resource in fact is available will be to attempt to interact with it.
   Applications should never treat an advertised Sunset date as a
   definitive prediction of what is going to happen at the specified
   point in time: the Sunset indication may have been inserted by an
   intermediary or the advertised date may get changed or withdrawn by
   the resource owner.

   The main purpose of the Sunset header field is to signal intent so
   that applications using resources may get a warning ahead of time and
   can react accordingly.  What an appropriate reaction is (such as
   switching to a different resource or service), what it will be based
   on (such as machine-readable formats that allow the switching to be
   done automatically), and when it will happen (such as ahead of the
   advertised date or only when the resource in fact becomes
   unavailable) is outside the scope of this specification.

   In cases where a sunset policy is linked by using the sunset link
   relation type, clients SHOULD be careful about taking any actions
   based on this information.  It SHOULD be verified that the
   information is authentic and accurate.  Furthermore, it SHOULD be



Wilde                         Informational                     [Page 8]

RFC 8594                      Sunset Header                     May 2019


   tested that this information is only applied to resources that are
   within the scope of the policy, making sure that sunset policies
   cannot "hijack" resources by for example providing migration
   information for them.

9.  Example

   If a resource has been created in an archive that, for management or
   compliance reasons, stores resources for ten years and permanently
   deletes them afterwards, the Sunset header field can be used to
   expose this information.  If such a resource has been created on
   November 11, 2016, then the following header field can be included in
   responses:

   Sunset: Wed, 11 Nov 2026 11:11:11 GMT

   This allows clients that are aware of the Sunset header field to
   understand that the resource likely will become unavailable at the
   specified point in time.  Clients can decide to ignore this
   information, adjust their own behavior accordingly, or alert
   applications or users about this timestamp.

   Even though the Sunset header field is made available by the resource
   itself, there is no guarantee that the resource indeed will become
   unavailable, and if so, how the response will look like for requests
   made after that timestamp.  In case of the archive used as an example
   here, the resource indeed may be permanently deleted, and requests
   for the URI after the Sunset timestamp may receive a "410 Gone" HTTP
   response.  (This is assuming that the archive keeps track of the URIs
   that it had previously assigned; if not, the response may be a more
   generic "404 Not Found".)

   Before the Sunset header field even appears for the first time (it
   may not appear from the very beginning), it is possible that the
   resource (or possibly just the "home" resource of the service
   context) communicates its sunset policy by using the sunset link
   relation type.  If communicated as an HTTP header field, it might
   look as follows:

   Link: <http://example.net/sunset>;rel="sunset";type="text/html"

   In this case, the linked resource provides sunset policy information
   about the service context.  It may be documentation aimed at
   developers, for example, informing them that the lifetime of a
   certain class of resources is ten years after creation and that
   Sunset header fields will be served as soon as the sunset date is





Wilde                         Informational                     [Page 9]

RFC 8594                      Sunset Header                     May 2019


   less than some given period of time.  It may also inform developers
   whether the service will respond with 410 or 404 after the sunset
   time, as discussed above.

10.  References

10.1.  Normative References

   [RFC2119]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119,
              DOI 10.17487/RFC2119, March 1997,
              <https://www.rfc-editor.org/info/rfc2119>.

   [RFC3864]  Klyne, G., Nottingham, M., and J. Mogul, "Registration
              Procedures for Message Header Fields", BCP 90, RFC 3864,
              DOI 10.17487/RFC3864, September 2004,
              <https://www.rfc-editor.org/info/rfc3864>.

   [RFC7231]  Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer
              Protocol (HTTP/1.1): Semantics and Content", RFC 7231,
              DOI 10.17487/RFC7231, June 2014,
              <https://www.rfc-editor.org/info/rfc7231>.

   [RFC8174]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC
              2119 Key Words", BCP 14, RFC 8174, DOI 10.17487/RFC8174,
              May 2017, <https://www.rfc-editor.org/info/rfc8174>.

   [RFC8288]  Nottingham, M., "Web Linking", RFC 8288,
              DOI 10.17487/RFC8288, October 2017,
              <https://www.rfc-editor.org/info/rfc8288>.

10.2.  Informative References

   [RFC7234]  Fielding, R., Ed., Nottingham, M., Ed., and J. Reschke,
              Ed., "Hypertext Transfer Protocol (HTTP/1.1): Caching",
              RFC 7234, DOI 10.17487/RFC7234, June 2014,
              <https://www.rfc-editor.org/info/rfc7234>.

Acknowledgements

   Thanks for comments and suggestions provided by Ben Campbell, Alissa
   Cooper, Benjamin Kaduk, Mirja Kuhlewind, Adam Roach, Phil Sturgeon,
   and Asbjorn Ulsberg.








Wilde                         Informational                    [Page 10]

RFC 8594                      Sunset Header                     May 2019


Author's Address

   Erik Wilde

   Email: erik.wilde@dret.net
   URI:   http://dret.net/netdret/













































Wilde                         Informational                    [Page 11]
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[I‚Äôm about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

I‚Äôve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren‚Äôt taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn‚Äôt very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that‚Äôs going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

I‚Äôm often asked, ‚Äúwhat is the future of programming?‚Äù Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it‚Äôs too late? My answer to all these questions is ‚ÄúI haven‚Äôt the foggiest‚Äù. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùá¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

I‚Äôm also asked: ‚Äúis AI a bubble‚Äù? To which my answer is ‚ÄúOF COURSE IT‚ÄôS A BUBBLE‚Äù. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don‚Äôt know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan‚Ä¶ but it did not kill Amazon.

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

I retired from public speaking a couple of years ago. But while I don‚Äôt miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I‚Äôm looking forward to catching up with many of them at GOTO Copenhagen. I‚Äôve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ú¢¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren‚Äôt a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it‚Äôs just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn‚Äôt ask an LLM to calculate an answer than we can calculate deterministically (yes, I‚Äôve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can‚Äôt measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

I‚Äôve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say ‚Äúall tests green‚Äù, yet when I run them, there are failures. If that was a junior engineer‚Äôs behavior, how long would it be before H.R. was involved?

¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†‚ùÑ

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (‚Äúexfiltration‚Äù). That ‚Äúuntrusted content‚Äù can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attacker‚Äôs web page, and it could trick the agent to go to your bank account in another tab and ‚Äúbuy you a present‚Äù by transferring your balance to the kind attacker. Willison‚Äôs view is that ‚Äúthe entire concept of an agentic browser extension is fatally flawed and cannot be built safely‚Äù.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Bot Auth]]></title>
            <link>https://developers.cloudflare.com/bots/reference/bot-verification/web-bot-auth/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055452</guid>
            <description><![CDATA[Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.]]></description>
            <content:encoded><![CDATA[          Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.
It relies on two active IETF drafts: a directory draft ‚Üó allowing the crawler to share their public keys, and a protocol draft ‚Üó defining how these keys should be used to attach crawler's identity to HTTP requests.
This documentation goes over specific integration within Cloudflare.

You need to generate a signing key which will be used to authenticate your bot's requests.



Generate a unique Ed25519 ‚Üó private key to sign your requests. This example uses the OpenSSL ‚Üó genpkey command:
openssl genpkey -algorithm ed25519 -out private-key.pem


Extract your public key.
openssl pkey -in private-key.pem -pubout -out public-key.pem


Convert the public key to JSON Web Key (JWK) using a tool of your choice. This example uses jwker ‚Üó command line application.
go install github.com/jphastings/jwker/cmd/jwker@latestjwker public-key.pem public-key.jwk


By following these steps, you have generated a private key and a public key, then converted the public key to a JWK.


You need to host a key directory which creates a way for your bot to authenticate its requests to Cloudflare.
This directory should follow the definition from the active IETF draft draft-meunier-http-message-signatures-directory-01 ‚Üó.


Host a key directory at /.well-known/http-message-signatures-directory (note that this is a requirement). This key directory should serve a JSON Web Key Set (JWKS) including the public key derived from your signing key.


Serve the web page over HTTPS (not HTTP).


Calculate the base64 URL-encoded JWK thumbprint ‚Üó associated with your Ed25519 public key.


Sign your HTTP response using the HTTP message signature specification by attaching one signature per key in your key directory. This ensures no one else can mirror your directory and attempt to register on your behalf. Your response must include the following headers:

Content-Type: This header must have the value application/http-message-signatures-directory+json.
Signature: Construct a Signature header ‚Üó over your chosen components.
Signature-Input: Construct a Signature-Input header ‚Üó over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to http-message-signatures-directory.keyidJWK thumbprint of the corresponding key in your directory.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message.


The following example shows the annotated request and response with required headers against https://example.com.
GET /.well-known/http-message-signatures-directory HTTP/1.1Host: example.comAccept: application/http-message-signatures-directory+jsonHTTP/1.1 200 OKContent-Type: application/http-message-signatures-directory+jsonSignature: sig1=:TD5arhV1ved6xtx63cUIFCMONT248cpDeVUAljLgkdozbjMNpJGr/WAx4PzHj+WeG0xMHQF1BOdFLDsfjdjvBA==:Signature-Input: sig1=("@authority");alg="ed25519";keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U";nonce="ZO3/XMEZjrvSnLtAP9M7jK0WGQf3J+pbmQRUpKDhF9/jsNCWqUh2sq+TH4WTX3/GpNoSZUa8eNWMKqxWp2/c2g==";tag="http-message-signatures-directory";created=1750105829;expires=1750105839Cache-Control: max-age=86400{  "keys": [{    "kty": "OKP",    "crv": "Ed25519",    "x": "JrQLj5P_89iXES9-vFgrIy29clF9CC_oPPsw3c5D0bs", // Base64 URL-encoded public key, with no padding  }]}



You can use the Cloudflare-developed http-signature-directory CLI tool ‚Üó to assist you in validating your directory.

You need to register your bot and its key directory to add your bot to the list of verified bots.

Log in to the Cloudflare dashboard ‚Üó, and select your account and domain.
Go to Manage Account > Configurations.
Go to the Verified Bots tab.
For Verification Method: select Request Signature.
For Validation Instructions: enter the URL of your key directory. You can additionally supply User Agents values (and their match patterns) that will be sent by your bot.
Select Submit.

Cloudflare accepts all valid Ed25519 keys found in your key directory. In the event a key already exists in Cloudflare's registered database, Cloudflare will work with you to supply a new key, or rotate your existing key.


After your bot has been successfully verified, your bot is ready to sign its requests. The signature protocol is defined in draft-meunier-web-bot-auth-architecture-02 ‚Üó

Choose a set of components to sign.
A component is either an HTTP header, or any derived components ‚Üó in the HTTP Message Signatures specification. Cloudflare recommends the following:

Choose at least the @authority derived component, which represents the domain you are sending requests to. For example, a request to https://example.com will be interpreted to have an @authority of example.com.
Use components that only contain ASCII values. HTTP Message Signature specification disallows non-ASCII characters, which will result in failure to validate your bot's requests.




Calculate the base64 URL-encoded JWK thumbprint ‚Üó from the public key you registered with Cloudflare.

Construct the three required headers for Web Bot Auth.

Construct a Signature-Input header ‚Üó over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to web-bot-auth.keyidThis should be equal to the thumbprint computed in step 2.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message. A short expires reduces the likelihood of replay attacks, and Cloudflare recommends choosing suitable short-lived intervals.

Construct a Signature header ‚Üó over your chosen components.

Construct a Signature-Agent header ‚Üó that points to your key directory. Note that Cloudflare will fail to verify a message if:

The message includes a Signature-Agent header that is not an https://.
The message includes a valid URI but does not enclose it in double quotes. This is due to Signature-Agent being a structured field.
The message has a valid Signature-Agent header, but does not include it in the component list in Signature-Input.


Attach these three headers to your bot's requests.
An example request may look like this:
Signature-Agent: "https://signature-agent.test"Signature-Input: sig2=("@authority" "signature-agent") ;created=1735689600 ;keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U" ;alg="ed25519" ;expires=1735693200 ;nonce="e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==" ;tag="web-bot-auth"Signature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:


You may wish to refer to the following resources.

Bots FAQs.
Cloudflare blog: Message Signatures are now part of our Verified Bots Program ‚Üó.
Cloudflare blog: Forget IPs: using cryptography to verify bot and agent traffic ‚Üó.
Cloudflare's web-bot-auth library in Rust ‚Üó.
Cloudflare's web-bot-auth npm package in Typescript ‚Üó.
        Resources     API     New to Cloudflare?     Directory     Sponsorships     Open Source     Support     Help Center     System Status     Compliance     GDPR     Company     cloudflare.com     Our team     Careers     Tools     Cloudflare Radar     Speed Test     Is BGP Safe Yet?     RPKI Toolkit     Certificate Transparency     Community     X     Discord     YouTube     GitHub      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison‚Äôs LLM CLI, you‚Äôve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren‚Äôt just chatbots or
      autocomplete tools - they‚Äôre agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that‚Äôs exactly what we did, and in
      this article I‚Äôll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You‚Äôll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they‚Äôre built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we‚Äôre
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (I‚Äôve
      shortened ours for the article). Now something magical happened. I could
      say ‚ÄúX isn‚Äôt working‚Äù and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes ‚Äúfixed‚Äù failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al‚Äôs default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned ‚ÄúBedrock is timing out‚Äù
      or ‚Äúthe model responses are getting truncated,‚Äù the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the iceberg‚Äîthe AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentation‚Äîrecent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask ‚ÄúWhy is
      this API call failing intermittently?‚Äù and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation‚Äîespecially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capable‚Äîit could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say ‚ÄúThe authentication tests are failing, please fix the issue‚Äù
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestions‚Äîit was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit‚Äîlike having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development‚Äîfrom CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourself‚Äîeven a simple version‚Äîgives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[VLT observations of interstellar comet 3I/ATLAS II]]></title>
            <link>https://arxiv.org/abs/2508.18382</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055335</guid>
            <description><![CDATA[We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000√Ö). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3œÉ$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.]]></description>
            <content:encoded><![CDATA[
    
    
    Authors:Rohan Rahatgaonkar, Juan Pablo Carvajal, Thomas H. Puzia, Baltasar Luco, Emmanuel Jehin, Damien Hutsem√©kers, Cyrielle Opitom, Jean Manfroid, Micha√´l Marsset, Bin Yang, Laura Buchanan, Wesley C. Fraser, John Forbes, Michele Bannister, Dennis Bodewits, Bryce T. Bolin, Matthew Belyakov, Matthew M. Knight, Colin Snodgrass, Erica Bufanda, Rosemary Dorsey, L√©a Ferellec, Fiorangela La Forgia, Manuela Lippi, Brian Murphy, Prasanta K. Nayak, Mathieu Vander Donckt            
    View PDF
    HTML (experimental)
            Abstract:We report VLT spectroscopy of the interstellar comet 3I/ATLAS (C/2025 N1) from $r_{\rm h}\!\simeq\!4.4$ to $2.85$ au using X-shooter (300-550 nm, $R\!\simeq\!3000$) and UVES (optical, $R\!\simeq\!35k-80k$). The coma is dust-dominated with a fairly constant red optical continuum slope ($\sim$21-22\%/1000√Ö). At $r_{\rm h}\!\simeq\!3.17$ au we derive $3\sigma$ limits of $Q({\rm OH})<7.76\times10^{23}\ {\rm s^{-1}}$, but find no indications for [O I], C$_2$, C$_3$ or NH$_2$. We report detection of CN emission and also detect numerous Ni I lines while Fe I remains undetected, potentially implying efficiently released gas-phase Ni. From our latest X-shooter measurements conducted on 2025-08-21 ($r_{\rm h} = 2.85$\,au) we measure production rates of $\log~Q(\mathrm{CN}) = 23.61\pm 0.05$ molecules s$^{-1}$ and $\log~Q$(Ni) $= 22.67\pm0.07$ atoms s$^{-1}$, and characterize their evolution as the comet approaches perihelion. We observe a steep heliocentric-distance scaling for the production rates $Q(\mathrm{Ni}) \propto r_h^{-8.43 \pm 0.79}$ and for $Q(\mathrm{CN}) \propto r_h^{-9.38 \pm 1.2}$, and predict a Ni-CO$_{(2)}$ correlation if the Ni I emission is driven by the carbonyl formation channel. Energetic considerations of activation barriers show that this behavior is inconsistent with direct sublimation of canonical metal/sulfide phases and instead favors low-activation-energy release from dust, e.g. photon-stimulated desorption or mild thermolysis of metalated organics or Ni-rich nanophases, possibly including Ni-carbonyl-like complexes. These hypotheses are testable with future coordinated ground-based and space-based monitoring as 3I becomes more active during its continued passage through the solar system.
    

    
    
              
          Comments:
          12 pages, 3 figures, 2 tables, submitted to ApJL
        

          Subjects:
          
            Solar and Stellar Astrophysics (astro-ph.SR); Earth and Planetary Astrophysics (astro-ph.EP)
        
          Cite as:
          arXiv:2508.18382 [astro-ph.SR]
        
        
          ¬†
          (or 
              arXiv:2508.18382v1 [astro-ph.SR] for this version)
          
        
        
          ¬†
                        https://doi.org/10.48550/arXiv.2508.18382
              
                                arXiv-issued DOI via DataCite
            
          
        
    
  
      Submission history From: Thomas H. Puzia [view email]          [v1]
        Mon, 25 Aug 2025 18:15:44 UTC (2,178 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates aren‚Äôt exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesn‚Äôt exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know what‚Äôs wrong with people?
                They‚Äôre too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence ‚Äî however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say ‚Äúit depends‚Äù all the time.
                Much more fun getting to say
                ‚Äúlet‚Äôs ship it and iterate‚Äù as Staff or
                ‚Äúthat won‚Äôt scale‚Äù as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? ü§®
}

              GPS coordinates aren‚Äôt exact.
                They‚Äôre noisy. They‚Äôre approximate. They‚Äôre probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              you‚Äôre probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesn‚Äôt capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach that‚Äôs both
              mathematically rigorous and surprisingly practical.
            
            As you‚Äôd expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2œÉ confidence ü§ì
}

            When you compare two Uncertain values,
              you don‚Äôt get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/m¬≥
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // m¬≤
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed ‚Äî
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesn‚Äôt exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              let‚Äôs take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or ‚Äúfruit machine‚Äù for our UK readers üá¨üáß):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "‚óªÔ∏è", "‚óªÔ∏è", "‚óªÔ∏è",  // blanks
            "üçí", "üçã", "üçä", "üçá", "üíé"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("üíé", "üíé", "üíé"): return 100  // Jackpot!
        case ("üçí", "üçí", "üçí"): return 10
        case ("üçá", "üçá", "üçá"): return 5
        case ("üçä", "üçä", "üçä"): return 3
        case ("üçã", "üçã", "üçã"): return 2
        case ("üçí", _, _), // Any cherry
             (_, "üçí", _),
             (_, _, "üçí"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically ‚Äî
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: ‚âà $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23¬∞C
let tempSpread = temperature.standardDeviation() // about 1¬∞C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)¬∞C and \(upper)¬∞C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp ‚â§ 25¬∞C) < 20%
    print("Unlikely to be 25¬∞C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users don‚Äôt notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is ¬±1000 meters,
            that‚Äôs a bad look ü§°
          So where do we go from here?
            Let‚Äôs channel our Senior+ memes from before for guidance.
          That Staff engineer saying ‚Äúlet‚Äôs ship it and iterate‚Äù
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineer‚Äôs warning of ‚Äúthat won‚Äôt scale‚Äù.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isn‚Äôt to eliminate uncertainty ‚Äî
            it‚Äôs to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ask HN: The government of my country blocked VPN access. What should I use?]]></title>
            <link>https://news.ycombinator.com/item?id=45054260</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054260</guid>
            <description><![CDATA[Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).]]></description>
            <content:encoded><![CDATA[
Hello! I've got experience working on censorship circumvention for a major VPN provider (in the early 2020s).- First things first, you have to get your hands on actual VPN software and configs. Many providers who are aware of VPN censorship and cater to these locales distribute their VPNs through hard-to-block channels and in obfuscated packages. S3 is a popular option but by no means the only one, and some VPN providers partner with local orgs who can figure out the safest and most efficient ways to distribute a VPN package in countries at risk of censorship or undergoing censorship.- Once you've got the software, you should try to use it with an obfuscation layer.Obfs4proxy is a popular tool here, and relies on a pre-shared key to make traffic look like nothing special. IIRC it also hides the VPN handshake. This isn't a perfectly secure model, but it's good enough to defeat most DPI setups.Another option is Shapeshifter, from Operator (https://github.com/OperatorFoundation). Or, in general, anything that uses pluggable transports. While it's a niche technology, it's quite useful in your case.In both cases, the VPN provider must provide support for these protocols.- The toughest step long term is not getting caught using a VPN. By its nature, long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a state actor). I don't know the situation on the ground in Indonesia, so I won't speculate about what the best way to avoid this would be, long-term.I will endorse Mullvad as a trustworthy and technically competent VPN provider in this niche (n.b., I do not work for them, nor have I worked for them; they were a competitor to my employer and we always respected their approach to the space).
> First things first, you have to get your hands on actual VPN software and configs.It would be nice if one of the big shortwave operators could datacast these packages to the world as a public service.
The problem is the countries, which censor Internet and block VPNs, also jam shortwave radio signals.
Could I ask for a source on that and how common it is?Seems like it was used way back in the cold war (and even then not blocked/jammed) and I'd guess that current authoritarian regimes would perhaps not bother considering how few could use it.
Streisand is extremely out of date and wouldn‚Äôt last long in China, but I don‚Äôt know how sophisticated Indonesia‚Äôs firewall is
This is no 'nothing special' with Obfs4proxy. DPI sees it as random byte stream, thus your government can decide to block unknown protocols. Instead, you should trick DPI into thinking it sees HTTPS. Unless your government decides to block HTTPS.
The only VPN technology I see that blends as HTTPS is MASQUE IP Proxying, and the only implementation I know that does this is iCloud Private Relay. It is also trivial to block because blocking 443/udp doesn't really affect accessing the Internet.
> your government can decide to block unknown protocolsHas any government ever done that? Seems like it would just break everything (because the world is full of devices that use custom protocols!) at great computational expense.
Thank you very much for a detailed answer. Might I rudely ask -- as you're knowledgeable in this space, what do you think of Mullvad's DAITA, which specifically aims to defeat traffic analysis by moving to a more pulsed constant bandwidth model?
DAITA was introduced after my time in the industry, but this isn't a new idea (though as far as I know, it's the first time this kind of thing's been commercialized).It's clever. It tries to defeat attacks against one of the tougher parts of VPN connections to reliably obfuscate, and the effort's commendable, but I'll stop short of saying it's a good solution for one big reason: with VPNs and censorship circumvention, the data often speaks for itself.A VPN provider working in this space will often have aggregate (and obviously anonymized, if they're working in good faith) stats about success rates and failure classes encountered from clients connecting to their nodes. Where I worked, we didn't publish this information. I'm not sure where Mullvad stands on this right now.In any case -- some VPN providers deploying new technology like this will partner with the research community (because there's a small, but passionate formal research community in this space!) and publish papers, studies, and other digests of their findings. Keep an eye out for this sort of stuff. UMD's Breakerspace in the US in particular had some extremely clever people working on this stuff when I was involved in the industry.
There are some techniques like fragmented TLS and reordered packets that work in some cases. Also using vanilla HTTPS transport is a good start for many places. URnetwork is an open source, decentralized option that does all of these out of the box. You can get it on the major stores or F-Droid.
Obfs4proxy and Shapeshifter are an absolute PITA to install.Get your own VPS server (VPS in EU/US with 2GB of ram, 40GB of disk space and TBs/month of traffic go for $10 a year, it's that cheap). Never get anything in the UK and even USA is weird. I'd stick with EU.Install your software (wireguard + obsfuscation or even tailscale with your own DERP server)Another simpler alternative is just `ssh -D port` and use it as a SOCKS server. It's usually not blocked but very obvious.
I just spent 3 months in China this summer. The GFW has become much more sophisticated than I remember. I found only one method that reliably worked. That was to use Holafly (an international eSIM provider) and use its built-in VPN. China largely doesn‚Äôt care if foreigners get around the GFW, I guess.Another method that usually worked was ProtonVPN with protocol set to Wireguard. Not sure why this worked, it‚Äôs definitely a lot more detectable than other methods I tried. But as long as I rotated which US server I used every few days, this worked fine.No luck with shadowsocks, ProtonVPN ‚Äústealth‚Äù mode, Outline+Digital Ocean, or even Jump / Remote Desktop. Jump worked the longest at several hours before it became unbearably slow, I‚Äôm still not sure if I was actually throttled or my home computer started misbehaving.I didn‚Äôt get around to setting up a pure TLS proxy, or proxying traffic through a domain that serves ‚Äúlegitimate‚Äù traffic, so no idea if that still works.
The "inspection" part of DPI isn't limited to encrypted payloads. It's straightforward enough to look at application-level protocol headers and identify e.g. a Wireguard or OpenVPN or SSH connection, even if you can't decrypt the payload. That could be used as sufficient grounds to either block the traffic or punish the user.
Because you are leaking information left and right with TCP / DNS and all these basic protocols that powering the internet today. When these were designed people were happy that it worked at all and nobody really tought that it should be state actor proof. Except maybe DJB. https://www.curvecp.org/
DPI refers to a broad class of products which attempt to find signals and categorize traffic according to a ruleset, either to block it or throttle the speeds, etc.While access to plaintext is useful, it's not required for other rules which are eg looking at the timing and frequency of packets.
There are a couple of ways.The main one is called an Eclipse Attack in cyber circles, and it can be done at any entity operating at the ASN layer so long as they can position themselves to relay your traffic.The adversary can invisibly (to victim PoV) modify traffic if they have a cooperating rootPKI cert (anywhere in the ecosystem) that isn't the originating content provider, so long as they recognize the network signature (connection handshake); solely by terminating encryption early.Without a cert, you can still listen in with traffic analysis, the fetched traffic that's already been encrypted with their key (bit for bit), as known plaintext the math quickly reduces. SNI and a few other artifacts referencing the resources/sites are not part of the encrypted payload.Its more commonly known in a crypto context, but that kind of attack can happen anywhere. It even works against TOR. One of the first instances (afaik) was disclosed by Princeton researches in 2015, under the Raptor paper.
Patterns of data transmission (network behavioral analysis, I just made that term up), analyzing IP and ports, inspecting SSL handshakes for destination site. In short, metadata.
I wonder if it can be embedded in a video stream, like a video of a lava lamp that you always have open, but the lsb of ever byte is meaningful.
That's an interesting idea, and probably something you might be able to achieve with a tool like h26forge.It's also probably more useful to just have a connection be fully dedicated to a VPN, and have the traffic volume over time mimic what you'd see in a video, rather than embedding it in a video -- thanks to letsencrypt, much of the web's served over TLS these days (asterisks for countries like KZ and TM which force the use of a state-sponsored CA), so going to great lengths to embed your VPN in a video isn't really practical.
I‚Äôm curious about what makes it difficult to block a vpn provider long term. You said getting the software is difficult, but can a country not block known vpn ingress points?
A country can and absolutely will block known VPN ingress points. There are two tricks that we can use to circumvent this:- Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc). Bonus points if you can leverage something like ECH (ex-ESNI) to make it harder to identify a single bucket or subdomain.- Keep spawning new domains and subdomains to distribute your binaries.There are complications with both approaches. Some countries block ECH outright. Some have no problem shutting the internet down wholesale for a little bit. The domain-hopping approach presents challenges w/r/t establishing trust (though not insurmountable ones, much of the time).These are thing that have to be judged and balanced on a case-by-case basis, and having partners on the ground in these places really helps reduce risk to users trying to connect from these places, but then you have to be very careful talking to then since they could themselves get in trouble for trying to organize a VPN distribution network with you. It's layers on layers, and at some point it helps to just have someone on the team with a background in working with people in vulnerable sectors and someone else from a global affairs and policy background to try and keep things as safe as they can be for people living under these regimes.
I've heard of domain fronting, where you host something on a subdomain of a large provider like Azure or Amazon. Is this what you're talking about when you say> - Host on a piece of infrastructure that's so big that you can't effectively block it without causing a major internet outage (think: S3, Cloudflare R2, etc).How can one bounce VPN traffic through S3? Or are you just talking about hosting client software, ingress IP address lists, etc?
That's generally for distribution, but yeah, it's a form of domain fronting.There are some more niche techniques that are _really_ cool but haven't gained widespread adoption, too, like refractive routing. The logistics of getting that working are particularly challenging since you need a willing partner who'll undermine some of their trustworthiness with some actors to support (what is, normally, to them) your project.
Sorry I‚Äôm referring to WireGuard/ovpn server IPs, not the binaries/configs used to setup a client. Unless you‚Äôre talking about fronting for both, but I imagine it is not economical to run a commercial -scale privacy vpn via a cloud provider.
This makes me wonder: are there "cloud drive virtual sneakernet" systems that will communicate e.g. by a client uploading URL request(s) as documents via OneDrive/SharePoint/Google Drive/Baidu etc., a server reacting to this via webhook and uploading (say) a PDF version of the rendered site, then allowing the client to download that PDF? You effectively use the CDN of that service as a (very slow) proxy.Of course, https://xkcd.com/538/ applies in full force, and I don't have any background in the space to make this a recommendation!
It doesn't apply imo as OP is probably not a high value target of the govt, he just wants to bypass his govt restrictions and I doubt the situation is so bad that the govt will send people physically to deal with people circumventing the block.Your solution could technically work over any kind of open connection / data transfer protocol that isn't blocked by the provider but it would be an absolute pain to browse the web that way and there are probably better solutions out there.
I lived in China for a while and there were several waves of VPN blocks. Also very few VPN services even try to actively support VPN-blocking nations anymore. Any commercial offering will be blocked eventually.What I settled on for decent reliability and speeds was a free-tier EC2 hosted in an international region. I then setup a SOCKS5 server and connected my devices to it. You mentioned Cloudflare so whatever their VM service is might also work.It's very low profile as it's just your traffic and the state can't easily differentiate your host from the millions of others in that cloud region.LPT for surviving the unfree internet: GitHub won't be blocked and you'll find all the resources and downloads you need for this method and others posted by Chinese engineers.Edit: If you're worried about being too identifiable because of your static IP, well it's just a computer, you can use a VPN on there too if you want to!
The VM instance is good for setting up a VPN tunnel, but it's not good in terms of bandwidth if it's hosted in. Because of DPI capacity, China has a very limited amount of "real internet" bandwidth. A more capable setup is to have one VM on each side of the firewall on an hosting service with peering between inside and outside - Aliyun (Alibaba Cloud) is an example. The "inside" VM could be just "socat UDP4-RECVFROM:<port>,fork UDP4-SENDTO:<remote>:<port>" or something done using netfilter.Like others commented in this thread, having an obfuscator is a good idea to ensure the traffic is not dropped by DPI.When the inevitable ban comes and your VPN stops working, rotate the IP of the external VPN and update the firewall/socat config to reflect it. Usually, the internal VM's IP doesn't need to be updated.
When I worked in China (not for long periods but frequently enough that the Great Firewall became an irritant) I hosted an OpenVPN server on port 443 and/or port 22 of a server I owned. That worked sufficiently well most of the time.
This doesn't work anymore; the GFW no longer detects VPN connections by port but instead by performing deep packet inspection to characterize the type of traffic going over every connection. Using this technique in combination with some advanced ML systems, they're able to detect any encrypted VPN connection and cut it off; it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore, and it's usually not even possible to _tunnel_ a VPN connection through some other protocol because the GFW now detects that too.Stepping back and looking at it from a purely technical perspective, it's actually insanely impressive.Here's a USENIX paper from a few years ago on how it is done: https://gfw.report/publications/usenixsecurity23/en/
This is what IPsec TFS is for [https://datatracker.ietf.org/doc/rfc9347/]> the focus in this document is to enhance IP Traffic Flow Security (IP-TFS) by adding Traffic Flow Confidentiality (TFC) to encrypted IP-encapsulated traffic.  TFC is provided by obscuring the size and frequency of IP traffic using a fixed-size, constant-send-rate IPsec tunnel(If they block a constant rate stream, that'll hit a whole ton of audio/video streaming setups)
Assuming they don't MITM SSH, you should still be able to use something like wireguard over an SSH tunnel.  At least I would think.. it's all SSH traffic as far as any DPI listener is concerned, you'd of course need to ensure the connection signature through another vector though.
> it's basically not possible to run any kind of outbound VPN connection (even to private servers) from inside of China anymore.Really? Because the paper you linked says they don't block any TLS connections so you can just run a VPN over TLS:> TLS connections start with a TLS Client Hello message, and the first three bytes of this message cause the GFW to exempt the connection from blocking.
> it's basically not possible to run any kind of output VPN connection (even to private servers) from inside of China anymore.What if you run your own HTTPS server that look semi-legitimate and just encapsulate it in that traffic?Can they still detect it?What about a VPS in HK? Is this even doable?
Which is ridiculous because OpenVPN is trivial to identify, even when over TCP since it's different from "regular" HTTPS/SSL traffic.Why they chose this I have no idea.You can even port share.443 -> Web server for HTTPS traffic
443 -> OpenVPN for OpenVPN trafficStill trivial to identify and not uncommon for even public WiFi to do so.Since I changed to tailscale+headscale with my own derp server all these issues have disappeared (for now).
GitHub was briefly blocked a couple of years ago in Indonesia. SSH was also blocked briefly by one of the largest mobile providers.
You've come to a wrong place to ask. Most people here (judging by recommendations of own VPN instances, Tor, Tailscale/other Wireguard-based VPNs, and Mullvad) don't have any experience with censorship circumvention.Just look for any VPNs that are advertised specifically for China, Russia, or Iran. These are the cutting edge tech, they may not be so privacy-friendly as Mullvad, but they will certainly work.
> Just look for any VPNs that are advertised specifically for China, Russia, or Iran.If I was working for a secret service for these countries, I would set up many "VPNs that are advertised specifically for x" as honeypots to gather data about any dissidents.
It doesn't matter, he should look into the open source protocols that these services use. He doesn't have to use them.VLESS / v2ray works in Russia, as far as I know.
Mr. Kafka, suspicion is healthy. However, abstraction provides no way forward when faced with practicalities instead of theory. Creates a Kafka-esque situation - anything suitable is by definition unsuitable. Better to focus on practical technical advice.
Sir Night: may I ask, what should it mean to me that some businesses are fronts?I hope I do not present the presence of a dullard unfamiliar with this.
I don't see parent abstracting. They are simply pointing out a very real risk, which you don't provide any counter points to. Instead you seem to dismiss their point based on a strawman
IMO, the safest route for an individual with tech competency is to setup a small instance server in the cloud outside your country and use ssh port forwarding and a proxy to get at information you want.For an example of a proxy service https://www.digitalocean.com/community/tutorials/how-to-set-...That will give you a hard to snoop proxy service that should completely circumvent a government blockaid (they likely aren't going to be watching or blocking ssh traffic).
VPNs that are advertised are for-profit products, which means:1. They are in most cases run by national spy agencies.2. They will at least appear to work, i.e., they will provide you with access to websites that are blocked by the country you are in.  Depending on which country's spies run the system, they may actually work in the sense of hiding your traffic from that country's spies, or they may mark you as a specific target and save all your traffic for later analysis.My inclination is to prefer free (open-source) software that isn't controlled by a company which can use that control against its users.
Well, you have to host your free open-source VPN software somewhere. And then, (N. B.: technical and usability stuff aside, I'm talking only about privacy bits here) everything boils down to two equally nightmarish options.First, you use well-known cloud or dedicated hoster. All your traffic is now tied to the single IP address of that hoster. It may be linked to you by visiting two different sites from the same IP address. Furthermore, this hoster is legally required to do anything with your VPN machine on demand of corresponding state actors (this is not a speculative scenario; i. e. Linode literally silently MitMed one of their customers on German request). Going ever further, residential and company IPs have quite different rules when it comes to law enforcement. Seeding Linux ISOs from your residential IP will be overlooked almost everywhere (sorry, Germany again), but seeding Linux ISOs from AWS can easily be a criminal offense.Second, you use some shady abuse-proof hosting company, which keeps no logs (or at least says that) and accepts payments in XMR. Now you're logging in to your bank account from an IP address that is used to seedbox pirate content or something even more illegal, and you still don't know if anyone meddles with your VPN instance looking for crypto wallet keys in your traffic.VPN services have a lot of "good" customers for a small amount of IP addresses, so even if they have some "bad" actors, their IPs as a whole remain "good enough". And, as the number of customers is big, each IP cannot be reliably tied to a specific customer without access logs.
Tor is a third option, at least as one layer, and seeding Linux ISOs is not, to my knowledge, a criminal offense in any jurisdiction, not even in China.  I don't know where you got that idea.
From gemini.. (edited for brevity)Kape Technologies Owns: ExpressVPN, CyberGhost, Private Internet Access, Zenmate> is there any suspicion that Kape Technologies is influenced or has ties to the Mossad?Yes, there is significant suspicion and public discussion about Kape Technologies having ties to former Israeli intelligence personnel. While a direct operational link to Mossad has not been proven, the concerns stem from the company's history, its key figures, and their backgrounds....Kape Technologies is owned by Israeli billionaire Teddy Sagi. While Sagi himself does not have a documented intelligence background, his business history, which includes a conviction for insider trading in the 1990s, has been a point of concern for some privacy advocates. The consolidation of several major VPN providers under his ownership has raised questions about the potential for centralized data access.----Sure there isn't direct proof but there wasn't any proof the CIA was driving drug trade while it was happening. Proof materializes when the dust settles on such matters.
It is absolutely self-evident that VPNs are considered high-value targets and that all spy agencies invest a chunk of resources to go after high-value targets.
I would invite you to read again the two claims made, and consider whether your statement actually addresses the veracity of either.To be a little trite: we all agree that chickens like grain, but it does not follow that a majority of grain producers are secretly controlled by a cabal of poultry.
Hmm. People who recommend widely used approaches, and well-known, well-established providers, "don't have any experience with cenorship circumvention".So the solution is no-name providers using random ad-hoc hackery, chosen according to a criterion more or less custom designed to lead you into watering hole attacks.Right.
It's very sad that every sane and informed comment (like reisse's) has to meet this kind of snarky comment whose only purpose is being snarky on HN.Perhaps you should stop and think about why people living in countries where governments actually censor a lot hardly use these "well-established providers" to circumvent censorship. Tip: it's not because they're stupid.
@reisse is 100% right. Most people outside of heavily censored regions have no clue what technology is actually used in those countries. The well-known, well-established providers don't actually work in censored regions because:1) The problem is very difficult and requires a lot of engineering resources
2) It's very hard to make money in these countries for many reasons, including sanctions or the government restricting payments (Alipay, WeChatPay, etc)The immediate response would be: "If the problem is so difficult, how can it be solved if not be well-known, well-established providers?"The answer is simple: the crowdsourcing power of open source combined with billions of people with a huge incentive to get around government blocking.
> It's very hard to make money in these countries for many reasonsTor and I2P, for example, don't actually make money anywhere. Which is not to say that they work for any of the users in all of these places, or for all of the users in any of these places.> The answer is simple: the crowdsourcing power of open source combined with billions of people with a huge incentive to get around government blocking.The actual answer is that (a) they're using so many different weird approaches that the censors and/or secret police can't easily keep up with the whack-a-mole, and (b) they're relying on folklore and survivorship bias to tell them what "works", without really knowing when or how it might fail, or even whether it's already failing.Oh, and most of them are playing for the limited stakes of being blocked, rather than for the larger stakes of being arrested. Or at least they think they are.Maybe that's "solving" it, maybe not.
None of the things I listed are "widely used approaches, and well-known, well-established providers" in the parts of the world where it does matter.Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.
> Yeah, maybe V* and derivatives are random ad-hoc hackery, but they also are the well-known standard now.A lot of people use Telegram and think it's private, too.What about the part about choosing your VPN provider in the way most likely to get you an untrustworthy one who's after you personally?
Furthermore, you can always run another VPN on top of that if you don‚Äôt trust the outer one with the actual plaintext traffic.
Not on mobile - iOS doesn't support nested VPNs, and neither does stock Android.
You can always do v2ray -> Mullvad in a docker container routed with gluetun for censorship avoidance and privacy
Mullvad worked okay in China in June for me. I imagine it will be better in Indonesia with their less sophisticated blocking.
This makes no sense.On the one hand they do DPI with ML.On the other hand a major player is open!Something is not right here...
OP: look into VLESS (and similar).  And read up on ntc.party (through Google translate).  There are certain VPN providers that offer the protocol.
nah, vless is the protocol, reality is a newer obfuscation method that works over vlessedit: op, protonvpn has a free tier that works in russia, so likely works everywhere, or if you're comfortable with buying a vps, sshing into it and running some commands, look up x-ray, and use on of their gui panels
Wrong threat model. Solutions like mullvad/proton focus on privacy not breaking the blockade. They have well known entry points and therefore easily blocked. You can play cat and mouse game switching servers faster than censorship agency blocks them (e.g. Telegram vs Roskomnadzor circa 2018 [1]) but that gets expensive and not really focus of these companies.What you need is open protocols and hundreds of thousands of small servers only known to their owners and their family/friends1: https://archive.is/sxiha
I have a little, maybe enough to be dangerous. SSH won‚Äôt be sufficient to avoid all traffic analysis. Everyone can see how much traffic and the pattern of that traffic, which can leak info about the sort of things you‚Äôre doing.If you‚Äôre worried about ending up on a list, using things that look like VPNs while the VPNs are locked down is likely to do so.Also‚Ä¶ your neighbors in Myanmar didn‚Äôt do a lockdown during the genocide and things got pretty fucking dire as a result. People have taken different lessons from this. I‚Äôm not sure what the right answer is, and which is the greater evil. Deplatforming and arresting people for inciting riots and hate speech is probably the best you can do to maintain life and liberty for the most people.
>Also‚Ä¶ your neighbors in Myanmar didn‚Äôt do a lockdown during the genocide and things got pretty fucking dire as a resultThe genocide in Myanmar was incited _by_ the government there; giving it more power to censor it's citizens' communications would have done absolutely nothing to help the people being genocided. Genocides don't just suddenly happen; the vast majority of genocide over the past century (including Indonesian genocides against ethnically Chinese Indonesians) had the support of the state.
This has been simmering for a very long time. The first I heard of it was violence that broke out after the defacement of a Buddhist temple statue. That would have been almost 20 years ago. Buddhists murdering people tends to lead one to ask a lot of questions.At that time I think the government was hands off, let it happen rather than tried to stop it.Regardless of who was behind the violence, the whole region has thought about what to do in such situations and they aren‚Äôt the same answers the West would choose.
Mullvad worked OK in China for me recently. Sometimes I'd have to try a few different endpoints before it worked. Something built specifically to work in those places would probably be better, but it wasn't too much trouble. Not necessarily a recommendation, just sharing one data point.
I remember always needing obfuscation enabled in Mullvad, but it would work in the end (as you said, after trying a few endpoints).
^ this comment is right on. The cutting edge of VPN circumvention is the one marketed to people in China. Last I poked at this there were a lot of options.
I live in Indonesia, and I don't find any recent news that mention X (formerly Twittwr) and or Discord being blocked by the government. The only relevant news from a quick Google search I can find is about the government threatened to block X due to pornography content in 2024.
You can even check for yourself if a domain is blocked by visiting https://trustpositif.komdigi.go.id/.Also for your unability to access the VPN, as far as my experience goes, in the past some providers do block access to VPN. But, I am not experiencing that for at least the last 5 years.So, maybe you can try changing your internet provider and see if you can connect to VPN?
Australia and UK might soon go down this path.Something quite depressing is if we (HN crowd) find workarounds, most regular folks won't have the budget/expertise to do so, so citizen journalism will have been successfully muted by government / big media.
I would have laughed in your face if you wrote this comment merely 6 months ago. Now I'm just depressed. (UK)
Don't worry. You'll call us conspiracy theories once you get used to the new goalposts and we warn you about the next thing.How about instead of being depressed you start being vocal and defiant?
You know what, I think I've become lethargic after all the backwards garbage going on in my country attacking my way of life on all fronts - from rampant crime to government censorship. 
Your comment just gave me kick up the ass. I'm gonna try and get some local stuff going in opposition to this lunacy.
When ES leaked his info to the Guardian people, they could still (2013) use the Guardian's US base to publish, protected by the US' stronger freedom of speech laws. Now, in 2025, if the same were to happen again, I'm not sure that would work quite the same way, with Trump aggressively taking American citizens' rights away.Maybe The Guardian should open a branch in Sealand...
It was David Graeber that said we should be wary of places like The Guardian. They are a wolf in sheeps clothing. Used a lot of the more liberal momentum of the early 2010s combined with promoting some of the more left leaning writters to gain a fair bit of clout. But underneath, they will conform to the power structures if it comes down to survival. Alas, they nay not be a Sealand edition although that would be neat.
In oz personally and yes, I warned folks of this a few years back, especially in the 12 months or so. Every time I was met with a fair bit of push back.They would argue back on technical merits, I was talking political, a politics doesn't give a damn about the tech. We have slowly been going down this path for a while now.‚ÄúThe laws of mathematics are very commendable, but the only law that applies in Australia is the law of Australia,‚Äù - PM Malcolm Turnbull in 2017.
Don't worry, you shouldn't underestimate the capability of society.I grew up in a pretty deprived area of the UK, and we all knew "a guy" who could get you access to free cable, or shim your electric line to bypass the meter, or get you pirated CD's and VHS' and whatever.There will always be "that guy down the pub" selling raspberry pi's with some deranged outdated firmware that runs a proxy for everything in the house or whatever. To be honest with you, I might end up being that guy for a bunch of people once I'm laid off from tech like the rest. :)
Normally I would agree with you, but the ability to pull this kind of thing off hinges on there being enough shadows that the Eye doesn't look at for prolonged periods of time. And the overall trajectory of technological advance lately is such that those shadows are rapidly shrinking. First it was the street cameras (and UK is already one of the most enthusiastic adopters in the world). And now comes AI which can automatically sift through all the mined data, performing sentiment analysis etc. I feel that the time will come pretty soon when "a guy" will need to be so adept at concealing the tracks in order to avoid detection that most people wouldn't have access to one.
I wouldn‚Äôt worry about it.They can barely handle wolf-whistlers let alone pedophile rape gangs consisting of the lowest IQ dregs of our society.I know it‚Äôs only painfully stupid people who think the law is stupid, but dodgy Dave down the way tends to fly under the radar. Otherwise there wouldn‚Äôt be so many of them.
One of the problems with authoritarianism is that even though most dodgy Daves will be fine because the political apparatus doesn't have the time or energy to arrest everyone for everything, they retain the ability to arrest anyone for anything.The moment your dodgy Dave offends your local cadre, even for reasons entirely other than being dodgy, they'll throw the book at him. And because there is now unpredictability around who will be arrested and for what reason, it acts as a chilling effect for everyone who values some degree of stability in their lives. So the arc of dodgy Daves bends toward compliance.
The eye doesn't care as long as you're not politically efficient in opposing their narratives or power.Authoritarianism in the UK doesn't correlate with crime.  The economy does.The point of these things is not really to help citizens. "there's no money for that" like there's no money for healthcare or education (although there is for bombings in foreign countries). The point is protecting power from any threat that could mount against it.
I think both sides of this are fair. Power is interested in stability of itself, to keeps its back to the wall so that nobody can sneak up on it. But also political power has teamed up with corporate power/determination to create a far more nasty beast.Seeing companies like Palantir (and many lesser known ones) buddy up to everyone that wants it, its a clear statement on how they want to monitor and control the populace.Long term I don't think it can be done, but the pain mid term can be vast.
I suppose that for this case, an underground black market of VPN providers might emerge - average individuals setting up VPN software on a cloud service provider, and then selling monthly access to people. Aside from the obvious danger of getting ripped off (someone might put you on a slow shared VPN with many other people, or shut down the server at any time), there is also the possibility of someone monitoring all your Internet activity.
I'd default assume black market VPNs will monitor internet activity since it's both easy and profitable
That absolutely sounds like a world I should be worried about, where our only choices are dodgy ones
Don't worry, you shouldn't underestimate the capability of society.You should be worried. Don't underestimate the capabilities of the government bureaucrats. That "guys down the pub" will quickly disappear once they start getting jail time for their activities.
I think you really overestimate the capability of the UK to enforce laws. Yes, they can write them and yes they can fine large corporations, that's basically it.They cannot enforce laws against such "petty" crimes, the reason society mostly functions in the UK is because most people don't try to break the law.Pretty sure the local punters would kick the cops out if they came for one of their own, especially if he got them their porn back.
What do you mean? They already arrest thousands of people a year for posting (or even retweeting) things online in the UK.What makes you think, if the Gov was to implement some sophisticated DPI firewall that blocks a million different things, they won't come after the people who circumvent it? They already enforce petty crimes. I could report you for causing me anxiety and you would have a copper show up at your door.
It's not just about UK abilities to enforce laws, but also about other factors. The described activities are extremely unattractive as criminal: small market, small margin, the need for planning, preparation and qualification.There is no need for special efforts to enforce the law. Put a few people in jail - and everyone else will quickly find safer and more legal ways to spend their time. No one will do something like that unless they are confident of their impunity.
Yes, it's also dystopian to pin one's future on such hopes. People need to stick it to the government and demand their freedoms. Far too many things are being forced on us in the West that go against fundamental values that have been established for centuries.Somehow, things that could be unifying protests where the working class of every political stripe are able to overlook their differences and push back against government never seem to happen. It is always polarized so that it's only ever one side at a time, and the other side is against them. How does that work?
Reflex. People's opinion on a subject changes if you tell them which political group supports it, sometimes even if they get asked twice in a row. Tribal identity determines ideology more than the other way around for a lot of people.So as soon as Labour comes out for something, Cons are inclined to be against it and so on. The only way to have neutral protests is if no one visibly backs them and they don't become associated with a side, but then how do they get support and organization?
> People need to stick it to the government and demand their freedoms.It will only work if they admit that they supported this and all forms of totalitarianism during COVID.  You can't fall for that and then be surprised when the world keeps going down that obvious path.
In matters of public health, you cannot trust the public to do the right thing.The problem with covid is that we weren't totalitarian enough. Regulations you could drive a coach & horses through and no way to enforce is a sop.The first lock down needed to be a proper 'papers, please' affair. When we get a properly lethal pandemic, we're fucked. Hopefully Laurence Fox and Piers Corbyn will catch it quickly and expire in a painful and televised way, it's the only hope of people complying with actual quarantine measures.
90% of ‚Äúcitizen journalism‚Äù is nothing of the sort. Just like ‚Äúcitizen science‚Äù researching vaccines.
> 90% of ‚Äúcitizen journalism‚Äù is (trash)You're right. But compared to what?I guess 99% of mainstream "journalism" is irrelevant and/or inaccurate, hence citizen journalism is a 10x improvement in accuracy and relevancy! Not 10% better, 900% better! This makes a huge difference to our society as a whole and in our daily lives!But this misses the most important point which is that the user should have the right to choose for themselves what they say and read. Making citizen journalism unduly burdensome deprives everyone of that choice.
Preach comrade!Those citizen journalists with their primary sources, disgusting.Thats nothing but propaganda.Remember it doesnt matter what the video shows, it only matters who showed it to you.
> Remember it doesnt matter what the video shows, it only matters who showed it to you.Both matter.
>Remember it doesnt matter what the video shows, it only matters who showed it to youIn an age of mass media (where there's a video for anything) or now one step further synthetic media knowing who makes something is much more important than the content, given that what's being shown can be created on demand. Propaganda in the modern world is taking something that actually happened, and then framing it as an authentic piece of information found "on the street", twisting its context."what's in the video" is now largely pointless, and anyone who isn't gullible will obviously always focus on where the promoter of any material wants to direct the audiences attention to, or what they want to deflect from.
Citizen journalism avoids the main weakness of a centralised system: it's incredible suspectible to capture. A prime example of this is the mass opposition around the world to Israel's genocide in Gaza.  Israel committed such genocides prior to the event of social media, such as the Nakba, but it was rarely reported on, due to media ownership being concentrated in the hands of a few pro-Zionist individuals.
I am just waiting for red states in the US to try this too since their current laws requiring ID verification for porn sites aren‚Äôt effective.
> red statesWell you'd be surprised to find out that this stupid policy (and many more) have been brought forward by Labour (Left).
At this point, anyone who has been watching politics for a few decades understands that the left/right dichotomy is primarily one designed to keep the majority of people within a certain set of bounds. We see it revealed when politicians and ideologies that should be in opposition to one another still cooperate on the same strategies, like this one.The goal right now is to make online anonymity impossible. Adult content is the wedge issue being used to make defending it unpalatable for any elected official, but nobody actually has it as a goal to prevent teenagers from looking at porn - if they did, they would be using more direct and efficient strategies.  No, it's very clear that anonymous online commentary is hurting politicians and they are striking back against it.
It has been my impression that in UK, both parties are strongly authoritarian, with the sole difference being what kinds of speech and expression, precisely, they want to police.
Both the major Australian parties (Liberal and Labor) seem as spineless as each other.They're being pushed by media conglomerates News Corp and Nine Entertainment [0] to crush competition (social media apps). With the soon-to-be-introduced 'internet licence' (euphemism: 'age verification'), and it's working. If they ban VPN's, it will make social media apps even more burdensome to access and use.[0] News Corp and Nine Entertainment together own 90% of Australian print media, and are hugely influential in radio, digital and paid and free-to-air TV. They have a lot to gain by removing access to social media apps, where many (especially young) people get their information now days.
How long until they produce an  generative AI version of Burt Newton to do new episodes of 20 to 1 based on some social media slop?Yep, not a great time line here.
Yep, here in Australia the social media age restriction was pushed through by both sides. Two sides of the same coin.
- Tor. Pros: Reasonably user friendly and easy to get online, strong anonymity, free. Cons: a common target for censorship, not very fast, exit nodes are basically universally distrusted by websites.- Tailscale with Mullvad exit nodes. Pros: little setup but not more than installing and configuring a program, faster than Got, very versatile. Cons: deep packet inspection can probably identify your traffic is using Mullvad, costs some money.- Your own VPSs with Wireguard/Tailscale. Pros: max control, you control how fast you want it, you can share with people you care about (and are willing to support). Cons: the admin effort isn't huge but requires some skill, cost is flexible but probably 20-30$ per month minimum in hosting.
> - Tailscale with Mullvad exit nodesTailscale is completely unnecessary here, unless OP can't connect to Mullvad.net in the first place to sign up. But if the Indonesian government blocks Mullvad nodes, they'll be out of luck either way.> - Your own VPSs with Wireguard/TailscaleKeep in mind that from the POV of any websites you visit, you will be easily identifiable due to your static IP.My suggestion would be to rent a VPS outside Indonesia, set up Mullvad or Tor on the VPS and route all traffic through that VPS (and thereby through Mullvad/Tor). The fastest way to set up the latter across devices is probably to use the VPS as Tailscale exit node.
Tailscale + Mullvad does have a privacy advantage over either one by itself: the party that could potentially spy on the VPN traffic (Mullvad) doesn‚Äôt know whose traffic it is beyond that it‚Äôs a Tailscale customer. Any government who wanted to trace specific traffic back to OP would need to get the cooperation of both Mullvad and Tailscale, which is a lot less likely than even the quite unlikely event of getting Mullvad to cooperate.
I mean multiple VPSs for redundancy. Contabo is maybe the cheapest I've seen and it's like 3$ mtl for the smallest?
And using another VPN like NordVPN or ProtonVPN is probably in the same category as Mullvad, but worth being cautious. If it's free, you are the product. If you pay, you're still sending your traffic to a publicly (usually) known server of a VPN. That metadata alone in some jurisdictions can still put you in danger.Stay safe
This is good overview, I just wanted to add that a VPS IP is not a residential IP. You will encounter roadblocks when you try to access services if you appear to be coming from a VPS. Not that I had a better solution, just to clarify what you can expect.
Tor also has anti-censorship mechanisms (snowflakes, ...). Depending on how aggressive the blocking is, Tor might be the most effective solution.
Wireguard is not censorship-resistant, and most VPN-averse countries block cross-border Wireguard. Why reply a practical question in an area in which you have no experience?
Yes. Fixed packet headers, predictable packet sizes. I don't know what "a common port" means in relation to wg.
Yeah. Tailscale uses 41641, and you can generally use whatever. I don't think there's any consensus, or majority.
Because Indonesia is new to the game and might still be catching up. They‚Äôre probably playing whackamole with the most common public VPN providers and might not be doing deep packet inspection yet. I worked with someone getting traffic out of Hong Kong a year ago and there was a lot trial and error figuring out what was blocked and what was not. Wireguard was one that worked.
They recommend Tailscale in particular. Tailscale control plane and DERPs (which are functionally required on mobile) will be among the first to go.Outline (shadowsocks-based) and amnezia (obfuscated wg and xray) both offer few-click install on your own VPS, which is easier than setting up headscale or static wg infrastructure, and will last you longer.Also, you did not answer my "why" question. I'm not sure what question you were answering.
IMO most people should have a VPS even if you don't need it for tunneling. Living without having a place to just leave services/files is very hard and often "free" services will hold your data hostage to manipulate your behavior which is annoying on a good day.
Yeah they can be cheap, but I would definitely recommend having at least 3 for redundancy. If one get shut down or it's IP blacklisted you still hopefully have a backup line to create a replacement.
No, unless you pay month to month. If you wait till BF you can find some really good deals on sites like lowendspirit
> cost is flexible but probably 20-30$ per month minimum in hosting.$4/month VPS from DigitalOcean is more than enough to handle a few users as per my experience. I have a Wireguard setup like this for more than a year. Didn't notice any issues.
I'm currently traveling in Uzbekistan and am surprised that wireguard as a protocol is just blocked. I use wireguard with my own server, because usually governments just block well known VPN providers and a small individual server is fine.It's the first time I've encountered where the entire protocol is just blocked. Worth checking what is blocked and how before deciding which VPN provider to use.
I should have mentioned that our use case isn't avoiding government firewalls, it's transiting through broken network environments.
WireGuard by itself has a pretty noticeable network pattern and I don't think they make obfuscating it a goal.There are some solutions that mimic the traffic and, say, route it through 443/TCP.
Wow, kinda crazy to think about a government blocking a protocol that just simply lets two computers talk securely over a tunnel.
Well, think about it - almost every other interaction you can have with an individual in another country is mediated by government. Physical interaction? You need to get through a border and customs. Phone call? Going through their exchanges, could be blocked, easy to spy on with wiretaps. Letter mail? Many cases historically of all letters being opened before being forwarded along.We lived through the golden age of the Internet where anyone was allowed to open a raw socket connection to anyone else, anywhere. That age is fading, now, and time may come where even sending an email to someone in Russia or China will be fraught with difficulty. Certainly encryption will be blocked.We're going to need steganographic tech that uses AI-hallucinated content as a carrier, or something.
> surprised that wireguard as a protocol is just blocked.Honestly this is the route I'm sure the UK will decide upon in the not too distant future.The job of us hackers is going to become even more important...
Cloak + wireguard should work fine on the server side. The problem is that I didn't find any clients for Android and I doubt there are clients for iOs that can (a) open a cloak tunnel and then (b) allow wireguard to connect to localhost...
A year ago I was traveling through Uzbekistan while also partly working remotely. IKEv2 VPN was blocked but thankfully I was able to switch to SSL VPN which worked fine. I didn't expect that, everything else (people, culture) in the country seemed quite open.
XRay protocol based VPN worked for me in Uzbekistan when I were travelling there.Wireguard is indeed blocked.
how can they detect it is wireguard, I thought the traffic is encrypted?how does it differ from regular TLS 1.3 traffic?
It's UDP, not TCP (like TLS) and has a distinguishable handshake. Wireguard is not designed as a censorship prevention tool, it's purely a networking solution.The tunnel itself is encrypted, but the tunnel creation and existence is not obfuscated.
There are many instances of Mastodon, and due to its federated nature, you can use any of them to access it, and even host your own.
Sure, but if you have an account on a different server, you can still see things posted on mastodon.social if you have followed someone there.
It would be easy to block on protocol level. Countries that block VPNs usually progress to that level pretty fast once they discover that simple IP blocks don't work.
I doubt that is the case once you do statistical analysis of it.Advanced VPN tunneling protocols, for example, have to take a lot of special measures to conceal their nature from China's and Russia's deep packet inspecting firewalls.
Also sing-box [1]. I don't use it for its primary use case of censorship circumvention, but rather for some highly complex routing configurations it supports.My use case consists of passing some apps on my Android through interface A (e.g. banking apps through my 5G modem), some apps through US residential proxy (for US banks that don't like me visiting from abroad), and all the rest through VPN. And no root required!It's wild that GFW triggered creation of this and nothing like it existed / exists.[1]: https://github.com/SagerNet/sing-box
im curious, isn't ALL of your traffic appearing to be to just one website the most obvious giveaway?
*ray clients typically allow configuration of routing. So you can send only blocked stuff through the tunnel; or, in reverse, send some known-working stuff (e. g. local domain) direct. Also works as adblock.
An expensive but functional option is to enable roaming on a foreign eSIM. Getting an eSIM is relatively easy. Roaming mobile traffic is routed from the country in which the SIM is from, not the country that you're in, meaning that an eSIM from e.g. an American carrier will not be subject to the censorship in your country.I've used this on multiple trips to China over the past decade (including a trip last year). You can find carriers that will charge very low (or even no) roaming rates.
The most effective solution is to use X-ray/V2ray with VLESS, or VMESS, or Trojan as a protocol.Another obfuscated solution is AmneziaIf you are not ready to set up your own VPN server and need any kind of connection right now, try Psiphon, but it's a proprietary centralized service and it's not the best solution.
Very possible, though many of our users are saying that in network environments where WireGuard is blocked they were able to use Obscura.
Hey, I went to take a look at Obscura and I like the ideas but I can't find the source code.You are making some bold claims but without the source I can't verify those claims.Any plans to open-source it?
Tunneling via SSH (ssh -D) is super easy to detect. The government doesn't need any sophisticated analysis to tell SSH connections for tunneling from SSH connections where a human is typing into a terminal.Countries like China have blocked SSH-based tunneling for years.It can also block sessions based on packet sizes: a typical web browsing session involves a short HTTP request and a long HTTP response, during which the receiving end sends TCP ACKs; but if the traffic traffic mimics the above except these "ACKs" are a few dozen bytes larger than a real ACK, it knows you are tunneling over a different protocol. This is how it detects the vast majority of VPNs.
One alternative would be to set up a VPS, run VNC on it, run your browser on that to access the various web sites, and connect over an SSH tunnel to the VNC instance. Then it actually is an interactive ssh session.
15 years ago, I was using EC2 at work, and realized it was surprisingly easy to SSH into it in a way where all my traffic went through EC2. I could watch local Netflix when traveling. It was a de facto VPN.Details are not at the top of my mind these years later, but you can probably rig something up yourself that looks like regular web dev shit and not a known commercial VPN. I think there was a preference in Firefox or something.
The issue these days is that all of the EC2 IP ranges are well known, and are usually not very high-reputation IPs, so a lot of services will block them, or at least aggressively require CAPTCHAs to prevent botting.Source: used to work for a shady SEO company that searched Google 6,000,000 times a day on a huge farm of IPs from every provider we could find
I watched a season of Doctor Who that way back when the BBC were being precious about it. But Digital Ocean, so $5.
Nations severing peoples connections to the world is awful. I'm so sorry for the chaos in general, and the state doing awful things both.Go on https://lowendbox.com and get a cheap cheap cheap VPS. Use ssh SOCKS proxy in your browser to send web traffic through it.Very unfancy, a 30+ year old solution, but uses such primitive internet basics that it will almost certainly never fail. Builtin to everything but Windows (which afaik doesn't have an ssh client built-in).Tailscale is also super fantastic.
>  uses such primitive internet basics that it will almost certainly never fail.It already fails in China and Russia. Simply tunneling HTTP through SSH is too easy to detect with DPI.> Windows (which afaik doesn't have an ssh client built-in)It has had both SSH client and SSH server built-in since Win10.
I‚Äôm not sure this is the right conversation right now, but is this thread heading towards ‚Äúhow do we make totalitarian governments become liberal democracies?‚ÄùIt‚Äôs a nice technical question on how to run a VPN but the ultimate goal is not the best technical solution but the ability to avoid detection by the state. And that‚Äôs not a technical problem but an opsec oneIf someone is participating in online discussions (discord and twitter) to spread local news - then it‚Äôs hard to know who is who, and who to trust - and that‚Äôs kind of the why Arab spring did not spring ‚Äúhey wear a red carnation and meet me by the corner‚Äù can become a death sentenceThe answer to opsec is avoid all digital comms - but at this point you are seriously into ‚Äúregieme change‚Äù, or just as Eastern Europe did, keep your heads down for forty years and hope those who leave you economically behind will half bankrupt them selves bringing you back.I think in the end, a thriving middle class with a sufficient amount of land reform, wealth taxes which can over a generation push for liberalisation sounds a good idea.Our job in the very lucky liberal
West is to keep what our forefathers won, and then push it further to show why our values are worth the sacrifice in copying
WireGuard should still work. Tons of different providers. I trust Mullvad but ProtonVPN has a free tier. If they start blocking WireGuard, check out v2ray and xray-core. If those get blocked... that means somehow they're restricting all HTTPS traffic going out of the country
In this scenario, Chinese have very rich experience.
you need to use the advance proxy tool like clash ,v2ray, shadowsocks etc.
shadowsocks was the winner of the state of the art I had to do at work. It address the "long-term statistical analysis will often reveal a VPN connection regardless of obfuscation and masking (and this approach can be cheaper to support than DPI by a stat)" comment.
In case known VPN providers are blocked you can pick a small VPS from a hoster like Hetzner and setup your own VPN.
What I'm worried most are that most people are not even aware of what is DNS and how to change it.I can't imagine those who are caught in the chaos with only their phone and unable to access information that could help them to be safe.
Generally speaking, the general population that wants to use blocked services will develop enough technical know-how to circumvent it. The biggest risk is that there are bad actors giving malicious advice and to such learners, looking to defraud or otherwise exploit them.
Chinese have developed a significant amount of sophisticated tools countering internet censorship. V2ray as far as I recall is the state-of-the-art.To use them, one need to first rent a (virtual) server somewhere from a foreign cloud provider as long as the payment does not pose a problem. The first step sometimes proves difficult for people in China, but hopefully Indonesia is not at that stage yet. What follows is relatively easy as there are many tutorials for the deployment like: https://guide.v2fly.org/en_US/
What is going on if you don‚Äôt mind my asking? Our local news does not mention anything. Nor does ddging help? Any sources?
> the housing allowance for a month for a parliamentarian is now ten times the minimum wage for a month.I'm almost positive that everyone in the US Congress is making at least ten times the minimum wage in this country. The "housing allowance" being referred to is separate from their normal salary in Indonesia, but still, interesting to imagine how much more seriously people there would take that disparity than in many other countries.This caught my attention more:> Indonesia passed a law in March allowing for the military to assume more civilian posts, while this month the government announced 100 new military battalions that will be trained in agriculture and animal husbandry. In July the government said the military would also start manufacturing pharmaceuticals.They're replacing civilian industry with military, apparently not out of any emergency requirement but just to benefit the military with jobs (and the government with control over those sectors) at the expense of civilian jobs.
The ratio between Indonesian parliamentary income and the median Indonesian income is ~18x, while the ratio in the US is ~4x. As someone who wants US congressional income to be substantially higher, it's hard for me to be upset at that on its own. There are plenty of other variables at play, though, and a direct comparison of these ones might not be getting at the issue.
A question related to the question, for which I apologize:It seems to me that using WireGuard (UDP) in conjunction with something like Raptor Forward Error Correction would be somewhat difficult to block. A client could send to and receive from a wide array of endpoints without ever establishing a session and communicate privately and reliably, is that correct?
there is a major protest currently happening due to the legislative body representative just giving themselves a monthly domicile stipend of ~$3300 on top of their salaries (yes, multiple), while the average people earned ~$330 monthly. the information about the protest are not broadcasted on local TVs, so the only spread of information is through social media. i guess since a lot of people went around it using VPN, the gov decided to block it too.
‚ÄúSome demonstrators on Monday were seen on television footage carrying a flag from the Japanese manga series One Piece, which has become a symbol of protest against government policies in the country.‚Äù
The official word is to counter gambling. Lately the government is not really popular after some decisions that could be interpreted as authoritative, and as citizens have spoken out about it online, causing more voices to join and protests erupting..So well, my guess is they're trying to control it.
I'd recommend using Outline - it's a one click setup that lets you provision your own VPN on a cloud provider (or your own hardware).Since you get to pick where the hardware is located and it is just you (or you and a small group of friends & family) using the VPN, blocking is more difficult.If you don't want the hassle of using your own hardware you can rent a Digital Ocean droplet for <$5 per month.https://getoutline.org/
I‚Äôve set this up for friends in fairly heavily censored countries before, it has been working well so far, but as others have said, this is a cat and mouse game
As someone based in China, it's a bit surprising that techniques used by Chinese people get very few mentions here, while I do think they are quite effective against access blocking, especially after coevolving with GFW for the past decade. While I do hope blocking in Indonesia won't get to GFW level, I will leave this here in case it helps.I found this article [0] summarizing the history of censorship and anti-censorship measures in China, and I think it might be of help to you if the national censorship ever gets worse. As is shown in the article, access blocking in China can be categorized into several kinds: (sorted by severity)1. DNS poisoning by intercepting DNS traffic. This can be easily mitigated by using a DOT/DOH DNS resolver.2. Keyword-based HTTP traffic resetting. You are safe as long as you use HTTPS.3. IP blocking/unencrypted SNI header checking. This will require the use of a VPN/proxy.4. VPN blocking by recognizing traffic signatures. (VPNs with identifiable signatures include OpenVPN and WireGuard (and Tor and SSH forwards if you count those as VPNs), or basically any VPN that was designed without obfuscation in mind.) This really levels up the blocking: if the government don't block VPN access, then maybe any VPN provider will do; but if they do, you will have a harder time finding providers and configuring things.5. Many other ways to detect and block obfuscated proxy traffic. It is the worse (that I'm aware of), but it will also cost the government a lot to pull off, so you probably don't need to worry about this. But if you do, maybe check out V2Ray, XRay, Trojan, Hysteria, NaiveProxy and many other obfuscated proxies.But anyways, bypassing techniques always coevolve with the blocking measures. And many suggestions here by non-Indonesian (including mine!) might not be of help. My personal suggestion is to find a local tech community and see what techniques they are using, which could suit you better.[0] https://danglingpointer.fun/posts/GFWHistory
Thanks for the link!Is there any good DoT/DoH DNS resolver that works well in China? I know I can build one myself, but forwarding all DNS requests to my home server in NA slows down all connections...
Shadowsocks used to be the thing that _really_ worked in CN. Not sure what's current there.AWS  ap-southeast-3 should still be up, and isn't in a different partition like CN, govcloud, iso etc. So a VM there and a vpc peer in the US should get you around a lot of stuff.
Personally, I like Amnezia VPN, it has some ways to work around blocks: https://amnezia.org/en
You can very easily self-host it, their installer automatically works on major cloud platforms.Though if Indonesia has blocked VPNs only now, possibly they only block major providers and don't try to detect the VPN protocol itself, which would make self-hosting any VPN possible.
Starlink, by policy, connects you through a ground station in the same country. They wouldn't be allowed to operate otherwise.
I live in Pakistan and two years back we had this exact same problem, (election interference) and frankly, you just try to scrape through solutions, but without an answerable government, there is little you can do.We tried things like Proton VPN and Windscribe VPN, as well as enabling MT proxy on Telegram, but soon govts find it easier to just mass ban internet access.Use Netblocks.org to analyse the level of internet blockage and try to react accordingly.
AmneziaWG is a decent option for censorship resistance, and it can be installed as a container on your own server.
You should use people power to work to make Indonesia a more open, democratic society.Yes, it's hard work. Yes, it will take a long time. Yes, you personally may not get very far with your efforts.But if Indonesians don't take responsibility for and work to improve Indonesia then the rest of it doesn't matter.
Part of that is knowing whats happening inside the country, of which they were previously using tools like discord, which have now been blocked. So the first step to using people power to make Indonesia a more open, democratic society would be to find a way to tunnel out to get and share that information. To that end the OP has created this Ask HN thread.
Nope. The outside doesn't matter. The problem is on the inside. External websites will never fix the internal problem.There are no technical solutions to what is fundamentally a problem of political culture.
How do you propose they coordinate the political activities when they can't use external communications sites/tools, and internal sites are actively monitored by an authoritarian government?Step 1 is establishing a secure means of communication.
Aren't there local (online or print) newspapers to get news from, as an alternative to Discord? Hope I'm not asking a dumb question
In countries where it comes to government blocking/censoring internet traffic, traditional media is cleared of all dissent and fully controlled long before. Last stages of that are happening in my country, Serbia, currently.
Right, that makes sense. Did some looking up and nonfree press seems to be indeed the case for Indonesia: https://rsf.org/en/country/indonesiaIt's a mixed bag apparently, free press is technically legal since 1998 but selective prosecution and harassment of those actually uncovering issues (mainly becomes clear in the last section, "Safety")Tried looking up Serbia next on that website but got a cloudflare block. I'm a robot now...
It's not a dumb question at all. Level on hn really got down lately if you're getting downvoted.Think about it Aachen. If the government has enough power to censor internet traffic, that what was the first thing it censored? Which media is traditionally known for being censored or just speaking propaganda? That's the classical newspapers. It's not uncommon in authoritarian countries for editors to need state to sign off on the day's paper. And if not that, articles are signed and publishers are known. They will auto-censor to avoid problems. Just like creators on YouTube don't comment on this one country's treatment of civilians to avoid problems.
It is not a real URI... lolThe point was to include something clowns can't filter without incurring collateral costs, and wrapping the ssh protocol in standard web traffic. =3
Your first option until you get settled is to use an SSH reverse proxy:    ssh -D 9999 user@my.server

Then configure your browser to use local port 9999 for your SOCKS5 proxy.This gets you a temporarily usable system and if you can tunnel this way successfully installing some WireGuard or OpenVPN stuff will likely work.EDIT: Thanks it's -D not -R
Censorship circumvention tools specialize in this, and are extensively used in China, Iran, and Russia. I work on Lantern, and we're not seeing any significant interruptions to connections in Indonesia at the moment.
https://lantern.io/downloadHope it helps!
I was wondering something like this but in a different capacity.What with certain countries (they know who they are) and their hatred for encryption, it got me wondering how people would communicate securely if - for example - Signal/WhatsApp/etc. pulled out and the country wound up disconnecting the submarine cables to "keep $MORAL_PANIC_OF_THE_DAY safe."How would people communicate securely and privately in a domestic situation like that?
In person or not at all.At that point you've essentially lost.You either hope another country sees value in spreading you some democracy, or you rise up and hope others join you.Or not and you accept the protection the state is graciously providing to you.
Use the Tor browser window in Brave. It's nowhere near as anonymous as the Tor browser, but the built in ad blocking makes browsing via Tor usable. And that's what you and your compatriots are interested in.Prepare to fill in Cloudflare captchas all day, but that's what it takes to have a bit of privacy nowadays.
Usually when countries block websites they don't block major cloud providers, like AWS and Google Cloud. Because most websites are hosted on them. So you can get a cheap VPS from AWS or GCP (always free VM is available) and host OpenVPN on it.
Try looking into tor bridges.You could also buy a VPS and use SSH tunneling to access a tor daemon running on a VPS. Host some sort of web service on the VPS so it looks inconspicuous
I like mullvad. You can buy a prepaid card off amazon. I figured out how to setup wireguard on various unixes Mac/linux/openbsd
In this case the blockage will probably just be up for a few days, until the protests calmed down.Other than that: tor
Set up a VM on AWS/azure/gcp/... in the desired cell, install a VPN server and done. Once you have automation in place it takes ~2 minutes to start, you can run it on demand so you can pay per minute.
All the various proxy solutions offered are good (although the simplest ones - like squid - haven't been mentioned yet). You can also use a remote desktop or even just ssh -Y me@remote-server "firefox"
Android doesn't come with system wide socks proxy support, and i couldn't find an open source app for it either. Is anyone aware of one?Nonetheless this is a surprisingly simple and bullet proof solution: SSH, that's not vpn boss, i need it for work.
Outline is an open source shadowsocks client, and you provision your own server to act as the proxy.  You can use it against any Shadowsocks server you want, and the protocol makes it look like regular https traffic.https://github.com/Jigsaw-Code/outline-appsAndroid & iOS & Linux & Mac & Windowstheir server installer will help set up a proxy for users that aren't familiar with shadowsocks, too
Try a ssh socks5 proxy to a cheap vps.It worked well for me in UAE when other solutions didn‚Äôt
Launch an EC2 instance in the US region (Ubuntu, open ports 22 and 1194), then connect via SSH and run the OpenVPN install script. Generate the .ovpn profile with the script and download it to your local machine. Finally, import the file into the OpenVPN client and connect to route traffic through the US server.
I'd recommend Obscura because it uses Wireguard over QUIC and it pretty good at avoiding these blocks. It's also open source.
Make your own VPN using a VPS and something like openvpn.Not every website will allow it, but it should get you access to more than you have now.
Depending on the circumstances, maybe ditch the landline local ISP for a satellite connection with a foreign ISP?
The closest I've come to this is on an airplane where almost everything was blocked. SSTP to a server I spun up worked well.
A proxy service like shadow socks works. There are thousands of providers for $X/month for a decent amount of traffic
OP, you can rent a VPS from a reputable and cheap provider within the NA region - OVH, Vultr, Linode etc. are decent. Also check out lowendtalk.comThen, setup Tailscale on the server. You can VPN into it and essentially browse the internet as someone from NA.
From some of the comments here I get why you are downvoted. But tbh I would also have gone that route. So are we just inexperienced? I read here indeed that wireguard is very easily blocked. It was at the company I worked for but then I just set port 23 (who uses ftp anyways??). And it worked. But why is this still bad then?Obviously I have 0 real experience with this.
Well, I mean, Tailscale is pretty easy overall. When client apps get blocked, you can literally hook up your router into Tailscale if needed, or you can run a headless version of Tailscale on your home server or the very machine you are on.It should also be possible to use a tunnel to get around the blocking of WireGuard, for example.You can then use it as an exit node if needed. It should work in theory, I have never tried this though. I just speak as a very frequent user of Tailscale with a bunch of nodes that are geographically located in different cities around me.
Sure, I know and use it too. But I saw you being downvoted so I responded to that. I think, reading the rest of the thread, your response (as mine would be) does not work as signals 0 experience with actually oppressing regimes. Not?
Full disclosure, I run a commercial VPN service (Windscribe).There are 2 paths you can take here:1. Roll your own VPN server on a VPS at a less common cloud provider and use it. If you're tech savvy and know what you're doing, you can get this going in <1hr. Be mindful of the downsides of being the sole user of your custom VPN server you pay for: cloud providers log all TCP flows and traffic correlation is trivial. You do something "bad", your gov subpoenas the provider who hands over your personal info. If you used fake info, your TCP flows are still there, which means your ISP's IP is logged, and deanonymizing you after that is a piece of cake (no court order needed in many countries).2. Get a paid commercial VPN service that values your privacy, has a diverse network of endpoints and protocols. Do not use any random free VPN apps from the Play/App stores, as they're either Chinese honeypots (https://www.bitdefender.com/en-us/blog/hotforsecurity/china-...) or total scams (https://www.tomsguide.com/computing/vpns/this-shady-vpn-has-...).Do not go with a VPN service that is "mainstream" (advertised by a Youtuber) or one that has an affiliate program. Doing/having both of these things essentially requires a provider to resort so dishonest billing practices where your subscription renews at 2-5x of the original price. This is because VPNs that advertise or run affiliate programs don't make a profit on the initial purchase for that amazing deal thats 27 months with 4 months free or whatever the random numbers are, they pay all of this to an affiliate, sometimes more. Since commercial VPNs are not charities, they need ROI and that comes only when someone rebills. Since many people cancel their subscriptions immediately after purchase (to avoid the thing that follows) the rebill price is usually significantly more than the initial "amazing deal". This is why both Nord and Express have multiple class action lawsuits for dishonest billing practices - they have to do it, to get their bag (back). It's a race to the bottom of who can offer the most $ to affiliates, and shaft their customers as the inevitable result.Billing quirks aside, a VPN you choose should offer multiple VPN protocols, and obfuscation techniques. There is no 1 magic protocol that just works everywhere, as every country does censorship differently, using different tools.- Some do basic DNS filtering, in which case you don't need a VPN at all, just use an encrypted DNS protocol like DOH, from any provider (Cloudflare, Google, Control D[I also run this company], NextDNS, Adguard DNS)- Then there is SNI filtering, where changing your DNS provider won't have any effect and you will have to use a VPN or a secure proxy (HTTPS forward proxy, or something fancier like shadowsocks or v2ray).- Finally there is full protocol aware DPI that can be implemented with various degrees of aggressiveness that will perform all kinds of unholy traffic inspection on all TCP and UDP flows, for some or all IP subnets.For this last type, having a variety of protocols and endpoints you can connect to is what's gonna define your chance of success to bypass restrictions. Beyond variety of protocols, some VPN providers (like Windscribe, and Mullvad) will mess with packets in order to bypass DPI engines, which works with variable degree of success and is very region/ISP specific. You can learn about some of these concepts in this very handy project: https://github.com/ValdikSS/GoodbyeDPI (we borrow some concepts from here, and have a few of our own).Soooo... what are good VPNs that don't do shady stuff, keeps your privacy in mind, have a reasonably sized server footprint and have features that go beyond basic traffic proxying? There is IVPN, Mullvad, and maybe even Windscribe. All are audited, have open source clients and in case of Windscribe, also court proven to keep no logs (ask me about that 1 time I got criminally charged in Greece for actions of a Windscribe user).If you have any questions, I'd be happy to answer them.
I can relate to this because my country has an election soon and I'm sure we wont have internet for 3 - 5 days then.
Tor should be pretty good even for environments where they crack down on VPNs, although it can be a bit slow, at least it works.
Yeah, sucks, but really should find better places for people to gather regardless, if you're in that sort of environment.
How is this practical advice in a thread where someone mentions that the clampdown happened without notice?The "shoulda done..." advice isn't useful in the slightest, and I'd argue is malicious with how often it's done simply to satiate a poster's ego.
You could rent a cheapo instance at a cloud provider and tunnel https over ssh.That‚Äôs basically undetectable. Long lived ssh connection? Totally normal. Lots of throughput? Also normal. Bursts throughput? Same.Not sure how to do this on mobile.Tailscale might be an option too (they have a free account for individuals and an exit node out of country nearly bypasses your problem) It uses wireguard which might not be blocked and which comes with some plausible deniability. It‚Äôs a secure network overlay not a VPN. It just connects my machines, honest officer.
Just please be safe and necessarily paranoidOne way they tend to "solve" workarounds is making examples of people
Use an Actual Private Network? Radio links that you control. Peer with someone who owns a Starlink terminal. Rent instances in GCP's Jakarta datacenter.
Blocking Twitter is a good start, now Facebook, Instagram, Whatsup and TikTok.This is a good start but more should be blocked. Then force ISP to block ads.Not just for Indonesia but all countries. But we still have a lot more to do to fix the web.
The issue with that is where do they draw the line. Next thing you know each country becomes North Korea.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Launch HN: Dedalus Labs (YC S25) ‚Äì Vercel for Agents]]></title>
            <link>https://news.ycombinator.com/item?id=45054040</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054040</guid>
            <description><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools ‚Äì local or hosted by us. No Dockerfiles or YAML configs required.]]></description>
            <content:encoded><![CDATA[Hey HN! We are Windsor and Cathy of Dedalus Labs (https://www.dedaluslabs.ai/), a cloud platform for developers to build agentic AI applications. Our SDK allows you to connect any LLM to any MCP tools ‚Äì local or hosted by us. No Dockerfiles or YAML configs required.Here‚Äôs a demo: https://youtu.be/s2khf1Monho?si=yiWnZh5OP4HQcAwL&t=11Last October, I was trying to build a stateful code execution sandbox in the cloud that LLMs could tool-call into. This was before MCP was released, and let‚Äôs just say it was super annoying to build‚Ä¶ I was thinking to myself the entire time ‚ÄúWhy can‚Äôt I just pass in `tools=code_execution` to the model and just have it‚Ä¶work?Even with MCP, you‚Äôre stuck running local servers and handwiring API auth and formatting across OpenAI, Anthropic, Google, etc. before you can ship anything. Every change means redeploys, networking configs, and hours lost wrangling AWS. Hours of reading docs and wrestling with cloud setup is not what you want when building your product!Dedalus simplifies this to just one API endpoint, so what used to take 2 weeks of setup can take 5 minutes. We allow you to upload streamable HTTP MCP servers to our platform. Once deployed, we offer OpenAI-compatible SDKs that you can drop into your codebase to use MCP-powered LLMs. The idea is to let anyone, anywhere, equip their LLMs with powerful tools for function calling.The code you write looks something like this:  python
  client = Dedalus()
  runner = DedalusRunner(client)
  
  result = runner.run(
    input=prompt,
    tools=[tool_1, tool_2],
    mcp_servers=["author/server-1‚Äù, ‚Äúauthor/server-2‚Äù],
    model=["openai/gpt-4.1‚Äù, ‚Äúanthropic/claude-sonnet-4-20250514‚Äù],  # Defaults to first model in list
    stream=True,
  )
  stream_sync(result)  # Streams result, supports tool calling too

Our docs start at https://docs.dedaluslabs.ai. Here‚Äôs a simple Hello World example: https://docs.dedaluslabs.ai/examples/01-hello-world. For basic tool execution, see https://docs.dedaluslabs.ai/examples/02-basic-tools. There are lots more examples on the site, including more complex ones like using the Open Meteo MCP to do weather forecasts: https://docs.dedaluslabs.ai/examples/use-case/weather-foreca....There are still a bunch of issues in the MCP landscape, no doubt. One big one is authentication (we joke that the ‚ÄúS‚Äù in MCP stands for ‚Äúsecurity‚Äù). MCP servers right now are expected to act as both the authentication server and the resource server. That is too much to ask of server writers. People just want to expose a resource endpoint and be done.Still, we are bullish on MCP. Current shortcomings are not irrecoverable, and we expect future amendments to resolve them. We think that useful AI agents are bound to be habitual tool callers, and MCP is a pretty decent way to equip models with tools.We aren‚Äôt quite yet at the stateful code execution sandbox that I wanted last October, but we‚Äôre getting there! Shipping secure and stateful MCP servers is high on our priority list, and we‚Äôll be launching our auth solution next month. We‚Äôre also working on an MCP marketplace, so people can monetize their tools, while we handle billing and rev-share.We‚Äôre big on open sourcing things and have these SDKs so far (MIT licensed):https://github.com/dedalus-labs/dedalus-sdk-pythonhttps://github.com/dedalus-labs/dedalus-sdk-typescripthttps://github.com/dedalus-labs/dedalus-sdk-gohttps://github.com/dedalus-labs/dedalus-openapiWe would love feedback on what you guys think are the biggest barriers that keep you from integrating MCP servers or using tool calling LLMs into your current workflow.Thanks HN!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Optimising for maintainability ‚Äì Gleam in production at Strand]]></title>
            <link>https://gleam.run/case-studies/strand/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45053462</guid>
            <description><![CDATA[A case study of Gleam in production at Strand]]></description>
            <content:encoded><![CDATA[Strand is a marketing agency based in London,
UK. The company specialises in copywriting and content creation for many of the
world‚Äôs largest enterprise technology companies, running marketing programmes
that produce hundreds of white papers, case studies, blog posts and articles
every year.
Challenge
For many years, Strand has relied on a custom-built project management system
to support the operational aspects of its business‚Äîcreating projects, tracking
activities and managing documents. However, managing the financial aspects of
project management had always been a more manual process, using spreadsheets to
ensure that billable work was assigned to the correct purchase orders and
invoices.
‚ÄúJust before the pandemic, we decided to build a new financial management
system,‚Äù recalls Ed Kelly, Director of Technology at Strand. ‚ÄúIt turned out to
be a very timely decision. When we had to pivot to remote working, the fact
that everyone could track their billable work in a centralised system helped us
keep the business on track.‚Äù
The new system quickly became an integral part of Strand‚Äôs daily workflow, and
users began requesting new features. As the application gradually grew larger
and more complex, the company‚Äôs small development team wanted to ensure that
the system would remain reliable, maintainable and scalable.
‚ÄúAlmost by accident, what we launched as a prototype became a business-critical
application,‚Äù says Ed Kelly. ‚ÄúOur development resources are limited, so our top
priority was to make sure the system would just run forever without needing
constant maintenance. At the same time, we also wanted to keep the codebase
simple and approachable, so it‚Äôs easy for developers to dive back into when
they need to make a change. The challenge for us was to build and maintain this
business-critical system cost-effectively with our lean development team.‚Äù
Solution
As a small business, Strand is not afraid to innovate. ‚ÄúWe do have systems that
are written in mainstream programming languages like Python and JavaScript, but
our strategy is to pick the best tool for the job, not just the most popular,‚Äù
explains Ed Kelly. ‚ÄúGleam was a good fit for our requirements.‚Äù
The features of Gleam that appealed to Strand were its robustness and
maintainability, its combination of modern language features with access to a
broad ecosystem of battle-tested, production-grade libraries, and its strong
focus on developer experience.
Safety and reliability
‚ÄúGleam is a safe language,‚Äù explains Ed Kelly. ‚ÄúBroadly speaking, if you write
a program in pure Gleam, it‚Äôs guaranteed not to crash. And in cases where you
need to interface with code written in other, less-safe languages, there is a
second layer of protection provided by Gleam‚Äôs runtime platform, the BEAM.‚Äù
The BEAM was developed by Ericsson in the 1980s as a fault-tolerant platform
for managing large telephone switches that need to handle thousands of calls
simultaneously and can never be taken offline for maintenance. The central idea
is that the platform is able to divide applications into thousands or even
millions of lightweight processes. Each process runs independently, and
processes can communicate by sending messages to each other. If an individual
process crashes, it can be restarted automatically without affecting any of the
other processes.
‚ÄúThe application that we‚Äôve built is composed of several services that interact
with the outside world,‚Äù explains Ed Kelly. ‚ÄúFor example, we have a service
that periodically downloads currency exchange rates from the UK government‚Äôs
website, and another that syncs data with our project management system. The
BEAM ensures that if there‚Äôs some unforeseen problem with any of these external
services, it won‚Äôt crash our application.‚Äù
Modernity and pragmatism
Gleam is designed to be a simple language that provides powerful features while
remaining resolutely practical. ‚ÄúIt gives us access to features from more
academic programming languages, but it makes them approachable,‚Äù says Ed Kelly.
‚ÄúThe language is small‚Äîan experienced developer can learn it in an
afternoon‚Äîand there is a strong focus on only having one way to do things. That
means you can onboard new developers into a Gleam codebase quickly.‚Äù
Because Gleam code runs on the BEAM, developers also have easy access to
thousands of high-quality software libraries. ‚ÄúThe Gleam library ecosystem is
growing rapidly year-on-year,‚Äù says Ed Kelly. ‚ÄúAnd when we need to, we can also
reach for 40 years‚Äô worth of battle-tested libraries written in other BEAM
languages such as Erlang and Elixir. The language prioritises pragmatism over
purity, which helps us get things done.‚Äù
Developer experience
In Strand‚Äôs experience, Gleam‚Äôs developer tools are second to none. ‚ÄúWhen you
download Gleam, you get all the tooling in a single package,‚Äù says Ed Kelly.
‚ÄúIt integrates with your code editor to provide features like formatting,
suggestions and autocomplete. The error messages are really friendly and
helpful‚Äîwhen you make a mistake, Gleam will often tell you what you should have
written. And it‚Äôs really fast‚Äîthe days of going for a coffee break while you
wait for your code to build are over.‚Äù
He adds: ‚ÄúWe‚Äôre heading into a new age of AI-assisted coding, and right now,
it‚Äôs difficult to predict how that will play out. But if I had to place a bet,
I would say that in the long run, AIs are more likely to generate high-quality
code in a language like Gleam. Gleam makes it quick and easy for AIs to check
their code, get instant feedback, and iterate. That should be an advantage
compared to languages that are slow to build, have cryptic error messages, and
can‚Äôt catch mistakes at build-time.‚Äù
Incremental adoption
For Strand, introducing Gleam into its codebase was a low-risk, incremental
process. ‚ÄúWe started with just one service‚Äîour integration with the UK
government‚Äôs currency exchange rate API,‚Äù says Ed Kelly. ‚ÄúWe were so pleased
with how it turned out that we then rewrote some of our other services in
Gleam. And recently, we‚Äôve decided to give Gleam an even more important role by
replacing the whole part of the backend that talks to our database. We‚Äôre very
confident that this will give us a safer and more maintainable codebase
overall.‚Äù
Results
As one of the first companies in the world to run Gleam in production, Strand
took a risk. Two years later, the development team is delighted with the
decision. ‚ÄúSince we started, the language has really matured and reached a
stable state,‚Äù says Ed Kelly. ‚ÄúThe community has grown massively and there‚Äôs a
real buzz around the language. It‚Äôs even starting to be recognised by
mainstream industry analysts like Thoughtworks in their Technology
Radar. I
think today, Gleam is a safe and solid choice for companies to use in
production.‚Äù
Since go-live, the Gleam code within Strand‚Äôs application has been rock-solid.
‚ÄúWe‚Äôve had zero Gleam-related crashes, and even when there have been issues
with other parts of the system, the BEAM has kept everything running,‚Äù says Ed
Kelly. ‚ÄúWe‚Äôve been able to fix problems without our users even noticing that
anything was wrong.‚Äù
The simplicity of the language and the sophistication of the development tools
also help to keep the codebase maintainable. ‚ÄúEven when we haven‚Äôt looked at
the codebase for a few weeks, it‚Äôs easy to get back into it,‚Äù says Ed Kelly.
‚ÄúThe language and tooling gently push you to use a consistent, idiomatic style,
and to write clearly and simply without trying to be too clever. So, we don‚Äôt
have to spend time puzzling out what our past selves were trying to do with the
code that we wrote six months ago.‚Äù
He concludes: ‚ÄúAdopting a new language is always a gamble, but Gleam has paid
off. The belt-and-braces approach to safety and fault-tolerance has given us a
system that just works, reliably, day in and day out, without constant
babysitting and maintenance. For a team like ours, with many other priorities
and projects we need to work on, the confidence that Gleam gives us is worth
its weight in gold.‚Äù
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Will AI Replace Human Thinking? The Case for Writing and Coding Manually]]></title>
            <link>https://www.ssp.sh/brain/will-ai-replace-humans/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052784</guid>
            <description><![CDATA[Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. It‚Äôs OK to use it, and everyone does it, but still learn your craft, and try to think.]]></description>
            <content:encoded><![CDATA[
Learning to Think Again, and the Cost of AI Dependency.
There are so many (hype/boring) posts about AI coming out every day. It‚Äôs OK to use it, and everyone does it, but still learn your craft, and try to think.
Similar to what DHH said:

It‚Äôs also more fun to be competent in something than constantly waiting for an AI to complete.
The probability that AI will make us unhappy is very high IMO. Use it, yes, but not for every task. For discovering, creating a historical overview, or creating diagrams (Canva, Figma), but a big no to the writing (or coding). Someone needs to add knowledge or new insights; AI cannot train itself. So articles, books, and words will be written, and writers will be more in demand as everyone relies on AI, which at some point just plateaus.
It will be a long-term loss; people stop thinking and learning. Time will tell. My two cents, if you are a senior in something, you know better. 

Bsky
# Guidance on When to Use It
I 

heard from ThePrimeagen: It depends on how far you fix into the future. Short-term autocomplete is fine, but architectural decisions are big no, no‚Äôs.

This is about the bottom where we have time and the left where we have amount of errors. This means that the longer we use AI for fixing something in the future, like an architecture, the more errors it will produce.
If we use it for quick autocomplete or creating a well-defined algorithm function, it‚Äôs less prone to errors. In that first phase, you gain 20% productivity; in the later phases, you lose more.

This is like in real life, the longer I wait with making m decision, the more information I have, the better the decison will be. And is exactly what Shape Up preaches with maximum deciding for 6 weeks (a cycle),  don‚Äôt have roadmaps and backlogs for longer than that in the future. Similar is it with using AI, as all of it is predicted probability.
Another great illustration by 

Forrest Brazeal:

Also to keep in mind what‚Äôs most imporant to your usecase like illustrated by Thomas Ptacek in 

My AI Skeptic Friends Are All Nuts ¬∑ The Fly Blog:

# Soulless
Nobody wants to read some soulless text, and what if it‚Äôs even good? Where do you get more from? I think this is a big trap that only over time people will realize. Sure, they help, and everyone needs to use them for ‚Äúcertain‚Äù tasks, but not the writing itself.
In the end, LLMs and AI require guidance; they‚Äôre just probabilities. See also Writing Manually.
# Distraction
I think we will be more distracted than ever. We can‚Äôt even have 2 seconds to think before Grammarly, Copilot, or Cursor suggests something. So instead of doing the thinking, we just cruise on. We are losing the driver‚Äôs seat.
It brings me back to the article I wrote recently about ¬´

Finding Flow¬ª. More on Don‚Äôt use AI for everything, you stop thinking-learning AI Use and Writing is Hard.
# Don‚Äôt Get Me Wrong
Don‚Äôt get me wrong, I use it every day, too. But more deliberately. I turned off my Grammarly and my Copilot (a long time ago), so I have the space to think and to learn. If you do it once or twice, that‚Äôs OK, but if you do it everywhere, you also lose the ability to learn new skills or the fun of it.
Interesting about the LCI (LLM Collaborative Intelligence), and sure, there will be a lot of benefits, but I am not sure if these insights are anything that comes close to a human insight that has felt, sensed, or experienced something through hardship. So yes, but I do not have many expectations, nor do I want it to create new insights. This is the fun part of my job :)
# Exercising a Skill
It‚Äôs never always or never; it‚Äôs in between. The problem with learning is if you use it often, I‚Äôd argue that you, in fact, don‚Äôt learn much. You just copy and paste in writing or just tab tab tab in coding. The learning is gone. And do that often enough; our brain is not used to learning or, more critically, thinking anymore. Same as remembering, how good can we remember mobile phone numbers? not really, but I could very well during the early phone times because I trained it every day.
It‚Äôs all a matter of exercise, and I learned for myself‚Äîit doesn‚Äôt have to be true for everyone‚Äîthat I didn‚Äôt learn or think anymore. And frankly, it was also not fun anymore. That‚Äôs to be said in the stuff I know well.
In other areas, like creating an image (like the one I created for this article üòÜ) or updating my website‚Äôs front page with HTML/CSS, which would have taken me much longer as I don‚Äôt practice, it helped a lot. But I‚Äôd argue the fact that I didn‚Äôt learn anything new except prompting Claude Code :). It‚Äôs all tradeoffs, as always, right? :)
# Other Opinions
# Paul Graham on Writing
Paul Graham says on 

Writes and Write-Nots (internal):

The result will be a world divided into writes and write-nots. There will still be some people who can write.
Yes, it‚Äôs bad. The reason is something I mentioned earlier: writing is thinking.
In fact there‚Äôs a kind of thinking that can only be done by writing.
If you‚Äôre thinking without writing, you only think you‚Äôre thinking.
So a world divided into writes and write-nots is more dangerous than it sounds. It will be a world of thinks and think-nots.

# Nathan Baugh
Nathan Baugh shares on 

About AI and ghostwriting:

1st Order Effect:

The world will be overrun with slop content and stories.
We already see this. Just look at AI written comments on this platform.

2nd Order Effect:

People will stop learning the foundational skills ‚Äì storytelling, writing, rhetoric ‚Äì required to communicate their experiences and ideas effectively.
They will over rely on AI. It starts as a tool, becomes a crutch, and ends as a hindrance.

3rd Order Effect:

People who invest in those same skills see massive returns.
Writing sharpens your ideas. Story gives leverage to those ideas.

# Ted Gioia
The good news, and why AI won‚Äôt replace writers on 2024-08-31 by Ted Gioia. Some of the reasons why he thinks AI Writing won‚Äôt be as good:



Source on Twitter/X. Full article 

Google Thinks Beethoven Looks Like Mr. Bean - by Ted Gioia.
# Mitchell Hashimoto

2.5 years into the AI craze, and I continue to firmly believe that if your company wasn‚Äôt already interesting/succeeding without AI, then doing ‚Äúwhatever plus AI‚Äù isn‚Äôt going to save you. For the few that seem this way (eg Cursor), I think their moat is a lot weaker than it seems. You have to play the game and the game is AI, but I don‚Äôt think it‚Äôs a defensible foundational capability. Might play out as an excellent land and grab strategy to buy you time to fill out the meat though. Mitchell Hashimoto on 

Twitter
# Andrew Ng
Andrew Ng on 

Twitter/X:

Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, ‚ÄúIt is far more likely that the programming occupation will become extinct [‚Ä¶] than that it will become all-powerful. More and more, computers will program themselves.‚Äù‚Äã Statements discouraging people from learning to code are harmful!
In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today‚Äôs arguments not to learn to code continue to echo his comment.
As coding becomes easier, more people should code, not fewer!
Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step.
I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals ‚Äî individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively.
den>
One question I‚Äôm asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is a great way to do that.
When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on ‚Äî using the language of art ‚Äî to get the result he wanted. I didn‚Äôt know this language, and my paltry attempts at prompting could not deliver as effective a result.
Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools are continuing to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do.
# Harry Dry

Big ideas are less about creativity and more about conviction. [..] So, what happened? ‚ÄòSauce and other shit‚Äô got incredibly cheap! [..] There is no AI prompt for conviction. Harry Dry
^64403f
More on Is AI solving this?.
# Jason Fried
As Jason Fried said, initially, it‚Äôs magical. After a while, you see it so clearly and it‚Äôs just average:



Cover letters? Yes!

The hardest thing is not making something.
The hardest thing is maintaining something.
It‚Äôs become so easy to just make stuff and vomit out ideas, and I mean this in the best possible way‚Ä¶ 

Jason Fried on LinkedIn
This another valid insights, it‚Äôs hard to maintain code that is not made by you, it‚Äôs losing it‚Äôs fun. Therefore this will be a big part of a winning business, to have sustainable, and energy to want to maintain a product. And not ‚Äújust making it‚Äù.
Also who takes responsibility for the generated (vibed) code?
# David Perell
David Perell has similar views as me on being soulless:

When you outsource your writing to AI, you end up with words that lack soul or personality. Gone go your quirks and your idiosyncrasies, which are the very things that make your writing irreplaceable. 

LinkedIn
# Ezra Klein
Ezra Klein has 

great insights that I very align with in terms of writing. He says that there are no shortcuts for research. When you grapple with a text or book for seven hours, it will change you. This will influence your writing, too. There‚Äôs no summary that gives you this kind of in-depth connection.
Also, you can‚Äôt prompt your way into it, as there‚Äôs no prompt that knows that you don‚Äôt know yet, or AI doesn‚Äôt know what you wanted to have read or what connections you would have made. On the contrary, you actually lose time reading something, and over time, we think we read lots of stuff, but we actually only read summaries. Full episode on 

The Case Against Writing with AI.
# Will It Replace X
# Writers


Are Cover letters still a thing? Yes. This reminded me of good writing is key for every job these days.  Writing was always an asset, but even more these days; although people think they don‚Äôt need it, as AI is doing that. But that‚Äôs a very dangerous bet I wouldn‚Äôt take.
I wrote more on Writing Manually.
# Data Engineers?
Probably not.
Nice 

comparison by Mehdi Ouazza:


Did the music record replace musicians 100 years ago? Nope, it changed them and the industry.
Did cloud computing take all IT jobs? Nope, it also changed the industry and our jobs.
Same here; it will change our industry and job, but we won‚Äôt disappear.

More on Will AI replace Data Engineers.
# Image Generation
Initial generation, yes. But final touch, no. Whenever I try to create images with AI, I am always initially impressed, but that quickly fades over time.
Yesterday, I updated my second brain image, but I changed it again today. I created some more with AI; prompted prompted prompted. In the end, I made one manually based on my copy. I think it‚Äôs more powerful. What do you think?
# ChatGPT


# My Own


Some AI-generated images I like too, but they were always missing something, and yeah, they looked so AI-generated. I started to feel the same as I did for AI writing () and AI data engineering (Will AI replace Data Engineers); now, with AI image generation, doing it yourself is more fulfilling, and you end up happier.
More on AI Generated Images.
# How to detect AI Writing
How to detect AI Writing
If we know how AI is writing, should we stop using em dashes or thing AI does?
I don‚Äôt think so. I love the em dash. I even have a keyboard shortcut for the em dash. And sometimes when I write a negation, I‚Äôm thinking ¬´could that look like it‚Äôs written by AI¬ª.
But at the end, convition is a good word. I can focus what an AI thinks while I write, I must write. So having something to say, and trying my best to communicate that, is the best I can do.  ^ebca60
# History Logs
# From 2024-10-12
What AI Writing can‚Äôt do, because it can only think one word at a time.
E.g., in the below example, as a writer you know you need to start all sentences the same, but the AI model can‚Äôt do that.

Writing from Abundance is the art of collecting ideas so you can think better and avoid writer‚Äôs block.
Writing from Conversation is the art of using dialogue to identify your best ideas and double down on them.¬†
Writing in Public is the art of broadcasting your ideas to the Internet so you become a beacon for people, opportunities, and serendipity.

More on Copywriting.
# AI Slop - Companies not doing great
AI Slop is generating more content, no matter the quality. It‚Äôs a the never ending Quality vs Quantity discussion, but now ever more important.
Here are some companies backpedaling after going full AI-first:

Klarna 

backpedaling AI customer service.:



‚ÄúAfter years of depicting Klarna as an AI-first company, the fintech‚Äôs CEO reversed himself, telling Bloomberg the company was once again recruiting humans after the AI approach led to ‚Äúlower quality.‚Äù An IBM survey reveals this is a common occurrence for AI use in business, where just 1 in 4 projects delivers the return it promised and even fewer are scaled up.‚Äù



Duolingo getting 

worse with AI
Next up, Shopify after the 

announcement to go full AI?

# Learning With AI
Learning with AI
# Future

Nice insights, why LLMs with token pretictors are not so good for understanding the worlds. It kinda works (but not so so good) for writing, but to understand physics, and world models, this is much harder he says: 

Metas AI Boss Says He DONE With LLMS‚Ä¶

# Further Reads



The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers
Smart Note Taking


My AI Skeptic Friends Are All Nuts ¬∑ The Fly Blog
Companies that used AI to generate a quick solutions and now spending humans to fix it, expensively



Companies That Tried to Save Money With AI Are Now Spending a Fortune Hiring People to Fix Its Mistakes


Companies That Replaced Humans With AI Are Realizing Their Mistake




AWS CEO says AI replacing junior staff is ‚Äòdumbest idea‚Äô


Origin: Artificial General Intelligence
References: ChatGPT, My AI Logs of Will AI replace humans, My AI Prompts
Created 2024-08-31

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI adoption linked to 13% decline in jobs for young U.S. workers: study]]></title>
            <link>https://www.cnbc.com/2025/08/28/generative-ai-reshapes-us-job-market-stanford-study-shows-entry-level-young-workers.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45052423</guid>
            <description><![CDATA[A Standford study has found evidence that the widespread adoption of generative AI is impacting the job prospects of early career workers.]]></description>
            <content:encoded><![CDATA[A Standford study has found evidence that the widespread adoption of generative AI is impacting the job prospects of early career workers.Vertigo3d | E+ | Getty ImagesThere is growing evidence that the widespread adoption of generative AI is impacting the job prospects of America's workers, according to a paper released on Tuesday by three Stanford University researchers.The study analyzed payroll records from millions of American workers, generated by ADP, the largest payroll software firm in the U.S.The report found "early, large-scale evidence consistent with the hypothesis that the AI revolution is beginning to have a significant and disproportionate impact on entry-level workers in the American labor market."Most notably, the findings revealed that workers between the ages of 22 and 25 in jobs most exposed to AI ‚Äî such as customer service, accounting and software development ‚Äî have seen a 13% decline in employment since 2022.By contrast, employment for more experienced workers in the same fields, and for workers of all ages in less-exposed occupations such as nursing aides, has stayed steady or grown.¬†Jobs for young health aides, for example, rose faster than their older counterparts.Front-line production and operations supervisors' roles also showed an increase in employment for young workers, though this growth was smaller than that for workers over the age of 35.The potential impact of AI on the job market has been a concern across industries and age groups, but the Stanford study appears to show that the results will be far from uniform.¬†The study sought to rule out factors that could skew the data, including education level, remote work, outsourced jobs, and broader economic shifts, which could impact hiring decisions.According to the Stanford study, their findings may explain why national employment growth for young workers has been stagnant, while overall employment has largely remained resilient since the global pandemic, despite recent signs of softening.Young workers were said to be especially vulnerable because AI can replace "codified knowledge," or "book-learning" that comes from formal education. On the other hand, AI may be less capable of replacing knowledge that comes from years of experience.¬†The researchers also noted that not all uses of AI are associated with declines in employment. In occupations where AI complements work and is used to help with efficiency, there have been muted changes in employment rates.The study ‚Äî which hasn't been peer-reviewed ‚Äî appears to show mounting evidence that AI will replace jobs, a topic that has been hotly debated.¬†Earlier this month, a Goldman Sachs economist said changes to the American labor market brought on by the arrival of generative AI were already showing up in employment data, particularly in the technology sector and among younger employees.¬†He also noted that most companies were yet to deploy artificial intelligence for day-to-day use, meaning that the job market impact had yet to be fully realized.]]></content:encoded>
        </item>
    </channel>
</rss>