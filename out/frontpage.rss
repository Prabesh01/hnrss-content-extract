<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 07 Sep 2025 22:07:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Print GitHub Repositories as Books]]></title>
            <link>https://gitprint.me/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45162110</guid>
            <description><![CDATA[Print your favorite Git repositories as PDF]]></description>
            <content:encoded><![CDATA[> Print your favorite Git repositories as PDFLooking for a fun way to explore your favorite GitHub repositories? Tired of staring at the screen for hours on end? Or maybe want to print out a hard copy as a keepsake?Simply sign in with your GitHub account and start printing public or private repositories in a beautiful, easy-to-read format.It is currently in beta, so please be patient with us as we work. Feel free to request features or report bugs.Made by @pliutau with ❤️]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pico CSS – Minimal CSS Framework for Semantic HTML]]></title>
            <link>https://picocss.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45161855</guid>
            <description><![CDATA[Minimalist and lightweight starter kit that prioritizes semantic syntax, making every HTML element responsive and elegant by default.]]></description>
            <content:encoded><![CDATA[A minimalist and lightweight starter kit that prioritizes semantic syntax, making every HTML element responsive and elegant by default.Write HTML, Add Pico CSS, and Voilà!Get StartedExamples14.8KGitHub Stars65.8KMonthly Npm Downloads(Last month)12.6MMonthly JSDelivr Requests(Last month)A Superpowered HTML ResetWith just the right amount of everything, Pico is a great starting point for a clean and lightweight design system.Class-light and SemanticThriving on simplicity, Pico directly styles your HTML tags, using fewer than 10 .classes overall. It also comes with a class-less version for wild HTML purists.Discover the class-less versionGreat Styles with Just CSSNo extra baggage needed. Pico works seamlessly without dependencies, package managers, external files, or JavaScript, achieving elegant and straightforward styles with pure HTML markup.Get startedResponsive EverythingEffortless elegance on every device. Pico natively scales font sizes and spacings with screen widths, resulting in a consistent and elegant look across devices. No extra classes or configuration needed.Discover the responsive font sizesLight or Dark ModePico comes with two accessible, neutral color schemes out of the box: light and dark. The best part? It automatically adapts to users' prefers-color-scheme, all without the use of JavaScript.Learn about the color schemesEasy CustomizationCustomize Pico with over 130 CSS variables, or dive deeper by using SASS. Switch between 20 handcrafted color themes and compose with 30+ modular components to tailor the UI to your brand's look and feel.Learn about the CSS variablesOptimized PerformanceSpeed meets elegance. Unlike bulky and overcomplicated frameworks that demand extensive class overrides and JavaScript, Pico keeps your HTML lean, decreases memory usage by avoiding excessive CSS specificity, and reduces loaded files.Quick startEmbrace the Power of LessA strong design foundation thrives on simplicity and ease of maintenance.😌 Pico CSS<form>
  <input type="text">
  <button type="submit">Action</button>
</form>🥵 Utility CSS Framework<div class="container display-flex my-md mx-sm">
  <form class="form shadow-md my-md mx-sm align-center">
    <div class="input-wrapper border-radius-sm">
      <input type="text" class="input text-color-gray placeholder-color-light-gray focus-outline-blue">
    </div>
    <div class="button-wrapper border-radius-sm">
      <button type="submit" class="button bg-color-blue text-color-white focus-light-blue hover-light-blue">
        Action
      </button>
    </div>
  </form>
</div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Everything from 1991 Radio Shack ad I now do with my phone]]></title>
            <link>https://www.trendingbuffalo.com/life/uncle-steves-buffalo/everything-from-1991-radio-shack-ad-now/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45161816</guid>
            <description><![CDATA[Some people like to spend $3 on a cup of coffee. While that sounds like a gamble I probably wouldn’t take, I’ll always like to gamble– especially as little as three bucks– on what I might be able to dig up on Buffalo and Western New York, our collective past, and what it means for our future.]]></description>
            <content:encoded><![CDATA[

        Some people like to spend $3 on a cup of coffee. While that sounds like a gamble I probably wouldn’t take, I’ll always like to gamble– especially as little as three bucks– on what I might be able to dig up on Buffalo and Western New York, our collective past, and what it means for our future.
I recently came across a big pile of Buffalo News front sections from 1991, every day for the first three months of the year… collected as the First Gulf War unfolded. $3. I probably could have chiseled the guy down a buck, but I happily paid to see what else was in those papers.
There’s plenty about a run up to the first Superbowl appearance ever for the Bills, and mixed in with the disappointment is an air of hope and expectation for what is to come. Harumph. There are also some great local ads commemorating and/or coat-tailing on the Bills success.
We’ll get to those someday, but today, something much simpler. The back page of the front section on Saturday, February 16, 1991 was 4/5ths covered with a Radio Shack ad.
There are 15 electronic gimzo type items on this page, being sold from America’s Technology Store. 13 of the 15 you now always have in your pocket.



So here’s the list of what I’ve replaced with my iPhone.

All weather personal stereo, $11.88. I now use my iPhone with an Otter Box
AM/FM clock radio, $13.88. iPhone.
In-Ear Stereo Phones, $7.88. Came with iPhone.
Microthin calculator, $4.88. Swipe up on iPhone.
Tandy 1000 TL/3, $1599. I actually owned a Tandy 1000, and I used it for games and word processing. I now do most of both of those things on my phone.
VHS Camcorder, $799. iPhone.
Mobile Cellular Telephone, $199. Obvs.
Mobile CB, $49.95. Ad says “You’ll never drive ‘alone’ again!” iPhone.
20-Memory Speed-Dial phone, $29.95.
Deluxe Portable CD Player, $159.95. 80 minutes of music, or 80 hours of music? iPhone.
10-Channel Desktop Scanner, $99.55. I still have a scanner, but I have a scanner app, too. iPhone.
Easiest-to-Use Phone Answerer, $49.95. iPhone voicemail.
Handheld Cassette Tape Recorder, $29.95. I use the Voice Memo app almost daily.
BONUS REPLACEMENT: It’s not an item for sale, but at the bottom of the ad, you’re instructed to ‘check your phone book for the Radio Shack Store nearest you.’  Do you even know how to use a phone book?

You’d have spent $3054.82 in 1991 to buy all the stuff in this ad that you can now do with your phone. That amount is roughly equivalent to about $5100 in 2012 dollars.
The only two items on the page that my phone really can’t replace:

Tiny Dual-Superhet Radar Detector, $79.95. But when is the last time you heard the term “fuzzbuster” anyway?
3-Way speaker with massive 15″ Woofer, $149.95.

It’s nothing new, but it’s a great example of the technology of only two decades ago now replaced by the 3.95 ounce bundle of plastic, glass, and processors in our pockets.
Buffalo story teller  and Historian Steve Cichon brings us along as he explores the nooks and crannies of Buffalo’s past present and future, which can mean  just about anything– twice a week on Trending Buffalo. 
As he collects WNY’s pop culture history, Steve looks for Buffalo’s good stories and creative ways to tell them as the President and founder of Buffalo Stories LLC. He’d love to help your business tell its story. For a decade, he’s also collected and shared Buffalo’s pop culture history at staffannouncer.com.  His latest book, Gimme Jimmy! The James D. Griffin Story, is available now at  www.mayorgriffin.com.

steve@buffalostories.com | @SteveBuffalo | www.facebook.com/stevecichon

        

	
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[No Silver Bullet: Essence and Accidents of Software Engineering (1986) [pdf]]]></title>
            <link>https://www.cs.unc.edu/techreports/86-020.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45161556</guid>
        </item>
        <item>
            <title><![CDATA[How to make metals from Martian dirt]]></title>
            <link>https://www.csiro.au/en/news/All/Articles/2025/August/Metals-out-of-martian-dirt</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45161229</guid>
            <description><![CDATA[Swinburne and CSIRO researchers have successfully made iron under Mars-like conditions, opening the door to off-world metal production.]]></description>
            <content:encoded><![CDATA[
        



      27 August 2025
  3 min read






    Key points
    
        Swinburne and CSIRO researchers are investigating ways to produce native metals using materials found on Mars.
              Martian settlements will require large amounts of metal that are difficult to ship from Earth.
              The team have successfully produced iron using regolith simulant that mimics what is available on the Red Planet.
    
  


The idea of building settlements on Mars is a popular goal of billionaires, space agencies and interplanetary enthusiasts.
But construction demands materials, and we can't ship it all from Earth: it cost US$243 million just to send NASA's one tonne Perseverance Rover to the Red Planet.
Unless we're building a settlement for ants, we'll need much, much more stuff. So how do we get it there?
CSIRO Postdoctoral Fellow and Swinburne alum Dr Deddy Nababan has been pondering this question for years. His answer lies in the Martian dirt, known as regolith.
"Sending metals to Mars from Earth might be feasible, but it's not economical. Can you imagine bringing tonnes of metals to Mars? It's just not practical," Dr Nababan says.
"Instead, we can use what's available on Mars. It's called in-situ resource utilisation, or ISRU."

More specifically, Dr Nababan is looking at astrometallurgy — making metals in space.
Building an off-world foundry
As it turns out, Mars has all the ingredients needed to make native metals. This includes iron-rich oxides in regolith and carbon from its thin atmosphere, which acts as a reducing agent.
Swinburne University of Technology astrometallurgist, Professor Akbar Rhamdhani, is working with Dr Nababan to test this process with regolith simulant - an artificial recreation of the stuff found of Mars.
"We picked a simulant with very similar properties to that found at Gale Crater on Mars and processed them on Earth with simulated Mars conditions. This gives us a good idea of how the process would perform off-world," he says.


      


      
        The researchers used a regolith simulant that mimics the materials found at Gale Crater on Mars.
          
            © 
Swinburne University          
      
  
The simulant is placed inside a chamber at Mars surface pressure and heated at increasing temperatures. The experiments showed pure iron metal formation around 1000°C, with liquid silicon-iron alloys produced around 1400°C.
"At high enough temperatures, all of the metals coalesced into one large droplet. This could then be separated from liquid slag the same way it is on Earth," Professor Rhamdhani says.
Along with Dr Nababan, Prof Rhamdhani is collaborating with CSIRO's Dr Mark Pownceby to further advance the process. They're particularly focused on making metals with zero waste, where the byproducts of the process are used to make useful items.
If you can't ship it, make it
ISRU is a growing area of space science because in rocket launches, every kilogram counts. While the cost of launches is going down, the demands of human exploration are immense.
But huge developments are already happening, including the first demonstration of ISRU off-world. The MOXIE experiment on board the Mars Perseverance rover produced breathable oxygen using only the carbon dioxide in the planet's atmosphere.
Metal production is the next giant leap. Prof Rhamdhani hopes Mars-made alloys could be used as shells for housing or research facilities, and in machinery for excavation.


      


      
        The regolith simulant is placed in a furnace that recreates Martian conditions and heated, producing iron. 
          
            © 
Swinburne University          
      
  
"There are certainly challenges. We need to better understand how these alloys would perform over time, and of course whether this process can be recreated on the real Martian surface," Prof Rhamdhani says.
But in the meantime, Swinburne and its partners are doubling down. Prof Rhamdhani together with Dr Nababan and Dr Matt Shaw, another CSIRO researcher and Swinburne alum, recently delivered a 4-day bespoke workshop on astrometallurgy in South Korea. The feedback was promising.
"We're starting to see increased interest in this field globally as the world gets serious about Mars exploration," he says.
"To make it happen, we're going to need experts from many fields — mining, engineering, geology, and much more."
For Dr Nababan, the benefits go beyond exploration. He hopes their research will also drive more efficient metallurgy here on Earth.
"By doing this, I wish that I can help the development of space exploration, and at the end it will bring good to human life here on Earth."
This was article is published from Swinburne University. Read the original article.




  





      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Electric bill may be paying for big data centers' energy use]]></title>
            <link>https://theconversation.com/how-your-electric-bill-may-be-paying-for-big-data-centers-energy-use-257794</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45161055</guid>
            <description><![CDATA[If state regulators allow utilities to follow the standard approach of splitting the costs of new infrastructure among all consumers, the public will end up paying for data centers’ power.]]></description>
            <content:encoded><![CDATA[
    In the race to develop artificial intelligence, large technology companies such as Google and Meta are trying to secure massive amounts of electricity to power new data centers. Electric utilities see the prospect of earning large profits by providing electricity to these power-hungry facilities and are competing for their business by offering discounts not available to average consumers. 

In our paper Extracting Profits from the Public, we explain how utilities are forcing regular ratepayers to pay for the discounts enjoyed by some of the nation’s largest companies and identify ways policymakers can limit the costs to the public. 

Shifting costs

In much of the U.S., utilities are monopolists. Within their service territories, they are the only companies allowed to deliver electricity to consumers. To fund their operations, utilities split the costs of maintaining and expanding their systems among all ratepayers – homeowners, businesses, warehouses, factories and anyone else who uses electricity.

Historically, a utility expanded its system to meet growing demand for electricity from new factories, businesses and homes. To pay for its expansion − new power plants, new transmission lines and other equipment − the utility would propose to raise electricity rates by different amounts for various types of consumers.

Public utility commissions are state agencies charged with ensuring that the public gets a fair deal. These commissions monitor how much money the utility spends to provide electric service and how its costs are shared among various types of ratepayers, including residential, commercial and industrial consumers. Ultimately, the public utility commission is supposed to approve any rate increases based on its assessment of what’s fair to consumers.

Splitting the utility’s costs among all consumers made perfect sense when population growth and economic development across the economy stimulated the need for new infrastructure. But today, in many utility service territories, most of the projected growth in electricity demand is due to new data centers.

Here’s the problem for consumers: To meet data center demand, utilities are building new power plants and power lines that are needed only because of data center growth. If state regulators allow utilities to follow the standard approach of splitting the costs of new infrastructure among all consumers, the public will end up paying to supply data centers with all that power.


            
            
              An artist’s rendering of a proposed Meta data center in Richland Parish, La.
              Meta via Facebook
            
          

A big price tag

One particularly acute example is in Louisiana. A Meta data center under development in the northeastern corner of the state is projected to use, by our calculations, twice as much energy as the city of New Orleans.

Entergy, the regional monopoly utility, is proposing to build more than US$3 billion worth of new gas-fired power plants and delivery infrastructure to meet the data center’s energy demand. Rather than billing Meta directly for these costs, Entergy is proposing to include the costs in rates paid by all customers.

Entergy claims its contract with Meta will cover some portion of the $3 billion price tag and that will mitigate any increases in consumers’ bills. But Entergy has asked state regulators to keep key terms of the contract secret, and only a redacted version of its application is available online.

The public has no idea how much it might pay if the commission approves the contract. And if the Meta data center ends up using much less power than the company anticipates, the public does not know whether it would be on the hook to pay higher electricity rates for longer periods to guarantee Entergy a profit.


            
            
              The electronics in data centers consume large amounts of electricity.
              RJ Sangosti/MediaNews Group/The Denver Post via Getty Images
            
          

Secret agreements

Our research, reviewing nearly 50 public utility commission proceedings about data centers’ power needs across 10 states, uncovered dozens of secretive contracts between utilities and data centers. Unlike Louisiana, most states require utilities to submit to the public utility commission their one-off deals with data centers, but they allow utilities to conceal the pricing terms from the public.

In normal rate-review cases, numerous parties advocate for their interests in a public proceeding, including members of the public, industry groups and the utility itself. But as our paper finds, utility commission reviews of data center contracts are based on confidential utility filings that are inaccessible to the general public. Few, if any, outsiders participate, and as a result the commission often hears only the utility’s version of the deal.

Because the pricing terms are secret, it is impossible to know whether the deal that a utility is offering to a data center is too low to cover the utility’s costs of providing power to the data center, which would mean that the public is subsidizing the deal. History shows, however, that utilities have a long history of exploiting their monopolies to shift costs to the public, including through secret contracts.


            
            
              Electric utilities also charge customers for the costs of building and maintaining transmission networks.
              Jay L. Clendenin/Getty Images
            
          

Other public costs

Our paper also explores other ways that the public pays for data center energy costs. For instance, many high-voltage interstate transmission projects, which connect large power plants to local delivery systems, are developed through regional planning processes run by numerous utilities. These alliances have complex rules for splitting the costs of new transmission lines and equipment among their utility members. 

Once a utility is charged its share, it spreads the costs of new transmission projects among its local ratepayers. Because some regions are building new transmission capacity to accommodate data centers, our analysis finds that the public has been forced to pay billions of dollars for data center growth.

Data center energy costs can also be shifted when data centers connect directly to existing power plants. Under what are called “co-location” deals, the power plant stops selling energy to the wider public and just sells to the data center. With less supply in the overall market, prices go up and the public faces higher bills as a result.

Many state legislatures are noticing these problems and working to figure out how to address them. Several recent bills would set new terms and conditions for future data center deals that could help protect the public from data center energy costs.
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Submarine Cable Map]]></title>
            <link>https://www.submarinecablemap.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45160780</guid>
        </item>
        <item>
            <title><![CDATA[Keeping secrets out of logs (2024)]]></title>
            <link>https://allan.reyes.sh/posts/keeping-secrets-out-of-logs/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45160774</guid>
            <description><![CDATA[There's no silver bullet, but if we put some "lead" bullets in the right
places, we have a good shot at keeping sensitive data out of logs.]]></description>
            <content:encoded><![CDATA[tl;dr:
There's no silver bullet, but if we put some "lead" bullets in the right
places, we have a good shot at keeping sensitive data out of logs."This is the blog version of a talk I gave at LocoMocoSec 2024. It’s
mostly a lightly edited transcript with some screenshots, so if you’d prefer,
you can watch the
video or just
flip through the slides."This post is about how to keep secrets out of logs, and my claim is that (like
many things in security) there isn’t a singular action or silver bullet that
lets you do this. I would go so far as to say that there’s not even an 80/20
rule, where one action fixes 80% of the problem. It’s not like preventing SQL
injection with prepared statements or preventing buffer overflows by using
memory-safe languages.What I will offer instead, are lead bullets, of which there are many. I’m going
to talk about 10 of them. They are imperfect and sometimes unreliable things
that, if put in the right places and with defense-in-depth, can still give us a
real good chance at succeeding. My hope is that by the end, you’ll
have a slightly better framework for how to reason about this problem and some
new ideas to add to your kit.Table of contents:The ProblemCauses🤦 Direct logging🚰 Kitchen sinks🔧 Configuration changes🥧 Embedded secrets📡 Telemetry🕺🏻 User inputFixes (lead bullets)📐 Data architecture🍞 Data transformations🪨 Domain primitivesCompile-timeRun-timeRun-time: part deux🎁 Read-once objects🔎 Taint checkingAwesomeNot awesome🗃️ Log formatters🧪 Unit tests🕵️ Sensitive data scannersSampling🤖 Log pre-processors🦸 PeopleRecapStrategy0. Lay the foundation1. Understand the data flow2. Protect at chokepoints3. Apply defense-in-depth4. Plan for response and recoveryConclusionThe ProblemWith that, let’s dive in and set the table by talking about the problem with
secrets in logs.So, there are some problems that are annoying. And there are some problems that
are difficult.This is both. I’m gonna level with you: I absolutely hate this problem. But I’m
not going to gaslight you and tell you that this is the most important thing to
work on worry about, because it probably isn’t!You have somewhere between 5 and 50 other problems in your backlog that seem
more important, 1 of which you found out about this morning. But I think it’s
likely that none of those problems are nearly as annoying. While researching
this topic, I interviewed about a dozen other engineers and, on this point, they
unanimously agreed! Nobody likes dealing with secrets in logs because it is
extraordinarily annoying.This is a problem that’s also difficult, but not even in the fun sense, like
being technically complex or interesting. Once you catch sensitive data in logs,
it’s usually pretty straightforward (at least in retrospect) to determine how
they got there. But, it’s also surprisingly elusive to prevent, and it crops up
in incredibly unexpected places and ways.Secrets could mean lots of different things to lots of different teams, but I’ll
use it interchangeably with “sensitive data”: stuff that you want to keep
confidential. What’s so frustrating when breaching confidentiality in logs is
the full spectrum of potential impact.In the best case (left), you might log an isolated, internal credential, like
an API key, which (kudos!) you rotate right after fixing the source of leak.
The impact is minimal, and you just move on. Of course, all the way on the other
end of the spectrum (right), you might log something that an attacker or
inside threat could use to do some real harm.And then somewhere in-between, where I suspect most of the incidents lie. You
might log secrets that you unfortunately, can’t rotate yourself. Things like PII
or your customer’s passwords, which are reused on other sites, because of course
they are. And, depending on your policies, threat model, or regulations, you
might choose to issue a disclosure or notification.And it is painful.You could be doing so many good data security practices, like
secure-by-design frameworks, database and field-level encryption, zero-touch
production, access control… but logging bypasses all of that… and ultimately
degrades trust, in your systems and in your company. It feels unfair because
it’s only a fraction of your security story.And this is a problem that happens to companies of all sizes:Something about “plaintext” just kinda stings, especially as a security
practitioner. It’s like… the most profane insult you can hurl at a security
engineer. Imagine retorting with, “Oh yea? Well, you store your passwords in
plaintext!”But logging passwords and storing them in plaintext are… kinda the same thing.Because while logs are rarely or purposefully public, they’re typically afforded
broader access than direct access to your databases.Everyone knows by now that storing plaintext secrets in your database is a
terrible idea. Logs, however, are still data-at-rest, and we should treat them
with the same level of scrutiny.I cherry picked those examples because they are established companies with very
mature security programs. I’m not trying to throw shade; in fact, I deeply
respect them for being public and transparent about this. I think this also
hints that preventing secrets in logs is a deceptively difficult and frustrating
problem.If we can understand some causes, we might gain a deeper appreciation for these
past occurrences, and stand a better chance at avoiding new incidents in the
future.CausesThis is certainly not comprehensive, but from my interviews and personal
experience, here are six of the most common causes.🤦 Direct logging🚰 Kitchen sinks🔧 Configuration changes🥧 Embedded secrets📡 Telemetry🕺🏻 User input🤦 Direct loggingconst temp = res.cookie["session"];

// TODO: remove after testing is done
Logger.info("session HERE", { temp });
Narrator: it was not removed after testing was doneThe first group is perhaps the most obvious and facepalm one: when sensitive
data is directly logged. Sometimes it’s purely accidental, like the example
above: someone wants to debug session cookies in their local environment and
then… accidentally commits the code. Sometimes it comes from an uninformed
position where the developer just doesn’t know any better.These tend to be fairly easy to trace down the exact line of code or commit that
introduces it. With this example, you can just grep the codebase for session here and you’ll find it instantly.🚰 Kitchen sinksconst client = googleSdk.admin(...);
try {
  const res = client.tokens.list(...);
} catch (e) {
  Logger.error("failed fetch", { e });
}
I’m sure you’ve seen or written code like this before. Here we have an API
client or SDK that is used to fetch some data. Exceptions are caught, kind of,
and then promptly logged so that on-call engineers can debug the errors.What happens?That error is decorated with a config object stuffed with secrets and the full
response object, which is also stuffed with secrets, and now they’re both in
your logs!{
  e: {
    status: 400,
    ...
    config: { 💥☠️🪦 },
    response: { 💣😭😱 },
  }
}
I call these “kitchen sinks,” objects that contain or hold secrets, often in
opaque or unexpected ways. Think of an actual kitchen sink that’s filled to the
brim with dirty dishes and you can’t easily tell what’s at the bottom without
reaching into it. Maybe it’s a spoon, or maybe it’s knife and now you have to
go to the hospital. What tends to happen is that the whole kitchen sink gets
logged, and the logging library happily serializes it, including parts that were
actually sensitive.This seems to happen with code that attaches additional data onto errors, or
code that logs full request and response objects. It’s typically a bit hard to
catch in code review unless you know to look for them. If you are blessed with
static types, seeing an any type flow into logs can be a good hint that you’re
logging too much.🔧 Configuration changesNarrator: it was not okayNext example: someone needs additional observability and changes a setting like
the global log level. You know exactly what happens, here. This dev is about to
have a bad time and find out that hope, in fact, is not a valid strategy.We started with an observability problem. Now we also have security problem: brand
new secrets are getting emitted into logs.In that example (that totally never happened to me ever), developers built
production around log levels set to WARN and above, but once you flip it to
DEBUG, all this new stuff comes out of the woodwork.These type of configuration changes tend to involve a system that was built with
one set of assumptions, but some kind of modification moves that system from a
known state into a unknown state, introducing a new set of problems.These often involve low-level or global utilities like logging config, HTTP
middleware, or some central piece of infra like a load balancer. They tend to be
singletons that are difficult or costly to test, or they crop up only at
runtime. On the positive side, it’s usually loud and quick to patch, but cleanup
can be kinda painful.🥧 Embedded secretsapp.get("/login/:slug", async (req, res) => {
  const magicLink = req.params["slug"];
  await login({ magicLink });
});
I completely made up this phrase, but the idea is that secrets are coupled to,
embedded into, and baked into more general formats like URLs or remote
procedure calls. The central idea is that it’s designed into the format and the
system, and can’t easily be separated.Say you have a magic login link handler (see above) where a user can click a
link and sign into a web app. There’s nothing in that code that logs the link,
but if you look at HTTP logs, it’s right there in plain view:47.29.201.179 - - [17/Jul/2024:13:17:10 +0000] "GET /login/Uj79z1pe01...
These types of leaks arise from fundamental designs that don’t take logging into
consideration or incorrectly assume some end-to-end flow. The sensitivity gets lost out of context, and ends up getting logged in another layer,
system, or service.📡 Telemetrytry:
    db_name = os.getenv("DB_NAME")
    db_pass = os.getenv("DB_PASS") # 🤫 Secret!
    conn = db.connect(db_name, db_pass)
    ...
except Error as e:
    # Don't log e! Not today!!11
    Logger.error("failed to connect")
finally:
    conn.close()
Next example: we have some Python code that’s connecting to a database, we’re
specifically NOT logging the error object, and we want to ensure we always close
out the connection.How can db_pass possibly make it into logs? Telemetry!"Oops, that's a log, too!"It turns out that things like error monitoring and analytics can totally be
logs, too. I kind of cheated in the code example, because there’s no mention of
telemetry in it at all, but it turns out that if you hook it up to error
monitoring like Sentry (above), run-time errors send the
local variable context right to the dashboard, and you can see the database
password in plaintext.These causes tend to bypass the central logging pipeline and become Yet Another
Place to have to worry about secrets.🕺🏻 User inputAlright, last example. Say there’s a sign in form and the entire dev team made
super duper sure that the password field is totally locked down from logging,
they read this super awesome post, and took care of all the causes we discussed.What happens?Users end up jamming passwords into the username field!So if you ever looked at login alerts for AWS and saw usernames replaced with
HIDDEN_DUE_TO_SECURITY_REASONS, this is precisely why!Everything that’s within proximity to sensitive user input tends to be
radioactive. It could be a UI issue, but users are surprisingly determined to
volunteer secrets in ways that you haven’t prepared for.We’ve touched on a half dozen causes, and the list of things goes on. We didn’t
even talk about the wonder that is crashdumps. But, I think it’s important to
zoom out and note that these are proximate causes.I stand by my claim that there’s no silver bullet to take these all out. If we
want to avoid playing whack-a-mole, we must bring out our lead bullets that
address these issues at a deeper level, and prevent these kinds of things from
happening.Fixes (lead bullets)So let’s dive in! We will survey 10 fixes, and the order we’ll go in is
somewhere between “a dependency graph of things that build on each other” and
“following the lifecycle of a secret.” Some of these are obvious or perhaps
things you’re already doing, so I’ll focus more on fixes that I think might be a
bit newer. That said, it is worth starting with the basics.📐 Data architecture🍞 Data transformations🪨 Domain primitives🎁 Read-once objects🗃️ Log formatters🧪 Unit tests🕵️ Sensitive data scanners🤖 Log pre-processors🔎 Taint checking🦸 People📐 Data architectureLead bullet #1 is the most basic and high-level: data architecture and
understanding that this is primarily a data flow problem. And part of the
solution is reducing the number of data flows and shrinking the problem space so
you simply have less things to worry about and protect.Instead of stray print statements or components that write directly to
filesystem, you instead centralize all your data flows through a single stream.
Make it so that there’s one and only one way to log something. If you can
understand and control the data structures that enter that funnel, you can
prohibit secrets from exiting it.This has the allure of being a silver bullet, because of course if you can get
to 100% of all the things we mentioned here, you’re golden! But in practice (and
as we’ve seen previously), that’s difficult because secrets find a way to sneak
in or new outflows and side channels are created.🍞 Data transformationsThe previous bullet was about controlling how data flows through your system,
this is about transforming, slicing, and disarming that data into safer forms
that can be logged. These are the data security fundamentals that you’re already
familiar with and likely already doing. This is your bread and butter, so I’m
not going to dive into every one. From top to bottom, this is generally arranged
from awesome to meh… basically, by how much information is retained.TransformationResultMinimization☁ (nothing)Redaction[redacted]Tokenization2706a40d-3d1d…Hashingdaadfab322b59…EncryptionAzKt7vBE7qEuf…Masking··········5309On the top, we have data minimization. The best way to not log secrets, is to
not have secrets to begin with! This is everything from going passwordless to
fetching only the data you need.Redaction is the next best thing. Blanking out the secret parts and before you pass
objects around in memory.Tokenization, hashing, encryption: these all have their pros, cons, and caveats.
Like… are you even doing it correctly?Dead last is masking. You leave parts of the secret intact. Maybe this works for you.
Maybe it doesn’t. Maybe you go straight to jail 🤷When these techniques work, they generally work well. But very often what
happens is that they aren’t used or are used too late, after something is
already logged. These have their places in our toolbox, but my claim again is
one bullet isn’t enough.🪨 Domain primitivesLet’s introduce lead bullet #3: domain primitives. Almost all the secrets you
run across in codebases are encoded in-memory as string primitives, and I think
that makes our jobs harder. Strings can be almost anything.Strings: any sequence of bytes from "" to "c̴̞̑ť̸͈̘̌ h̸͝ ̭̘̊ü̶̜̫̦̠͋̆͠ ļ̵̮̤̟̉̀͂ṹ̴̝̂🤷867-53-0999"const secret = "..."
There’s very little about them——at compile time or run-time——that lets you
know that it’s sensitive, dangerous to log, or somehow different than any other
vanilla string.The alternative is a concept I learned from the book Secure by
Design, and I think it’s one of
the most powerful concepts you can add to your codebase, for logs or anything
else where you want to layer in security at a fundamental level.Domain primitives: “combines secure constructs and value objects to define the
smallest building block of a domain”const secret = new Secret("...")
You use them as basic building blocks that hold secret values, and they provide
security invariants and guarantees that basic string primitives simply cannot.It’s one of the easiest things you can do. If you shift from “any string can be
a secret” to “secrets are secrets”, it makes things a lot easier to reason about
and protect.Compile-time"Check out a previous blog post on branded types, which are fantastic domain primitives with strong
compile-time guardrails."You can use these to great advantage at compile-time, giving developers
immediate feedback right in their editors.We can type a logging function (log()) so that it never accepts secrets.
Then, we use some fetching function that returns secrets, typed as secrets (and
not as strings). If we try to log that secret, it will not compile. The type
system will not let you log this secret.// Types
declare const brand: unique symbol;
type Secret = string & { [brand]: string }; // Branded type that extends string
type NotSecret<T> = T extends Secret ? never : T; // Type that excludes secrets

// Logging function
function log<T extends string>(message: NotSecret<T>) { ... };
const message: string = "this is fine"; // 🧵 string primitive
const secretz: Secret = getSecret();    // 👈 domain primitive

log(message); // 👌 compiles!
log(secretz); // 💥 error!
See this example in the TypeScript Playground.I’m omitting and glossing over a ton of details here, because I don’t want you
to focus on the implementation or even TypeScript, for that matter. The salient
point here is that instead of tossing secret strings around, you brand them as
secret types, providing useful context to both compiler and developer.Run-timeIt’s really easy to get started, even with code that is functionally a no-op.
This is basically the simplest form I can think of—an almost empty
class:class OpenAIToken extends String { /* that could be it! */ }

const token = new OpenAIToken(...);
It’s supposed to represent OpenAI credentials, but it’s just using and extending
basic language primitives. You can introduce these objects where secrets
originate, like password fields or anytime you decrypt sensitive data fetched
from the database. And then layer in behaviors and invariants for where they
tend to end up. You progressively start introducing these at both sources and
sinks, allowing you to control where secrets should or shouldn’t go. You can
embed these into data structures so you know what contains secrets. And along
the way, you increase the clarity and safety of your codebase: not only can you
prevent these tokens from going into logs, you can make sure you’re sending them
only to OpenAI and not to some other API by accident.I think in the long run, domain primitives are the most powerful control we have
because it makes our code secure by design, but it does take some time to get
there. These can easily address the direct logging cause we discussed earlier,
and with some modifications can help with many more.Run-time: part deuxWe can extend this and make it so that the default serialization behavior is
redaction.class Secret extends String {
    toString() { return "[redacted]" } // Override!
}
const secret = new Secret("shhh!");
console.log(secret);
Secret: "[redacted]"
If you try to stuff this into logs, into JSON, into kitchen sinks, into error
monitoring, wherever, it’ll always spit out the word “redacted”. You have to
intentionally reach for the value.Let’s take it further. We can create a custom class with an explicit unwrap()
function:class Secret<T> {
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" } // Override serialization
    unwrap() { return this.value }     // Explicit getter function
}
There’s so many things you can do here, like maybe you want to encrypt or zero it out
in memory, because that’s in your threat model. You can take this as far as you need
to or are comfortable with. We’ll take it just one step further.🎁 Read-once objectsThere’s a bit to unpack here, but these build off domain primitives in a very
powerful way.class Secret<T> {
    private locked = false;
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" }

    /* @returns the sensitive value (once and only once) */
    unwrap() {
        if (this.locked) { throw new Error("already read") }
        this.locked = true;
        return this.value;
    }
}
These objects wrap and keep the secret safe, until you actually
need it. The code in the unwrap() function is the crux: there’s a latch or
lock that activates after the secret is retrieved the first time. It goes into a
“locked” state, and any following reads result in an error that fails loudly.const secret = getSecret();
const res = await authenticate(secret.unwrap()); // Proper usage

Logger.info(secret);          // [redacted]
Logger.info(secret.unwrap()); // 💥 Error!
Once you get a secret (from user input, database, decryption, etc.) you wrap
it in a read-once object immediately and keep it wrapped for as long as you can.
And for its single, intended purpose, like using it for some kind of API
authentication, you unwrap the value, use it, and then the object stays locked
for good. This is surprisingly effective at preventing and detecting
unintentional use. It addresses and disarms many of the proximate causes that we
discussed earlier.This object pairs extremely well with static analysis. Tools like CodeQL or
Semgrep can help ensure that developers aren’t bypassing any safety guarantees.These are generally high signal, especially when you have good unit test
coverage. One drawback is that read-once objects, if handled incorrectly but not
necessarily unsafely, could cause errors at run-time. But I think the tradeoffs
are usually worth it, especially if you complement it with testing, static
analysis, and taint-checking. Speaking of which…🔎 Taint checkingI like to think of taint checking as static analysis with superpowers. I
absolutely love it and the first time I used it, it was like someone just handed
me a lightsaber. Quick review for the uninitiated: the general idea here is that
you add taint to various sources (like database objects), and yell loudly if the
data flows into certain sinks (like logs).The red data flow trace on the right detects the secret flowing into logs. But
the green path is fine, because the secret is tokenized. Let’s walk through a
quick example:
semgrep.dev/playground/s/4bq5LOn the left, we’ve marked a couple sources like decrypt and a database fetcher.
We’ve also marked our logger as a sink, and the tokenize() function as a
sanitizer.On the right in red, we can see that taint was created from the decrypt
function, propagated through the getSSN() function, and then flagged for
going into the logs on line 18.In blue, there’s a much shorter path where the user model from the database is
tainted and then flagged for going into logs.And then lastly, in green, we’re tokenizing the decrypted SSN, so it’s not
flagging that it’s logged.The idea that this is checking millions or more different data flows is the
real magic part for me.AwesomeSome of the strengths of taint analysis: obviously automation. Tracing these
data flows is 100% a job for a machine. This can really help with domain
primitives but also can be used standalone and can even key in on heuristics
like variable names: for example, all variables containing “password”. You can
tie this into all of your critical tools, from code review to CI/CD.This is especially potent against kitchen sinks and embedded secrets, because
those data structures can be tainted by secret values and checked accordingly.Not awesomeSome personal opinions on drawbacks: I do feel like taint checking rules tend to
be a bit difficult to write. I really, really like Semgrep, but I’m also not the
biggest fan of YAML.It also turns out that data flow analysis is an NP-hard problem so for large
codebases and monorepos, you likely can’t run full taint analysis on every pull
request or commit. Because it runs in CI/CD and as part of change management,
when it works, it can prevent the introduction of insecure logging into the
codebase.But, like all of the lead bullets we’ve discussed and will discuss, they can
miss. How can we handle that?🗃️ Log formattersLet’s say we made the mistake of logging too much data with our email service:{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '<a href="https://acme.com/login/98fPm...">Click here</a> to log in!',
    from: "AcmeCorp <[email protected]>",
    to: "Darth Plagueis (The Wise) <[email protected]>",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: {
        Authorization: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIi..."
      },
      body: '{"html": "<a href=\\"https://acme.com/login/98fP...\\">Click..."}',
      ...
    }
    ....
  },
  ...
}
We have a couple of our usual suspects here. Because we’re logging email
contents, magic links show up in logs… twice! We’re also logging some kitchen
sinks, like email metadata and the original request, so we have PII and
authorization headers also in logs. But because this data is structured, if we
can traverse these objects, it turns out that we can zero in on these leaks
quite effectively.{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '<a href="https://acme.com/login/REDACTED">Click here</a> to log in!',
    from: "AcmeCorp <[email protected]>",
    to: "REDACTED",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: "REDACTED",
      body: '{"html": "<a href=\\"https://acme.com/login/REDACTED\\">..."}',
      ...
    }
    ....
  },
  ...
}
If we can introspect these objects, we can scan for dangerous substrings like our
login links, and then drop or redact them. Or we can drop whole values, if we
know that certain paths like email.to are particularly dangerous. Fields like
request or headers tend to be risky objects that we can also remove. We can
even drop the whole log object if it doesn’t meet some admission criteria,
or—we can simply error out.So, how and where do we deploy something like this? Most application loggers
should have some type of middleware stack or pipeline, kinda like here on the
right. These are typically configured for operations like converting objects
into JSON, turning error objects into readable formats, or enriching logs by
inserting useful context like network information. We can invert that, and
instead of enriching with useful data, we can remove or redact sensitive data.export const logger = createLogger({
  format: format.combine(
    transform(),
    handleErrors(),
    enrich(),

      redact(), // 👈 insert here!

    truncate(),
    jsonify(),
    ...
  ),
  ...
});
This is a type of guardrail that helps catch many of the common problems we
described previously, like request headers or config objects. I’ve used this
with decent success and found that it works best as a rifle instead of a
shotgun. Because it’s at the application tier, you can customize it for the type
of data or context that each application handles. For example, we can make it so
that any of our domain primitives that reach this layer are quickly detected and
removed.This is extremely cheap to introduce, but there are some trade-offs. It’s
certainly more of a safety net than hard control, and a developer determined to
bypass it, can and will. Steady state, I measured this at less than 1% of clock
time, but there are some deeply unfortunate ways this can go wrong such as
poorly written regexes and self-ReDoS.More or less, these risks can be mitigated with solid unit-testing. Which leads
us to…🧪 Unit testsLead bullet #7: hooking into and using the existing test suite—that’s already
there—to our advantage. We can use several of the tools we discussed, but
instead of simply detecting or redacting secrets, we can ramp up the sensitivity
in our test environment to fail or error loudly.TechniqueProdTest🪨 Domain primitivesRedactError🎁 Read-once objectsErrorError🗃️ Log formattersRedactError🕵️ Sensitive data scannersDetectErrorI’ll cover sensitive data scanners next, but many test suites are already set up
to capture stdout and stderr, and so you can even point your scanners to
these capture buffers.The takeaway here is that you can reap the same benefits of CI/CD and change
management by catching unsafe code before it’s merged or deployed, but of
course, you’re also dependent on coverage and if the right code and data paths
are exercised.🕵️ Sensitive data scannersThese are fairly blunt but effective tools that can discover and remove
sensitive data. I’m actively going to avoid diving deep here, because it does
seem like many teams and vendors focus on this as the solution. So instead, I’d
like to pose a few questions that might help you reason about trade-offs:Where and when in your logging pipeline is it most effective?Is it a gate, in-line of the critical path, or does it scan asynchronously?Do you simply want to detect or do you bias towards masking and
redaction? How will your team handle and deal with false positives?How far do the general, out-of-box rules take you? Can you tailor it
specifically to your usage patterns?Can you verify the credentials? Can that even keep up with log throughput?And then perhaps what tends to be the long pole in the tent: what are the
costs, and can you sample instead?I think these tools tend to be better suited for defense-in-depth, because
they presume that secrets made it into logs to begin with. They can help catch the
more elusive causes we discussed like configuration changes or user input.SamplingA very brief segue into sampling. Logs tend to have a kind of power law
distribution, where certain types of logs vastly outnumber others. And typically
what you see is that log sources have static points in code, generally with the
same type of data running through them. And so within each log type, scanning
and finding a single true positive might be highly representative of that group.And so you might run into a scenario where, given some global sample rate,
you’re wasting a lot of work for high frequency logs and not even scanning lower
frequency logs. I think a better alternative to a global sample rate is to
aggregate logs by some heuristic like type or origin, and to ensure you hit some
minimum threshold.In practice, I’ve found this difficult or impossible to configure with
out-of-box solutions. I’ve had to introduce additional infrastructure to help.
And that’s our next lead bullet.🤖 Log pre-processorsSecond to last lead bullet, #9: log pre-processors. These sit between apps that
emit logs, and the final data stores.In the above example, something like Vector can receive
and process logs from our microservices before dispatching them to DataDog or
wherever logs end up. We can configure it to drop sensitive data in-place using
many of the techniques we discussed before. And we can sample some subset of
them and store them onto an S3 bucket, using a more powerful tool like
Trufflehog or an LLM to catch
and verify secrets.The idea here is to process logs streams before they’re persisted. It doesn’t
need to be Vector, chances are, you already have this existing infrastructure
that’s used for deduping, aggregation, and dropping noisy debug logs. We can
re-use it to prevent and detect secrets in logs. This pairs very well with
sensitive data scanners that we discussed earlier, and might even unlock new
ones you thought were out of reach.🦸 People“Human practitioners are the adaptable element of complex systems.
Practitioners and first line management actively adapt the system to maximize
production and minimize accidents.”-Richard Cook, https://how.complexsystems.fail/#12Our last stop is people. Modern software is a complex system. And while people
will write the code that accidentally introduces sensitive data into logs,
they’re also the ones that will report, respond, and fix them. They’ll build out
the systems and infrastructure that will keep these complex systems safe. And
early on in your maturity story and before you’re able to build out
secure-by-design frameworks, this is the lead bullet you’ll most likely use the
most.The most important message I want to convey here is that your security team isn’t
alone, especially if you:educate your teammates on secure logging designempower them to report and address these issuesand equip them with tools that get out of their way and helps them succeed.RecapAlright, so we’ve covered lead bullets that protect code, protect data, and
protect logs:📐 Data architecture🍞 Data transformations🪨 Domain primitives🎁 Read-once objects🗃️ Log formatters🧪 Unit tests🕵️ Sensitive data scanners🤖 Log pre-processors🔎 Taint checking🦸 PeopleSome of these might work for you, some of these won’t, and some that we haven’t
even mentioned could be a homerun for you. Maybe you have super tight control
over your log schemas or maybe you’re using LLMs in a really neat and effective
way. Or maybe you’re building or using a language that has first class support
for controlling secrets.These worked for me. I have some personal opinions on ones which are
foundational, some that are powerful in the long-run, and some that are really
easy to get started. But your story is different, so I’d like to zoom
out and close out with a high-level, methodical strategy that you can apply for
your security programs, and that we’ll apply and walk through with an example.StrategyHere’s a general strategy:Lay the foundationUnderstand the data ﬂowProtect at chokepointsApply defense-in-depthPlan for response and recoveryI’m not shooting for a Nobel, here. You’re probably doing some of these already,
and chances are, you have some type of playbook or process that looks just like
this. The key idea here is to not miss the forest for the trees, and use these
explicit steps to place our efforts where they’ll matter most. I’ll walk you
through a hypothetical system and we’ll apply these in order.0. Lay the foundationStep zero is the foundation. Table stakes. This is like the base tier of
Maslow’s hierarchy, and we need these before we try anything else.Developing expectations, culture, and support is a must-have. They’re easy to
ignore or forget about, but can make or break success. If you work at place that
hasn’t addressed these in the past, it can be quite jarring or difficult to
shift that mentality.I don’t have a ton of advice here other than making sure your org is aligned on
this. It’ll probably feel like it’s getting worse before it’s getting better,
but that is a sign of progress. A great litmus test for a solid foundation is if
developers will (or already have) come to you to report secrets they found in
logs.The second thing we’ll need is to decide is what we consider a secret to begin
with. I, admittedly, used secrets and sensitive data interchangeably. This may
not be the case for you. It doesn’t need to be perfect or comprehensive, and
maybe it’s just a framework. But employees, especially the security team, need
common understanding.The third item is technical. If our logs aren’t structured or aren’t JSON, it’ll
make this endeavor a lot more difficult. A lot of the techniques we discussed
just won’t work. If we don’t have that central pipeline or there isn’t One and
Only One Way to both dispatch and view logs, we’ll have to do a lot more
lifting. We’ve seen a few ways that logs bypass this, but having a central
pipeline should cover most of the bases.1. Understand the data flowWith the foundation laid, the next best thing to do is to understand and chart
out how secrets flow through your system. This is basically a Data Flow Diagram,
and we’ll go through a fairly modest example.On the left, we have users that visit some type of single-page web app. Requests
and data flow through an application load balancer to several web application
services running in containers. This is our core compute and where all the
application code runs. Let’s assume that these are disparate microservices
processing all types of data, some of which are considered secret. For the most
sensitive data, they use KMS to encrypt and then store the ciphertext blobs in
their respective database.And then, applications use a standard logging library to emit to stdout, which
gets shipped to CloudWatch and then forwarded to Datadog. That’s the final stop,
and that’s where employees, devs, support staff, etc. can view them.I highly recommend going through an exercise like this, because not only does it
force you to understand the flows and boundaries of the system, if you spend
time at each node and threat model it, you end up finding a bunch of unexpected
ways and places that secrets make it into logs. For example…Front-end analytics! It turns out that secrets from things like form contents
to session replays could end up getting sent to your user analytics platform.And then what about our application load balancers? These ship their HTTP logs
directly to CloudWatch, so we could be logging embedded secrets in URLs, and
it’s totally bypassing our application tiers.Last surprise: error monitoring! Let’s just say that some team wired up Sentry
instead of DataDog for error monitoring, because of course they did, and now
you have another stream of secrets in logs.We could go further, and we haven’t even drilled into application architecture,
but I think this is a good time to move from discovery to action.2. Protect at chokepointsThe next step we want to take is to protect the chokepoints. And if some flow
isn’t going through that chokepoint, like our rogue team that yeeted Sentry
into prod, we fix it! We can get rid of Sentry and get that team onto the paved
path of our logging pipeline.We have a very clear chokepoint; a narrow path that most logs eventually flow
through. Here’s where most of our lead bullets should go.Here’s that chokepoint splayed out. I also added an upstream node to represent
CI/CD, because that’s how code get into our apps. We can then put the bulk of
our protective controls here on the critical path.We can re-architect the app to use a single logging library and
secure-by-default domain primitives. Then we could use those to build out and
augment our static analysis, taint-checking, and unit tests. These give us a
decent front-line defense for our logging pipeline.3. Apply defense-in-depth“Every preventative control should have a detective control at the same level
and/or one level downstream in the architecture.”
-Phil Venables, https://www.philvenables.com/post/defense-in-depthThe third step is about adding depth to that defense, a concept we’re all
familiar with. I really like how Phil Venables crystallizes what
defense-in-depth means and I think he generally gives great advice. The idea is
that our controls are not simply overlapping, but mutually supportive.
Something’s always got your back.Along this chokepoint we add our downstream components, in depth. Some are
preventative, while some are detective.We can add additional protections like tokenization and read-once objects. We
can add the downstream tools like our custom log formatters, and employ various
sensitive data scanners at different points. And then finally, we can educate
and equip our team.This is what defense-in-depth looks like to me, and I think this maximizes
chances of success.4. Plan for response and recoveryDetermine the scopeRestrict accessStop the bleeding / ﬁx the sourceClean up all the places, e.g. indexesRestore accessDo a post-mortemMake it ~impossible to happen againBut, of course, if we do miss or if we manage to only detect vs. prevent, we
should be prepared for response and recovery. You already know how to respond to
incidents like this, so I won’t add much here, other than making sure you’re
sticking to a playbook in the right order, pulling levers to restrict and
restore access while you’re responding, as well as thinking about all the weird
places secrets might persist in logs, like indexes.ConclusionAnd that’s it. This is the culmination of our strategy, our work, and about 30
some minutes of blabber.With a solid foundation and understanding of our data
flows, we protected our chokepoints in-depth and kept secrets out of logs. We’ve
also introduced a lot of other strong primitives that materially improve our
security program. So is that it? Is the job done?Well, no, because the data team wired up some ETL jobs that are now spewing
secrets into data lake logs, because of course they did.Like most things in security, the job often isn’t ever done. But we have the
understanding, the tools, and a strategy to fight the next fight. Keeping
secrets out of logs is in your hands.*meIf you liked what you heard, or if you hated it, I’d love to hear your story.
Please, reach out! Thanks! ✌️]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The MacBook has a sensor that knows the exact angle of the screen hinge]]></title>
            <link>https://twitter.com/samhenrigold/status/1964428927159382261</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158968</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SQLite's Use of Tcl]]></title>
            <link>https://www.tcl-lang.org/community/tcl2017/assets/talk93/Paper.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158814</guid>
            <description><![CDATA[SQLite is a TCL extension that has escaped into the wild.]]></description>
            <content:encoded><![CDATA[



D. Richard Hipp
24th Annual Tcl/Tk Conference
Houston, TX
2017-10-19


1.0 Introduction


SQLite is a TCL extension that has escaped into the wild.



The design of SQLite was inspired by the design of TCL, both in
the way it handles datatypes and in the formatting of its source
code.
The index use case for SQLite was in a Tcl/Tk application for an
industrial company.
From its inception, SQLite has always depended heavily on TCL.
These days, SQLite no longer uses TCL internally and can be run
separately from any TCL interpreter, and yet the SQLite development
process still depends heavily on TCL.


1.1 About SQLite


SQLite is an SQL database engine, and 
the most widely used database engine in the world.
SQLite is built into all cellphones as a core component and is the
primary means of data persistence on phones.  SQLite is also an
integral part of most web browsers.  SQLite is built into MacOS and
is used by most of the default applications on that platform.
Windows10 requires the C:\Windows\System32\winsqlite3.dll file in
order to boot.  Countless other popular applications like Skype 
and WhatsApp and iTunes depend on SQLite.


Because SQLite is open source and can be freely downloaded and
duplicated, exact usage numbers are unavailable.  But reasonable
estimates are that there are more SQLite instances in operation
today than there are people on earth.  Most devices that
use the SQLite database engine contain hundreds of separate databases,
and there are billions of such devices.  Hence, there are likely
around one trillion SQLite databases in active use.


There are more copies of SQLite in use than there are copies of Linux.
We know this because SQLite is used on almost all Linux systems, but
SQLite is also used on many other non-linux systems such as Macs, iPhones,
and Windows computers.
By similar arguments, there are more copies of SQLite in use than
there are Macs, or Windows PCs.
There are probably more copies of SQLite in use than all other database
engines combined.
It seems likely that SQLite is the most widely used and deployed
software component of any kind, with the possible exception of the
zLib compression library.

1.2 SQLite Is A TCL Extension, Not A TCL Application


SQLite is not written in TCL.  Rather, SQLite is intended to be used
by TCL.  Like the TCL interpreter, SQLite is written in ANSI C.


The fact that SQLite was primarily intended to be used from TCL
is evident in an number of ways.


All programming languages, other than C/C++, require some kind of
adaptor in order to interface with the SQLite C implementation.  Language
adaptors for SQLite are widely available from third party programmers.
The adaptors for PHP and Python are built into those languages, for
example.  A Java adaptor is baked into Android.  And so forth.  Only
the TCL adaptor is included as part of the SQLite core.  The source code
file that implements the TCL adaptor for SQLite, "tclsqlite.c",
was part of the very first check-in on the SQLite source repository
on 2000-05-29.  (See 
https://sqlite.org/src/ci/6f3655f79f9b6fc9.)


All modern SQL implementations provide a means to do late binding of
parameter values to the SQL statements.  Usually a naked "?" character,
or a "?" followed by an integer is used.  For example:

SELECT passwd, photo FROM user WHERE uid=?1



The "?1" token in the SQL above would be assigned a value at run-time
in order to look up the password and photograph of a particular user.


SQLite supports this syntax.  But because of its TCL heritage, SQLite
also allows the parameter to take the form of a TCL variable.  Hence:

SELECT passwd, photo FROM user WHERE uid=$uid



When a statement such as the above is run, the TCL language adaptor
automatically binds the value of the $uid TCL variable to the SQL statement
before it is evaluated, providing an intuitive and seamless interface
between TCL and SQL.  SQLite is the only database engine that behaves
this way.


The TCL heritage of SQLite is visible in the type system of
SQLite.  Early versions of SQLite (prior to 2004) operated on the classic
TCL principal that "everything is a string".  Beginning with SQLite3
(2004-06-18), SQLite also supports binary data.  However, types are still
very flexible in SQLite, just as they are in TCL.  SQLite treats the datatypes
on column names in a CREATE TABLE statement as suggestions rather than
hard requirements.  SQLite is happy to store a 100KB string value in a
column that is declared "SHORT INT", just as TCL is happy to store either
a large string or a small integer in the same variable.  There are some
differences in how SQLite deals with datatypes, in comparison to TCL, due
to the different nature of the SQL language.  SQLite has the concept of
"type affinity".  If a column is declared "INT" and one inserts a string
into that column that looks like an integer and can be safely converted into
an integer without loss of information, then that conversion occurs
automatically.  This provides a measure of compatibility with the rigid
type systems of other SQL database engines.


The flexible type system of SQLite seems natural and intuitive to programmers
with prior experience programming in TCL.  Curiously, though, it is a source
of frustration and frequent complaining from programmers accustomed to the
rigid and unforgiving type systems of languages like Java.


The similarities in the type systems of TCL and SQLite extends to more than
just the interface.  An important part of the C/C++ interface for SQLite
is the "sqlite3_value" object
(https://sqlite.org/c3ref/value.html) which is analogous to the
Tcl_Obj object in TCL.  Both TCL and SQLite use a dual-representation approach,
where each value can be represented simultaneously as both a string and
some other type.

2.0 How SQLite Depends On Tcl


SQLite began as a TCL extension, though these days most uses of SQLite
are in applications written in languages other than TCL.  Many programmers
who use SQLite in their applications have no knowledge or awareness of TCL.
The SQLite source code used by most developers is a single file of pure
C code named "sqlite3.c" that contains no TCL code.  
This is what we mean when we
say that SQLite as "escaped" into the wild.  Deployed instances of SQLite
no longer depends on TCL.


Nevertheless, SQLite is still heavily dependent upon TCL and the ongoing
support, maintenance, and enhancement of SQLite would not be possible without
TCL, and would be seriously inconvenienced without Tk.


2.1 SQLite Source Code Is Mostly TCL


The deliverable source code for SQLite is a single file named
"sqlite3.c" and its companion header "sqlite3.h".
Both files are 100% ANSI-C code.  But developers do not edit these
files directly.  The sqlite3.c and sqlite3.h source files are build
products, and the source tree used to build those files is over 50%
TCL code.  Figure 1 nearby shows the exact ratios.



Figure 1: Breakdown Of SQLite Source Code By Language



Figure 1 is for the main SQLite source repository.  Many of the test
cases and much of the documentation is held in separate repositories,
not included in Figure 1.  The separate repositories also contain a great
deal of TCL code.


Much of the TCL code in the main SQLite repository consists of test scripts.
At this writing, the core repository contains 1153 separate test scripts
totally about 389 KB of space.
But this is not the only use of TCL in SQLite.

2.2 Machine Generated C Code


A non-trivial amount of the deliverable C code for SQLite is machine
generated.  Some of the machine generated code is created by C programs,
such as LEMON which translates the SQL language grammar into C code to
implement a push-down automaton to parse the SQL language.  But much of
the automatically generated code is created using TCL scripts.  TCL is
well suited for scanning source files to extract information to be
merged with other files and for making mechanized edits.  For example,
the byte-code engine used to evaluate SQL statements inside of SQLite
is implemented as a large "switch" statement inside a "for" loop, with a
separate "case" for each opcode, all in the "vdbe.c" source file.
At build-time, TCL scripts scan the vdbe.c source file looking for the
appropriate "case" statements and then build header files that assign
consecutive integers to each symbolic opcode name.  (The opcodes used by
the SQLite byte-code engine are not an API as they are in TCL and thus
can change from one build to the next.)  This mapping of symbolic opcode
names into integers is not a simple as one might suppose.  For reasons
of optimization, there are many constraints on the specific values that
are assigned to opcodes.  For example, many opcodes such as OP_Add must
have the same numeric value as the corresponding "+" token in the SQL
language parser.  Sometimes a group of related opcodes, such as the
comparison operators OP_Eq, OP_Ne, OP_Lt, OP_Le, OP_Ge, and OP_Gt, need
to be assigned consecutive integers in a specific order.  These constraints
are all handled effortlessly in TCL.  Accomplishing the
same with AWK would be rather more difficult.


Perhaps the most important task for TCL during the SQLite build process
is constructing the SQLite amalgamation source code file.
Recall that most developers use SQLite in the form of a single big file
of C code named "sqlite3.c" and referred to as "the amalgamation".
A TCL script named "mksqlite3c.tcl" runs in order to construct the
amalgamation from over one hundred separate input files.  Each of these
inputs files must be added to the amalgamation in just the right order.
Furthermore, the source files are edited as part of the amalgamation building
process.  When mksqlite3c.tcl encounters a "#include" for an SQLite header,
it replaces the "#include" with a copy of that header file, taking care
to make sure each header file is only included once.  The mksqlite3.tcl
script automatically adds the "static" keyword to internal SQLite APIs
to give them file linkage, and makes other similar edits.

2.3 The sqlite3_analyzer.exe Utility


In addition to the core SQLite library, the SQLite source tree also contains
code for several analysis and control programs.  One of these programs
is called "sqlite3_analyzer" (or "sqlite3_analyzer.exe" on Windows).
The sqlite3_analyzer program examines an SQLite database and generates
a detailed report on the disk usage by the various tables and indexes
within that database.  The sqlite3_analyzer program is very useful in
understanding how an application is using disk space.


It turns out that sqlite3_analyzer, though disguised as an ordinary
executable, is really a TCL application.  The main source code file for
this application is
tool/spaceanal.tcl.
During the build process, this script is converted into a C-language
string constant (using another TCL script) and added to a very simple
C-language wrapper than starts a TCL interpreter and then passes the
application script to that interpreter.


The sqlite3_analyzer program could be rewritten in pure C.  But that
would be a lot of code.  The TCL script that implements sqlite3_analyzer is
less than 1000 lines long.  The equivalent C program would surely be at
least ten times larger.


Note that the sqlite3_analyzer utility program statically links a TCL
interpreter and so does not require a TCL installation on the target
computer to use.  The sqlite3_analyzer utility program is used by
tens of thousands of developers, most of whom do not realize that they
are really running a TCL application.

2.4 TCL Used For Testing SQLite


One of the key features of SQLite is that it uses aviation-grade testing.
The tests of SQLite, at a minimum, provide 100% modified condition/decision
coverage (MC/DC) of the SQLite code, with independence.  100% MC/DC roughly
means that every branch instruction at the machine code level is exercised
at least once in each direction.  The precise definition of
MC/DC is slightly stricter than this, for example when comparing boolean
vectors, but the 100% branch tests coverage definition is very close
approximation.  The "with independence" term means that SQLite is tested
in multiple ways with test code being written and maintained by different
individuals.


The amount of testing done on SQLite is fanatical.  On the other hand,
that level of testing is necessary for a fundamental low-level component,
such as a database engine, that is used by billions of devices.  If an
ordinary application encounters an obscure bug, it can normally be rebooted 
to clear the problem.  But the job of a database engine is to remember things,
and so databases tend to remember their mistakes across reboots.  For these
reasons, it is important that SQLite have a very low bug density.


TCL is used in every aspect of SQLite testing.
The test cases that are part of the primary SQLite source code
repository are written in TCL.
Other test cases such as 
TH3 and
SQLLogicTest are written
in C but still depend on TCL for operation.


The TH3 test suite is a set of proprietary tests for SQLite that form the
primary means of achieving 100% MC/DC.  TH3 is designed to
run on embedded hardware without the support of a desktop operating system.
TH3 consists of over 1350 test modules together with over 100 control files.
The test modules are written in either C or SQL or a combination of both.
The control files are text formatted in a way that easily parsed by TCL.
To build a TH3 test, a TCL script is run that combines some subset of the
test modules and control files into a single large C program that will
automatically run the required tests.  This C program is then linked against
the "sqlite3.c" amalgamation and the resulting binary is moved to the target
computer and executed.  TCL scripts automate this entire process on
all major host platforms.


To verify that the TH3 tests really do provide 100% MC/DC, special options
are added to the TCL scripts that run the tests, causing the GCC coverage
analysis tools to be invoked.  The output of gcov is then postprocessed to
reveal and report any branch instructions that were missed by the tests.
The TH3 tests themselves are all implemented in C and SQL, but the operation
and management of those tests is all done with TCL.

2.5 TCL Generated Documentation


The extensive documentation for SQLite available on the SQLite website
(https://sqlite.org/) is all generated
by TCL.  Many of the documents, such as the API reference documentation and
the descriptions of the byte-code engine opcodes, are created by TCL scripts
that scan C source code and extract the necessary information from the code
and comments.  Thus, the API documentation is largely derived from
comments in the source code.  Keeping the official documentation (in
comments) and the source code close together helps ensure that they are
in agreement.


Other whitepaper documents are generated from source files that look 
mostly like
HTML but which contain additional TCL code embedded inside of
<tcl>...</tcl> markup.  The added TCL code is used for
advanced formatting techniques, for automatically creating cross-references
and cross-links, and for constructing complex displays such as the
popular "rail-road" syntax diagrams for SQL.

2.6 Tcl/Tk Used During Development Of SQLite


The text editor used by the primary author of SQLite is a custom editor
with emacs-style key bindings that is built on top of the Tk Text widget.
The "e" editor, as it is called, is cross-platform, which helps in the
development of a cross-platform software library like SQLite.  The "e"
editor has been used for the entire 17-year history of SQLite.  It has
been enhanced over the years with various customizations created especially
to help manage the SQLite source code.


The Fossil version control system used for the SQLite source code
(and written specifically for that purpose) uses Tcl/Tk to show graphical
side-by-side diffs in a separate window.  When the "fossil diff --tk"
command is run, Fossil generates a script to show the diff graphically
and then kicks off a separate "wish" process to run that script and
display the diff in a separate window.  This graphical diff window has
a "Save" button which will cause the Tcl/Tk code needed to reproduce itself
to be written to a file.  This file can be, in turn, sent to a collaborator
for display.  Passing around graphical diffs as ordinary text files is
much simpler and easier than passing around JPEG images or text "context"
diffs.


No two SQLite developers work in the same office.  The team is
geographically distributed.  To help the team stay in touch, a custom
chatroom has been created using a Tcl/Tk script.  The same script works
as both client and server.  The chatroom is private and uses a proprietary
protocol, so that developers are free to discuss sensitive matters without
fear of eavesdropping.  The chatroom is implemented as just over
1000 lines of Tk code, and is thus accessible and easy to customize.
Among the customizations is the ability to send saved "fossil diff --tk"
graphical diffs to collaborators and have the diff pop automatically on
the collaborators screen.  Small features like this seem trivial
by themselves, but together than help the developers
to work much more efficiently.  These kinds of productivity-enhancing
features are unavailable to users of commercial business collaboration 
packages such as HipChat.

3.0 Summary


SQLite is an indispensable element of most modern computer systems, and
TCL is an indispensable tool used in the production and maintenance of
SQLite.  Thus, the computing infrastructure we enjoy today would not exist
except for TCL.


As deployed, SQLite contains no TCL code.
However, the design of SQLite is inspired by TCL.
And TCL is used extensively in the code generation, testing,
analysis, documentation, and development of SQLite.  Without
TCL, SQLite would not exist.


Every developer and every team has a finite number of "brain cycles"
available to do their job.  The fewer cycles spent messing with tools,
the more cycles are available to devote towards solving the problem.
So for maximum productivity, it is important to use tools that get
the job done with a minimum of fuss and bother.
Our 17-year experience using TCL in the SQLite project has convinced
us that TCL is just such a tool.  Tcl provides the most help per
brain cycle of any similar technology.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Belling the Cat]]></title>
            <link>https://en.wikipedia.org/wiki/Belling_the_Cat</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157906</guid>
            <description><![CDATA[From Wikipedia, the free encyclopedia]]></description>
            <content:encoded><![CDATA[
							

						From Wikipedia, the free encyclopedia
					

Gustave Doré's illustration of La Fontaine's fable, c. 1868
Belling the Cat is a fable also known under the titles The Bell and the Cat and The Mice in Council. In the story, a group of mice agree to attach a bell to a cat's neck to warn of its approach in the future, but they fail to find a volunteer to perform the job. The term has become an idiom describing a group of persons, each agreeing to perform an impossibly difficult task under the misapprehension that someone else will be chosen to run the risks and endure the hardship of actual accomplishment.[1]
Although often attributed to Aesop, it was not recorded before the Middle Ages and has been confused with the quite different fable of Classical origin titled The Cat and the Mice. In the classificatory system established for the fables by Ben Edwin Perry, it is numbered 613, which is reserved for Mediaeval attributions outside the Aesopic canon.[2]


Synopsis and idiomatic use[edit]
The fable concerns a group of mice who debate plans to nullify the threat of a marauding cat. One of them proposes placing a bell around its neck, so that they are warned of its approach. The plan is applauded by the others, until one mouse asks who will volunteer to place the bell on the cat. All of them make excuses. The story is used to teach the wisdom of evaluating a plan on not only how desirable the outcome would be but also how it can be executed. It provides a moral lesson about the fundamental difference between ideas and their feasibility, and how this affects the value of a given plan.[3]
The fable gives rise to the idiom to bell the cat, which means to attempt, or agree to perform, an impossibly difficult task.[4] Historically 'Bell the Cat' is frequently claimed to have been a nickname given to fifteenth-century Scottish nobleman Archibald Douglas, 5th Earl of Angus in recognition of his part in the arrest and execution of James III's alleged favourite, Thomas (often misnamed as Robert) Cochrane. In fact the earliest evidence for this use is from Hume of Godscroft's history of the Douglases published in 1644,[5] and therefore is more reflective of perception of the idiom in the seventeenth century than the fifteenth.[6] In the 21st century the idiom was adopted by the investigative journalism group Bellingcat.[7] 
The first English collection to attribute the fable to Aesop was John Ogilby's of 1687; in this there is a woodcut (by Francis Barlow), followed by a 10-line verse synopsis by Aphra Behn with the punning conclusion:


Good Councell's easily given, but the effect
Oft renders it uneasy to transact.[8]


Early versions and later interpretations[edit]
"Belling the cat" is one of the proverbs illustrated in Pieter Bruegel I's painting Netherlandish Proverbs (1559).
One of the earliest versions of the story appears as a parable critical of the clergy in Odo of Cheriton's Parabolae.[9] Written around 1200, it was afterwards translated into Welsh, French and Spanish. Sometime later, the story is found in the work now referred to as Ysopet-Avionnet, which is largely made up of Latin poems by the 12th century Walter of England, followed by a French version dating from as much as two centuries later. It also includes four poems not found in Walter's Esopus; among them is the tale of "The Council of the Mice" (De muribus consilium facientibus contra catum). The author concludes with the scornful comment that laws are of no effect without the means of adequately enforcing them and that such parliamentary assemblies as he describes are like the proverbial mountain in labour that gives birth to a mouse.[10]
The fable also appeared as a cautionary tale in Nicholas Bozon's Anglo-Norman Contes Moralisés (1320), referring to the difficulty of curbing the outrages of superior lords.[11] It was in this context too that the story of a parliament of rats and mice was retold in William Langland's allegorical poem Piers Plowman.[12] The episode is said to refer to the Parliament of 1376 which attempted unsuccessfully to remedy popular dissatisfaction over the exactions made by nobles acting in the royal name.[13]
Langland's French contemporary, the satirical Eustache Deschamps, also includes the story among his other moral ballades based on fables as "Les souris et les chats".[14] It has been suggested that in this case too there is a political subtext. The poem was written as a response to the aborted invasion of England in 1386 and contrasts French dithering in the face of English aggression.[15]  The refrain of Deschamps' ballade, Qui pendra la sonnette au chat (who will bell the cat) was to become proverbial in France if, indeed, it does not record one already existing.
In the following century, the Italian author Laurentius Abstemius made of the fable a Latin cautionary tale titled De muribus tintinnabulum feli appendere volentibus (The mice who wanted to bell the cat)[16] in 1499. A more popular version in Latin verse was written by Gabriele Faerno and printed posthumously in his Fabulae centum ex antiquis auctoribus delectae (100 delightful fables from ancient authors, Rome 1564), a work that was to be many times reprinted and translated up to start of the 19th century. Titled simply "The Council of the Mice", it comes to rest on the drily stated moral that 'a risky plan can have no good result'. The story was evidently known in Flanders too, since 'belling the cat' was included among the forty Netherlandish Proverbs in the composite painting of Pieter Bruegel the Elder (1559). In this case a man in armour is performing the task in the lower left foreground.[17] A century later, La Fontaine's Fables made the tale even better known under the title Conseil tenu par les rats (II.2).[18]

A Japanese woodblock illustration by Kawanabe Kyōsai of La Fontaine's fable, 1894.
In mediaeval times the fable was applied to political situations and British commentaries on it were sharply critical of the limited democratic processes of the day and their ability to resolve social conflict when class interests were at stake. This applies equally to the plot against the king's favourite in 15th century Scotland and the direct means that Archibald Douglas chose to resolve the issue. While none of the authors who used the fable actually incited revolution, the 1376 Parliament that Langland satirised was followed by Wat Tyler's revolt five years later, while Archibald Douglas went on to lead a rebellion against King James. During the Renaissance the fangs of the fable were being drawn by European authors, who restricted their criticism to pusillanimous conduct in the face of rashly proposed solutions. A later exception was the Russian fabulist Ivan Krylov, whose adaptation of the story satirises croneyism. In his account only those with perfect tails are to be allowed into the assembly; nevertheless, a tailless rat is admitted because of a family connection with one of the lawmakers.[19]
There still remains the perception of a fundamental opposition between consensus and individualism. This is addressed in the lyrics of "Bell the Cat",[20] a performance put out on DVD by the Japanese rock band LM.C in 2007.[21] This is the monologue of a house cat that wants to walk alone since "Society is by nature evil". It therefore refuses to conform and is impatient of restriction: "your hands hold on to everything – bell the cat". While the lyric is sung in Japanese, the final phrase is in English. Another modernised adaptation based on this fable, that updates the moral, has been published by Patricia McKissack in her Who Will Bell the Cat? (illustrated by Christopher Cyr).[22][23]
There is a Tibetan proverb that is very similar, "Putting a bell on the cat's neck after the mother of mice was consulted"[24]


Several French artists depicted the fable during the 19th century, generally choosing one of two approaches. Gustave Doré and the genre painter Aurélie Léontine Malbet (fl. 1868–1906)[25] pictured the rats realistically acting out their debate. The illustrator Grandville,[26] along with the contemporaries Philibert Léon Couturier [fr] (1823–1901)[27] and Auguste Delierre (1829–1890),[28] caricature the backward practice and pomposity of provincial legislatures, making much the same point as did the Mediaeval authors who first recorded the tale. At the end of the century a publishing curiosity reverts to the first approach. This was in the woodblock print by Kawanabe Kyōsui that appeared in the collection of La Fontaine's fables that was commissioned and printed in Tokyo in 1894 and then exported to France.[29] In the upper left-hand corner a cat is seen through a warehouse window as it approaches across the roofs while inside the rats swarm up the straw-wrapped bales of goods. At its summit the chief rat holds the bell aloft. An earlier Japanese woodblock formed part of Kawanabe Kyōsai's Isoho Monogotari series (1870–80). This shows an assembly of mice in Japanese dress with the proposer in the foreground, brandishing the belled collar.[30]


In the 18th century the fable was one among many set by Louis-Nicolas Clérambault in the fables section of Nouvelles poésies spirituelles et morales sur les plus beaux airs (1730–37).[31] In the following century the text of La Fontaine's fable was set for male voices by Louis Lacombe[32] and by the Catalan composer Isaac Albéniz for medium voice and piano in 1889.[33] In 1950 it was set for four male voices by Florent Schmitt.[34] But while La Fontaine's humorously named cat Rodilardus, and antiquated words like discomfiture (déconfiture), may fit an art song, there have also been faithful interpretations in the field of light music. A popular composer of the day, Prosper Massé, published such a setting in 1846.[35] More recently there has been Pierre Perret's interpretation as part of his 20 Fables inspirées de Jean de la Fontaine (1995),[36] and a jazz arrangement on Daniel Roca's 10 Fables de La Fontaine (2005).[37]


Collective action problem
Who Will Bell the Cat?, a children's picture book based on the fable



^ Strouf, Judie L. H. (2005). The literature teacher's book of lists. Jossey-Bass. p. 13. ISBN 0787975508.

^ Ben Edwin Perry (1965). Babrius and Phaedrus. Loeb Classical Library. Cambridge, MA: Harvard University Press. pp. 545, no. 613. ISBN 0-674-99480-9.

^ "Belling The Cat". Fables of Aesop. 2016-07-05. Retrieved 2021-03-04.

^ "To Bell the Cat" thefreedictionary.com. Retrieved 9 November 2007.

^ David Reid, David Hume of Godscroft's History of the House of Angus, vol. 1 (STS: Edinburgh, 2005), p. 26.

^ Macdougall, Norman (1982). James III: A Political Study. Edinburgh: John Donald. pp. 287–288. ISBN 0859760782.

^ "Bellingcat: Digital Sleuths on the Hunt for Truth"

^ "21. De cato et muribus (1687), illustrated by Francis Barlow". Mythfolklore.net. Retrieved January 26, 2011.

^ Laura (15 May 2009). "Christianizing Aesop: The Fables of Odo of Cheriton". Journey to the Sea. Retrieved 26 January 2011.

^ Ysopet-Avionnet, the Latin and French texts, University of Illinois 1919; fable LXII, pp. 190–192; this is archived online

^ Les contes moralisés de Nicole BozonParis, 1889, pp. 144–145; archived here

^ William's Vision of Piers Plowman by William Langland, edited by Ben Byram-Wigfield (2006), Prologue, lines 146–181; online text here Archived 2011-08-07 at the Wayback Machine

^ "The Parliament of the Rats and Mice". Medieval Forum. SFSU. Archived from the original on 10 March 2022. Retrieved 26 January 2011.

^ Poésies morales et historiques d'Eustache Deschamps, Paris 1832, pp. 188–189

^ Robert Landru, Eustache Deschamps, Fédération des sociétés d'histoire et d'archéologie de l'Aisne, vol. XV 1969, p. 126

^ Fable 195

^ View on Wikimedia Commons

^ "Elizur Wright's translation". Oaks.nvg.org. Retrieved 26 January 2011.

^ Kriloff's Fables, translated by C. Fillingham Coxwell, London 1920, pp. 38–39; archived online

^ "Lyrics | LM.C – Bell The Cat (English)". SongMeanings. 25 April 2010. Retrieved 26 January 2011.

^ "Bell the CAT/LM.C". YouTube. 18 November 2007. Archived from the original on 2021-12-12. Retrieved 26 January 2011.

^ Who will bell the cat?. OCLC 1037155724.

^ "Who Will Bell the Cat?". Publishers Weekly. PWxyz LLC. February 19, 2018. Retrieved April 6, 2022.

^ p. 135, Tsewang, Pema. 2012. Like a Yeti Catching Marmots. Boston: Wisdom Publications.

^ Exhibited at the 1888 Salon; photo online

^ "See online". Archived from the original on July 20, 2011. Retrieved 17 August 2012.

^ "In the Musée Denon de Chalon-sur-Saône". Philibert-leon-couturier.com. Retrieved 17 August 2012.

^ "In the Musée La Fontaine at Château Thierry". Retrieved 17 August 2012.

^ George Baxley. "baxleystamps.com". baxleystamps.com. Retrieved 17 August 2012.

^ View online Archived 2012-03-25 at the Wayback Machine

^ The score is printed in: John Metz, The Fables of La Fontaine: A Critical Edition of the Eighteenth-Century, Pendragon Press 1986, p. 45

^  Op. 85, 1879, Score at Gallica

^ Liedernet

^ Op. 123, Liedernet

^ Bibliographie de la France, 14 March 1846, 127

^ "Pierre Perret chante 20 fables inspirées de Jean de La Fontaine Perret, Pierre, 1934–..." bibliotheques.avignon.fr.

^ Track available on Jamendo








]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Algebraic Effects in Practice with Flix]]></title>
            <link>https://www.relax.software/blog/flix-effects-intro/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157466</guid>
            <description><![CDATA[Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here's why you'd want to do that, in order of importance.]]></description>
            <content:encoded><![CDATA[ Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here’s why you’d want to do that, in order of importance:


Effects make your code testable
One of the central goals of enterprise software development. Dependency injection, mocking, architecture patterns like clean, hexagonal, DDD are all meant to tackle this. Effects solve this elegantly by separating the “what” from the “how”.


Effects give immediate visibility into what your own and 3rd-party code is doing
Supply chain attacks are real. And they will get worse with more AI slop entering our codebases. Tools like Go’s Capslock fix this by following the whole chain of calls to stdlib functions. Effects provide this by design, as all effects are tracked by the type and effect system.


Effects enable user-defined control flow abstractions
Solving the “what color is your function” problem1. You can also leverage effects to implement Async/await, coroutines, backtracking search and other control flow patterns as user libraries without hard-coding these features into the language.


Algebraic effects come from the pure functional world, serving a purpose similar to monads — keeping track of and having control over side effects. Like monads, they enable us to write our core logic with pure functions and push side effects like IO outwards, closer to application boundaries.
Unlike monads, effects are easy to grasp for a regular developer and give immediate benefits when starting out. For me personally they’re a more natural abstraction for managing side effects — after all, effects are in the name.
Starting out as an academic concept, algebraic effects were introduced to the world by research languages like Eff, Koka, Effekt, Frank, Links, and more recently Ante.
People have also applied effects in practice, so far usually via a monad-based approach, by making libraries in established languages like Scala Kyo / Cats Effect / ZIO; Typescript Effect and Effector, C# language-ext, C libhandler and libmprompt, C++ cpp-effects, various Haskell libraries, etc.
In addition to forcing you into a monadic way of thinking, libraries implementing effects are limited by their host languages.
In this article, I will walk you through applying algebraic effects on a real world example using Flix, a new programming language that is built with effects from the ground up, and supports functional, logic and imperative paradigms.

Table of Contents

Type and Effect System: A Motivating Example
Effect Handlers: Building Intuition
Real-World App: AI movie recommendations
Where to Go From Here
Extra: Why Algebraic Effects are Algebraic and how they relate to monads
Footnotes


Currently only few languages support effects out of the box. The only one that I know of besides Flix is Unison. OCaml has a language extension, but there is no support yet in the type system. Haskell has added support for delimited continuations, but effects are still only available via libraries.
In addition to having a “type and effect system” that improves function signatures and makes sure all effects are handled, Flix supports traits, local mutability via regions, working with immutable or mutable data, and Go/Rust-like structured concurrency. It also has a first-class Datalog integration. But I will only focus on effects here. Let’s start.
Type and Effect System: A Motivating Example 🔗
Imagine a function called calculateSalary:
def calculateSalary(base_salary, bonus_percent):

Based on the function name and the signature, one can assume it’s just a pure function that does some calculations. In a statically typed language you are also guaranteed that the function arguments and outputs will be of a certain type.
But even if the types are correct, nothing stops our little calculateSalary() from, say, sending an offensive email to your grandma2:
def calculateSalary(base_salary, bonus_percent):
    server.sendmail("grandma@family.com", "Your cookies are terrible!")
    return base_salary * (1 + bonus_percent/100)

If, on the other hand, you extend your type system with effects, you will see immediately in the signature that this function may do something fishy:
def calculateSalary(salary: Float64, percent: Float64): 
    Float64 \ {Email} = {
//            ^^^^^^^ Notice the Email effect!

Of course, in real life the issue it’s not usually about the grandma. Instead, this function could throw an exception — still quite dangerous. If you forget to handle the exception, your app will crash. Or another very realistic scenario is that calculateSalary() calls a database to get some employee details for calculations, and you forgot to provide a database connection string. That can also result in an exception or a panic.
Effect Handlers: Building Intuition 🔗
The job of the type and effect system is not just to improve our function signatures. It’s also making sure all the effects are handled somewhere. This is where effect handlers come in.
Usually when people talk about algebraic effects what they’re actually talking about is effect handlers. If you know exceptions, effect handlers are super easy to understand. Here’s a Jenga analogy:
Imagine the call stack is a Jenga tower. New blocks are carefully added each time you call a function.
Saurav S, Unsplash
When an exception is thrown, your whole nice Jenga tower gets destroyed, all the way up to the catch() block. The catch block can safely handle the error, but the stack is unwinded, meaning you lose all of the state you had in your program before throwing the exception. You have to build your tower again, from scratch.
When using effect handlers you can actually go back to your original computation after the handler is done handling the effect. The handler can also return some values back to your program, and it can even resume multiple times with different return values. You also still have the option of not resuming at all and aborting the program — that would be the effect equivalent of exceptions.
Back to the Jenga analogy: if your tower is about to fall down, with effects you can freeze it mid-collapse. You then call someone for help (handler), and they decide whether to let the tower fall, magically restore it to the previous statlte. Or even hand you different blocks to try the same move (call the continuation) again, possibly multiple times with different inputs. Your Jenga tower ends up looking more like a fork or a tree, with multiple different copies of your blocks branching out at some point from the base.
To make this more concrete, let’s start by reproducing exceptions with effects. Here’s how a try/catch looks like in Python:
def divide(x, y):
    try:
        return x / y
    except ZeroDivisionError:
        print("Division by zero!")
        return None

Here’s the equivalent code in Flix. We first define an Exception effect and a divide() function:
eff Exception {
    def throw(msg: String): Void
}

def divide(x: Int32, y: Int32): Int32 \ Exception = 
    if (y == 0) {
        Exception.throw("Division by zero!")
    } else {
        x / y
    }

And then provide a handler for this effect somewhere, preferably close to main():
def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler Exception {
        def throw(msg, _resume) = println("Error: ${msg}")
    }

What this does is registers an effect called Exception with a method throw(). We then perform this effect in our function when there’s an error, similar to throwing an exception in the Python version. Control is transferred to the effect handler, which then decides how to handle the exception, similar to a catch() block in Python.
Notice we never call resume() from the handler. This results in the program being aborted, just like with exceptions. Graphically, this can be represented as follows:
block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["Statement 3"] space:1
    D["Perform Effect"] space:1
    space:1 E["Handle Effect"]
    space:1 F["Process & Exit"]
    space:1 space:1
    
    D --> E
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F fill:#ccffcc,color:#000
So far so good, but this is not much different from Python. To really take full advantage of effect handlers, we can use resume() to return to the original computation and proceed from the line after the effect was performed:
eff ResumableException {
    def askForInput(): Int32
}

def divide(x: Int32, y: Int32): Int32 \ ResumableException = 
    if (y == 0) {
        let newY = ResumableException.askForInput();
        x / newY
    } else {
        x / y
    }

def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler ResumableException {
        def askForInput(_, resume) = {
            println("Enter a new divisor:");
            resume(5) // Or get from user input
        }
    }

block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["Statement 3"] space:1
    D["Perform Effect"] space:1
    space:1 E["Handle Effect"]
    space:1 F["Resume"]
    space:1 space:1
    G["Statement 4"] space:1
    H["Statement 5"] space:1
    I["Complete"] space:1
    
    D --> E
    F --> G
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F fill:#ffffcc,color:#000
I called the effect ResumableException here, but it’s not really an exception anymore, because the program continues normally.
At this point we can use this power bestowed on us by effects and handlers to roll our own Async/await:
eff Async {
    def await(url: String): String
}

def fetchData(): String \ Async = 
    Async.await("https://api.example.com/data")

def processData(): String \ Async = {
    let data = fetchData();
    "processed: ${data}"
}

def main(): Unit \ IO = 
    run {
        let result = processData();
        println(result)
    } with handler Async {
        def await(url, resume) = {
            // Simulate async HTTP request
            let result = "data from ${url}";
            resume(result)
        }
    }

See how easy that was? This approach also avoids function coloring, since we didn’t need to use special keywords anywhere. Here’s a graphic version:
block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["await operation"] space:1
    space:1 H1["Start async work"]
    space:1 H2["⏳ Long pause..."]
    space:1 H3["⏳ Still waiting..."]
    space:1 H4["✅ Async complete"]
    space:1 F["Resume with result"]
    space:1 space:1
    D["Statement 3"] space:1
    E["Complete"] space:1
    
    C --> H1
    F --> D
    
    style C fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#fff3cd,color:#000
    style H3 fill:#fff3cd,color:#000
    style H4 fill:#d1ecf1,color:#000
    style F fill:#ffffcc,color:#000
    style D fill:#e7f3ff,color:#000
    style E fill:#d4edda,color:#000
That’s cool, but we can do more. Effect handlers allow you to resume multiple times:
eff Choose {
    def choose(): Int32
}

def explore(): String \ Choose = {
    let x = Choose.choose();
    let y = Choose.choose();
    "${x}, ${y}"
}

def main(): Unit \ IO = 
    run {
        println(explore())
    } with handler Choose {
        def choose(_, resume) = {
            resume(1);
            resume(2);
            resume(3)
        }

block-beta
    columns 4
    
    A["Statement 1"] space:1 space:1 space:1
    B["Statement 2"] space:1 space:1 space:1
    C["Statement 3"] space:1 space:1 space:1
    D["Perform Effect"] space:1 space:1 space:1
    space:1 space:1 E["Handle Effect"] space:1
    space:1 F1["Resume 1"] F2["Resume 2"] F3["Resume 3"]
    space:1 G1["Statement 4a"] G2["Statement 4b"] G3["Statement 4c"]
    space:1 H1["Statement 5a"] H2["Statement 5b"] H3["Statement 5c"]
    space:1 R1["Resume to Main"] R2["Resume to Main"] R3["Resume to Main"]
    J["Statement 6"] space:1 space:1 space:1
    K["Complete"] space:1 space:1 space:1
    
    D --> E
    F1 --> G1
    F2 --> G2
    F3 --> G3
    H1 --> R1
    H2 --> R2
    H3 --> R3
    R1 --> J
    R2 --> J
    R3 --> J
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F1 fill:#ffffcc,color:#000
    style F2 fill:#ffffcc,color:#000
    style F3 fill:#ffffcc,color:#000
    style G1 fill:#e6f3ff,color:#000
    style G2 fill:#ffe6f3,color:#000
    style G3 fill:#f3ffe6,color:#000
    style H1 fill:#e6f3ff,color:#000
    style H2 fill:#ffe6f3,color:#000
    style H3 fill:#f3ffe6,color:#000
    style R1 fill:#d4edda,color:#000
    style R2 fill:#d4edda,color:#000
    style R3 fill:#d4edda,color:#000
    style J fill:#cce5ff,color:#000
    style K fill:#b3d9ff,color:#000
With this, you can implement things like coroutines:
block-beta
    columns 3
    
    A1["Coroutine 1: Start"] space:1 A2["Coroutine 2: Start"]
    B1["Statement 1"] space:1 B2["Statement 1"]
    C1["yield to Co2"] H1["Scheduler"] space:1
    space:1 space:1 C2["Statement 2"]
    space:1 space:1 D2["yield to Co1"]
    space:1 H2["Scheduler"] space:1
    D1["Statement 2"] space:1 space:1
    E1["yield to Co2"] H3["Scheduler"] space:1
    space:1 space:1 E2["Statement 3"]
    space:1 space:1 F2["Complete"]
    F1["Complete"] space:1 space:1
    
    C1 --> H1
    H1 --> C2
    D2 --> H2
    H2 --> D1
    E1 --> H3
    H3 --> E2
    
    style C1 fill:#ffcccc,color:#000
    style D2 fill:#ffcccc,color:#000
    style E1 fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#ccffcc,color:#000
    style H3 fill:#ccffcc,color:#000
    style A1 fill:#e6f3ff,color:#000
    style B1 fill:#e6f3ff,color:#000
    style D1 fill:#e6f3ff,color:#000
    style F1 fill:#e6f3ff,color:#000
    style A2 fill:#ffe6f3,color:#000
    style B2 fill:#ffe6f3,color:#000
    style C2 fill:#ffe6f3,color:#000
    style E2 fill:#ffe6f3,color:#000
    style F2 fill:#ffe6f3,color:#000
Generators:
block-beta
    columns 2
    
    A["Start generator"] space:1
    B["Statement 1"] space:1
    C["yield value 1"] H1["Return value"]
    space:1 H2["⏸️ Paused"]
    D["next() called"] H3["Resume generator"]
    E["Statement 2"] space:1
    F["yield value 2"] H4["Return value"]
    space:1 H5["⏸️ Paused"]
    G["next() called"] H6["Resume generator"]
    H["Statement 3"] space:1
    I["return (done)"] H7["Signal complete"]
    
    C --> H1
    H3 --> D
    F --> H4
    H6 --> G
    I --> H7
    
    style C fill:#ffcccc,color:#000
    style F fill:#ffcccc,color:#000
    style I fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H3 fill:#ffffcc,color:#000
    style H4 fill:#ccffcc,color:#000
    style H6 fill:#ffffcc,color:#000
    style H7 fill:#ccffcc,color:#000
    style H2 fill:#fff3cd,color:#000
    style H5 fill:#fff3cd,color:#000
    style D fill:#e7f3ff,color:#000
    style G fill:#e7f3ff,color:#000
And backtracking search:
block-beta
    columns 4
    
    A["Start search"] space:1 space:1 space:1
    B["choose option"] space:1 space:1 space:1
    space:1 H1["Try option 1"] space:1 space:1
    space:1 space:1 C1["Explore path 1"] space:1
    space:1 space:1 D1["❌ Dead end"] space:1
    space:1 H2["Backtrack"] space:1 space:1
    space:1 H3["Try option 2"] space:1 space:1
    space:1 space:1 space:1 C2["Explore path 2"]
    space:1 space:1 space:1 D2["✅ Success!"]
    E["Resume with solution"] space:1 space:1 space:1
    F["Complete"] space:1 space:1 space:1
    
    B --> H1
    H1 --> C1
    D1 --> H2
    H2 --> H3
    H3 --> C2
    D2 --> E
    
    style B fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#f8d7da,color:#000
    style H3 fill:#ccffcc,color:#000
    style C1 fill:#fff3cd,color:#000
    style D1 fill:#f8d7da,color:#000
    style C2 fill:#d1ecf1,color:#000
    style D2 fill:#d4edda,color:#000
    style E fill:#ffffcc,color:#000
    style F fill:#d4edda,color:#000
Hopefully this gives you a taste of how effect handlers work. This is just a sketch though — you can read more on this and see examples in the Flix docs.
 Question What's your primary programming language?   TypeScript/JavaScript   Python   Java or other JVM e.g Scala/Kotlin   C#   C++/C   PHP   Go   Rust   Other    ✓ Thanks for your input!
    
These questions help direct new content. Want to get notified when something new is posted? ✓ Saved!
  
Defining our own control flow abstractions is great, but most of the time regular async/await and/or coroutines are enough for the job.
What is extremely useful for daily programming is that effects let you separate the declaration of the effect (the operation, or the effect “constructor”) from it’s implementation, defined by the effect handler.
Add some effect definitions:
eff Database {
    def getUser(id: Int32): Option[User],
    def saveUser(user: User): Unit
}

Then use these definitions to perform effects in your code:
def updateUserEmail(userId: Int32, newEmail: String): Result[String, User] \ {Database} = {
    match Database.getUser(userId) {
        case Some(user) => {
            let updatedUser = {user | email = newEmail};
            Database.saveUser(updatedUser);
            Ok(updatedUser)
        }
        case None => {
            Err("User not found")
        }
    }
}

This replaces the need for dependency injection, since you can provide different handlers for these database operations in production vs testing:
def main(): Unit \\ IO = { // production handler, uses a real database
    run {
        updateUserEmail(123, "new@example.com")
    } with handler Database {
        def getUser(id, resume) = {
		        // real db query
            resume(user)
        }
        def saveUser(user, resume) = {
		        // real db query
            resume()
        }
    }
}

def testUpdateUserEmail(): Unit = { // test handler, just stubs
    let testUser = {id = 123, email = "old@example.com"};
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Ok({testUser | email = "new@example.com"}))
    } with handler Database {
        def getUser(id, resume) = resume(Some(testUser))
        def saveUser(user, resume) = {
            assert(user.email == "new@example.com");
            resume()
        }
    
}

In my opinion, the biggest advantage that effect handlers give is that they abstract away the patterns associated with DDD, Clean Architecture, Hexagonal architecture, etc. commonly found in enterprise code.
All these architectures give you some sort of way to isolate your core logic, which should be pure, from infrastructure and app logic, with deals with external dependencies. But you have to commit to an architecture and the whole team has to be disciplined enough to stick to for this to work.
Using effects encourages separating the definition of effect operations from implementation by default, meaning you don’t really need these architecture patterns anymore.
This is great, since relying on team discipline exclusively rarely works. It also saves a bunch of time otherwise spent on bike shedding.
Effect handlers also allow you to easily install stubs, which you can use to create quick test cases without boilerplate, just by swapping handlers:
def testErrorConditions(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Err("User not found"))
    } with handler Database {
        def getUser(_, resume) = resume(None) // Stub: always return None
        def saveUser(_, resume) = resume()             // Won't be called
    }
}

def testSlowDatabase(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result.isOk())
    } with handler Database {
        def getUser(id, resume) = {
            Thread.sleep(100);  // Simulate slow query
            resume(Some({id = id, email = "old@example.com"}))
        }
        def saveUser(user, resume) = {
            Thread.sleep(50);   // Simulate slow save
            resume()
        }
    }
}

You can even make a handler that records all interactions instead of executing them. There are many possibilities here.
Real-World App: AI movie recommendations 🔗
To bring this all together, let’s make a real application using effects.
Our app will fetch some movie data from TheMovieDB, and then use an LLM to recommend some movies based on user preferences provided from the console.
Flix interoperates with the JVM, meaning we can call code from Java, Kotlin, Scala, etc.
First, let’s define the two custom effects we will need: MovieAPI and LLM:
eff MovieAPI {
    def getPopularMovies(): String
}

eff LLM {
    def recommend(movies: String, preferences: String): String
}

We can then perform the effects in main like so, providing some basic handlers that use the Flix’s stdlib HTTP client:
def getRecommendation(preferences: String): String \ {MovieAPI, LLM} = {
    let movies = MovieAPI.getPopularMovies();
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        let suggestion = getRecommendation("action movies");
        println(suggestion)
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            let response = HttpWithResult.get("https://api.themoviedb.org/3/movie/popular", Map.empty());
            match response {
                case Result.Ok(resp) => resume(Http.Response.body(resp))
                case Result.Err(_) => resume("[]")
            }
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            let prompt = "Movies: ${movies}. User likes: ${prefs}. Recommend one movie.";
            let response = HttpWithResult.post("https://api.openai.com/v1/completions", Map.empty(), prompt);
            match response {
                case Result.Ok(resp) => resume(Http.Response.body(resp))
                case Result.Err(_) => resume("Try watching a classic!")
            }
        }
    } with HttpWithResult.runWithIO

Notice that both effects are quite generic. So we can easily swap either the movie API or the LLM provider without touching anything in the core logic:
// Switch to different movie provider
with handler MovieAPI {
    def getPopularMovies(_, resume) = {
        let response = HttpWithResult.get("https://api.imdb.com/popular", Map.empty());
        // ... handle IMDB response format
    }
}

// Switch to different LLM provider  
with handler LLM {
    def recommend(movies, prefs, resume) = {
        let response = HttpWithResult.post("https://api.anthropic.com/v1/messages", Map.empty(), prompt);
        // ... handle Claude response format
    }
}

To get the user input we will need to include the standard Console effect:
def main(): Unit \ {Net, IO} = 
    run {
        Console.println("What movie genres do you enjoy?");
        let preferences = Console.readln();
        let suggestion = getRecommendation(preferences);
        Console.println("Recommendation: ${suggestion}")
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with HttpWithResult.runWithIO

We can also add some basic logs using the standard Logger effect:
def getRecommendation(preferences: String): String \ {MovieAPI, LLM, Logger} = {
    Logger.info("Fetching popular movies...");
    let movies = MovieAPI.getPopularMovies();
    Logger.info("Getting LLM recommendation...");
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        /* ... console interaction ... */
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with Logger.runWithIO
      with HttpWithResult.runWithIO

That’s it! Let’s run the app and test it manually like so:
 flix run Main.flix
What movie genres do you enjoy?
> sci-fi horror
[INFO] Fetching popular movies...
[INFO] Getting LLM recommendation...
Recommendation: Based on your interest in sci-fi horror, I recommend "Alien" - a perfect blend of both genres!

We can also easily write tests for the core logic by providing test handlers for our movie and LLM effects:
def testRecommendation(): String = 
    run {
        getRecommendation("comedy")
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            resume("""[{"title": "The Grand Budapest Hotel", "genre": "comedy"}]""")
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            resume("I recommend The Grand Budapest Hotel - perfect for comedy lovers!")
        }
    } with handler Logger {
        def log(_, _, resume) = resume()  // Silent in tests
    }

def runTests(): Unit \ IO = {
    let result = testRecommendation();
    println("Test result: ${result}")
}

Where to Go From Here 🔗
Read the Flix docs
Especially on cool features like effect polymorphism, effect exclusion etc. Check out code examples in the repo
Join the community and contribute with libraries
The Flix compiler and stdlib are quite feature-rich at this point, and having JVM interop means you have all the essentials you need to write practical code. But there are still very few pure Flix libraries. So it’s very valuable to contribute some. The ideas I can think of are, for example, rebuilding standard things like Web frameworks in an effect oriented way,. Or taking advantage of the unique feature set in Flix to build something entirely new.
Explore effect-oriented programming
While I personally like Flix and can recommend it to others, there are other ways you can use effects for real-world software. If you’re in Typescript or Scala, try out Effect or ZIO/Kyo/Cats. If you’re looking for other languages that support effects natively, and you’re not afraid of Haskell-like syntax, check out Unison. They have a bunch of other concepts I find cool, like a better distributed computing model and the code being content-addressed.
Thanks for reading! I hope this article was useful. Hit me up if you have questions or feedback, and check out my website, where I’m exploring sustainable tech and coding practices: relax.software
 Question What should I write about next?   More on Effect-oriented programming   More on the Flix programming language   Comparison of languages and libraries with effects support   General correct and maintainable software content    ✓ Thanks for your input!
    
These questions help direct new content. Want to get notified when something new is posted? ✓ Saved!
  
Extra: Why Algebraic Effects are Algebraic and how they relate to monads 🔗
Okay, practical people have left the room. Following sections are nerds-only.
For some reason, all the content I’ve been reading on algebraic effects uses this term a lot, but no one explains why specifically they’re called “algebraic”. So I did some digging.
Turns out, algebraic effects are “algebraic” because they can be described with laws and equations, like in algebra — the kind we learn at school. Which is I guess why they’re easier to grasp than monads — unlike algebra, you usually don’t study category theory in high school.
But the algebraic part only applies to the effect “constructors”, i.e the operations themselves like get() or put() for the state effect.
Effect handlers, on the other hand, are not algebraic at all, which can be a bit confusing. But it makes sense if you think about it — the purpose of handlers is to act as “deconstructors”, interpreting our algebraic effect operations by means of things that cannot be described by algebraic equations alone, such as continuations .
In fact, effect handlers are often (but not always) implemented via delimited continuations. There are also other, static/lexically scoped and maybe more performant approaches being explored, such as this one
“Real” algebraic effects don’t require monads. Monads and algebraic effects are two different concepts tackling similar problems. One is expressible in terms of the other, but algebraic effects are arguably more flexible.
You could actually implement algebraic effects using a continuation monad. If we don’t care about types, effects are perfectly expressible with monads and vice versa
The problems appear when we introduce types into the picture. In a properly typed world, you can’t actually reproduce the same expressiveness you get with effects using monads. You’ll end up breaking the type system or reducing expressiveness at some point.
Effects are, in this sense, more “powerful” than monads with their natural type system: you can express infinitely many computations with them. E.g if you use a tick() effect and you do a bunch of sequential tick() s, the result will be a distinct computation each time. With monads and their natural type system the set of computations you could express is finite.
Additionally, with monads you commit to a specific interpretation of an effect in advance, while effects completely decouple effect definition from it’s implementation.
Finally, effects are easier to compose than monads. With monad transformers you quickly hit the wall having to define a bunch of different combinations that each have distinct semantics. Effects compose naturally.
So while effect libraries in languages like Typescript and Scala are able to express effects using monads3, and the behavior could be identical at runtime, this cannot replace having an actual type and effect system, with effects being properly typed.
 Question How do you usually learn about new things?   Technical articles (like this one)   Books   Screencasts or videos   Courses, bootcamps or workshops   Other    ✓ Thanks for your input!
    

Footnotes 🔗
Footnotes


“What color is your function” is a problem explored in this article. In languages which have Async baked in via special keywords (e.g JavaScript async/await) it becomes a pain to refactor and to combine synchronous and asynchronous code. If you make one function deep in the call stack async, all the callers will have to be made Async as well, or await() the results. With effects you don’t have this issue as there are no keywords and no special behavior. Async is simply done with effect handlers. ↩


I like the grandma example more than the “launch missiles” popular in the Haskell world. Took it from this article by Kevin Mahoney. It’s somehow more offensive ↩


See some examples in this article. This also shows how Haskell’s new delimited continuation support can be used to implement algebraic effects and handlers ↩


 ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Serverless Horrors]]></title>
            <link>https://serverlesshorrors.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157110</guid>
            <description><![CDATA[Stories you never want to feel on your own skin]]></description>
            <content:encoded><![CDATA[       ServerlessHorrors is a simple blog where you can read all the horror stories of serverless. Yikes!

Made by Andras who is working on an open-source & self-hostable Heroku / Netlify / Vercel alternative called Coolify.

Have a story?

Write me
Open a PR on Github
     Posts  New 13  May 2025     New 3  May 2025     10  Apr 2025     17  Jan 2025      13  Jan 2025      $22.639,69   I received an insanely bill of 22k USD today from simply using BigQuery on a public data set in the playground...     google    bigquery    sql       13  Jan 2025     6  Jan 2025     5  Jan 2025     1  Sep 2024     25  Jun 2024     6  Jun 2024     26  May 2024     29  Apr 2024     10  Mar 2024     27  Feb 2024      13  Feb 2024      $23,000.420   What is happening?! Someone spammed EchoFox and spiked my Vercel bill to $23k and caused 56k+ accounts and trials...     vercel    bandwidth    ddos       5  Apr 2023     14  Jan 2023     27  Mar 2020           ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I'm a dermatologist and I vibe coded a skin cancer learning app]]></title>
            <link>https://molecheck.info/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157020</guid>
            <description><![CDATA[For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.]]></description>
            <content:encoded><![CDATA[
            Designed for Mobile
            For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.
            
            https://molecheck.info
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What is the origin of the private network address 192.168.*.*? (2009)]]></title>
            <link>https://lists.ding.net/othersite/isoc-internet-history/2009/oct/msg00000.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45156826</guid>
        </item>
        <item>
            <title><![CDATA[The "impossibly small" Microdot web framework]]></title>
            <link>https://lwn.net/Articles/1034121/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155682</guid>
            <description><![CDATA[The Microdot web framework is quite small, as its name would imply; it supports both standard C [...]]]></description>
            <content:encoded><![CDATA[

This article brought to you by LWN subscribers

Subscribers to LWN.net made this article — and everything that
       surrounds it — possible.  If you appreciate our content, please
       buy a subscription and make the next
       set of articles possible.



The Microdot
web framework is quite small, as its name would imply; it supports both
standard CPython and MicroPython,
so it can be used on systems ranging from internet-of-things (IoT) devices
all the way up to large, cloudy servers.  It was developed by Miguel
Grinberg, who gave a presentation about it at EuroPython 2025.  His name
may sound familiar from his well-known Flask
Mega-Tutorial, which has introduced many to the Flask lightweight Python-based
web framework.  It should come as no surprise, then, that Microdot is
inspired by its rather larger cousin, so Flask enthusiasts will find much
to like in Microdot—and will come up to speed quickly should their needs turn
toward smaller systems.



We have looked at various pieces of this software stack along the way: Microdot itself in January 2024, MicroPython in 2023, and Flask as part of a look at Python microframeworks in
2019.



Grinberg began his talk with an introduction.  He has been living in
Ireland for a few years and "I make stuff".  That includes open-source projects, blog posts (on a
Flask-based blog platform that he wrote), and "a bunch of books".
He works for Elastic and is one of the maintainers of the Elasticsearch
Python client, "so maybe you have used some of the things that I
made for money".


Why?


With a chuckle, he asked: "Why do we need another web framework?  We
have so many already."  The story starts with a move that he made to
Ireland from the US in 2018; he rented a house with a "smart" heating
controller and was excited to use it.  There were two thermostats, one for
each level of the house, and he was "really looking forward to the
winter" to see the system in action.



As might be guessed, he could set target temperatures in each thermostat;
they would communicate with the controller that would turn the heating on
and off as needed.  In addition, the system had a web server that could be
used to query various parameters or to start and stop the heaters.  You
could even send commands via SMS text messages; "there's a SIM card
somewhere in that box [...] very exciting stuff".



When winter rolled around, it did not work that well, however; sometimes
the house was too chilly or warm and he had to start and stop the heaters
himself. He did some debugging and found that the thermostats were
reporting temperatures that were off by ±3°C, "which is too much for
trying to keep the house at 20°".  The owner of the house thought that
he was too used to the US where things just work; "at least she thinks that in America everything is super-efficient,
everything works, and she thought 'this is the way things work in
Ireland'".  So he did not make any progress with the owner.



At that point, most people would probably just give up and live with the
problem; "I hacked my heating controller instead".  He set the
temperatures in both thermostats to zero, which effectively disabled their
ability to affect the heaters at all, and built two small boards running
MicroPython, each connected to a temperature and humidity sensor device.
He wrote code that would check the temperature every five minutes and send
the appropriate commands to start or stop the heaters based on what it
found.



So the second half of his first winter in Ireland went great.  The sensors
are accurate to ±0.5°C, so "problem solved".  But, that led to a new
problem for him.  "I wanted to know things: What's the temperature right
now?  Is the heating running right now or not?  How many hours did it run
today compared to yesterday?"  And so on.



He added a small LCD screen to display some information, but he had to
actually go to the device and look at it; what he really wanted was to be
able to talk to the device over WiFi and get information from the couch
while he was watching TV. "I wanted to host a web server [...]  that
will show me a little dashboard".



So he searched for web frameworks
for MicroPython; in the winter of 2018-2019, "there were none".
Neither Flask nor Bottle,
which is a good bit smaller, would run on MicroPython; both are too large
for the devices,
but, in addition, the standard library for MicroPython is a subset of that of
CPython, so many things that they need are missing. A "normal
person" would likely have just accepted that and moved on; "I
created a web framework instead."


Demo


He brought one of his thermostat devices to Prague for the conference and
did a small demonstration of it operating during the talk.  The device was
connected to his laptop using USB, which provided power, but also a serial
connection to the board.  On the laptop, he used the rshell
remote MicroPython shell to talk to the board, effectively using the laptop
as a terminal.





He started the MicroPython read-eval-print loop (REPL) on the board in
order to simulate the normal operation of the board.  When it is plugged
into the wall, rather than a laptop, it will boot to the web server, so he
made that happen with a soft-reboot command.  The device then connected to
the conference WiFi and gave him the IP address (and port) where the server
was running.



He switched over to Firefox on his laptop and visited the site, which showed a
dashboard that had the current temperature (24.4°) and relative humidity
(56.9%) of the room.  He also used curl from the laptop to contact the
api endpoint of the web application, which returned JSON with the
two values and the time.  There is no persistent clock on the board, so the
application contacts an NTP server to pick up the time when it boots; that
allows it to report the last time a measurement was taken.



Grinberg said that he wanted to set the expectations at the right level by
looking at the capabilities of the microcontrollers he often uses with
Microdot.  For example, the ESP8266 in his thermostat device has 64KB of
RAM and up to 4MB of flash.  The ESP8266 is the smallest and least expensive (around €5)
device with WiFi that
he has found; there are many even smaller devices, but they lack
the networking required for running a web server.  The other devices
he uses are the Raspberry Pi Pico W with 2MB of flash and 256KB of RAM and
the ESP32 with up to 8MB of flash and 512KB of RAM.  He contrasted those
with his laptop, which has 32GB of RAM, so "you need 500,000
ESP8266s" to have the same amount of memory.


Features


The core framework of Microdot is in a single microdot.py
file.  It is fully asynchronous, using the MicroPython
subset of the CPython
asyncio module, so it can run on both interpreters.  It uses
asyncio because that is the only way to do concurrency on the
microcontrollers; there is no support for processes or threads on those devices.



Microdot has Flask-style route
decorators to define URLs for the application.  It has Request
and Response
classes, as well as hooks
to run before and after requests, he said.  Handling query strings,
form data, and JSON are all available in Microdot via normal Python
dictionaries.  Importantly, it can handle streaming requests and responses;
because of the limited memory of these devices, it may be necessary to split
up the handling of larger requests or responses.



It supports setting
cookies and sending static
files.  Web applications can be constructed from a set of modules, using sub-applications,
which are similar to Flask
blueprints.  It also has its own web
server with TLS support.  "I'm very proud of all the stuff I was
able to fit in the core Microdot framework", Grinberg said.



He hoped that attendees would have to think for a minute to come up with
things that are missing from Microdot, but they definitely do exist.  There
are some officially
maintained extensions, each in its own single .py file, to
fill some of those holes.  They encompass functionality that is important,
but he did not want to add to the core because that would make it too large
to fit on the low-end ESP8266 that he is using.



There is an extension for multipart
forms, which includes file uploads; "this is extremely complicated
to parse, it didn't make sense to add it into the core because most people
don't do this".  There is support for WebSocket
and server-sent
events (SSE).  Templates
are supported using utemplate
for both Python implementations or Jinja, which only
works on CPython.  There are extensions for basic
and token-based authentication and for secure
user logins with session data; the latter required a replacement for
the CPython-only PyJWT, which Grinberg
wrote and contributed to MicroPython as jwt.py.
There is a small handful of other extensions that he quickly mentioned as well.



"I consider the documentation as part of the framework"; he is
"kind of fanatical" about documenting everything.  If there is
something missing or not explained well, "it's a bug that I need to
fix".  He writes books, so the documentation is organized similarly;
it comes in at 9,267 words, which equates to around 47 minutes of reading
time.  There is 100% test coverage, he said, and there are around 30
examples, with more coming.



A design principle that he follows is "no dark magic".  An example
of dark magic to him is the Flask
application context, "which very few people understand".  In
Microdot, the request object is explicitly passed to each route function.
Another example is the dependency
injection that is used by the FastAPI framework to add
components; Microdot uses explicit decorators instead.



He used the cloc
utility to count lines of code, while ignoring comments and blank
lines.  Using that, Django
comes in at 110,000 lines, Flask plus its essential Werkzeug library
is 15,500 lines, FastAPI with Starlette is 14,900 lines, Bottle is
around 3,000 lines, while the Microdot core has 765 lines ("believe it
or not") and a full
install with all the extensions on MicroPython comes in at just shy of 1,700
lines of code.



He ended with an example of how Microdot can be so small by comparing the
URL matching in Flask with Microdot.  The Flask version does lots more than
Microdot, with more supported types of arguments in a URL and multiple classes
in the werkzeug.routing
module; it has 1,362 lines of code.  For Microdot, there is a more
limited set of URL arguments, though there is still the ability to define
custom types, and a single
URLPattern class; all of that is done in 63 lines of
code. "I don't intend to support everything that Flask supports, in
terms of routing, but I intend to support the 20% that covers 80% of the
use cases."  That is the overall mechanism that he has used to get to
something that is so small.



An audience member asked about whether the Microdot code was minified in
order to get it to fit.  Grinberg said that doing so was not all that
useful for MicroPython, but the code is smaller on the board because it is
precompiled on another system; that results in a
microdot.mpy file, which is bytecode for MicroPython.  For
example, on the
low-end device he is using for his thermostats, Microdot would not be able
to be compiled on the device itself.  There are some other tricks that can
also be used for reducing the RAM requirements, like putting the code into
the firmware as part of the MicroPython binary.



The final question was about performance, and how many requests per second
could be handled. Grinberg said that he did not
have any real numbers, but that the device he demonstrated is "really
really slow".  That question led to a blog
post in late July where Grinberg tried to more fully answer it.



[I would like to thank the Linux Foundation, LWN's travel sponsor, for
travel assistance to Prague for EuroPython.]

           Index entries for this article
           ConferenceEuroPython/2025
            PythonWeb
            

            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The key to getting MVC correct is understanding what models are]]></title>
            <link>https://stlab.cc/tips/about-mvc.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154501</guid>
            <description><![CDATA[stlab hosts modern, modular C++ algorithms and data structures.]]></description>
            <content:encoded><![CDATA[
        Smalltalk MVC is defined in Design Pattern as:


  MVC Consists of three kinds of objects. The Model is the application object, the View is its screen presentation, and the Controller defines the way the user interface reacts to user input.1


However this definition has been abused over the years - Back in 2003 I gave a talk citing how bad Apple’s definition was. At the time it stated:


  A view object knows how to display and possibly edit data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects… Controllers are often the least reusable objects in an application, but that’s acceptable…2


Of course it isn’t acceptable and, over the years, Apple has refined their definition and now acknowledge the distinction between the traditional Smalltalk version of MVC and the Cocoa version.3 But the Cocoa version is still defined much as it was before:


  A view object knows how to display, and might allow users to edit, the data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects…3


In looking at how iOS applications are written the sentiment that controllers (and now view-controllers) are often the least reusable components in an application still flourishes, even if it is now unstated.

MVC (I’ll always use that term to refer to the Smalltalk form) has the following structure:















figure: Smalltalk MVC4


Here the solid lines imply a direct association. And the dashed lines an indirect association by an observer. So what we see is that the model is unaware of the view and controller, except indirectly through notifications, and hence the code in the Model is reusable. The controller and view bind to the model, not the other way around.

Often the function of the Controller and View are tightly coupled into a “widget” or “control”. When Apple talks about a View-Controller in their model they are talking about a grab-bag of an uber-widget that is a composite of UIView widgets and multiple models. From what I’ve seen, including in Apple’s example code, it is usually a pretty big mess.

The key to getting MVC correct is understanding what models are. A model is simply an object5 which can be observed (a requirement for attaching views). For example, in ObjC an int is an object, but it is not observable. However, an ObjC object with an int property is observable using Key-Value Observing6.  A model may encapsulate complex relationships between the model’s properties. A trivial model is one where each property is completely independent (think C struct vs. C++ class). From a notification the view should be able to determine, at a minimum:


  What changed. It may be as simple as “the model bound to the view”.
  The new value to display.


For example, let’s say our model is a trivial observable boolean (I can’t imagine a simpler model). What we want is a checkbox that binds to the observable boolean. When the controller requests a change in value, the boolean is updated, and the view is notified of the new state of the model. The model is unaware of what UI is attached to it, and in fact there could be multiple UIs, including something like a scripting system, attached to the same instance of the model. This is a form of data binding - though most data binding systems replicate the problems of their underlying widget set by treating the model as if it were observing the view, not the other way around.

Contrast this with most UI frameworks where you have a checkbox widget from which you can query the value and you receive a notification when the value has changed. This is pushing a model into the widget. With MVC you never ask a question like “what is the default state of this checkbox?” - the default state of the view is always the current state of the model. You would also never get the state of the checkbox - the state of the checkbox is simply a reflection of the state of the model. In a system where you get the state of a checkbox you are binding two models together by treating one as a view/controller of the other. Such a pattern doesn’t scale beyond trivial models, and even for those it introduces some ambiguity.

I conjecture that one of the reasons why MVC has been so screwed up is because, unlike in Smalltalk, writing something as simple as an observable boolean is a bit of a pain in a language like Pascal or C. You quickly get into object ownership and lifetime issues and how to write bind expressions. If one also assumes that you have a 1:1 mapping from UI to model then there is some inherent inefficiency in the generalization. The Lisa team made some major compromises and the rest of the industry followed along.7

To support more complex views, the notification may need to specify what parts of the model changed and how those parts changed. For example, “image 58 was removed from the sequence”. A complete model is one that can support any view of that model type efficiently (related to the notion of a complete type and a type’s efficient basis).

One additional attribute of MVC is that it is a composite pattern. This is hinted at by the direct connection between the Controller and the View. As I said early, the view may contain state, this state is itself an object, and because this state is also displayed within the view it is observable. It is another model. I refers to this as the view’s model. This model may include things such as the visibility of a window, the tab the user was last looking at, and the portion of the model being viewed.

Identifying what the models are in your system is important. We usually do pretty good at identifying the major models. Such as “this is an image” - but often fall short of identifying the complete model, i.e. “this is an image with a collection of settings.” We end up with our model spread out within the code (an incidental type) and it makes it more difficult to deal with it.

A common model that is often completely overlooked is the model for function arguments. When you have a command, button, gesture, or menu item in your application, these are bound to a function. The function itself is not typically a zeroary function but rather has a set of arguments that are constructed through other parts of the UI. For example, if I have a list of images in my application, I might have a button to delete the selected images. Here the current selection is the argument to my delete command. To create a UI for the selection I must create a model of the arguments to my function. A precondition of delete is that the selection is not empty. This precondition must be observable in the argument model so it can be reflected in the view by disabling or hiding the button and in the controller be disallowing the user to click the button and issue the command. The same argument model can be shared for multiple commands within an application.


  
    

      Gamma, Erich. “1.2 Design Patterns in Smalltalk MVC.” Design Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-Wesley, 1995. N. pag. Print. ↩
    
    

      http://smartfriends.com/U/Presenters/untangling_software.pdf (Don’t bother reading, this was an incomprehensible talk.) ↩
    
    

      https://developer.apple.com/library/content/documentation/General/Conceptual/CocoaEncyclopedia/Model-View-Controller/Model-View-Controller.html ↩ ↩2
    
    

      https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller ↩
    
    

      Stepanov, Alexander A., and Paul McJones. “1.3 Objects.” Elements of Programming. Upper Saddle River, NJ: Addison-Wesley, 2009. N. pag. Print. ↩
    
    

      https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/KeyValueObserving/KeyValueObserving.html ↩
    
    

      https://en.wikipedia.org/wiki/Object-oriented_user_interface ↩
    
  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I'm making an open-source platform for learning Japanese]]></title>
            <link>https://kanadojo.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152940</guid>
        </item>
        <item>
            <title><![CDATA[GPT-5 Thinking in ChatGPT (a.k.a. Research Goblin) is good at search]]></title>
            <link>https://simonwillison.net/2025/Sep/6/research-goblin/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152284</guid>
            <description><![CDATA[“Don’t use chatbots as search engines” was great advice for several years... until it wasn’t. I wrote about how good OpenAI’s o3 was at using its Bing-backed search tool back …]]></description>
            <content:encoded><![CDATA[

6th September 2025



“Don’t use chatbots as search engines” was great advice for several years... until it wasn’t.
I wrote about how good OpenAI’s o3 was at using its Bing-backed search tool back in April. GPT-5 feels even better.
I’ve started calling it my Research Goblin. I can assign a task to it, no matter how trivial or complex, and it will do an often unreasonable amount of work to search the internet and figure out an answer.
This is excellent for satisfying curiosity, and occasionally useful for more important endeavors as well.
I always run my searches by selecting the “GPT-5 Thinking” model from the model picker—in my experience this leads to far more comprehensive (albeit much slower) results.
Here are some examples from just the last couple of days. Every single one of them was run on my phone, usually while I was doing something else. Most of them were dictated using the iPhone voice keyboard, which I find faster than typing. Plus, it’s fun to talk to my Research Goblin.


  Bouncy travelators
  Identify this building
  Starbucks UK cake pops
  Britannica to seed Wikipedia
  Official name for the University of Cambridge
  History of the caverns in Exeter quay
  Aldi vs Lidl
  AI labs scanning books for training data
  GPT-5 for search feels competent
  Tips for using search in ChatGPT


Bouncy travelators

They used to be rubber bouncy travelators at Heathrow and they were really fun, have all been replaced by metal ones now and if so, when did that happen?

I was traveling through Heathrow airport pondering what had happened to the fun bouncy rubber travelators.
Here’s what I got. Research Goblin narrowed it down to some time between 2014-2018 but, more importantly, found me this delightful 2024 article by Peter Hartlaub in the San Francisco Chronicle with a history of the SFO bouncy walkways, now also sadly retired.
Identify this building


Identify this building in reading

This is a photo I snapped out of the window on the train. It thought for 1m4s and correctly identified it as The Blade.
Starbucks UK cake pops

Starbucks in the UK don’t sell cake pops! Do a deep investigative dive

The Starbucks in Exeter railway station didn’t have cake pops, and the lady I asked didn’t know what they were.
Here’s the result. It turns out Starbucks did launch cake pops in the UK in September 2023 but they aren’t available at all outlets, in particular the licensed travel locations such as the one at Exeter St Davids station.
I particularly enjoyed how it established definitive proof by consulting the nutrition and allergen guide PDF on starbucks.co.uk, which does indeed list both the Birthday Cake Pop (my favourite) and the Cookies and Cream one (apparently discontinued in the USA, at least according to r/starbucks).
Britannica to seed Wikipedia

Someone on hacker News said:
> I was looking at another thread about how Wikipedia was the best thing on the internet. But they only got the head start by taking copy of Encyclopedia Britannica and everything else
Find what they meant by that

The result. It turns out Wikipedia did seed itself with content from the out-of-copyright 1911 Encyclopædia Britannica... but that project took place in 2006, five years after Wikipedia first launched in 2001.
I asked:

What is the single best article I can link somebody to that explains the 1911 Britannica thing

And it pointed me to Wikipedia:WikiProject Encyclopaedia Britannica which includes a detailed explanation and a link to the 13,000 pages still tagged with the template from that project. I posted what I found in a comment.
Notably (for me anyway) I didn’t feel the need to disclose my use of ChatGPT in finding that information—at this point that feels a little like disclosing that I ran a Google search.
Official name for the University of Cambridge

What is the official legal name of the university of Cambridge?

Here’s the context for that one. It thought for 19 seconds—the thinking trace reveals it knew the answer but wanted to confirm it. It answered:

“The Chancellor, Masters, and Scholars of the University of Cambridge.” University of Cambridge, Cambridge University Press & Assessment

That first link gave me the citation I needed in order to be sure this was right.
Since this is my shortest example, here’s a screenshot of the expanded “Thought for 19s” panel. I always expand the thoughts—seeing how it pulled together its answer is crucial for evaluating if the answer is likely to be useful or not.

History of the caverns in Exeter quay

Research On the waterfront restaurant in Exeter, is it dug into the cliffs somehow? History of the building, who built it, why and how

We were out to dinner here and noticed that the interior of the restaurant appeared to be a space dug into the cliff, which piqued my interest.
This was the ChatGPT session that inspired the Research Goblin nickname. It just kept on digging!
The first reply took 2m40s and confirmed that yes, these quay buildings were carved into the red sandstone cliff in the 1820s-1830s.
ChatGPT with GPT-5 really likes to suggest additional steps it can take. In this case:

If you’d like, I can dig up the exact Historic England entry that covers the “Southern Warehouse” address and overlay it on a map of the vaults.

I often say “yes” purely out of curiosity to see what it will do next, and the offer to “overlay it on a map” was irresistible, like how would it even do that?
It did a ton of extra searches, found latitude and longitude coordinates for the restaurant (from Wikimedia Commons) and the warehouse buildings (from National Heritage List for England via Wikipedia), showed me that data in a table and then used Python to render this image:

Yeah, so that one is a bit of a miss!
But... the underlying research chain, as always, provided fascinating. I noticed that at one point it had searched for “Exeter Quay vaults map PDF”, which inspired my next question:

Did you find any PDF diagrams, showing the tunnels into the cliff

2m20s later it had found a planning document and a letting brochure, both PDFs, but noted that neither of them had the diagram I wanted. Evidently it actually “reads” the PDFs it finds as part of its activities, since it appears to have knowledge of the images contained within them as well as the text.
It concluded with this intriguing note:

There’s strong evidence that a detailed plan/section does exist in the Exeter Archaeology report often cited by planners—“Archaeological, Historical and Conservation Study of the Exeter Canal Basin” (Exeter Archaeology Report 00.18, 2000)—but it isn’t online; it’s just cited in planning documents. You’d likely need to request it via the Devon Heritage Centre/Historic Environment Record.
If you want, I can keep hunting specialist archives for a digitized copy or a different report that includes the vault layouts.

Obviously I said “Yes do that” and 2m24s later it chucked out a whole host of extra links, an apology that it still hadn’t found a detailed plan diagram and, under the section heading “What I recommend next (actionable + exact ask)”:

If you’re happy, I can draft an email you can paste to the right archivists. [...]

Subject: Exeter Quayside cliff vaults – request for plan/section drawings
Hello,
I’m researching the cliff-cut vaults behind The Quay (On The Waterfront / Kings Wharf).
Could you supply scans or copies of any plans/sections/diagrams of these vaults—especially those in Exeter Archaeology Report 00.18 (2000) (Archaeological, Historical and Conservation Study of the Exeter Canal Basin) and any EMAFU/Exeter Archaeology drawings for “The Quay” showing the vault/tunnel layouts cut into the red sandstone cliff?
For reference: NHLE list entries 1223045 (1–4 Warehouse Vaults), 1223046 (6–11 Warehouse Vaults), and 1223114 (14–25 Vaults).
Many thanks!


It’s a good thing my Research Goblin doesn’t have the ability to send email (which we already know is very risky business) or at this point I wouldn’t be surprised if it started autonomously bothering archivists on my behalf!
Aldi vs Lidl

In UK supermarket terms compare lidl and aldi

I saw an Aldi and a Lidl next to each other in Exeter and was curious as to how they compare. In this case my Research Goblin went way deeper than I had intended, thinking for 1m25s about market share and price surveys and loyalty schemes and answering with ten paragraphs of detailed analysis.
I replied:

Yeah, I just wanted the basic vibes like is one fancier than the other?

It thought for 6 seconds, didn’t run any additional searches and replied:

Short version: neither is “fancy” — both are no-frills discounters — but Lidl often feels a touch more polished, while Aldi feels a touch more bare-bones. [...]

Since I was now enjoying having my Goblin express judgement over British supermarket chains I said:

How do they rate in fancinews compared to all of the other major UK supermarket chains?

(That was a typo for “fanciness” which it figured out.)
It thought for another 1m1s, ran 7 searches, consulted 27 URLs and gave me back a ranking that looked about right to me.

AI labs scanning books for training data

Anthropic bought lots of physical books and cut them up and scan them for training data. Do any other AI labs do the same thing?

Relevant to today’s big story. Research Goblin was unable to find any news stories or other evidence that any labs other than Anthropic are engaged in large scale book scanning for training data. That’s not to say it isn’t happening, but it’s happening very quietly if that’s the case.
GPT-5 for search feels competent
The word that best describes how I feel about GPT-5 search is that it feels competent.
I’ve thrown all sorts of things at it over the last few weeks and it rarely disappoints me. It almost always does better than if I were to dedicate the same amount of time to manually searching myself, mainly because it’s much faster at running searches and evaluating the results than I am.
I particularly love that it works so well on mobile. I used to reserve my deeper research sessions to a laptop where I could open up dozens of tabs. I’ll still do that for higher stakes activities but I’m finding the scope of curiosity satisfaction I can perform on the go with just my phone has increased quite dramatically.
I’ve mostly stopped using OpenAI’s Deep Research feature, because ChatGPT search now gives me the results I’m interested in far more quickly for most queries.
As a developer who builds software on LLMs I see ChatGPT search as the gold standard for what can be achieved using tool calling combined with chain-of-thought. Techniques like RAG are massively more effective if you can reframe them as several levels of tool calling with a carefully selected set of powerful search tools.
The way that search tool integrates with reasoning is key, because it allows GPT-5 to execute a search, reason about the results and then execute follow-up searches—all as part of that initial “thinking” process.
Anthropic call this ability interleaved thinking and it’s also supported by the OpenAI Responses API.
Tips for using search in ChatGPT
As with all things AI, GPT-5 search rewards intuition gathered through experience. Any time a curious thought pops into my head I try to catch it and throw it at my Research Goblin. If it’s something I’m certain it won’t be able to handle then even better! I can learn from watching it fail.
I’ve been trying out hints like “go deep” which seem to trigger a more thorough research job. I enjoy throwing those at shallow and unimportant questions like the UK Starbucks cake pops one just to see what happens!
You can throw questions at it which have a single, unambiguous answer—but I think questions which are broader and don’t have a “correct” answer can be a lot more fun. The UK supermarket rankings above are a great example of that.
Since I love a questionable analogy for LLMs Research Goblin is... well, it’s a goblin. It’s very industrious, not quite human and not entirely trustworthy. You have to be able to outwit it if you want to keep it gainfully employed.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing Speed Brain: helping web pages load 45% faster]]></title>
            <link>https://blog.cloudflare.com/introducing-speed-brain/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45143124</guid>
            <description><![CDATA[We are excited to announce the latest leap forward in speed – Speed Brain. Speed Brain uses the Speculation Rules API to prefetch content for the user's likely next navigations. The goal is to download a web page to the browser before a user navigates to it, allowing pages to load instantly.]]></description>
            <content:encoded><![CDATA[2024-09-2518 min readEach time a user visits your web page, they are initiating a race to receive content as quickly as possible. Performance is a critical factor that influences how visitors interact with your site. Some might think that moving content across the globe introduces significant latency, but for a while, network transmission speeds have approached their theoretical limits. To put this into perspective, data on Cloudflare can traverse the 11,000 kilometer round trip between New York and London in about 76 milliseconds – faster than the blink of an eye.However, delays in loading web pages persist due to the complexities of processing requests, responses, and configurations. In addition to pushing advancements in connection establishment, compression, hardware, and software, we have built a new way to reduce page load latency by anticipating how visitors will interact with a given web page. Today we are very excited to share the latest leap forward in speed: Speed Brain. It relies on the Speculation Rules API to prefetch the content of the user's likely next navigations. The main goal of Speed Brain is to download a web page to the browser cache before a user navigates to it, allowing pages to load almost instantly when the actual navigation takes place. Our initial approach uses a conservative model that prefetches static content for the next page when a user starts a touch or click event. Through the fourth quarter of 2024 and into 2025, we will offer more aggressive speculation models, such as speculatively prerendering (not just fetching the page before the navigation happens but rendering it completely) for an even faster experience. Eventually, Speed Brain will learn how to eliminate latency for your static website, without any configuration, and work with browsers to make sure that it loads as fast as possible.  To illustrate, imagine an ecommerce website selling clothing. Using the insights from our global request logs, we can predict with high accuracy that a typical visitor is likely to click on ‘Shirts’ when viewing the parent page ‘Mens > Clothes’. Based on this, we can start delivering static content, like images, before the shopper even clicks the ‘Shirts’ link. As a result, when they inevitably click, the page loads instantly. Recent lab testing of our aggressive loading model implementation has shown up to a 75% reduction in Largest Contentful Paint (LCP), the time it takes for the largest visible element (like an image, video, or text block) to load and render in the browser.The best part? We are making Speed Brain available to all plan types immediately and at no cost. Simply toggle on the Speed Brain feature for your website from the dashboard or the API. It’ll feel like magic, but behind the scenes it's a lot of clever engineering. We have already enabled Speed Brain by default on all free domains and are seeing a reduction in LCP of 45% on successful prefetches. Pro, Business, and Enterprise domains need to enable Speed Brain manually. If you have not done so already, we strongly recommend also enabling Real User Measurements (RUM) via your dashboard so you can see your new and improved web page performance. As a bonus, enabling RUM for your domain will help us provide improved and customized prefetching and prerendering rules for your website in the near future!
    
      How browsers work at a glance
      
    
    Before discussing how Speed Brain can help load content exceptionally fast, we need to take a step back to review the complexity of loading content on browsers. Every time a user navigates to your web page, a series of request and response cycles must be completed. After the browser establishes a secure connection with a server, it sends an HTTP request to retrieve the base document of the web page. The server processes the request, constructs the necessary HTML document and sends it back to the browser in the response. 
          
          
          When the browser receives an HTML document, it immediately begins parsing the content. During this process, it may encounter references to external resources such as CSS files, JavaScript, images, and fonts. These subresources are essential for rendering the page correctly, so the browser issues additional HTTP requests to fetch them. However, if these resources are available in the browser’s cache, the browser can retrieve them locally, significantly reducing network latency and improving page load times.As the browser processes HTML, CSS, and JavaScript, the rendering engine begins to display content on the screen. Once the page’s visual elements are displayed, user interactions — like clicking a link — prompt the browser to restart much of this process to fetch new content for the next page. This workflow is typical of every browsing session: as users navigate, the browser continually fetches and renders new or uncached resources, introducing a delay before the new page fully loads.Take the example of a user navigating the shopping site described above. As the shopper moves from the homepage to the ‘men's’ section of the site to the ‘clothing’ section to the ‘shirts’ section, the time spent on retrieving each of those subsequent pages can add up and contribute to the shopper leaving the site before they complete the transaction.  Ideally, having prefetched and prerendered pages present in the browser at the time each of those links are clicked would eliminate much of the network latency impact, allowing the browser to load content instantly and providing a smoother user experience. 
    
      Wait, I’ve heard this story before (how did we get to Speed Brain?)
      
    
    We know what you’re thinking. We’ve had prefetching for years. There have even been several speculative prefetching efforts in the past. You’ve heard this all before. How is this different now?You’re right, of course. Over the years, there has been a constant effort by developers and browser vendors to optimize page load times and enhance user experience across the web. Numerous techniques have been developed, spanning various layers of the Internet stack — from optimizing network layer connectivity to preloading application content closer to the client.
    
      Early prefetching: lack of data and flexibility
      
    
    Web prefetching has been one such technique that has existed for more than a decade. It is based on the assumption that certain subresources are likely to be needed in the near future, so why not fetch them proactively? This could include anything from HTML pages to images, stylesheets, or scripts that the user might need as they navigate through a website. In fact, the core concept of speculative execution is not new, as it's a general technique that's been employed in various areas of computer science for years, with branch prediction in CPUs as a prime example.In the early days of the web, several custom prefetching solutions emerged to enhance performance. For example, in 2005, Google introduced the Google Web Accelerator, a client-side application aimed at speeding up browsing for broadband users. Though innovative, the project was short-lived due to privacy and compatibility issues (we will describe how Speed Brain is different below). Predictive prefetching at that time lacked the data insights and API support for capturing user behavior, especially those handling sensitive actions like deletions or purchases.
    
      Static lists and manual effort
      
    
    Traditionally, prefetching has been accomplished through the use of the <link rel="prefetch"> attribute as one of the Resource Hints. Developers had to manually specify the attribute on each page for each resource they wanted the browser to preemptively fetch and cache in memory. This manual effort has not only been laborious but developers often lacked insight into what resources should be prefetched, which reduced the quality of their specified hints.In a similar vein, Cloudflare has offered a URL prefetching feature since 2015. Instead of prefetching in browser cache, Cloudflare allows customers to prefetch a static list of resources into the CDN cache. The feature allows prefetching resources in advance of when they are actually needed, usually during idle time or when network conditions are favorable. However, similar concerns apply for CDN prefetching, since customers have to manually decide on what resources are good candidates for prefetching for each page they own. If misconfigured, static link prefetching can be a footgun, causing the web page load time to actually slow down.
    
      Server Push and its struggles
      
    
    HTTP/2’s "server push" was another attempt to improve web performance by pushing resources to the client before they were requested. In theory, this would reduce latency by eliminating the need for additional round trips for future assets. However, the server-centric dictatorial nature of "pushing" resources to the client raised significant challenges, primarily due to lack of context about what was already cached in the browser. This not only wasted bandwidth but had the potential to slow down the delivery of critical resources, like base HTML and CSS, due to race conditions on browser fetches when rendering the page. The proposed solution of cache digests, which would have informed servers about client cache contents, never gained widespread implementation, leaving servers to push resources blindly. In October 2022, Google Chrome removed Server Push support, and in September 2024, Firefox followed suit.
    
      A step forward with Early Hints
      
    
    As a successor, Early Hints was specified in 2017 but not widely adopted until 2022, when we partnered with browsers and key customers to deploy it. It offers a more efficient alternative by "hinting" to clients which resources to load, allowing better prioritization based on what the browser needs. Specifically, the server sends a 103 Early Hints HTTP status code with a list of key page assets that the browser should start loading while the main response is still being prepared. This gives the browser a head start in fetching essential resources and avoids redundant preloading if assets are already cached. Although Early Hints doesn't adapt to user behaviors or dynamic page conditions (yet), its use is primarily limited to preloading specific assets rather than full web pages — in particular, cases where there is a long server “think time” to produce HTML.As the web evolves, tools that can handle complex, dynamic user interactions will become increasingly important to balance the performance gains of speculative execution with its potential drawbacks for end-users. For years Cloudflare has offered performance-based solutions that adapt to user behavior and work to balance the speed and correctness decisions across the Internet like Argo Smart Routing, Smart Tiered Cache, and Smart Placement. Today we take another step forward toward an adaptable framework for serving content lightning-fast. 
    
      Enter Speed Brain: what makes it different?
      
    
    Speed Brain offers a robust approach for implementing predictive prefetching strategies directly within the browser based on the ruleset returned by our servers. By building on lessons from previous attempts, it shifts the responsibility for resource prediction to the client, enabling more dynamic and personalized optimizations based on user interaction – like hovering over a link, for example – and their device capabilities. Instead of the browser sitting idly waiting for the next web page to be requested by the user, it takes cues from how a user is interacting with a page and begins asking for the next web page before the user finishes clicking on a link.Behind the scenes, all of this magic is made possible by the Speculation Rules API, which is an emerging standard in the web performance space from Google. When Cloudflare’s Speed Brain feature is enabled, an HTTP header called Speculation-Rules is added to web page responses. The value for this header is a URL that hosts an opinionated Rules configuration. This configuration instructs the browser to initiate prefetch requests for future navigations. Speed Brain does not improve page load time for the first page that is visited on a website, but it can improve it for subsequent web pages that are visited on the same site.
          
          
          The idea seems simple enough, but prefetching comes with challenges, as some prefetched content may never end up being used. With the initial release of Speed Brain, we have designed a solution with guardrails that addresses two important but distinct issues that limited previous speculation efforts — stale prefetch configuration and incorrect prefetching. The Speculation Rules API configuration we have chosen for this initial release has been carefully designed to balance safety of prefetching while still maintaining broad applicability of rules for the entire site.
    
      Stale prefetch configuration
      
    
    As websites inevitably change over time, static prefetch configurations often become outdated, leading to inefficient or ineffective prefetching. This has been especially true for techniques like the rel=prefetch attribute or static CDN prefetching URL sets, which have required developers to manually maintain relevant prefetchable URL lists for each page of their website. Most static prefetch lists are based on developer intuition rather than real user navigation data, potentially missing important prefetch opportunities or wasting resources on unnecessary prefetches. 
    
      Incorrect prefetching
      
    
    Since prefetch requests are just like normal requests except with a `sec-purpose` HTTP request header, they incur the same overhead on the client, network, and server. However, the crucial difference is that prefetch requests anticipate user behavior and the response might not end up being used, so all that overhead might be wasted. This makes prefetch accuracy extremely important — that is, maximizing the percentage of prefetched pages that end up being viewed by the user. Incorrect prefetching can lead to inefficiencies and unneeded costs, such as caching resources that aren't requested, or wasting bandwidth and network resources, which is especially critical on metered mobile networks or in low-bandwidth environments.
    
      Guardrails
      
    
    With the initial release of Speed Brain, we have designed a solution with important side effect prevention guardrails that completely removes the chance of stale prefetch configuration, and minimizes the risk of incorrect prefetching. This opinionated configuration is achieved by leveraging the document rules and eagerness settings from the Speculation Rules API. Our chosen configuration looks like the following:
            {
  "prefetch": [{
    "source": "document",
    "where": {
      "and": [
        { "href_matches": "/*", "relative_to": "document" },
      ]
    },
    "eagerness": "conservative"
  }]
}

            
    
      
Document Rules
      
    
    Document Rules, indicated by "source": "document" and the "where" key in the configuration, allows prefetching to be applied dynamically over the entire web page. This eliminates the need for a predefined static URL list for prefetching. Hence, we remove the problem of stale prefetch configuration as prefetch candidate links are determined based on the active page structure.Our use of "relative_to": "document" in the where clause instructs the browser to limit prefetching to same-site links. This has the added bonus of allowing our implementation to avoid cross-origin prefetches to avoid any privacy implications for users, as it doesn’t follow them around the web. 
    
      Eagerness
      
    
    Eagerness controls how aggressively the browser prefetches content. There are four possible settings:immediate: Used as soon as possible on page load — generally as soon as the rule value is seen by the browser, it starts prefetching the next page.eager: Identical to immediate setting above, but the prefetch trigger additionally relies on slight user interaction events, such as moving the cursor towards the link (coming soon).moderate: Prefetches if you hold the pointer over a link for more than 200 milliseconds (or on the pointerdown event if that is sooner, and on mobile where there is no hover event).conservative: Prefetches on pointer or touch down on the link.Our initial release of Speed Brain makes use of the conservative eagerness value to minimize the risk of incorrect prefetching, which can lead to unintended resource waste while making your websites noticeably faster. While we lose out on the potential performance improvements that the more aggressive eagerness settings offer, we chose this cautious approach to prioritize safety for our users. Looking ahead, we plan to explore more dynamic eagerness settings for sites that could benefit from a more liberal setting, and we'll also expand our rules to include prerendering.Another important safeguard we implement is to only accept prefetch requests for static content that is already stored in our CDN cache. If the content isn't in the cache, we reject the prefetch request. Retrieving content directly from our CDN cache for prefetching requests lets us bypass concerns about their cache eligibility. The rationale for this is straightforward: if a page is not eligible for caching, we don't want it to be prefetched in the browser cache, as it could lead to unintended consequences and increased origin load. For instance, prefetching a logout page might log the user out prematurely before the user actually finishes their action. Stateful prefetching or prerendering requests can have unpredictable effects, potentially altering the server's state for actions the client has not confirmed. By only allowing prefetching for pages already in our CDN cache, we have confidence those pages will not negatively impact the user experience.These guardrails were implemented to work in performance-sensitive environments. We measured the impact of our baseline conservative deployment model on all pages across Cloudflare’s developer documentation in early July 2024. We found that we were able to prefetch the correct content, content that would in fact be navigated to by the users, 94% of the time. We did this while improving the performance of the navigation by reducing LCP at p75 quantile by 40% without inducing any unintended side effects. The results were amazing!
    
      Explaining Cloudflare’s implementation 
      
    
    Our global network spans over 330 cities and operates within 50 milliseconds of 95% of the Internet-connected population. This extensive reach allows us to significantly improve the performance of cacheable assets for our customers. By leveraging this network for smart prefetching with Speed Brain, Cloudflare can serve prefetched content directly from the CDN cache, reducing network latency to practically instant.Our unique position on the network provides us the leverage to automatically enable Speed Brain without requiring any changes from our customers to their origin server configurations. It's as simple as flipping a switch! Our first version of Speed Brain is now live.
          
          
          Upon receiving a request for a web page with Speed Brain enabled, the Cloudflare server returns an additional "Speculation-Rules" HTTP response header. The value for this header is a URL that hosts an opinionated Rules configuration (as mentioned above).When the browser begins parsing the response header, it fetches our Speculation-Rules configuration, and loads it as part of the web page.The configuration guides the browser on when to prefetch the next likely page from Cloudflare that the visitor may navigate to, based on how the visitor is engaging with the page.When a user action (such as mouse down event on the next page link) triggers the Rules application, the browser sends a prefetch request for that page with the "sec-purpose: prefetch" HTTP request header.Our server parses the request header to identify the prefetch request. If the requested content is present in our cache, we return it; otherwise, we return a 503 HTTP status code and deny the prefetch request. This removes the risk of unsafe side-effects of sending requests to origins or Cloudflare Workers that are unaware of prefetching. Only content present exclusively in the cache is returned.On a success response, the browser successfully prefetches the content in memory, and when the visitor navigates to that page, the browser directly loads the web page from the browser cache for immediate rendering.
    
      Common troubleshooting patterns 
      
    
    Support for Speed Brain relies on the Speculation Rules API, an emerging web standard. As of September 2024, support for this emerging standard is limited to Chromium-based browsers (version 121 or later), such as Google Chrome and Microsoft Edge. As the web community reaches consensus on API standardization, we hope to see wider adoption across other browser vendors.Prefetching by nature does not apply to dynamic content, as the state of such content can change, potentially leading to stale or outdated data being delivered to the end user as well as increased origin load. Therefore, Speed Brain will only work for non-dynamic pages of your website that are cached on our network. It has no impact on the loading of dynamic pages. To get the most benefit out of Speed Brain, we suggest making use of cache rules to ensure that all static content (especially HTML content) on your site is eligible for caching.When the browser receives a 503 HTTP status code in response to a speculative prefetch request (marked by the sec-purpose: prefetch header), it cancels the prefetch attempt. Although a 503 error appearing in the browser's console may seem alarming, it is completely harmless for prefetch request cancellation. In our early tests, the 503 response code has caused some site owners concern. We are working with our partners to iterate on this to improve the client experience, but for now follow the specification guidance, which suggests a 503 response for the browser to safely discard the speculative request. We're in active discussions with Chrome, based on feedback from early beta testers, and believe a new non-error dedicated response code would be more appropriate, and cause less confusion. In the meantime, 503 response logs for prefetch requests related to Speed Brain are harmless. If your tooling makes ignoring these requests difficult, you can temporarily disable Speed Brain until we work out something better with the Chrome Team.Additionally, when a website uses both its own custom Speculation Rules and Cloudflare's Speed Brain feature, both rule sets can operate simultaneously. Cloudflare’s guardrails will limit speculation rules to cacheable pages, which may be an unexpected limitation for those with existing implementations. If you observe such behavior, consider disabling one of the implementations for your site to ensure consistency in behavior. Note that if your origin server responses include the Speculation-Rules header, it will not be overridden. Therefore, the potential for ruleset conflicts primarily applies to predefined in-line speculation rules.
    
      How can I see the impact of Speed Brain?
      
    
    In general, we suggest that you use Speed Brain and most other Cloudflare performance features with our RUM performance measurement tool enabled. Our RUM feature helps developers and website operators understand how their end users are experiencing the performance of their application, providing visibility into:Loading: How long did it take for content to become available?Interactivity: How responsive is the website when users interact with it?Visual stability: How much does the page move around while loading?With RUM enabled, you can navigate to the Web Analytics section in the dashboard to see important information about how Speed Brain is helping reduce latency in your core web vitals metrics like Largest Contentful Paint (LCP) and load time. 
          
          
          Example RUM dashboard for a website with a high amount of prefetchable content that enabled Speed Brain around September 16.
    
      What have we seen in our rollout so far? 
      
    
    We have enabled this feature by default on all free plans and have observed the following:
    
      Domains
      
    
    Cloudflare currently has tens of millions of domains using Speed Brain. We have measured the LCP at the 75th quantile (p75) for these sites and found an improvement for these sites between 40% and 50% (average around 45%). We found this improvement by comparing navigational prefetches to normal (non-prefetched) page loads for the same set of domains. 
          
          
          
    
      Requests
      
    
    Before Speed Brain is enabled, the p75 of free websites on Cloudflare experience an LCP around 2.2 seconds. With Speed Brain enabled, these sites see significant latency savings on LCP. In aggregate, Speed Brain saves about 0.88 seconds on the low end and up to 1.1 seconds on each successful prefetch! 
    
      Applicable browsers
      
    
    Currently, the Speculation Rules API is only available in Chromium browsers. From Cloudflare Radar, we can see that approximately 70% of requests from visitors are from Chromium (Chrome, Edge, etc) browsers.
    
      Across the network
      
    
    Cloudflare sees hundreds of billions of requests for HTML content each day. Of these requests, about half are cached (make sure your HTML is cacheable!). Around 1% of those requests are for navigational prefetching made by the visitors. This represents significant savings every day for visitors to websites with Speed Brain enabled. Every 24 hours, Speed Brain can save more than 82 years worth of latency!
          
          
          
    
      What’s next? 
      
    
    What we’re offering today for Speed Brain is only the beginning. Heading into 2025, we have a number of exciting additions to explore and ship. 
    
      Leveraging Machine Learning
      
    
    Our unique position on the Internet provides us valuable insights into web browsing patterns, which we can leverage for improving web performance while maintaining individual user privacy. By employing a generalized data-driven machine learning approach, we can define more accurate and site-specific prefetch predictors for users’ pages. We are in the process of developing an adaptive speculative model that significantly improves upon our current conservative offering. This model uses a privacy-preserving method to generate a user traversal graph for each site based on same-site Referrer headers. For any two pages connected by a navigational hop, our model predicts the likelihood of a typical user moving between them, using insights extracted from our aggregated traffic data.This model enables us to tailor rule sets with custom eagerness values to each relevant next page link on your site. For pages where the model predicts high confidence in user navigation, the system will aggressively prefetch or prerender them. If the model does not provide a rule for a page, it defaults to our existing conservative approach, maintaining the benefits of baseline Speed Brain model. These signals guide browsers in prefetching and prerendering the appropriate pages, which helps speed up navigation for users, while maintaining our current safety guardrails.In lab tests, our ML model improved LCP latency by 75% and predicted visitor navigation with ~98% accuracy, ensuring the correct pages were being prefetched to prevent resource waste for users. As we move toward scaling this solution, we are focused on periodic training of the model to adapt to varying user behaviors and evolving websites. Using an online machine learning approach will drastically reduce the need for any manual update, and content drifts, while maintaining high accuracy — the Speed Brain load solution that gets smarter over time!
    
      Finer observability via RUM
      
    
    As we’ve mentioned, we believe that our RUM tools offer the best insights for how Speed Brain is helping the performance of your website. In the future, we plan on offering the ability to filter RUM tooling by navigation type so that you can compare the browser rendering of prefetched content versus non-prefetched content. 
    
      Prerendering
      
    
    We are currently offering the ability for prefetching on cacheable content. Prefetching downloads the main document resource of the page before the user’s navigation, but it does not instruct the browser to prerender the page or download any additional subresources.In the future, Cloudflare’s Speed Brain offering will prefetch content into our CDN cache and then work with browsers to know what are the best prospects for prerendering. This will help get static content even closer to instant rendering. 
    
      Argo Smart Browsing: Speed Brain & Smart Routing
      
    
    Speed Brain, in its initial implementation, provides an incredible performance boost whilst still remaining conservative in its implementation; both from an eagerness, and a resource consumption perspective.As was outlined earlier in the post, lab testing of a more aggressive model, powered by machine-learning and a higher eagerness, yielded a 75% reduction in LCP. We are investigating bundling this more aggressive, additional implementation of Speed Brain with Argo Smart Routing into a product called “Argo Smart Browsing”. Cloudflare customers will be free to continue using Speed Brain, however those who want even more performance improvement will be able to enable Argo Smart Browsing with a single button click.  With Argo Smart Browsing, not only will cacheable static content load up to 75% faster in the browser, thanks to the more aggressive models, however in times when content can’t be cached, and the request must go forward to an origin server, it will be sent over the most performant network path resulting in an average 33% performance increase. Performance optimizations are being applied to almost every segment of the request lifecycle regardless if the content is static or dynamic, cached or not. 
    
      Conclusion
      
    
    To get started with Speed Brain, navigate to Speed > Optimization > Content Optimization > Speed Brain in the Cloudflare Dashboard and enable it. That's all! The feature can also be enabled via API.  Free plan domains have had Speed Brain enabled by default.We strongly recommend that customers also enable RUM, found in the same section of the dashboard, to give visibility into the performance improvements provided by Speed Brain and other Cloudflare features and products. We’re excited to continue to build products and features that make web performance reliably fast. If you’re an engineer interested in improving the performance of the web for all, come join us!
          
          
          
    
      Watch on Cloudflare TV
      
    
    Cloudflare's connectivity cloud protects entire corporate networks, helps customers build Internet-scale applications efficiently, accelerates any website or Internet application, wards off DDoS attacks, keeps hackers at bay, and can help you on your journey to Zero Trust.Visit 1.1.1.1 from any device to get started with our free app that makes your Internet faster and safer.To learn more about our mission to help build a better Internet, start here. If you're looking for a new career direction, check out our open positions.Birthday WeekSpeed & ReliabilityResearchCacheSpeed BrainProduct News]]></content:encoded>
        </item>
    </channel>
</rss>