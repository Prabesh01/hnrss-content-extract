<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sat, 06 Sep 2025 21:27:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[Utah's hottest new power source is 15k feet below the ground]]></title>
            <link>https://www.gatesnotes.com/utahs-hottest-new-power-source-is-below-the-ground</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152569</guid>
        </item>
        <item>
            <title><![CDATA[How often do health insurers say no to patients?]]></title>
            <link>https://www.propublica.org/article/how-often-do-health-insurers-deny-patients-claims</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152403</guid>
            <description><![CDATA[Insurers’ denial rates — a critical measure of how reliably they pay for customers’ care — remain mostly secret to the public. Federal and state regulators have done little to change that.]]></description>
            <content:encoded><![CDATA[

                    


        
                    
                

                                                
            ProPublica is a nonprofit newsroom that investigates abuses of power. Sign up to receive our biggest stories as soon as they’re published.

                

            
        
        
        




                    
It’s one of the most crucial questions people have when deciding which health plan to choose: If my doctor orders a test or treatment, will my insurer refuse to pay for it?
        
    
                    
After all, an insurance company that routinely rejects recommended care could damage both your health and your finances. The question becomes ever more pressing as many working Americans see their premiums rise as their benefits shrink.
        
    
                    
Yet, how often insurance companies say no is a closely held secret. There’s nowhere that a consumer or an employer can go to look up all insurers’ denial rates — let alone whether a particular company is likely to decline to pay for procedures or drugs that its plans appear to cover.
        
    
                    
The lack of transparency is especially galling because state and federal regulators have the power to fix it, but haven’t.
        
    
                    
ProPublica, in collaboration with The Capitol Forum, has been examining the hidden world of insurance denials. A previous story detailed how one of the nation’s largest insurers flagged expensive claims for special scrutiny; a second story showed how a different top insurer used a computer program to bulk-deny claims for some common procedures with little or no review.
        
    
                        
        
    
                    
The findings revealed how little consumers know about the way their claims are reviewed — and denied — by the insurers they pay to cover their medical costs.

When ProPublica set out to find information on insurers’ denial rates, we hit a confounding series of roadblocks.
        
    
                    
In 2010, federal regulators were granted expansive authority through the Affordable Care Act to require that insurers provide information on their denials. This data could have meant a sea change in transparency for consumers. But more than a decade later, the federal government has collected only a fraction of what it’s entitled to. And what information it has released, experts say, is so crude, inconsistent and confusing that it’s essentially meaningless.
        
    
                    
The national group for state insurance commissioners gathers a more detailed, reliable trove of information. Yet, even though commissioners’ primary duty is to protect consumers, they withhold nearly all of these details from the public. ProPublica requested the data from every state’s insurance department, but none provided it.
        
    
                    
    
        
    
                    
Two states collect their own information on denials and make it public, but their data covers only a tiny subset of health plans serving a small number of people.

The minuscule amount of details available about denials robs consumers of a vital tool for comparing health plans.
        
    
                    
“This is life and death for people: If your insurance won’t cover the care you need, you could die,” said Karen Pollitz, a senior fellow at KFF (formerly known as the Kaiser Family Foundation) who has written repeatedly about the issue. “It’s all knowable. It’s known to the insurers, but it is not known to us.”
        
    
                    
The main trade groups for health insurance companies, AHIP (formerly known as America’s Health Insurance Plans) and the Blue Cross Blue Shield Association, say the industry supports transparency and complies with government disclosure requirements. Yet the groups have often argued against expanding this reporting, saying the burdens it would impose on insurance companies would outweigh the benefits for consumers.
        
    
                        
        
    
                    
“Denial rates are not directly comparable from one health plan to another and could lead consumers to make inaccurate conclusions on the robustness of the health plan,” Kelly Parsons, director of media relations for the Blue Cross Blue Shield Association, said in an email.
        
    
                    
The trade groups stress that a substantial majority of patient claims are approved and that there can be good reasons — including errors and incomplete information from doctors — for some to be denied.
        
    
                    
“More abstract data about percentages of claims that are approved or denied have no context and are not a reliable indicator of quality — it doesn’t address why a claim was or was not approved, what happened after the claim was not approved the first time, or how a patient or their doctor can help ensure a claim will be approved,” AHIP spokesperson Kristine Grow said in a written response to questions from ProPublica. “Americans deserve information and data that has relevance to their own personal health and circumstances.”
        
    
                    
The limited government data available suggests that, overall, insurers deny between 10% and 20% of the claims they receive. Aggregate numbers, however, shed no light on how denial rates may vary from plan to plan or across types of medical services.
        
    
                    
Some advocates say insurers have a good reason to dodge transparency. Refusing payment for medical care and drugs has become a staple of their business model, in part because they know customers appeal less than 1% of denials, said Wendell Potter, who oversaw Cigna’s communications team for more than a decade before leaving the industry in 2008 to become a consumer advocate.
        
    
                    
“That’s money left on the table that the insurers keep,” he said.
        
    
                    
At least one insurer disputes this. Potter’s former employer, Cigna, said in an email that his “unsubstantiated opinions” don’t reflect the company’s business model. In a separate written statement, Cigna said it passes on the money it saves “by lowering the cost of health care services and reducing wasteful spending” to the employers who hire it to administer their plans or insure their workers.
        
    
                    
The few morsels insurers have served up on denials stand in stark contrast to the avalanche of information they’ve divulged in recent years on other fronts, often in response to government mandates. Starting last year, for example, insurers began disclosing the prices they’ve negotiated to pay medical providers for most services.
        
    
                    
Experts say it’ll take similar mandates to make insurers cough up information on denials, in part because they fear plans with low denial rates would be a magnet for people who are already ailing.
        
    
                    
“Health plans would never do that voluntarily, would give you what their claim denial rates are, because they don’t want to attract sicker people,” said Mila Kofman, who leads the District of Columbia’s Affordable Care Act exchange and previously served as Maine’s superintendent of insurance.
        
    
                    
About 85% of people with insurance who responded to a recent KFF survey said they want regulators to compel insurers to disclose how often they deny claims. Pollitz, who co-authored a report on the survey, is a cancer survivor who vividly recalls her own experiences with insurance denials.
        
    
                    
“Sometimes it would just make me cry when insurance would deny a claim,” she said. “It was like, ‘I can’t deal with this now, I’m throwing up, I just can’t deal with this.’”
        
    
                    



    


                    
    


    

            
    

        Karen Pollitz, a senior fellow at KFF, has written repeatedly about the lack of data on how often insurance companies deny claims.
    
        
        Credit: 
        Alyssa Schukar, special to ProPublica
    
    
    
    



        
    
                    
She should have been able to learn how her plan handled claims for cancer treatment compared with other insurers, she said.

“There could be much more accountability.”



        
    
                        
        
    
                    
In September 2009, amid a roiling national debate over health care, the California Nurses Association made a startling announcement: Three of the state’s six largest health insurers had each denied 30% or more of the claims submitted to them in the first half of the year.
        
    
                    
California insurers instantly said the figures were misleading, inflated by claims submitted in error or for patients ineligible for coverage.
        
    
                    
But beyond the unexpectedly high numbers, the real surprise was that the nurses association was able to figure out the plans’ denial rates at all, by using information researchers found on the California Department of Managed Health Care’s website.
        
    
                    
At the time, no other state or federal regulatory agency was collecting or publishing details about how often private insurers denied claims, a 2009 report by the Center for American Progress found.
        
    
                    
The Affordable Care Act, passed the following year, was a game changer when it came to policing insurers and pushing them to be more transparent.
        
    
                    
The law took aim at insurers’ practice of excluding people with preexisting conditions, the most flagrant type of denial, and required companies offering plans on the marketplaces created under the law to disclose their prices and detail their benefits.
        
    
                    
A less-noticed section of the law demanded transparency from a much broader group of insurers about how many claims they turned down, and it put the Department of Health and Human Services in charge of making this information public. The disclosure requirements applied not only to health plans sold on the new marketplaces but also to the employer plans that cover most Americans.
        
    
                    
The law’s proponents in the Obama administration said they envisioned a flow of accurate, timely information that would empower consumers and help regulators spot problematic insurers or practices.

That’s not what happened.
        
    
                    
The federal government didn’t start publishing data until 2017 and thus far has only demanded numbers for plans on the federal marketplace known as Healthcare.gov. About 12 million people get coverage from such plans — less than 10% of those with private insurance. Federal regulators say they eventually intend to compel health plans outside the Obamacare exchanges to release details about denials, but so far have made no move to do so.
        
    
                    
Within the limited universe of Healthcare.gov, KFF’s analyses show that insurers, on average, deny almost 1 in 5 claims and that each year some reject more than 1 in 3.
        
    
                    
But there are red flags that suggest insurers may not be reporting their figures consistently. Companies’ denial rates vary more than would be expected, ranging from as low as 2% to as high as almost 50%. Plans’ denial rates often fluctuate dramatically from year to year. A gold-level plan from Oscar Insurance Company of Florida rejected 66% of payment requests in 2020, then turned down just 7% in 2021. That insurer’s parent company, Oscar Health, was co-founded by Joshua Kushner, the younger brother of former President Donald Trump’s son-in-law Jared Kushner.
        
    
                        
        
    
                    
An Oscar Health spokesperson said in an email that the 2020 results weren’t a fair reflection of the company’s business “for a variety of reasons,” but wouldn’t say why. “We closely monitor our overall denial rates and they have remained comfortably below 20% over the last few years, including the 2020-2021 time period,” the spokesperson wrote.
        
    
                    
Experts say they can’t tell if insurers with higher denial rates are counting differently or are genuinely more likely to leave customers without care or stuck with big bills.
        
    
                    
“It’s not standardized, it’s not audited, it’s not really meaningful,” Peter Lee, the founding executive director of California’s state marketplace, said of the federal government’s information. Data, he added, “should be actionable. This is not by any means right now.”
        
    
                    
Officials at the Centers for Medicare & Medicaid Services, which collects the denial numbers for the federal government, say they’re doing more to validate them and improve their quality. It’s notable, though, that the agency doesn’t use this data to scrutinize or take action against outliers.
        
    
                    
“They’re not using it for anything,” Pollitz said.

Pollitz has co-authored four reports that call out the data’s shortcomings. An upshot of all of them: Much of what consumers would most want to know is missing.
        
    
                    
The federal government provides numbers on insurers’ denials of claims for services from what the industry calls “in-network” medical providers, those who have contracts with the insurer. But it doesn’t include claims for care outside those networks. Patients often shoulder more costs for out-of-network services, ramping up the import of these denials.
        
    
                    
In recent years, doctors and patients have complained bitterly that insurers are requiring them to get approval in advance for an increasing array of services, causing delays and, in some instances, harm. The government, however, hasn’t compelled insurers to reveal how many requests for prior authorization they get or what percent they deny.
        
    
                    
These and other specifics — particularly about which procedures and treatments insurers reject most — would be necessary to turn the government’s data into a viable tool to help consumers choose health plans, said Eric Ellsworth, the director of health data strategy at Consumers' Checkbook, which designs such tools.
        
    
                    
A spokesperson for CMS said that, starting in plan year 2024, the agency will require insurers offering federal marketplace plans to submit a few more numbers, including on out-of-network claims, but there’s no timeline yet for much of what advocates say is necessary.
        
    
                    

Another effort, launched by a different set of federal regulators, illustrates the resistance that government officials encounter when they consider demanding more.
        
    
                    
The U.S. Department of Labor regulates upwards of 2 million health plans, including many in which employers pay directly for workers’ health care coverage rather than buying it from insurance companies. Roughly two-thirds of American workers with insurance depend on such plans, according to KFF.
        
    
                        
        
    
                    
In July 2016, an arm of the Labor Department proposed rules requiring these plans to reveal a laundry list of never-before-disclosed information, including how many claims they turned down.
        
    
                    
In addition, the agency said it was considering whether to demand the dollar amount of what the denied care cost, as well as a breakdown of the reasons why plans turned down claims or denied behavioral health services.
        
    
                    
The disclosures were necessary to “remedy the current failure to collect data about a large sector of the health plan market,” as well as to satisfy mandates in the Affordable Care Act and provide critical information for agency oversight, a Labor Department factsheet said.
        
    
                    
Trade groups for employers, including retailers and the construction industry, immediately pushed back.
        
    
                    
The U.S. Chamber of Commerce said complying with the proposal would take an amount of work not justified by “the limited gains in transparency and enforcement ability.” The powerful business group made it sound like having to make the disclosures could spark insurance Armageddon: Employers might cut back benefits or “eliminate health and welfare benefits altogether.”
        
    
                    
Trade groups for health insurance companies, which often act as administrators for employers that pay directly for workers’ health care, joined with business groups to blast the proposal. The Blue Cross Blue Shield Association called the mandated disclosures “burdensome and expensive.” AHIP questioned whether the Labor Department had the legal authority to collect the data and urged the agency to withdraw the idea “in its entirety.”
        
    
                    
The proposal also drew opposition from another, less expected quarter: unions. Under some collective bargaining agreements, unions co-sponsor members’ health plans and would have been on the hook for the new reporting requirements, too. The AFL-CIO argued the requirements created a higher standard of disclosure for plans overseen by the Labor Department. To be fair and avoid confusion, the group said, the Labor Department should put its rules on ice until federal health regulators adopted equivalent ones for plans this proposal didn’t cover.
        
    
                    
That left the transparency push without political champions on the left or the right, former Assistant Secretary of Labor Phyllis Borzi, who ran the part of the agency that tried to compel more disclosure, said in a recent interview.
        
    
                    
“When you’re up against a united front from the industry, the business community and labor, it’s really hard to make a difference,” she said.

By the time the Labor Department stopped accepting feedback, Donald Trump had been elected president.
        
    
                    
One trade association for large employers pointed out that the Affordable Care Act, which partly drove the new rules, was “a law that the incoming Administration and the incoming leadership of the 115th Congress have vowed to repeal, delay, dismantle, and otherwise not enforce.”
        
    
                    
The law managed to survive the Trump administration, but the Labor Department’s transparency push didn’t. The agency withdrew its proposal in September 2019.

A Labor Department spokesperson said the Biden administration has no immediate plan to revive it.



        
    
                        
        
    
                    
Ultimately, it’s the National Association of Insurance Commissioners, a group for the top elected or appointed state insurance regulators, that has assembled the most robust details about insurance denials.
        
    
                    
The association’s data encompasses more plans than the federal information, is more consistent and captures more specifics, including numbers of out-of-network denials, information about prior authorizations and denial rates for pharmacy claims. All states except New York and North Dakota participate.
        
    
                    
Yet, consumers get almost no access. The commissioners’ association only publishes national aggregate statistics, keeping the rest of its cache secret.
        
    
                    
When ProPublica requested the detailed data from each state’s insurance department, none would hand it over. More than 30 states said insurers had submitted the information under the authority commissioners are granted to examine insurers’ conduct. And under their states’ codes, they said, examination materials must be kept confidential.
        
    
                    
The commissioners association said state insurance regulators use the information to compare companies, flag outliers and track trends.
        
    
                    
Birny Birnbaum, a longtime insurance watchdog who serves on the group’s panel of consumer representatives, said the association’s approach reflects how state insurance regulators have been captured by the insurance industry’s demands for secrecy.
        
    
                    
“Many seem to view their roles as protectors of industry information, as opposed to enforcers of public information laws,” Birnbaum said in an email.
        
    
                    
Connecticut and Vermont compile their own figures and make them publicly accessible. Connecticut began reporting information on denials first, adding these numbers to its annual insurer report card in 2011.
        
    
                    
Vermont demands more details, requiring insurers that cover more than 2,000 Vermonters to publicly release prior authorization and prescription drug information that is similar to what the state insurance commissioners collect. Perhaps most usefully, insurers have to separate claims denied because of administrative problems — many of which will be resubmitted and paid — from denials that have “member impact.” These involve services rejected on medical grounds or because they are contractually excluded.
        
    
                    
Mike Fisher, Vermont’s state health care advocate, said there’s little indication consumers or employers are using the state’s information, but he still thinks the prospect of public scrutiny may have affected insurers’ practices. The most recent data shows Vermont plans had denial rates between 7.7% and 10.26%, considerably lower than the average for plans on Healthcare.gov.
        
    
                    
“I suspect that’s not a coincidence,” Fisher said. “Shining a light on things helps.”
        
    
                    
Despite persistent complaints from insurers that Vermont’s requirements are time-consuming and expensive, no insurers have left the state over it. “Certainly not,” said Sebastian Arduengo, who oversees the reporting for the Vermont Department of Financial Regulation.
        
    
                        
        
    
                    
In California, once considered the most transparent state, the Department of Managed Health Care in 2011 stopped requiring insurance carriers to specify how many claims they rejected.

A department spokesperson said in an email that the agency follows the requirements in state law, and the law doesn’t require health plans to disclose denials.
        
    
                                  
        
    
                    
The state posts reports that flag some plans for failing to pay claims fairly and on time. Consumers can use those to calculate bare-bones denial rates for some insurers, but for others, you’d have to file a public records request to get the details needed to do the math.
        
    
                    
Despite the struggles of the last 15 years, Pollitz hasn’t given up hope that one day there will be enough public information to rank insurers by their denial rates and compare how reliably they provide different services, from behavioral health to emergency care.
        
    
                    
“There’s a name and shame function that is possible here,” she said. “It holds some real potential for getting plans to clean up their acts.”
        
    
                    
                        Kirsten Berg contributed research. David Armstrong and Patrick Rucker contributed reporting.

        
        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Over 80% of Sunscreen Performed Below Their Labelled Efficacy]]></title>
            <link>https://www.consumer.org.hk/en/press-release/528-sunscreen-test</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152374</guid>
            <description><![CDATA[The Consumer Council keens to be the trusted voice of consumers in striving to build in the market an environment of safe, fair and sustainable consumption.]]></description>
            <content:encoded><![CDATA[The use of effective sunscreen can reduce the harm caused to the skin by ultraviolet rays (UV) and slow down skin aging. The Consumer Council tested 30 models of sunscreen for daily use and over 80% of them were found to perform below their respective labelled efficacy. The measured sunscreen efficacy of 4 models were below SPF15, of which 2 were sunscreen products with very high protection i.e. labelled with SPF50+. Among the 23 models using the “PA System” which is commonly adopted by Asian countries to denote the UVA protection efficacy, only 7 were measured with an UVA Protection Factor (UVAPF) value met with their labelled PA levels. In addition, only 19 models stated the major ingredients on their packaging and consumers may not be able to identify possible allergens as a result. The Council urges manufacturers to critically review their production technology and processes, and to accurately label its product efficacy as well as to provide clear product information and usage guidelines. If consumers engage in outdoor activities for a prolonged period and use sunscreens with insufficient protection will possibly increase their risks of skin darkening or sunburn, and even skin cancer. UVA emits from the sun may lead to skin aging, create wrinkles, darken skin colour, and may even induce skin cancer. However, internationally there is no unified system for product labelling of UVA protection, yet “PA System” is commonly adopted by Asian countries. UVB as ultraviolet rays with a higher energy level, can destroy DNA on skin surface, causing sunburn and is one of the main reasons of skin cancer. Currently, the Sun Protection Factor (SPF) index is an internationally recognised system to indicate the level of UVB protection in sunscreen products, the higher the value, the longer the protection offered against UVB.Among the 30 daily-use sunscreen models tested, their price ranged from $80 to $550, i.e. $0.7 to $16.1 per g/ml, marking a difference of 23 times. 14 of them belonged to high protection and were labelled from SPF30 to SPF50 while the remaining 16 models belonged to very high protection and were labelled as SPF50+. 23 models showed their UVA protection ratings by “PA System”. The test result revealed the second cheapest model ($85) scored the highest 5 points in overall performance but the most expensive model ($550) only rated 3.5 points, indicating once again that there is no correlation between the price and product quality.Currently, Hong Kong has no legislation or standard in regulating both SPF and UVA efficacy in products. Taking reference to the Cosmetics Regulation in the European Union (EU), this test covered SPF test and UVA protection test, as well as reviewing the labelling of each model.According to the product labelling requirements of the EU Cosmetics Regulation, SPF labelling on sunscreens must meet 3 criteria, including passing the in vivo test of the related SPF; the measured UVAPF value reaching one-third or above of SPF; and the measured critical wavelength should be 370nm or above. The in vivo SPF test applied a fixed amount of the models on the skin of the back of 10 trial users before they were exposed to UV light. The SPF value of each model was calculated based on the erythema reactions measured on skin surface within 24 hours. Sunscreen labelled with SPF50+ should reach a measured SPF value of 60 or above, whereas products of SPF30 should reach a measured SPF value between 30 to 49.9. For the UVA blocking protection test, the UVA efficacy and the critical wavelength were calculated by detecting the penetration rate of UV light source through the special plastic film simulating human skin after applying the sunscreen models.SPF test results revealed only 4 sunscreen models labelled with high protection (SPF30 to SPF50) fully complied with the efficacy labelling requirement under the EU Cosmetic Regulation. In the 14 models, 8 were measured with SPF value below their claims in the in vivo test. 1 model labelled as SPF30 had the largest discrepancy with its measured SPF value of only 9.8. Although the SPF values measured in the other 6 models were higher than or equal to their claims, the UVAPF value in 2 of them were only 8.0 and 4.0 respectively, failing to meet the requirement that UVAPF value need to be one-third of its SPF, and were therefore not in compliance with the labelling requirements of the EU.In the 16 models labelled with very high protection (SPF50+), only 1 fully complied with the EU requirement. The measured SPF value in 14 of them were below SPF60, of which the lowest performing 2 were recorded with a measured value of just 11.7 and 14.3 respectively. The 2 models with the highest SPF values reached 87.2 and 61.7 respectively, but the UVAPF and critical wavelength of 1 of them could not meet the relevant criteria.Unlike UVB, there is no unified international system for labelling UVA protection efficacy in products. The Council thus rated such efficacy of all models by converting the UVAPF values measured into the “PA system” which is commonly adopted by Asian countries. All 30 tested models were detected with different degrees of UVA protection with the measured UVAPF values ranging between 3.3 to 67.3, whereas UVAPF values of 9 of them were above 16, which were roughly the highest level in the PA system (i.e. PA++++) while another 10 models were rated at PA+++.As for product labelling, 6 models listed their ingredients in Japanese only and general consumers may not be able to identify possible allergens or apply the products correctly. Suggested usage quantity cannot be found in 21 models. If consumers apply insufficient amount of sunscreens, they may incur the risk of inadequate protection. Moreover, 3 models were not marked with any expiry date. The Council urges manufacturers to improve product labelling. On the other hand, the Council reminds that some sunscreen products may have high water content level, once the product has reached the expiry date, preservative may lose its effectiveness, and this could accelerate bacterial and microbial growth. These products should be used up well before the expiry date after opening.Consumers should try to avoid exposing their skin under direct sunlight to minimise the harm to the skin caused by UV radiation. When purchasing and using sunscreens, consumers need to be aware of the following:In selecting sunscreen products, read the labels carefully to check the presence of allergens. Consumers with skin allergies or eczema should consider sunscreens with physical filters to reduce the risk of allergy;Sunscreens of physical filters are relatively mild and less likely to cause allergy but are relatively whiter in colour and more viscous in texture, thus it is harder to be applied evenly. While those with chemical filters are thinner and give a lighter feeling after application, they may pose a greater risk in skin and eyes irritation, thus resulting in allergy more easily;Sunscreens with SPF50 are basically adequate in providing 98% protection to the skin while those with a larger SPF value may instead clog up pores or cause skin allergy. Thus, for normal use, it is not necessary to look for sunscreen products with a very high SPF value;Make reference to the UV index announced by the Hong Kong Observatory before going out for outdoor activities, and choose appropriate sunscreens according to the UV index, type and duration of their activities;Apply sunscreen according to the product label, normally it is around 1 teaspoon for face and should be re-applied every 2 to 3 hours to ensure sufficient protection to the skin;Sunscreen should be cleansed by make-up remover or facial cleanser according to the packaging instruction to prevent any residue from affecting skin’s health;Pay attention to the product expiry date, disposal is necessary if the product is expired to avoid the risks of microbial growth upon contact with air or skin.The Consumer Council reserves all its right (including copyright) in respect of CHOICE magazine and Online CHOICE]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Europe enters the exascale supercomputing league with Jupiter]]></title>
            <link>https://ec.europa.eu/commission/presscorner/detail/en/ip_25_2029</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152369</guid>
        </item>
        <item>
            <title><![CDATA[Show HN: Greppers – fast CLI cheat sheet with instant copy and shareable search]]></title>
            <link>https://www.greppers.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152086</guid>
            <description><![CDATA[Clean, fast CLI commands with real‑world examples and instant copy.]]></description>
            <content:encoded><![CDATA[
    
      Stop Googling the same command twice.    A tiny, blazing‑fast directory of CLI commands with copy‑ready examples. Offline friendly. No BS.      
      Try:   
    

    

    
        Help grow the community!
        Know a useful command or recipe that's missing? Help other developers by contributing.
        Suggest a Command
      

    
      
    

    
      Built for speed and memory.
      
        Instant search: Runs entirely in your browser.
        Copy‑to‑clipboard: One click, no ceremony.
        Opinionated examples: Real‑world flags and patterns.
        Keyboard first: / focuses search, ↑↓ navigate, ⏎ copies.
      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How the "Kim" dump exposed North Korea's credential theft playbook]]></title>
            <link>https://dti.domaintools.com/inside-the-kimsuky-leak-how-the-kim-dump-exposed-north-koreas-credential-theft-playbook/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152066</guid>
            <description><![CDATA[A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor's operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.]]></description>
            <content:encoded><![CDATA[
                                
Contents:Part I: Technical AnalysisPart II: Goals AnalysisPart III: Threat Intelligence Report



Executive Summary



A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor’s operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.



Screen shot of the adversary’s desktop VM



This report is broken down into three parts: 




Technical Analysis of the dump materials



Motivation and Goals of the APT actor (group)



A CTI report compartment for analysts




While this leak only gives a partial idea of what the Kimusky/PRC activities have been, the material provides insight into the expansion of activities, nature of the actor(s), and goals they have in their penetration of the South Korean governmental systems that would benefit not only DPRK, but also PRC.



Phrack article



Without a doubt, there will be more coming out from this dump in the future, particularly if the burned assets have not been taken offline and access is still available, or if others have cloned those assets for further analysis. We may revisit this in the future if additional novel information comes to light.



Part I: Technical Analysis



The Leak at a Glance



The leaked dataset attributed to the “Kim” operator offers a uniquely operational perspective into North Korean-aligned cyber operations. Among the contents were terminal history files revealing active malware development efforts using NASM (Netwide Assembler), a choice consistent with low-level shellcode engineering typically reserved for custom loaders and injection tools. These logs were not static forensic artifacts but active command-line histories showing iterative compilation and cleanup processes, suggesting a hands-on attacker directly involved in tool assembly.



File list of dump



In parallel, the operator ran OCR (Optical Character Recognition) commands against sensitive Korean PDF documents related to public key infrastructure (PKI) standards and VPN deployments. These actions likely aimed to extract structured language or configurations for use in spoofing, credential forgery, or internal tool emulation.



Privileged Access Management (PAM) logs also surfaced in the dump, detailing a timeline of password changes and administrative account use. Many were tagged with the Korean string 변경완료 (“change complete”), and the logs included repeated references to elevated accounts such as oracle, svradmin, and app_adm01, indicating sustained access to critical systems.



The phishing infrastructure was extensive. Domain telemetry pointed to a network of malicious sites designed to mimic legitimate Korean government portals. Sites like nid-security[.]com were crafted to fool users into handing over credentials via advanced AiTM (Adversary-in-the-Middle) techniques.



nid-security[.]com phishing domain (anon reg 2024)



Finally, network artifacts within the dump showed targeted reconnaissance of Taiwanese government and academic institutions. Specific IP addresses and .tw domain access, along with attempts to crawl .git repositories, reveal a deliberate focus on high-value administrative and developer targets.



Perhaps most concerning was the inclusion of a Linux rootkit using syscall hooking (khook) and stealth persistence via directories like /usr/lib64/tracker-fs. This highlights a capability for deep system compromise and covert command-and-control operations, far beyond phishing and data theft.



Artifacts recovered from the dump include:




Terminal history files demonstrating malware compilation using NASM



OCR commands parsing Korean PDF documents related to PKI and VPN infrastructure



PAM logs reflecting password changes and credential lifecycle events



Phishing infrastructure mimicking Korean government sites



IP addresses indicating reconnaissance of Taiwanese government and research institutions



Linux rootkit code using syscall hooking and covert channel deployment




Credential Theft Focus



The dump strongly emphasizes credential harvesting as a central operational goal. Key files such as 136백운규001_env.key (The presence of 136백운규001_env.key is a smoking gun indicator of stolen South Korean Government PKI material, as its structure (numeric ID + Korean name + .key) aligns uniquely with SK GPKI issuance practices and provides clear evidence of compromised, identity-tied state cryptographic keys.) This was discovered alongside plaintext passwords, that indicate clear evidence of active compromise of South Korea’s GPKI (Government Public Key Infrastructure). Possession of such certificates would allow for highly effective identity spoofing across government systems.











PAM logs further confirmed this focus, showing a pattern of administrative account rotation and password resets, all timestamped and labeled with success indicators (변경완료: Change Complete). The accounts affected were not low-privilege; instead, usernames like oracle, svradmin, and app_adm01, often used by IT staff and infrastructure services, suggested access to core backend environments.



These findings point to a strategy centered on capturing and maintaining access to privileged credentials and digital certificates, effectively allowing the attacker to act as an insider within trusted systems.




Leaked .key files (e.g., 136백운규001_env.key) with plaintext passwords confirm access to GPKI systems



PAM logs show administrative password rotations tagged with 변경완료 (change complete)



Admin-level accounts such as oracle, svradmin, and app_adm01 repeatedly appear in compromised logs




Phishing Infrastructure



The operator’s phishing infrastructure was both expansive and regionally tailored. Domains such as nid-security[.]com and webcloud-notice[.]com mimicked Korean identity and document delivery services, likely designed to intercept user logins or deploy malicious payloads. More sophisticated spoofing was seen in sites that emulated official government agencies like dcc.mil[.]kr, spo.go[.]kr, and mofa.go[.]kr.



Whoisof domains created by dysoni91@tutamail[.]com



Historical Whois of webcloud-notice[.]com



Burner email usage added another layer of operational tradecraft. The address jeder97271[@]wuzak[.]com is likely linked to phishing kits that operated through TLS proxies, capturing credentials in real time as victims interacted with spoofed login forms.



These tactics align with previously known Kimsuky behaviors but also demonstrate an evolution in technical implementation, particularly the use of AiTM interception rather than relying solely on credential-harvesting documents.



Domain connections map




Domains include: nid-security[.]com, html-load[.]com, webcloud-notice[.]com, koala-app[.]com, and wuzak[.]com



Mimicked portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Burner email evidence: jeder97271[@]wuzak[.]com



Phishing kits leveraged TLS proxies for AiTM credential capture




Malware Development Activity



Kim’s malware development environment showcased a highly manual, tailored approach. Shellcode was compiled using NASM, specifically with flags like -f win32, revealing a focus on targeting Windows environments. Commands such as make and rm were used to automate and sanitize builds, while hashed API call resolution (VirtualAlloc, HttpSendRequestA, etc.) was implemented to evade antivirus heuristics.



The dump also revealed reliance on GitHub repositories known for offensive tooling. TitanLdr, minbeacon, Blacklotus, and CobaltStrike-Auto-Keystore were all cloned or referenced in command logs. This hybrid use of public frameworks for private malware assembly is consistent with modern APT workflows.



A notable technical indicator was the use of the proxyres library to extract Windows proxy settings, particularly via functions like proxy_config_win_get_auto_config_url. This suggests an interest in hijacking or bypassing network-level security controls within enterprise environments.




Manual shellcode compilation via nasm -f win32 source/asm/x86/start.asm



Use of make, rm, and hash obfuscation of Win32 API calls (e.g., VirtualAlloc, HttpSendRequestA)



GitHub tools in use: TitanLdr, minbeacon, Blacklotus, CobaltStrike-Auto-Keystore



Proxy configuration probing through proxyres library (proxy_config_win_get_auto_config_url)




Rootkit Toolkit and Implant Structure



The Kim dump offers deep insight into a stealthy and modular Linux rootkit attributed to the operator’s post-compromise persistence tactics. The core implant, identified as vmmisc.ko (alternatively VMmisc.ko in some shells), was designed for kernel-mode deployment across multiple x86_64 Linux distributions and utilizes classic syscall hooking and covert channeling to maintain long-term undetected access.







Google Translation of Koh doc: Rootkit Endpoint Reuse Authentication Tool



“This tool uses kernel-level rootkit hiding technology, providing a high degree of stealth and penetration connection capability. It can hide while running on common Linux systems, and at the kernel layer supports connection forwarding, allowing reuse of external ports to connect to controlled hosts. Its communication behavior is hidden within normal traffic.



The tool uses binary merging technology: at compile time, the application layer program is encrypted and fused into a .ko driver file. When installed, only the .ko file exists. When the .ko driver starts, it will automatically decompress and release the hidden application-layer program.



Tools like chkrootkit, rkhunter, and management utilities (such as ps, netstat, etc.) are bypassed through technical evasion and hiding, making them unable to detect hidden networks, ports, processes, or file information.



To ensure software stability, all functions have also passed stress testing.



Supported systems: Linux Kernel 2.6.x / 3.x / 4.x, both x32 and x64 systems”.



Implant Features and Behavior



This rootkit exhibits several advanced features:




Syscall Hooking: Hooks critical kernel functions (e.g., getdents, read, write) to hide files, directories, and processes by name or PID.



SOCKS5 Proxy: Integrated remote networking capability using dynamic port forwarding and chained routing.



PTY Backdoor Shell: Spawns pseudoterminals that operate as interactive reverse shells with password protection.



Encrypted Sessions: Session commands must match a pre-set passphrase (e.g., testtest) to activate rootkit control mode.




Once installed (typically using insmod vmmisc.ko), the rootkit listens silently and allows manipulation via an associated client binary found in the dump. The client supports an extensive set of interactive commands, including:



+p              # list hidden processes



+f              # list hidden files



callrk          # load client ↔ kernel handshake



exitrk          # gracefully unload implant



shell           # spawn reverse shell



socks5          # initiate proxy channel



upload / download # file transfer interface



These capabilities align closely with known DPRK malware behaviors, particularly from the Kimsuky and Lazarus groups, who have historically leveraged rootkits for lateral movement, stealth, persistence, and exfiltration staging.



Observed Deployment



Terminal history (.bash_history) shows the implant was staged and tested from the following paths:



.cache/vmware/drag_and_drop/VMmisc.ko

/usr/lib64/tracker-fs/vmmisc.ko

Execution logs show the use of commands such as:

insmod /usr/lib64/tracker-fs/vmmisc.ko

./client 192.168.0[.]39 testtest



These paths were not random—they mimic legitimate system service locations to avoid detection by file integrity monitoring (FIM) tools.



Deployment map



This structure highlights the modular, command-activated nature of the implant and its ability to serve multiple post-exploitation roles while maintaining stealth through kernel-layer masking.



Strategic Implications



The presence of such an advanced toolkit in the “Kim” dump strongly suggests the actor had persistent access to Linux server environments, likely via credential compromise. The use of kernel-mode implants also indicates long-term intent and trust-based privilege escalation. The implant’s pathing, language patterns, and tactics (e.g., use of /tracker-fs/, use of test passwords) match TTPs previously observed in operations attributed to Kimsuky, enhancing confidence in North Korean origin.



OCR-Based Recon



A defining component of Kim’s tradecraft was the use of OCR to analyze Korean-language security documentation. The attacker issued commands such as ocrmypdf -l kor+eng “file.pdf” to parse documents like 별지2)행정전자서명_기술요건_141125.pdf (“Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf”) and SecuwaySSL U_카달로그.pdf (“SecuwaySSL U_Catalog.pdf”). These files contain technical language around digital signatures, SSL implementations, and identity verification standards used in South Korea’s PKI infrastructure.



This OCR-based collection approach indicates more than passive intelligence gathering – it reflects a deliberate effort to model and potentially clone government-grade authentication systems. The use of bilingual OCR (Korean + English) further confirms the operator’s intention to extract usable configuration data across documentation types.



OCR run on Korean PDFs




OCR commands used to extract Korean PKI policy language from PDFs such as (별지2)행정전자서명_기술요건_141125.pdf and SecuwaySSL U_카달로그.pdf

별지2)행정전자서명_기술요건_141125.pdf → (Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf



SecuwaySSL U_카달로그.pdf → SecuwaySSL U_Catalog.pdf





Command examples: ocrmypdf -l kor+eng “file.pdf”




SSH and Log-Based Evidence



The forensic evidence contained within the logs, specifically SSH authentication records and PAM outputs, provides clear technical confirmation of the operator’s tactics and target focus.



Several IP addresses stood out as sources of brute-force login attempts. These include 23.95.213[.]210 (a known VPS provider used in past credential-stuffing campaigns), 218.92.0[.]210 (allocated to a Chinese ISP), and 122.114.233[.]77 (Henan Mobile, China). These IPs were recorded during multiple failed login events, strongly suggesting automated password attacks against exposed SSH services. Their geographic distribution and known history in malicious infrastructure usage point to an external staging environment, possibly used for pivoting into Korean and Taiwanese systems.



Beyond brute force, the logs also contain evidence of authentication infrastructure reconnaissance. Multiple PAM and OCSP (Online Certificate Status Protocol) errors referenced South Korea’s national PKI authority, including domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. These errors appear during scripted or automated access attempts, indicating a potential strategy of credential replay or certificate misuse against GPKI endpoints, an approach that aligns with Kim’s broader PKI-targeting operations.



Perhaps the most revealing detail was the presence of successful superuser logins labeled with the Korean term 최고 관리자 (“Super Administrator”). This suggests the actor was not just harvesting credentials but successfully leveraging them for privileged access, possibly through cracked accounts, reused credentials, or insider-sourced passwords. The presence of such accounts in conjunction with password rotation entries marked as 변경완료 (“change complete”) further implies active control over PAM-protected systems during the operational window captured in the dump.



Together, these logs demonstrate a methodical campaign combining external brute-force access, PKI service probing, and administrative credential takeover, a sequence tailored for persistent infiltration and lateral movement within sensitive government and enterprise networks.



Brute force mapping




Brute-force IPs: 23.95.213[.]210, 218.92.0[.]210, 122.114.233[.]77




IP AddressOriginRole / Threat Context218.92.0[.]210China Telecom (Jiangsu)Part of Chinanet backbone, likely proxy or scanning node23.95.213[.]210Colocrossing (US)Frequently used in brute-force and anonymized hosting for malware ops122.114.233[.]77Presumed PRC local ISPPossibly mobile/ISP-based proxy used to obfuscate lateral movement




PAM/OCSP errors targeting gva.gpki.go[.]kr, ivs.gpki.go[.]kr



Superuser login events under 최고 관리자 (Super Administrator)




Part II: Goals Analysis



Targeting South Korea: Identity, Infrastructure, and Credential Theft



The “Kim” operator’s campaign against South Korea was deliberate and strategic, aiming to infiltrate the nation’s digital trust infrastructure at multiple levels. A central focus was the Government Public Key Infrastructure (GPKI), where the attacker exfiltrated certificate files, including .key and .crt formats, some with plaintext passwords, and attempted repeated authentication against domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. OCR tools were used to parse Korean technical documents detailing PKI and VPN architectures, demonstrating a sophisticated effort to understand and potentially subvert national identity frameworks. These efforts were not limited to reconnaissance; administrative password changes were logged, and phishing kits targeted military and diplomatic webmail, including clones of mofa.go[.]kr and credential harvesting through adversary-in-the-middle (AiTM) proxy setups.



Attempts at user account authentication



Servlet requests for KR domains



Beyond authentication systems, Kim targeted privileged accounts (oracle, unwadm, svradmin) and rotated credentials to maintain persistent administrative access, as evidenced by PAM and SSH logs showing elevated user activity under the title 최고 관리자 (“Super Administrator”). The actor also showed interest in bypassing VPN controls, parsing SecuwaySSL configurations for exploitation potential, and deployed custom Linux rootkits using syscall hooking to establish covert persistence on compromised machines. Taken together, the dump reveals a threat actor deeply invested in credential dominance, policy reconnaissance, and system-level infiltration, placing South Korea’s public sector identity systems, administrative infrastructure, and secure communications at the core of its long-term espionage objectives.



Taiwan Reconnaissance



Among the most notable aspects of the “Kim” leak is the operator’s deliberate focus on Taiwanese infrastructure. The attacker accessed a number of domains with clear affiliations to the island’s public and private sectors, including tw.systexcloud[.]com (linked to enterprise cloud solutions), mlogin.mdfapps[.]com (a mobile authentication or enterprise login portal), and the .git/ directory of caa.org[.]tw, which belongs to the Chinese Institute of Aeronautics, a government-adjacent research entity.



This last domain is especially telling. Accessing .git/ paths directly implies an attempt to enumerate internal source code repositories, a tactic often used to discover hardcoded secrets, API keys, deployment scripts, or developer credentials inadvertently exposed via misconfigured web servers. This behavior points to  more technical depth than simple phishing; it indicates supply chain reconnaissance and long-term infiltration planning.



Taiwanese target map



The associated IP addresses further reinforce this conclusion. All three, 163.29.3[.]119, 118.163.30[.]45, and 59.125.159[.]81, are registered to academic, government, or research backbone providers in Taiwan. These are not random scans; they reflect targeted probing of strategic digital assets.



Summary of Whois & Ownership Insights




118.163.30[.]45

Appears as part of the IP range used for the domain dtc-tpe.com[.]tw, linked to Taiwan’s HINET provider (118.163.30[.]46 )Site Indices page of HINET provider.





163.29.3[.]119

Falls within the 163.29.3[.]0/24 subnet identified with Taiwanese government or institutional use, notably in Taipei. This corresponds to B‑class subnets assigned to public/government entities IP地址 (繁體中文).





59.125.159[.]81

Belongs to the broader 59.125.159[.]0–59.125.159[.]254 block, commonly used by Taiwanese ISP operators such as Chunghwa Telecom in Taipei






Taken together, this Taiwan-focused activity reveals an expanded operational mandate. Whether the attacker is purely DPRK-aligned or operating within a DPRK–PRC fusion cell, the intent is clear: compromise administrative and developer infrastructure in Taiwan, likely in preparation for broader credential theft, espionage, or disruption campaigns.




Targeted domains: tw.systexcloud[.]com, caa.org[.]tw/.git/, mlogin.mdfapps[.]com



IPs linked to Taiwanese academic/government assets: 163.29.3[.]119, 118.163.30[.]45, 59.125.159[.]81



Git crawling suggests interest in developer secrets or exposed tokens




Hybrid Attribution Model



The “Kim” operator embodies the growing complexity of modern nation-state attribution, where cyber activities often blur traditional boundaries and merge capabilities across geopolitical spheres. This case reveals strong indicators of both North Korean origin and Chinese operational entanglement, presenting a textbook example of a hybrid APT model.







On one hand, the technical and linguistic evidence strongly supports a DPRK-native operator. Terminal environments, OCR parsing routines, and system artifacts consistently leverage Korean language and character sets. The operator’s activities reflect a deep understanding of Korean PKI systems, with targeted extraction of GPKI .key files and automation to parse sensitive Korean government PDF documentation. These are hallmarks of Kimsuky/APT43 operations, known for credential-focused espionage against South Korean institutions and diplomatic targets. The intent to infiltrate identity infrastructure is consistent with North Korea’s historical targeting priorities. Notably, the system time zone on Kim’s host machine was set to UTC+9 (Pyongyang Standard Time), reinforcing the theory that the actor maintains direct ties to the DPRK’s internal environment, even if operating remotely.



However, this actor’s digital footprint extends well into Chinese infrastructure. Browser and download logs reveal frequent interaction with platforms like gitee[.]com, baidu[.]com, and zhihu[.]com, highly popular within the PRC but unusual for DPRK operators who typically minimize exposure to foreign services. Moreover, session logs include simplified Chinese content and PRC browsing behaviors, suggesting that the actor may be physically operating within China or through Chinese-language systems. This aligns with longstanding intelligence on North Korean cyber operators stationed in Chinese border cities such as Shenyang and Dandong, where DPRK nationals often conduct cyber operations with tacit approval or logistical consent from Chinese authorities. These locations provide higher-speed internet, relaxed oversight, and convenient geopolitical proximity.



Browser History viewing Taiwanese and Chinese sites



The targeting of Taiwanese infrastructure further complicates attribution. Kimsuky has not historically prioritized Taiwan, yet in this case, the actor demonstrated direct reconnaissance of Taiwanese government and developer networks. While this overlaps with Chinese APT priorities, recent evidence from the “Kim” dump, including analysis of phishing kits and credential theft workflows, suggests this activity was likely performed by a DPRK actor exploring broader regional interests, possibly in alignment with Chinese strategic goals. Researchers have noted that Kimsuky operators have recently asked questions in phishing lures related to potential Chinese-Taiwanese conflicts, implying interest beyond the Korean peninsula.



Some tooling overlaps with PRC-linked APTs, particularly GitHub-based stagers and proxy-resolving modules, but these are not uncommon in the open-source malware ecosystem and may reflect opportunistic reuse rather than deliberate mimicry.



IMINT Analysis: Visual Tradecraft and Cultural Camouflage



A review of image artifacts linked to the “Kim” actor reveals a deliberate and calculated use of Chinese social and technological visual content as part of their operational persona. These images, extracted from browser history and uploads attributed to the actor, demonstrate both strategic alignment with DPRK priorities and active cultural camouflage within the PRC digital ecosystem.



Uploads of images by Kim found in browser history



Images downloaded from aixfan[.]com



The visual set includes promotional graphics for Honor smartphones, SoC chipset evolution charts, Weibo posts featuring vehicle registration certificates, meme-based sarcasm, and lifestyle imagery typical of Chinese internet users. Notably, the content is exclusively rendered in simplified Chinese, reinforcing prior assessments that the operator either resides within mainland China or maintains a working digital identity embedded in Chinese platforms. Devices and services referenced, such as Xiaomi phones, Zhihu, Weibo, and Baidu, suggest intimate familiarity with PRC user environments.



Operationally, this behavior achieves two goals. First, it enables the actor to blend in seamlessly with native PRC user activity, which complicates attribution and helps bypass platform moderation or behavioral anomaly detection. Second, the content itself may serve as bait or credibility scaffolding (e.g. A framework to give the illusion of trust to allow for easier compromise ) in phishing and social engineering campaigns, especially those targeting developers or technical users on Chinese-language platforms.



Some images, such as the detailed chipset timelines and VPN or device certification posts, suggest a continued interest in supply chain reconnaissance and endpoint profiling—both tradecraft hallmarks of Kimsuky and similar APT units. Simultaneously, meme humor, sarcastic overlays, and visual metaphors (e.g., the “Kaiju’s tail is showing” idiom) indicate the actor’s fluency in PRC netizen culture and possible mockery of operational security breaches—whether their own or others’.



Taken together, this IMINT corpus supports the broader attribution model: a DPRK-origin operator embedded, physically or virtually, within the PRC, leveraging local infrastructure and social platforms to facilitate long-term campaigns against South Korea, Taiwan, and other regional targets while maintaining cultural and technical deniability.



Attribution Scenarios:




Option A: DPRK Operator Embedded in PRC

Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.



Use of PRC infrastructure (e.g., Baidu, Gitee) and simplified Chinese content implies the operator is physically located in China or benefits from access to Chinese internet infrastructure.





Option B: PRC Operator Emulating DPRK

Taiwan-focused reconnaissance aligns with PRC cyber priorities.



Use of open-source tooling and phishing methods shared with PRC APTs could indicate tactical emulation.






The preponderance of evidence supports the hypothesis that “Kim” is a North Korean cyber operator embedded in China or collaborating with PRC infrastructure providers. This operational model allows the DPRK to amplify its reach, mask attribution, and adopt regional targeting strategies beyond South Korea, particularly toward Taiwan. As this hybrid model matures, it reflects the strategic adaptation of DPRK-aligned threat actors who exploit the permissive digital environment of Chinese networks to evade detection and expand their operational playbook.



Targeting Profiles



The “Kim” leak provides one of the clearest windows to date into the role-specific targeting preferences of the operator, revealing a deliberate focus on system administrators, credential issuers, and backend developers, particularly in South Korea and Taiwan.



In South Korea, the operator’s interest centers around PKI administrators and infrastructure engineers. The recovered OCR commands were used to extract technical details from PDF documents outlining Korea’s digital signature protocols, such as identity verification, certificate validation, and encrypted communications, components that form the backbone of Korea’s secure authentication systems. The goal appears to be not only credential theft but full understanding and potential replication of government-trusted PKI procedures. This level of targeting suggests a strategic intent to penetrate deeply trusted systems, potentially for use in later spoofing or identity masquerading operations.



PKI attack targets



In Taiwan, the operator shifted focus to developer infrastructure and cloud access portals. Specific domains accessed, like caa.org[.]tw/.git/, indicate attempts to enumerate internal repositories, most likely to discover hardcoded secrets, authentication tokens, or deployment keys. This is a classic supply chain targeting method, aiming to access downstream systems via compromised developer credentials or misconfigured services.



Additional activity pointed to interaction with cloud service login panels such as tw.systexcloud[.]com and mlogin.mdfapps[.]com. These suggest an attempt to breach centralized authentication systems or identity providers, granting the actor broader access into enterprise or government networks with a single credential set.



Taken together, these targeting profiles reflect a clear emphasis on identity providers, backend engineers, and those with access to system-level secrets. This reinforces the broader theme of the dump: persistent, credential-first intrusion strategies, augmented by reconnaissance of authentication standards, key management policies, and endpoint development infrastructure.



South Korean:




PKI admins, infrastructure engineers



OCR focus on Korean identity standards




Taiwanese:




Developer endpoints and internal .git/ repos



Access to cloud panels and login gateways




Final Assessment



The “Kim” leak represents one of the most comprehensive and technically intimate disclosures ever associated with Kimsuky (APT43) or its adjacent operators. It not only reaffirms known tactics, credential theft, phishing, and PKI compromise, but exposes the inner workings of the operator’s environment, tradecraft, and operational intent in ways rarely observed outside of active forensic investigations.



At the core of the leak is a technically competent actor, well-versed in low-level shellcode development, Linux-based persistence mechanisms, and certificate infrastructure abuse. Their use of NASM, API hashing, and rootkit deployment points to custom malware authorship. Furthermore, the presence of parsed government-issued Korean PDFs, combined with OCR automation, shows not just opportunistic data collection but a concerted effort to model, mimic, or break state-level identity systems, particularly South Korea’s GPKI.



The operator’s cultural and linguistic fluency in Korean, and their targeting of administrative and privileged systems across South Korean institutions, support a high-confidence attribution to a DPRK-native threat actor. However, the extensive use of Chinese platforms like gitee[.]com, Baidu, and Zhihu, and Chinese infrastructure for both malware hosting and browsing activity reveals a geographical pivot or collaboration: a hybrid APT footprint rooted in DPRK tradecraft but operating from or with Chinese support.



Most notably, this leak uncovers a geographical expansion of operational interest; the actor is no longer solely focused on the Korean peninsula. The targeting of Taiwanese developer portals, government research IPs, and .git/ repositories shows a broadened agenda that likely maps to both espionage and supply chain infiltration priorities. This places Taiwan, like South Korea, at the forefront of North Korean cyber interest, whether for intelligence gathering, credential hijacking, or as staging points for more complex campaigns.



The threat uncovered here is not merely malware or phishing; it is an infrastructure-centric, credential-first APT campaign that blends highly manual operations (e.g., hand-compiled shellcode, direct OCR of sensitive PDFs) with modern deception tactics such as AiTM phishing and TLS proxy abuse.



Organizations in Taiwan and South Korea, particularly those managing identity, certificate, and cloud access infrastructure, should consider themselves under persistent, credential-focused surveillance. Defensive strategies must prioritize detection of behavioral anomalies (e.g., use of OCR tools, GPKI access attempts), outbound communications with spoofed Korean domains, and the appearance of low-level toolchains like NASM or proxyres-based scanning utilities within developer or admin environments.



In short: the “Kim” actor embodies the evolution of nation-state cyber threats—a fusion of old-school persistence, credential abuse, and modern multi-jurisdictional staging. The threat is long-term, embedded, and adaptive.



Part III: Threat Intelligence Report



TLP WHITE:



Targeting Summary



The analysis of the “Kim” operator dump reveals a highly focused credential-theft and infrastructure-access campaign targeting high-value assets in both South Korea and Taiwan. Victims were selected based on their proximity to trusted authentication systems, administrative control panels, and development environments.



CategoryDetailsRegionsSouth Korea, TaiwanTargetsGovernment, Telecom, Enterprise ITAccountssvradmin, oracle, app_adm01, unwadm, shkim88, jaejung91Domainstw.systexcloud[.]com, nid-security[.]com, spo.go[.]kr, caa.org[.]tw/.git/



Indicators of Compromise (IOCs)



Domains




Phishing: nid-security[.]com, html-load[.]com, wuzak[.]com, koala-app[.]com, webcloud-notice[.]com



Spoofed portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Pastebin raw links: Used for payload staging and malware delivery




IP Addresses




External Targets (Taiwan):

163.29.3[.]119     National Center for High-performance Computing



118.163.30[.]45   Taiwanese government subnet



59.125.159[.]81   Chunghwa Telecom





Brute Forcing / Infrastructure Origins:

23.95.213[.]210   VPS provider with malicious history



218.92.0[.]210     China Unicom



122.114.233[.]77  Henan Mobile, PRC






Internal Host IPs (Operator Environment)




192.168.130[.]117



192.168.150[.]117



192.168.0[.]39




Operator Environment: Internal Host IP Narrative



The presence of internal IP addresses such as 192.168.130[.]117, 192.168.150[.]117, and 192.168.0[.]39 within the dump offers valuable insight into the attacker’s local infrastructure, an often-overlooked element in threat intelligence analysis. These addresses fall within private, non-routable RFC1918 address space, commonly assigned by consumer off-the-shelf (COTS) routers and small office/home office (SOHO) network gear.



The use of the 192.168.0[.]0/16 subnet, particularly 192.168.0.x and 192.168.150.x, strongly suggests that the actor was operating from a residential or low-profile environment, not a formal nation-state facility or hardened infrastructure. This supports existing assessments that North Korean operators, particularly those affiliated with Kimsuky, often work remotely from locations in third countries such as China or Southeast Asia, where they can maintain inconspicuous, low-cost setups while accessing global infrastructure.



Moreover, the distinction between multiple internal subnets (130.x, 150.x, and 0.x) may indicate segmentation of test environments or multiple virtual machines running within a single NATed network. This aligns with the forensic evidence of iterative development and testing workflows seen in the .bash_history files, where malware stagers, rootkits, and API obfuscation utilities were compiled, cleaned, and rerun repeatedly.



Together, these IPs reveal an operator likely working from a clandestine, residential base of operations, with modest hardware and commercial-grade routers. This operational setup is consistent with known DPRK remote IT workers and cyber operators who avoid attribution by blending into civilian infrastructure. It also suggests the attacker may be physically located outside of North Korea, possibly embedded in a friendly or complicit environment, strengthening the case for China-based activity by DPRK nationals.



MITRE ATT&CK Mapping



PhaseTechnique(s)Initial AccessT1566.002 ,  Adversary-in-the-Middle (AiTM) PhishingExecutionT1059.005 ,  Native API ShellcodeT1059.003 ,  Bash/Shell ScriptsCredential AccessT1555 ,  Credential Store DumpingT1557.003 ,  Session HijackingPersistenceT1176 ,  Rootkit (via khook syscall manipulation)Defense EvasionT1562.001 ,  Disable Security ToolsT1552 ,  Unsecured Credential FilesDiscoveryT1592 ,  Technical Information DiscoveryT1590 ,  Network InformationExfiltrationT1041 ,  Exfiltration over C2 ChannelT1567.002 ,  Exfil via Cloud Services



Tooling and Capabilities



The actor’s toolkit spans multiple disciplines, blending malware development, system reconnaissance, phishing, and proxy evasion:




NASM-based shellcode loaders: Compiled manually for Windows execution.



Win32 API hashing: Obfuscated imports via hashstring.py to evade detection.



GitHub/Gitee abuse: Tooling hosted or cloned from public developer platforms.



OCR exploitation: Used ocrmypdf to parse Korean PDF specs related to digital certificates and VPN appliances.



Rootkit deployment: Hidden persistence paths including /usr/lib64/tracker-fs and /proc/acpi/pcicard.



Proxy config extraction: Investigated PAC URLs using proxyres-based recon.




Attribution Confidence Assessment



Attribution CandidateConfidence LevelDPRK-aligned (Kimsuky)High, Native Korean targeting, GPKI focus, OCR behaviorChina-blended infrastructureModerate, PRC hosting, Gitee usage, Taiwan focusSolely PRC ActorLow-to-Moderate, Tooling overlap but weak linguistic match



Assessment: The actor appears to be a DPRK-based APT operator working from within or in partnership with Chinese infrastructure, representing a hybrid attribution model.



Defensive Recommendations



AreaRecommendationPKI SecurityMonitor usage of .key, .sig, .crt artifacts; enforce HSM or 2FA for key usePhishing DefenseBlock domains identified in IoCs; validate TLS fingerprints and referrer headersEndpoint HardeningDetect use of nasm, make, and OCR tools; monitor /usr/lib*/tracker-* pathsNetwork TelemetryAlert on .git/ directory access from external IPs; monitor outbound to Pastebin/GitHubTaiwan FocusEstablish watchlists for .tw domains targeted by PRC-originating IPsAdmin AccountsReview usage logs for svradmin, oracle, app_adm01, and ensure rotation policies



APPENDIX A



Overlap or Confusion with Chinese Threat Actors



There is notable evidence of operational blur between Kimsuky and Chinese APTs in the context of Taiwan. The 2025 “Kim” data breach revealed an attacker targeting Taiwan whose tools and phishing kits matched Kimsuky’s, yet whose personal indicators (language, browsing habits) suggested a Chinese national. Researchers concluded this actor was likely a Chinese hacker either mimicking Kimsuky tactics or collaborating with them.. In fact, the leaked files on DDoS Secrets hint that Kimsuky has “openly cooperated with other Chinese APTs and shared their tools and techniques”. This overlap can cause attribution confusion – a Taiwan-focused operation might initially be blamed on China but could involve Kimsuky elements, or vice versa. So far, consensus is that North Korean and Chinese cyber operations remain separate, but cases like “Kim” show how a DPRK-aligned actor can operate against Taiwan using TTPs common to Chinese groups, muddying the waters of attribution.



File List from dump:















Master Evidence Inventory:



File NameLanguageContent SummaryCategoryRelevance.bash_historyMixed (EN/KR)Operator shell history commandsSystem/LogShows rootkit compilation, file ops, network testsuser-bash_historyMixed (EN/KR)User-level shell commandsSystem/LogDevelopment and test activityroot-bash_historyMixed (EN/KR)Root-level shell commandsSystem/LogPrivilege-level activity, implant deploymentauth.log.2EN/KRAuthentication logs (PAM/SSH)System/LogCredential changes marked 변경완료, brute force IPs20190315.logENSystem log fileSystem/LogAuth and system access eventschrome-timeline.txtENBrowser activity timelineBrowserVisited domains extractionchromehistory.txtENBrowser history exportBrowserURLs visitedhistory.sqliteENEmpty DB fileBrowserNo useful dataMedia HistoryENEmpty SQLite DBBrowserNo playback activityHistoryENEmpty Brave/Chromium DBBrowserNo visited URLsWeb DataENAutofill/search DBBrowserSearch engines used (Google, DuckDuckGo, Qwant, Startpage, Ecosia)Visited LinksBinaryLevelDB/binary structureBrowserCould not extract URLsCookiesENSQLite DB with cookiesBrowserGoogle cookies foundrequest_log.txt.20250220ENCaptured phishing sessionPhishingSpoofed spo.go.kr, base64 credential logging技术说明书 – 22.docxZHChinese rootkit stealth manualRootkitKernel hiding, binary embedding1.ko 图文编译 .docZHChinese compilation guideRootkitRootkit build process1. build ko .txtZHBuild notesRootkitImplant compilation instructions0. 使用.txtZHUsage notesRootkitImplant usage and commandsre 正向工具修改建议 1.0.txtZHModification notesRootkitReverse tool modification suggestions1111.txtZHRootkit/tool snippetRootkitPart of implant notesclientBinaryRootkit client binaryRootkitController for implant communicationSSA_AO_AD_WT_002_웹보안 프로토콜설계서_Ver1.0_.docKRGPKI protocol design docPKIKorean web PKI standards행자부 웹보안API 인수인계.docKRGPKI API deployment manualPKIDeployment and cert API internalsHIRA-IR-T02_의약품처방조제_ComLibrary_통신전문.docKRMedical ComLibrary XML specHealthcarePrescription system communication(별지2)행정전자서명_기술요건_141125.pdfKRPKI requirements PDFPKIOCR targetSecuwaySSL U_카달로그.pdfKRVPN catalogPKI/VPNOCR targetphrack-apt-down-the-north-korea-files.pdfENPhrack articleReferenceBackground on Kimsuky dumpMuddled Libra Threat Assessment.pdfENThreat intel reportReferenceComparative threat actor studyLeaked North Korean Linux Stealth Rootkit Analysis.pdfENRootkit analysisReferenceDetailed implant studyInside the Kimsuky Leak.docx (various)ENThreat report draftsReportWorking versionsaccount (2).txtENDB export (DBsafer, TrustedOrange)InfraAccounts and DB changesresult.txtKRCert-related parsed dataInfraIncluded GPKI .key/.sigenglish_wikipedia.txtENWikipedia dumpReferenceUnrelated baselinebookmarks-2021-01-04.jsonlz4ENFirefox bookmarks (compressed)BrowserNeeds decompressionScreenshot translationsZHChinese text (rootkit marketing blurb)RootkitKernel hiding tool description




                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Historical Housing Prices Project]]></title>
            <link>https://www.philadelphiafed.org/surveys-and-data/regional-economic-analysis/historical-housing-prices</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152063</guid>
            <description><![CDATA[The Historical Housing Prices (HHP) Project at the Philadelphia Fed provides new data on the price of housing for sale and for rent over the 20th century using the real estate sections of historical newspapers. The data cover 30 major cities, including Philadelphia.]]></description>
            <content:encoded><![CDATA[
        
            

    
    
    
    
    
        
                
                    
                    About Us
                    
                
                
                    
                    Our People
                    
                
                
                    
                    Education 
                    
                
                
                    
                    Banking
                    
                
                
                    
                    Careers
                    
                
                
                    
                    Calendar of Events
                    
                
        
        
                
                    
                    The Economy
                    
                    
                            
                                    
                                        
                                        Monetary Policy
                                        
                                    
                                    
                                        
                                        Banking & Financial Markets
                                        
                                    
                                    
                                        
                                        Macroeconomics
                                        
                                    
                                    
                                        
                                        Regional Economics
                                        
                                    
                            
                
                
                    
                    Consumer Finance
                    
                    
                            
                                    
                                        
                                        Payment Systems
                                        
                                    
                                    
                                        
                                        Consumer Credit
                                        
                                    
                                    
                                        
                                        Education Finance
                                        
                                    
                                    
                                        
                                        Mortgage Markets
                                        
                                    
                            
                
                
                    
                    Community Development
                    
                    
                            
                
                
                    
                    SURVEYS & DATA
                    
                    
                            
                                    
                                        
                                        REAL-TIME DATA RESEARCH
                                        
                                    
                                    
                                        
                                        Regional Economic Analysis
                                        
                                    
                                    
                                        
                                        CONSUMER FINANCE DATA
                                        
                                    
                                    
                                        
                                        COMMUNITY DEVELOPMENT DATA
                                        
                                    
                            
                
            
                
            
        
    






        

        

            

            
        
        Housing is central to the economy of cities across the United States. The Historical Housing Prices (HHP) Project at the Philadelphia Fed provides new data on the price of housing for sale and for rent over the 20th century using the real estate sections of historical newspapers. The data cover 30 major cities, including Philadelphia.
The data set has been expanded and new series on the rental return, capital gains, and total return to owning housing are available for download.
This data page is part of our Center for the REstoration of Economic Data (CREED).
    
    Data Explorer

    

        
    
    
            
                
                    Time Series
                
            
            
                
                    Percent Change
                
            
    
    


        
        

        
        

    
        
            Slide the circles to compare the total change in housing price indices between the two years you select.
            
            
            The total data set spans 1890–2006, but data for some cities start after 1890.
        

        
            Slide the circles to see changes in housing prices over time, compared with the earliest year you select.
            
            
            The total data set spans 1890–2006, but data for some cities start after 1890.
        

        
    
    


    





    Data Downloads

    
        
            Type
            Title
            Description
        

            
                

                
                    City-Level Indices
                

                This file contains the HHP national housing price and returns series for both housing for sale and for rent.
            
            
                

                
                    National Indices
                

                This file contains the HHP national housing price and returns series for both housing for sale and for rent. 
            
    



    

        Related Content

    

    
            
                    Consumer Finance Data
                
                        Data

                    
                

                As part of the Philadelphia Fed’s efforts to understand how housing affects the economy in our District, our researchers are studying the links between past housing discrimination and present-day outcomes.
            
            
                    Consumer Finance
                
                        About

                    
                

                Welcome to CREED (Center for the REstoration of Economic Data). CREED advances research in many fields, including topics in regional economics and consumer finance, by converting information in books, images, and other formats into ready-to-use, publicly available digital data.
            
    



        About the Research
        
                Housing impacts individual economic well-being, influences wealth-building opportunities, and is vital to the overall economy. Despite the critical importance of housing to the U.S. economy, existing long-run housing price series are limited, particularly covering the years before 1970.
The Historical Housing Prices (HHP) Project, now housed at the Philadelphia Fed, began with support from the National Science Foundation (SES-1918554), the Lincoln Institute of Land Policy, and Trinity College in Dublin, with principal investigators Allison Shertzer (now at the Philadelphia Fed), Ronan C. Lyons (Trinity College Dublin), and Rowena Gray (University of California, Merced). (The Philadelphia Fed did not receive funding from external funders in connection with this project.)
As part of our efforts to ensure a strong overall economy, the Philadelphia Fed identifies housing-related issues and informs solutions.  This project aims to bring new data on the price of housing over the long run to inform research and policymaking.

        
    

        Suggested Citation:
            Federal Reserve Bank of Philadelphia.
            Historical Housing Prices Project.
            Accessed Sep 06, 2025,
            https://www.philadelphiafed.org/surveys-and-data/regional-economic-analysis/historical-housing-prices.
        


            

            

        
        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Normalization of deviance (2015)]]></title>
            <link>https://danluu.com/wat/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151661</guid>
            <description><![CDATA[Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples.]]></description>
            <content:encoded><![CDATA[ Have you ever mentioned something that seems totally normal to you only to be greeted by surprise? Happens to me all the time when I describe something everyone at work thinks is normal. For some reason, my conversation partner's face morphs from pleasant smile to rictus of horror. Here are a few representative examples. There's the company that is perhaps the nicest place I've ever worked, combining the best parts of Valve and Netflix. The people are amazing and you're given near total freedom to do whatever you want. But as a side effect of the culture, they lose perhaps half of new hires in the first year, some voluntarily and some involuntarily. Totally normal, right? Here are a few more anecdotes that were considered totally normal by people in places I've worked. And often not just normal, but laudable. There's the company that's incredibly secretive about infrastructure. For example, there's the team that was afraid that, if they reported bugs to their hardware vendor, the bugs would get fixed and their competitors would be able to use the fixes. Solution: request the firmware and fix bugs themselves! More recently, I know a group of folks outside the company who tried to reproduce the algorithm in the paper the company published earlier this year. The group found that they couldn't reproduce the result, and that the algorithm in the paper resulted in an unusual level of instability; when asked about this, one of the authors responded “well, we have some tweaks that didn't make it into the paper” and declined to share the tweaks, i.e., the company purposely published an unreproducible result to avoid giving away the details, as is normal. This company enforces secrecy by having a strict policy of firing leakers. This is introduced at orientation with examples of people who got fired for leaking (e.g., the guy who leaked that a concert was going to happen inside a particular office), and by announcing firings for leaks at the company all hands. The result of those policies is that I know multiple people who are afraid to forward emails about things like updated info on health insurance to a spouse for fear of forwarding the wrong email and getting fired; instead, they use another computer to retype the email and pass it along, or take photos of the email on their phone. There's the office where I asked one day about the fact that I almost never saw two particular people in the same room together. I was told that they had a feud going back a decade, and that things had actually improved — for years, they literally couldn't be in the same room because one of the two would get too angry and do something regrettable, but things had now cooled to the point where the two could, occasionally, be found in the same wing of the office or even the same room. These weren't just random people, either. They were the two managers of the only two teams in the office. There's the company whose culture is so odd that, when I sat down to write a post about it, I found that I'd not only written more than for any other single post, but more than all other posts combined (which is well over 100k words now, the length of a moderate book). This is the same company where someone recently explained to me how great it is that, instead of using data to make decisions, we use political connections, and that the idea of making decisions based on data is a myth anyway; no one does that. This is also the company where all four of the things they told me to get me to join were false, and the job ended up being the one thing I specifically said I didn't want to do. When I joined this company, my team didn't use version control for months and it was a real fight to get everyone to use version control. Although I won that fight, I lost the fight to get people to run a build, let alone run tests, before checking in, so the build is broken multiple times per day. When I mentioned that I thought this was a problem for our productivity, I was told that it's fine because it affects everyone equally. Since the only thing that mattered was my stack ranked productivity, so I shouldn't care that it impacts the entire team, the fact that it's normal for everyone means that there's no cause for concern. There's the company that created multiple massive initiatives to recruit more women into engineering roles, where women still get rejected in recruiter screens for not being technical enough after being asked questions like "was your experience with algorithms or just coding?". I thought that my referral with a very strong recommendation would have prevented that, but it did not. There's the company where I worked on a four person effort with a multi-hundred million dollar budget and a billion dollar a year impact, where requests for things that cost hundreds of dollars routinely took months or were denied. You might wonder if I've just worked at places that are unusually screwed up. Sure, the companies are generally considered to be ok places to work and two of them are considered to be among the best places to work, but maybe I've just ended up at places that are overrated. But I have the same experience when I hear stories about how other companies work, even places with stellar engineering reputations, except that it's me that's shocked and my conversation partner who thinks their story is normal. There's the companies that use @flaky, which includes the vast majority of Python-using SF Bay area unicorns. If you don't know what this is, this is a library that lets you add a Python annotation to those annoying flaky tests that sometimes pass and sometimes fail. When I asked multiple co-workers and former co-workers from three different companies what they thought this did, they all guessed that it re-runs the test multiple times and reports a failure if any of the runs fail. Close, but not quite. It's technically possible to use @flaky for that, but in practice it's used to re-run the test multiple times and reports a pass if any of the runs pass. The company that created @flaky is effectively a storage infrastructure company, and the library is widely used at its biggest competitor. There's the company with a reputation for having great engineering practices that had 2 9s of reliability last time I checked, for reasons that are entirely predictable from their engineering practices. This is the second thing in a row that can't be deanonymized because multiple companies fit the description. Here, I'm not talking about companies trying to be the next reddit or twitter where it's, apparently, totally fine to have 1 9. I'm talking about companies that sell platforms that other companies rely on, where an outage will cause dependent companies to pause operations for the duration of the outage. Multiple companies that build infrastructure find practices that lead to 2 9s of reliability. As far as I can tell, what happens at a lot these companies is that they started by concentrating almost totally on product growth. That's completely and totally reasonable, because companies are worth approximately zero when they're founded; they don't bother with things that protect them from losses, like good ops practices or actually having security, because there's nothing to lose (well, except for user data when the inevitable security breach happens, and if you talk to security folks at unicorns you'll know that these happen). The result is a culture where people are hyper-focused on growth and ignore risk. That culture tends to stick even after company has grown to be worth well over a billion dollars, and the companies have something to lose. Anyone who comes into one of these companies from Google, Amazon, or another place with solid ops practices is shocked. Often, they try to fix things, and then leave when they can't make a dent. Google probably has the best ops and security practices of any tech company today. It's easy to say that you should take these things as seriously as Google does, but it's instructive to see how they got there. If you look at the codebase, you'll see that various services have names ending in z, as do a curiously large number of variables. I'm told that's because, once upon a time, someone wanted to add monitoring. It wouldn't really be secure to have google.com/somename expose monitoring data, so they added a z. google.com/somenamez. For security. At the company that is now the best in the world at security. They're now so good at security that multiple people I've talked to (all of whom joined after this happened) vehemently deny that this ever happened, even though the reasons they give don't really make sense (e.g., to avoid name collisions) and I have this from sources who were there at the time this happened. Google didn't go from adding z to the end of names to having the world's best security because someone gave a rousing speech or wrote a convincing essay. They did it after getting embarrassed a few times, which gave people who wanted to do things “right” the leverage to fix fundamental process issues. It's the same story at almost every company I know of that has good practices. Microsoft was a joke in the security world for years, until multiple disastrously bad exploits forced them to get serious about security. This makes it sound simple, but if you talk to people who were there at the time, the change was brutal. Despite a mandate from the top, there was vicious political pushback from people whose position was that the company got to where it was in 2003 without wasting time on practices like security. Why change what's worked? You can see this kind of thing in every industry. A classic example that tech folks often bring up is hand-washing by doctors and nurses. It's well known that germs exist, and that washing hands properly very strongly reduces the odds of transmitting germs and thereby significantly reduces hospital mortality rates. Despite that, trained doctors and nurses still often don't do it. Interventions are required. Signs reminding people to wash their hands save lives. But when people stand at hand-washing stations to require others walking by to wash their hands, even more lives are saved. People can ignore signs, but they can't ignore being forced to wash their hands. This mirrors a number of attempts at tech companies to introduce better practices. If you tell people they should do it, that helps a bit. If you enforce better practices via code review, that helps a lot. The data are clear that humans are really bad at taking the time to do things that are well understood to incontrovertibly reduce the risk of rare but catastrophic events. We will rationalize that taking shortcuts is the right, reasonable thing to do. There's a term for this: the normalization of deviance. It's well studied in a number of other contexts including healthcare, aviation, mechanical engineering, aerospace engineering, and civil engineering, but we don't see it discussed in the context of software. In fact, I've never seen the term used in the context of software. Is it possible to learn from other's mistakes instead of making every mistake ourselves? The state of the industry make this sound unlikely, but let's give it a shot. John Banja has a nice summary paper on the normalization of deviance in healthcare, with lessons we can attempt to apply to software development. One thing to note is that, because Banja is concerned with patient outcomes, there's a close analogy to devops failure modes, but normalization of deviance also occurs in cultural contexts that are less directly analogous. The first section of the paper details a number of disasters, both in healthcare and elsewhere. Here's one typical example:  A catastrophic negligence case that the author participated in as an expert witness involved an anesthesiologist's turning off a ventilator at the request of a surgeon who wanted to take an x-ray of the patient's abdomen (Banja, 2005, pp. 87-101). The ventilator was to be off for only a few seconds, but the anesthesiologist forgot to turn it back on, or thought he turned it back on but had not. The patient was without oxygen for a long enough time to cause her to experience global anoxia, which plunged her into a vegetative state. She never recovered, was disconnected from artificial ventilation 9 days later, and then died 2 days after that. It was later discovered that the anesthesia alarms and monitoring equipment in the operating room had been deliberately programmed to a “suspend indefinite” mode such that the anesthesiologist was not alerted to the ventilator problem. Tragically, the very instrumentality that was in place to prevent such a horror was disabled, possibly because the operating room staff found the constant beeping irritating and annoying.  Turning off or ignoring notifications because there are too many of them and they're too annoying? An erroneous manual operation? This could be straight out of the post-mortem of more than a few companies I can think of, except that the result was a tragic death instead of the loss of millions of dollars. If you read a lot of tech post-mortems, every example in Banja's paper will feel familiar even though the details are different. The section concludes,  What these disasters typically reveal is that the factors accounting for them usually had “long incubation periods, typified by rule violations, discrepant events that accumulated unnoticed, and cultural beliefs about hazards that together prevented interventions that might have staved off harmful outcomes”. Furthermore, it is especially striking how multiple rule violations and lapses can coalesce so as to enable a disaster's occurrence.  Once again, this could be from an article about technical failures. That makes the next section, on why these failures happen, seem worth checking out. The reasons given are: The rules are stupid and inefficient The example in the paper is about delivering medication to newborns. To prevent “drug diversion,” nurses were required to enter their password onto the computer to access the medication drawer, get the medication, and administer the correct amount. In order to ensure that the first nurse wasn't stealing drugs, if any drug remained, another nurse was supposed to observe the process, and then enter their password onto the computer to indicate they witnessed the drug being properly disposed of. That sounds familiar. How many technical postmortems start off with “someone skipped some steps because they're inefficient”, e.g., “the programmer force pushed a bad config or bad code because they were sure nothing could go wrong and skipped staging/testing”? The infamous November 2014 Azure outage happened for just that reason. At around the same time, a dev at one of Azure's competitors overrode the rule that you shouldn't push a config that fails tests because they knew that the config couldn't possibly be bad. When that caused the canary deploy to start failing, they overrode the rule that you can't deploy from canary into staging with a failure because they knew their config couldn't possibly be bad and so the failure must be from something else. That postmortem revealed that the config was technically correct, but exposed a bug in the underlying software; it was pure luck that the latent bug the config revealed wasn't as severe as the Azure bug. Humans are bad at reasoning about how failures cascade, so we implement bright line rules about when it's safe to deploy. But the same thing that makes it hard for us to reason about when it's safe to deploy makes the rules seem stupid and inefficient. Knowledge is imperfect and uneven People don't automatically know what should be normal, and when new people are onboarded, they can just as easily learn deviant processes that have become normalized as reasonable processes. Julia Evans described to me how this happens: new person joins new person: WTF WTF WTF WTF WTF old hands: yeah we know we're concerned about it new person: WTF WTF wTF wtf wtf w... new person gets used to it new person #2 joins new person #2: WTF WTF WTF WTF new person: yeah we know. we're concerned about it. The thing that's really insidious here is that people will really buy into the WTF idea, and they can spread it elsewhere for the duration of their career. Once, after doing some work on an open source project that's regularly broken and being told that it's normal to have a broken build, and that they were doing better than average, I ran the numbers, found that project was basically worst in class, and wrote something about the idea that it's possible to have a build that nearly always passes with relatively low effort. The most common comment I got in response was, "Wow that guy must work with superstar programmers. But let's get real. We all break the build at least a few times a week", as if running tests (or for that matter, even attempting to compile) before checking code in requires superhuman abilities. But once people get convinced that some deviation is normal, they often get really invested in the idea. I'm breaking the rule for the good of my patient The example in the paper is of someone who breaks the rule that you should wear gloves when finding a vein. Their reasoning is that wearing gloves makes it harder to find a vein, which may result in their having to stick a baby with a needle multiple times. It's hard to argue against that. No one wants to cause a baby extra pain! The second worst outage I can think of occurred when someone noticed that a database service was experiencing slowness. They pushed a fix to the service, and in order to prevent the service degradation from spreading, they ignored the rule that you should do a proper, slow, staged deploy. Instead, they pushed the fix to all machines. It's hard to argue against that. No one wants their customers to have degraded service! Unfortunately, the fix exposed a bug that caused a global outage. The rules don't apply to me/You can trust me  most human beings perceive themselves as good and decent people, such that they can understand many of their rule violations as entirely rational and ethically acceptable responses to problematic situations. They understand themselves to be doing nothing wrong, and will be outraged and often fiercely defend themselves when confronted with evidence to the contrary.  As companies grow up, they eventually have to impose security that prevents every employee from being able to access basically everything. And at most companies, when that happens, some people get really upset. “Don't you trust me? If you trust me, how come you're revoking my access to X, Y, and Z?” Facebook famously let all employees access everyone's profile for a long time, and you can even find HN comments indicating that some recruiters would explicitly mention that as a perk of working for Facebook. And I can think of more than one well-regarded unicorn where everyone still has access to basically everything, even after their first or second bad security breach. It's hard to get the political capital to restrict people's access to what they believe they need, or are entitled, to know. A lot of trendy startups have core values like “trust” and “transparency” which make it difficult to argue against universal access. Workers are afraid to speak up There are people I simply don't give feedback to because I can't tell if they'd take it well or not, and once you say something, it's impossible to un-say it. In the paper, the author gives an example of a doctor with poor handwriting who gets mean when people ask him to clarify what he's written. As a result, people guess instead of asking. In most company cultures, people feel weird about giving feedback. Everyone has stories about a project that lingered on for months or years after it should have been terminated because no one was willing to offer explicit feedback. This is a problem even when cultures discourage meanness and encourage feedback: cultures of niceness seem to have as many issues around speaking up as cultures of meanness, if not more. In some places, people are afraid to speak up because they'll get attacked by someone mean. In others, they're afraid because they'll be branded as mean. It's a hard problem. Leadership withholding or diluting findings on problems In the paper, this is characterized by flaws and weaknesses being diluted as information flows up the chain of command. One example is how a supervisor might take sub-optimal actions to avoid looking bad to superiors. I was shocked the first time I saw this happen. I must have been half a year or a year out of school. I saw that we were doing something obviously non-optimal, and brought it up with the senior person in the group. He told me that he didn't disagree, but that if we did it my way and there was a failure, it would be really embarrassing. He acknowledged that my way reduced the chance of failure without making the technical consequences of failure worse, but it was more important that we not be embarrassed. Now that I've been working for a decade, I have a better understanding of how and why people play this game, but I still find it absurd. Solutions Let's say you notice that your company has a problem that I've heard people at most companies complain about: people get promoted for heroism and putting out fires, not for preventing fires; and people get promoted for shipping features, not for doing critical maintenance work and bug fixing. How do you change that? The simplest option is to just do the right thing yourself and ignore what's going on around you. That has some positive impact, but the scope of your impact is necessarily limited. Next, you can convince your team to do the right thing: I've done that a few times for practices I feel are really important and are sticky, so that I won't have to continue to expend effort on convincing people once things get moving. But if the incentives are aligned against you, it will require an ongoing and probably unsustainable effort to keep people doing the right thing. In that case, the problem becomes convincing someone to change the incentives, and then making sure the change works as designed. How to convince people is worth discussing, but long and messy enough that it's beyond the scope of this post. As for making the change work, I've seen many “obvious” mistakes repeated, both in places I've worked and those whose internal politics I know a lot about. Small companies have it easy. When I worked at a 100 person company, the hierarchy was individual contributor (IC) -> team lead (TL) -> CEO. That was it. The CEO had a very light touch, but if he wanted something to happen, it happened. Critically, he had a good idea of what everyone was up to and could basically adjust rewards in real-time. If you did something great for the company, there's a good chance you'd get a raise. Not in nine months when the next performance review cycle came up, but basically immediately. Not all small companies do that effectively, but with the right leadership, they can. That's impossible for large companies. At large company A (LCA), they had the problem we're discussing and a mandate came down to reward people better for doing critical but low-visibility grunt work. There were too many employees for the mandator to directly make all decisions about compensation and promotion, but the mandator could review survey data, spot check decisions, and provide feedback until things were normalized. My subjective perception is that the company never managed to achieve parity between boring maintenance work and shiny new projects, but got close enough that people who wanted to make sure things worked correctly didn't have to significantly damage their careers to do it. At large company B (LCB), ICs agreed that it's problematic to reward creating new features more richly than doing critical grunt work. When I talked to managers, they often agreed, too. But nevertheless, the people who get promoted are disproportionately those who ship shiny new things. I saw management attempt a number of cultural and process changes at LCB. Mostly, those took the form of pronouncements from people with fancy titles. For really important things, they might produce a video, and enforce compliance by making people take a multiple choice quiz after watching the video. The net effect I observed among other ICs was that people talked about how disconnected management was from the day-to-day life of ICs. But, for the same reasons that normalization of deviance occurs, that information seems to have no way to reach upper management. It's sort of funny that this ends up being a problem about incentives. As an industry, we spend a lot of time thinking about how to incentivize consumers into doing what we want. But then we set up incentive systems that are generally agreed upon as incentivizing us to do the wrong things, and we do so via a combination of a game of telephone and cargo cult diffusion. Back when Microsoft was ascendant, we copied their interview process and asked brain-teaser interview questions. Now that Google is ascendant, we copy their interview process and ask algorithms questions. If you look around at trendy companies that are younger than Google, most of them basically copy their ranking/leveling system, with some minor tweaks. The good news is that, unlike many companies people previously copied, Google has put a lot of thought into most of their processes and made data driven decisions. The bad news is that Google is unique in a number of ways, which means that their reasoning often doesn't generalize, and that people often cargo cult practices long after they've become deprecated at Google. This kind of diffusion happens for technical decisions, too. Stripe built a reliable message queue on top of Mongo, so we build reliable message queues on top of Mongo1. It's cargo cults all the way down2. The paper has specific sub-sections on how to prevent normalization of deviance, which I recommend reading in full.  Pay attention to weak signals Resist the urge to be unreasonably optimistic Teach employees how to conduct emotionally uncomfortable conversations System operators need to feel safe in speaking up Realize that oversight and monitoring are never-ending  Let's look at how the first one of these, “pay attention to weak signals”, interacts with a single example, the “WTF WTF WTF” a new person gives off when the join the company. If a VP decides something is screwed up, people usually listen. It's a strong signal. And when people don't listen, the VP knows what levers to pull to make things happen. But when someone new comes in, they don't know what levers they can pull to make things happen or who they should talk to almost by definition. They give out weak signals that are easily ignored. By the time they learn enough about the system to give out strong signals, they've acclimated. “Pay attention to weak signals” sure sounds like good advice, but how do we do it? Strong signals are few and far between, making them easy to pay attention to. Weak signals are abundant. How do we filter out the ones that aren't important? And how do we get an entire team or org to actually do it? These kinds of questions can't be answered in a generic way; this takes real thought. We mostly put this thought elsewhere. Startups spend a lot of time thinking about growth, and while they'll all tell you that they care a lot about engineering culture, revealed preference shows that they don't. With a few exceptions, big companies aren't much different. At LCB, I looked through the competitive analysis slide decks and they're amazing. They look at every last detail on hundreds of products to make sure that everything is as nice for users as possible, from onboarding to interop with competing products. If there's any single screen where things are more complex or confusing than any competitor's, people get upset and try to fix it. It's quite impressive. And then when LCB onboards employees in my org, a third of them are missing at least one of, an alias/account, an office, or a computer, a condition which can persist for weeks or months. The competitive analysis slide decks talk about how important onboarding is because you only get one chance to make a first impression, and then employees are onboarded with the impression that the company couldn't care less about them and that it's normal for quotidian processes to be pervasively broken. LCB can't even to get the basics of employee onboarding right, let alone really complex things like acculturation. This is understandable — external metrics like user growth or attrition are measurable, and targets like how to tell if you're acculturating people so that they don't ignore weak signals are softer and harder to determine, but that doesn't mean they're any less important. People write a lot about how things like using fancier languages or techniques like TDD or agile will make your teams more productive, but having a strong engineering culture is much larger force multiplier.  Thanks to Sophie Smithburg and Marc Brooker for introducing me to the term Normalization of Deviance, and Kelly Eskridge, Leah Hanson, Sophie Rapoport, Sophie Smithburg, Julia Evans, Dmitri Kalintsev, Ralph Corderoy, Jamie Brandon, Egor Neliuba, and Victor Felder for comments/corrections/discussion.      People seem to think I'm joking here. I can understand why, but try Googling mongodb message queue. You'll find statements like “replica sets in MongoDB work extremely well to allow automatic failover and redundancy”. Basically every company I know of that's done this and has anything resembling scale finds this to be non-optimal, to say the least, but you can't actually find blog posts or talks that discuss that. All you see are the posts and talks from when they first tried it and are in the honeymoon period. This is common with many technologies. You'll mostly find glowing recommendations in public even when, in private, people will tell you about all the problems. Today, if you do the search mentioned above, you'll get a ton of posts talking about how amazing it is to build a message queue on top of Mongo, this footnote, and a maybe couple of blog posts by Kyle Kingsbury depending on your exact search terms. If there were an acute failure, you might see a postmortem, but while we'll do postmortems for "the site was down for 30 seconds", we rarely do postmortems for "this takes 10x as much ops effort as the alternative and it's a death by a thousand papercuts", "we architected this thing poorly and now it's very difficult to make changes that ought to be trivial", or "a competitor of ours was able to accomplish the same thing with an order of magnitude less effort". I'll sometimes do informal postmortems by asking everyone involved oblique questions about what happened, but more for my own benefit than anything else, because I'm not sure people really want to hear the whole truth. This is especially sensitive if the effort has generated a round of promotions, which seems to be more common the more screwed up the project. The larger the project, the more visibility and promotions, even if the project could have been done with much less effort. [return] I've spent a lot of time asking about why things are the way they are, both in areas where things are working well, and in areas where things are going badly. Where things are going badly, everyone has ideas. But where things are going well, as in the small company with the light-touch CEO mentioned above, almost no one has any idea why things work. It's magic. If you ask, people will literally tell you that it seems really similar to some other place they've worked, except that things are magically good instead of being terrible for reasons they don't understand. But it's not magic. It's hard work that very few people understand. Something I've seen multiple times is that, when a VP leaves, a company will become a substantially worse place to work, and it will slowly dawn on people that the VP was doing an amazing job at supporting not only their direct reports, but making sure that everyone under them was having a good time. It's hard to see until it changes, but if you don't see anything obviously wrong, either you're not paying attention or someone or many someones have put a lot of work into making sure things run smoothly. [return]   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Stop writing CLI validation. Parse it right the first time]]></title>
            <link>https://hackers.pub/@hongminhee/2025/stop-writing-cli-validation-parse-it-right-the-first-time</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151622</guid>
            <description><![CDATA[This post introduces Optique, a new library created to address the pervasive problem of repetitive and often messy validation code in CLI tools. The author was motivated by the observation that nearly every CLI tool reinvents the wheel with similar validation patterns for dependent options, mutually exclusive options, and environment-specific requirements. Optique leverages parser combinators and TypeScript's type inference to ensure that CLI arguments are parsed directly into valid configurations, eliminating the need for manual validation. By describing the desired CLI configuration with Optique, TypeScript automatically infers the types and constraints, catching potential bugs at compile time. The author shares their experience of deleting large chunks of validation code and simplifying refactoring tasks. Optique aims to provide a more robust and maintainable approach to CLI argument parsing, potentially saving developers from writing the same validation logic repeatedly.]]></description>
            <content:encoded><![CDATA[I have this bad habit. When something annoys me enough times,
I end up building a library for it. This time, it was CLI validation code.
See, I spend a lot of time reading other people's code. Open source projects,
work stuff, random GitHub repos I stumble upon at 2 AM. And I kept noticing this
thing: every CLI tool has the same ugly validation code tucked away somewhere.
You know the kind:
if (!opts.server && opts.port) {
  throw new Error("--port requires --server flag");
}

if (opts.server && !opts.port) {
  opts.port = 3000; // default port
}

// wait, what if they pass --port without a value?
// what if the port is out of range?
// what if...
It's not even that this code is hard to write. It's that it's everywhere.
Every project. Every CLI tool. The same patterns, slightly different flavors.
Options that depend on other options. Flags that can't be used together.
Arguments that only make sense in certain modes.
And here's what really got me: we solved this problem years ago for other types
of data. Just… not for CLIs.
The problem with validation 
There's this blog post that completely changed how I think about parsing.
It's called Parse, don't validate by Alexis King. The gist? Don't parse data
into a loose type and then check if it's valid. Parse it directly into a type
that can only be valid.
Think about it. When you get JSON from an API, you don't just parse it as any
and then write a bunch of if-statements. You use something like Zod to parse
it directly into the shape you want. Invalid data? The parser rejects it. Done.
But with CLIs? We parse arguments into some bag of properties and then spend
the next 100 lines checking if that bag makes sense. It's backwards.
So yeah, I built Optique. Not because the world desperately needed another CLI
parser (it didn't), but because I was tired of seeing—and writing—the same
validation code everywhere.
Three patterns I was sick of validating 
Dependent options 
This one's everywhere. You have an option that only makes sense when another
option is enabled.
The old way? Parse everything, then check:
const opts = parseArgs(process.argv);
if (!opts.server && opts.port) {
  throw new Error("--port requires --server");
}
if (opts.server && !opts.port) {
  opts.port = 3000;
}
// More validation probably lurking elsewhere...
With Optique, you just describe what you want:
const config = withDefault(
  object({
    server: flag("--server"),
    port: option("--port", integer()),
    workers: option("--workers", integer())
  }),
  { server: false }
);
Here's what TypeScript infers for config's type:
type Config = 
  | { readonly server: false }
  | { readonly server: true; readonly port: number; readonly workers: number }
The type system now understands that when server is false, port literally
doesn't exist. Not undefined, not null—it's not there. Try to access it and
TypeScript yells at you. No runtime validation needed.
Mutually exclusive options 
Another classic. Pick one output format: JSON, YAML, or XML. But definitely not
two.
I used to write this mess:
if ((opts.json ? 1 : 0) + (opts.yaml ? 1 : 0) + (opts.xml ? 1 : 0) > 1) {
  throw new Error('Choose only one output format');
}
(Don't judge me, you've written something similar.)
Now?
const format = or(
  map(option("--json"), () => "json" as const),
  map(option("--yaml"), () => "yaml" as const),
  map(option("--xml"), () => "xml" as const)
);
The or() combinator means exactly one succeeds. The result is just
"json" | "yaml" | "xml". A single string. Not three booleans to juggle.
Environment-specific requirements 
Production needs auth. Development needs debug flags. Docker needs different
options than local. You know the drill.
Instead of a validation maze, you just describe each environment:
const envConfig = or(
  object({
    env: constant("prod"),
    auth: option("--auth", string()),      // Required in prod
    ssl: option("--ssl"),
    monitoring: option("--monitoring", url())
  }),
  object({
    env: constant("dev"),
    debug: optional(option("--debug")),    // Optional in dev
    verbose: option("--verbose")
  })
);
No auth in production? Parser fails immediately. Trying to access --auth in
dev mode? TypeScript won't let you—the field doesn't exist on that type.
“But parser combinators though…” 
I know, I know. “Parser combinators” sounds like something you'd need
a CS degree to understand.
Here's the thing: I don't have a CS degree. Actually, I don't have any degree.
But I've been using parser combinators for years because they're actually… not
that hard? It's just that the name makes them sound way scarier than they are.
I'd been using them for other stuff—parsing config files, DSLs, whatever.
But somehow it never clicked that you could use them for CLI parsing until
I saw Haskell's optparse-applicative. That was a real “wait, of course”
moment. Like, why are we doing this any other way?
Turns out it's stupidly simple. A parser is just a function. Combinators are
just functions that take parsers and return new parsers. That's it.
// This is a parser
const port = option("--port", integer());

// This is also a parser (made from smaller parsers)
const server = object({
  port: port,
  host: option("--host", string())
});

// Still a parser (parsers all the way down)
const config = or(server, client);
No monads. No category theory. Just functions. Boring, beautiful functions.
TypeScript does the heavy lifting 
Here's the thing that still feels like cheating: I don't write types for my CLI
configs anymore. TypeScript just… figures it out.
const cli = or(
  command("deploy", object({
    action: constant("deploy"),
    environment: argument(string()),
    replicas: option("--replicas", integer())
  })),
  command("rollback", object({
    action: constant("rollback"),
    version: argument(string()),
    force: option("--force")
  }))
);

// TypeScript infers this type automatically:
type Cli = 
  | { 
      readonly action: "deploy"
      readonly environment: string
      readonly replicas: number
    }
  | { 
      readonly action: "rollback"
      readonly version: string
      readonly force: boolean
    }
TypeScript knows that if action is "deploy", then environment exists but
version doesn't. It knows replicas is a number. It knows force is
a boolean. I didn't tell it any of this.
This isn't just about nice autocomplete (though yeah, the autocomplete is great).
It's about catching bugs before they happen. Forget to handle a new option
somewhere? Code won't compile.
What actually changed for me 
I've been dogfooding this for a few weeks. Some real talk:
I delete code now. Not refactor. Delete. That validation logic that used to
be 30% of my CLI code? Gone. It feels weird every time.
Refactoring isn't scary. Want to know something that usually terrifies me?
Changing how a CLI takes its arguments. Like going from --input file.txt to
just file.txt as a positional argument. With traditional parsers,
you're hunting down validation logic everywhere. With this?
You change the parser definition, TypeScript immediately shows you every place
that breaks, you fix them, done. What used to be an hour of “did I catch
everything?” is now “fix the red squiggles and move on.”
My CLIs got fancier. When adding complex option relationships doesn't mean
writing complex validation, you just… add them. Mutually exclusive groups?
Sure. Context-dependent options? Why not. The parser handles it.
The reusability is real too:
const networkOptions = object({
  host: option("--host", string()),
  port: option("--port", integer())
});

// Reuse everywhere, compose differently
const devServer = merge(networkOptions, debugOptions);
const prodServer = merge(networkOptions, authOptions);
const testServer = merge(networkOptions, mockOptions);
But honestly? The biggest change is trust. If it compiles, the CLI logic works.
Not “probably works” or “works unless someone passes weird arguments.”
It just works.
Should you care? 
If you're writing a 10-line script that takes one argument, you don't need this.
process.argv[2] and call it a day.
But if you've ever:

Had validation logic get out of sync with your actual options
Discovered in production that certain option combinations explode
Spent an afternoon tracking down why --verbose breaks when used with
--json
Written the same “option A requires option B” check for the fifth time

Then yeah, maybe you're tired of this stuff too.
Fair warning: Optique is young. I'm still figuring things out, the API might
shift a bit. But the core idea—parse, don't validate—that's solid.
And I haven't written validation code in months.
Still feels weird. Good weird.
Try it or don't 
If this resonates:

Tutorial: Build something real, see if you hate it
Concepts: Primitives, constructs, modifiers, value parsers,
the whole thing
GitHub: The code, issues, angry rants

I'm not saying Optique is the answer to all CLI problems. I'm just saying
I was tired of writing the same validation code everywhere, so I built something
that makes it unnecessary.
Take it or leave it. But that validation code you're about to write?
You probably don't need it.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GigaByte CXL memory expansion card with up to 512GB DRAM]]></title>
            <link>https://www.gigabyte.com/PC-Accessory/AI-TOP-CXL-R5X4</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151598</guid>
            <description><![CDATA[Expand Memory Pool 16-Layers HDI PCB Equipped with an AIO Fan DDR5 RDIMM Support Support PCle 5.0 Lower Total Cost of Ownership (TCO)]]></description>
            <content:encoded><![CDATA[


            
            
    
    
    




    








    
			
                
                            
                                
                                     FEATURES 
                                    
                                         Expand Memory Pool 
                                         16-Layers HDI PCB 
                                         Equipped with an AIO Fan 
                                         DDR5 RDIMM Support 
                                         Support PCle 5.0 
                                         Lower Total Cost of Ownership (TCO) 
                                    
                                
                                
                                    
                                    
                                
                            
                            
                            
                        
                
                                    
                                        
                                            
                                            
                                        
                                        
                                                 GIGABYTE AI TOP 
                                                 Train your own AI on your desk. 
                                            
                                    
                                    
                                        
                                            
                                            
                                        
                                        
                                                 Ultimate scalability and Connectivity​ 
                                                 Brings you the future-proofing capability. 
                                            
                                    
                                
                
                    
                    
                        
                            
                                     GIGABYTE AI TOP 
                                     Train your own AI on your desk 
                                     In the age of local AI, GIGABYTE AI TOP is the all-round solution to win advantages ahead of traditional AI training methods. It features a variety of groundbreaking technologies that can be easily adapted by beginners or experts, for most common open-source LLMs, in anyplace even on your desk. 
                                
                            
                                
                                    Supports 236B LLM Local Training
                                
                                
                                    Intuitive Set-up
                                
                                
                                    Flexbility & Upgradability
                                
                                
                                    Privacy & Security
                                
                                
                                    Suitable for Home Use
                                
                            
                        
                        
                            
                                    
                                        
                                    
                                    
                                         AI TOP Utility 2.0 
                                         Reinventing AI Training 
                                    
                                     The all-new AI TOP Utility 2.0 brings enhanced features and broader model support, empowering users from novices to experts to unlock AI's potential right at their desks. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                
                            
                                    
                                         AI TOP Hardware 
                                         Enhanced Performance for AI Training 
                                    
                                     The AI TOP Hardware features a series of GIGABYTE AI TOP products that are optimized in power efficiency and durability for AI training workloads. It includes upgradeable components and is easy to build at home. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                     * Images for reference only, system configuration will vary by model. 
                                
                        
                    
                
                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultimate scalability 
                                                                 Exclusive Memory Offloading Solution 
                                                                 Expand training capacity by offloading data to system memory and SSDs, enabling efficient training of Big models through optimized memory management. 
                                                            
                                                            
                                                            
                                                                                         Offers diverse memory expandability with support for DDR5 ECC Registered memory modules (RDIMM): 
                                                                                        
                                                                                             - 4 x DDR5 DIMM sockets, supporting up to 512 GB of memory (up to 128 GB per DIMM) 
                                                                                        
                                                                                         Provides users with High flexibility and options suitable for various requirement scenarios. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Future Connectivity 
                                                                 Express to PCIE5 Performance 
                                                                 The new AI TOP CXL PCI Express card provides the most efficient solution to enjoy boosted performance of PCIe Gen5 on the existing platform. PCIe 5.0 doubles the data transfer rate of PCIe 4.0 from 16 GT/s to 32 GT/s per lane. This substantial increase in bandwidth particularly benefits high-performance AIO cards, storage devices, and AI accelerators that demand faster data movement. 
                                                            
                                                            
                                                                                         Benefits of CXL Memory: 
                                                                                        
                                                                                            
                                                                                                 *Overall lower total cost of ownership (TCO)* 
                                                                                                 -Improve computational and memory resource utilization for specific applications, thereby reducing capital expenditure and operating costs. 
                                                                                            
                                                                                            
                                                                                                 *Breaking Memory Bottlenecks:* 
                                                                                                 - Traditional server memory capacity limits the development of complex applications like AI. CXL memory provides a way to break through this barrier, allowing servers to expand memory capacity and meet the demands of increasingly complex AI applications. 
                                                                                            
                                                                                            
                                                                                                 *Improving Memory Utilization Efficiency:* 
                                                                                                 - CXL memory enables efficient use of memory resources through cross-device memory sharing and memory pooling technologies, thereby increasing memory utilization rates. 
                                                                                            
                                                                                            
                                                                                                 *Accelerating Computation Speed:* 
                                                                                                 - CXL memory allows the CPU to access memory on accelerators, achieving cross-device memory sharing and heterogeneous computing, which accelerates computation speed. 
                                                                                            
                                                                                            
                                                                                                 *Supporting Memory Consistency:* 
                                                                                                 - The CXL protocol (such as CXL.io, CXL.cache, CXL.mem) ensures consistency in memory operations across devices, avoiding data inconsistency issues. 
                                                                                            
                                                                                            
                                                                                                 *Flexible Scaling and Sharing:* 
                                                                                                 - CXL memory supports many-to-many flexible connections, allowing for many-to-many connections between accelerators and CPUs, achieving flexible scaling and memory sharing. 
                                                                                            
                                                                                        
                                                                                        
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Superior thermal Design 
                                                                 Comprehensive Thermal 
                                                                 Advanced full-metal thermal design and durable heatsinks to keep your system cool and efficient. 
                                                            
                                                            
                                                                                         AIO Fan Thermal Design 
                                                                                         High-efficiency fan design has achieved a simultaneous increase in both airflow and pressure, it helps to quickly dissipate heat and stabilize output performance. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultra Durable 
                                                                 Born from Ultra Durable™ 
                                                                 Technology embodies our commitment to excellence, providing gamers with a platform that's not only powerful but also built for longevity and reliability. AI TOP series Product are engineered to endure and excel. 
                                                            
                                                            
                                                                                    
                                                                                         Long Lifespan Durable™ Solid Caps 
                                                                                         GIGABYTE motherboards integrate the absolute best quality solid state capacitors that are rated to perform at maximum efficiency for extended periods, even in extreme performance configurations. With ultra-low ESR no matter how high the load, this provides peace of mind for end users who want to push their system hard, yet demand absolute reliability and stability. These exclusive capacitors also come in customized jet, exclusively on GIGABYTE product. 
                                                                                        
                                                                                    
                                                                                    
                                                                                         Humidity Protection with New Glass Fabric PCB 
                                                                                         There is nothing more harmful to the longevity of your PC than moisture, and most parts of the world experience moisture in the air as humidity at some point during the year. GIGABYTE motherboards have been designed to make sure that humidity is never an issue, incorporating a new Glass Fabric PCB technology that repels moisture caused by humid and damp conditions.  Glass Fabric PCB technology uses a new PCB material which reduces the amount of space between the fiber weave, making it much more difficult for moisture to penetrate compared to traditional motherboard PCBs. This offers much better protection from short circuit and system malfunction caused by humid and damp conditions. 
                                                                                        
                                                                                    
                                                                                
                                                        
                                                
                                        
                
                    
                        
                    
                
                
                         * This product requires compatible AI TOP hardware to be supported. A single CXL card cannot properly activate the AI TOP Utility. We recommend consulting with our Sales representative or authorized dealer contact before purchasing this product. 
                    
            
	

		* Product specifications and product appearance may differ from country to country. We recommend that you check with your local dealers for the specifications and appearance of the products available in your country. Colors of products may not be perfectly accurate due to variations caused by photographic variables and monitor settings so it may vary from images shown on this site. Although we endeavor to present the most accurate and comprehensive information at the time of publication, we reserve the right to make changes without prior notice.
	







                



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Claude Code SDK to reduce E2E test time]]></title>
            <link>https://jampauchoa.substack.com/p/best-of-both-worlds-using-claude</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151447</guid>
        </item>
        <item>
            <title><![CDATA[Patterns, Predictions, and Actions – A story about machine learning]]></title>
            <link>https://mlstory.org/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45150820</guid>
            <description><![CDATA[Image copyright: Princeton University Press]]></description>
            <content:encoded><![CDATA[
 
Image copyright: Princeton University Press
Hardcover
Princeton
University Press
Errata

Full preprint as
PDF
Problem sets (pdf)


Table of contents
Preface Acknowledgments

Introduction (PDF)
Fundamentals of prediction (PDF)
Supervised learning (PDF)
Representations and features (PDF)
Optimization (PDF)
Generalization (PDF)
Deep learning (PDF)
Datasets (PDF)
Causality (PDF)
Causal inference in practice (PDF)
Sequential decision making and dynamic
programming (PDF)
Reinforcement learning (PDF)
Epilogue (PDF)
Mathematical background (PDF)



Contact us
We welcome your feedback, questions, and suggestions. You can reach
us at contact@mlstory.org. If you taught from the book,
we’d love to hear about it.


Citations, license, typesetting
Please cite the print edition of this book as:
@book{hardtrecht2022patterns,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: Foundations of machine learning},
  year = {2022},
  publisher = {Princeton University Press}
}

We maintain an archival version of the book at arXiv:2102.05242. The web
version is more up-to-date than the arXiv version. The print version
contains additional improvements and editing not present in the web
version.
The text available on this website is licensed under the Creative
Commons BY-NC-ND 4.0 license.
This book is typeset using pandoc with the unbuch setup.


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oldest recorded transaction]]></title>
            <link>https://avi.im/blag/2025/oldest-txn/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149626</guid>
            <description><![CDATA[The oldest recorded transaction was in 3100 BC]]></description>
            <content:encoded><![CDATA[The other day I posted a tweet with this image which I thought was funny:This is the oldest transaction database from 3100 BC - recording accounts of malt and barley groats. Considering this thing survived 5000 years (holy shit!) with zero downtime and has stronger durability guarantees than most databases today.I call it rock solid durability.This got me thinking, can I insert this date in today’s database? What is the oldest timestamp a database can support?So I checked the top three databases: MySQL, Postgres, and SQLite:MySQL1000 ADPostgres4713 BCSQLite4713 BCToo bad you cannot use MySQL for this. Postgres and SQLite support the Julian calendar and the lowest date is Jan 01, 4713 BC:sales=# INSERT INTO orders VALUES ('4713-01-01 BC'::date);
INSERT 0 1
sales=# SELECT * FROM orders;
   timestamp
---------------
 4713-01-01 BC
(1 row)
sales=# INSERT INTO orders VALUES ('4714-01-01 BC'::date);
ERROR:  date out of range: "4714-01-01 BC"
I wonder how people store dates older than this. Maybe if I’m a British Museum manager, and I want to keep theft inventory details. How do I do it? As an epoch? Store it as text? Use some custom system? How do I get it to support all the custom operations that a typical TIMESTAMP supports?Thanks to aku, happy_shady, Mr. Bhat, and General Bruh for reading an early draft of this post.1. Source of the image: Sumer civilization2. I found this from the talk 1000x: The Power of an Interface for Performance by
Joran Dirk Greef, CEO of TigerBeetle, timestamped @ 38:10.3. The talk has other bangers too, like this or this.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI surveillance should be banned while there is still time]]></title>
            <link>https://gabrielweinberg.com/p/ai-surveillance-should-be-banned</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149281</guid>
            <description><![CDATA[All the same privacy harms with online tracking are also present with AI, but worse.]]></description>
            <content:encoded><![CDATA[Original cartoon by Dominique Lizaambard (left), updated for AI, by AI (right).All the same privacy harms with online tracking are also present with AI, but worse.While chatbot conversations resemble longer search queries, chatbot privacy harms have the potential to be significantly worse because the inference potential is dramatically greater. Longer input invites more personal information to be provided, and people are starting to bare their souls to chatbots. The conversational format can make it feel like you’re talking to a friend, a professional, or even a therapist. While search queries reveal interests and personal problems, AI conversations take their specificity to another level and, in addition, reveal thought processes and communication styles, creating a much more comprehensive profile of your personality. This richer personal information can be more thoroughly exploited for manipulation, both commercially and ideologically, for example, through behavioral chatbot advertising and models designed (or themselves manipulated through SEO or hidden system prompts) to nudge you towards a political position or product. Chatbots have already been found to be more persuasive than humans and have caused people to go into delusional spirals as a result. I suspect we’re just scratching the surface, since they can become significantly more attuned to your particular persuasive triggers through chatbot memory features, where they train and fine-tune based on your past conversations, making the influence much more subtle. Instead of an annoying and obvious ad following you around everywhere, you can have a seemingly convincing argument, tailored to your personal style, with an improperly sourced “fact” that you’re unlikely to fact-check or a subtle product recommendation you’re likely to heed. That is, all the privacy debates surrounding Google search results from the past two decades apply one-for-one to AI chats, but to an even greater degree. That’s why we (at DuckDuckGo) started offering Duck.ai for protected chatbot conversations and optional, anonymous AI-assisted answers in our private search engine. In doing so, we’re demonstrating that privacy-respecting AI services are feasible. But unfortunately, such protected chats are not yet standard practice, and privacy mishaps are mounting quickly. Grok leaked hundreds of thousands of chatbot conversations that users thought were private. Perplexity’s AI agent was shown to be vulnerable to hackers who could slurp up your personal information. Open AI is openly talking about their vision for a “super assistant” that tracks everything you do and say (including offline). And Anthropic is going to start training on your chatbot conversations by default (previously the default was off). I collected these from just the past few weeks!It would therefore be ideal if Congress could act quickly to ensure that protected chats become the rule rather than the exception. And yet, I’m not holding my breath because it’s 2025 and the U.S. still doesn’t have a general online privacy law, let alone privacy enshrined in the Constitution as a fundamental right, as it should be. However, there does appear to be an opening right now for AI-specific federal legislation, despite the misguided attempts to ban state AI legislation.Time is running out because every day that passes further entrenches bad privacy practices. Congress must move before history completely repeats itself and everything that happened with online tracking happens again with AI tracking. AI surveillance should be banned while there is still time. No matter what happens, though, we will still be here, offering protected services, including optional AI services, to consumers who want to reap the productivity benefits of online tools without the privacy harms. Share]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[We hacked Burger King: How auth bypass led to drive-thru audio surveillance]]></title>
            <link>https://bobdahacker.com/blog/rbi-hacked-drive-thrus/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148944</guid>
        </item>
        <item>
            <title><![CDATA[Qwen3 30B A3B Hits 13 token/s on 4xRaspberry Pi 5]]></title>
            <link>https://github.com/b4rtaz/distributed-llama/discussions/255</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45148237</guid>
            <description><![CDATA[qwen3_30b.mov Setup [🔀 TP-Link LS1008G Switch] | | | | | | | |_______ 🔸 raspberrypi2 (ROOT) 10.0.0.2 | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1 | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0....]]></description>
            <content:encoded><![CDATA[
      



    
      Skip to content

      
    




  
  
  






      

          

              





  Navigation Menu

  

  
          
            
                
      

      
        
            

                  
                      
  
      
      
        
          GitHub Copilot

        

        Write better code with AI
      

    


                      
  
      
      
        
          GitHub Spark

            
              New
            
        

        Build and deploy intelligent apps
      

    


                      
  
      
      
        
          GitHub Models

            
              New
            
        

        Manage and compare prompts
      

    


                      
  
      
      
        
          GitHub Advanced Security

        

        Find and fix vulnerabilities
      

    


                      
  
      
      
        
          Actions

        

        Automate any workflow
      

    


                  
                
            

                  
                      
  
      
      
        
          Codespaces

        

        Instant dev environments
      

    


                      
  
      
      
        
          Issues

        

        Plan and track work
      

    


                      
  
      
      
        
          Code Review

        

        Manage code changes
      

    


                      
  
      
      
        
          Discussions

        

        Collaborate outside of code
      

    


                      
  
      
      
        
          Code Search

        

        Find more, search less
      

    


                  
                
            
        

          
            
              View all features
              
          
      



                
      

      



                
      

      

                      Explore
                      
  
      Learning Pathways

    


                      
  
      Events & Webinars

    


                      
  
      Ebooks & Whitepapers

    


                      
  
      Customer Stories

    


                      
  
      Partners

    


                      
  
      Executive Insights

    


                  
                



                
      

      
                

                  
                      
  
      
      
        
          GitHub Sponsors

        

        Fund open source developers
      

    


                  
                
                

                  
                      
  
      
      
        
          The ReadME Project

        

        GitHub community articles
      

    


                  
                
                
            



                
      

      

                  
                      
  
      
      
        
          Enterprise platform

        

        AI-powered developer platform
      

    


                  
                



                
    Pricing


            
          

        
                



  
  
  
    

  
    
    
      
        Provide feedback
      
        
    
    
  
      
        
      
      


    
    

  
    
    
      
        Saved searches
      
        Use saved searches to filter your results more quickly
    
    
  
      
        
      
      

    
  



            

              
                Sign up
              
    
      Appearance settings

      
    
  

          
      


      
    

  








    


    






  
    
      
  




    

      






  
  

      
            
    
      

  
                Notifications
    You must be signed in to change notification settings

  

  
              Fork
    168

  

  
        
            
          Star
          2.5k

  



        

        


          

  
    


  

  




    

        
  


            
    
      
    
  
        
  
    
    qwen3_30b.mov
    
  

  

  


Setup
[🔀 TP-Link LS1008G Switch]
      | | | |
      | | | |_______ 🔸 raspberrypi2 (ROOT)     10.0.0.2
      | | |_________ 🔹 raspberrypi1 (WORKER 1) 10.0.0.1
      | |___________ 🔹 raspberrypi3 (WORKER 2) 10.0.0.3
      |_____________ 🔹 raspberrypi4 (WORKER 3) 10.0.0.4

Device: 4 x Raspberry Pi 5 8GB
Distributed Llama version: 0.16.0
Model: qwen3_30b_a3b_q40
Benchmark




Evaluation
Prediction




4 x Raspberry Pi 5 8GB
14.33 tok/s
13.04 tok/s



b4rtaz@raspberrypi2:~/distributed-llama $ ./dllama inference --prompt "<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant
" --steps 128 --model models/qwen3_30b_a3b_q40/dllama_model_qwen3_30b_a3b_q40.m --tokenizer models/qwen3_30b_a3b_q40/dllama_tokenizer_qwen3_30b_a3b_q40.t --buffer-float-type q80 --nthreads 4 --max-seq-len 4096 --workers 10.0.0.1:9999 10.0.0.3:9999 10.0.0.4:9999
📄 AddBos: 0
📄 BosId: 151643 (<|endoftext|>)
📄 EosId: 151645 (<|im_end|>) 
📄 RegularVocabSize: 151643
📄 SpecialVocabSize: 26
Tokenizer vocab size (151669) does not match the model vocab size (151936)
💡 Arch: Qwen3 MoE
💡 HiddenAct: Silu
💡 Dim: 2048
💡 HeadDim: 128
💡 QDim: 4096
💡 KvDim: 512
💡 HiddenDim: 6144
💡 VocabSize: 151936
💡 nLayers: 48
💡 nHeads: 32
💡 nKvHeads: 4
💡 OrigSeqLen: 262144
💡 nExperts: 128
💡 nActiveExperts: 8
💡 MoeHiddenDim: 768
💡 SeqLen: 4096
💡 NormEpsilon: 0.000001
💡 RopeType: Falcon
💡 RopeTheta: 10000000
📀 RequiredMemory: 5513 MB
⭕ Socket[0]: connecting to 10.0.0.1:9999 worker
⭕ Socket[0]: connected
⭕ Socket[1]: connecting to 10.0.0.3:9999 worker
⭕ Socket[1]: connected
⭕ Socket[2]: connecting to 10.0.0.4:9999 worker
⭕ Socket[2]: connected
⭕ Network is initialized
🧠 CPU: neon dotprod fp16
💿 Loading weights...
💿 Weights loaded
🚁 Network is in non-blocking mode
<|im_start|>user
Please explain me where is Poland as I have 1 year<|im_end|>
<|im_start|>assistant

🔷️ Eval  996 ms Sync  330 ms | Sent 12084 kB Recv 20085 kB | (19 tokens)
🔶 Pred   49 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | Of
🔶 Pred   50 ms Sync   94 ms | Sent   636 kB Recv  1057 kB |  course
🔶 Pred   60 ms Sync   37 ms | Sent   636 kB Recv  1057 kB | !
🔶 Pred   60 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Let
🔶 Pred   59 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  me
🔶 Pred   49 ms Sync   27 ms | Sent   636 kB Recv  1057 kB |  explain
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  where
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  Poland
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  is
🔶 Pred   49 ms Sync   18 ms | Sent   636 kB Recv  1057 kB | ,
🔶 Pred   53 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |  in
...
🔶 Pred   70 ms Sync   15 ms | Sent   636 kB Recv  1057 kB | zech
🔶 Pred   53 ms Sync   24 ms | Sent   636 kB Recv  1057 kB |  Republic
🔶 Pred   69 ms Sync   14 ms | Sent   636 kB Recv  1057 kB | **
🔶 Pred   59 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  –
🔶 Pred   55 ms Sync   20 ms | Sent   636 kB Recv  1057 kB |  to
🔶 Pred   64 ms Sync   16 ms | Sent   636 kB Recv  1057 kB |  the
🔶 Pred   53 ms Sync   36 ms | Sent   636 kB Recv  1057 kB |  south
🔶 Pred   62 ms Sync   18 ms | Sent   636 kB Recv  1057 kB |   

🔶 Pred   61 ms Sync   16 ms | Sent   636 kB Recv  1057 kB | 3

Evaluation
   nBatches: 32
    nTokens: 19
   tokens/s: 14.33 (69.80 ms/tok)
Prediction
    nTokens: 109
   tokens/s: 13.04 (76.69 ms/tok)
⭕ Network is closed

    
    


          

        

         







  

  

  

  

  

  

  

  


    




    
  

          



    



  

    

    

    





    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Why language models hallucinate]]></title>
            <link>https://openai.com/index/why-language-models-hallucinate/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45147385</guid>
        </item>
        <item>
            <title><![CDATA[Rug pulls, forks, and open-source feudalism]]></title>
            <link>https://lwn.net/SubscriberLink/1036465/e80ebbc4cee39bfb/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45146967</guid>
            <description><![CDATA[Like almost all human endeavors, open-source software development involves a range of power dyn [...]]]></description>
            <content:encoded><![CDATA[


Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider subscribing to LWN.  Thank you
for visiting LWN.net!



Like almost all human endeavors, open-source software development involves
a range of power dynamics.  Companies, developers, and users are all
concerned with the power to influence the direction of the software — and,
often, to profit from it.  At the 2025 Open
Source Summit Europe, Dawn Foster talked about how those dynamics can
play out, with an eye toward a couple of tactics — rug pulls and forks — that
are available to try to shift power in one direction or another.
Power dynamics

Since the beginning of history, Foster began, those in power have tended to
use it against those who were weaker.  In the days of feudalism, control of
the land led to exploitation at several levels.  In the open-source world,
the large cloud providers often seem to have the most power, which they use
against smaller companies.  Contributors and maintainers often have less
power than even the smaller companies, and users have less power yet.  



We have built a world where it is often easiest to just use whatever a
cloud provider offers, even with open-source software.  Those providers may
not contribute back to the projects they turn into services, though,
upsetting the smaller companies that are, likely as not, doing the bulk of
the work to provide the software in question in the first place.  Those
companies can have a power of their own, however: the power to relicense
the software.  Pulling the rug out from under users of the software in this
way can change the balance of power with regard to cloud providers, but it
leaves contributors and users in a worse position than before.  But
there is a power at this level too: the power to fork the software,
flipping the power balance yet again.

Companies that control a software project have the power to carry out this
sort of rug pull, and they are often not shy about exercising it.
Single-company projects, clearly, are at a much higher risk of rug pulls;
the company has all the power in this case, and others have little
recourse.  So one should look at a company's reputation before adopting a
software project, but that is only so helpful.  Companies can change
direction without notice, be acquired, or go out of business, making
previous assessments of their reputation irrelevant.

The problem often comes down to the simple fact that companies have to
answer to their investors, and that often leads to pressure to relicense
the software they have created in order to increase revenue.  This is
especially true in cases where cloud providers are competing for the same
customers as the company that owns the project.  The result can be a switch
to a more restrictive license aimed at making it harder for other companies
to profit from the project.

A rug pull of this nature can lead to a fork of the project — a rebellious,
collective action aimed at regaining some power over the code.  But a fork
is not a simple matter; it is a lot of work, and will fail without people
and resources behind it.  The natural source for that is a large company;
cloud providers, too, can try to shift power via a fork, and they have the
ability to back their fork up with the resources it needs to succeed.



A relicensing event does not always lead to a popular fork; that did not
happen with MongoDB or Sentry, for example.  Foster said she had not looked
into why that was the case.  Sometimes rug pulls take other forms, such as
when Perforce, after acquiring Puppet in 2022, moved it development and
releases behind closed doors, with a reduced frequency of releases back to
the public repository.  That action kicked off the OpenVox fork.
Looking at the numbers

Foster has spent some time analyzing rug pulls, forks, and what happens
thereafter; a lot of the results are available
for download as Jupyter notebooks.  For each rug-pull event, she looked
at the contributor makeup of the project before and after the ensuing fork
in an attempt to see what effects are felt by the projects involved.

In 2021, Elastic relicensed Elasticsearch
under the non-free Server Side Public License (SSPL).  Amazon Web Services
then forked the project as OpenSearch.  Before the fork, most of
the Elasticsearch contributors were Elastic employees; that,
unsurprisingly, did not change afterward.  OpenSearch started with no
strong contributor base, so had to build its community from scratch.  As a
result, the project has been dominated by Amazon contributors ever since;
the balance has shifted slowly over time, but there was not a big uptick in
outside contributors even after OpenSearch became a Linux Foundation
project in 2024.  While starting a project under a neutral foundation can
help attract contributors, she said, moving a project under a foundation's
umbrella later on does not seem to provide the same benefit.

Terraform was
developed mostly by Hashicorp, which relicensed
the software under the non-free Business Source License in 2023.  One
month later, the OpenTofu fork was
started under the Linux Foundation.  While the contributor base for
Terraform, which was almost entirely Hashicorp employees, changed little
after the fork, OpenTofu quickly acquired a number of contributors from
several companies, none of whom had been Terraform contributors before.  In
this case, users drove the fork and placed it under a neutral foundation,
resulting in a more active developer community.

In 2024, Redis was relicensed under the
SSPL; the Valkey fork was quickly organized, under the Linux Foundation,
by Redis contributors.  The Redis project differed from the others
mentioned here in that, before the fork, it had nearly twice as many
contributors from outside the company as from within; after the fork, the
number of external Redis contributors dropped to zero.  All of the external
contributors fled to Valkey, with the result that Valkey started with a
strong community representing a dozen or so companies.

Looking at how the usage of these projects changes is harder, she
said, but there appears to be a correlation between the usage of a project
and the number of GitHub forks (cloned repository copies) it has.  There is
typically a spike in these clones after a relicensing event, suggesting
that people are considering creating a hard fork of the project.  In all
cases, the forks that emerged appeared to have less usage than the original
by the "GitHub forks" metric; both branches of the fork continue to go
forward.  But, she said, projects that are relicensed do tend to show
reduced usage, especially when competing forks are created under foundations.
What to do

This kind of power game creates problems for both contributors and users,
she said; we contribute our time to these projects, and need them to not be
pulled out from under us.  There is no way to know when a rug pull might
happen, but there are some warning signs to look out for.  At the top of
her list was the use of a contributor license agreement (CLA); these
agreements create a power imbalance, giving the company involved the power
to relicense the software.  Projects with CLAs more commonly are subject to
rug pulls; projects using a developers certificate of origin do not have the
same power imbalance and are less likely to be rug pulled.

One should also look at the governance of a project; while being housed
under a foundation reduces the chance of a rug pull, that can still happen,
especially in cases where the contributors are mostly from a single
company.  She mentioned the Cortex project, housed under
the Cloud Native Computing Foundation, which was controlled by Grafana; that
company eventually forked its own project to create Mimir.  To avoid this kind of
surprise, one should look for projects with neutral governance, with
leaders from multiple organizations.

Projects should also be evaluated on their contributor base; are there
enough contributors to keep things going?  Companies can help, of course,
by having their employees contribute to the projects they depend on,
increasing influence and making those projects more sustainable.  She
mentioned the CHAOSS project, which
generates metrics to help in the judgment of the viability of development
projects.  CHAOSS has put together a set of
"practitioner guides" intended to help contributors and maintainers
make improvements within a project.

With the sustained rise of the big cloud providers, she concluded, the
power dynamics around open-source software are looking increasingly feudal.
Companies can use relicensing to shift power away from those providers, but
they also take power from contributors when the pull the rug in this way.
Those contributors, though, are in a better position than the serfs of old,
since they have the ability to fork a project they care about, shifting
power back in their direction.


Hazel Weakly asked if there are other protections that contributors and
users might develop to address this problem.  Foster answered that at least
one company changed its mind about a planned relicensing action after
seeing the success of the Valkey and OpenTofu forks.  The ability to fork
has the effect of making companies think harder, knowing that there may be
consequences that follow a rug pull.  Beyond that, she reiterated that
projects should be pushed toward neutral governance.

Dirk Hohndel added that the best thing to do is to bring more outside
contributors into a project; the more of them there are, the higher the
risk associated with a rug pull.  Anybody who just sits back within a
project, he said, is just a passenger; it is better to be driving.

Foster's
slides are available for interested readers.

[Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my
travel to this event.]
           Index entries for this article
           ConferenceOpen Source Summit Europe/2025
            

               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kenvue stock drops on report RFK Jr will link autism to Tylenol during pregnancy]]></title>
            <link>https://www.cnbc.com/2025/09/05/rfk-tylenol-autism-kenvue-stock-for-url.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45144123</guid>
            <description><![CDATA[HHS will release the report that could draw that link between the pain medication and autism this month, according to the Wall Street Journal.]]></description>
            <content:encoded><![CDATA[Kenvue Inc. Tylenol brand pain reliever for sale at a pharmacy in New York, US, on Wednesday, March 27, 2024. Bloomberg | Bloomberg | Getty ImagesShares of Kenvue fell more than 10% on Friday after a report that Health and Human Services Secretary Robert F. Kennedy Jr. will likely link autism to the use of the company's pain medication Tylenol in pregnant women. HHS will release the report that could draw that link this month, the Wall Street Journal reported on Friday.That report will also suggest a medicine derived from folate – a water-soluble vitamin – can be used to treat symptoms of the developmental disorder in some people, according to the Journal.In a statement, an HHS spokesperson said "We are using gold-standard science to get to the bottom of America's unprecedented rise in autism rates." "Until we release the final report, any claims about its contents are nothing more than speculation," they added. Tylenol could be the latest widely used and accepted treatment that Kennedy has undermined at the helm of HHS, which oversees federal health agencies that regulate drugs and other therapies. Kennedy has also taken steps to change vaccine policy in the U.S., and has amplified false claims about safe and effective shots that use mRNA technology.Kennedy has made the disorder a key focus of HHS, pledging in April that the agency will "know what has caused the autism epidemic" by September and eliminate exposures. He also said that month that the agency has launched a "massive testing and research effort" involving hundreds of scientists worldwide that will determine the cause.In a statement, Kenvue said it has "continuously evaluated the science and [continues] to believe there is no causal link" between the use of acetaminophen, the generic name for Tylenol, during pregnancy and autism.The company added that the Food and Drug Administration and leading medical organizations "agree on the safety" of the drug, its use during pregnancy and the information provided on the Tylenol label.The FDA website says the agency has not found "clear evidence" that appropriate use of acetaminophen during pregnancy causes "adverse pregnancy, birth, neurobehavioral, or developmental outcomes." But the FDA said it advises pregnant women to speak with their health-care providers before using over-the-counter drugs.The American College of Obstetricians and Gynecologists maintains that acetaminophen is safe during pregnancy when taken as directed and after consulting a health-care provider. Some previous studies have suggested the drug poses risks to fetal development, and some parents have brought lawsuits claiming that they gave birth to children with autism after using it.But a federal judge in Manhattan ruled in 2023 that some of those lawsuits lacked scientific evidence and later ended the litigation in 2024. Some research has also found no association between acetaminophen use and autism.In a note on Friday, BNP Paribas analyst Navann Ty said the firm believes the "hurdle to proving causation [between the drug and autism] is high, particularly given that the litigation previously concluded in Kenvue's favor."-- CNBC's Angelica Peebles contributed to this report.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[William James at CERN (1995)]]></title>
            <link>http://bactra.org/wm-james-at-cern/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45143019</guid>
            <description><![CDATA[This is obviously true of action.  Whatever views your views on free
will, it is indubitable that differing options occur to us, that we compare
them, that we prefer some to others, that eventually we elect one and dismiss
the rest.  More interestingly, James describes the role of selection in
perception, and finds it at every level of neural and mental life.
The sense organs, to begin with, are insensitive to almost all that happens
around them.  When they are excited and transmit nervous impulses to the brain,
these are sifted for significant patterns (often found on dubious grounds).
News of these is relayed to other parts of the brain, which look for more
subtle, more detailed, and more broad patterns, until at last we reach our
perceptions, grouped together by another process of selection into things.
Some of these we attend to; the rest we ignore.]]></description>
            <content:encoded><![CDATA[
William James at CERN
Some Examples of Selection in Minds and Computers
Cosma Rohilla Shalizi



In the famous chapter on ``The Stream of Thought'' in his Principles of
Psychology --- the one where he coins the phrase ``stream of
consciousness'' --- William James considers as the last of the five ``important
characters'' of the stream, the fact that ``It is always more interested in
one part of its object [thought] than in another, and welcomes or rejects, or
chooses, all the while it thinks.''
This is obviously true of action.  Whatever views your views on free
will, it is indubitable that differing options occur to us, that we compare
them, that we prefer some to others, that eventually we elect one and dismiss
the rest.  More interestingly, James describes the role of selection in
perception, and finds it at every level of neural and mental life.
The sense organs, to begin with, are insensitive to almost all that happens
around them.  When they are excited and transmit nervous impulses to the brain,
these are sifted for significant patterns (often found on dubious grounds).
News of these is relayed to other parts of the brain, which look for more
subtle, more detailed, and more broad patterns, until at last we reach our
perceptions, grouped together by another process of selection into things.
Some of these we attend to; the rest we ignore.

``The mind is at every stage a theatre of simultaneous
possibilities.  Consciousness consists in the comparison of these with each
other, the selection of some, and the suppression of the rest by the
reinforcing and inhibiting agency of attention.  The highest and most
elaborated mental products are filtered from the data chosen by the faculty
next beneath, out of the mass offered by the faculty below that, which mass in
turn was sifted from a still larger amount of yet simpler material, and so on.
The mind, in short, works on the data it receives very much as a sculptor works
on his block of stone.  In a sense the statue stood there from eternity.  But
there were a thousand different ones beside it, and the sculptor alone is to
thank for having extricated this one from the rest.  Just so the world of each
of us, how so ever different our several views of it may be, all lay embedded
in the primordial chaos of sensations, which gave the mere matter to
the thought of all of us indifferently.  We may, if we like, by our reasonings
unwind things back to that black and jointless continuity of space and moving
clouds of swarming atoms which science calls the only real world.  But all the
while the world we feel and live in will be that which our ancestors
and we, by slowly cumulative strokes of choice, have extricated out of this,
like sculptors, by simply removing portions of the given stuff.  Other
sculptors, other statues from the same stone!  Other minds, other worlds from
the same monotonous and inexpressive chaos!  My world is but one in a million
alike embedded, alike real to those who may abstract them.  How different must
be the worlds in the consciousness of ant, cuttlefish, or crab!''

James wrote in 1890, and the last century of research into brain and mind have
done nothing to diminish our confidence in this (admittedly very general)
picture.  Indeed, we can now point to parts of the brain which select specific
features out of the signals of the sense organs --- cells in the occipital
lobe, for instance, which respond only to straight lines at certain angles, or
motion, or contrasts of light and darkness --- and we are beginning to
understand the more elaborate construction of things out of such elements.
From almost any authority on neurobiology, cognitive science, psychology, the
philosophy of mind, artificial intelligence or computer science, I could have
quoted passages saying substantially the same things as James, though in worse
prose and without quite the same emphasis on selection.

I propose, now, to see whether these ideas of selection shed any light on
the various uses of ``thinking machines,'' that is to say, computers.

2. The Julia Set
The first is computer-generated art.

The reader is almost certainly familiar with fractals, whether of the
abstract or the naturalistic variety, but is perhaps less likely to know that
computer programs have written verse (rhymed, blank and free), short stories,
and even a novel.  Art critics --- and more particularly, theoretical art
critics --- have been understandably interested in these developments.  Some
have dismissed them as, at best, amusements for the boys in the basement
computer lab across campus, a folk art for those whose native language is C.
Others --- such as the late O. B. Hardison Jr., whose views are set forth with
admirable clarity in Disappearing Through the Skylight --- have
been thrown into a kind of ecstasy of obsolescence.  ``Gazing at the
thirty-nine sea-horses of the Mandelbrot set,'' they say in effect, ``we can
see that human art, Art with a capital A, is at an end, not perhaps this
week-end, but soon: it is later than you think.  The day will come when a human
artist could no more rival a computer than than a sprinter out-race a Ferrari.
The coming art will be digital, perfect, timeless, inimitable, perhaps
incomprehensible.  We and all our works shall pass to dust, and only they will
remain, dreaming their silicon dreams of unknown space.''  Such, in essence and
composite, is the rhetoric.  It seems to me to miss the most interesting aspect
of the new computer art, which is its human angle, and with it the most
plausible future.

The connection between selection and art is, to revert to James,
``obvious''.

``The artist notoriously selects his items, rejecting all tones,
colors, shapes, which do not harmonize with each other and with the main
purpose of his work.  That unity, harmony, `convergence of characters,' as M.
Taine calls it, which gives to works of art their superiority over works of
nature, is wholly due to elimination. Any natural subject will do, if
the artist has wit enough to pounce upon some one feature of it as
characteristic, and suppress all merely accidental items which do not harmonize
with this.''

Now the peculiarity of computer art is that computer programs are very bad at
just this sort of pruning.  They will make a pattern --- of colors, of sounds,
of words --- according to a rule, and that is all.  Give, let us say, a fractal
program one rule, and it will draw you the corresponding picture; change the
rule slightly and it draws another, similar picture.  It does not linger over
the interesting, balk at the trite, turn away from the boring or disturbing: it
is a machine without preferences, ``a gaze blank and pitiless as the sun.''

Artists, notoriously, are different.  Even those who use ``found objects''
select the ones they find interesting, relevant or marketable, and eliminate
everything else in the world.  Selection is inescapable --- or at least not yet
escaped.  Computer programs, then, are poor artists because their powers of
choice are absolutely miniscule; they select not from a pool of possibilities
but a handful of drops, often not even that.

Yet the fact remains that computer-made graphics, if not music or
literature, are quite popular, even delightful.  How can this be?

We have all had the experience of writing out a sentence and then crossing
it out, in bits and pieces, putting in new words and phrases until we find ones
which fit to our satisfaction; until, that is, we select one sentence out of a
mob of candidates.  Computer art programs show us the more promising members of
this mob.  We then pick and choose among them, according to our tastes and
purposes.  It is a strange man who would put a view of the Mandelbrot set on a
condolence-card; and a rash one who would say there is none suitable for this
purpose.

In reality, then, the computer is not an independent artist, but a sort of
dumb assistant, an automatic producer of first sketches.  If the initial
attempt is not perfectly satisfying --- and what first sketch is? --- either
improve it by hand, or modify the computer's instructions slightly and have it
try again.  The afore-mentioned novel was written in the first way, most
fractal pictures are arrived at in the second.

It may be asked, Must this be so? Must the computer always be just an
adjunct, a patient but moronic apprentice?  The key, as we have seen, is to
give the computer preferences, and this does not sound impossible.
Let it produce one picture, one tune, one sonnet, and see whether it satisfy
its principles.  If not change it - a more drastic change the further the
sketch falls short of giving satisfaction - and repeat this cycle until all the
criteria are met.  In fact, the computer could consider a number of sketches in
parallel, working on them simultaneously and combining promising features, and
in this way progress much faster than if it had (so to speak) a one-track mind.
(Some will recognize this as an application of the technique of ``genetic algorithms.'')

The difficulty, and it is a large one, lies in spelling out those principles
in ways simple and clear enough for a computer to act on.  It is hard enough to
give an intelligible account of why we like a painting, switch off the radio
when that tune comes on, gaze at one statue for inspiration and use
another for a door-stop.  It is easy to despair of ever being able to
deduce the Ninth Symphony's superiority to one of its crossed-out
early drafts; it may be better not to contemplate even the attempt.  Yet
without such formal criteria, independent computer art will remain, at most,
a curiosity.

I will only close this subject by saying that there has, recently, been some
work done in this field, though I do not know if the investigators have
considered it in quite this light.  In the closing pages of his recent book
Strange Attractors, Professor Julien Sprott of the University of
Wisconsin describes a survey he and colleagues made of taste in fractal
pictures.  People were asked to rank different fractal pictures, and these
preferences were plotted against the fractal dimension of the picture (a number
which measures how rough, convoluted and ``space-filling'' the image is) and
the ``Lyapunov exponent'' of the equations which generated it (another number,
which measures how quickly the equations amplify small differences in their
variables, the fabled ``sensitive dependence on initial conditions.'')  Strange
as it may seem, people consistently prefer pictures whose fractal dimension and
Lyapunov exponents cluster rather tightly about a constant center.  If, then, a
computer could be programmed to consistently produce images in that area, it
would be a small first step towards ``automated taste.''  A fascinating
speculation on where this might lead --- uninfluenced, as far as I know, by
Prof. Sprott --- is found in Ian McDonald's science fiction novel
Scissors Cut Paper Wrap Stone.

3. James at CERN

After this excursion into Art, a reminder of the notion of perception we have
taken from James is perhaps not out of place: The stream of thought is
incurably and necessarily selective.  Out of the ``blooming, buzzing
confusion'' of sensory nerve impulses, variously organized, bundled and
transformed by different parts of the cortex, a miniscule fraction are selected
--- elected? --- for awareness and conscious attention.  Most of the world,
even most of what impinges on the sense-organs, is simply thrown away.  What
remains is not pure sensation, but an elaborate construction or reconstruction,
influenced by memory, expectation, attention and hypothesis, as well as those
quirks and kluges of the nervous systems whose effects can be learned from any
book on illusions.  If all goes well, this represents the world, not in all its
breadth and detail --- how could it? --- but fairly enough for our purposes.
If not, then, as we say nowadays, ``the model is inadequate and must be
revised'', with luck through a restful stay at a sympathetic institution,
through the elimination of the modeler without it.

It is curious, and I believe not previously noticed, that something very
similar is essential to high-energy physics.  (Physics also needs normal
perception, of course.)

At this point alarms ring in the minds of my colleagues, since we are all
too familiar with books on the profound connection between ``the new physics''
and consciousness and various sophomoric distortions of Asian mysticism.  The
authors of this school are seldom discussed, save by graduate students who
laugh at the errors and covet the royalties.  Rest assured, I shall not discuss
the torture of cats, Buddhist puns, interpretive dance, the Tao of the
relativistic Euler-Lagrange equations, the maya-aspects of
renormalizable gauge field theories, or even how to find a cheap Chinese
restaurant in Copenhagen without a Danish interpreter.

My subject is, instead, rather more massive and solid and sweaty: the
detectors attached to particle accelerators.  A word or three of reminder about
these, too, may not be out of place.

Particle physicists are interested in what the smallest discoverable bits of
matter are, and how they behave.  They are especially interested in how they
behave at very high energies, since these let them probe very short distances
and led to unusual (and hence informative) events, like the creation of new
kinds of particles.  The only practical way to give elementary particles lots
of energy is to accelerate them to very high speeds; the electro-magnetic
machines which do this are called, imaginatively enough, ``accelerators''.
Some accelerators send a beam of particles into a fixed target of more normal
matter, say, gold foil.  The really high-energy ones collide two beams of
particles moving in opposite directions.  There are all sorts of 
fascinating technical issues, on which I may well end up writing a
dissertation --- but another time.

More interesting for us than the accelerators are the detectors, the
machines which sense what happens when the particles collide.  The need for
such machines is quite real.  The events happen far too quickly (over 10^-23
to, at the most lackadaisical, 10^-10, seconds) and in too small a region (on
the order of 10^-18 meters) for human perception.

I come at last to the heart of the matter.  Most of the oceans of data from
detectors are uninteresting and worthless.  Recall that physicists want to
learn about unusual, hard-to-achieve or anomalous events; everything else is
noise.  But common, easily occurring events are by definition the majority;
therefore most events are uninteresting.  Sturgeon's Law states that ``ninety
percent of everything is crap.''  For particle physics, this is wildly
optimistic; interesting events can be outnumbered by billions or trillions to
one.  In theory, combing haystacks for needles is what professors have graduate
students for.  In practice, not even an army would suffice.

What does suffice is very high speed electronics, working on
time-scales of under a microsecond.  The lowest level, known as the trigger,
scans the signals from the detector for an interesting pattern, usually
something very simple, like ``two diametrically opposed detectors activated.''
The data is recorded only if the trigger is (for want of a better word)
triggered.  Once it is recorded, the computers set to work on it, attempting a
more and more detailed reconstruction of the event.  At each stage in the
reconstruction there are ``cuts'', i.e. some events are selected for their
interesting characteristics and the rest discarded.  (For instance, we might
want events where all the outgoing particles concentrate into two back-to-back
jets, and so cut those where lots of other detectors got triggered, along with
a diametrically opposed pair.) Great care is lavished on both the design of the
cuts and the reconstruction, for figuring out what to ignore is, practically,
as important as figuring out what happened.  What bubbles up, in the end, are a
handful of reconstructions selected --- elected? --- for conscious, human
attention.

Piling layers of selection atop each other is essential if we are,
reasonably quickly, to direct our resources where we they ought to be most
useful; triage is a dramatic example.  And in fact successive cuts give
experiments remarkable leverage. (See again our  back of the envelope calculation.)
Physicists have been in love with leverage since Archimedes, but there is a
cost, and to illustrate it I shall, with the reader's kind permission, once
again quote William James, this time on attention:

``[I]n those puzzles where certain lines in a picture form by their
combination an object that has no connection with what the picture ostensibly
represents; or indeed in every case where an object is inconspicuous and hard
to discern from the background; we may not be able to see it for a long time;
but, having once seen it, we can attend to it whenever we like, on account of
the mental duplicate of it which our imagination now bears.  In the meaningless
French words `pas de lieu Rhone que nous,' who can recognize
immediately the English `paddle your own canoe?'  But who that has once noticed
the identity can fail to have it arrest his attention again?  When watching for
the distant clock to strike, our mind is so filled with its image that at every
moment we think we hear the longed-for or dreaded sound.  So of an awaited
footstep.  Every stir in the wood is for the the hunter his game; for the
fugitive his pursuers.  Every bonnet in the street is momentarily taken by the
lover to enshroud the head of his idol.  The image in the mind
is the attention; the preperception, as Mr. Lewes calls it,
is half of the perception of the looked-for thing.

``It is for this reason that men have no eyes but for those aspects of
things which they have already been taught to discern.  Any one of us can
notice a phenomenon after it has once been pointed out, which not one in ten
thousand could ever have discovered for himself.  Even in poetry and the arts,
some one has to come and tell us what aspects we may single out, and what
effects we may admire, before our aesthetic nature can `dilate' to its full
extent and never `with the wrong emotion.'  In kindergarten instruction one of
the exercises is to make the children see how many features they can point out
in such an object as a flower or a stuffed bird.  They readily name the
features they know already, such as leaves, tail, bill, feet.  But they may
look for hours without distinguishing nostrils, claws, scales, etc., until
their attention is called to these details; thereafter, however, they see them
every time.  In short, the only things which we commonly see are those
which we preperceive, and the only things which we preperceive are those
which have been labelled for us, and the labels stamped into our
minds.'' In detectors, ``preperception'' takes the form of the
hard-wired trigger and programmed cuts.  An event which might be fantastically
interesting, if only we knew about it, will be sent into oblivion if it does
not match our a priori criteria at every step.  In this sense,
experimenters only find what they are looking for --- if it exists.

The analogy between detectors and our view of perception is rather close.
(Technological determinists, kindly note that James began writing in 1880, but
the first accelerators were built in the 1930s.)  It would be rash to claim
that large particle detectors are conscious.  In them we have perhaps
the foundations and basic plumbing (with special attention to sewage disposal)
of the building of consciousness; perhaps some scaffolding for the higher
floors as well.

4. The Immoral Equivalent of War, War Itself

Handling outrageous amounts of information arriving very fast, most of it
utterly worthless, all of it of uncertain veracity, and picking out from it a
few events of absolutely essential importance, recording them very fully, and
inferring a detailed picture of the outside world: the job of detector
electronics and military commanders during combat.  The major differences are
that military commanders need the data represented to them in real time, and
accelerator physicists don't give (fancy) orders to the machinery while it's
running.  This is because high energy physics happens much faster than battle.


5. Questions

Has DARPA, the Defense Advanced Research Projects Agency, (nee ARPA, the
Advanced Research Projects Agency, fairy-godmother to AI and the net) ever
funded work on processing experimental data from detectors?  Has Edward Teller
read William James? Have the artificial intelligentsia taken a look at CDF or
ALEPH?  How much fancier must detector electronics be, before they become
conscious?  What is it like to be CDF or ALEPH, to have your consciousness
restricted to two colliding proton beams?  Is it at all significant that WWW is
headquartered at CERN?

Books

William James to begin with, of course.  The Principles of
Psychology was first published in 1890, and is currently available in
two paperback volumes from Dover Books in New York, and in a single hardback as
vol. 53 of the Britannica Great Books.  I have quoted from the latter.  His
Shorter Course is, indeed, substantially shorter, and issued by a
number of different publishing houses.  An even more condensed presentation is
found in Jacques Barzun's worshipful A Stroll with William James
(University of Chicago Press, 1981, currently in paperback).

Following James, P. N. Johnson-Laird's The Computer and the
Mind presents more or less the standard view of the ``artificial
intelligentsia and cognitive cognoscenti'' with lucidity and no more detail
than a common reader may be expected to accept.  Cognitive science is a
``top-down'' approach; good sources for the complentary ``bottom-up'' view of
the brain scientists, which is considerably wetter and messier, are William
Calvin and George Ojemann,
Conversations with Neil's Brain (New York: Addison-Wesley, 1994)
and, at a higher level but still very accessible, Shepherd's delightful
Neurobiology (Oxford University Press, 1983) and A. R. Luria's
The Working Brain (New York: Basic Books, 1973).

More idiosyncratic but still broadly mainstream views can be found in Marvin
Minsky, The Society of Mind (artificial intelligence), William
Calvin, The Cerebral Symphony (neurology) and Daniel Dennett,
Consciousness Explained (philosophy).

The literature on fractals and computer art is swiftly becoming as
unsurveyable as that on anything else.  James Gleick's Chaos is
too well-known to need a plug here.  Benoit Mandelbrot's The Fractal
Geometry of Nature is recommended only for the strongly mathematical.
The picture-books of fractals are beyond counting.  In addition to the results
of his work on aesthetics, Prof. Sprott's book Strange Attractors
contains details of procedures for rapidly making fractal pictures.  It is
interesting to compare abstract fractals with the pictures in James O'Brien,
Design by Accident (New York: Dover, 1964).

Manuel De Landa, War in the Age of Intelligent Machines. New
York: Zone Books, 1991 (distributed by the MIT Press).  Interesting military
history and fascinating, horrifying reports on the latest Pentagon uses of
computers and AI, along with very dubious history of philosophy and ideas,
smothered throughout in an appalling mis-use of technical terms from dynamics.
(Even as metaphors, they don't make much sense.)  Alas, I can't find a better
book on the subject.



Things to Do

Expand the military section.

Consider whether any finite cognitive entity (ugh! cogitator?
cognitator? --- double ugh! --- knower!) wouldn't be forced to be
selective, and hierarchically selective at that.  (Selection I think is a
necessary consequence of finitude; but hierarchies and combinations a la James
seem to follow more from needs to accomplish some purposes quickly, i.e. from
functions.  Cf. Dennett in Elbow Room (especially the discussion
of Laplace's Vast and Considerable Intellect) and Simon in
Sciences of the Artificial on the nature of ``artifacts.''



(Sat Mar 18 20:42:22 CST 1995)



]]></content:encoded>
        </item>
    </channel>
</rss>