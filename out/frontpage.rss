<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 07 Sep 2025 19:01:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[An empty S3 bucket can make your AWS bill explode]]></title>
            <link>https://medium.com/@maciej.pocwierz/how-an-empty-s3-bucket-can-make-your-aws-bill-explode-934a383cb8b1</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45160672</guid>
            <description><![CDATA[Imagine you create an empty, private AWS S3 bucket in a region of your preference. What will your AWS bill be the next morning?]]></description>
            <content:encoded><![CDATA[4 min readApr 29, 2024--Update 7.05.2024The S3 team is working on a fix:https://twitter.com/jeffbarr/status/1785386554372042890Imagine you create an empty, private AWS S3 bucket in a region of your preference. What will your AWS bill be the next morning?A few weeks ago, I began working on a PoC of a document indexing system for my client. I created a single S3 bucket in the eu-west-1 region and uploaded some files there for testing. Two days later, I checked my AWS billing page, primarily to make sure that what I was doing was well within the free-tier limits. Apparently, it wasn’t. My bill was over $1,300, with the billing console showing nearly 100,000,000 S3 PUT requests executed within just one day!Press enter or click to view image in full sizeMy billed S3 usage per day, per regionWhere were these requests coming from?By default, AWS doesn’t log requests executed against your S3 buckets. However, such logs can be enabled using AWS CloudTrail or S3 Server Access Logging. After enabling CloudTrail logs, I immediately observed thousands of write requests originating from multiple accounts or entirely outside of AWS.But why would some third parties bombard my S3 bucket with unauthorised requests?Was it some kind of DDoS-like attack against my account? Against AWS? As it turns out, one of the popular open-source tools had a default configuration to store their backups in S3. And, as a placeholder for a bucket name, they used… the same name that I used for my bucket. This meant that every deployment of this tool with default configuration values attempted to store its backups in my S3 bucket!Note: I can’t disclose the name of the tool I’m referring to, as that would put the impacted companies at risk of data leak (as explained further).So, a horde of misconfigured systems is attempting to store their data in my private S3 bucket. But why should I be the one paying for this mistake? Here’s why:S3 charges you for unauthorized incoming requestsThis was confirmed in my exchange with AWS support. As they wrote:Yes, S3 charges for unauthorized requests (4xx) as well[1]. That’s expected behavior.So, if I were to open my terminal now and type:aws s3 cp ./file.txt s3://your-bucket-name/random_keyI would receive an AccessDenied error, but you would be the one to pay for that request. And I don’t even need an AWS account to do so.Another question was bugging me: why was over half of my bill coming from the us-east-1 region? I didn’t have a single bucket there! The answer to that is that the S3 requests without a specified region default to us-east-1 and are redirected as needed. And the bucket’s owner pays extra for that redirected request.The security aspectWe now understand why my S3 bucket was bombarded with millions of requests and why I ended up with a huge S3 bill. At that point, I had one more idea I wanted to explore. If all those misconfigured systems were attempting to back up their data into my S3 bucket, why not just let them do so? I opened my bucket for public writes and collected over 10GB of data within less than 30 seconds. Of course, I can’t disclose whose data it was. But it left me amazed at how an innocent configuration oversight could lead to a dangerous data leak!What did I learn from all this?Lesson 1: Anyone who knows the name of any of your S3 buckets can ramp up your AWS bill as they like.Other than deleting the bucket, there’s nothing you can do to prevent it. You can’t protect your bucket with services like CloudFront or WAF when it’s being accessed directly through the S3 API. Standard S3 PUT requests are priced at just $0.005 per 1,000 requests, but a single machine can easily execute thousands of such requests per second.Lesson 2: Adding a random suffix to your bucket names can enhance security.This practice reduces vulnerability to misconfigured systems or intentional attacks. At least avoid using short and common names for your S3 buckets.Lesson 3: When executing a lot of requests to S3, make sure to explicitly specify the AWS region.This way you will avoid additional costs of S3 API redirects.Aftermath:I reported my findings to the maintainers of the vulnerable open-source tool. They quickly fixed the default configuration, although they can’t fix the existing deployments.I notified the AWS security team. I suggested that they restrict the unfortunate S3 bucket name to protect their customers from unexpected charges, and to protect the impacted companies from data leaks. But they were unwilling to address misconfigurations of third-party products.I reported the issue to two companies whose data I found in my bucket. They did not respond to my emails, possibly considering them as spam.AWS was kind enough to cancel my S3 bill. However, they emphasized that this was done as an exception.Thank you for taking the time to read my post. I hope it will help you steer clear of unexpected AWS charges!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Campfire: Web-Based Chat Application]]></title>
            <link>https://github.com/basecamp/once-campfire</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45159742</guid>
            <description><![CDATA[Contribute to basecamp/once-campfire development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Campfire
Campfire is web-based chat application. It supports many of the features you'd
expect, including:

Multiple rooms, with access controls
Direct messages
File attachments with previews
Search
Notifications (via Web Push)
@mentions
API, with support for bot integrations

Campfire is single-tenant: any rooms designated "public" will be accessible by
all users in the system. To support entirely distinct groups of customers, you
would deploy multiple instances of the application.
Running in development
bin/setup
bin/rails server

Deploying with Docker
Campfire's Docker image contains everything needed for a fully-functional,
single-machine deployment. This includes the web app, background jobs, caching,
file serving, and SSL.
To persist storage of the database and file attachments, map a volume to /rails/storage.
To configure additional features, you can set the following environment variables:

SSL_DOMAIN - enable automatic SSL via Let's Encrypt for the given domain name
DISABLE_SSL - alternatively, set DISABLE_SSL to serve over plain HTTP
VAPID_PUBLIC_KEY/VAPID_PRIVATE_KEY - set these to a valid keypair to
allow sending Web Push notifications. You can generate a new keypair by running
/script/admin/create-vapid-key
SENTRY_DSN - to enable error reporting to sentry in production, supply your
DSN here

For example:
docker build -t campfire .

docker run \
  --publish 80:80 --publish 443:443 \
  --restart unless-stopped \
  --volume campfire:/rails/storage \
  --env SECRET_KEY_BASE=$YOUR_SECRET_KEY_BASE \
  --env VAPID_PUBLIC_KEY=$YOUR_PUBLIC_KEY \
  --env VAPID_PRIVATE_KEY=$YOUR_PRIVATE_KEY \
  --env SSL_DOMAIN=chat.example.com \
  campfire

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The MacBook has a sensor that knows the exact angle of the screen hinge]]></title>
            <link>https://twitter.com/samhenrigold/status/1964428927159382261</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158968</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nepal Bans 26 Social Media Platforms, Including Facebook and YouTube]]></title>
            <link>https://www.nytimes.com/2025/09/07/world/asia/nepal-bans-social-media-platforms.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158877</guid>
        </item>
        <item>
            <title><![CDATA[Delayed Security Patches for AOSP (Android Open Source Project)]]></title>
            <link>https://twitter.com/grapheneos/status/1964561043906048183</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45158523</guid>
            <description><![CDATA[Something went wrong, but don’t fret — let’s give it another shot.]]></description>
            <content:encoded><![CDATA[Something went wrong, but don’t fret — let’s give it another shot. Some privacy related extensions may cause issues on x.com. Please disable them and try again.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Belling the Cat]]></title>
            <link>https://en.wikipedia.org/wiki/Belling_the_Cat</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157906</guid>
            <description><![CDATA[From Wikipedia, the free encyclopedia]]></description>
            <content:encoded><![CDATA[
							

						From Wikipedia, the free encyclopedia
					

Gustave Doré's illustration of La Fontaine's fable, c. 1868
Belling the Cat is a fable also known under the titles The Bell and the Cat and The Mice in Council. In the story, a group of mice agree to attach a bell to a cat's neck to warn of its approach in the future, but they fail to find a volunteer to perform the job. The term has become an idiom describing a group of persons, each agreeing to perform an impossibly difficult task under the misapprehension that someone else will be chosen to run the risks and endure the hardship of actual accomplishment.[1]
Although often attributed to Aesop, it was not recorded before the Middle Ages and has been confused with the quite different fable of Classical origin titled The Cat and the Mice. In the classificatory system established for the fables by Ben Edwin Perry, it is numbered 613, which is reserved for Mediaeval attributions outside the Aesopic canon.[2]


Synopsis and idiomatic use[edit]
The fable concerns a group of mice who debate plans to nullify the threat of a marauding cat. One of them proposes placing a bell around its neck, so that they are warned of its approach. The plan is applauded by the others, until one mouse asks who will volunteer to place the bell on the cat. All of them make excuses. The story is used to teach the wisdom of evaluating a plan on not only how desirable the outcome would be but also how it can be executed. It provides a moral lesson about the fundamental difference between ideas and their feasibility, and how this affects the value of a given plan.[3]
The fable gives rise to the idiom to bell the cat, which means to attempt, or agree to perform, an impossibly difficult task.[4] Historically 'Bell the Cat' is frequently claimed to have been a nickname given to fifteenth-century Scottish nobleman Archibald Douglas, 5th Earl of Angus in recognition of his part in the arrest and execution of James III's alleged favourite, Thomas (often misnamed as Robert) Cochrane. In fact the earliest evidence for this use is from Hume of Godscroft's history of the Douglases published in 1644,[5] and therefore is more reflective of perception of the idiom in the seventeenth century than the fifteenth.[6] In the 21st century the idiom was adopted by the investigative journalism group Bellingcat.[7] 
The first English collection to attribute the fable to Aesop was John Ogilby's of 1687; in this there is a woodcut (by Francis Barlow), followed by a 10-line verse synopsis by Aphra Behn with the punning conclusion:


Good Councell's easily given, but the effect
Oft renders it uneasy to transact.[8]


Early versions and later interpretations[edit]
"Belling the cat" is one of the proverbs illustrated in Pieter Bruegel I's painting Netherlandish Proverbs (1559).
One of the earliest versions of the story appears as a parable critical of the clergy in Odo of Cheriton's Parabolae.[9] Written around 1200, it was afterwards translated into Welsh, French and Spanish. Sometime later, the story is found in the work now referred to as Ysopet-Avionnet, which is largely made up of Latin poems by the 12th century Walter of England, followed by a French version dating from as much as two centuries later. It also includes four poems not found in Walter's Esopus; among them is the tale of "The Council of the Mice" (De muribus consilium facientibus contra catum). The author concludes with the scornful comment that laws are of no effect without the means of adequately enforcing them and that such parliamentary assemblies as he describes are like the proverbial mountain in labour that gives birth to a mouse.[10]
The fable also appeared as a cautionary tale in Nicholas Bozon's Anglo-Norman Contes Moralisés (1320), referring to the difficulty of curbing the outrages of superior lords.[11] It was in this context too that the story of a parliament of rats and mice was retold in William Langland's allegorical poem Piers Plowman.[12] The episode is said to refer to the Parliament of 1376 which attempted unsuccessfully to remedy popular dissatisfaction over the exactions made by nobles acting in the royal name.[13]
Langland's French contemporary, the satirical Eustache Deschamps, also includes the story among his other moral ballades based on fables as "Les souris et les chats".[14] It has been suggested that in this case too there is a political subtext. The poem was written as a response to the aborted invasion of England in 1386 and contrasts French dithering in the face of English aggression.[15]  The refrain of Deschamps' ballade, Qui pendra la sonnette au chat (who will bell the cat) was to become proverbial in France if, indeed, it does not record one already existing.
In the following century, the Italian author Laurentius Abstemius made of the fable a Latin cautionary tale titled De muribus tintinnabulum feli appendere volentibus (The mice who wanted to bell the cat)[16] in 1499. A more popular version in Latin verse was written by Gabriele Faerno and printed posthumously in his Fabulae centum ex antiquis auctoribus delectae (100 delightful fables from ancient authors, Rome 1564), a work that was to be many times reprinted and translated up to start of the 19th century. Titled simply "The Council of the Mice", it comes to rest on the drily stated moral that 'a risky plan can have no good result'. The story was evidently known in Flanders too, since 'belling the cat' was included among the forty Netherlandish Proverbs in the composite painting of Pieter Bruegel the Elder (1559). In this case a man in armour is performing the task in the lower left foreground.[17] A century later, La Fontaine's Fables made the tale even better known under the title Conseil tenu par les rats (II.2).[18]

A Japanese woodblock illustration by Kawanabe Kyōsai of La Fontaine's fable, 1894.
In mediaeval times the fable was applied to political situations and British commentaries on it were sharply critical of the limited democratic processes of the day and their ability to resolve social conflict when class interests were at stake. This applies equally to the plot against the king's favourite in 15th century Scotland and the direct means that Archibald Douglas chose to resolve the issue. While none of the authors who used the fable actually incited revolution, the 1376 Parliament that Langland satirised was followed by Wat Tyler's revolt five years later, while Archibald Douglas went on to lead a rebellion against King James. During the Renaissance the fangs of the fable were being drawn by European authors, who restricted their criticism to pusillanimous conduct in the face of rashly proposed solutions. A later exception was the Russian fabulist Ivan Krylov, whose adaptation of the story satirises croneyism. In his account only those with perfect tails are to be allowed into the assembly; nevertheless, a tailless rat is admitted because of a family connection with one of the lawmakers.[19]
There still remains the perception of a fundamental opposition between consensus and individualism. This is addressed in the lyrics of "Bell the Cat",[20] a performance put out on DVD by the Japanese rock band LM.C in 2007.[21] This is the monologue of a house cat that wants to walk alone since "Society is by nature evil". It therefore refuses to conform and is impatient of restriction: "your hands hold on to everything – bell the cat". While the lyric is sung in Japanese, the final phrase is in English. Another modernised adaptation based on this fable, that updates the moral, has been published by Patricia McKissack in her Who Will Bell the Cat? (illustrated by Christopher Cyr).[22][23]
There is a Tibetan proverb that is very similar, "Putting a bell on the cat's neck after the mother of mice was consulted"[24]


Several French artists depicted the fable during the 19th century, generally choosing one of two approaches. Gustave Doré and the genre painter Aurélie Léontine Malbet (fl. 1868–1906)[25] pictured the rats realistically acting out their debate. The illustrator Grandville,[26] along with the contemporaries Philibert Léon Couturier [fr] (1823–1901)[27] and Auguste Delierre (1829–1890),[28] caricature the backward practice and pomposity of provincial legislatures, making much the same point as did the Mediaeval authors who first recorded the tale. At the end of the century a publishing curiosity reverts to the first approach. This was in the woodblock print by Kawanabe Kyōsui that appeared in the collection of La Fontaine's fables that was commissioned and printed in Tokyo in 1894 and then exported to France.[29] In the upper left-hand corner a cat is seen through a warehouse window as it approaches across the roofs while inside the rats swarm up the straw-wrapped bales of goods. At its summit the chief rat holds the bell aloft. An earlier Japanese woodblock formed part of Kawanabe Kyōsai's Isoho Monogotari series (1870–80). This shows an assembly of mice in Japanese dress with the proposer in the foreground, brandishing the belled collar.[30]


In the 18th century the fable was one among many set by Louis-Nicolas Clérambault in the fables section of Nouvelles poésies spirituelles et morales sur les plus beaux airs (1730–37).[31] In the following century the text of La Fontaine's fable was set for male voices by Louis Lacombe[32] and by the Catalan composer Isaac Albéniz for medium voice and piano in 1889.[33] In 1950 it was set for four male voices by Florent Schmitt.[34] But while La Fontaine's humorously named cat Rodilardus, and antiquated words like discomfiture (déconfiture), may fit an art song, there have also been faithful interpretations in the field of light music. A popular composer of the day, Prosper Massé, published such a setting in 1846.[35] More recently there has been Pierre Perret's interpretation as part of his 20 Fables inspirées de Jean de la Fontaine (1995),[36] and a jazz arrangement on Daniel Roca's 10 Fables de La Fontaine (2005).[37]


Collective action problem
Who Will Bell the Cat?, a children's picture book based on the fable



^ Strouf, Judie L. H. (2005). The literature teacher's book of lists. Jossey-Bass. p. 13. ISBN 0787975508.

^ Ben Edwin Perry (1965). Babrius and Phaedrus. Loeb Classical Library. Cambridge, MA: Harvard University Press. pp. 545, no. 613. ISBN 0-674-99480-9.

^ "Belling The Cat". Fables of Aesop. 2016-07-05. Retrieved 2021-03-04.

^ "To Bell the Cat" thefreedictionary.com. Retrieved 9 November 2007.

^ David Reid, David Hume of Godscroft's History of the House of Angus, vol. 1 (STS: Edinburgh, 2005), p. 26.

^ Macdougall, Norman (1982). James III: A Political Study. Edinburgh: John Donald. pp. 287–288. ISBN 0859760782.

^ "Bellingcat: Digital Sleuths on the Hunt for Truth"

^ "21. De cato et muribus (1687), illustrated by Francis Barlow". Mythfolklore.net. Retrieved January 26, 2011.

^ Laura (15 May 2009). "Christianizing Aesop: The Fables of Odo of Cheriton". Journey to the Sea. Retrieved 26 January 2011.

^ Ysopet-Avionnet, the Latin and French texts, University of Illinois 1919; fable LXII, pp. 190–192; this is archived online

^ Les contes moralisés de Nicole BozonParis, 1889, pp. 144–145; archived here

^ William's Vision of Piers Plowman by William Langland, edited by Ben Byram-Wigfield (2006), Prologue, lines 146–181; online text here Archived 2011-08-07 at the Wayback Machine

^ "The Parliament of the Rats and Mice". Medieval Forum. SFSU. Archived from the original on 10 March 2022. Retrieved 26 January 2011.

^ Poésies morales et historiques d'Eustache Deschamps, Paris 1832, pp. 188–189

^ Robert Landru, Eustache Deschamps, Fédération des sociétés d'histoire et d'archéologie de l'Aisne, vol. XV 1969, p. 126

^ Fable 195

^ View on Wikimedia Commons

^ "Elizur Wright's translation". Oaks.nvg.org. Retrieved 26 January 2011.

^ Kriloff's Fables, translated by C. Fillingham Coxwell, London 1920, pp. 38–39; archived online

^ "Lyrics | LM.C – Bell The Cat (English)". SongMeanings. 25 April 2010. Retrieved 26 January 2011.

^ "Bell the CAT/LM.C". YouTube. 18 November 2007. Archived from the original on 2021-12-12. Retrieved 26 January 2011.

^ Who will bell the cat?. OCLC 1037155724.

^ "Who Will Bell the Cat?". Publishers Weekly. PWxyz LLC. February 19, 2018. Retrieved April 6, 2022.

^ p. 135, Tsewang, Pema. 2012. Like a Yeti Catching Marmots. Boston: Wisdom Publications.

^ Exhibited at the 1888 Salon; photo online

^ "See online". Archived from the original on July 20, 2011. Retrieved 17 August 2012.

^ "In the Musée Denon de Chalon-sur-Saône". Philibert-leon-couturier.com. Retrieved 17 August 2012.

^ "In the Musée La Fontaine at Château Thierry". Retrieved 17 August 2012.

^ George Baxley. "baxleystamps.com". baxleystamps.com. Retrieved 17 August 2012.

^ View online Archived 2012-03-25 at the Wayback Machine

^ The score is printed in: John Metz, The Fables of La Fontaine: A Critical Edition of the Eighteenth-Century, Pendragon Press 1986, p. 45

^  Op. 85, 1879, Score at Gallica

^ Liedernet

^ Op. 123, Liedernet

^ Bibliographie de la France, 14 March 1846, 127

^ "Pierre Perret chante 20 fables inspirées de Jean de La Fontaine Perret, Pierre, 1934–..." bibliotheques.avignon.fr.

^ Track available on Jamendo








]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Air pollution directly linked to increased dementia risk]]></title>
            <link>https://www.nature.com/articles/d41586-025-02844-9</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157897</guid>
            <description><![CDATA[Long-term exposure accelerates the development of Lewy body dementia and Parkinson’s disease with dementia in people who are predisposed to the conditions. Long-term exposure accelerates the development of Lewy body dementia and Parkinson’s disease with dementia in people who are predisposed to the conditions.]]></description>
            <content:encoded><![CDATA[ A study has found that exposure to air pollution can increase the risk of developing Lewy body dementia.Credit: Sonu Mehta/Hindustan Times/ShutterstockAn analysis of 56 million people has shown that exposure to air pollution increases the risk of developing a particular form of dementia, the third most common type after Alzheimer’s disease and vascular dementia.The study, published in Science on 4 September1, suggests that there is a clear link between long-term exposure to PM2.5 — airborne particles that are smaller than 2.5 micrometres in diameter — and the development of dementia in people with Lewy body dementia or Parkinson’s disease.The study found that PM2.5 exposure does not necessarily induce Lewy body dementia, but “accelerates the development,” in people who are already genetically predisposed to it, says Hui Chen, a clinician–neuroscientist at the University of Technology Sydney in Australia.PM2.5 exposureLewy body dementia is an umbrella term for two different types of dementia: Parkinson’s disease with dementia, and dementia with Lewy bodies. In both cases, dementia is caused by the build-up of α-synuclein (αSyn) proteins into clumps, called Lewy bodies, in the brain’s nerve cells, which cause the cells to stop working and eventually die. Studies have suggested that long-term exposure to air pollution from car-exhaust, wildfires and factory fumes, is linked with increased risks of developing neurodegenerative illnesses, including Parkinson's disease with dementia2.Study co-author Xiaobo Mao, who researches neurodegenerative conditions at Johns Hopkins University in Baltimore, Maryland, says he and his colleagues wanted to determine if PM2.5 exposure also influenced the risk of developing Lewy body dementia. They analysed 2000–2014 hospital-admissions data from 56.5 million people with Lewy body dementia and Parkinson’s disease with or without dementia. The data served to identify people with severe neurological diseases.They found that long-term PM2.5 exposure was associated with an increased risk of hospitalization for all three neurodegenerative conditions, including a 12% increased risk for severe dementia with Lewy bodies that required hospitalization. They noted that living in areas of higher PM2.5 exposure was linked with a higher relative risk of Lewy body dementia — including dementia with Lewy bodies and Parkinson’s disease dementia — compared with Parkinson’s without dementia.Source: Ref. 1The team then performed experiments in mice to investigate why exposure to air pollution affected dementia risk. Mice were exposed to PM2.5 pollution through their nostrils, then the researchers tested for behaviours linked with dementia-like problems. After ten months of PM2.5 exposure, mice showed behavioural challenges in maze exploration tests for spatial memory, and tasks that tested their recognition of new objects. At ten months, the team also observed a substantial increase in the build-up of αSyn in the animals’ brains.Exposure to PM2.5 for ten months also caused the shrinkage of the medial temporal lobe in mice — a brain region which is responsible for memory formation and retrieval. In comparison, there were no changes to the brains of genetically modified mice lacking αSyn, suggesting the protein is required for neurodegenerative pathology.The team also found clumps of αSyn in the gut and lungs of mice exposed to PM2.5, but not in the control or genetically modified mice. Mao says that αSyn acts like a seed, which can propagate and spread from the gut to the brain by way of the gut–brain axis, and eventually cause Alzheimer’s disease or Lewy body dementia. PM2.5 also accumulates in the lungs, causing inflammation before entering the bloodstream and crossing the blood–brain barrier.Predisposition neededThe researchers next investigated gene-expression changes caused by PM2.5 exposure in mice and compared them with gene-expression changes observed in people with Lewy body dementia. They focused on the anterior cingulate cortex — a brain region linked with cognitive deficits in people with dementia. They found a strong correlation of gene-expression changes between PM2.5-exposed mice and people with Lewis body dementia and Parkinson’s disease with dementia, but no correlation with Parkinson’s disease without dementia.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Algebraic Effects in Practice with Flix]]></title>
            <link>https://www.relax.software/blog/flix-effects-intro/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157466</guid>
            <description><![CDATA[Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here's why you'd want to do that, in order of importance.]]></description>
            <content:encoded><![CDATA[ Algebraic effects are not just a research concept anymore. You can use them in real software, today. Here’s why you’d want to do that, in order of importance:


Effects make your code testable
One of the central goals of enterprise software development. Dependency injection, mocking, architecture patterns like clean, hexagonal, DDD are all meant to tackle this. Effects solve this elegantly by separating the “what” from the “how”.


Effects give immediate visibility into what your own and 3rd-party code is doing
Supply chain attacks are real. And they will get worse with more AI slop entering our codebases. Tools like Go’s Capslock fix this by following the whole chain of calls to stdlib functions. Effects provide this by design, as all effects are tracked by the type and effect system.


Effects enable user-defined control flow abstractions
Solving the “what color is your function” problem1. You can also leverage effects to implement Async/await, coroutines, backtracking search and other control flow patterns as user libraries without hard-coding these features into the language.


Algebraic effects come from the pure functional world, serving a purpose similar to monads — keeping track of and having control over side effects. Like monads, they enable us to write our core logic with pure functions and push side effects like IO outwards, closer to application boundaries.
Unlike monads, effects are easy to grasp for a regular developer and give immediate benefits when starting out. For me personally they’re a more natural abstraction for managing side effects — after all, effects are in the name.
Starting out as an academic concept, algebraic effects were introduced to the world by research languages like Eff, Koka, Effekt, Frank, Links, and more recently Ante.
People have also applied effects in practice, so far usually via a monad-based approach, by making libraries in established languages like Scala Kyo / Cats Effect / ZIO; Typescript Effect and Effector, C# language-ext, C libhandler and libmprompt, C++ cpp-effects, various Haskell libraries, etc.
In addition to forcing you into a monadic way of thinking, libraries implementing effects are limited by their host languages.
In this article, I will walk you through applying algebraic effects on a real world example using Flix, a new programming language that is built with effects from the ground up, and supports functional, logic and imperative paradigms.

Table of Contents

Type and Effect System: A Motivating Example
Effect Handlers: Building Intuition
Real-World App: AI movie recommendations
Where to Go From Here
Extra: Why Algebraic Effects are Algebraic and how they relate to monads
Footnotes


Currently only few languages support effects out of the box. The only one that I know of besides Flix is Unison. OCaml has a language extension, but there is no support yet in the type system. Haskell has added support for delimited continuations, but effects are still only available via libraries.
In addition to having a “type and effect system” that improves function signatures and makes sure all effects are handled, Flix supports traits, local mutability via regions, working with immutable or mutable data, and Go/Rust-like structured concurrency. It also has a first-class Datalog integration. But I will only focus on effects here. Let’s start.
Type and Effect System: A Motivating Example 🔗
Imagine a function called calculateSalary:
def calculateSalary(base_salary, bonus_percent):

Based on the function name and the signature, one can assume it’s just a pure function that does some calculations. In a statically typed language you are also guaranteed that the function arguments and outputs will be of a certain type.
But even if the types are correct, nothing stops our little calculateSalary() from, say, sending an offensive email to your grandma2:
def calculateSalary(base_salary, bonus_percent):
    server.sendmail("grandma@family.com", "Your cookies are terrible!")
    return base_salary * (1 + bonus_percent/100)

If, on the other hand, you extend your type system with effects, you will see immediately in the signature that this function may do something fishy:
def calculateSalary(salary: Float64, percent: Float64): 
    Float64 \ {Email} = {
//            ^^^^^^^ Notice the Email effect!

Of course, in real life the issue it’s not usually about the grandma. Instead, this function could throw an exception — still quite dangerous. If you forget to handle the exception, your app will crash. Or another very realistic scenario is that calculateSalary() calls a database to get some employee details for calculations, and you forgot to provide a database connection string. That can also result in an exception or a panic.
Effect Handlers: Building Intuition 🔗
The job of the type and effect system is not just to improve our function signatures. It’s also making sure all the effects are handled somewhere. This is where effect handlers come in.
Usually when people talk about algebraic effects what they’re actually talking about is effect handlers. If you know exceptions, effect handlers are super easy to understand. Here’s a Jenga analogy:
Imagine the call stack is a Jenga tower. New blocks are carefully added each time you call a function.
Saurav S, Unsplash
When an exception is thrown, your whole nice Jenga tower gets destroyed, all the way up to the catch() block. The catch block can safely handle the error, but the stack is unwinded, meaning you lose all of the state you had in your program before throwing the exception. You have to build your tower again, from scratch.
When using effect handlers you can actually go back to your original computation after the handler is done handling the effect. The handler can also return some values back to your program, and it can even resume multiple times with different return values. You also still have the option of not resuming at all and aborting the program — that would be the effect equivalent of exceptions.
Back to the Jenga analogy: if your tower is about to fall down, with effects you can freeze it mid-collapse. You then call someone for help (handler), and they decide whether to let the tower fall, magically restore it to the previous statlte. Or even hand you different blocks to try the same move (call the continuation) again, possibly multiple times with different inputs. Your Jenga tower ends up looking more like a fork or a tree, with multiple different copies of your blocks branching out at some point from the base.
To make this more concrete, let’s start by reproducing exceptions with effects. Here’s how a try/catch looks like in Python:
def divide(x, y):
    try:
        return x / y
    except ZeroDivisionError:
        print("Division by zero!")
        return None

Here’s the equivalent code in Flix. We first define an Exception effect and a divide() function:
eff Exception {
    def throw(msg: String): Void
}

def divide(x: Int32, y: Int32): Int32 \ Exception = 
    if (y == 0) {
        Exception.throw("Division by zero!")
    } else {
        x / y
    }

And then provide a handler for this effect somewhere, preferably close to main():
def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler Exception {
        def throw(msg, _resume) = println("Error: ${msg}")
    }

What this does is registers an effect called Exception with a method throw(). We then perform this effect in our function when there’s an error, similar to throwing an exception in the Python version. Control is transferred to the effect handler, which then decides how to handle the exception, similar to a catch() block in Python.
Notice we never call resume() from the handler. This results in the program being aborted, just like with exceptions. Graphically, this can be represented as follows:
block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["Statement 3"] space:1
    D["Perform Effect"] space:1
    space:1 E["Handle Effect"]
    space:1 F["Process & Exit"]
    space:1 space:1
    
    D --> E
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F fill:#ccffcc,color:#000
So far so good, but this is not much different from Python. To really take full advantage of effect handlers, we can use resume() to return to the original computation and proceed from the line after the effect was performed:
eff ResumableException {
    def askForInput(): Int32
}

def divide(x: Int32, y: Int32): Int32 \ ResumableException = 
    if (y == 0) {
        let newY = ResumableException.askForInput();
        x / newY
    } else {
        x / y
    }

def main(): Unit \ IO = 
    run {
        println(divide(10, 0))
    } with handler ResumableException {
        def askForInput(_, resume) = {
            println("Enter a new divisor:");
            resume(5) // Or get from user input
        }
    }

block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["Statement 3"] space:1
    D["Perform Effect"] space:1
    space:1 E["Handle Effect"]
    space:1 F["Resume"]
    space:1 space:1
    G["Statement 4"] space:1
    H["Statement 5"] space:1
    I["Complete"] space:1
    
    D --> E
    F --> G
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F fill:#ffffcc,color:#000
I called the effect ResumableException here, but it’s not really an exception anymore, because the program continues normally.
At this point we can use this power bestowed on us by effects and handlers to roll our own Async/await:
eff Async {
    def await(url: String): String
}

def fetchData(): String \ Async = 
    Async.await("https://api.example.com/data")

def processData(): String \ Async = {
    let data = fetchData();
    "processed: ${data}"
}

def main(): Unit \ IO = 
    run {
        let result = processData();
        println(result)
    } with handler Async {
        def await(url, resume) = {
            // Simulate async HTTP request
            let result = "data from ${url}";
            resume(result)
        }
    }

See how easy that was? This approach also avoids function coloring, since we didn’t need to use special keywords anywhere. Here’s a graphic version:
block-beta
    columns 2
    
    A["Statement 1"] space:1
    B["Statement 2"] space:1
    C["await operation"] space:1
    space:1 H1["Start async work"]
    space:1 H2["⏳ Long pause..."]
    space:1 H3["⏳ Still waiting..."]
    space:1 H4["✅ Async complete"]
    space:1 F["Resume with result"]
    space:1 space:1
    D["Statement 3"] space:1
    E["Complete"] space:1
    
    C --> H1
    F --> D
    
    style C fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#fff3cd,color:#000
    style H3 fill:#fff3cd,color:#000
    style H4 fill:#d1ecf1,color:#000
    style F fill:#ffffcc,color:#000
    style D fill:#e7f3ff,color:#000
    style E fill:#d4edda,color:#000
That’s cool, but we can do more. Effect handlers allow you to resume multiple times:
eff Choose {
    def choose(): Int32
}

def explore(): String \ Choose = {
    let x = Choose.choose();
    let y = Choose.choose();
    "${x}, ${y}"
}

def main(): Unit \ IO = 
    run {
        println(explore())
    } with handler Choose {
        def choose(_, resume) = {
            resume(1);
            resume(2);
            resume(3)
        }

block-beta
    columns 4
    
    A["Statement 1"] space:1 space:1 space:1
    B["Statement 2"] space:1 space:1 space:1
    C["Statement 3"] space:1 space:1 space:1
    D["Perform Effect"] space:1 space:1 space:1
    space:1 space:1 E["Handle Effect"] space:1
    space:1 F1["Resume 1"] F2["Resume 2"] F3["Resume 3"]
    space:1 G1["Statement 4a"] G2["Statement 4b"] G3["Statement 4c"]
    space:1 H1["Statement 5a"] H2["Statement 5b"] H3["Statement 5c"]
    space:1 R1["Resume to Main"] R2["Resume to Main"] R3["Resume to Main"]
    J["Statement 6"] space:1 space:1 space:1
    K["Complete"] space:1 space:1 space:1
    
    D --> E
    F1 --> G1
    F2 --> G2
    F3 --> G3
    H1 --> R1
    H2 --> R2
    H3 --> R3
    R1 --> J
    R2 --> J
    R3 --> J
    
    style D fill:#ffcccc,color:#000
    style E fill:#ccffcc,color:#000
    style F1 fill:#ffffcc,color:#000
    style F2 fill:#ffffcc,color:#000
    style F3 fill:#ffffcc,color:#000
    style G1 fill:#e6f3ff,color:#000
    style G2 fill:#ffe6f3,color:#000
    style G3 fill:#f3ffe6,color:#000
    style H1 fill:#e6f3ff,color:#000
    style H2 fill:#ffe6f3,color:#000
    style H3 fill:#f3ffe6,color:#000
    style R1 fill:#d4edda,color:#000
    style R2 fill:#d4edda,color:#000
    style R3 fill:#d4edda,color:#000
    style J fill:#cce5ff,color:#000
    style K fill:#b3d9ff,color:#000
With this, you can implement things like coroutines:
block-beta
    columns 3
    
    A1["Coroutine 1: Start"] space:1 A2["Coroutine 2: Start"]
    B1["Statement 1"] space:1 B2["Statement 1"]
    C1["yield to Co2"] H1["Scheduler"] space:1
    space:1 space:1 C2["Statement 2"]
    space:1 space:1 D2["yield to Co1"]
    space:1 H2["Scheduler"] space:1
    D1["Statement 2"] space:1 space:1
    E1["yield to Co2"] H3["Scheduler"] space:1
    space:1 space:1 E2["Statement 3"]
    space:1 space:1 F2["Complete"]
    F1["Complete"] space:1 space:1
    
    C1 --> H1
    H1 --> C2
    D2 --> H2
    H2 --> D1
    E1 --> H3
    H3 --> E2
    
    style C1 fill:#ffcccc,color:#000
    style D2 fill:#ffcccc,color:#000
    style E1 fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#ccffcc,color:#000
    style H3 fill:#ccffcc,color:#000
    style A1 fill:#e6f3ff,color:#000
    style B1 fill:#e6f3ff,color:#000
    style D1 fill:#e6f3ff,color:#000
    style F1 fill:#e6f3ff,color:#000
    style A2 fill:#ffe6f3,color:#000
    style B2 fill:#ffe6f3,color:#000
    style C2 fill:#ffe6f3,color:#000
    style E2 fill:#ffe6f3,color:#000
    style F2 fill:#ffe6f3,color:#000
Generators:
block-beta
    columns 2
    
    A["Start generator"] space:1
    B["Statement 1"] space:1
    C["yield value 1"] H1["Return value"]
    space:1 H2["⏸️ Paused"]
    D["next() called"] H3["Resume generator"]
    E["Statement 2"] space:1
    F["yield value 2"] H4["Return value"]
    space:1 H5["⏸️ Paused"]
    G["next() called"] H6["Resume generator"]
    H["Statement 3"] space:1
    I["return (done)"] H7["Signal complete"]
    
    C --> H1
    H3 --> D
    F --> H4
    H6 --> G
    I --> H7
    
    style C fill:#ffcccc,color:#000
    style F fill:#ffcccc,color:#000
    style I fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H3 fill:#ffffcc,color:#000
    style H4 fill:#ccffcc,color:#000
    style H6 fill:#ffffcc,color:#000
    style H7 fill:#ccffcc,color:#000
    style H2 fill:#fff3cd,color:#000
    style H5 fill:#fff3cd,color:#000
    style D fill:#e7f3ff,color:#000
    style G fill:#e7f3ff,color:#000
And backtracking search:
block-beta
    columns 4
    
    A["Start search"] space:1 space:1 space:1
    B["choose option"] space:1 space:1 space:1
    space:1 H1["Try option 1"] space:1 space:1
    space:1 space:1 C1["Explore path 1"] space:1
    space:1 space:1 D1["❌ Dead end"] space:1
    space:1 H2["Backtrack"] space:1 space:1
    space:1 H3["Try option 2"] space:1 space:1
    space:1 space:1 space:1 C2["Explore path 2"]
    space:1 space:1 space:1 D2["✅ Success!"]
    E["Resume with solution"] space:1 space:1 space:1
    F["Complete"] space:1 space:1 space:1
    
    B --> H1
    H1 --> C1
    D1 --> H2
    H2 --> H3
    H3 --> C2
    D2 --> E
    
    style B fill:#ffcccc,color:#000
    style H1 fill:#ccffcc,color:#000
    style H2 fill:#f8d7da,color:#000
    style H3 fill:#ccffcc,color:#000
    style C1 fill:#fff3cd,color:#000
    style D1 fill:#f8d7da,color:#000
    style C2 fill:#d1ecf1,color:#000
    style D2 fill:#d4edda,color:#000
    style E fill:#ffffcc,color:#000
    style F fill:#d4edda,color:#000
Hopefully this gives you a taste of how effect handlers work. This is just a sketch though — you can read more on this and see examples in the Flix docs.
 Question What's your primary programming language?   TypeScript/JavaScript   Python   Java or other JVM e.g Scala/Kotlin   C#   C++/C   PHP   Go   Rust   Other    ✓ Thanks for your input!
    
These questions help direct new content. Want to get notified when something new is posted? ✓ Saved!
  
Defining our own control flow abstractions is great, but most of the time regular async/await and/or coroutines are enough for the job.
What is extremely useful for daily programming is that effects let you separate the declaration of the effect (the operation, or the effect “constructor”) from it’s implementation, defined by the effect handler.
Add some effect definitions:
eff Database {
    def getUser(id: Int32): Option[User],
    def saveUser(user: User): Unit
}

Then use these definitions to perform effects in your code:
def updateUserEmail(userId: Int32, newEmail: String): Result[String, User] \ {Database} = {
    match Database.getUser(userId) {
        case Some(user) => {
            let updatedUser = {user | email = newEmail};
            Database.saveUser(updatedUser);
            Ok(updatedUser)
        }
        case None => {
            Err("User not found")
        }
    }
}

This replaces the need for dependency injection, since you can provide different handlers for these database operations in production vs testing:
def main(): Unit \\ IO = { // production handler, uses a real database
    run {
        updateUserEmail(123, "new@example.com")
    } with handler Database {
        def getUser(id, resume) = {
		        // real db query
            resume(user)
        }
        def saveUser(user, resume) = {
		        // real db query
            resume()
        }
    }
}

def testUpdateUserEmail(): Unit = { // test handler, just stubs
    let testUser = {id = 123, email = "old@example.com"};
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Ok({testUser | email = "new@example.com"}))
    } with handler Database {
        def getUser(id, resume) = resume(Some(testUser))
        def saveUser(user, resume) = {
            assert(user.email == "new@example.com");
            resume()
        }
    
}

In my opinion, the biggest advantage that effect handlers give is that they abstract away the patterns associated with DDD, Clean Architecture, Hexagonal architecture, etc. commonly found in enterprise code.
All these architectures give you some sort of way to isolate your core logic, which should be pure, from infrastructure and app logic, with deals with external dependencies. But you have to commit to an architecture and the whole team has to be disciplined enough to stick to for this to work.
Using effects encourages separating the definition of effect operations from implementation by default, meaning you don’t really need these architecture patterns anymore.
This is great, since relying on team discipline exclusively rarely works. It also saves a bunch of time otherwise spent on bike shedding.
Effect handlers also allow you to easily install stubs, which you can use to create quick test cases without boilerplate, just by swapping handlers:
def testErrorConditions(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result == Err("User not found"))
    } with handler Database {
        def getUser(_, resume) = resume(None) // Stub: always return None
        def saveUser(_, resume) = resume()             // Won't be called
    }
}

def testSlowDatabase(): Unit = {
    run {
        let result = updateUserEmail(123, "new@example.com");
        assert(result.isOk())
    } with handler Database {
        def getUser(id, resume) = {
            Thread.sleep(100);  // Simulate slow query
            resume(Some({id = id, email = "old@example.com"}))
        }
        def saveUser(user, resume) = {
            Thread.sleep(50);   // Simulate slow save
            resume()
        }
    }
}

You can even make a handler that records all interactions instead of executing them. There are many possibilities here.
Real-World App: AI movie recommendations 🔗
To bring this all together, let’s make a real application using effects.
Our app will fetch some movie data from TheMovieDB, and then use an LLM to recommend some movies based on user preferences provided from the console.
Flix interoperates with the JVM, meaning we can call code from Java, Kotlin, Scala, etc.
First, let’s define the two custom effects we will need: MovieAPI and LLM:
eff MovieAPI {
    def getPopularMovies(): String
}

eff LLM {
    def recommend(movies: String, preferences: String): String
}

We can then perform the effects in main like so, providing some basic handlers that use the Flix’s stdlib HTTP client:
def getRecommendation(preferences: String): String \ {MovieAPI, LLM} = {
    let movies = MovieAPI.getPopularMovies();
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        let suggestion = getRecommendation("action movies");
        println(suggestion)
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            let response = HttpWithResult.get("https://api.themoviedb.org/3/movie/popular", Map.empty());
            match response {
                case Result.Ok(resp) => resume(Http.Response.body(resp))
                case Result.Err(_) => resume("[]")
            }
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            let prompt = "Movies: ${movies}. User likes: ${prefs}. Recommend one movie.";
            let response = HttpWithResult.post("https://api.openai.com/v1/completions", Map.empty(), prompt);
            match response {
                case Result.Ok(resp) => resume(Http.Response.body(resp))
                case Result.Err(_) => resume("Try watching a classic!")
            }
        }
    } with HttpWithResult.runWithIO

Notice that both effects are quite generic. So we can easily swap either the movie API or the LLM provider without touching anything in the core logic:
// Switch to different movie provider
with handler MovieAPI {
    def getPopularMovies(_, resume) = {
        let response = HttpWithResult.get("https://api.imdb.com/popular", Map.empty());
        // ... handle IMDB response format
    }
}

// Switch to different LLM provider  
with handler LLM {
    def recommend(movies, prefs, resume) = {
        let response = HttpWithResult.post("https://api.anthropic.com/v1/messages", Map.empty(), prompt);
        // ... handle Claude response format
    }
}

To get the user input we will need to include the standard Console effect:
def main(): Unit \ {Net, IO} = 
    run {
        Console.println("What movie genres do you enjoy?");
        let preferences = Console.readln();
        let suggestion = getRecommendation(preferences);
        Console.println("Recommendation: ${suggestion}")
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with HttpWithResult.runWithIO

We can also add some basic logs using the standard Logger effect:
def getRecommendation(preferences: String): String \ {MovieAPI, LLM, Logger} = {
    Logger.info("Fetching popular movies...");
    let movies = MovieAPI.getPopularMovies();
    Logger.info("Getting LLM recommendation...");
    LLM.recommend(movies, preferences)
}

def main(): Unit \ {Net, IO} = 
    run {
        /* ... console interaction ... */
    } with handler MovieAPI { /* ... */ }
      with handler LLM { /* ... */ }
      with Console.runWithIO
      with Logger.runWithIO
      with HttpWithResult.runWithIO

That’s it! Let’s run the app and test it manually like so:
 flix run Main.flix
What movie genres do you enjoy?
> sci-fi horror
[INFO] Fetching popular movies...
[INFO] Getting LLM recommendation...
Recommendation: Based on your interest in sci-fi horror, I recommend "Alien" - a perfect blend of both genres!

We can also easily write tests for the core logic by providing test handlers for our movie and LLM effects:
def testRecommendation(): String = 
    run {
        getRecommendation("comedy")
    } with handler MovieAPI {
        def getPopularMovies(_, resume) = {
            resume("""[{"title": "The Grand Budapest Hotel", "genre": "comedy"}]""")
        }
    } with handler LLM {
        def recommend(movies, prefs, resume) = {
            resume("I recommend The Grand Budapest Hotel - perfect for comedy lovers!")
        }
    } with handler Logger {
        def log(_, _, resume) = resume()  // Silent in tests
    }

def runTests(): Unit \ IO = {
    let result = testRecommendation();
    println("Test result: ${result}")
}

Where to Go From Here 🔗
Read the Flix docs
Especially on cool features like effect polymorphism, effect exclusion etc. Check out code examples in the repo
Join the community and contribute with libraries
The Flix compiler and stdlib are quite feature-rich at this point, and having JVM interop means you have all the essentials you need to write practical code. But there are still very few pure Flix libraries. So it’s very valuable to contribute some. The ideas I can think of are, for example, rebuilding standard things like Web frameworks in an effect oriented way,. Or taking advantage of the unique feature set in Flix to build something entirely new.
Explore effect-oriented programming
While I personally like Flix and can recommend it to others, there are other ways you can use effects for real-world software. If you’re in Typescript or Scala, try out Effect or ZIO/Kyo/Cats. If you’re looking for other languages that support effects natively, and you’re not afraid of Haskell-like syntax, check out Unison. They have a bunch of other concepts I find cool, like a better distributed computing model and the code being content-addressed.
Thanks for reading! I hope this article was useful. Hit me up if you have questions or feedback, and check out my website, where I’m exploring sustainable tech and coding practices: relax.software
 Question What should I write about next?   More on Effect-oriented programming   More on the Flix programming language   Comparison of languages and libraries with effects support   General correct and maintainable software content    ✓ Thanks for your input!
    
These questions help direct new content. Want to get notified when something new is posted? ✓ Saved!
  
Extra: Why Algebraic Effects are Algebraic and how they relate to monads 🔗
Okay, practical people have left the room. Following sections are nerds-only.
For some reason, all the content I’ve been reading on algebraic effects uses this term a lot, but no one explains why specifically they’re called “algebraic”. So I did some digging.
Turns out, algebraic effects are “algebraic” because they can be described with laws and equations, like in algebra — the kind we learn at school. Which is I guess why they’re easier to grasp than monads — unlike algebra, you usually don’t study category theory in high school.
But the algebraic part only applies to the effect “constructors”, i.e the operations themselves like get() or put() for the state effect.
Effect handlers, on the other hand, are not algebraic at all, which can be a bit confusing. But it makes sense if you think about it — the purpose of handlers is to act as “deconstructors”, interpreting our algebraic effect operations by means of things that cannot be described by algebraic equations alone, such as continuations .
In fact, effect handlers are often (but not always) implemented via delimited continuations. There are also other, static/lexically scoped and maybe more performant approaches being explored, such as this one
“Real” algebraic effects don’t require monads. Monads and algebraic effects are two different concepts tackling similar problems. One is expressible in terms of the other, but algebraic effects are arguably more flexible.
You could actually implement algebraic effects using a continuation monad. If we don’t care about types, effects are perfectly expressible with monads and vice versa
The problems appear when we introduce types into the picture. In a properly typed world, you can’t actually reproduce the same expressiveness you get with effects using monads. You’ll end up breaking the type system or reducing expressiveness at some point.
Effects are, in this sense, more “powerful” than monads with their natural type system: you can express infinitely many computations with them. E.g if you use a tick() effect and you do a bunch of sequential tick() s, the result will be a distinct computation each time. With monads and their natural type system the set of computations you could express is finite.
Additionally, with monads you commit to a specific interpretation of an effect in advance, while effects completely decouple effect definition from it’s implementation.
Finally, effects are easier to compose than monads. With monad transformers you quickly hit the wall having to define a bunch of different combinations that each have distinct semantics. Effects compose naturally.
So while effect libraries in languages like Typescript and Scala are able to express effects using monads3, and the behavior could be identical at runtime, this cannot replace having an actual type and effect system, with effects being properly typed.
 Question How do you usually learn about new things?   Technical articles (like this one)   Books   Screencasts or videos   Courses, bootcamps or workshops   Other    ✓ Thanks for your input!
    

Footnotes 🔗
Footnotes


“What color is your function” is a problem explored in this article. In languages which have Async baked in via special keywords (e.g JavaScript async/await) it becomes a pain to refactor and to combine synchronous and asynchronous code. If you make one function deep in the call stack async, all the callers will have to be made Async as well, or await() the results. With effects you don’t have this issue as there are no keywords and no special behavior. Async is simply done with effect handlers. ↩


I like the grandma example more than the “launch missiles” popular in the Haskell world. Took it from this article by Kevin Mahoney. It’s somehow more offensive ↩


See some examples in this article. This also shows how Haskell’s new delimited continuation support can be used to implement algebraic effects and handlers ↩


 ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Semantic grep for Claude Code (local embeddings)]]></title>
            <link>https://github.com/BeaconBay/ck</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157223</guid>
            <description><![CDATA[Semantic grep tool for use by AI and humans! Contribute to BeaconBay/ck development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[ck - Semantic Grep by Embedding
ck (seek) finds code by meaning, not just keywords. It's a drop-in replacement for grep that understands what you're looking for — search for "error handling" and find try/catch blocks, error returns, and exception handling code even when those exact words aren't present.
Quick start
cargo install ck-search
# Find error handling patterns (finds try/catch, Result types, etc.)
ck --sem "error handling" src/

# Traditional grep-compatible search still works  
ck -n "TODO" *.rs

# Combine both: semantic relevance + keyword filtering
ck --hybrid "connection timeout" src/
Why ck?
For Developers: Stop hunting through thousands of regex false positives. Find the code you actually need by describing what it does.
For AI Agents: Get structured, semantic search results in JSON format. Perfect for code analysis, documentation generation, and automated refactoring.
For Teams: Works exactly like grep with the same flags and behavior, but adds semantic intelligence when you need it.
Quick Start
# Build from source
cargo build --release

# Index your project for semantic search
./target/debug/ck index src/

# Search by meaning
./target/debug/ck --sem "authentication logic" src/
./target/debug/ck --sem "database connection pooling" src/
./target/debug/ck --sem "retry mechanisms" src/

# Use all the grep features you know
./target/debug/ck -n -C 3 "error" src/
./target/debug/ck -r "TODO|FIXME" .
Core Features
🔍 Semantic Search
Find code by concept, not keywords. Searches understand synonyms, related terms, and conceptual similarity.
# These find related code even without exact keywords:
ck --sem "retry logic"           # finds backoff, circuit breakers
ck --sem "user authentication"   # finds login, auth, credentials  
ck --sem "data validation"       # finds sanitization, type checking

# Get complete functions/classes containing matches (NEW!)
ck --sem --full-section "error handling"  # returns entire functions
ck --full-section "async def" src/        # works with regex too
⚡ Drop-in grep Compatibility
All your muscle memory works. Same flags, same behavior, same output format.
ck -i "warning" *.log              # Case-insensitive  
ck -n -A 3 -B 1 "error" src/       # Line numbers + context
ck --no-filename "TODO" src/        # Suppress filenames (grep -h equivalent)
ck -l "error" src/                  # List files with matches only (NEW!)
ck -L "TODO" src/                   # List files without matches (NEW!)
ck -r --exclude "*.test.js" "bug"  # Recursive with exclusions
ck "pattern" file1.txt file2.txt   # Multiple files
🎯 Hybrid Search
Combine keyword precision with semantic understanding using Reciprocal Rank Fusion.
ck --hybrid "async timeout" src/    # Best of both worlds
ck --hybrid --scores "cache" src/   # Show relevance scores with color highlighting
ck --hybrid --threshold 0.02 query  # Filter by minimum relevance
ck -l --hybrid "database" src/      # List files using hybrid search
🤖 Agent-Friendly Output
Perfect JSON output for LLMs, scripts, and automation.
ck --json --sem "error handling" src/ | jq '.file'
ck --json --topk 5 "TODO" . | jq -r '.preview'
ck --json --full-section --sem "database" . | jq -r '.preview'  # Complete functions
📁 Smart File Filtering
Automatically excludes cache directories, build artifacts, and system files.
# These are excluded by default:
# .git, node_modules, target/, .fastembed_cache, __pycache__

# Override defaults:
ck --no-default-excludes "pattern" .     # Search everything
ck --exclude "dist" --exclude "logs" .   # Add custom exclusions
How It Works
1. Index Once, Search Many
# Create semantic index (one-time setup)
ck index /path/to/project

# Now search instantly by meaning
ck --sem "database queries" .
ck --sem "error handling" .
ck --sem "authentication" .
2. Three Search Modes

--regex (default): Classic grep behavior, no indexing required
--sem: Pure semantic search using embeddings (requires index)
--hybrid: Combines regex + semantic with intelligent ranking

3. Relevance Scoring
ck --sem --scores "machine learning" docs/
# [0.847] ./ai_guide.txt: Machine learning introduction...
# [0.732] ./statistics.txt: Statistical learning methods...
# [0.681] ./algorithms.txt: Classification algorithms...
Advanced Usage
Search Specific Files
# Glob patterns work
ck --sem "authentication" *.py *.js *.rs

# Multiple files
ck --sem "error handling" src/auth.rs src/db.rs

# Quoted patterns prevent shell expansion  
ck --sem "auth" "src/**/*.ts"
Threshold Filtering
# Only high-confidence semantic matches
ck --sem --threshold 0.7 "query"

# Low-confidence hybrid matches (good for exploration)
ck --hybrid --threshold 0.01 "concept"

# Get complete code sections instead of snippets (NEW!)
ck --sem --full-section "database queries"
ck --full-section "class.*Error" src/     # Complete classes
Top-K Results
# Limit results for focused analysis
ck --sem --topk 5 "authentication patterns"

# Great for AI agent consumption
ck --json --topk 10 "error handling" | process_results.py
Directory Management
# Check index status
ck status .

# Clean up and rebuild
ck clean .
ck index .

# Add single file to index
ck add new_file.rs
File Support



Language
Indexing
Tree-sitter Parsing
Semantic Chunking




Python
✅
✅
✅ Functions, classes


JavaScript
✅
✅
✅ Functions, classes, methods


TypeScript
✅
✅
✅ Functions, classes, methods


Haskell
✅
✅
✅ Functions, types, instances



Text Formats: Markdown, JSON, YAML, TOML, XML, HTML, CSS, shell scripts, SQL, and plain text.
Smart Exclusions: Automatically skips .git, node_modules, target/, build/, dist/, __pycache__/, .fastembed_cache, .venv, venv, and other common build/cache/virtual environment directories.
Installation
From Source
git clone https://github.com/BeaconBay/ck
cd ck
cargo install --path ck-cli
Package Managers (Planned)
# Coming soon:
brew install ck-search
apt install ck-search
Architecture
ck uses a modular Rust workspace:

ck-cli - Command-line interface and argument parsing
ck-core - Shared types, configuration, and utilities
ck-search - Search engine implementations (regex, BM25, semantic)
ck-index - File indexing, hashing, and sidecar management
ck-embed - Text embedding providers (FastEmbed, API backends)
ck-ann - Approximate nearest neighbor search indices
ck-chunk - Text segmentation and language-aware parsing
ck-models - Model registry and configuration management

Index Storage
Indexes are stored in .ck/ directories alongside your code:
project/
├── src/
├── docs/  
└── .ck/           # Semantic index (can be safely deleted)
    ├── embeddings.json
    ├── ann_index.bin
    └── tantivy_index/

The .ck/ directory is a cache — safe to delete and rebuild anytime.
Examples
Finding Code Patterns
# Find authentication/authorization code
ck --sem "user permissions" src/
ck --sem "access control" src/
ck --sem "login validation" src/

# Find error handling strategies  
ck --sem "exception handling" src/
ck --sem "error recovery" src/
ck --sem "fallback mechanisms" src/

# Find performance-related code
ck --sem "caching strategies" src/
ck --sem "database optimization" src/  
ck --sem "memory management" src/
Integration Examples
# Git hooks
git diff --name-only | xargs ck --sem "TODO"

# CI/CD pipeline
ck --json --sem "security vulnerability" . | security_scanner.py

# Code review prep
ck --hybrid --scores "performance" src/ > review_notes.txt

# Documentation generation
ck --json --sem "public API" src/ | generate_docs.py
Team Workflows
# Find related test files
ck --sem "unit tests for authentication" tests/
ck -l --sem "test" tests/           # List test files by semantic content

# Identify refactoring candidates  
ck --sem "duplicate logic" src/
ck --sem "code complexity" src/
ck -L "test" src/                   # Find source files without tests

# Security audit
ck --hybrid "password|credential|secret" src/
ck --sem "input validation" src/
ck -l --hybrid --scores "security" src/  # Files with security-related code
Configuration
Default Exclusions
# View current exclusion patterns
ck --help | grep -A 20 exclude

# These directories are excluded by default:
# .git, .svn, .hg                    # Version control
# node_modules, target, build        # Build artifacts  
# .cache, __pycache__, .fastembed_cache  # Caches
# .vscode, .idea                     # IDE files
Custom Configuration (Planned)
# .ck/config.toml
[search]
default_mode = "hybrid"
default_threshold = 0.05

[indexing]  
exclude_patterns = ["*.log", "temp/"]
chunk_size = 512
overlap = 64

[models]
embedding_model = "BAAI/bge-small-en-v1.5"
Performance

Indexing: ~1M LOC in under 2 minutes (with smart exclusions and optimized embedding computation)
Search: Sub-500ms queries on typical codebases
Index size: ~2x source code size with compression
Memory: Efficient streaming for large repositories with span-based content extraction
File filtering: Automatic exclusion of virtual environments and build artifacts
Output: Clean stdout/stderr separation for reliable piping and scripting

Testing
Run the comprehensive test suite:
# Full test suite (40+ tests)
./test_ck.sh

# Quick smoke test (14 core tests)
./test_ck_simple.sh
Tests cover grep compatibility, semantic search, index management, file filtering, and more.
Contributing
ck is actively developed and welcomes contributions:

Issues: Report bugs, request features
Code: Submit PRs for bug fixes, new features
Documentation: Improve examples, guides, tutorials
Testing: Help test on different codebases and languages

Development Setup
git clone https://github.com/your-org/ck
cd ck
cargo build
cargo test
./target/debug/ck index test_files/
./target/debug/ck --sem "test query" test_files/
Roadmap
Current (v0.3+)

✅ grep-compatible CLI with semantic search and file listing flags (-l, -L)
✅ FastEmbed integration with BGE models
✅ File exclusion patterns and glob support
✅ Threshold filtering and relevance scoring with visual highlighting
✅ Tree-sitter parsing and intelligent chunking (Python, TypeScript, JavaScript, Haskell)
✅ Complete code section extraction (--full-section)
✅ Enhanced indexing strategy with v3 semantic search optimization
✅ Clean stdout/stderr separation for reliable scripting
✅ Incremental index updates with hash-based change detection

Near-term (v0.4-0.5)

🚧 Configuration file support
🚧 Package manager distributions

Medium-term (v0.4-0.6)

🔮 Multiple embedding model support
🔮 Advanced ranking algorithms
🔮 Plugin architecture for custom chunkers
🔮 Distributed/remote index support

Long-term (v1.0+)

🔮 IDE integrations (VS Code, IntelliJ, etc.)
🔮 Git integration (semantic diffs, blame)
🔮 Web interface for team usage
🔮 Multi-language semantic understanding

FAQ
Q: How is this different from grep/ripgrep/silver-searcher?
A: ck includes all the features of traditional search tools, but adds semantic understanding. Search for "error handling" and find relevant code even when those exact words aren't used.
Q: Does it work offline?
A: Yes, completely offline. The embedding model runs locally with no network calls.
Q: How big are the indexes?
A: Typically 1-3x the size of your source code, depending on content. The .ck/ directory can be safely deleted to reclaim space.
Q: Is it fast enough for large codebases?
A: Yes. Indexing is a one-time cost, and searches are sub-second even on large projects. Regex searches require no indexing and are as fast as grep.
Q: Can I use it in scripts/automation?
A: Absolutely. The --json flag provides structured output perfect for automated processing. Use --full-section to get complete functions for AI analysis.
Q: What about privacy/security?
A: Everything runs locally. No code or queries are sent to external services. The embedding model is downloaded once and cached locally.
License
Licensed under either of:

Apache License, Version 2.0 (LICENSE-APACHE)
MIT License (LICENSE-MIT)

at your option.
Credits
Built with:

Rust - Systems programming language
FastEmbed - Fast text embeddings
Tantivy - Full-text search engine
clap - Command line argument parsing

Inspired by the need for better code search tools in the age of AI-assisted development.

Start finding code by what it does, not what it says.
cargo build --release
./target/release/ck index .
./target/release/ck --sem "the code you're looking for"
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Serverless Horrors]]></title>
            <link>https://serverlesshorrors.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157110</guid>
            <description><![CDATA[Stories you never want to feel on your own skin]]></description>
            <content:encoded><![CDATA[       ServerlessHorrors is a simple blog where you can read all the horror stories of serverless. Yikes!

Made by Andras who is working on an open-source & self-hostable Heroku / Netlify / Vercel alternative called Coolify.

Have a story?

Write me
Open a PR on Github
     Posts  New 13  May 2025     New 3  May 2025     10  Apr 2025     17  Jan 2025      13  Jan 2025      $22.639,69   I received an insanely bill of 22k USD today from simply using BigQuery on a public data set in the playground...     google    bigquery    sql       13  Jan 2025     6  Jan 2025     5  Jan 2025     1  Sep 2024     25  Jun 2024     6  Jun 2024     26  May 2024     29  Apr 2024     10  Mar 2024     27  Feb 2024      13  Feb 2024      $23,000.420   What is happening?! Someone spammed EchoFox and spiked my Vercel bill to $23k and caused 56k+ accounts and trials...     vercel    bandwidth    ddos       5  Apr 2023     14  Jan 2023     27  Mar 2020           ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I'm a dermatologist and I vibe coded a skin cancer learning app]]></title>
            <link>https://molecheck.info/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45157020</guid>
            <description><![CDATA[For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.]]></description>
            <content:encoded><![CDATA[
            Designed for Mobile
            For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.
            
            https://molecheck.info
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Expression Problem and its solutions]]></title>
            <link>https://eli.thegreenplace.net/2016/the-expression-problem-and-its-solutions/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155877</guid>
            <description><![CDATA[The craft of programming is almost universally concerned with different types of
data and operations/algorithms that act on this data [1]. Therefore, it's
hardly surprising that designing abstractions for data types and operations has
been on the mind of software engineers and programming-language designers
since... forever.]]></description>
            <content:encoded><![CDATA[
                
                The craft of programming is almost universally concerned with different types of
data and operations/algorithms that act on this data [1]. Therefore, it's
hardly surprising that designing abstractions for data types and operations has
been on the mind of software engineers and programming-language designers
since... forever.
Yet I've only recently encountered a name for a software design problem which I
ran into multiple times in my career. It's a problem so fundamental that I was
quite surprised that I haven't seen it named before. Here is a quick problem
statement.
Imagine that we have a set of data types and a set of operations that act on
these types. Sometimes we need to add more operations and make sure they work
properly on all types; sometimes we need to add more types and make sure all
operations work properly on them. Sometimes, however, we need to add both - and
herein lies the problem. Most of the mainstream programming languages don't
provide good tools to add both new types and new operations to an existing
system without having to change existing code. This is called the "expression
problem". Studying the problem and its possible solutions gives great insight
into the fundamental differences between object-oriented and functional
programming and well as concepts like interfaces and multiple dispatch.

A motivating example
As is my wont, my example comes from the world of compilers and interpreters.
To my defense, this is also the example used in some of the seminal historic
sources on the expression problem, as the historical perspective section below
details.
Imagine we're designing a simple expression evaluator. Following the standard
interpreter design pattern, we have
a tree structure consisting of expressions, with some operations we can do on
such trees. In C++ we'd have an interface every node in the expression tree
would have to implement:
class Expr {
public:
  virtual std::string ToString() const = 0;
  virtual double Eval() const = 0;
};

This interface shows that we currently have two operations we can do on
expression trees - evaluate them and query for their string representations.
A typical leaf node expression:
class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  std::string ToString() const {
    std::ostringstream ss;
    ss << value_;
    return ss.str();
  }

  double Eval() const {
    return value_;
  }

private:
  double value_;
};

And a typical composite expression:
class BinaryPlus : public Expr {
public:
  BinaryPlus(const Expr& lhs, const Expr& rhs) : lhs_(lhs), rhs_(rhs) {}

  std::string ToString() const {
    return lhs_.ToString() + " + " + rhs_.ToString();
  }

  double Eval() const {
    return lhs_.Eval() + rhs_.Eval();
  }

private:
  const Expr& lhs_;
  const Expr& rhs_;
};

Until now, it's all fairly basic stuff. How extensible is this design? Let's
see... if we want to add new expression types ("variable reference", "function
call" etc.), that's pretty easy. We just define additional classes inheriting
from Expr and implement the Expr interface (ToString and Eval).
However, what happens if we want to add new operations that can be applied to
expression trees? Right now we have Eval and ToString, but we may want
additional operations like "type check" or "serialize" or "compile to
machine code" or whatever.
It turns out that adding new operations isn't as easy as adding new types. We'd
have to change the Expr interface, and consequently change every existing
expression type to support the new method(s). If we don't control the original
code or it's hard to change it for other reasons, we're in trouble.
In other words, we'd have to violate the venerable open-closed principle,
one of the main principles of object-oriented design, defined
as:

software entities (classes, modules, functions, etc.) should be open for
extension, but closed for modification
The problem we're hitting here is called the expression problem, and the
example above shows how it applies to object-oriented programming.
Interestingly, the expression problem bites functional programming languages as
well. Let's see how.


The expression problem in functional programming
Update 2018-02-05: a new post
discusses the problem and its solutions in Haskell in more depth.
Object-oriented approaches tend to collect functionality in objects (types).
Functional languages cut the cake from a different angle, usually preferring
types as thin data containers, collecting most functionality in functions
(operations) that act upon them. Functional languages don't escape the
expression problem - it just manifests there in a different way.
To demonstrate this, let's see how the expression evaluator / stringifier looks
in Haskell. Haskell is a good poster child for functional programming since its
pattern matching on types makes such code especially succinct:
module Expressions where

data Expr = Constant Double
          | BinaryPlus Expr Expr

stringify :: Expr -> String
stringify (Constant c) = show c
stringify (BinaryPlus lhs rhs) = stringify lhs
                                ++ " + "
                                ++ stringify rhs

evaluate :: Expr -> Double
evaluate (Constant c) = c
evaluate (BinaryPlus lhs rhs) = evaluate lhs + evaluate rhs

Now let's say we want to add a new operation - type checking. We simply have
to add a new function typecheck and define how it behaves for all known
kinds of expressions. No need to modify existing code.
On the other hand, if we want to add a new type (like "function call"), we get
into trouble. We now have to modify all existing functions to handle this new
type. So we hit exactly the same problem, albeit from a different angle.



A visual representation of the expression problem can be helpful to appreciate
how it applies to OOP and FP in different ways, and how a potential solution
would look.
The following 2-D table (a "matrix") has types in its rows and operations in its
columns. A matrix cell row, col is checked when the operation col is
implemented for type row:
In object-oriented languages, it's easy to add new types but difficult to add
new operations:
Whereas in functional languages, it's easy to add new operations but difficult
to add new types:



A historical perspective
The expression problem isn't new, and has likely been with us since the early
days; it pops its head as soon as programs reach some not-too-high level of
complexity.
It's fairly certain that the name expression problem comes from an email sent
by Philip Wadler to a mailing
list deailing with adding generics to Java (this was back in the 1990s).
In that email, Wadler points to the paper "Synthesizing Object-Oriented and
Functional Design to Promote Re-Use" by
Krishnamurthi, Felleisen and Friedman as an earlier work describing the problem
and proposed solutions. This is a great paper and I highly recommend reading it.
Krishnamurthi et.al., in their references, point to papers from as early as 1975
describing variations of the problem in Algol.


Flipping the matrix with the visitor pattern
So far the article has focused on the expression problem, and I hope it's
clear by now. However, the title also has the word solution in it, so let's
turn to that.
It's possible to kinda solve (read on to understand why I say "kinda") the
expression problem in object-oriented languages; first, we have to look at how
we can flip the problem on its side using the visitor pattern. The visitor
pattern is very common for this kind of problems, and for a good reason. It lets
us reformulate our code in a way that makes it easier to change in some
dimensions (though harder in others).
For the C++ sample shown above, rewriting it using the visitor pattern means
adding a new "visitor" interface:
class ExprVisitor {
public:
  virtual void VisitConstant(const Constant& c) = 0;
  virtual void VisitBinaryPlus(const BinaryPlus& bp) = 0;
};

And changing the Expr interface to be:
class Expr {
public:
  virtual void Accept(ExprVisitor* visitor) const = 0;
};

Now expression types defer the actual computation to the visitor, as
follows:
class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  void Accept(ExprVisitor* visitor) const {
    visitor->VisitConstant(*this);
  }

  double GetValue() const {
    return value_;
  }

private:
  double value_;
};

// ... similarly, BinaryPlus would have
//
//    void Accept(ExprVisitor* visitor) const {
//      visitor->VisitBinaryPlus(*this);
//    }
//
// ... etc.

A sample visitor for evaluation would be [2]:
class Evaluator : public ExprVisitor {
public:
  double GetValueForExpr(const Expr& e) {
    return value_map_[&e];
  }

  void VisitConstant(const Constant& c) {
    value_map_[&c] = c.GetValue();
  }

  void VisitBinaryPlus(const BinaryPlus& bp) {
    bp.GetLhs().Accept(this);
    bp.GetRhs().Accept(this);
    value_map_[&bp] = value_map_[&(bp.GetLhs())] + value_map_[&(bp.GetRhs())];
  }

private:
  std::map<const Expr*, double> value_map_;
};

It should be obvious that for a given set of data types, adding new visitors is
easy and doesn't require modifying any other code. On the other hand, adding new
types is problematic since it means we have to update the ExprVisitor
interface with a new abstract method, and consequently update all the visitors
to implement it.
So it seems that we've just turned the expression problem on its side: we're
using an OOP language, but now it's hard to add types and easy to add ops, just
like in the functional approach. I find it extremely interesting that we can do
this. In my eyes this highlights the power of different abstractions and
paradigms, and how they enable us to rethink a problem in a completely different
light.
So we haven't solved anything yet; we've just changed the nature of the problem
we're facing. Worry not - this is just a stepping stone to an actual solution.


Extending the visitor pattern
The following is code excerpts from a C++ solution that follows the extended
visitor pattern proposed by Krishnamurthi et. al. in their paper; I strongly
suggest reading the paper (particularly section 3) if you want to understand
this code on a deep level. A complete code sample in C++ that compiles and runs
is available here.
Adding new visitors (ops) with the visitor pattern is easy. Our challenge is to
add a new type without upheaving too much existing code. Let's see how it's
done.
One small design change that we should make to the original visitor pattern is
use virtual inheritance for Evaluator, for reasons that will soon become
obvious:
class Evaluator : virtual public ExprVisitor {
  // .. the rest is the same
};

Now we're going to add a new type - FunctionCall:
// This is the new ("extended") expression we're adding.
class FunctionCall : public Expr {
public:
  FunctionCall(const std::string& name, const Expr& argument)
      : name_(name), argument_(argument) {}

  void Accept(ExprVisitor* visitor) const {
    ExprVisitorWithFunctionCall* v =
        dynamic_cast<ExprVisitorWithFunctionCall*>(visitor);
    if (v == nullptr) {
      std::cerr << "Fatal: visitor is not ExprVisitorWithFunctionCall\n";
      exit(1);
    }
    v->VisitFunctionCall(*this);
  }

private:
  std::string name_;
  const Expr& argument_;
};

Since we don't want to modify the existing visitors, we create a new one,
extending Evaluator for function calls. But first, we need to extend the
ExprVisitor interface to support the new type:
class ExprVisitorWithFunctionCall : virtual public ExprVisitor {
public:
  virtual void VisitFunctionCall(const FunctionCall& fc) = 0;
};

Finally, we write the new evaluator, which extends Evaluator and supports
the new type:
class EvaluatorWithFunctionCall : public ExprVisitorWithFunctionCall,
                                  public Evaluator {
public:
  void VisitFunctionCall(const FunctionCall& fc) {
    std::cout << "Visiting FunctionCall!!\n";
  }
};

Multiple inheritance, virtual inheritance, dynamic type checking... that's
pretty hard-core C++ we have to use here, but there's no choice. Unfortunately,
multiple inheritance is the only way C++ lets us express the idea that a class
implements some interface while at the same time deriving functionality from
another class. What we want to have here is an evaluator
(EvaluatorWithFunctionCall) that inherits all functionality from
Evaluator, and also implements the ExprVisitorWithFunctionCall
interface. In Java, we could say something like:
class EvaluatorWithFunctionCall extends Evaluator implements ExprVisitor {
  // ...
}

But in C++ virtual multiple inheritance is the tool we have. The virtual part of
the inheritance is essential here for the compiler to figure out that the
ExprVisitor base underlying both Evaluator and
ExprVisitorWithFunctionCall is the same and should only appear once in
EvaluatorWithFunctionCall. Without virtual, the compiler would complain that
EvaluatorWithFunctionCall doesn't implement the ExprVisitor interface.
This is a solution, alright. We kinda added a new type FunctionCall and can
now visit it without changing existing code (assuming the virtual inheritance
was built into the design from the start to anticipate this approach). Here I
am using this "kinda" word again... it's time to explain why.
This approach has multiple flaws, in my opinion:

Note the dynamic_cast in FunctionCall::Accept. It's fairly ugly that
we're forced to mix in dynamic checks into this code, which should supposedly
rely on static typing and the compiler. But it's just a sign of a larger
problem.
If we have an instance of an Evaluator, it will no longer work on the
whole extended expression tree since it has no understanding of
FunctionCall. It's easy to say that all new evaluators should rather be
EvaluatorWithFunctionCall, but we don't always control this. What about
code that was already written? What about Evaluators created in
third-party or library code which we have no control of?
The virtual inheritance is not the only provision we have to build into the
design to support this pattern. Some visitors would need to create new,
recursive visitors to process complex expressions. But we can't anticipate
in advance which dynamic type of visitor needs to be created. Therefore,
the visitor interface should also accept a "visitor factory" which extended
visitors will supply. I know this sounds complicated, and I don't want to
spend more time on this here - but the Krishnamurthi paper addresses this
issue extensively in section 3.4
Finally, the solution is unwieldy for realistic applications. Adding one
new type looks manageable; what about adding 15 new types, gradually over
time? Imagine the horrible zoo of ExprVisitor extensions and dynamic
checks this would lead to.

Yeah, programming is hard. I could go on and on about the limitations of
classical OOP and how they surface in this example [3]. Instead, I'll just
present how the expression problem can be solved in a language that supports
multiple dispatch and separates the defintion of methods from the bodies of
types they act upon.


Solving the expression problem in Clojure
There are a number of ways the expression problem as displayed in this article
can be solved in Clojure using the language's built-in features. Let's start
with the simplest one - multi-methods.
First we'll define the types as records:
(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])

Then, we'll define evaluate as a multimethod that dispatches upon the type
of its argument, and add method implementations for Constant and
BinaryPlus:
(defmulti evaluate class)

(defmethod evaluate Constant
  [c] (:value c))

(defmethod evaluate BinaryPlus
  [bp] (+ (evaluate (:lhs bp)) (evaluate (:rhs bp))))

Now we can already evaluate expressions:
user=> (use 'expression.multimethod)
nil
user=> (evaluate (->BinaryPlus (->Constant 1.1) (->Constant 2.2)))
3.3000000000000003

Adding a new operation is easy. Let's add stringify:
(defmulti stringify class)

(defmethod stringify Constant
  [c] (str (:value c)))

(defmethod stringify BinaryPlus
  [bp]
  (clojure.string/join " + " [(stringify (:lhs bp))
                              (stringify (:rhs bp))]))

Testing it:
user=> (stringify (->BinaryPlus (->Constant 1.1) (->Constant 2.2)))
"1.1 + 2.2"

How about adding new types? Suppose we want to add FunctionCall. First,
we'll define the new type. For simplicity, the func field of
FunctionCall is just a Clojure function. In real code it could be some sort
of function object in the language we're interpreting:
(defrecord FunctionCall [func argument])

And define how evaluate and stringify work for FunctionCall:
(defmethod evaluate FunctionCall
  [fc] ((:func fc) (evaluate (:argument fc))))

(defmethod stringify FunctionCall
  [fc] (str (clojure.repl/demunge (str (:func fc)))
            "("
            (stringify (:argument fc))
            ")"))

Let's take it for a spin (the full code is here):
user=> (def callexpr (->FunctionCall twice (->BinaryPlus (->Constant 1.1)
                                                         (->Constant 2.2))))
#'user/callexpr
user=> (evaluate callexpr)
6.6000000000000005
user=> (stringify callexpr)
"expression.multimethod/twice@52e29c38(1.1 + 2.2)"

It should be evident that the expression problem matrix for Clojure is:
We can add new ops without touching any existing code. We can also add new types
without touching any existing code. The code we're adding is only the new code
to handle the ops/types in question. The existing ops and types could come from
a third-party library to which we don't have source access. We could still
extend them for our new ops and types, without ever having to touch (or even
see) the original source code [4].


Is multiple dispatch necessary to cleanly solve the expression problem?
I've written about multiple dispatch in Clojure
before, and in the previous section we see another example of how to use the
language's defmulti/defmethod constructs. But is it multiple dispatch at
all? No! It's just single dispatch, really. Our ops (evaluate and
stringify) dispatch on a single argument - the expression type) [5].
If we're not really using multiple dispatch, what is the secret sauce that
lets us solve the expression problem so elegantly in Clojure? The answer is -
open methods. Note a crucial difference between how methods are defined in
C++/Java and in Clojure. In C++/Java, methods have to be part of a class and
defined (or at least declared) in its body. You cannot add a method to a class
without changing the class's source code.
In Clojure, you can. In fact, since data types and multimethods are orthogonal
entities, this is by design. Methods simply live outside types - they are first
class citizens, rather than properties of types. We don't add methods to a
type, we add new methods that act upon the type. This doesn't require
modifying the type's code in any way (or even having access to its code).
Some of the other popular programming languages take a middle way. In languages
like Python, Ruby and JavaScript methods belong to types, but we can dynamically
add, remove and replace methods in a class even after it was created. This
technique is lovingly called monkey patching. While initially enticing, it
can lead to big maintainability headaches in code unless we're very careful.
Therefore, if I had to face the expression problem in Python I'd prefer to roll
out some sort of multiple dispatch mechanism
for my program rather than rely on monkey patching.


Another Clojure solution - using protocols
Clojure's multimethods are very general and powerful. So general, in fact, that
their performance may not be optimal for the most common case - which is single
dispatch based on the type of the sole method argument; note that this is
exactly the kind of dispatch I'm using in this article. Therefore, starting with
Clojure 1.2, user code gained the ability to define and use protocols - a
language feature that was previously restricted only to built-in types.
Protocols leverage the host platform's (which in Clojure's case is mostly Java)
ability to provide quick virtual dispatch, so using them is a very efficient way
to implement runtime polymorphism. In addition, protocols retain enough of
the flexibility of multimethods to elegantly solve the expression problem.
Curiously, this was on the mind of Clojure's designers right from the start. The
Clojure documentation page about protocols lists this as one of their
capabilities:

[...] Avoid the 'expression problem' by allowing independent extension of the
set of types, protocols, and implementations of protocols on types, by
different parties. [...] do so without wrappers/adapters
Clojure protocols are an interesting topic, and while I'd like to spend some
more time on them, this article is becoming too long as it is. So I'll leave
a more thorough treatment for some later time and for now will just show how
protocols can also be used to solve the expression problem we're discussing.
The type definitions remain the same:
(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])

However, instead of defining a multimethod for each operation, we now define
a protocol. A protocol can be thought of as an interface in a language like
Java, C++ or Go - a type implements an interface when it defines the set of
methods declared by the interface. In this respect, Clojure's protocols are more
like Go's interfaces than Java's, as we don't have to say a-priori which
interfaces a type implements when we define it.
Let's start with the Evaluatable protocol, that consists of a single method
- evaluate:
(defprotocol Evaluatable
  (evaluate [this]))

Another protocol we'll define is Stringable:
(defprotocol Stringable
  (stringify [this]))

Now we can make sure our types implement these protocols:
(extend-type Constant
  Evaluatable
    (evaluate [this] (:value this))
  Stringable
    (stringify [this] (str (:value this))))

(extend-type BinaryPlus
  Evaluatable
    (evaluate [this] (+ (evaluate (:lhs this)) (evaluate (:rhs this))))
  Stringable
    (stringify [this]
      (clojure.string/join " + " [(stringify (:lhs this))
                                  (stringify (:rhs this))])))

The extend-type macro is a convenience wrapper around the more general
extend - it lets us implement multiple protocols for a given type.
A sibling macro named extend-protocol lets us implement the same protocol
for multiple types in the same invocation [6].
It's fairly obvious that adding new data types is easy - just as we did above,
we simply use extend-type for each new data type to implement our current
protocols. But how do we add a new protocol and make sure all existing data
types implement it? Once again, it's easy because we don't have to modify any
existing code. Here's a new protocol:
(defprotocol Serializable
  (serialize [this]))

And this is its implementation for the currently supported data types:
(extend-protocol Serializable
  Constant
    (serialize [this] [(type this) (:value this)])
  BinaryPlus
    (serialize [this] [(type this)
                       (serialize (:lhs this))
                       (serialize (:rhs this))]))

This time, extending a single protocol for multiple data types -
extend-protocol is the more convenient macro to use.


Small interfaces are extensibility-friendly
You may have noted that the protocols (interfaces) defined in the Clojure
solution are very small - consisting of a single method. Since adding methods to
an existing protocol is much more problematic (I'm not aware of a way to do this
in Clojure), keeping protocols small is a good idea. This guideline comes up in
other contexts as well; for example, it's good practice to keep interfaces in Go
very minimal.
In our C++ solution, splitting the Expr interface could also be a good idea,
but it wouldn't help us with the expression problem, since we can't modify which
interfaces a class implements after we've defined it; in Clojure we can.




[1]"Types of data" and "operations" are two terms that should be fairly
obvious to modern-day programmers. Philip Wadler, in his discussion of
the expression problem (see the "historical perspective" section of the
article) calls them "datatypes" and "functions". A famous quote from Fred
Brooks's The Mythical Man Month (1975) is "Show me your flowcharts and
conceal your tables, and I shall continue to be mystified. Show me your
tables, and I won’t usually need your flowcharts; they’ll be obvious."





[2]Note the peculiar way in which data is passed between Visit* methods
in a Expr* -> Value map kept in the visitor. This is due to our
inability to make Visit* methods return different types in different
visitors. For example, in Evaluator we'd want them to return
double, but in Stringifier they'd probably return
std::string. Unfortunately C++ won't let us easily mix templates and
virtual functions, so we have to resort to either returning void* the
C way or the method I'm using here.
Curiously, in their paper Krishnamurthi et.al. run into the same issue in
the dialect of Java they're using, and propose some language extensions
to solve it. Philip Wadler uses proposed Java generics in his approach.






[3]I can't resist, so just in brief: IMHO inheritance is only good for a
very narrow spectrum of uses, but languages like C++ hail it as the main
extension mechanism of types. But inheritance is deeply
flawed for many other use cases, such as implementations of
interfaces. Java is a bit better in this regard, but in the end the
primacy of classes and their "closed-ness" make a lot of tasks - like the
expression problem - very difficult to express in a clean way.





[4]In fact, there are plenty of examples in which the Clojure implementation
and the standard library provide protocols that can be extended by the
user for user-defined types. Extending user-written protocols and
multimethods for built-in types is trivial. As an exercise, add an
evaluate implementation for java.lang.Long, so that built-in
integers could participate in our expression trees without requiring
wrapping in a Constant.





[5]FWIW, we can formulate a multiple dispatch solution to the expression
problem in Clojure. The key idea is to dispatch on two things: type and
operation. Just for fun, I coded a prototype that does this which you can
see here.
I think the approach presented in the article - each operation being its
own multimethod - is preferable, though.





[6]The sharp-eyed reader will notice a cool connection to the expression
problem matrix. extend-type can add a whole new row to the matrix,
while extend-protocol adds a column. extend adds just a single
cell.




            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The "impossibly small" Microdot web framework]]></title>
            <link>https://lwn.net/Articles/1034121/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155682</guid>
            <description><![CDATA[The Microdot web framework is quite small, as its name would imply; it supports both standard C [...]]]></description>
            <content:encoded><![CDATA[

Ready to give LWN a try?

With a subscription to LWN, you can stay current with what is happening in the Linux and free-software community and take advantage of subscriber-only site features.  We are pleased to offer you a free trial subscription, no credit card required, so that you can see for yourself.  Please, join us!



The Microdot
web framework is quite small, as its name would imply; it supports both
standard CPython and MicroPython,
so it can be used on systems ranging from internet-of-things (IoT) devices
all the way up to large, cloudy servers.  It was developed by Miguel
Grinberg, who gave a presentation about it at EuroPython 2025.  His name
may sound familiar from his well-known Flask
Mega-Tutorial, which has introduced many to the Flask lightweight Python-based
web framework.  It should come as no surprise, then, that Microdot is
inspired by its rather larger cousin, so Flask enthusiasts will find much
to like in Microdot—and will come up to speed quickly should their needs turn
toward smaller systems.



We have looked at various pieces of this software stack along the way: Microdot itself in January 2024, MicroPython in 2023, and Flask as part of a look at Python microframeworks in
2019.



Grinberg began his talk with an introduction.  He has been living in
Ireland for a few years and "I make stuff".  That includes open-source projects, blog posts (on a
Flask-based blog platform that he wrote), and "a bunch of books".
He works for Elastic and is one of the maintainers of the Elasticsearch
Python client, "so maybe you have used some of the things that I
made for money".


Why?


With a chuckle, he asked: "Why do we need another web framework?  We
have so many already."  The story starts with a move that he made to
Ireland from the US in 2018; he rented a house with a "smart" heating
controller and was excited to use it.  There were two thermostats, one for
each level of the house, and he was "really looking forward to the
winter" to see the system in action.



As might be guessed, he could set target temperatures in each thermostat;
they would communicate with the controller that would turn the heating on
and off as needed.  In addition, the system had a web server that could be
used to query various parameters or to start and stop the heaters.  You
could even send commands via SMS text messages; "there's a SIM card
somewhere in that box [...] very exciting stuff".



When winter rolled around, it did not work that well, however; sometimes
the house was too chilly or warm and he had to start and stop the heaters
himself. He did some debugging and found that the thermostats were
reporting temperatures that were off by ±3°C, "which is too much for
trying to keep the house at 20°".  The owner of the house thought that
he was too used to the US where things just work; "at least she thinks that in America everything is super-efficient,
everything works, and she thought 'this is the way things work in
Ireland'".  So he did not make any progress with the owner.



At that point, most people would probably just give up and live with the
problem; "I hacked my heating controller instead".  He set the
temperatures in both thermostats to zero, which effectively disabled their
ability to affect the heaters at all, and built two small boards running
MicroPython, each connected to a temperature and humidity sensor device.
He wrote code that would check the temperature every five minutes and send
the appropriate commands to start or stop the heaters based on what it
found.



So the second half of his first winter in Ireland went great.  The sensors
are accurate to ±0.5°C, so "problem solved".  But, that led to a new
problem for him.  "I wanted to know things: What's the temperature right
now?  Is the heating running right now or not?  How many hours did it run
today compared to yesterday?"  And so on.



He added a small LCD screen to display some information, but he had to
actually go to the device and look at it; what he really wanted was to be
able to talk to the device over WiFi and get information from the couch
while he was watching TV. "I wanted to host a web server [...]  that
will show me a little dashboard".



So he searched for web frameworks
for MicroPython; in the winter of 2018-2019, "there were none".
Neither Flask nor Bottle,
which is a good bit smaller, would run on MicroPython; both are too large
for the devices,
but, in addition, the standard library for MicroPython is a subset of that of
CPython, so many things that they need are missing. A "normal
person" would likely have just accepted that and moved on; "I
created a web framework instead."


Demo


He brought one of his thermostat devices to Prague for the conference and
did a small demonstration of it operating during the talk.  The device was
connected to his laptop using USB, which provided power, but also a serial
connection to the board.  On the laptop, he used the rshell
remote MicroPython shell to talk to the board, effectively using the laptop
as a terminal.





He started the MicroPython read-eval-print loop (REPL) on the board in
order to simulate the normal operation of the board.  When it is plugged
into the wall, rather than a laptop, it will boot to the web server, so he
made that happen with a soft-reboot command.  The device then connected to
the conference WiFi and gave him the IP address (and port) where the server
was running.



He switched over to Firefox on his laptop and visited the site, which showed a
dashboard that had the current temperature (24.4°) and relative humidity
(56.9%) of the room.  He also used curl from the laptop to contact the
api endpoint of the web application, which returned JSON with the
two values and the time.  There is no persistent clock on the board, so the
application contacts an NTP server to pick up the time when it boots; that
allows it to report the last time a measurement was taken.



Grinberg said that he wanted to set the expectations at the right level by
looking at the capabilities of the microcontrollers he often uses with
Microdot.  For example, the ESP8266 in his thermostat device has 64KB of
RAM and up to 4MB of flash.  The ESP8266 is the smallest and least expensive (around €5)
device with WiFi that
he has found; there are many even smaller devices, but they lack
the networking required for running a web server.  The other devices
he uses are the Raspberry Pi Pico W with 2MB of flash and 256KB of RAM and
the ESP32 with up to 8MB of flash and 512KB of RAM.  He contrasted those
with his laptop, which has 32GB of RAM, so "you need 500,000
ESP8266s" to have the same amount of memory.


Features


The core framework of Microdot is in a single microdot.py
file.  It is fully asynchronous, using the MicroPython
subset of the CPython
asyncio module, so it can run on both interpreters.  It uses
asyncio because that is the only way to do concurrency on the
microcontrollers; there is no support for processes or threads on those devices.



Microdot has Flask-style route
decorators to define URLs for the application.  It has Request
and Response
classes, as well as hooks
to run before and after requests, he said.  Handling query strings,
form data, and JSON are all available in Microdot via normal Python
dictionaries.  Importantly, it can handle streaming requests and responses;
because of the limited memory of these devices, it may be necessary to split
up the handling of larger requests or responses.



It supports setting
cookies and sending static
files.  Web applications can be constructed from a set of modules, using sub-applications,
which are similar to Flask
blueprints.  It also has its own web
server with TLS support.  "I'm very proud of all the stuff I was
able to fit in the core Microdot framework", Grinberg said.



He hoped that attendees would have to think for a minute to come up with
things that are missing from Microdot, but they definitely do exist.  There
are some officially
maintained extensions, each in its own single .py file, to
fill some of those holes.  They encompass functionality that is important,
but he did not want to add to the core because that would make it too large
to fit on the low-end ESP8266 that he is using.



There is an extension for multipart
forms, which includes file uploads; "this is extremely complicated
to parse, it didn't make sense to add it into the core because most people
don't do this".  There is support for WebSocket
and server-sent
events (SSE).  Templates
are supported using utemplate
for both Python implementations or Jinja, which only
works on CPython.  There are extensions for basic
and token-based authentication and for secure
user logins with session data; the latter required a replacement for
the CPython-only PyJWT, which Grinberg
wrote and contributed to MicroPython as jwt.py.
There is a small handful of other extensions that he quickly mentioned as well.



"I consider the documentation as part of the framework"; he is
"kind of fanatical" about documenting everything.  If there is
something missing or not explained well, "it's a bug that I need to
fix".  He writes books, so the documentation is organized similarly;
it comes in at 9,267 words, which equates to around 47 minutes of reading
time.  There is 100% test coverage, he said, and there are around 30
examples, with more coming.



A design principle that he follows is "no dark magic".  An example
of dark magic to him is the Flask
application context, "which very few people understand".  In
Microdot, the request object is explicitly passed to each route function.
Another example is the dependency
injection that is used by the FastAPI framework to add
components; Microdot uses explicit decorators instead.



He used the cloc
utility to count lines of code, while ignoring comments and blank
lines.  Using that, Django
comes in at 110,000 lines, Flask plus its essential Werkzeug library
is 15,500 lines, FastAPI with Starlette is 14,900 lines, Bottle is
around 3,000 lines, while the Microdot core has 765 lines ("believe it
or not") and a full
install with all the extensions on MicroPython comes in at just shy of 1,700
lines of code.



He ended with an example of how Microdot can be so small by comparing the
URL matching in Flask with Microdot.  The Flask version does lots more than
Microdot, with more supported types of arguments in a URL and multiple classes
in the werkzeug.routing
module; it has 1,362 lines of code.  For Microdot, there is a more
limited set of URL arguments, though there is still the ability to define
custom types, and a single
URLPattern class; all of that is done in 63 lines of
code. "I don't intend to support everything that Flask supports, in
terms of routing, but I intend to support the 20% that covers 80% of the
use cases."  That is the overall mechanism that he has used to get to
something that is so small.



An audience member asked about whether the Microdot code was minified in
order to get it to fit.  Grinberg said that doing so was not all that
useful for MicroPython, but the code is smaller on the board because it is
precompiled on another system; that results in a
microdot.mpy file, which is bytecode for MicroPython.  For
example, on the
low-end device he is using for his thermostats, Microdot would not be able
to be compiled on the device itself.  There are some other tricks that can
also be used for reducing the RAM requirements, like putting the code into
the firmware as part of the MicroPython binary.



The final question was about performance, and how many requests per second
could be handled. Grinberg said that he did not
have any real numbers, but that the device he demonstrated is "really
really slow".  That question led to a blog
post in late July where Grinberg tried to more fully answer it.



[I would like to thank the Linux Foundation, LWN's travel sponsor, for
travel assistance to Prague for EuroPython.]

           Index entries for this article
           ConferenceEuroPython/2025
            PythonWeb
            

            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Lightweight tool for managing Linux virtual machines]]></title>
            <link>https://github.com/ccheshirecat/flint</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154857</guid>
            <description><![CDATA[Lightweight tool for managing linux virtual machines - ccheshirecat/flint]]></description>
            <content:encoded><![CDATA[🌀 Flint — KVM Management, Reimagined

  


  
    A single <8MB binary with a modern Web UI, CLI, and API for KVM.
    No XML. No bloat. Just VMs.
  


  
    
  
  
    
  
  
    
  



Flint is a modern, self-contained KVM management tool built for developers, sysadmins, and home labs who want zero bloat and maximum efficiency. It was built in a few hours out of a sudden urge for something better.

🚀 One-Liner Install
Prerequisites: A Linux host with libvirt and qemu-kvm installed.
curl -fsSL https://raw.githubusercontent.com/ccheshirecat/flint/main/install.sh | sh
Auto-detects OS/arch, installs to /usr/local/bin, and you're ready in seconds.

✨ Core Philosophy

🖥️ Modern UI — A beautiful, responsive Next.js + Tailwind interface, fully embedded.
⚡ Single Binary — No containers, no XML hell. A sub-8MB binary is all you need.
🛠️ Powerful CLI & API — Automate everything. If you can do it in the UI, you can do it from the command line or API.
📦 Frictionless Provisioning — Native Cloud-Init support and a simple, snapshot-based template system.
💪 Non-Intrusive — Flint is a tool that serves you. It's not a platform that locks you in.


🏎️ Quickstart
1. Start the Server
flint serve

Web UI: http://localhost:5550
API: http://localhost:5550/api

2. Use the CLI
# List your VMs
flint vm list --all

# Launch a new Ubuntu VM named 'web-01'
flint launch ubuntu-24.04 --name web-01

# SSH directly into your new VM
flint ssh web-01

# Create a template from your configured VM
flint snapshot create web-01 --tag baseline-setup

# Launch a clone from your new template
flint launch --from web-01 --name web-02

📖 Full Documentation
While Flint is designed to be intuitive, the full CLI and API documentation, including all commands and examples, is available at:
➡️ DOCS.md

🔧 Tech Stack

Backend: Go 1.25+
Web UI: Next.js + Tailwind + Bun
KVM Integration: libvirt-go
Binary Size: ~8.4MB (stripped)



  🚀 Flint is young, fast-moving, and designed for builders.
  Try it. Break it. Star it. Contribute.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I recreated Windows XP as my portfolio]]></title>
            <link>https://mitchivin.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154609</guid>
            <description><![CDATA[A Windows XP–style interactive portfolio showcasing design, video, and UI work by Mitch Ivin.]]></description>
            <content:encoded><![CDATA[To begin, click on Mitch Ivin to log inMitch IvinGraphic DesignerRestart MitchIvin XPAfter you log on, the system's yours to explore. Every detail has been designed with a purpose. Tap on the user icon to begin]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The key to getting MVC correct is understanding what models are]]></title>
            <link>https://stlab.cc/tips/about-mvc.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154501</guid>
            <description><![CDATA[stlab hosts modern, modular C++ algorithms and data structures.]]></description>
            <content:encoded><![CDATA[
        Smalltalk MVC is defined in Design Pattern as:


  MVC Consists of three kinds of objects. The Model is the application object, the View is its screen presentation, and the Controller defines the way the user interface reacts to user input.1


However this definition has been abused over the years - Back in 2003 I gave a talk citing how bad Apple’s definition was. At the time it stated:


  A view object knows how to display and possibly edit data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects… Controllers are often the least reusable objects in an application, but that’s acceptable…2


Of course it isn’t acceptable and, over the years, Apple has refined their definition and now acknowledge the distinction between the traditional Smalltalk version of MVC and the Cocoa version.3 But the Cocoa version is still defined much as it was before:


  A view object knows how to display, and might allow users to edit, the data from the application’s model… A controller object acts as the intermediary between the application’s view objects and its model objects…3


In looking at how iOS applications are written the sentiment that controllers (and now view-controllers) are often the least reusable components in an application still flourishes, even if it is now unstated.

MVC (I’ll always use that term to refer to the Smalltalk form) has the following structure:















figure: Smalltalk MVC4


Here the solid lines imply a direct association. And the dashed lines an indirect association by an observer. So what we see is that the model is unaware of the view and controller, except indirectly through notifications, and hence the code in the Model is reusable. The controller and view bind to the model, not the other way around.

Often the function of the Controller and View are tightly coupled into a “widget” or “control”. When Apple talks about a View-Controller in their model they are talking about a grab-bag of an uber-widget that is a composite of UIView widgets and multiple models. From what I’ve seen, including in Apple’s example code, it is usually a pretty big mess.

The key to getting MVC correct is understanding what models are. A model is simply an object5 which can be observed (a requirement for attaching views). For example, in ObjC an int is an object, but it is not observable. However, an ObjC object with an int property is observable using Key-Value Observing6.  A model may encapsulate complex relationships between the model’s properties. A trivial model is one where each property is completely independent (think C struct vs. C++ class). From a notification the view should be able to determine, at a minimum:


  What changed. It may be as simple as “the model bound to the view”.
  The new value to display.


For example, let’s say our model is a trivial observable boolean (I can’t imagine a simpler model). What we want is a checkbox that binds to the observable boolean. When the controller requests a change in value, the boolean is updated, and the view is notified of the new state of the model. The model is unaware of what UI is attached to it, and in fact there could be multiple UIs, including something like a scripting system, attached to the same instance of the model. This is a form of data binding - though most data binding systems replicate the problems of their underlying widget set by treating the model as if it were observing the view, not the other way around.

Contrast this with most UI frameworks where you have a checkbox widget from which you can query the value and you receive a notification when the value has changed. This is pushing a model into the widget. With MVC you never ask a question like “what is the default state of this checkbox?” - the default state of the view is always the current state of the model. You would also never get the state of the checkbox - the state of the checkbox is simply a reflection of the state of the model. In a system where you get the state of a checkbox you are binding two models together by treating one as a view/controller of the other. Such a pattern doesn’t scale beyond trivial models, and even for those it introduces some ambiguity.

I conjecture that one of the reasons why MVC has been so screwed up is because, unlike in Smalltalk, writing something as simple as an observable boolean is a bit of a pain in a language like Pascal or C. You quickly get into object ownership and lifetime issues and how to write bind expressions. If one also assumes that you have a 1:1 mapping from UI to model then there is some inherent inefficiency in the generalization. The Lisa team made some major compromises and the rest of the industry followed along.7

To support more complex views, the notification may need to specify what parts of the model changed and how those parts changed. For example, “image 58 was removed from the sequence”. A complete model is one that can support any view of that model type efficiently (related to the notion of a complete type and a type’s efficient basis).

One additional attribute of MVC is that it is a composite pattern. This is hinted at by the direct connection between the Controller and the View. As I said early, the view may contain state, this state is itself an object, and because this state is also displayed within the view it is observable. It is another model. I refers to this as the view’s model. This model may include things such as the visibility of a window, the tab the user was last looking at, and the portion of the model being viewed.

Identifying what the models are in your system is important. We usually do pretty good at identifying the major models. Such as “this is an image” - but often fall short of identifying the complete model, i.e. “this is an image with a collection of settings.” We end up with our model spread out within the code (an incidental type) and it makes it more difficult to deal with it.

A common model that is often completely overlooked is the model for function arguments. When you have a command, button, gesture, or menu item in your application, these are bound to a function. The function itself is not typically a zeroary function but rather has a set of arguments that are constructed through other parts of the UI. For example, if I have a list of images in my application, I might have a button to delete the selected images. Here the current selection is the argument to my delete command. To create a UI for the selection I must create a model of the arguments to my function. A precondition of delete is that the selection is not empty. This precondition must be observable in the argument model so it can be reflected in the view by disabling or hiding the button and in the controller be disallowing the user to click the button and issue the command. The same argument model can be shared for multiple commands within an application.


  
    

      Gamma, Erich. “1.2 Design Patterns in Smalltalk MVC.” Design Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-Wesley, 1995. N. pag. Print. ↩
    
    

      http://smartfriends.com/U/Presenters/untangling_software.pdf (Don’t bother reading, this was an incomprehensible talk.) ↩
    
    

      https://developer.apple.com/library/content/documentation/General/Conceptual/CocoaEncyclopedia/Model-View-Controller/Model-View-Controller.html ↩ ↩2
    
    

      https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller ↩
    
    

      Stepanov, Alexander A., and Paul McJones. “1.3 Objects.” Elements of Programming. Upper Saddle River, NJ: Addison-Wesley, 2009. N. pag. Print. ↩
    
    

      https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/KeyValueObserving/KeyValueObserving.html ↩
    
    

      https://en.wikipedia.org/wiki/Object-oriented_user_interface ↩
    
  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I'm making an open-source platform for learning Japanese]]></title>
            <link>https://kanadojo.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152940</guid>
        </item>
        <item>
            <title><![CDATA[A Navajo weaving of an integrated circuit: the 555 timer]]></title>
            <link>https://www.righto.com/2025/09/marilou-schultz-navajo-555-weaving.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152779</guid>
            <description><![CDATA[The noted Diné (Navajo) weaver Marilou Schultz recently completed an intricate weaving composed of thick white lines on a black background, ...]]></description>
            <content:encoded><![CDATA[
The noted Diné (Navajo) weaver Marilou Schultz recently completed an intricate weaving
composed of
thick white lines on a black background, punctuated with reddish-orange diamonds.
Although this striking rug may appear abstract, it shows the internal circuitry of a tiny silicon chip known
as the 555 timer.
This chip has hundreds of applications in
everything from a sound generator to a windshield wiper controller.
At one point, the 555 was the world's best-selling integrated circuit with billions sold.
But how did the chip get turned into a rug?

The 555 chip is constructed from a tiny flake of silicon with a layer of metallic wiring on top.
In the rug, this wiring is visible as the thick white lines, while the silicon forms the black background.
One conspicuous feature of the rug is the reddish-orange diamonds around the perimeter.
These correspond to the connections between the silicon chip and its eight pins. Tiny golden bond wires—thinner than a human hair—are attached to the square bond pads to provide these connections.
The circuitry of the 555 chip contains 25 transistors, silicon devices that can switch
on and off.
The rug is dominated by three large transistors, the filled squares with a 王 pattern inside, while the remaining transistors are represented by small dots.
The weaving was inspired by a photo of the 555 timer die taken by
Antoine Bercovici
(Siliconinsider); I suggested this photo to Schultz as a possible subject
for a rug.  The diagram below compares the 
weaving (left) with the die photo (right).
As you can see, the weaving closely follows the actual chip, but there are a few artistic differences.
For instance, two of the bond pads have been removed, the circuitry at the top has been simplified,
and the part number at the bottom has been removed.
A comparison of the rug (left) and the original photograph (right).
Dark-field image of the 555 timer is courtesy of Antoine Bercovici.
Antoine took the die photo with a dark field microscope, a special type of microscope that
produces an image on a black background.
This image emphasizes the metal layer on the top of the die.
In comparison, a standard bright-field microscope produced the image below.
When a chip is manufactured, regions of silicon are "doped" with impurities to create transistors
and resistors.
These regions are visible in the image below as subtle changes in the color of the silicon.

In the weaving, the chip's design appears almost monumental, making it easy to forget that the
actual chip is microscopic.
For the photo below,
I obtained a version of the chip packaged in a metal can, rather than the typical rectangle of
black plastic.
Cutting the top off the metal can reveals the tiny chip inside, with eight gold bond wires connecting the
die to the pins of the package.
If you zoom in on the photo, you may recognize the three large transistors that dominate the rug.
The 555 timer die inside a metal-can package, with a penny for comparison. Click this image (or any other) for a larger version.
The artist, Marilou Schultz, has been creating chip rugs since 1994, when Intel commissioned a
rug based on the Pentium as a gift to AISES (American Indian Science & Engineering Society).
Although Schultz learned weaving as a child, the Pentium rug was a challenge due to its complex pattern
and lack of symmetry; a day's work might add just an inch to the rug.
This dramatic weaving was created with wool from the long-horned Navajo-Churro sheep, colored with
traditional plant dyes.
"Replica of a Chip", created by Marilou Schultz, 1994. Wool. Photo taken at the National Gallery of Art, 2024.
For the 555 timer weaving, Schultz experimented with different materials. Silver and gold metallic threads
represent the aluminum and copper in the chip.
The artist explains that "it took a lot more time to incorporate the metallic threads," but it was
worth the effort because "it is spectacular to see the rug with the metallics in the dark with a little light hitting it."
Aniline dyes provided the black and lavender colors.
Although natural logwood dye
produces a beautiful purple, it fades over time, so Schultz used an aniline dye instead.
The lavender colors are dedicated to the weaver's mother, who passed away in February;
purple was her favorite color.
Inside the chip
How does the 555 chip produce a particular time delay?
You add external components—resistors and a capacitor—to select the time.
The capacitor is filled (charged) at a speed controlled by the resistor. When the capacitor get "full",
the 555 chip switches operation and starts emptying (discharging) the capacitor.
It's like filling a sink: if you have a large sink (capacitor) and a trickle of water (large resistor),
the sink fills slowly. But if you have a smal sink (capacitor) and a lot of water (small resistor),
the sink fills quickly.
By using different resistors and capacitors, the 555 timer can provide time intervals from microseconds
to hours.
I've constructed an interactive chip browser that shows how the regions of the rug correspond to specific
electronic components in the physical chip. Click on any part of the rug to learn the function of
the corresponding component in the chip.


Click the die or schematic for details...











For instance, two of the large square transistors turn the chip's output on or off, while the third
large transistor discharges the capacitor when it is full. (To be precise, the capacitor goes between 1/3 full
and 2/3 full to avoid issues near "empty" and "full".)
The chip has circuits called comparators that detect when the capacitor's voltage reaches 1/3 or 2/3,
switching between emptying and filling at those points.
If you want more technical details about the 555 chip, see my previous articles:
an early 555 chip,
a 555 timer similar to the rug,
and a more modern CMOS version of the 555.
Conclusions
The similarities between Navajo weavings and the patterns in integrated circuits have long been recognized.
Marilou Schultz's weavings of integrated circuits make these visual metaphors into concrete works of art.
This connection is not just metaphorical, however; in the 1960s, the semiconductor company Fairchild employed numerous Navajo workers to assemble chips in Shiprock, New Mexico.
I wrote about this complicated history in The Pentium as a Navajo Weaving.


This work is being shown at SITE Santa Fe's Once Within a Time exhibition (running until January 2026).
I haven't seen the exhibition in person, so let me know if you visit it.
For more about Marilou Schultz's art, see The Diné Weaver Who Turns Microchips Into Art, or
A Conversation with Marilou Schultz on YouTube.
Many thanks to Marilou Schultz for discussing her art with me.
Thanks to First American Art Magazine for providing the photo of her 555 rug.
Follow me on Mastodon (@[email protected]),
Bluesky (@righto.com),
or RSS for updates.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How the “Kim” dump exposed North Korea's credential theft playbook]]></title>
            <link>https://dti.domaintools.com/inside-the-kimsuky-leak-how-the-kim-dump-exposed-north-koreas-credential-theft-playbook/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152066</guid>
            <description><![CDATA[A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor's operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.]]></description>
            <content:encoded><![CDATA[
                                
Contents:Part I: Technical AnalysisPart II: Goals AnalysisPart III: Threat Intelligence Report



Executive Summary



A rare and revealing breach attributed to a North Korean-affiliated actor, known only as “Kim” as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor’s operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The “Kim” dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.



Screen shot of the adversary’s desktop VM



This report is broken down into three parts: 




Technical Analysis of the dump materials



Motivation and Goals of the APT actor (group)



A CTI report compartment for analysts




While this leak only gives a partial idea of what the Kimusky/PRC activities have been, the material provides insight into the expansion of activities, nature of the actor(s), and goals they have in their penetration of the South Korean governmental systems that would benefit not only DPRK, but also PRC.



Phrack article



Without a doubt, there will be more coming out from this dump in the future, particularly if the burned assets have not been taken offline and access is still available, or if others have cloned those assets for further analysis. We may revisit this in the future if additional novel information comes to light.



Part I: Technical Analysis



The Leak at a Glance



The leaked dataset attributed to the “Kim” operator offers a uniquely operational perspective into North Korean-aligned cyber operations. Among the contents were terminal history files revealing active malware development efforts using NASM (Netwide Assembler), a choice consistent with low-level shellcode engineering typically reserved for custom loaders and injection tools. These logs were not static forensic artifacts but active command-line histories showing iterative compilation and cleanup processes, suggesting a hands-on attacker directly involved in tool assembly.



File list of dump



In parallel, the operator ran OCR (Optical Character Recognition) commands against sensitive Korean PDF documents related to public key infrastructure (PKI) standards and VPN deployments. These actions likely aimed to extract structured language or configurations for use in spoofing, credential forgery, or internal tool emulation.



Privileged Access Management (PAM) logs also surfaced in the dump, detailing a timeline of password changes and administrative account use. Many were tagged with the Korean string 변경완료 (“change complete”), and the logs included repeated references to elevated accounts such as oracle, svradmin, and app_adm01, indicating sustained access to critical systems.



The phishing infrastructure was extensive. Domain telemetry pointed to a network of malicious sites designed to mimic legitimate Korean government portals. Sites like nid-security[.]com were crafted to fool users into handing over credentials via advanced AiTM (Adversary-in-the-Middle) techniques.



nid-security[.]com phishing domain (anon reg 2024)



Finally, network artifacts within the dump showed targeted reconnaissance of Taiwanese government and academic institutions. Specific IP addresses and .tw domain access, along with attempts to crawl .git repositories, reveal a deliberate focus on high-value administrative and developer targets.



Perhaps most concerning was the inclusion of a Linux rootkit using syscall hooking (khook) and stealth persistence via directories like /usr/lib64/tracker-fs. This highlights a capability for deep system compromise and covert command-and-control operations, far beyond phishing and data theft.



Artifacts recovered from the dump include:




Terminal history files demonstrating malware compilation using NASM



OCR commands parsing Korean PDF documents related to PKI and VPN infrastructure



PAM logs reflecting password changes and credential lifecycle events



Phishing infrastructure mimicking Korean government sites



IP addresses indicating reconnaissance of Taiwanese government and research institutions



Linux rootkit code using syscall hooking and covert channel deployment




Credential Theft Focus



The dump strongly emphasizes credential harvesting as a central operational goal. Key files such as 136백운규001_env.key (The presence of 136백운규001_env.key is a smoking gun indicator of stolen South Korean Government PKI material, as its structure (numeric ID + Korean name + .key) aligns uniquely with SK GPKI issuance practices and provides clear evidence of compromised, identity-tied state cryptographic keys.) This was discovered alongside plaintext passwords, that indicate clear evidence of active compromise of South Korea’s GPKI (Government Public Key Infrastructure). Possession of such certificates would allow for highly effective identity spoofing across government systems.











PAM logs further confirmed this focus, showing a pattern of administrative account rotation and password resets, all timestamped and labeled with success indicators (변경완료: Change Complete). The accounts affected were not low-privilege; instead, usernames like oracle, svradmin, and app_adm01, often used by IT staff and infrastructure services, suggested access to core backend environments.



These findings point to a strategy centered on capturing and maintaining access to privileged credentials and digital certificates, effectively allowing the attacker to act as an insider within trusted systems.




Leaked .key files (e.g., 136백운규001_env.key) with plaintext passwords confirm access to GPKI systems



PAM logs show administrative password rotations tagged with 변경완료 (change complete)



Admin-level accounts such as oracle, svradmin, and app_adm01 repeatedly appear in compromised logs




Phishing Infrastructure



The operator’s phishing infrastructure was both expansive and regionally tailored. Domains such as nid-security[.]com and webcloud-notice[.]com mimicked Korean identity and document delivery services, likely designed to intercept user logins or deploy malicious payloads. More sophisticated spoofing was seen in sites that emulated official government agencies like dcc.mil[.]kr, spo.go[.]kr, and mofa.go[.]kr.



Whoisof domains created by dysoni91@tutamail[.]com



Historical Whois of webcloud-notice[.]com



Burner email usage added another layer of operational tradecraft. The address jeder97271[@]wuzak[.]com is likely linked to phishing kits that operated through TLS proxies, capturing credentials in real time as victims interacted with spoofed login forms.



These tactics align with previously known Kimsuky behaviors but also demonstrate an evolution in technical implementation, particularly the use of AiTM interception rather than relying solely on credential-harvesting documents.



Domain connections map




Domains include: nid-security[.]com, html-load[.]com, webcloud-notice[.]com, koala-app[.]com, and wuzak[.]com



Mimicked portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Burner email evidence: jeder97271[@]wuzak[.]com



Phishing kits leveraged TLS proxies for AiTM credential capture




Malware Development Activity



Kim’s malware development environment showcased a highly manual, tailored approach. Shellcode was compiled using NASM, specifically with flags like -f win32, revealing a focus on targeting Windows environments. Commands such as make and rm were used to automate and sanitize builds, while hashed API call resolution (VirtualAlloc, HttpSendRequestA, etc.) was implemented to evade antivirus heuristics.



The dump also revealed reliance on GitHub repositories known for offensive tooling. TitanLdr, minbeacon, Blacklotus, and CobaltStrike-Auto-Keystore were all cloned or referenced in command logs. This hybrid use of public frameworks for private malware assembly is consistent with modern APT workflows.



A notable technical indicator was the use of the proxyres library to extract Windows proxy settings, particularly via functions like proxy_config_win_get_auto_config_url. This suggests an interest in hijacking or bypassing network-level security controls within enterprise environments.




Manual shellcode compilation via nasm -f win32 source/asm/x86/start.asm



Use of make, rm, and hash obfuscation of Win32 API calls (e.g., VirtualAlloc, HttpSendRequestA)



GitHub tools in use: TitanLdr, minbeacon, Blacklotus, CobaltStrike-Auto-Keystore



Proxy configuration probing through proxyres library (proxy_config_win_get_auto_config_url)




Rootkit Toolkit and Implant Structure



The Kim dump offers deep insight into a stealthy and modular Linux rootkit attributed to the operator’s post-compromise persistence tactics. The core implant, identified as vmmisc.ko (alternatively VMmisc.ko in some shells), was designed for kernel-mode deployment across multiple x86_64 Linux distributions and utilizes classic syscall hooking and covert channeling to maintain long-term undetected access.







Google Translation of Koh doc: Rootkit Endpoint Reuse Authentication Tool



“This tool uses kernel-level rootkit hiding technology, providing a high degree of stealth and penetration connection capability. It can hide while running on common Linux systems, and at the kernel layer supports connection forwarding, allowing reuse of external ports to connect to controlled hosts. Its communication behavior is hidden within normal traffic.



The tool uses binary merging technology: at compile time, the application layer program is encrypted and fused into a .ko driver file. When installed, only the .ko file exists. When the .ko driver starts, it will automatically decompress and release the hidden application-layer program.



Tools like chkrootkit, rkhunter, and management utilities (such as ps, netstat, etc.) are bypassed through technical evasion and hiding, making them unable to detect hidden networks, ports, processes, or file information.



To ensure software stability, all functions have also passed stress testing.



Supported systems: Linux Kernel 2.6.x / 3.x / 4.x, both x32 and x64 systems”.



Implant Features and Behavior



This rootkit exhibits several advanced features:




Syscall Hooking: Hooks critical kernel functions (e.g., getdents, read, write) to hide files, directories, and processes by name or PID.



SOCKS5 Proxy: Integrated remote networking capability using dynamic port forwarding and chained routing.



PTY Backdoor Shell: Spawns pseudoterminals that operate as interactive reverse shells with password protection.



Encrypted Sessions: Session commands must match a pre-set passphrase (e.g., testtest) to activate rootkit control mode.




Once installed (typically using insmod vmmisc.ko), the rootkit listens silently and allows manipulation via an associated client binary found in the dump. The client supports an extensive set of interactive commands, including:



+p              # list hidden processes



+f              # list hidden files



callrk          # load client ↔ kernel handshake



exitrk          # gracefully unload implant



shell           # spawn reverse shell



socks5          # initiate proxy channel



upload / download # file transfer interface



These capabilities align closely with known DPRK malware behaviors, particularly from the Kimsuky and Lazarus groups, who have historically leveraged rootkits for lateral movement, stealth, persistence, and exfiltration staging.



Observed Deployment



Terminal history (.bash_history) shows the implant was staged and tested from the following paths:



.cache/vmware/drag_and_drop/VMmisc.ko

/usr/lib64/tracker-fs/vmmisc.ko

Execution logs show the use of commands such as:

insmod /usr/lib64/tracker-fs/vmmisc.ko

./client 192.168.0[.]39 testtest



These paths were not random—they mimic legitimate system service locations to avoid detection by file integrity monitoring (FIM) tools.



Deployment map



This structure highlights the modular, command-activated nature of the implant and its ability to serve multiple post-exploitation roles while maintaining stealth through kernel-layer masking.



Strategic Implications



The presence of such an advanced toolkit in the “Kim” dump strongly suggests the actor had persistent access to Linux server environments, likely via credential compromise. The use of kernel-mode implants also indicates long-term intent and trust-based privilege escalation. The implant’s pathing, language patterns, and tactics (e.g., use of /tracker-fs/, use of test passwords) match TTPs previously observed in operations attributed to Kimsuky, enhancing confidence in North Korean origin.



OCR-Based Recon



A defining component of Kim’s tradecraft was the use of OCR to analyze Korean-language security documentation. The attacker issued commands such as ocrmypdf -l kor+eng “file.pdf” to parse documents like 별지2)행정전자서명_기술요건_141125.pdf (“Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf”) and SecuwaySSL U_카달로그.pdf (“SecuwaySSL U_Catalog.pdf”). These files contain technical language around digital signatures, SSL implementations, and identity verification standards used in South Korea’s PKI infrastructure.



This OCR-based collection approach indicates more than passive intelligence gathering – it reflects a deliberate effort to model and potentially clone government-grade authentication systems. The use of bilingual OCR (Korean + English) further confirms the operator’s intention to extract usable configuration data across documentation types.



OCR run on Korean PDFs




OCR commands used to extract Korean PKI policy language from PDFs such as (별지2)행정전자서명_기술요건_141125.pdf and SecuwaySSL U_카달로그.pdf

별지2)행정전자서명_기술요건_141125.pdf → (Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf



SecuwaySSL U_카달로그.pdf → SecuwaySSL U_Catalog.pdf





Command examples: ocrmypdf -l kor+eng “file.pdf”




SSH and Log-Based Evidence



The forensic evidence contained within the logs, specifically SSH authentication records and PAM outputs, provides clear technical confirmation of the operator’s tactics and target focus.



Several IP addresses stood out as sources of brute-force login attempts. These include 23.95.213[.]210 (a known VPS provider used in past credential-stuffing campaigns), 218.92.0[.]210 (allocated to a Chinese ISP), and 122.114.233[.]77 (Henan Mobile, China). These IPs were recorded during multiple failed login events, strongly suggesting automated password attacks against exposed SSH services. Their geographic distribution and known history in malicious infrastructure usage point to an external staging environment, possibly used for pivoting into Korean and Taiwanese systems.



Beyond brute force, the logs also contain evidence of authentication infrastructure reconnaissance. Multiple PAM and OCSP (Online Certificate Status Protocol) errors referenced South Korea’s national PKI authority, including domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. These errors appear during scripted or automated access attempts, indicating a potential strategy of credential replay or certificate misuse against GPKI endpoints, an approach that aligns with Kim’s broader PKI-targeting operations.



Perhaps the most revealing detail was the presence of successful superuser logins labeled with the Korean term 최고 관리자 (“Super Administrator”). This suggests the actor was not just harvesting credentials but successfully leveraging them for privileged access, possibly through cracked accounts, reused credentials, or insider-sourced passwords. The presence of such accounts in conjunction with password rotation entries marked as 변경완료 (“change complete”) further implies active control over PAM-protected systems during the operational window captured in the dump.



Together, these logs demonstrate a methodical campaign combining external brute-force access, PKI service probing, and administrative credential takeover, a sequence tailored for persistent infiltration and lateral movement within sensitive government and enterprise networks.



Brute force mapping




Brute-force IPs: 23.95.213[.]210, 218.92.0[.]210, 122.114.233[.]77




IP AddressOriginRole / Threat Context218.92.0[.]210China Telecom (Jiangsu)Part of Chinanet backbone, likely proxy or scanning node23.95.213[.]210Colocrossing (US)Frequently used in brute-force and anonymized hosting for malware ops122.114.233[.]77Presumed PRC local ISPPossibly mobile/ISP-based proxy used to obfuscate lateral movement




PAM/OCSP errors targeting gva.gpki.go[.]kr, ivs.gpki.go[.]kr



Superuser login events under 최고 관리자 (Super Administrator)




Part II: Goals Analysis



Targeting South Korea: Identity, Infrastructure, and Credential Theft



The “Kim” operator’s campaign against South Korea was deliberate and strategic, aiming to infiltrate the nation’s digital trust infrastructure at multiple levels. A central focus was the Government Public Key Infrastructure (GPKI), where the attacker exfiltrated certificate files, including .key and .crt formats, some with plaintext passwords, and attempted repeated authentication against domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. OCR tools were used to parse Korean technical documents detailing PKI and VPN architectures, demonstrating a sophisticated effort to understand and potentially subvert national identity frameworks. These efforts were not limited to reconnaissance; administrative password changes were logged, and phishing kits targeted military and diplomatic webmail, including clones of mofa.go[.]kr and credential harvesting through adversary-in-the-middle (AiTM) proxy setups.



Attempts at user account authentication



Servlet requests for KR domains



Beyond authentication systems, Kim targeted privileged accounts (oracle, unwadm, svradmin) and rotated credentials to maintain persistent administrative access, as evidenced by PAM and SSH logs showing elevated user activity under the title 최고 관리자 (“Super Administrator”). The actor also showed interest in bypassing VPN controls, parsing SecuwaySSL configurations for exploitation potential, and deployed custom Linux rootkits using syscall hooking to establish covert persistence on compromised machines. Taken together, the dump reveals a threat actor deeply invested in credential dominance, policy reconnaissance, and system-level infiltration, placing South Korea’s public sector identity systems, administrative infrastructure, and secure communications at the core of its long-term espionage objectives.



Taiwan Reconnaissance



Among the most notable aspects of the “Kim” leak is the operator’s deliberate focus on Taiwanese infrastructure. The attacker accessed a number of domains with clear affiliations to the island’s public and private sectors, including tw.systexcloud[.]com (linked to enterprise cloud solutions), mlogin.mdfapps[.]com (a mobile authentication or enterprise login portal), and the .git/ directory of caa.org[.]tw, which belongs to the Chinese Institute of Aeronautics, a government-adjacent research entity.



This last domain is especially telling. Accessing .git/ paths directly implies an attempt to enumerate internal source code repositories, a tactic often used to discover hardcoded secrets, API keys, deployment scripts, or developer credentials inadvertently exposed via misconfigured web servers. This behavior points to  more technical depth than simple phishing; it indicates supply chain reconnaissance and long-term infiltration planning.



Taiwanese target map



The associated IP addresses further reinforce this conclusion. All three, 163.29.3[.]119, 118.163.30[.]45, and 59.125.159[.]81, are registered to academic, government, or research backbone providers in Taiwan. These are not random scans; they reflect targeted probing of strategic digital assets.



Summary of Whois & Ownership Insights




118.163.30[.]45

Appears as part of the IP range used for the domain dtc-tpe.com[.]tw, linked to Taiwan’s HINET provider (118.163.30[.]46 )Site Indices page of HINET provider.





163.29.3[.]119

Falls within the 163.29.3[.]0/24 subnet identified with Taiwanese government or institutional use, notably in Taipei. This corresponds to B‑class subnets assigned to public/government entities IP地址 (繁體中文).





59.125.159[.]81

Belongs to the broader 59.125.159[.]0–59.125.159[.]254 block, commonly used by Taiwanese ISP operators such as Chunghwa Telecom in Taipei






Taken together, this Taiwan-focused activity reveals an expanded operational mandate. Whether the attacker is purely DPRK-aligned or operating within a DPRK–PRC fusion cell, the intent is clear: compromise administrative and developer infrastructure in Taiwan, likely in preparation for broader credential theft, espionage, or disruption campaigns.




Targeted domains: tw.systexcloud[.]com, caa.org[.]tw/.git/, mlogin.mdfapps[.]com



IPs linked to Taiwanese academic/government assets: 163.29.3[.]119, 118.163.30[.]45, 59.125.159[.]81



Git crawling suggests interest in developer secrets or exposed tokens




Hybrid Attribution Model



The “Kim” operator embodies the growing complexity of modern nation-state attribution, where cyber activities often blur traditional boundaries and merge capabilities across geopolitical spheres. This case reveals strong indicators of both North Korean origin and Chinese operational entanglement, presenting a textbook example of a hybrid APT model.







On one hand, the technical and linguistic evidence strongly supports a DPRK-native operator. Terminal environments, OCR parsing routines, and system artifacts consistently leverage Korean language and character sets. The operator’s activities reflect a deep understanding of Korean PKI systems, with targeted extraction of GPKI .key files and automation to parse sensitive Korean government PDF documentation. These are hallmarks of Kimsuky/APT43 operations, known for credential-focused espionage against South Korean institutions and diplomatic targets. The intent to infiltrate identity infrastructure is consistent with North Korea’s historical targeting priorities. Notably, the system time zone on Kim’s host machine was set to UTC+9 (Pyongyang Standard Time), reinforcing the theory that the actor maintains direct ties to the DPRK’s internal environment, even if operating remotely.



However, this actor’s digital footprint extends well into Chinese infrastructure. Browser and download logs reveal frequent interaction with platforms like gitee[.]com, baidu[.]com, and zhihu[.]com, highly popular within the PRC but unusual for DPRK operators who typically minimize exposure to foreign services. Moreover, session logs include simplified Chinese content and PRC browsing behaviors, suggesting that the actor may be physically operating within China or through Chinese-language systems. This aligns with longstanding intelligence on North Korean cyber operators stationed in Chinese border cities such as Shenyang and Dandong, where DPRK nationals often conduct cyber operations with tacit approval or logistical consent from Chinese authorities. These locations provide higher-speed internet, relaxed oversight, and convenient geopolitical proximity.



Browser History viewing Taiwanese and Chinese sites



The targeting of Taiwanese infrastructure further complicates attribution. Kimsuky has not historically prioritized Taiwan, yet in this case, the actor demonstrated direct reconnaissance of Taiwanese government and developer networks. While this overlaps with Chinese APT priorities, recent evidence from the “Kim” dump, including analysis of phishing kits and credential theft workflows, suggests this activity was likely performed by a DPRK actor exploring broader regional interests, possibly in alignment with Chinese strategic goals. Researchers have noted that Kimsuky operators have recently asked questions in phishing lures related to potential Chinese-Taiwanese conflicts, implying interest beyond the Korean peninsula.



Some tooling overlaps with PRC-linked APTs, particularly GitHub-based stagers and proxy-resolving modules, but these are not uncommon in the open-source malware ecosystem and may reflect opportunistic reuse rather than deliberate mimicry.



IMINT Analysis: Visual Tradecraft and Cultural Camouflage



A review of image artifacts linked to the “Kim” actor reveals a deliberate and calculated use of Chinese social and technological visual content as part of their operational persona. These images, extracted from browser history and uploads attributed to the actor, demonstrate both strategic alignment with DPRK priorities and active cultural camouflage within the PRC digital ecosystem.



Uploads of images by Kim found in browser history



Images downloaded from aixfan[.]com



The visual set includes promotional graphics for Honor smartphones, SoC chipset evolution charts, Weibo posts featuring vehicle registration certificates, meme-based sarcasm, and lifestyle imagery typical of Chinese internet users. Notably, the content is exclusively rendered in simplified Chinese, reinforcing prior assessments that the operator either resides within mainland China or maintains a working digital identity embedded in Chinese platforms. Devices and services referenced, such as Xiaomi phones, Zhihu, Weibo, and Baidu, suggest intimate familiarity with PRC user environments.



Operationally, this behavior achieves two goals. First, it enables the actor to blend in seamlessly with native PRC user activity, which complicates attribution and helps bypass platform moderation or behavioral anomaly detection. Second, the content itself may serve as bait or credibility scaffolding (e.g. A framework to give the illusion of trust to allow for easier compromise ) in phishing and social engineering campaigns, especially those targeting developers or technical users on Chinese-language platforms.



Some images, such as the detailed chipset timelines and VPN or device certification posts, suggest a continued interest in supply chain reconnaissance and endpoint profiling—both tradecraft hallmarks of Kimsuky and similar APT units. Simultaneously, meme humor, sarcastic overlays, and visual metaphors (e.g., the “Kaiju’s tail is showing” idiom) indicate the actor’s fluency in PRC netizen culture and possible mockery of operational security breaches—whether their own or others’.



Taken together, this IMINT corpus supports the broader attribution model: a DPRK-origin operator embedded, physically or virtually, within the PRC, leveraging local infrastructure and social platforms to facilitate long-term campaigns against South Korea, Taiwan, and other regional targets while maintaining cultural and technical deniability.



Attribution Scenarios:




Option A: DPRK Operator Embedded in PRC

Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.



Use of PRC infrastructure (e.g., Baidu, Gitee) and simplified Chinese content implies the operator is physically located in China or benefits from access to Chinese internet infrastructure.





Option B: PRC Operator Emulating DPRK

Taiwan-focused reconnaissance aligns with PRC cyber priorities.



Use of open-source tooling and phishing methods shared with PRC APTs could indicate tactical emulation.






The preponderance of evidence supports the hypothesis that “Kim” is a North Korean cyber operator embedded in China or collaborating with PRC infrastructure providers. This operational model allows the DPRK to amplify its reach, mask attribution, and adopt regional targeting strategies beyond South Korea, particularly toward Taiwan. As this hybrid model matures, it reflects the strategic adaptation of DPRK-aligned threat actors who exploit the permissive digital environment of Chinese networks to evade detection and expand their operational playbook.



Targeting Profiles



The “Kim” leak provides one of the clearest windows to date into the role-specific targeting preferences of the operator, revealing a deliberate focus on system administrators, credential issuers, and backend developers, particularly in South Korea and Taiwan.



In South Korea, the operator’s interest centers around PKI administrators and infrastructure engineers. The recovered OCR commands were used to extract technical details from PDF documents outlining Korea’s digital signature protocols, such as identity verification, certificate validation, and encrypted communications, components that form the backbone of Korea’s secure authentication systems. The goal appears to be not only credential theft but full understanding and potential replication of government-trusted PKI procedures. This level of targeting suggests a strategic intent to penetrate deeply trusted systems, potentially for use in later spoofing or identity masquerading operations.



PKI attack targets



In Taiwan, the operator shifted focus to developer infrastructure and cloud access portals. Specific domains accessed, like caa.org[.]tw/.git/, indicate attempts to enumerate internal repositories, most likely to discover hardcoded secrets, authentication tokens, or deployment keys. This is a classic supply chain targeting method, aiming to access downstream systems via compromised developer credentials or misconfigured services.



Additional activity pointed to interaction with cloud service login panels such as tw.systexcloud[.]com and mlogin.mdfapps[.]com. These suggest an attempt to breach centralized authentication systems or identity providers, granting the actor broader access into enterprise or government networks with a single credential set.



Taken together, these targeting profiles reflect a clear emphasis on identity providers, backend engineers, and those with access to system-level secrets. This reinforces the broader theme of the dump: persistent, credential-first intrusion strategies, augmented by reconnaissance of authentication standards, key management policies, and endpoint development infrastructure.



South Korean:




PKI admins, infrastructure engineers



OCR focus on Korean identity standards




Taiwanese:




Developer endpoints and internal .git/ repos



Access to cloud panels and login gateways




Final Assessment



The “Kim” leak represents one of the most comprehensive and technically intimate disclosures ever associated with Kimsuky (APT43) or its adjacent operators. It not only reaffirms known tactics, credential theft, phishing, and PKI compromise, but exposes the inner workings of the operator’s environment, tradecraft, and operational intent in ways rarely observed outside of active forensic investigations.



At the core of the leak is a technically competent actor, well-versed in low-level shellcode development, Linux-based persistence mechanisms, and certificate infrastructure abuse. Their use of NASM, API hashing, and rootkit deployment points to custom malware authorship. Furthermore, the presence of parsed government-issued Korean PDFs, combined with OCR automation, shows not just opportunistic data collection but a concerted effort to model, mimic, or break state-level identity systems, particularly South Korea’s GPKI.



The operator’s cultural and linguistic fluency in Korean, and their targeting of administrative and privileged systems across South Korean institutions, support a high-confidence attribution to a DPRK-native threat actor. However, the extensive use of Chinese platforms like gitee[.]com, Baidu, and Zhihu, and Chinese infrastructure for both malware hosting and browsing activity reveals a geographical pivot or collaboration: a hybrid APT footprint rooted in DPRK tradecraft but operating from or with Chinese support.



Most notably, this leak uncovers a geographical expansion of operational interest; the actor is no longer solely focused on the Korean peninsula. The targeting of Taiwanese developer portals, government research IPs, and .git/ repositories shows a broadened agenda that likely maps to both espionage and supply chain infiltration priorities. This places Taiwan, like South Korea, at the forefront of North Korean cyber interest, whether for intelligence gathering, credential hijacking, or as staging points for more complex campaigns.



The threat uncovered here is not merely malware or phishing; it is an infrastructure-centric, credential-first APT campaign that blends highly manual operations (e.g., hand-compiled shellcode, direct OCR of sensitive PDFs) with modern deception tactics such as AiTM phishing and TLS proxy abuse.



Organizations in Taiwan and South Korea, particularly those managing identity, certificate, and cloud access infrastructure, should consider themselves under persistent, credential-focused surveillance. Defensive strategies must prioritize detection of behavioral anomalies (e.g., use of OCR tools, GPKI access attempts), outbound communications with spoofed Korean domains, and the appearance of low-level toolchains like NASM or proxyres-based scanning utilities within developer or admin environments.



In short: the “Kim” actor embodies the evolution of nation-state cyber threats—a fusion of old-school persistence, credential abuse, and modern multi-jurisdictional staging. The threat is long-term, embedded, and adaptive.



Part III: Threat Intelligence Report



TLP WHITE:



Targeting Summary



The analysis of the “Kim” operator dump reveals a highly focused credential-theft and infrastructure-access campaign targeting high-value assets in both South Korea and Taiwan. Victims were selected based on their proximity to trusted authentication systems, administrative control panels, and development environments.



CategoryDetailsRegionsSouth Korea, TaiwanTargetsGovernment, Telecom, Enterprise ITAccountssvradmin, oracle, app_adm01, unwadm, shkim88, jaejung91Domainstw.systexcloud[.]com, nid-security[.]com, spo.go[.]kr, caa.org[.]tw/.git/



Indicators of Compromise (IOCs)



Domains




Phishing: nid-security[.]com, html-load[.]com, wuzak[.]com, koala-app[.]com, webcloud-notice[.]com



Spoofed portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Pastebin raw links: Used for payload staging and malware delivery




IP Addresses




External Targets (Taiwan):

163.29.3[.]119     National Center for High-performance Computing



118.163.30[.]45   Taiwanese government subnet



59.125.159[.]81   Chunghwa Telecom





Brute Forcing / Infrastructure Origins:

23.95.213[.]210   VPS provider with malicious history



218.92.0[.]210     China Unicom



122.114.233[.]77  Henan Mobile, PRC






Internal Host IPs (Operator Environment)




192.168.130[.]117



192.168.150[.]117



192.168.0[.]39




Operator Environment: Internal Host IP Narrative



The presence of internal IP addresses such as 192.168.130[.]117, 192.168.150[.]117, and 192.168.0[.]39 within the dump offers valuable insight into the attacker’s local infrastructure, an often-overlooked element in threat intelligence analysis. These addresses fall within private, non-routable RFC1918 address space, commonly assigned by consumer off-the-shelf (COTS) routers and small office/home office (SOHO) network gear.



The use of the 192.168.0[.]0/16 subnet, particularly 192.168.0.x and 192.168.150.x, strongly suggests that the actor was operating from a residential or low-profile environment, not a formal nation-state facility or hardened infrastructure. This supports existing assessments that North Korean operators, particularly those affiliated with Kimsuky, often work remotely from locations in third countries such as China or Southeast Asia, where they can maintain inconspicuous, low-cost setups while accessing global infrastructure.



Moreover, the distinction between multiple internal subnets (130.x, 150.x, and 0.x) may indicate segmentation of test environments or multiple virtual machines running within a single NATed network. This aligns with the forensic evidence of iterative development and testing workflows seen in the .bash_history files, where malware stagers, rootkits, and API obfuscation utilities were compiled, cleaned, and rerun repeatedly.



Together, these IPs reveal an operator likely working from a clandestine, residential base of operations, with modest hardware and commercial-grade routers. This operational setup is consistent with known DPRK remote IT workers and cyber operators who avoid attribution by blending into civilian infrastructure. It also suggests the attacker may be physically located outside of North Korea, possibly embedded in a friendly or complicit environment, strengthening the case for China-based activity by DPRK nationals.



MITRE ATT&CK Mapping



PhaseTechnique(s)Initial AccessT1566.002 ,  Adversary-in-the-Middle (AiTM) PhishingExecutionT1059.005 ,  Native API ShellcodeT1059.003 ,  Bash/Shell ScriptsCredential AccessT1555 ,  Credential Store DumpingT1557.003 ,  Session HijackingPersistenceT1176 ,  Rootkit (via khook syscall manipulation)Defense EvasionT1562.001 ,  Disable Security ToolsT1552 ,  Unsecured Credential FilesDiscoveryT1592 ,  Technical Information DiscoveryT1590 ,  Network InformationExfiltrationT1041 ,  Exfiltration over C2 ChannelT1567.002 ,  Exfil via Cloud Services



Tooling and Capabilities



The actor’s toolkit spans multiple disciplines, blending malware development, system reconnaissance, phishing, and proxy evasion:




NASM-based shellcode loaders: Compiled manually for Windows execution.



Win32 API hashing: Obfuscated imports via hashstring.py to evade detection.



GitHub/Gitee abuse: Tooling hosted or cloned from public developer platforms.



OCR exploitation: Used ocrmypdf to parse Korean PDF specs related to digital certificates and VPN appliances.



Rootkit deployment: Hidden persistence paths including /usr/lib64/tracker-fs and /proc/acpi/pcicard.



Proxy config extraction: Investigated PAC URLs using proxyres-based recon.




Attribution Confidence Assessment



Attribution CandidateConfidence LevelDPRK-aligned (Kimsuky)High, Native Korean targeting, GPKI focus, OCR behaviorChina-blended infrastructureModerate, PRC hosting, Gitee usage, Taiwan focusSolely PRC ActorLow-to-Moderate, Tooling overlap but weak linguistic match



Assessment: The actor appears to be a DPRK-based APT operator working from within or in partnership with Chinese infrastructure, representing a hybrid attribution model.



Defensive Recommendations



AreaRecommendationPKI SecurityMonitor usage of .key, .sig, .crt artifacts; enforce HSM or 2FA for key usePhishing DefenseBlock domains identified in IoCs; validate TLS fingerprints and referrer headersEndpoint HardeningDetect use of nasm, make, and OCR tools; monitor /usr/lib*/tracker-* pathsNetwork TelemetryAlert on .git/ directory access from external IPs; monitor outbound to Pastebin/GitHubTaiwan FocusEstablish watchlists for .tw domains targeted by PRC-originating IPsAdmin AccountsReview usage logs for svradmin, oracle, app_adm01, and ensure rotation policies



APPENDIX A



Overlap or Confusion with Chinese Threat Actors



There is notable evidence of operational blur between Kimsuky and Chinese APTs in the context of Taiwan. The 2025 “Kim” data breach revealed an attacker targeting Taiwan whose tools and phishing kits matched Kimsuky’s, yet whose personal indicators (language, browsing habits) suggested a Chinese national. Researchers concluded this actor was likely a Chinese hacker either mimicking Kimsuky tactics or collaborating with them.. In fact, the leaked files on DDoS Secrets hint that Kimsuky has “openly cooperated with other Chinese APTs and shared their tools and techniques”. This overlap can cause attribution confusion – a Taiwan-focused operation might initially be blamed on China but could involve Kimsuky elements, or vice versa. So far, consensus is that North Korean and Chinese cyber operations remain separate, but cases like “Kim” show how a DPRK-aligned actor can operate against Taiwan using TTPs common to Chinese groups, muddying the waters of attribution.



File List from dump:















Master Evidence Inventory:



File NameLanguageContent SummaryCategoryRelevance.bash_historyMixed (EN/KR)Operator shell history commandsSystem/LogShows rootkit compilation, file ops, network testsuser-bash_historyMixed (EN/KR)User-level shell commandsSystem/LogDevelopment and test activityroot-bash_historyMixed (EN/KR)Root-level shell commandsSystem/LogPrivilege-level activity, implant deploymentauth.log.2EN/KRAuthentication logs (PAM/SSH)System/LogCredential changes marked 변경완료, brute force IPs20190315.logENSystem log fileSystem/LogAuth and system access eventschrome-timeline.txtENBrowser activity timelineBrowserVisited domains extractionchromehistory.txtENBrowser history exportBrowserURLs visitedhistory.sqliteENEmpty DB fileBrowserNo useful dataMedia HistoryENEmpty SQLite DBBrowserNo playback activityHistoryENEmpty Brave/Chromium DBBrowserNo visited URLsWeb DataENAutofill/search DBBrowserSearch engines used (Google, DuckDuckGo, Qwant, Startpage, Ecosia)Visited LinksBinaryLevelDB/binary structureBrowserCould not extract URLsCookiesENSQLite DB with cookiesBrowserGoogle cookies foundrequest_log.txt.20250220ENCaptured phishing sessionPhishingSpoofed spo.go.kr, base64 credential logging技术说明书 – 22.docxZHChinese rootkit stealth manualRootkitKernel hiding, binary embedding1.ko 图文编译 .docZHChinese compilation guideRootkitRootkit build process1. build ko .txtZHBuild notesRootkitImplant compilation instructions0. 使用.txtZHUsage notesRootkitImplant usage and commandsre 正向工具修改建议 1.0.txtZHModification notesRootkitReverse tool modification suggestions1111.txtZHRootkit/tool snippetRootkitPart of implant notesclientBinaryRootkit client binaryRootkitController for implant communicationSSA_AO_AD_WT_002_웹보안 프로토콜설계서_Ver1.0_.docKRGPKI protocol design docPKIKorean web PKI standards행자부 웹보안API 인수인계.docKRGPKI API deployment manualPKIDeployment and cert API internalsHIRA-IR-T02_의약품처방조제_ComLibrary_통신전문.docKRMedical ComLibrary XML specHealthcarePrescription system communication(별지2)행정전자서명_기술요건_141125.pdfKRPKI requirements PDFPKIOCR targetSecuwaySSL U_카달로그.pdfKRVPN catalogPKI/VPNOCR targetphrack-apt-down-the-north-korea-files.pdfENPhrack articleReferenceBackground on Kimsuky dumpMuddled Libra Threat Assessment.pdfENThreat intel reportReferenceComparative threat actor studyLeaked North Korean Linux Stealth Rootkit Analysis.pdfENRootkit analysisReferenceDetailed implant studyInside the Kimsuky Leak.docx (various)ENThreat report draftsReportWorking versionsaccount (2).txtENDB export (DBsafer, TrustedOrange)InfraAccounts and DB changesresult.txtKRCert-related parsed dataInfraIncluded GPKI .key/.sigenglish_wikipedia.txtENWikipedia dumpReferenceUnrelated baselinebookmarks-2021-01-04.jsonlz4ENFirefox bookmarks (compressed)BrowserNeeds decompressionScreenshot translationsZHChinese text (rootkit marketing blurb)RootkitKernel hiding tool description




                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What to do with an old iPad]]></title>
            <link>http://odb.ar/blog/2025/09/05/hosting-my-blog-on-an-iPad-2.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45140973</guid>
            <description><![CDATA[I recently inherited my parents’ old iPad 2. It had iOS 9 on it and was barely usable, in part due to how slow it was but mostly because of old SSL certificates and apps stopping support. But I wanted to give it some life because I hate to see a working machine go to waste. So I asked around and got a great answer: The iPad 2 can be downgraded to iOS 6.1.3 or 8.4.1. Both offer better performance than iOS 9 and have untethered jailbreaks. Once downgraded and jailbroken you can sideload hours and hours worth of games. A lot from that era are IAP-free.]]></description>
            <content:encoded><![CDATA[
        

  

  
    I recently inherited my parents’ old iPad 2. It had iOS 9 on it and was barely usable, in part due to how slow it was but mostly because of old SSL certificates and apps stopping support. But I wanted to give it some life because I hate to see a working machine go to waste. So I asked around and got a great answer:


  The iPad 2 can be downgraded to iOS 6.1.3 or 8.4.1. Both offer better performance than iOS 9 and have untethered jailbreaks. Once downgraded and jailbroken you can sideload hours and hours worth of games. A lot from that era are IAP-free.



So that’s where my journey began.

Downgrading

Jailbreaking the iPad 2 is fairly easy thanks to iOS CFW Guide(I didn’t even realize I was following the guide for iPad 3, but it worked perfectly nonetheless). Once that is done, you can downgrade using Legacy-iOS-kit which is also pretty straightforward. And that’s it: you have your iPad jailbroken and on the software it was meant to run. iOS 6.1.3 is not as useful as some of the newer versions, but it’s pretty snappy and very aesthetically pleasing.



But what can you do with it?

A lot. You can download iFile and access files on device, something that was not possible on iOS 6. So that basically means you can load and watch any kind of media on the device. You can torrent files if you want with iTransmission (remember when everything started with a lowercase i?). Load up some epubs and pdfs and read them on iBooks. Get f.lux on device to get warm light on screen. Download some old iOS games.

But that’s pretty boring. I wanted to do more with it and was unsatisfied with my big-ass reader. One thing that I didn’t mention is that when you jailbreak, you get root access to the device. That means you can ssh into it (which you do for the downgrade) and use the unix terminal to do whatever1 you want. So I thought, what if I use this as a tool to learn a bit more about hosting/networking?

Hosting on iPad

My general idea when I began was:


  Get a server utility
  Load a static site into the iPad
  Tunnel out
  ???
  Profit


1. Server

Since the hardware is old, I needed a light web server that could run on very little resources. Enter lighttpd (pronounced lighty I’m told by Gemini). And since there was no package manager available on the iPad’s terminal, I had to load it from Cydia (like an appstore for tweaks that you get once you jailbreak). Easy. Done.

2. Static Site

I had no idea what I was going to host, but it was irrelevant anyway since the site itself was not the purpose of the project. So I got Jekyll installed on my pc, ran the jekyll build command, and loaded the HTML + CSS files on the iPad. Another step done. At this point I had the server up and running locally, serving the blog on the local network. Pretty good already,

3. Tunnel out

I knew tunneling was an option for self hosting, but I didn’t know what it really was. Tunneling, as I understand it, is setting an SSH connection to a server which processes requests, and forwards them to your local machine, in this case the iPad. It’s supposed to be safer than port forwarding since you’re not exposing your home’s IP this way. The most common provider is cloudflare, but you need to install cloudflared on the machine you’re using to host, so that was a no-go (a Cydia search for cloudflare yielded exactly 0 results).

Enter localhost.run, or the first wall I hit. LHR allows you to ssh into it without needing any special software on your machine, connecting a tunnel to your web app running locally. It seems very easy at first, and I guess if you’re doing it on a regular computer it is, but it frustrated the hell out of me. First, the encryption algorithms on iOS 6.1.3 are very old, and that proves a challenge when you’re trying to SSH into anything. I figured this out by adding a flag to the ssh command to accept rsa encryption. It seemed to work, but wouldn’t recognize my user, so I went and created one. Upon account creation I realized that I needed to pay $9 monthly to set a custom domain, otherwise LHR assigns a random domain which changes periodically. Paying for it kinda defeated the purpose, but I decided to try the free version anyway. It didn’t work. On to the next attempt.

So if pre-built solutions don’t work what do you do? You build your own, of course! The idea was to get a free Virtual Private Server from Google Cloud Platform, get nginx to handle the requests, set up a custom tunnel between VPS and iPad, and serve requests from lighttpd. Sounds very simple, however, I ran into a problem almost instantly: there was no way to autossh (same as ssh, but reconnects if the connection drops) from the iPad into the VPS! Since iOS 6 there have been many advances in cryptography it seems, and due to the ancient OpenSSL version, the VPS wouldn’t allow the connection. After much debugging, Gemini summarized the issue perfectly:



  The server receives the signature from your iPad, but it rejects it. Your iPad’s ancient crypto library (OpenSSL 0.9.8zg from 2015) is creating a signature that the modern server considers invalid or insecure.



And so, my workaround had failed. At this point I realize that what I’m doing is kinda pointless and that I could just host the blog on the VPS and call it a day. But I had sunk much time into this and wasn’t about to give up. So port forwarding time! If I can’t connect from the iPad into the VPS, maybe the VPS can connect into the iPad. After all, my computer was perfectly capable of ssh-ing into the iPad. So after much assurance from both Gemini and my tech-wizard uncle that it was okay to port forward, I decided to give it a go. I’ll omit many details here just in case I’m exposing myself, but basically, I opened up a port to my router, got a dynamic DNS from FreeDNS, and once that was done, I could get the VPS to connect to my iPad remotely. It felt like magic, but again came the problems: autossh was not working, only regular ssh using the same flag to accept rsa encryption that I had to use on my computer. Key-based login was failing due to a bug in the iPad’s SSH server. Password login, however, worked. In came sshpass, which enables autossh with password-based authentication. Turns out, this didn’t work either, because the autossh command needed a -t (force terminal) flag for the password prompt. With that cleared up, I could use nohup to run the sshpass command and keep it running in the background. And that’s it! My blog was up and running.

So, if you’re reading this post right now, it means my server is working, and that this site is being served by an iPad 2 from 2012, running iOS 6.1.3 and Insomnia to keep it connected to Wi-Fi. And you may ask, was it worth it? Hell yeah. It was frustrating and had many more hurdles than I anticipated, but it’s really satisfying to know that I managed to achieve my vision. And I learned a lot in the process, as this was my first time using a VPS and really, hosting anything semi-serious. Regarding the blog, I can’t promise to post regularly, but if I do so and it becomes something kind of serious, maybe I’ll consider moving the hosting to a VPS. Or keep it on the iPad. If it accomplishes the task of serving this website, maybe it’s okay for the job after all. Tools are only as interesting as you make them with your own creations.

This post is dedicated to my uncle Mariano, my family’s designated tech wizard. Hope you enjoy it! Also, shout out to Google Gemini’s 1m token context window.

  
    
      many unix utilities are missing, but some can be loaded from Cydia ↩
    
  


  


      ]]></content:encoded>
        </item>
    </channel>
</rss>