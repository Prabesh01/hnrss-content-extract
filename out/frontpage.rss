<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 05 Sep 2025 17:07:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[South Korea: 'many' of its nationals detained in ICE raid on GA Hyundai facility]]></title>
            <link>https://www.nbcnews.com/news/us-news/ice-hyundai-plant-georgia-enforcement-action-rcna229148</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139954</guid>
            <description><![CDATA[The Fortune 500 company confirmed the presence of law enforcement at the LG Energy Solution and Hyundai battery joint venture construction site in Ellabell.]]></description>
            <content:encoded><![CDATA[South Korea said Friday that it had expressed â€œconcern and regretâ€ to the U.S. Embassy over an immigration raid on a Hyundai facility in Georgia during which it said â€œmanyâ€ South Korean nationals had been detained.â€œThe economic activities of our companies investing in the U.S. and the rights and interests of our nationals must not be unfairly violated,â€ said Lee Jae-woong, a spokesperson for the foreign ministry of the key U.S. ally, according to the Yonhap news agency.Agents from Immigration and Customs Enforcement (ICE) as well as Homeland Security Investigations and other federal agencies were involved in the operation on Thursday, which an ICE spokesperson said was conducted in connection with an investigation into â€œunlawful employment practices and other serious federal crimes.â€ Steven Schrank, special agent in charge of Homeland Security Investigations in Georgia, told reporters on Thursday afternoon that the alleged unlawful practices were taking place at the â€œmulti-hundred acreâ€ construction site where South Korean companies Hyundai and LG Energy Solution are jointly building a new battery plant next to their manufacturing facility for electric vehicles.The facility in the town of Ellabell, about 28 miles west of the city of Savannah, employs about 1,400 people. It is considered one of Georgiaâ€™s largest and most high-profile manufacturing sites, according to The Associated Press.NBC News verified a video posted on social media showing HSI agents inside the construction site at Hyundaiâ€™s facility in Ellabell. One of the agents can be heard telling workers they had a search warrant for the entire site and asked that construction â€œbe ceased immediately.â€A worker who was there but whose name is being withheld told NBC News that agents came late Thursday morning and asked everyone on the premises whether they were U.S. citizens.Other videos on social media show agents lining workers up. In some instances, agents can be seen asking workers questions and searching their bags.In a statement to NBC News, Hyundai spokesperson Michael Stewart confirmed the presence of law enforcement at the LG Energy Solution and Hyundai battery joint venture construction site in Bryan County, where Ellabell is located.â€œWe are cooperating with law enforcement and are committed to abiding by all labor and immigration regulations,â€ Stewart said.It remains unclear how many people have been taken into custody, but Schrank said, â€œWe are making many arrests of undocumented individuals.â€NBC affiliate WSAV of Savannah reported that hundreds of undercover law enforcement vehicles and Humvees were reportedly seen at the scene. Large buses were also seen entering the site.Mary Beth Kennedy, a spokesperson for HL-GA Battery Co., LG Energy Solution and Hyundaiâ€™s joint venture, told WSAV in a statement that the company â€œis cooperating fully with the appropriate authorities regarding activity at our construction site. To assist their work, we have paused construction. We do not have further details at this time.â€Schrank added that the investigation was expected to continue beyond Thursday but did not provide a timeline.The ICE spokesperson added: â€œThis investigation is focused on ensuring accountability for those who violate the law and upholding the rule of law. Complex cases like this require strong collaboration and extensive investigative efforts.â€South Korea, the worldâ€™s 10th-largest economy, is a major automotive and electronics manufacturer whose companies have multiple plants in the United States. In July, Seoul pledged $350 billion in U.S. investment in an effort to lower President Donald Trumpâ€™s threatened tariffs on its products, which he ended up setting at 15%.In March, Hyundai said it would invest $21 billion in U.S. onshoring from 2025 to 2028, a number it said last month had increased to $26 billion. It said the initiatives involved in the investment â€” including a new $5.8 billion steel plant in Louisiana, expanded U.S. auto production capacity and a state-of-the-art robotics facility â€” were expected to create about 25,000 new direct jobs in the U.S. over the next four years.Nicole AcevedoNicole Acevedo is a national reporter for NBC News and NBC Latino.Laura StricklerLaura Strickler is the senior investigative producer on the national security team where she produces television stories and writes for NBCNews.com.Colin SheeleyColin Sheeley is a senior reporter for NBC News' Social Newsgathering team based in New York.Jennifer JettJennifer Jett is the Asia Digital Editor for NBC News, based in Hong Kong.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Protobuffers Are Wrong]]></title>
            <link>https://reasonablypolymorphic.com/blog/protos-are-wrong/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139656</guid>
            <description><![CDATA[Iâ€™ve spent a good deal of my professional life arguing against using protobuffers. Theyâ€™re clearly written by amateurs, unbelievably ad-hoc, mired in gotchas, tricky to compile, and solve a problem that nobody but Google really has. If these problems of protobuffers remained quarantined in serialization abstractions, my complaints would end there. But unfortunately, the bad design of protobuffers is so persuasive that these problems manage to leak their way into your code as well.]]></description>
            <content:encoded><![CDATA[
    Iâ€™ve spent a good deal of my professional life arguing against using protobuffers. Theyâ€™re clearly written by amateurs, unbelievably ad-hoc, mired in gotchas, tricky to compile, and solve a problem that nobody but Google really has. If these problems of protobuffers remained quarantined in serialization abstractions, my complaints would end there. But unfortunately, the bad design of protobuffers is so persuasive that these problems manage to leak their way into your code as well.
Ad-Hoc and Built By Amateurs
Stop. Put away your email client that is half-way through writing me about how â€œGoogle is filled with the worldâ€™s best engineers,â€ and that â€œanything they build is, by definition, not built by amateurs.â€ I donâ€™t want to hear it.
Letâ€™s just get this out of the way. Full disclosure: I used to work at Google. It was the first (but unfortunately, not the last) place I ever used protobuffers. All of the problems I want to talk about today exist inside of Googleâ€™s codebase; itâ€™s not just a matter of â€œusing protobuffers wrongâ€ or some such nonsense like that.
By far, the biggest problem with protobuffers is their terrible type-system. Fans of Java should feel right at home with protobuffers, but unfortunately, literally nobody considers Java to have a well-designed type-system. The dynamic typing guys complain about it being too stifling, while the static typing guys like me complain about it being too stifling without giving you any of the things you actually want in a type-system. Lose lose.
The ad-hoc-ness and the built-by-amateurs-itude go hand-in-hand. So much of the protobuffer spec feels bolted on as an afterthought that it clearly was bolted on as an afterthought. Many of its restrictions will make you stop, scratch your head and ask â€œwat?â€ But these are just symptoms of the deeper answer, which is this:
Protobuffers were obviously built by amateurs because they offer bad solutions to widely-known and already-solved problems.
No Compositionality
Protobuffers offer several â€œfeaturesâ€, but none of them see to work with one another. For example, look at the list of orthogonal-yet-constrained typing features that I found by skimming the documentation.

oneof fields canâ€™t be repeated.
map<k,v> fields have dedicated syntax for their keys and values, but this isnâ€™t used for any other types.
Despite map fields being able to be parameterized, no user-defined types can be. This means youâ€™ll be stuck hand-rolling your own specializations of common data structures.
map fields cannot be repeated.
map keys can be strings, but can not be bytes. They also canâ€™t be enums, even though enums are considered to be equivalent to integers everywhere else in the protobuffer spec.
map values cannot be other maps.

This insane list of restrictions is the result of unprincipled design choices and bolting on features after the fact. For example, oneof fields canâ€™t be repeated because rather than resulting in a coproduct type, instead the code generator will give you a product of mutually-exclusive optional fields. Such a transformation is only valid for a singular field (and, as weâ€™ll see later, not even then.)
The restriction behind map fields being unable to be repeated is related, but shows off a different limitation of the type-system. Behind the scenes, a map<k,v> is desugared into something spiritually similar to repeated Pair<k,v>. And because repeated is a magical language keyword rather than a type in its own right, it doesnâ€™t compose with itself.
Your guess is as good as mine for why an enum canâ€™t be used as a map key.
Whatâ€™s so frustrating about all of this is a little understanding of how modern type-systems work would be enough to drastically simplify the protobuffer spec and simultaneously remove all of the arbitrary restrictions.
The solution is as follows:

Make all fields in a message required. This makes messages product types.
Promote oneof fields to instead be standalone data types. These are coproduct types.
Give the ability to parameterize product and coproduct types by other types.

Thatâ€™s it! These three features are all you need in order to define any possible piece of data. With these simpler pieces, we can re-implement the rest of the protobuffer spec in terms of them.
For example, we can rebuild optional fields:
product Unit {
  // no fields
}

coproduct Optional<t> {
  t    value = 0;
  Unit unset = 1;
}
Building repeated fields is simple too:
coproduct List<t> {
  Unit empty = 0;
  Pair<t, List<t>> cons = 1;
}
Of course, the actual serialization logic is allowed to do something smarter than pushing linked-lists across the networkâ€”after all, implementations and semantics donâ€™t need to align one-to-one.
Questionable Choices
In the vein of Java, protobuffers make the distinction between scalar types and message types. Scalars correspond more-or-less to machine primitivesâ€”things like int32, bool and string. Messages, on the other hand, are everything else. All library- and user-defined types are messages.
The two varieties of types have completely different semantics, of course.
Fields with scalar types are always present. Even if you donâ€™t set them. Did I mention that (at least in proto31) all protobuffers can be zero-initialized with absolutely no data in them? Scalar fields get false-y valuesâ€”uint32 is initialized to 0 for example, and string is initialized as "".
Itâ€™s impossible to differentiate a field that was missing in a protobuffer from one that was assigned to the default value. Presumably this decision is in place in order to allow for an optimization of not needing to send default scalar values over the wire. Presumably, though the encoding guide makes no mention of this optimization being performed, so your guess is as good as mine.
As weâ€™ll see when we discuss protobuffersâ€™ claim to being godâ€™s gift to backwards- and forwards-compatible APIs, this inability to distinguish between unset and default values is a nightmare. Especially if indeed itâ€™s a design decision made in order to save one bit (set or not) per field.
Contrast this behavior against message types. While scalar fields are dumb, the behavior for message fields is outright insane. Internally, message fields are either there or theyâ€™re notâ€”but their behavior is crazy. Some pseudocode for their accessor is worth a thousand words. Pretend this is Java or something similar:
private Foo m_foo;

public Foo foo {
  // only if `foo` is used as an expression
  get {
    if (m_foo != null)
      return m_foo;
    else
      return new Foo();
  }

  // instead if `foo` is used as an lvalue
  mutable get {
    if (m_foo = null)
      m_foo = new Foo();
    return m_foo;
  }
}
The idea is that if the foo field is unset, youâ€™ll see a default-initialized copy whenever you ask for it, but wonâ€™t actually modify its container. But if you modify foo, it will modify its parent as well! All of this just to avoid using a Maybe Foo type and the associated â€œheadachesâ€ of the nuance behind needing to figure out what an unset value should mean.
This behavior is especially egregious, because it breaks a law! Weâ€™d expect the assignment msg.foo = msg.foo; to be a no-op. Instead the implementation will actually silently change msg to have a zero-initialized copy of foo if it previously didnâ€™t have one.
Unlike scalar fields, at least itâ€™s possible to detect if a message field is unset. Language bindings for protobuffers offer something along the lines of a generated bool has_foo() method. In the frequent case of copying a message field from one proto to another, iff it was present, youâ€™ll need to write the following code:
if (src.has_foo(src)) {
  dst.set_foo(src.foo());
}
Notice that, at least in statically-typed languages, this pattern cannot be abstracted due to the nominal relationship between the methods foo(), set_foo() and has_foo(). Because all of these functions are their own identifiers, we have no means of programmatically generating them, save for a preprocessor macro:
#define COPY_IFF_SET(src, dst, field) \
if (src.has_##field(src)) { \
  dst.set_##field(src.field()); \
}
(but preprocessor macros are verboten by the Google style guide.)
If instead all optional fields were implemented as Maybes, youâ€™d get abstract-able, referentially transparent call-sites for free.
To change tack, letâ€™s talk about another questionable decision. While you can define oneof fields in protobuffers, their semantics are not of coproduct types! Rookie mistake my dudes! What you get instead is an optional field for each case of the oneof, and magic code in the setters that will just unset any other case if this one is set.
At first glance, this seems like it should be semantically equivalent to having a proper union type. But instead it is an accursed, unutterable source of bugs! When this behavior teams up with the law-breaking implementation of msg.foo = msg.foo;, it allows this benign-looking assignment to silently delete arbitrary amounts of data!
What this means at the end of the day is that oneof fields do not form law-abiding Prisms, nor do messages form law-abiding Lenses. Which is to say good luck trying to write bug-free, non-trivial manipulations of protobuffers. It is literally impossible to write generic, bug-free, polymorphic code over protobuffers.
Thatâ€™s not the sort of thing anybody likes to hear, let alone those of us who have grown to love parametric polymorphismâ€”which gives us the exact opposite promise.
The Lie of Backwards- and Forwards-Compatibility
One of the frequently cited killer features of protobuffers is their â€œhassle-free ability to write backwards- and forwards-compatible APIs.â€ This is the claim that has been pulled over your eyes to blind you from the truth.
What protobuffers are is permissive. They manage to not shit the bed when receiving messages from the past or from the future because they make absolutely no promises about what your data will look like. Everything is optional! But if you need it anyway, protobuffers will happily cook up and serve you something that typechecks, regardless of whether or not itâ€™s meaningful.
This means that protobuffers achieve their promised time-traveling compatibility guarantees by silently doing the wrong thing by default. Of course, the cautious programmer can (and should) write code that performs sanity checks on received protobuffers. But if at every use-site you need to write defensive checks ensuring your data is sane, maybe that just means your deserialization step was too permissive. All youâ€™ve managed to do is decentralize sanity-checking logic from a well-defined boundary and push the responsibility of doing it throughout your entire codebase.
One possible argument here is that protobuffers will hold onto any information present in a message that they donâ€™t understand. In principle this means that itâ€™s nondestructive to route a message through an intermediary that doesnâ€™t understand this version of its schema. Surely thatâ€™s a win, isnâ€™t it?
Granted, on paper itâ€™s a cool feature. But Iâ€™ve never once seen an application that will actually preserve that property. With the one exception of routing software, nothing wants to inspect only some bits of a message and then forward it on unchanged. The vast majority of programs that operate on protobuffers will decode one, transform it into another, and send it somewhere else. Alas, these transformations are bespoke and coded by hand. And hand-coded transformations from one protobuffer to another donâ€™t preserve unknown fields between the two, because itâ€™s literally meaningless.
This pervasive attitude towards protobuffers always being compatible rears its head in other ugly ways. Style guides for protobuffers actively advocate against DRY and suggest inlining definitions whenever possible. The reasoning behind this is that it allows you to evolve messages separately if these definitions diverge in the future. To emphasize that point, the suggestion is to fly in the face of 60 yearsâ€™ worth of good programming practice just in case maybe one day in the future you need to change something.
At the root of the problem is that Google conflates the meaning of data with its physical representation. When youâ€™re at Google scale, this sort of thing probably makes sense. After all, they have an internal tool that allows you to compare the finances behind programmer hours vs network utilization vs the cost to store \(x\) bytes vs all sorts of other things. Unlike most companies in the tech space, paying engineers is one of Googleâ€™s smallest expenses. Financially it makes sense for them to waste programmersâ€™ time in order to shave off a few bytes.
Outside of the top five tech companies, none of us is within five orders of magnitude of being Google scale. Your startup cannot afford to waste engineer hours on shaving off bytes. But shaving off bytes and wasting programmersâ€™ time in the process is exactly what protobuffers are optimized for.
Letâ€™s face it. You are not Google scale and you never will be. Stop cargo-culting technology just because â€œGoogle uses itâ€ and therefore â€œitâ€™s an industry best-practice.â€
Protobuffers Contaminate Codebases
If it were possible to restrict protobuffer usage to network-boundaries I wouldnâ€™t be nearly as hard on it as a technology. Unfortunately, while there are a few solutions in principle, none of them is good enough to actually be used in real software.
Protobuffers correspond to the data you want to send over the wire, which is often related but not identical to the actual data the application would like to work with. This puts us in the uncomfortable position of needing to choose between one of three bad alternatives:

Maintain a separate type that describes the data you actually want, and ensure that the two evolve simultaneously.
Pack rich data into the wire format for application use.
Derive rich information every time you need it from a terse wire format.

Option 1 is clearly the â€œrightâ€ solution, but its untenable with protobuffers. The language isnâ€™t powerful enough to encode types that can perform double-duty as both wire and application formats. Which means youâ€™d need to write a completely separate datatype, evolve it synchronously with the protobuffer, and explicitly write serialization code between the two. Seeing as most people seem to use protobuffers in order to not write serialization code, this is obviously never going to happen.
Instead, code that uses protobuffers allows them to proliferate throughout the codebase. True story, my main project at Google was a compiler that took â€œprogramsâ€ written in one variety of protobuffer, and spit out an equivalent â€œprogramâ€ in another. Both the input and output formats were expressive enough that maintaining proper parallel C++ versions of them could never possibly work. As a result, my code was unable to take advantage of any of the rich techniques weâ€™ve discovered for writing compilers, because protobuffer data (and resulting code-gen) is simply too rigid to do anything interesting.
The result is that a thing that could have been 50 lines of recursion schemes was instead 10,000 lines of ad-hoc buffer-shuffling. The code I wanted to write was literally impossible when constrained by having protobuffers in the mix.
While this is an anecdote, itâ€™s not in isolation. By virtue of their rigid code-generation, manifestations of protobuffers in languages are never idiomatic, nor can they be made to beâ€”short of rewriting the code-generator.
But even then, you still have the problem of needing to embed a shitty type-system into the targeted language. Because most of protobuffersâ€™ features are ill-conceived, these unsavory properties leak into our codebases. It means weâ€™re forced to not only implement, but also use these bad ideas in any project which hopes to interface with protobuffers.
While itâ€™s easy to implement inane things out of a solid foundation, going the other direction is challenging at best and the dark path of Eldrich madness at worst.
In short, abandon all hope ye who introduce protobuffers into your projects.



To this day, thereâ€™s a raging debate inside Google itself about proto2 and whether fields should ever be marked as required. Manifestos with both titles â€œoptional considered harmfulâ€ and â€œrequired considered harmful.â€ Good luck sorting that out.â†©ï¸Ž




    
        â†
    
    
        â†’
    


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Does anyone still use Morse code?]]></title>
            <link>https://morse-coder.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139640</guid>
            <description><![CDATA[Free Morse code translator with image & audio decoding. Convert text to Morse, extract from images, play sound, flash light & download audio instantly.]]></description>
            <content:encoded><![CDATA[Translate Morse to English & English to Morse code InstantlyTextMorse Code0 charsHow to Use the Morse Code Translator1Text to Morse Code TranslationType or paste any text in the top input box, or click the random button (ðŸ”€) . we supports letters, numbers, and punctuation.2Morse Code to Text DecodingEnter Morse code in the bottom box using dots (.) and dashes (-). Separate letters with spaces and words with forward slashes (/).3Audio Playback & TrainingClick the play button to hear your Morse code with authentic audio signals. Adjust playback speed, frequency and WPM.4Visual Light IndicatorWatch the visual light indicator flash in sync with audio playback. Perfect for learning the rhythm and timing of Morse code signals.5Download & Export OptionsDownload your conversions as text files or export Morse code as audio files (WAV/MP3) for offline practice and sharing.6Professional SettingsAccess advanced audio settings to customize frequency (200-1000 Hz), playback speed, and WPM for professional training standards.ðŸ’¡ Pro Tips for Best Results:â€¢Morse Code Creator: use the green morse code generator button to create random phrasesâ€¢Use the copy button for quick text sharingâ€¢Toggle slash separators for different formatting stylesâ€¢Practice with repeat mode for skill developmentâ€¢Real-time character count for message trackingâ€¢Supports complete alphabet, numbers, and punctuationâ€¢Perfect for amateur radio and emergency communications]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A computer upgrade has shut down BART]]></title>
            <link>https://www.bart.gov/news/articles/2025/news20250905</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139270</guid>
            <description><![CDATA[Update 09/05/25, 9:15am:]]></description>
            <content:encoded><![CDATA[
            Update 09/05/25, 9:15am:Â Limited East Bay service will start at approximately 9:30am. There is no service to San Francisco.Yellow Line will service will resume from Antioch to 12th Street Oakland. Blue Line service will resume from Dublin to MacArthur. Orange line service will resume from Berryessa to Richmond. BART to Antioch service is resuming now.A computer equipment problem following network upgrade work is preventing the start of service this morning. Seek alternative means of transportation. bart.gov/alternatives provides options without BART service.
      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Don't Need Animations]]></title>
            <link>https://emilkowal.ski/ui/you-dont-need-animations</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45139088</guid>
            <description><![CDATA[Why you are animating more often than you should.]]></description>
            <content:encoded><![CDATA[When done right, animations make an interface feel predictable, faster, and more enjoyable to use. They help you and your product stand out.
But they can also do the opposite. They can make an interface feel unpredictable, slow, and annoying. They can even make your users lose trust in your product.
So how do you know when and how to animate to improve the experience?
Step one is making sure your animations have a purpose.
Purposeful animations
Before you start animating, ask yourself: whatâ€™s the purpose of this animation? As an example, whatâ€™s the purpose of this marketing animation we built at Linear?

This animation explains how Product Intelligence (Linearâ€™s feature) works. We could have used a static asset, but the animated version helps the user understand what this feature does, straight in the initial viewport of the page.
Another purposeful animation is this subtle scale down effect when pressing a button. Itâ€™s a small thing, but it helps the interface feel more alive and responsive.

Sonnerâ€™s enter animation, on the other hand, has two purposes:

- Having a toast suddenly appear would feel off, so we animate it in.
- Because it comes from and leaves in the same direction, it creates spatial consistency, making the swipe-down-to-dismiss gesture feel more intuitive.


But sometimes the purpose of an animation might just be to bring delight.
Morphing of the feedback component below helps make the experience more unique and memorable. This works as long as the user will rarely interact with it. Itâ€™ll then become a pleasant surprise, rather than a daily annoyance.
Press on the button to see it morph.
Used multiple times a day, this component would quickly become irritating. The initial delight would fade and the animation would slow users down.
How often users will see an animation is a key factor in deciding whether to animate or not. Letâ€™s dive deeper into it next.
Frequency of use
I use Raycast hundreds of times a day. If it animated every time I opened it, it would be very annoying. But thereâ€™s no animation at all. Thatâ€™s the optimal experience.
To see it for yourself, try to toggle the open state of the menu below by using the buttons belowpressing J and then K. Which one feels better if used hundreds of times a day?
Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommand
When I open Raycast, I have a clear goal in mind. I donâ€™t expect to be â€œdelightedâ€, I donâ€™t need to be. I just want to do my work with no unnecessary friction.
Think about what the user wants to achieve and how often they will see an animation. A hover effect is nice, but if used multiple times a day, it would likely benefit the most from having no animation at all.
Imagine you interact with this list often during the day.
Imagine you interact with this list often during the day.The same goes for keyboard-initiated actions. These actions may be repeated hundreds of times a day, an animation would make them feel slow, delayed, and disconnected from the userâ€™s actions. You should never animate them.
Since we canâ€™t really use a keyboard on touch devices, you can press the buttons below to see how it feels with and without animation.
To see it for yourself, focus on the input below and use arrow keys to navigate through the list. Notice how the highlight feels delayed compared to the keys you press. Now press  (shift) and see how this interaction feels without animation.Command MenuLinearApplicationChatGPTApplicationCursorApplicationFigmaApplicationObsidianApplicationClipboard HistoryCommandEmoji PickerCommandPress shift to toggle the animation
But even if your animation wonâ€™t be used too often and it fulfills a clear purpose, you still have to think about its speedâ€¦
Perception of speed
Unless you are working on marketing sites, your animations have to be fast. They improve the perceived performance of your app, stay connected to userâ€™s actions, and make the interface feel as if itâ€™s truly listening to the user.
To give you an example, a faster-spinning spinner makes the app seem to load faster, even though the load time is the same. This improves perceived performance.
Which one works harder to load the data?
A 180ms dropdown animation feels more responsive than a 400ms one:
Click on the buttons to compare the speed.
As a rule of thumb, UI animations should generally stay under 300ms.
Another example of the importance of speed: tooltips should have a slight delay before appearing to prevent accidental activation. Once a tooltip is open however, hovering over other tooltips should open them with no delay and no animation.
This feels faster without defeating the purpose of the initial delay.
Radix UI and Base UI skip the delay once a tooltip is shown.
Radix UI and Base UI skip the delay once a tooltip is shown.Building great interfaces
The goal is not to animate for animationâ€™s sake, itâ€™s to build great user interfaces. The ones that users will happily use, even on a daily basis. Sometimes this requires animations, but sometimes the best animation is no animation.
Knowing when to animate is just one of many things you need to know in order to craft great animations. If youâ€™d like to dive deeper into the theory and practice of it, Iâ€™ve created a course that covers everything you need to know:
Check out "Animations on the Web"]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[1TB Raspberry Pi SSD on sale now for $70]]></title>
            <link>https://www.raspberrypi.com/news/1tb-raspberry-pi-ssd-on-sale-now-for-70/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45138932</guid>
        </item>
        <item>
            <title><![CDATA[Development Speed Has Never Been a Bottleneck]]></title>
            <link>https://pawelbrodzinski.substack.com/p/development-speed-is-not-a-bottleneck</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45138156</guid>
        </item>
        <item>
            <title><![CDATA[Data Modeling Guide for Real-Time Analytics with ClickHouse]]></title>
            <link>https://www.ssp.sh/blog/practical-data-modeling-clickhouse/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137927</guid>
            <description><![CDATA[Learn how to build sub-second real-time analytics with ClickHouse. Complete guide covering data modeling strategies, optimization techniques, and practical S3-to-dashboard examples.]]></description>
            <content:encoded><![CDATA[
            
                Contents
                
                
  
    Data Flow for Real-time Analytics
      
        Data Flow is Knowing the Requirements
        Real-Time Analytics: A Tradeoff
        The Payoff of Great Data Flow
      
    
    ClickHouse Modeling Strategies: From Theory to Practice
      
        Modeling Data with ClickHouse
      
    
    Demo: Using S3 -> ClickHouse -> Rill
      
        Ingest and Transformation
        Visualizing in Rill
        What Did We Learn so Far?
      
    
    Applicable Tips & Tricks
      
        Deduplication Strategies
        Performance Optimization
          
            Partitioning Strategy
            Predicate Pushdown Optimization
            Pre-Aggregation with AggregatingMergeTree
          
        
        Storage Efficiency
          
            Data Sketches for Approximation
            Rollup to Optimal Time Granularity
          
        
        Sampling Strategies
          
            Statistical Sampling for Large Datasets
          
        
        Schema Management
          
            Table Projections for Query Optimization
            Schema Evolution Best Practices
          
        
        Time Series Optimization
          
            Always Store in UTC
          
        
      
    
    Choosing the Right ClickHouse Modeling Strategy
  

            
                    This article was written as part of my services
                Querying billions of weather records and getting results in under 200 milliseconds isnâ€™t theory; itâ€™s what real-time analytics solutions provide. Processing streaming IoT data from thousands of sensors while delivering real-time dashboards with no lag is what certain business domains need. Thatâ€™s what youâ€™ll learn at the end of this guide through building a ClickHouse-modeled analytics use case.
Youâ€™ll learn how to land data in ClickHouse that is optimized for real-time data applications, going from basic ingestion to advanced techniques like statistical sampling, pre-aggregation strategies, and multi-level optimization. Iâ€™ve included battle-tested practices from Rillâ€™s years of implementing real-time analytics for customers processing everything from financial transactions and programmatic advertising to IoT telemetry.
This article is for data engineers and practitioners who want to build analytics that deliver sub-second query responses, and who want to unlock ClickHouseâ€™s full potential for real-time analytics demands. By the end, youâ€™ll have a playbook for ClickHouse data modeling plus a working example that ingests NOAA weather data from S3 and visualizes it with a single configuration file.
Why ClickHouseIf you havenâ€™t heard of ClickHouse or are wondering why itâ€™s becoming the go-to choice for real-time analytics, hereâ€™s what sets it apart from traditional data warehouses.
ClickHouse achieves blazingly fast analytical performance through column-oriented storage that reads only relevant data, advanced compression (LZ4/ZSTD), and vectorized query execution that maximizes CPU capabilities. Its sparse primary indexing with data skipping eliminates irrelevant data blocks, while the C++ implementation avoids JVM overhead for bare-metal performance.
These innovations enable sub-second query responses on billions of rows, performance that would take minutes or hours in traditional data warehouses. Storage efficiency has a direct impact on both cost and speed at scale, making ClickHouse the ideal foundation for the real-time analytics modeling strategies covered in this article.

Before we see a concrete example of modeling data with ClickHouse, specifically for real-time and online analytical processing (OLAP) cubes, itâ€™s important to understand the flow of data, its trade-offs, and payoffs. Where Does Data Come From, and Where Does It Go?
Data Flow is Knowing the Requirements
Data flows from sources to analytics. In the simplest terms, we have sources of data, a transformation with aggregations, and the visualization. Most often, the data should travel from source to visualization as quickly as possible and respond fast to queries.

Most data flow modeling is handled in the transformation phase. Connecting to a source, whether it is an S3 or R2 bucket, a relational database like Postgres or others, or visualization on an analytical tool. We need to aggregate and combine the data to extract business insights out of masses of information to answer the questions our business needs to answer.
Obviously data modeling can get much more involvedâ€”looking at modeling open data stack, or looking at The State of Data Engineering and its challenges. However, modeling data has nothing to do with choosing tools in the first place. If we have the best tools but a bad data flow, itâ€™s not worth much.
The below illustration shows where the modeling part actually happens:

































From the book Data Modeling with Snowflake by Serge Gershkovich | Like seeing a forest for the trees, ubiquitous modeling allows us to see the business for the data.

Most often, modeling is more about offline, off-computer, and real conversations with the business people involved than figuring it out ourselves. We have to answer the questions â€œWhatâ€™s needed on a dashboard?â€ â€œWhich numbers are even possible with the data at hand?â€ and â€œHow can we get them, join and aggregate them with other data from the company to get the best possible insights?â€
Shifting Left: Another form of modelingShifting Left is another important concept related to data modeling. It means that the better we model and structure data at the source (left side of the data pipeline), the more efficient and accurate our analytics become downstream (right side). When raw data is properly typed, deduplicated, and structured early in the pipeline, we avoid expensive transformations later and reduce the risk of data quality issues propagating through our entire analytics stack. This is especially critical for real-time systems where you canâ€™t afford lengthy batch cleanup processes.
Real-Time Analytics: A Tradeoff
Real-time analytics specifics are always a tradeoff between data freshness and accuracy.
The moment the data is loaded, it is outdated. But to avoid pulling the latest all the time, we need to make sure the data is consistent across tables, meaning related data is pulled too when we refresh, so that itâ€™s cohesive and accurate.
In the end, you need a set of metrics that are business critical for your organization. Some businesses like IoT and e-commerce donâ€™t need all data, but specific data such as IP or location to identify quickly where users come from. Use cases like this especially need and benefit from low-latency query responses. Data needs to load near real-time and needs to deliver fast, flexible access to core analytics.
The Payoff of Great Data Flow
The payoffs of modeling are higher performance, insights on consistent data, and lower cost as we do not need to query production with a reduced aggregated data set and without the need for heavy overnight ETL jobs. We need less storage for aggregated data and get even faster query responses.
Imagine a fast river that flows constantly with great volume. This is what good data will look like when new data is coming in steady and accurate.
Letâ€™s see that in action with ClickHouse real-time modeling.
ClickHouse Modeling Strategies: From Theory to Practice
Now that we understand the data flow requirements for real-time analytics such as fast ingestion, efficient transformation, and sub-second query responses, letâ€™s explore how ClickHouse specifically addresses these challenges through its modeling approaches.
Remember our data flow: Sources â†’ Transformation & Aggregation â†’ ClickHouse â†’ Visualization. The key insight is that ClickHouse doesnâ€™t only serve as storage but can handle much of the transformation and aggregation work directly, eliminating traditional ETL bottlenecks.
ClickHouse offers several strategies to optimize this flow, each addressing different aspects of the freshness-accuracy tradeoff we discussed:
For Minimizing Query-Time Complexity:

Denormalizing data: Move joins from query time to insert time by flattening related tables into a single structure (One Big Table, approach). This trades some storage efficiency for dramatic query performance gains. Especially recommended for tables that change infrequently and not for high-cardinality or many-to-many relationships.
Dictionaries: Handle dimension lookups through in-memory key-value structures, perfect for enriching streaming data with relatively static reference information.

For Real-Time Aggregation:

Incremental Materialized Views: Shift computational cost from query time to insert time, computing aggregates as data arrives rather than when users request it. Most suitable for real-time aggregations and transformations, especially for single-table aggregations or simple enrichments with static dimension tables.
Refreshable Materialized Views: Handle complex multi-table joins and transformations on a scheduled basis, suitable when real-time freshness isnâ€™t critical. They are also useful for batch denormalization and building view dependencies (like DAGs) and can be scheduled with dbt, Airflow, and other data orchestrators. Refreshable MVs are similar to materialized views in traditional OLTP databases.

The fundamental principle underlying all these approaches is minimizing joins at query time. In traditional OLAP cubes, much of this complexity is handled by pre-built logical modeling layers. ClickHouse takes a different approach where you explicitly choose where in the pipeline to handle complexity based on your specific performance and freshness requirements.
Modeling Data with ClickHouse
An interesting new dimension is modeling multi-dimensional cubes. Whatâ€™s the difference, you might ask? Besides the difference between traditional OLAP cubes and modern OLAP cubes, which first stores measures and joins within the cube and pre-processes, whereas modern real-time databases systems like ClickHouse, Pinot, Druid, and StarRocks do not. This is at first glance a disadvantage, but on the other hand an advantage, that we can change our queries at query time without re-processing needed.
What else do we need to know about OLAP data modeling? We need to understand that OLAP cubes store data in a column-oriented (or columnar) way. This is important to the ClickHouse architecture. Unlike traditional row-oriented databases that store all values in a row together, ClickHouse stores all values that belong to a single column together. This also influences how we model our data and enables fast analytical queries based on a few columns out of potentially hundreds. ClickHouse only needs to read the data files for those specific columns, drastically reducing disk I/O compared to reading entire rows.
Usually when we model a multi-dimensional cube, we deal with facts and dimensions. The queries are optimized for sub-second response times and the users might be our clients or business users; there might only be one visualization layer in between such as a BI tool or Excel. This means itâ€™s mission-critical.
In ClickHouse and in general with cubes, we are working with dimensions, measures, and operators that operate on time aggregations and dimensions. You want rollups and drill-downs along multiple axes, with subtotals and potentially pivots.
SQL can sometimes be hard work to get right as we constantly pivot along different dimensions, and there are joins involved, different granularity, and all of a sudden, you accidentally duplicate your counting by adding a wrong dimension.
So how do we effectively model ClickHouse to get real-time data from start to end with no more than needed effort?
In the following example, weâ€™ll see several of these strategies in action: denormalization through data transformation during ingestion, partitioning for query optimization, and incremental processing for real-time updates.
Traditional CubesThereâ€™s no logical modeling layer like in SQL Server Analysis Services (SSAS), meaning we need to model our data outside of ClickHouse to create pre-defined optimized tables to query with the methods explained above such as materialized views, small lookup tables, or denormalized tables.
Demo: Using S3 -> ClickHouse -> Rill
But we can design and model the data flow easily to source data from an S3/R2 bucket, load from Kafka, or other streaming data sources.
Letâ€™s have a look at a practical example where we ingest data from S3, using ClickHouse as the engine to do transformation and aggregation, ingesting the data incrementally with the built-in refresh by ClickHouse, and visualizing with Rill.

































Dashboard overview in Rill

Watch the short video for the interactive version - below we are going to explain each config step by step.

      
    


Ingest and Transformation
This example represents an end-to-end data project, loading NOAA weather data that gets updated from S3 via ClickHouse and visualized in Rill. All within a single YAML shown here (expand to see the full code):


 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65


type: model
materialize: true
incremental: true

# Do an incremental refresh every hour.
refresh:
  cron: "0 * * * *"

# Use SQL partitions to define year-based partitions for NOAA data
# This demonstrates ClickHouse's S3 capabilities with yearly partitioning
partitions:
  sql: SELECT generate_series AS year FROM generate_series(2024, 2025)
  #grabbing all files
  #glob: s3://noaa-ghcn-pds/csv.gz/*.csv.gz 

# Load and transform NOAA weather data with proper column names and types
# This showcases ClickHouse's data transformation capabilities
sql: >
  SELECT
      '{{ .partition.year }}' AS __partition,
      now() AS __load_time,
      -- Transform raw CSV columns to proper NOAA weather schema
      COALESCE(c1, 'UNKNOWN') AS station_id,
      COALESCE(toDate(toString(c2)), toDate('1900-01-01')) AS measurement_date,
      COALESCE(c3, 'UNKNOWN') AS measurement_type, -- TMIN, TMAX, PRCP, SNOW, etc.
      toFloat32(c4) / 10.0 AS measurement_value, -- Convert from tenths
      c5 AS measurement_flag,
      c6 AS quality_flag,
      c7 AS source_flag,
      c8 AS observation_time,
      -- Add derived fields for analytics
      toYear(toDate(toString(c2))) AS measurement_year,
      toMonth(toDate(toString(c2))) AS measurement_month,
      toDayOfYear(toDate(toString(c2))) AS measurement_day_of_year,
      -- Temperature conversions for common analysis
      CASE 
        WHEN c3 = 'TMIN' THEN toFloat32(c4) / 10.0
        ELSE NULL 
      END AS temp_min_celsius,
      CASE 
        WHEN c3 = 'TMAX' THEN toFloat32(c4) / 10.0  
        ELSE NULL
      END AS temp_max_celsius,
      CASE 
        WHEN c3 = 'PRCP' THEN toFloat32(c4) / 10.0
        ELSE NULL
      END AS precipitation_mm
  FROM s3(
      's3://noaa-ghcn-pds/csv.gz/by_year/{{ .partition.year }}.csv.gz',
      'CSV'
  )

# Insert the results into a partitioned table that uses the MergeTree engine.
# Optimized for time-series weather data analytics
output:
  incremental_strategy: partition_overwrite
  partition_by: __partition
  engine: MergeTree
  # Optimize ordering for typical weather queries: by date, station, measurement type
  # Using COALESCE ensures non-nullable columns in sorting key
  order_by: (measurement_date, station_id, measurement_type)
  # Primary key for fast weather station and date lookups
  primary_key: (measurement_date, station_id)
  # TTL for data retention (optional - uncomment if needed)
  # ttl: measurement_date + INTERVAL 10 YEAR

Source code and full project can be found on GitHub at clickhouse-modeling-rill-example
So what happens here?
This YAML configuration demonstrates how ClickHouse can serve as both your data transformation engine and storage layer, eliminating the need for traditional ETL tools.
Data ingestion and transformation in one step: The sql section directly reads compressed CSV files from S3 using ClickHouseâ€™s native s3() function. Rather than requiring a separate ETL process to extract, clean, and load the data, ClickHouse performs all transformations during the ingestion process itself. The query handles data type conversions (like converting temperature readings from tenths to actual values with toFloat32(c4) / 10.0), creates derived fields for analytics (such as extracting year, month, and day components), and applies data quality measures using COALESCE to handle null values.
MergeTree is your built-in ETL engine: The engine: MergeTree specification transforms ClickHouse into what you can think of as â€œlocal ETL without the need for an ETL tool.â€ MergeTree engines are specifically designed for high data ingest rates and massive data volumes. When new data arrives, ClickHouse creates table parts that are automatically merged by background processes, maintaining optimal query performance without manual intervention. This means your data pipeline becomes very lightweight and self-managing â€“ new weather data gets ingested, transformed, and optimized automatically based on defined cron triggers.
Multi-level optimization strategy: This example demonstrates ClickHouseâ€™s ability to optimize at multiple levels simultaneously. At the query level, the order_by: (measurement_date, station_id, measurement_type) ensures that data is physically sorted for optimal access patterns typical in weather analytics. This is very important to your end query and how your response will perform. At the storage level, the partition_by: __partition creates year-based partitions that enable ClickHouse to skip entire data segments when querying specific time ranges. The incremental strategy with partition_overwrite means only changed partitions are reprocessed, not the entire dataset.
Real-time processing without complexity: The refresh: cron: "0 * * * *" configuration creates an automated pipeline that updates hourly without requiring external orchestration tools like Airflow or Dagster. ClickHouse handles the scheduling, dependency management, and incremental processing internally.
Further optimizations are TTL (time-to-live), which deletes data after a defined retention period such as hour + INTERVAL 90 DAY DELETE, or we can apply further table features such as:


1
2
3
4
5
6
7
8


# Additional optimizations for data lifecycle and projection management
table_settings: >
    # Handle projections during deduplication: 'rebuild' recreates projections after merge
    deduplicate_merge_projection_mode = 'rebuild',
    # Speed up TTL-based data compression merges (0 = immediate, default: 4 hours)
    merge_with_recompression_ttl_timeout = 0,
    # Speed up TTL-based data deletion merges (0 = immediate, default: 4 hours)  
    merge_with_ttl_timeout = 0

These settings optimize both deduplication behavior with projections and accelerate automatic data lifecycle management through more frequent TTL merges, ensuring expired data is cleaned up promptly rather than waiting for the default 4-hour intervals.
Native Deduplication FeaturesClickHouse provides built-in insert deduplication for retry scenarios by creating unique block_id hashes for each inserted block. Duplicate blocks are skipped automatically.
Key settings are insert_deduplicate=1 enables block-level deduplication (default for replicated tables) and insert_deduplication_token provides custom deduplication keys for explicit control. This is block-level deduplication at insert time, unlike ReplacingMergeTreeâ€™s row-level deduplication during merges. For more details, see the deduplication token documentation.
Visualizing in Rill
The above YAML is the source noaa-weather.yaml and when you start rill after cloning the example above with:


1


rill start git@github.com:sspaeti/clickhouse-modeling-rill-example.git

You can click on the source, and the data will be automatically loaded from the S3 source, and the above-defined transformations and conversions will be made:

































Source-View in Rill ingesting 58 million rows

What Did We Learn so Far?
To recap this example, ClickHouse offers a fundamentally different approach compared to other real-time databases like Druid, where most heavy lifting must be done ahead of ingestion using Spark or other compute engines. With ClickHouse, the engine itself handles complex aggregations and optimizations at ingestion time, during query execution, and even post-ingestion.

        Rill does all the orchestration
        Interestingly, Rill automatically spawns up ClickHouse and orchestrates the incremental loads and ingests data. If you will, Rill is doing orchestration work.


1
2
3


>  â¯ ps aux | grep clickhouse
sspaeti  1406478  0.1  0.4 477508 136528 pts/28  Sl+  22:42   0:00 clickhouse-watchdog                                 server --config-file /tmp/rill-modeling/clickhouse-modeling-rill-example/tmp/default/clickhouse/default/config.xml
sspaeti  1406566 54.8  3.2 12216304 899476 pts/28 Sl+ 22:42   1:12 /home/sspaeti/.rill/clickhouse/25.2.2.39/clickhouse server --config-file /tmp/rill-modeling/clickhouse-modeling-rill-example/tmp/default/clickhouse/default/config.xml


    
ClickHouse provides multiple levels of optimization that can be applied independently or combined:
Query-Level Optimizations:

Simple GROUP BY aggregations that process data from milliseconds to hours on the fly.
Data partitioning: Data is organized into directories based on partition keys for parallel processing.
Filter and partition pushdown: ClickHouseâ€™s optimizer pushes filters closer to the data source and skips irrelevant partitions, dramatically reducing I/O.

Storage-Level Pre-Aggregation Optimizations:
4. Incremental materialized views shift computation cost from query time to insert time for faster SELECT queries.
5. AggregatingMergeTree stores partial aggregation states directly in the table engine, merging rows with the same primary key into single rows containing combined aggregate statesâ€”enabling orders of magnitude data reduction and sub-second query performance.
This flexibility allows you to choose the right optimization strategy based on your specific use case, query patterns, and performance requirements.
Example of running ClickHouse locally with the StackOverflow dataset, 22 million rows







Querying stackoverflow data in ClickHouse locally | X Post
Alternative for more Powerful and managed Ingestion: ClickPipesClickPipes is ClickHouse Cloudâ€™s managed integration platform that makes ingesting data from diverse sources as simple as clicking a few buttons, providing a scalable, serverless ingestion experience with high throughput and low latency. Beyond object storage, ClickPipes supports Kafka/Confluent, database CDC from MySQL and Postgres, and streaming platforms like Kinesis and Event Hubs.
The platform includes fully managed operations with built-in error handling, automatic retries, schema evolution, and monitoring through dedicated error tables, plus enterprise features like API/Terraform integration and Prometheus metrics). For object storage specifically, ClickPipes supports continuous ingestion with configurable polling where new files must be lexically ordered (e.g., file1, file2, file3) for proper ingestion sequencing.
Applicable Tips & Tricks
In this chapter we look at practical strategies for data modeling with ClickHouse with practical tips and tricks for real-time analytics.
Deduplication Strategies
Why it matters: Real-time data streams often contain duplicate records due to network retries, system failures, or multiple data sources. Without deduplication, your analytics might show inflated metrics and incorrect insights.
How to implement: ClickHouse offers several deduplication approaches:

ReplacingMergeTree: Automatically deduplicates rows based on the sorting key during background merges.
Refreshable Materialized Views: Use GROUP BY with argMax() to keep the latest version of each record.
Custom Deduplication Logic: Implement application-level deduplication before insertion.

Best Practice: For high-throughput real-time scenarios, use ReplacingMergeTree with a proper sorting key that includes your natural deduplication fields (e.g., user_id, event_id, timestamp).
Performance Optimization
ClickHouse is all about performance and speed out of the gate. But here are some tips and practical examples to optimize even more.
Partitioning Strategy
Why it matters: Proper partitioning enables query pruning and parallel processing, dramatically reducing query times from minutes to seconds.
How to implement:

Partition by time (daily/monthly) for time-series data.
Use secondary partitioning for high-cardinality dimensions. This means adding additional partition keys beyond just time to handle columns with many distinct values (region in the example below).
Design partitions to match your most common query patterns.



1
2
3
4


-- Advanced: Combine with AggregatingMergeTree for maximum efficiency
PARTITION BY (toYYYYMM(timestamp), region)
ORDER BY (user_id, timestamp)
ENGINE = AggregatingMergeTree()

Predicate Pushdown Optimization
Why it matters: Moving filters closer to the data source reduces the amount of data processed at each query stage.
How to implement:

Structure your WHERE clauses to match your sorting key order.
Use low-cardinality columns early in filtering.
Leverage ClickHouseâ€™s automatic index usage for range queries with sparse index.
Advanced tip: Combine with materialized views to push aggregations to insert time, not just filters to data source.

Pre-Aggregation with AggregatingMergeTree
When to use: High-volume time-series data where the same aggregation queries run frequently.
Implementation: Use -State functions during INSERT and -Merge functions during SELECT to work with pre-computed aggregate states rather than raw data. More Information
Storage Efficiency
Data modeling has a real impact on cost when done correctly. Here are some strategies to reduce storage, therefore save cost, and speed up query responses by an order of magnitude.
Data Sketches for Approximation
Why it matters: Exact distinct counts and percentiles on billions of rows are very expensive and time-consuming. Data sketches use clever algorithms to deliver 99%+ accuracy for 1% of the cost and storage.
How to implement:


1
2
3
4
5
6
7


-- Challenge: Count unique users from 1B+ events without storing all IDs - from: https://datasketches.apache.org/docs/Background/TheChallenge.html
SELECT 
    uniqHLL12(user_id) as approx_unique_users,  -- Uses ~1.5KB vs 8GB+
    quantile(0.95)(response_time_ms) as p95_response_time,  -- 95th percentile approximation
    countDistinct(session_id) as approx_unique_sessions  -- Approximate distinct sessions
FROM events 
WHERE date >= '2024-01-01'

Impact: The above example has an accuracy of 99%+ and a memory footprint of <2KB with a speedboost of 100x by reducing storage.
Rollup to Optimal Time Granularity
Why it matters: Storing every millisecond-level event creates significant storage overhead. Most business analytics work at hourly or daily granularity.
How to implement:

Aggregate raw events to hourly summaries using materialized views or SQL aggregations.
Keep detailed data for recent periods (last 30 days) and aggregated monthly data for historical analysis, for example.
Use different retention policies per granularity level.

Sampling Strategies
Sampling is a statistical way to reduce data without compromising on getting the right insights.
Statistical Sampling for Large Datasets
Why it matters: When dealing with billions of events, sometimes a representative sample provides sufficient accuracy for analytics while dramatically reducing processing time and storage costs.
How to implement:


1
2
3
4
5
6
7
8
9


-- Random sampling: Take 1% of all events
SELECT * FROM events 
WHERE cityHash64(user_id) % 100 = 0

-- Time-based sampling: Higher resolution for recent data
SELECT * FROM events 
WHERE 
  timestamp >= now() - INTERVAL 7 DAY  -- Keep all recent data
  OR cityHash64(event_id) % 100 = 0    -- Sample older data

Best Practice: Use stratified sampling when you need to maintain proportional representation across important business dimensions (customer segments, product categories, geographic regions). Use consistent hash functions to ensure reproducible samples.
Impact: Can reduce data volumes by 90-99% while maintaining statistical significance for trend analysis and aggregate metrics.
Schema Management
Table Projections for Query Optimization
Table projections are ClickHouseâ€™s native feature for pre-computed, physically stored copies of your table data with different sort orders or pre-aggregations. Think â€œsame table, multiple indexes on steroidsâ€.
Why it matters: Different queries need different sort orders or aggregations. Projections let you maintain multiple optimized access patterns without duplicating tables, and the query optimizer automatically picks the projection with the least data to scan.


1
2
3
4
5
6
7
8
9


-- ClickHouse: Create projection optimized for user-based queries
ALTER TABLE events_obt ADD PROJECTION user_timeline
(SELECT user_id, timestamp, event_type ORDER BY user_id, timestamp);

-- ClickHouse: Pre-aggregated projection for analytics
ALTER TABLE events_obt ADD PROJECTION daily_stats  
(SELECT toDate(timestamp) as date, event_type, count() 
 GROUP BY date, event_type ORDER BY date);
 

dbt + ClickHouse ApproachUse dbt to create a denormalized One Big Table (OBT) in ClickHouse, then leverage ClickHouse projections for different query patterns instead of maintaining separate OLAP cubes.
Schema Evolution Best Practices
Why it matters: Real-time systems need to handle schema changes without breaking existing queries or requiring full data reloads.
How to implement:

Use nullable columns for new fields to maintain backward compatibility.
Implement â€œlatest stateâ€ modeling for slowly changing dimensions.
Leverage ClickHouseâ€™s automatic schema detection for JSON fields.
Snapshot approach: Daily/weekly full snapshots of dimensional data.

Time Series Optimization
When working with time series, dates are an important part of how we query and store data.
Always Store in UTC
Why it matters: Mixed timezones in analytical data lead to incorrect aggregations and confusing results when data spans multiple regions.
How to implement:

Convert all timestamps to UTC at ingestion time.
Store the original timezone as a separate column if needed for display.
Use ClickHouseâ€™s timezone functions for display conversion only.



 1
 2
 3
 4
 5
 6
 7
 8
 9
10


-- Convert and store in UTC, keep original timezone for reference
INSERT INTO events 
SELECT 
    toDateTime(local_timestamp, source_timezone) as timestamp_utc, -- Store in UTC (the key storage column)
    -- Display examples in different timezones
    toTimeZone(toDateTime(local_timestamp, source_timezone), 'Europe/London') as london_time,
    toTimeZone(toDateTime(local_timestamp, source_timezone), source_timezone) as original_local_time,
    
    -- other fields
FROM source_table;

Some of the Limitations of ClickHouseBesides all the strengths, some limitations canâ€™t be neglected. For example, itâ€™s more difficult to do updates and deletes (Mutations). Joins are limited in performance and functionality and thereâ€™s no full ACID transactions support. Thereâ€™s also no notion of foreign keys. This means referential integrity is left to the user to manage at an application level. Read more about this on ClickHouse Architecture 101 as well.
Choosing the Right ClickHouse Modeling Strategy
After exploring ClickHouseâ€™s capabilities for real-time analytics, the key question becomes: How do you choose the right modeling approach for your specific use case? As always, the answer depends on your data volume, latency requirements, complexity needs, and team capabilities. But we can say that ClickHouse lets us handle powerful use cases without the need for expensive ETL pipelines or an additional semantic layer.
For straightforward real-time scenarios, ClickHouseâ€™s native features shine. You can deduplicate within ClickHouse to land consistent data in your cube, and use the FINAL modifier to let ClickHouse fully merge data before returning results. This performs all data transformations that happen during merges for the given table engine, eliminating the complexity of external processing.
The ETL Pipeline Approach
However, for more complex data projects, you can always handle execution through external ETL performed outside of ClickHouse using tools like dbt, Airflow, Dagster, Kestra, Flink, BladePipe, or dlt. These tools can orchestrate batch or streaming transformations before loading data into ClickHouse, which is especially useful for complex pipelines or when you want to manage schema evolution, data quality, or referential integrity outside the database. The ClickHouse integration for dbt ensures this is performed atomically with a new version of the target table created and then atomically swapped with the version receiving queries via the EXCHANGE command.
Modeling outside of ClickHouse is a common approach with more complex landscapes, but if we want real-time analytics, batch ETL can break the flow of continuously updated streams. Thatâ€™s why this shouldnâ€™t be the first choice if you want real-time data, quickly updated.
The BI Approach  Thereâ€™s also a tradeoff with storing metrics within the OLAP cube versus outside of it. Because SQL aggregations and measures can be queried on the fly but canâ€™t be stored within ClickHouse easily, data modeling often happens outside ClickHouse or gets stored within BI tools. The advantage is you can change metrics at any time without running an ETL pipeline. The downside is you canâ€™t easily store or manage them except in your UI, whether itâ€™s a web app with an OLAP-ORM, notebooks, or Business Intelligence tools.
This is one reason why Rill pairs so well with ClickHouseâ€”it has a full-blown metrics layer built-in with all its capabilities out of the box. You can store metrics declaratively based on YAML, version control them, and update them in a governed way. For example, put them in a git repository and let users collaborate on these metrics, which then get blazingly fast query returns on ClickHouse. Rill gives you another layer of data modeling while using ClickHouse as a sub-second response query engine.
Ultimately, the choice between native ClickHouse modeling, external ETL pipelines, or BI tool integration comes down to balancing three key factors: data freshness requirements, transformation complexity, and team capabilities. ClickHouseâ€™s native approach eliminates traditional ETL overhead for most real-time use cases, but the flexibility to layer additional tools when needed ensures your analytics architecture can evolve with your business requirements.

To get started, check out the practical example that demonstrates ClickHouse ETL with NOAA weather data, or explore ClickHouseâ€™s comprehensive Schema Design documentation, which guides you through all the steps including querying large datasets like StackOverflowâ€™s 60+ million records locally within seconds.


Written as part of my services
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lava RGB]]></title>
            <link>https://amaiorano.io/2025/09/03/lava-rgb.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137914</guid>
            <description><![CDATA[Back in 2021, I installed an NESRGB on a front loader, which has been working great. For years now, NESRGB was pretty much the only mod available to get RGB out of the NES; but recently, a new mod known as Lava RGB came on the scene from a company in China. I bought one, and in this post I go over how I installed it on another front loader.]]></description>
            <content:encoded><![CDATA[
      
      
  
  
    Parts #
  
  

    


  Lava RGB 2.0 mod board and power module - $81.37 CAD
  Two 20 rounded pin headers - $16 CAD for ten on Amazon
  SNES multiout parts (more on that later)
  47K resistor for expansion audio


The Lava RGB kit took about a week to arrive, and was packaged decently well:



  
  
    The Build #
  
  

    

  
  
    Prepare NES main board #
  
  

    

  
  
    Remove PPU #
  
  

    

I picked a front loader with an NES-CPU-10 main board that was in excellent shape. I took it apart, and extracted the main board from the shell:



First order of business was to desolder the PPU:



I started by adding fresh solder, using flux to make it flow into the existing solder:





I then used my trusty desoldering pump to remove all the solder from the pins:







Using a trick I learned from Voultar, I used my finger nails to move each pin back and forth until they moved freely. For the usual more stubborn pins on the thicker ground plane, I added more solder and desoldered again until I was able to move the pins. With that, I extracted the PPU:



  
  
    Remove power module #
  
  

    

Next I needed to remove the original power module to replace it with the new one:



This can be pretty challenging, but with the right tools and technique, itâ€™s not too bad. I started by adding flux to the four large pins that anchor the power module to the main board and use my desoldering gun to remove most of the solder:





Then I use wicking braid to remove the solder lodged around the pins. I added a little more solder and flux to the pins until I was able to wick away most of the remaining solder:





At this point, the gaps around the pins are pretty clear:



The next trick is to desolder the 5 pins visible above - but not from this side, but rather from inside the power module. This is actually necessary for properly installing the new power module that comes with the Lava RGB kit. I pried off the metal plate:





I switched tips on my desoldering gun to one with a larger pitch, and desoldered the 5 pins:





With that done, I grabbed hold of the power module with one hand, while holding the main board with the other, and rocked the power module back and forth until it came free:





Weâ€™re left with the 5 pins still attached to the main board, ready to be soldered to the new power module later:



  
  
    Push main board capacitors flat #
  
  

    

As with the NESRGB, the caps on the main board need to be pushed down flat for the Lava RGB PCB to fit:







Normally this is simply a matter of pushing the caps down while heating the vias with a soldering iron. However, as I have a bunch of NES cap kits that I ordered from Console5, I decided to replace them:



I desoldered the three caps:



And replaced them with the new ones, making sure to match the capacitance values, and ensuring the voltage rating is equal or above the original one. When placing the caps, I laid them flat before soldering them in:



  
  
    Solder wires #
  
  

    

At this point, I decided to solder the ends of the Lava RGB wire connector to the power module and to the main board:



Whatâ€™s nice is that the order of the pads on the power module and on the mod board match, so itâ€™s easy to solder the first eight wires:







I soldered the eight wires to the pads:



The remaining four wires are used to allow controller 1 to perform an in-game reset (IGR) as well as palette swapping using key combos. IGR is performed by holding Select for about 2 seconds, then pressing A; while for palette swapping, you hold Select for 2 seconds and press Up on the dpad. To make this work, the RST wire needs to be wired to the reset line on the main board (where the actual Reset button is wired to), while the CLK, DATA, and LATCH pins need to be wired to their respective pins on the player 1 controller input lines.

I put this handy image together to identify the pins to solder to:



Noting the colors of each pin, I first soldered the RST line:





Then I soldered CLK, DATA, and LATCH. Note that DATA and LATCH are not in the same order as on the Lava RGB PCB:



  
  
    Power module #
  
  

    

As already mentioned, for my install I planned to add a SNES-style multiout. Rather than grab these connections from the Lava RGB mod board itself, I decided to get them from the power module since itâ€™s closer to where the port would be installed. For this, I used an 8 wire ribbon cable:



In retrospect, I should have made this cable longer, as I was limited to where I could place the multiout. Anyway, I soldered the cable to the power module. Conveniently, the power module PCB offers two ways to connect wires to it, pads and vias, so I used the vias for this:





Note that two of the wires, the grey and the purple, are not soldered yet. One of these will be used for 5V, and the other for audio, both of which will be wired later to the PV and PA pins of the 5 pin connector.

I also soldered a wire to GND, and soldered it to the ground plane of the main board:







At this point, I loosely positioned the PCBs to get a sense for where I was heading:



Next, I soldered the power module to the five pins coming from the main board:





And now I could solder the 5V and audio wires to PV and PA respectively:





This is what everything looked like at this point:



  
  
    Mod board #
  
  

    

Next up was installing the mod board:



As can be seen above, the kit includes two 20 square pin headers. Their official instructions expect you to solder the mod board PCB directly to the NES main board using these square pin headers, making sure to first solder the round pin socket onto the mod board itself so that the PPU can be inserted into it. The apparent advantage here is being able to swap out the PPU, but with the major disadvantage that the mod board cannot be removed from the main board, making it difficult to service the mod in the future. So instead of this, I did as is usually done with the NESRGB mod: solder the socket to the main board, and the PPU directly to the mod board. For this, I needed to either get a socket that works with the included square pin headers, or rounded pin headers - I chose the latter.

I soldered the socket to the main board, making sure to line up the notch:







I inserted the two rounded pin headers I purchased separately into the sockets, longer pins down:







Then laid the PCB over the pins, making sure to insert them into the right vias, and soldered them in place:





I carefully detached the mod board from the socket by rocking each edge back and forth. Some care has to be taken here as these rounded pins can bend and break very easily:



Before soldering the PPU to the mod board, I use my flush cutters to cut down the one set of pins that would be underneath the PPU, and then touched them up with the soldering iron. Although not strictly necessary, this allows the PPU to lay flat on the PCB:







I inserted the PPU onto the mod board, making sure to line up the notch, and soldered it in place:







Next, I soldered the wire connector to the mod board:







Now I carefully inserted the mod board back into the socket, making sure to line up the socket pins, while also routing the wires down and to the side, as there isnâ€™t much space between the connector and the expansion port (note that weâ€™re seeing the reflection of the wires on the edge of the expansion port):





  
  
    First test #
  
  

    

At this point, I could finally test the mod out. I hadnâ€™t soldered the multiout yet, and I didnâ€™t have a way to hook up the Saturn-style DIN, so I couldnâ€™t test RGB, but I could test composite. So I loosely put everything together and composite video and audio worked perfectly:







  
  
    Multiout #
  
  

    

For the SNES multiout, I 3D printed Laser Bearâ€™s Multiout Panel Mount Snap In Connector, and used PCBWay to print The Real Pheonixâ€™s PCB. I also needed to order the right #2 x 1/2â€ screw. Alternatively, one could order the parts directly from Laser Bear and The Real Phoenix. Anyway, here are the parts:







  
  
    Solder ribbon to multiout PCB #
  
  

    

I mapped out the pins Iâ€™d need to solder to, along with the specific color wires from my ribbon cable:



I soldered the ribbon cable to the multiout PCB:









For testing, I inserted the PCB into the 3D printed frame and screwed it in:





  
  
    Second test #
  
  

    

Using an old cable I made, I gave it a quick test:





It worked! Although note that the colors arenâ€™t quite right. It turned out that my cable was defective - green wasnâ€™t being passed through. I later used a better cable, and all was fine.

  
  
    Cut shell for multiout connector #
  
  

    

Using the 3D printed bracket, I marked off where I would need to cut into the shell to fit the 3D printed connector:







I made sure to position the hole as to not interfere with the posts. I actually wanted to position it more to the right, but as already mentioned, I had cut the ribbon cable too short, and didnâ€™t feel like redoing it.

Next came the most annoying part for me, since I still donâ€™t own a proper dremel: I used a drill, flush cutters, and filing tools to cut out the hole:















Finally, after way too long, the connector fit in perfectly:







I used the 3D printed clip to hold the connector in place:





The fit was a little snug considering that raised portion inside, but thankfully it was fine:



  
  
    Expansion audio #
  
  

    

Before closing everything up, I also wanted to enable expansion audio. I decided to use Voultarâ€™s PCB because this is what I had done for my NESRGB install. What I didnâ€™t realize at the time was that this wasnâ€™t necessary at all since the Lava RGB doesnâ€™t process audio like the NESRGB. Iâ€™ll show my misteps here, and how I corrected it, but this could definitely have been simpler!

I soldered a 1K and 47K resistor to the PCB:





I positioned the PCB on pins 2 and 9 on the expansion port pins and soldered the PCB in place:





It was at this point that I tested expansion audio and realized it wasnâ€™t working. After chatting with Toxic_Tripod0 on Discord, I realized my mistake. To enable expansion audio without NESRGB, all thatâ€™s needed is a 47K resistor between pins 3 and 9. This PCB is expressly designed to route expansion audio to the NESRGB for processing. At this point, I could have removed the PCB, but I realized I could fix this relatively easily by removing the 1K resistor, and soldering a wire from the exposed pad to pin 3:





As required, pins 3 and 9 were now connected via a 47K resistor:



I tested expansion audio, and it worked! If I were to redo this, Iâ€™d probably just connect a 47K throughole resistor between pins 3 and 9 instead.

  
  
    Closing everything up #
  
  

    

It was finally time to put everything back together:



I attached the power and controller cables to the main board:



I inserted the multiout PCB into the connector and screwed it in:





I reattached the cartridge slot to the main board, and carefully inserted the main board into the bottom shell. The new power module is only held by those 5 pins to the main board, so some care has to be taken when fitting it in place:



I decided not to put in the RF shielding as itâ€™s not really necessary, and would pinch and possibly short the reset and controller 1 wires used for IGR and palette switching. I put in all the screws, including the ones in the posts used for the RF shielding:



I screwed on the top shell, and was finally done! Here it is in all its glory:









  
  
    Final test #
  
  

    

With everything back together, I hooked it up using my SNES2VGA with a VGA cable to my gbs-control:





Looking good in RGB:



Interestingly, the colors donâ€™t exactly match the composite output:



Using the palette switching control (hold select + Up on dpad), I cycled through the palettes, which displays the name of the palette for a few seconds. Here are some samples:







  
  
    Thoughts #
  
  

    

Iâ€™ve been playing with this modded NES for a few days now, and it works and looks great. The Lava RGB 2.0 is definitely a worthy contender to the NESRGB, especially for the price. Although it doesnâ€™t process audio, tapping the NES-produced audio out from the new power module works fine, and the sound is very clean.

Itâ€™s worth noting that this mod doesnâ€™t require the new power module at all, especially in my case as I added the SNES-style multiout. However, for those who donâ€™t want the multiout, and want a no-cut mod, this is a nice way to do it. In my case, the power module made it easier to wire up the multiout, and possibly improved the audio output as the older power modules are known to add interference to the audio signal.

I just wanted to give a quick shout out to the folks on the ConsoleMods discord, especially Toxic_Tripod0, manadream, and RobStrange for their help.

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You're absolutely Right!]]></title>
            <link>https://absolutelyright.lol/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137802</guid>
            <description><![CDATA[Claude Code said it 0 times today]]></description>
            <content:encoded><![CDATA[
    
    Claude Code said it 0 times today
    

    
      
      
          
          Absolutely right
        
        
          
          Just right
        
      
    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenAI eats jobs, then offers to help you find a new one at Walmart]]></title>
            <link>https://www.theregister.com/2025/09/05/openai_jobs_board/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137658</guid>
            <description><![CDATA[: Move over LinkedIn, Altman's crew wants a piece of the action]]></description>
            <content:encoded><![CDATA[
For those worried that AI is going to disrupt their jobs, OpenAI has the solution â€“ take its certification and use a newly announced jobs board to find a new role.
On Thursday, Fidji Simo, OpenAI's head of applications (and former CEO of Instacart), announced the plan for workers to advertise themselves to the company's customers for new jobs. She said that while AI is going to shake up the employment market, who better to solve that problem than the people doing the shaking?
"AI will be disruptive. Jobs will look different, companies will have to adapt, and all of us â€“ from shift workers to CEOs â€“ will have to learn how to work in new ways," she said in a blog post.

    

"At OpenAI, we can't eliminate that disruption. But what we can do is help more people become fluent in AI and connect them with companies that need their skills, to give people more economic opportunities."

        


        

Simo's plan is that workers should take courses in tech literacy at its OpenAI Academy and then advertise themselves on a forthcoming jobs platform. She said the company has already signed up some big names to the scheme, although maybe the choice of Walmart as an early adopter might not encourage IT admins in their future career paths.
OpenAI declined to comment further on the plans.

        

"At Walmart, we know the future of retail won't be defined by technology alone â€“ it will be defined by people who know how to use it," Walmart US CEO John Furner said in a canned statement.
"By bringing AI training directly to our associates, we're putting the most powerful technology of our time in their hands â€“ giving them the skills to rewrite the playbook and shape the future of retail."


Biased bots: AI hiring managers shortlist candidates with AI resumes

OpenAI wants to bend copyright rules. Study suggests it isn't waiting for permission

White House bans 'woke' AI, but LLMs don't know the truth

Microsoft unveils home-made ML models amid OpenAI negotiations

The OpenAI Academy has had some big-name sign-ups, particularly the respected computer science teachers at Georgia Tech, but Simo says that the business is pushing hard to build on a White House plan to make AI a core skill for American workers â€“ so long as the engines they use aren't too woke.
What Simo didn't mention directly is that getting into the jobs market would bring the company into competition with Microsoft, one of its biggest backers. LinkedIn is the primary Western jobs site and OpenAI setting up a competitor might get in the way of cordial relations.
Microsoft had no comment on the matter, but OpenAI appears to be only scooping the AI cream, and whatever else floats to the top of the market, on its proposed employment register. There's also the question of whether or not the skills OpenAI is shilling will have any validity in the actual jobs market.

        

Meanwhile, CEO Sam Altman and most of the tech glitterati attended a dinner hosted by First Lady Melania Trump to discuss AI last night. Elon Musk wasn't there, but insists he was invited. Â®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using Your Phone on Toilet May Give You Hemorrhoids: Study]]></title>
            <link>https://www.nbcnews.com/health/health-news/phone-use-hemorrhoids-bathroom-social-media-scrolling-rcna228080</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137656</guid>
            <description><![CDATA[Excessive scrolling on smartphones while on the toilet can lead to itchy, painful hemorrhoids, finds a new study.]]></description>
            <content:encoded><![CDATA[Of all the crappy ways smartphones have affected our health, this one is a real kick in the pants.A first-of-its-kind study links excessive scrolling on the phone while sitting on the toilet with hemorrhoids. (Insert poo emojis.)  But, seriously. Sitting on an open bowl offers no support for the pelvic floor. That puts pressure on veins in the rectum, making them swollen and inflamed. â€œThe longer you sit on the toilet, the worse it is for you,â€ said Dr. Trisha Pasricha, director of the Beth Israel Deaconess Medical Centerâ€™s Institute for Gut-Brain Research Institute in Boston. Pasricha is also an author of the study, which was published Wednesday in PLOS One.And smartphones are designed to keep people fixated for as long as possible. â€œTheyâ€™re completely consuming to us in ways that wasnâ€™t happening to the casual bathroom reader in the 80s,â€ Pasricha said. â€œThey could much more easily put the newspaper down and get up and leave.â€Pasricha and colleagues surveyed 125 adults just before they were about to have a routine colonoscopy to screen for colorectal cancer. Eighty-three (66%) of the participants admitted to using their phones in the bathroom â€” mostly to catch up on news of the day and scroll through social media.Gastroenterologists performing the colonoscopies looked for evidence of inflamed veins, or hemorrhoids. People who said they took their phone into the bathroom were 46% more likely to have hemorrhoids compared to the others. The risk remained even when researchers accounted for other factors associated with hemorrhoids, including dietary fiber, exercise and constipation or straining while using the toilet. Hemorrhoids arenâ€™t necessarily dangerous, but they can be bothersome, itchy and even painful. They also bleed sometimes, understandably causing concern and leading to nearly 4 million doctorâ€™s office and emergency department visits a year. Over time, â€œpelvic floor dysfunction can also lead to incontinence, worsen constipation and be associated with rectal pain,â€ said Dr. Reezwana Chowdhury, an inflammatory bowel disorder specialist at the Johns Hopkins University School of Medicine. Chowdhury was not involved with the new research.Whatâ€™s more, microscopic particles from urine and feces are sent flying through the air when a toilet is flushed. Taking a phone into the bathroom, Chowdhury said, â€œis kind of gross.â€ Younger patientsIn the new study, smartphone users in the bathroom tended to be younger, meaning adults in their 40s and 50s, versus people over age 60.Dr. Robert Cima, a colorectal surgeon at the Mayo Clinic in Rochester, Minnesota, said heâ€™s noticed an uptick in recent years of people coming in with hemorrhoids. â€œI am seeing younger, earlier- and middle-aged people having more hemorrhoidal complaints, but I canâ€™t tie it to smartphones,â€ said Cima, who was not involved with the new study. â€œMaybe itâ€™s because theyâ€™re using smartphones or they have better access to care or theyâ€™re not eating appropriately.â€The 5-minute ruleThe experts agreed that business on the toilet should take no longer than 5 minutes. More than 37% of study participants who used a smartphone in the bathroom stayed for longer than that, compared to 7% of people who kept their phones out of the bathroom.  Pasricha and other experts do not advocate for taking a phone into the bathroom. If you absolutely must, set a timer.â€œIf the magic is not happening within five minutes, itâ€™s not going to happen,â€ Pasricha said. â€œTake a breather and try again later.â€Erika EdwardsErika Edwards is a health and medical news writer and reporter for NBC News and "TODAY."]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Relace (YC W23) Is Hiring for Code LLM's (SF)]]></title>
            <link>https://news.ycombinator.com/item?id=45137554</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137554</guid>
            <description><![CDATA[Hey, we're a highly technical team building code generation models, and growing fast. We're looking for people who are down to scrap and love to build -- on both technical and GTM/Devrel roles.]]></description>
            <content:encoded><![CDATA[Hey, we're a highly technical team building code generation models, and growing fast. We're looking for people who are down to scrap and love to build -- on both technical and GTM/Devrel roles.If you have a Physics, Math, CS degree; and training fast codegen models is something that piques your interest, please email me directly at pzhou@relace.ai.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I Ditched Docker for Podman (and You Should Too)]]></title>
            <link>https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137525</guid>
        </item>
        <item>
            <title><![CDATA[Interview with Japanese Demoscener â€“ 0b5vr]]></title>
            <link>https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45137245</guid>
            <description><![CDATA[â†’æ—¥æœ¬èªžã§èª­ã‚€Welcome to &amp;quot;Interviews with Demosceners&amp;quot;! This time, we welcome Japanese demoscener 0b5vr, who mainly creates 64K...â€¦]]></description>
            <content:encoded><![CDATA[â†’æ—¥æœ¬èªžã§èª­ã‚€Welcome to â€œInterviews with Demoscenersâ€! This time, we welcome Japanese demoscener 0b5vr, who mainly creates 64K and 4K intros.For many, 0b5vr is best remembered for his 64K demo â€œ0b5vr GLSL Techno Live Setâ€, released at Revision 2023. In this interview, he talks about how this piece was created, as well as his recent live music performance.He also talks about trends around the Japanese demoscene, like music production with GLSL, machine live, and generative VJ. I also took the chance to ask how he feels about sceners like meâ€”that is, people who know nothing about programming or technology! Happy reading!Note: If you donâ€™t know what demoscene is, you may want toÂ start from here!First of all, could you introduce yourself?Iâ€™m 0b5vr, and I donâ€™t belong to any particular group. I mainly work on 64k intros and 4k intros/exegfx using WebGL. I also compete in Shader Jam and perform live coding and VJ sets at club events and similar venues.Your demo â€œ0b5vr GLSL Techno Live Setâ€ had a strong impact on me. I was curious about this. It says â€œLive Set,â€ but was released in the 64K category. What is this exactly? Is this live coding?0b5vr GLSL Techno Live Set (â€œ0mixâ€) is indeed a 64K intro demo. Just like any other 64K intro, this audiovisual piece is generated from a 64KB fileâ€•an HTML file, in this case.That said, as described in the title, its format is â€œLive Set.â€ It can be somewhat tricky, because it looks like a recorded video of a live performance at an event, but itâ€™s actually a 64K intro.Hmmâ€¦ Iâ€™m still not sure if I understood correctly. Could you elaborate a bit more?0mix was inspired by three different scenes: techno demos, live coding, and 64K intros.Let me start with techno demos. There are many techno-themed demos in the history of the demoscene. If you look at the demos such as â€œMediumâ€ by Einklang.net, â€œX-MIX 2004: Ion Traxxâ€ by Kewlers & mfx, and â€œEmixâ€ by Epoch, they use multiple tracks mixed together like a DJ set, rather than a single techno soundtrack. They also use VJ-style visuals to create an atmosphere similar to a club event. Emix has black-and-white visuals with unique textures that fit perfectly with cold, mechanical techno, and itâ€™s one of my favorites.Next is live coding. Live coding is a live performance where visuals and music are generated with programming in real time. On the screen, youâ€™ll see the visuals and sound waveforms being generated alongside the code youâ€™re writing. This highlights that the artwork is generated by code. In the demoscene, live coding sessions focus mostly on visuals in GLSL (eg, Shader Showdown, Shader Jam). But in live coding events like Algorave and Eulerroom, music live coding is as popular as, or even more popular than, visual coding. From what I see, Tidal Cycles and Sonic Pi are the most commonly used tools in those environments. (Reference video)Finally, thereâ€™s the 64K intro. Itâ€™s a category where you create visuals and audio with an executable file of just 64KB. This is the most challenging category since every element has to be procedurally generated within the intro. Most 64K creators build their own engines and tools from scratch. This category requires a broad range of knowledge and skills including modeling, animation, rendering, post-processing, music, and compression.If I managed to merge all three inspirations and create a 64K techno demo with music generated by live coding, I knew I could present it to demosceners and other creators around the scene with confidence. I came up with the idea about a year before Revision 2023. Over the course of that year, I refined a demo engine, built a live coding environment, composed music, and created visual assets almost entirely on my own. Hereâ€™s the working environment for 0mix. The top screen shows the preview, timeline, etc., while the bottom screen is the code editor. Basically, I spend most of the time in the code editor. So, you climbed the highest mountain by yourself. What was the process like?It was extremely tough and painful to spend a year working on a challenging 64K project by myself. My advice is to collaborate with others. At the very least, you should find someone you can discuss the progress with. It was indeed fun to surprise many friends at demoparties, but at the end of the day, completing the project is more important.You entered 64K compo, but it ended up being released in the PC Demo compo. Did that bother you?Itâ€™s true that 0mix was released in the PC Demo Compo at Revision 2023. That was because it was the only entry in the PC 64K intro, which wasnâ€™t enough to hold a separate compo. So the two compos were merged. The same thing happened at Revision 2022. PC 64K intro compo was incorporated into the 8K intro compo because there were only two entries. Nevertheless, Iâ€™ve always pursued uncompromising quality, so I was down with it. Along with the works of other demo groups (such as Fairlight, mfx, and Still), I think I could contribute to making that compo interesting.Ah, youâ€™re right. That felt like a never-ending compo!There were so many entries for Revision 2023, and from the chat I got the impression that many participants and viewers were exhausted after the compo. Still, it was a great compo. All of the top works featured demoscene-style visuals built with their own engines, and their narratives were also impressive. So Iâ€™m happy with my result. When thereâ€™s a big entry in the compo Iâ€™m in, I feel more accomplished because it means I helped make that compo exciting together with those great pieces.Thatâ€™s right, I remember some big names rushing in at the end. Nevertheless, this demo stood out for its originality.Thank you. Revision has an award called â€œCrowd Favoriteâ€ where viewers can vote for their favorite demo in any category, and 0mix received first prize. 0mix is a piece that reflects what I love, so I felt happy that everyone else enjoyed it, too.photo provided by 0b5vrCongratulations! It was indeed a cool demo.Oh, I have a question for you. How do you feel about the code constantly shown in 0mix? What kind of impression does it give you?  (Interviewerâ€™s note: Iâ€™m not from the programming field. Iâ€™m the type of person who chooses a laptop by its color.)Maybe itâ€™s more like a design or typography? It says â€œlive coding,â€ so I figured this code is for its visuals, but I have absolutely no idea if the code itself is cool or not. If I didnâ€™t know what live coding is, Iâ€™d probably just look at it as part of the design, just like seeing the typography in a language I donâ€™t understand.Ah, thatâ€™s interesting! Actually, the code displayed on the screen is not for visuals but for music. I use a programming language called GLSL, which is normally used to generate visuals. But 0mix is a live performance-themed demo where I use GLSL for music, and thatâ€™s why itâ€™s called â€œGLSL Techno Live.â€ If you look at the code closely, youâ€™ll see the parts for instruments, like â€œKICK,â€ â€œHIHAT,â€ and â€œBASS.â€ And by adding and subtracting these elements, I shaped the flow of music.Ohh, so that was code for music! But even after knowing this fact, my impression of this piece hasnâ€™t really changed. I guess that shows I interpreted the code as part of the design. Is it okay if a viewer like me sees it that way? (laughs)In my post about this production on Scrapbox, I wrote, â€œfor viewers without coding knowledge, it feels like music-making magic. And for viewers who know programming languages and environments, itâ€™s a hint to guess the next move.â€ So I expected that some people would see it as part of the design.To reveal a bit more about my understanding, now I do understand that â€œdemo is generated from an executable fileâ€ and that â€œa 64K piece has a 64KB file.â€ But I still donâ€™t see things like â€œthis is real-time rendering, so itâ€™s more impressive than live-actionâ€ or â€œitâ€™s great quality considering this is 64KB.â€ Basically, I watch demos like I watch music videos, and the only thing that matters to me is whether I find it cool or not.Ryoji Ikeda has a work that presents data including planets and genes using 5Ã—5 pixel fonts. Of course, only experts can truly understand such data, so most of us simply enjoy the visual design that comes out of it. Even if we try to find deeper meaning in it, we probably just end up saying something like, â€œWow, the world is huge.â€ Iâ€™ve read that Ikeda actually intended for viewers to see it that way.Oh, then Iâ€™m actually one of his intended viewers. When I first saw his installation video, I knew him as a musician, so I thought, â€œWow, thatâ€™s his MV? Cool! Very futuristic!â€ I later realized that it wasnâ€™t just design. Itâ€™s nice to know that creators and demosceners expected viewers like me, and personally, I feel relieved. Iâ€™d always thought they might be annoyed to hear a clueless person like me commenting on their piece. (laughs)To me, how others first got interested in a piece or in the culture is as fascinating as the motives behind its creation. So I do appreciate sceners who are not from the tech side!Thank you! Thatâ€™s really nice and reassuring to hear! OK, letâ€™s go back to that music code. You wrote in your post on Scrapbox that you put a lot of time and effort into the music.Actually, I had never really made this type of techno music before, so I watched a lot of live performances of this style and tutorials on YouTube. I also bought and tried hardware for â€œmachine liveâ€ performances, like the Elektron Syntakt and Dirtywave M8, for research.What is â€œmachine liveâ€?â€œMachine liveâ€ is a type of music performance similar to live coding. Performers use music equipment like grooveboxes and modular synths in real-time to control the sound during the performance. What you can do depends on the features of the equipment, so performers always have to be aware of limitationsâ€”something somewhat similar to the demoscene. Itâ€™s a fascinating culture. Thereâ€™s even a â€œDAWless Liveâ€ category where you perform without using a DAW, the standard PC-based music production system. For 0mix, I drew a lot of inspiration from the philosophy and methods of machine live and applied them to GLSL live coding. (Reference video)I just watched the reference video you sent me. Does everyone in this scene really use that much gear?Of course not. Not everyone uses this much equipment, or equipment of this size, for live performance. Lately, it seems like the palm-sized Dirtywave M8 is trending for live sets. The Dirtywave M8 uses a tracker-style UI, and itâ€™s fun to compose with. Plus, it fits well with the demoscene aesthetic.I did a lot of research on machine live and live coding performances, and this gave me ideas about how to create sound and how to evolve live performance. But that only covered the technical side. When it comes to making techno, especially abstract sounds, I had to learn through trial and error and trust my feelings. Even after I learned how to make sounds on standard hardware or software, GLSL follows a completely different set of rules, and I had to be really fired up to tackle it.I heard that you did a live performance recently. What kind of event was it?I performed a live coding set at â€œdraw(tokyo); #2â€ in March 2025. â€œdraw(); â€ is a club event focused on audiovisuals, especially live coding and generative VJ (the so-called â€œgene-keiâ€ performances). It takes place from time to time in VRChat and at physical venues.At draw(tokyo); #2, I performed using Wavenerd, my custom GLSL live coding environment. For my 40-minute live set, I mainly used techno patterns created for 0mix. It was a really memorable experience, since it was my first time doing a live music performance with Wavenerd. Iâ€™d love to do more live performances in the future.The â€œWavenerdâ€ system I used for my live coding performance at draw(tokyo); #2. Since we were chroma keying with VJ visuals, the background is blue. The performers are always lit up in blue.When a coder does a live music performance, arenâ€™t you too busy typing code in front of the PC to even look at the audienceâ€™s reaction?During the performances, I rewrite parts of prewritten code, so I donâ€™t need to constantly keep typing. But Iâ€™m busy adding and removing parts, changing parameters, and doing some DJ-style mixing, so basically I completely zone in on the screen. That said, I can still see the audienceâ€™s reactions to some extent, and I felt really happy when they reacted at the moments I expected.Do you know who the primary audience is? I guess this kind of live performance requires some knowledge to really enjoy it.I still donâ€™t know what kind of audience it attracts. From what I saw, I got the impression that many of them are interested in musical experiences and visual production at least. But Iâ€™m not sure how many are interested in coding, or actually create things with code. How technical it should get, how strictly you stick to the technical restrictions, and how much you make the audience danceâ€”I think performers are expected to balance these elements well. Probably, this is something gene-kei performers constantly have to tackle. In fact, quite a few performers change their set depending on the tone of the event.Did you have VJs for your live performance?Yes, I asked fellow demosceners, ukonpower and Renard, and they generated visuals that matched the techno. I just told them, â€œIâ€™m going to do 0mix,â€ and they both knew what it meant, so everything went very smoothly. (laughs) They created visuals in my style, but their own personalities also shone through. It was really cool.Oh, thatâ€™s really cool!  According to your discography, you also have 4K as well as 64K works. Is there a reason for that? For the 4K intros Iâ€™ve released lately, I can usually create them in one or two weeks. But 64K is my soul, so I want to keep making 64K intros. The thing is, 64K requires hundreds of times more work than 4K. So, when I donâ€™t have the time or motivation but still want to contribute to a demoparty, I just make a 4K intro.I must say that the production environment for 4K intros is well-supported in the current demoscene. Recently Iâ€™ve been using 0x4015â€™s minimalGL. With this demotool, I can easily create 4K intros just by writing GLSL. That being said, I wouldnâ€™t recommend it to everyone, because you also have to write the music in GLSL.In 2023, I released a 4K intro called â€œArchitectural Shapeshifterâ€ with Renard. For this piece, Renard was in charge of the concept and visuals, while I was in charge of the music and direction. We used minimalGL for this piece as well. It was the first time for Renard to create a 4K intro, but he was able to create it easily. We collaborated by tweaking the source code on GitHub and communicating via Discord. We exchanged ideas and suggestions on each otherâ€™s code, and it turned out to be a very efficient workflow.There are many coders who can write GLSL in Japan, but not many of them take on 4K. So Iâ€™d love to collaborate more using minimalGL.Whatâ€™s hot in the Japanese demoscene these days? What category is popular? I noticed there was a demoparty called SESSIONS in Japan last year.It seems like a lot of people are coming into the demoscene from shader culture centered around VRChat. The people I got to know at demoparties like SESSIONS were mostly active in VRChat. In particular, the event draw(); seems to have a strong influence, and many of the people who got interested in live coding or generative VJ through draw();â€™s audiovisual experience also developed an interest in the demoscene.Live coding and generative VJ becoming a gateway into the demoscene sounds like a new path to me.Yes, indeed. draw();â€™s main crew, Saina-san, purposefully aims for a crossover with demoscene culture, like SESSIONS, and this accelerates the influx. Weâ€™re really grateful for that.Iâ€™m sure a person like that is supporting the demoscene in Japan and around the world.  OK, letâ€™s go back to the production. Is there anything you do in everyday life to get inspired for your creations?I check Pouet and Demozoo as much as possible to stay in the know about recent demoscene productions. If I ever stopped checking Pouet and Demozoo, I think that would be the end of me as a demoscener.I also try to take in other cultures as well. Recently, Iâ€™ve been fascinated by the flashy audiovisual productions in pachinko and pachislot machines. They use dazzling visuals and music to stir up the spirit of gambling. These productions thoroughly pursue how to exploit the human reward system, all within machines that operate under very strict legal restrictions. In a way, I think this represents the highest peak of visual entertainment.I also go for walks frequently. Especially walking around Tokyo late at night gives me a strong sense of urban life and social activity, and it inspires me a lot. â€œDomainâ€œ, a 64K intro I released at Tokyo Demo Fest 2021, was heavily inspired by night Tokyo. I find the concept of the night city very interesting, and Iâ€™d like to explore it further.Which areas do you usually walk around?I mainly walk around downtown. I can feel the rhythm of social activity through peopleâ€™s movements, clothing, and buildings. Itâ€™s also very fun to walk around residential areas. When I imagine that this is someoneâ€™s everyday life, I can sense their presence through the scenery.Do you have anything you always keep in mind when you create, like a routine or your own personal rule?For my demo source code, I use Git for version control and share as much of the code as possible on GitHub. Basically, I publish my source code under the Creative Commons BY-NC 4.0 license, and users can adapt and use it freely for non-profit purposes. By publishing my source code, I allow other people to refer to my production methods. In fact, Iâ€™ve often heard that people have made demos based on my code. Getting more chances to discover other demoscenersâ€™ great works is valuable for me too, so Iâ€™ll continue to publish my source code.Also, when I do version control on Git, I try to write commit logsâ€”comments you can add to each versionâ€”as detailed as possible. Commit logs explain which part of the code I changed, and they also serve as a kind of production journal. In addition to information like what type of change I made and for what purpose, they help me recall my state of mind or what I was thinking during the creative process.For programmers, is it a hassle to write detailed commit logs?Commit logs arenâ€™t considered a direct contribution to a program, just like READMEs or documentation. So, engineers who want to focus on coding and dislike communicating often donâ€™t write them at all. Usually, detailed commit logs are recommended when you work with other people on business projects. However, even for a one-off piece of code written by a single person, I think we should consider how detailed we make the commit logs, because someone elseâ€”or even yourselfâ€”may end up reading them like archaeology.Archaeologyâ€¦ thatâ€™s interesting.  Okay, let me go to the classic question: your favorite demo, a memorable demo, or a demo that changed your lifeâ€¦ anything. Tell us about a demo, or demos that are special to you.As I mentioned, â€œEmixâ€ by Epoch is the demo I like the most. From the theme of each effect to the color grading, glitch effects, music, and direction, this piece defined what a demo should have, for me. Other pieces that helped define my standards include â€œcdakâ€ by Quite & Orange, â€œTransformer 3â€ by Limp Ninja, and â€œClean Slateâ€ by Conspiracy. I put them together in my Pouet playlist â€œ0b5vrâ€™s bibleâ€, if youâ€™re interested.Among many other forms of self-expression, why did you choose the demoscene? Or are you trapped by this culture? Tell me whatâ€™s so attractive about it.The demoscene is a creative activity free from art as a capital asset or from commercial value. We mostly create and present pieces in a format that has little value in todayâ€™s society, and we purely inspire one anotherâ€™s technical curiosity and the craving for expression. Also, the demoscene ecosystem is cooperative. Anyone can access demotools, ask questions to veterans, and start creating a piece. I respect the works, workflows, and ideas of active demosceners in the community, and thatâ€™s what motivates me to create something that earns their recognition.On the other hand, due to the methods used in the demoscene, a lot of pieces look similar, and thatâ€™s clearly a weak point of the scene. If I only keep exploring the demoscene, I canâ€™t expand my range of expression. As a creator, I think itâ€™s important to look at various cultures and absorb many different methods of expression. The easy exchange of fresh inspiration is one of the features of the demoscene, so Iâ€™d like to take in many forms of expression both inside and outside the scene, and keep inspiring each other.Is there anything you want to do in the future?What I want to do most is live music performance using GLSL, as I mentioned. Seemingly, this format of live music with GLSL is currently performed only by me and â€œRakuto-iceâ€ san. So I want to perform more to develop my style further, and I hope more people will enjoy it.And of course, I want to create demos like 64K, but right now I donâ€™t have enough motivation or ideas. To find more motivation and inspiration, I think itâ€™s about time I formed a demogroup.Sounds like thereâ€™s much to look forward to!  Finally, your message for demosceners and demo fans out there, please.For those of you who are not yet demosceners:  Iâ€™ve seen many people who have an interest in the demoscene but also fears about the culture itself. And itâ€™s not just Japanese people, people in other countries have reacted that way too. Please donâ€™t be afraid of us. If you are interested in creating something with a computer and having fun at a demoparty, then you are a demoscener. Whether you already have a medium of expression or not, if you join the party, you may naturally feel inspired to think, â€œI want to express myself too.â€ Demoparties like Tokyo Demo Fest, SESSIONS, and Revision have various compos, including simple programs, illustration, photography, music, along with the demo compo. Of course, if you want to create a demo, fellow creators will help you. We demosceners hope you will have fun in this scene.For those who are already demosceners (including me):  Make 64K!Thank you very much for answering my question, 0b5vr!0b5vrâ€™s works can be found on Pouet and Demozoo. Also, be sure to check his essay on the production of 0mix on Scrapbox, where he goes deeper into his thoughts on the demoscene and the creative process.Thank you very much for reading this to the end!â€”â€”â€”â€”â€”-In case youâ€™re wondering what â€œdemoâ€ or â€œdemosceneâ€ is, better check outÂ the well-made documentary called Moleman2.Â Â (and the director, M.Â SzilÃ¡rd Matusikâ€™sÂ interview can be read inÂ here.)Â #1: q from nonoil/gorakubu isÂ here.Â  #2: Gargaj from Conspiracy,Â ÃœmlaÃ¼t Design isÂ here.Â  #3: Preacher from Brainstorm, Traction isÂ here.Â  #4: Zavie fromÂ Ctrl-Alt-Test isÂ here.Â  #5: Smash from Fairlight isÂ here.Â  #6: Gloom from Excess, Dead Roman isÂ here.Â  #7: kioku from System K isÂ here.Â  #8: kb from Farbrausch isÂ here.Â  #9: iq from RGBA isÂ here. #10:Â Navis from Andromeda Software Development isÂ here. #11:Â Pixtur fromÂ Still, LKCCÂ isÂ here. #12:Â Cryptic fromÂ ApproximateÂ isÂ here. #13: 0x4015 aka Yosshin isÂ here. #14:Â Flopine from Cookie Collective isÂ here.Â  #15: noby from Epoch, Prismbeings is here.Why Iâ€™m interested in demoscene is explained inÂ this article. And for some of my other posts related to â€œdemo and â€œdemosceneâ€ culture isÂ here.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fil's Unbelievable Garbage Collector]]></title>
            <link>https://fil-c.org/fugc</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133938</guid>
            <description><![CDATA[Fil-C uses a parallel concurrent on-the-fly grey-stack Dijkstra accurate non-moving garbage collector called FUGC (Fil's Unbelievable Garbage Collector). You can find the source code for the collector itself in fugc.c, though be warned, that code cannot possibly work without lots of support logic in the rest of the runtime and in the compiler.]]></description>
            <content:encoded><![CDATA[
        
        
Fil's Unbelievable Garbage Collector

Fil-C uses a parallel concurrent on-the-fly grey-stack Dijkstra accurate non-moving garbage collector called FUGC (Fil's Unbelievable Garbage Collector). You can find the source code for the collector itself in fugc.c, though be warned, that code cannot possibly work without lots of support logic in the rest of the runtime and in the compiler.

Let's break down FUGC's features:


Parallel: marking and sweeping happen in multiple threads, in parallel. The more cores you have, the
faster the collector finishes.
Concurrent: marking and sweeping happen on some threads other than the mutator threads (i.e. your
program's threads). Mutator threads don't have to stop and wait for the collector. The interaction
between the collector thread and mutator threads is mostly non-blocking (locking is only used on
allocation slow paths).
On-the-fly: there is no global stop-the-world, but instead we use
"soft handshakes" (aka "ragged safepoints"). This means that the GC may ask threads to do some work (like scan stack), but threads do this
asynchronously, on their own time, without waiting for the collector or other threads. The only "pause"
threads experience is the callback executed in response to the soft handshake, which does work bounded
by that thread's stack height. That "pause" is usually shorter than the slowest path you might take
through a typical malloc implementation.
Grey-stack: the collector assumes it must rescan thread stacks to fixpoint. That is, GC starts with
a soft handshake to scan stack, and then marks in a loop. If this
loop runs out of work, then FUGC does another soft handshake. If that reveals more objects, then
concurrent marking resumes. This prevents us from having a load barrier (no instrumentation runs
when loading a pointer from the heap into a local variable). Only a store barrier is
necessary, and that barrier is very simple. This fixpoint converges super quickly because all newly
allocated objects during GC are pre-marked.
Dijkstra: storing a pointer field in an object that's in the heap or in a global variable while FUGC
is in its marking phase causes the newly pointed-to object to get marked. This is called a Dijkstra
barrier and it is a kind of store barrier. Due to the grey stack, there is no load barrier like
in the classic Dijkstra collector. The FUGC store
barrier uses a compare-and-swap with relaxed memory ordering on the slowest path (if the GC is running
and the object being stored was not already marked).
Accurate: the GC accurately (aka precisely, aka exactly) finds all pointers to objects, nothing more,
nothing less. llvm::FilPizlonator ensures that the runtime always knows where the root pointers are
on the stack and in globals. The Fil-C runtime has a clever API and Ruby code generator for tracking
pointers in low-level code that interacts with pizlonated code. All objects know where their outgoing
pointers are - they can only be in the InvisiCap auxiliary allocation.
Non-moving: the GC doesn't move objects. This makes concurrency easy to implement and avoids
a lot of synchronization between mutator and collector. However, FUGC will "move" pointers to free
objects (it will repoint the capability pointer to the free singleton so it doesn't have to mark the
freed allocation).


This makes FUGC an advancing wavefront garbage collector. Advancing wavefront means that the
mutator cannot create new work for the collector by modifying the heap. Once an
object is marked, it'll stay marked for that GC cycle. It's also an incremental update collector, since
some objects that would have been live at the start of GC might get freed if they become free during the
collection cycle.

FUGC relies on safepoints, which comprise:


Pollchecks emitted by the compiler. The llvm::FilPizlonator compiler pass emits pollchecks often enough that only a
bounded amount of progress is possible before a pollcheck happens. The fast path of a pollcheck is
just a load-and-branch. The slow path runs a pollcheck callback, which does work for FUGC.
Soft handshakes, which request that a pollcheck callback is run on all threads and then waits for
this to happen.
Enter/exit functionality. This is for allowing threads to block in syscalls or long-running
runtime functions without executing pollchecks. Threads that are in the exited state will have
pollcheck callbacks executed by the collector itself (when it does the soft handshake). The only
way for a Fil-C program to block is either by looping while entered (which means executing a
pollcheck at least once per loop iteration, often more) or by calling into the runtime and then
exiting.


Safepointing is essential for supporting threading (Fil-C supports pthreads just fine) while avoiding
a large class of race conditions. For example, safepointing means that it's safe to load a pointer from
the heap and then use it; the GC cannot possibly delete that memory until the next pollcheck or exit.
So, the compiler and runtime just have to ensure that the pointer becomes tracked for stack scanning at
some point between when it's loaded and when the next pollcheck/exit happens, and only if the pointer is
still live at that point.

The safepointing functionality also supports stop-the-world, which is currently used to implement
fork(2) and for debugging FUGC (if you set the FUGC_STW environment variable to 1 then the
collector will stop the world and this is useful for triaging GC bugs; if the bug reproduces in STW
then it means it's not due to issues with the store barrier). The safepoint infrastructure also allows
safe signal delivery; Fil-C makes it possible to use signal handling in a practical way. Safepointing is
a common feature of virtual machines that support multiple threads and accurate garbage collection,
though usually, they are only used to stop the world rather than to request asynchronous activity from all
threads. See here for a write-up about
how OpenJDK does it. The Fil-C implementation is in filc_runtime.c.

Here's the basic flow of the FUGC collector loop:


Wait for the GC trigger.
Turn on the store barrier, then soft handshake with a no-op callback.
Turn on black allocation (new objects get allocated marked), then soft handshake with a callback
that resets thread-local caches.
Mark global roots.
Soft handshake with a callback that requests stack scan and another reset of thread-local caches.
If all collector mark stacks are empty after this, go to step 7.
Tracing: for each object in the mark stack, mark its outgoing references (which may grow the mark
stack). Do this until the mark stack is empty. Then go to step 5.
Turn off the store barrier and prepare for sweeping, then soft handshake to reset thread-local
caches again.
Perform the sweep. During the sweep, objects are allocated black if they happen to be allocated out
of not-yet-swept pages, or white if they are allocated out of alraedy-swept pages.
Victory! Go back to step 1.


If you're familiar with the literature, FUGC is sort of like the DLG (Doligez-Leroy-Gonthier) collector
(published in two
papers because they
had a serious bug in the first one), except it uses the Dijkstra barrier and a grey stack, which
simplifies everything but isn't as academically pure (FUGC fixpoints, theirs doesn't). I first came
up with the grey-stack Dijkstra approach when working on
Fiji VM's CMR and
Schism garbage collectors. The main
advantage of FUGC over DLG is that it has a simpler (cheaper) store barrier and it's a slightly more
intuitive algorithm. While the fixpoint seems like a disadvantage, in practice it converges after a few
iterations.

Additionally, FUGC relies on a sweeping algorithm based on bitvector SIMD. This makes sweeping insanely
fast compared to marking. This is made thanks to the
Verse heap config
that I added to
libpas. FUGC
typically spends <5% of its time sweeping.

Bonus Features

FUGC supports a most of C-style, Java-style, and JavaScript-style memory management. Let's break down what that means.

Freeing Objects

If you call free, the runtime will flag the object as free and all subsequent accesses to the object will trap. Additionally, FUGC will not scan outgoing references from the object (since they cannot be accessed anymore).

Also, FUGC will redirect all capability pointers (lowers in InvisiCaps jargon) to free objects to point at the free singleton object instead. This allows freed object memory to really be reclaimed.

This means that freeing objects can be used to prevent GC-induced leaks. Surprisingly, a program that works fine with malloc/free (no leaks, no crashes) that gets converted to GC the naive way (malloc allocates from the GC and free is a no-op) may end up leaking due to dangling pointers that the program never accesses. Those dangling pointers will be treated as live by the GC. In FUGC, if you freed those pointers, then FUGC will really kill them.

Finalizers

FUGC supports finalizer queues using the zgc_finq API in stdfil.h. This feature allows you to implement finalizers in the style of Java, except that you get to set up your own finalizer queues and choose which thread processes them.

Weak References

FUGC supports weak references using the zweak API in stdfil.h. Weak references work just like the weak references in Java, except there are no reference queues. Fil-C does not support phantom or soft references.

Weak Maps

FUGC supports weak maps using the zweak_map API in stdfil.h. This API works almost exactly like the JavaScript WeakMap, except that Fil-C's weak maps allow you to iterate all of their elements and get a count of elements.

Conclusion

FUGC allows Fil-C to give the strongest possible guarantees on misuse of free:


Freeing an object and then accessing it is guaranteed to result in a trap. Unlike tag-based approaches, which will trap on use after free until until memory reclamation is forced, FUGC means you will trap even after memory is reclaimed (due to lower repointing to the free singleton).
Freeing an object twice is guaranteed to result in a trap.
Failing to free an object means the object gets reclaimed for you.

        
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Evolving the OCaml Programming Language (2025) [pdf]]]></title>
            <link>https://kcsrk.info/slides/Evolution_Ashoka_2025.pdf</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45133652</guid>
        </item>
        <item>
            <title><![CDATA[What Is the Fourier Transform?]]></title>
            <link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132810</guid>
            <description><![CDATA[Amid the chaos of revolutionary France, one manâ€™s mathematical obsession gave way to a calculation that now underpins much of mathematics and physics. The calculation, called the Fourier transform, decomposes any function into its parts.]]></description>
            <content:encoded><![CDATA[
    As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.
It took mathematicians until the 19th century to master this same calculation.
In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and youâ€™ll get your original function. The technique, today called the Fourier transform, allowed the mathematician â€” previously an ardent proponent of the French revolution â€” to spur a mathematical revolution as well.
Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.
â€œItâ€™s hard to overestimate the influence of Fourier analysis in math,â€ said Leslie Greengard of New York University and the Flatiron Institute. â€œIt touches almost every field of math and physics and chemistry and everything else.â€
Flames of Passion 
Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.

Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 â€” shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British â€” Fourier could think of nothing else.
If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly â€” meaning they have more energy â€” decay first, followed eventually by the lower frequencies. Itâ€™s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.
The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work â€œnothing short of impossible.â€
What troubled his peers most were strange cases where the heat distribution might be sharply irregular â€” like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.
Today, we know that Fourier was broadly right.
â€œYou can represent anything as a sum of these very, very simple oscillations,â€ said Charles Fefferman, a mathematician at Princeton University. â€œItâ€™s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethovenâ€™s Ninth Symphony.â€ The process only fails for the most bizarre functions, like those that oscillate wildly no matter how much you zoom in on them.
So how does the Fourier transform work?
A Well-Trained Ear
Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.
Mathematically, the Fourier transform is a function. It takes a given function â€” which can look complicated â€” as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, youâ€™ll get the original function.

        
            
            Samuel Velasco/Quanta Magazine
        
    

To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Letâ€™s look at a simple example.
Consider the following function:

        
    

The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Hereâ€™s what happens if we multiply the original by a sine wave with a frequency of 3:

        
    

There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.
Now letâ€™s test if the frequency 5 is present. Hereâ€™s what you get when you multiply the original function by a sine wave with the frequency 5:

        
    

There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.
The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)
In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.
If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and â€” despite mathematiciansâ€™ early hesitation to accept such a thing â€” it is now an essential tool in the analysis of functions.

        
    

Encore
The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns â€” and simple combinations of them that resemble checkerboards â€” can be added together to re-create any image.
Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.

        
    

In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly â€”Â aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. â€œItâ€™s now a part of everyday life,â€ Greengard said.
It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics â€” the physics of the very small â€” it even provides the mathematical foundation for the uncertainty principle, which says that itâ€™s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particleâ€™s possible positions; the Fourier transform of that function will describe the particleâ€™s possible momenta. When your function can tell you where a particle will be located with high probability â€” represented by a sharp peak in the graph of the function â€” the Fourier transform will be very spread out. It will be impossible to determine what the particleâ€™s momentum should be. The opposite is also true.
        
        
The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis â€” which studies the Fourier transform, as well as how to reverse it to rebuild the original function â€” is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. Theyâ€™ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.
â€œIf people didnâ€™t know about the Fourier transform, I donâ€™t know what percent of math would then disappear,â€ Fefferman said. â€œBut it would be a big percent.â€
Editorâ€™s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between Quanta Magazine and the Simons Foundation is available here. 
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[io_uring is faster than mmap]]></title>
            <link>https://www.bitflux.ai/blog/memory-is-slow-part2/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132710</guid>
            <description><![CDATA[Sourcing data directly from disk IS faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.]]></description>
            <content:encoded><![CDATA[
                TL;DR
Sourcing data directly from disk IS faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.
Introduction
In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades.  I then asserted, without proof, that sourcing data from disk can be faster than from memory.  What follows is the proof.
Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast.  Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.
Experimental set up
We need data and something straight forward to do with the data.  I used my free will or the illusion thereof to create a benchmark I cleverly call "counting 10s".  I write some pseudo random integers between 0 and 20 to a buffer and then count how many of the integers are 10.  I want to make sure we are doing all the counting in a single thread to simulate an Amdahl's Law situation.
So how fast can we expect this to run?  The upper limit would be the memory bandwidth.
My testing rig is a server with an old AMD EPYC 7551P 32-Core Processor on a Supermicro H11SSL-i and 96GB of DDR4 2133 MHz and a couple of 1.92TB Samsung PM983a PCIe 3.0 SSDs I pieced together from EBay parts.  Given the way this server is configured, the upper limit for memory bandwidth can be calculated as 3 channels * 2133MT/s * 8B/T / 4 numa domains = ~13GB/s for a single thread.  It's kind of an odd system but that just makes it more fun to optimize for!
The disks are rated at 3.1GB/s read BW each for an upper limit of 6.2GB/s.  I made a raid0 volume with 4KB stripe size, formatted the the raid as ext4 with no journaling, and made sure it fully finished initializing the metadata before running the tests.
sudo mdadm --create /dev/md0 --level=0 --raid-devices=2 --chunk=4K /dev/nvme1n1 /dev/nvme2n1
sudo mkfs.ext4 -F -L data -O ^has_journal -E lazy_itable_init=0 /dev/md0
sudo mount -o noatime /dev/md0 mnt

We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.
Simple Loop
The simple and cleanest way to do this in C would look like this.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }

    printf("Found %ld 10s\n", count);
}

Just mmap() the file which will give us a buffer that we can read from.  Then we just loop and count the 10s.
Because the point is to benchmark we will integrate some timing mechanisms before we move on.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <sys/time.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    long start = get_time_us();
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

For the first run we're going to be reading from the disk. The disk/filesystem read is going to limit the performance before the memory bandwidth can.
â¯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 0.61 GB/s

As expected, it's not anywhere near memory speeds because as everyone knows, disk is slow.  We can look at the system and confirm that the first run cached the data to memory.

Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.
â¯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 3.71 GB/s


It is faster, but clearly thatâ€™s slower than the memory can feed it to the processor.  What bottleneck might we be hitting?  This speed does look possibly correlated to the instructions per second limit for this generation of CPU (between 2GHz * 1.5 IPC = 3G and 3GHz boost * 1.5 IPC = 4.5G instructions per second).
We can use perf to see if the CPU is using vector instructions, if not then the actual compute is the bottleneck.
Percentâ”‚      test     %rbp,%rbp
       â”‚    â†“ je       84
       â”‚      lea      (%rbx,%rbp,4),%rcx
       â”‚      mov      %rbx,%rax
       â”‚      xor      %ebp,%ebp
       â”‚      nop
       â”‚70:   xor      %edx,%edx
  1.31 â”‚      cmpl     $0xa,(%rax)
 42.38 â”‚      sete     %dl
 45.72 â”‚      add      $0x4,%rax
  0.01 â”‚      add      %rdx,%rbp
 10.42 â”‚      cmp      %rax,%rcx
  0.16 â”‚    â†‘ jne      70
       â”‚84:   xor      %eax,%eax
       â”‚      shr      $0x14,%r12
       â”‚    â†’ call     get_time_us
       â”‚      pxor     %xmm0,%xmm0
       â”‚      pxor     %xmm1,%xmm1

Confirmed. We're running non-vectorized instructions, with a single thread counting that's as fast as it can go with a 2GHz CPU.  Well crap.  Weâ€™ve hit our first non-exponential limit.  Even a brand new CPU running this machine code would probably struggle to do much better than a 50% improvement, still well below the memory bandwidth limit.
Unrolling the loop
Good news is this code can definitely be vectorized if we help the compiler.  Unroll the loop!
We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <stdint.h>
#include <sys/time.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_unrolled
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    void* buffer = mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    // Get the compiler to align the buffer
    const int * __restrict data = (const int * __restrict)__builtin_assume_aligned(buffer, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    long start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(data[i+ 0] == 10);
        c1  += (unsigned)(data[i+ 1] == 10);
        c2  += (unsigned)(data[i+ 2] == 10);
        c3  += (unsigned)(data[i+ 3] == 10);
        c4  += (unsigned)(data[i+ 4] == 10);
        c5  += (unsigned)(data[i+ 5] == 10);
        c6  += (unsigned)(data[i+ 6] == 10);
        c7  += (unsigned)(data[i+ 7] == 10);
        c8  += (unsigned)(data[i+ 8] == 10);
        c9  += (unsigned)(data[i+ 9] == 10);
        c10 += (unsigned)(data[i+10] == 10);
        c11 += (unsigned)(data[i+11] == 10);
        c12 += (unsigned)(data[i+12] == 10);
        c13 += (unsigned)(data[i+13] == 10);
        c14 += (unsigned)(data[i+14] == 10);
        c15 += (unsigned)(data[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    long elapsed = get_time_us() - start;

    printf("unrolled loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

Check if we now have vectorized instructions with perf.
Percentâ”‚       movq      %xmm0,%rcx
       â”‚       movdqa    %xmm7,%xmm14
       â”‚       pxor      %xmm0,%xmm0
       â”‚       nop
       â”‚ e8:   movdqa    %xmm6,%xmm4
  0.30 â”‚       movdqa    %xmm6,%xmm3
  0.12 â”‚       movdqa    %xmm6,%xmm2
  0.35 â”‚       add       $0x1,%rdx
  1.54 â”‚       pcmpeqd   (%rax),%xmm4
 54.64 â”‚       pcmpeqd   0x10(%rax),%xmm3
  1.62 â”‚       movdqa    %xmm6,%xmm1
  0.99 â”‚       add       $0x40,%rax
  0.12 â”‚       pcmpeqd   -0x20(%rax),%xmm2
  3.03 â”‚       pcmpeqd   -0x10(%rax),%xmm1
  1.32 â”‚       pand      %xmm5,%xmm4
  1.25 â”‚       pand      %xmm5,%xmm3
  1.55 â”‚       movdqa    %xmm4,%xmm15
  0.24 â”‚       punpckhdq %xmm0,%xmm4


Confirmed. We're using 128bit vector instructions, this should be up to 4x faster than the original.

NOTE: These are 128-bit vector instructions, but I expected 256-bit.  I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions.  I forced the compiler to use 256-bit instructions and found it was actually slower.  Looks like the compiler was smart enough to know that here.

Let's benchmark this unrolled version with the data as page cache in memory.
â¯ sudo  ./count_10_unrolled ./mnt/datafile.bin 53687091200
unrolled loop found 167802249 10s processed at 5.51 GB/s


We're still nowhere close to hitting the memory bus speed limit of 13GB/s but 50% faster than the original is a win.  There must be some other bottleneck.
Can the SSDs beat that?
5.51GB/s?  On paper the SSDs can read at 6.2GB/s, but the first run from disk only did 0.61GB/s.  How can I meet or beat this performance sourcing the data directly from disk?
Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk.  When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk.  Unfortunately these legacy mechanisms just aren't set up for serious high performance IO.  Note that at 610MB/s it's faster than what a disk SATA can do.  On the other hand, it only managed 10% of our disk's potential.  Clearly we're going to have to do something else.
SSDs don't just automatically read data at multigigabyte speeds.  You need to put some real effort into an IO pipeline to get serious performance.
I made a io_uring based IO engine, a kind of userspace driver, that can hit these speeds.  The main thread will request data, the IO engine will handle the IO, then the main thread will do the counting when the data is in a buffer.  We will use a set of queues to manage the IO requests, responses, and buffers.  The IO engine will start 6 workers, target a queue depth of 8192, and have a buffer size of 16KB.
I wish I had tighter code here, but A) I didnâ€™t have time to clean it up B) some of the complexity is intractable.  The IO engine code was a lot to scroll through so I moved it to github link
#include "io_engine.h"
#include <sys/mman.h>
#include <getopt.h>
#include <stdio.h>
#include <stdlib.h>
#include <fcntl.h>
#include <sys/mman.h>
#include <stdint.h>
#include <sys/time.h>

#define DEFAULT_WORKERS 6
#define DEFAULT_BLOCK_SIZE 16384
#define DEFAULT_QUEUE_DEPTH 8192

// Count the number of "10" (int format) in the buffer
static inline size_t count_tens_unrolled(void* data, size_t size_bytes) {
    const size_t total = size_bytes / sizeof(int);
    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned(data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    return (t0 + t1) + (t2 + t3);
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);

    // Set up the io engine
    ioengine_t* na = ioengine_alloc(filename, size_bytes, DEFAULT_QUEUE_DEPTH, DEFAULT_BLOCK_SIZE, DEFAULT_WORKERS);

    sleep(1);

    // Use the background workers to read file directly
    size_t total_blocks = na->file_size / na->block_size;
    uint64_t uid = 1;
    size_t count = 0;

    long start = get_time_us();

    // Read all blocks
    size_t blocks_queued = 0;
    size_t blocks_read = 0;
    int buffer_queued = 0;
    while (blocks_read < total_blocks) {
        //// Queue IO phase //////
        //     Do we have more blocks to queue up?
        if (buffer_queued < na->num_io_buffers/2 && blocks_queued <= total_blocks) {
            // Calculate how many blocks on average we want our workers to queue up
            size_t free_buffers = (size_t)(na->num_io_buffers - buffer_queued - 4); // hold back a few buffers
            size_t blocks_remaining = total_blocks - blocks_queued;  // how many blocks have we not queued
            size_t blocks_to_queue = free_buffers > blocks_remaining ? blocks_remaining : free_buffers;
            int blocks_to_queue_per_worker = (int) (blocks_to_queue + na->num_workers - 1) / na->num_workers;
            // Iterate through workers and assign work
            for (int i = 0; i < na->num_workers; i++) {
                worker_thread_data_t* worker = &na->workers[i];
                // Try to queue N blocks to this worker
                for (int j = 0; j < blocks_to_queue_per_worker; j++) {
                    if (blocks_queued == total_blocks) break;
                    int bgio_tail = worker->bgio_tail;
                    int bgio_head = worker->bgio_head;
                    int bgio_next = (bgio_tail + 1) % worker->num_max_bgio;
                    int next_bhead = (worker->buffer_head + 1) % worker->num_max_bgio;
                    if (bgio_next == bgio_head) break;  // queue for send requests is full
                    if (next_bhead == worker->buffer_tail) break; // queue for recieving completed IO is full
                    // Queue this block with the worker.  We have to track which buffer it's going to.
                    int buffer_idx = worker->buffer_start_idx + worker->buffer_head;
                    na->buffer_state[buffer_idx] = BUFFER_PREFETCHING;
                    worker->bgio_uids[bgio_tail] = (uid++)<<16; // unique id helps track IOs in io_uring, we encode 4 bytes later
                    worker->bgio_buffer_idx[bgio_tail] = buffer_idx;
                    worker->bgio_block_idx[bgio_tail] = blocks_queued++;  // block sized index into file
                    worker->bgio_queued[bgio_tail] = -1;  // Requested but not yet queued
                    int next_tail = (bgio_tail + 1) % worker->num_max_bgio;
                    worker->bgio_tail = next_tail;
                    // Log the buffer in an ordered queue for us to read
                    worker->complete_ring[worker->buffer_head] = buffer_idx;
                    worker->buffer_head = next_bhead;
                    buffer_queued++;
                }
                // Tell the worker to submit IOs as a group
                worker->bgio_submit++;
            }
        }

        //// Completion Phase //////
        //     Iterate through worker and check if they have complete IOs
        for (int i = 0; i < na->num_workers; i++) {
            worker_thread_data_t* worker = &na->workers[i];
            int current = worker->buffer_tail;
            // We know what IO's we're waiting on, but we have to poll
            //  to see if they are done.
            for (int scan = 0; scan < worker->num_max_bgio; scan++) {
                // Scan until we get to the end of the list
                if (current == worker->buffer_head) break;
                int buffer_idx = worker->complete_ring[current];
                int state = na->buffer_state[buffer_idx];
                if (state == BUFFER_PREFETCHED) {
                    // This buffer is completed - Process this buffer.
                    count += count_tens_unrolled(na->io_buffers[buffer_idx], na->block_size);
                    na->buffer_state[buffer_idx] = BUFFER_UNUSED;
                    blocks_read++;
                    buffer_queued--;
                }
                current = (current + 1) % worker->num_max_bgio;
            }
            // IO's might have been completed out of order, advance the tail when we can
            current = worker->buffer_tail;
            while (current != worker->buffer_head) {
                int buffer_idx = worker->complete_ring[current];
                int state = na->buffer_state[buffer_idx];
                if (state != BUFFER_UNUSED) break;
                current = (current + 1) % worker->num_max_bgio;
            }
            worker->buffer_tail = current;
            worker->bgio_submit++;  // probably unnecessary
        }
    }
    long elapsed = get_time_us() - start;
    printf("diskbased found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));

    // Cleanup I/O system
    ioengine_free(na);

    return 0;
}

I hope all this extra code makes it faster.
â¯ sudo ./diskbased/benchmark ./mnt/datafile.bin 53687091200
diskbased found 167802249 10s processed at 5.81 GB/s


Boom!  Disk is faster than memory!  It takes several hundred lines of code but now we can source the data from my SSDs faster than the copy from the page cache in memory.
So what's going on here?
Of course my 6GB/s disk stripe isnâ€™t actually faster than the memory bus, even on this weird hack of a system.  So what is happening?  Where is the bottleneck?  It's got to be the way the data is being read from the page cache in memory.
What if we replace the mmap() with a read() from disk into a preallocated buffer.  That way we can measure the counting with the data in-memory without any page cache related overhead mmap() can introduce.
#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdint.h>
#include <string.h>

long get_time_us() {
    struct timeval tv;
    gettimeofday(&tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY|O_DIRECT);
    void *buf;
    posix_memalign(&buf, 4096, size_bytes);
    int *data = buf;

    size_t off = 0;
    while (off < size_bytes) {
        ssize_t n = read(fd, (char*)data + off, size_bytes - off);
        off += (size_t)n;   // YOLO: assume n > 0 until done
    }

    long start = get_time_us();
    for (size_t i = 0; i < total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));


    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned((void*)data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i < total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    elapsed = get_time_us() - start;

    printf("unrolled loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}

If we keep the dataset smaller than a numa domain and we bind this to a single numa node to prevent numa overheads we see that the theoretical memory bandwidth we projected seems to be the primary bottleneck for the unrolled loop as we hoped to see at the outset.
â¯  sudo numactl --cpunodebind=0   ./in_ram mnt/datafile.bin 2147483648
simple loop 6709835 10s processed at 4.76 GB/s
unrolled loop 6709835 10s processed at 13.04 GB/s

But this isn't useful to compare the with the other runs with the 50GB dataset.  However if we do the full 50GB dataset the performance suffers.  We have to get much of the data across numa domains which is going to be higher cost.
â¯ sudo ./in_ram ./mnt/datafile.bin 53687091200
simple loop 167802249 10s processed at 3.76 GB/s
unrolled loop 167802249 10s processed at 7.90 GB/s


Comparing the results of "fully in-memory (50GB)" which is pre-loaded in memory before measuring against the "unrolled loop" that is only cached in memory we see 40% overhead.  That's 2.75 seconds out of 9 seconds that was spent waiting on the caching system instead of counting.  Why so much?
mmap()
The mmap() call presents the process with a buffer that is a blank slate even when the data is already in memory.  The buffer is populated page by page as it's accessed from the page cache.  This isn't a copy, it's just the operating system mapping the cached memory into the process.  This costs more than it might seem.  The worst case with mmap() the counting has to pause at every 4KB page boundary while the kernel processes a fault, tracks down the page of data in the page cache, then updates the page table of the process to insert the memory into the process.  Fundamentally this is a process that is limited by the memory latency, not the CPU speed or memory bandwidth.  With the potential for TLB walks and searching lists that track the page cache, weâ€™re taking potentially dozens of CPU cache misses and several microseconds of waiting on memory for every 4KB page.
direct IO
Using our direct from disk approach uses pipelines and streams which avoids the kind of memory latency dominated bottleneck that mmap() has.  In our case we're limited by the bandwidth of our disks yet because of the pipelining, the larger latency of the IOs doesn't get in the critical path of the counting very much.  Allowing for higher throughput.
Scaling
Consider the implications of these experiments as we scale.  The well vetted solution to get data from memory to a process is slower than using the disk directly.  This isn't because the memory is slower than the disk.  The memory has higher bandwidth than the disk, not by an order of magnitude, but a decent margin.  But the latency of the memory is orders of magnitude lower than the disk.  Nevertheless the way the data in memory is accessed is the culprit.  Its a synchronous approach that assumes memory operations are cheap and low latency.  These accesses add up and it ends up waiting on memory latencies.  The disk method on the other hand is as a streaming approach built to leverage bandwidth and hide latencies.
extending the existing rig
If I got a few more of these disks I could push the IO bandwidth to be greater than the 13GB/s per thread memory bandwidth limit.  IO is DMA'ed to buffers that are pretty small compared to the total dataset. These buffers scale with the throughput capabilities of the CPU and the disks, not the dataset size. The buffers can be located in a single numa domain allowing us to avoid the overhead of accessing the buffers between NUMA domains.  Add more disks to this system I might be able to create a disk based solution to count at the full 13GB/s rather than be limited to the 7.90GB/s we see with the in memory example at the full 50GB dataset.  With such a system our throughput would not be affected by the dataset size, unlike the in-memory case, which has numa overhead and eventually runs out of memory to scale.
faster than memory is possible
On a proper modern server the CPUs will let you do IO directly to the L3 cache, bypassing memory altogether.  Because PCIe bandwidth is higher than memory bandwidth, on paper we could even get more max bandwidth than we can get from memory if we carefully pin the buffers into the CPU cache.  I haven't confirm this works in practice, however, it could be made to work and is the sort of thing that CPU designs will be forced to lean into to push performance forward.
memory is changing too
This isn't just about disks vs memory.  Similar techniques and principles apply to memory.  Memory bandwidth is still scaling even if the latency is not.  This means to take advantage of memory performance you have to actually treat it more like a disk and less like Random Access Memory.  To scale performance with generational updates you have to make sure to stream data from memory into the CPU caches in blocks, similar to how data is streamed from disk to memory.  If not you end up with 90s level memory throughput.  A custom mechanism to cache data in memory could easily avoid the memory latency problems seen with the default mmap() solution with much less code than the io_uring solution.
Is this worth it?
I'm not going to say that going to the effort of implementing something like this is always worth it.  The mmap() method is sure elegant from a coding perspective, especially when compared to all the code I had to write to get the io_uring setup working.  Sometimes the simple way is the way to go.
Is using 6 cores of IO for 1 core of compute is always the right answer?  Probably not.  This was an extreme situation to prove a point.  In realworld situations you'll need to look at the tradeoffs and decide what's best for your use case.  Correctly understanding the strengths and weaknesses of the hardware can open up a number of possibilities where you can get a lot more performance for a lot less money.
The kind of overhead demonstrated with mmap() isnâ€™t going to go away, new hardware isn't going to fix it.  At the same time disk bandwidth and the number of cores are scaling each generation.  But doing things that scale performance with new technology is going to take extra code and effort.
But don't just blow this stuff off.  Sure you can dedicate a server with 3TB of memory to serve 10K client connections. Memory in the cloud is like ~$5/GB/month, if you can afford it, then you do you.  However it is worth considering that humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app.  I figure either the karmic debt to the planet, or a vengeful AI demigod hungry for silicon and electricity will come for those that don't heed this warning, eventually.  Either way my conscience is clear.
Recap

Memory is slow - when you use it oldschool.
Disk is fast - when you are clever with it.
Test the dogma - compounded exponentials are flipping somethings from true to false.

Bad news is that this cleverness requires extra code and effort.
Good news is we now have AI to write and test the extra code this cleverness requires.
Better news is that, for those that are willing to learn, AI's don't do this unless you know how to ask them.
Lean into things that scale, avoid things that donâ€™t.
Next Time
What will be revealed in the next episode?

Is O(âˆšn) actually faster than O(log n)?  Will the foundations of Computer Science survive this unveiling?
Will traditional code be consumed into the latent space of our AI overlords?
Is AI hiding these performance gains from me?  Is AI even capable of writing optimized code?


Jared Hulbert

A few notes for the "um actually" haters commenting on Hacker News:

This is not and does not claim to be an academic paper.
I do not intend to prove that NAND is a drop in replacement for DRAM.
Tis but a humble and hopefully fun exercise in exploring the limits and trends of modern hardware and the tradeoffs needed to maximize performance.
As I stated before I have no problem with your choice to ignore this and write lazy code that will perform just as fast on new hardware in 15 years as it does on todays hardware.  In fact I applaud your choice.  Jeff Bezos has an orbital yacht to build, someone has to pay for it, why not you?
I am not an AI.  I am a human with a computer that don't write perfect.



source code can be found here.


            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite?]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45132498</guid>
            <content:encoded><![CDATA[





Small. Fast. Reliable.Choose any three.











Introduction

Suppose the
OpenDocument file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:

Smaller documents
Faster File/Save times
Faster startup times
Less memory used
Document versioning
A better user experience



Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

About OpenDocument And OpenDocument Presentation


The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
NeoOffice on Mac, or
LibreOffice on Linux and Windows.


An OpenDocument Presentation or "ODP" file is a
ZIP archive containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
SouthEast LinuxFest
conference:

Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
... 58 other pictures omitted...
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files



The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

application/vnd.oasis.opendocument.presentation


The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

Limitations Of The OpenDocument Presentation Format


The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:


Incremental update is hard.

It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

Startup is slow.

In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

More memory is required.

Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.


One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentations) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.


Crash recovery is difficult.

The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

Content is inaccessible.

One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

<?xml version="1.0" encoding="UTF-8"?>


The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.


First Improvement:  Replace ZIP with SQLite


Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);



For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.




Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
SQLAR utility:

-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp



The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.


The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
SQLite databases are atomic.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

Second Improvement:  Split content into smaller pieces


A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
of all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.


So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional


The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

SELECT slideContent FROM slide WHERE pageNumber=1;


This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order to render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facilitate rapid transitions to the next slide.


Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just a
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.


Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.


One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

Third Improvement:  Versioning


Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);



In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.


On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;



Or perhaps the application would rather use the
most recent checkinTime:

SELECT manifest, versionId, max(checkinTime) FROM version;



Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

Or, multiple presentations could be stored within the same document.

With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.


There are details to fill in here.
Perhaps a screen can be provided that displays all historical changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

And So Forth...


In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:

 Store an automated undo/redo stack in a database table so that
     Undo could go back into prior edit sessions.
 Add full text search capabilities to the slide deck, or across
     multiple slide decks.
 Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
 Break out the "Presenter Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
 Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.



An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.


Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
I/O performance is better than using separate
files.


Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into
Third Normal Form (3NF)
and store only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

Review Of The Benefits Of Using SQLite


In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:



An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.


The atomic update capabilities
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.


Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.


The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.


The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.



These are just a few of the benefits of using SQLite as an application file
format â€” the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the Application File Format
document for additional ideas.


Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.
This page last modified on  2025-05-12 11:56:41 UTC 

]]></content:encoded>
        </item>
    </channel>
</rss>