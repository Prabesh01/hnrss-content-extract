<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Fri, 29 Aug 2025 08:12:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[The Synology End Game]]></title>
            <link>https://lowendbox.com/blog/they-used-to-be-good-but-now-theyve-turned-to-evil-the-synology-end-game/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060920</guid>
            <description><![CDATA[Synology has recently implemented some pretty user-hostile policies, which means my long-term love affair with them is going to end in heartbreak.]]></description>
            <content:encoded><![CDATA[I’ve been a Synology fan for many years.  I used to roll my own NAS servers for home, but eventually decided that quieter, more energy-friendly dedicated NAS solutions were a better path forward.  I don’t use a lot of their on-board apps, just basic file storage.Right now I’ve got a DS920, a DS418, and a DS1522…but I probably won’t be buying another Synology again.Why?Their abusive, customer-hostile policies.Samba LimitsI started getting queasy when I read earlier this year that on some models, they limit how many concurrent connections you can make.  I though this was just something setup by default in smb.conf, but in fact Synology has a proprietary wrapper around the daemon that artificially limits it.Whiskey.  Tango.  Foxtrot.You Must Buy Your Hard Drives From UsFor a long time, Synology has only officially supported certain hard drives.  I don’t have a problem with this, for three reasons.  First, it was a pretty extensive list and included all the major players (WD, Seagate, etc.).  Second, it’s unreasonable to expect Synology to certify every single hard drive from every maker on the planet.  And finally, it was just a support limit.  In other words, you could use whatever hard drives you wanted, but if there was a problem, they wouldn’t be able to support you if the drive wasn’t on their list.I could live with that.  What I can’t live with is the new policy, implemented this year, where you must buy your drives from Synology.  This only affects new models from this year forward.  Details still seem sketchy, but rumor is that it’s going to be along the lines of “we don’t recognize your WD Black hard drive, therefore we won’t use it.”And by the way, Synology’s hard drives aren’t all that great.  My WD Blacks come with a 5 year warranty.  Synology’s only come with 3 years.Golf.  Foxtrot.  Yankee.Where to Now?I could go back to building my own, with TrueNAS.  In the past, my home-build NAS boxes were hand-me-down gaming PCs (because they were big enough towers) but I have to imagine one can find a case that allows tons of drives and is still powered by something modest.Or I may look at UGREEN.  Or Buffalo.  Or someone else.Raindog308 is a longtime LowEndTalk community administrator, technical writer, and self-described techno polymath. With deep roots in the *nix world, he has a passion for systems both modern and vintage, ranging from Unix, Perl, Python, and Golang to shell scripting and mainframe-era operating systems like MVS. He’s equally comfortable with relational database systems, having spent years working with Oracle, PostgreSQL, and MySQL.As an avid user of LowEndBox providers, Raindog runs an empire of LEBs, from tiny boxes for VPNs, to mid-sized instances for application hosting, and heavyweight servers for data storage and complex databases. He brings both technical rigor and real-world experience to every piece he writes.Beyond the command line, Raindog is a lover of German Shepherds, high-quality knives, target shooting, theology, tabletop RPGs, and hiking in deep, quiet forests.His goal with every article is to help users, from beginners to seasoned sysadmins, get more value, performance, and enjoyment out of their infrastructure.You can find him daily in the forums at LowEndTalk under the handle @raindog308.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Strange CW Keys]]></title>
            <link>https://sites.google.com/site/oh6dccw/strangecwkeys</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060161</guid>
            <description><![CDATA[Made by OH6DC
You can also use the Text-only index page (divided into useful categories).]]></description>
            <content:encoded><![CDATA[HomeStrange CW KeysMade by OH6DCYou can also use the Text-only index page (divided into useful categories).Lever arch file CW key Lambic pedals Valentine's day lollipop CW paddle Rubber stamp CW key Letter scale CW key Clamp cootie Code book Pepper mill CW key Lightsaber CW key Nutcracker CW key Straight(ener) key Smoke alarm CW key Teletubbygraph key Soap dispenser CW key Vinyl record player CW key Moomin triangle CW key Antiperspirant roll-on CW key Dual banana CW paddle Power twister CW key Power twister CW key Handsaw CW key Hole punch CW key Watering can CW key Toilet brush CW key CW glove Remote control CW key Tea bag CW key Eyebrow-raising CW key with optical transmitter Back scratcher CW key Whisk CW key Pliers CW key Liver casserole CW key Licorice pipe CW key Chocolate CW key Ski-W key Power drill CW keyer Six megapixel CW key Suspenders CW key Spirit bottle cap CW key Speed skate CW key Flower CW key Knee pad sideswiper CW key for portable operation QRP transmitter powered by a CW key Alarm clock CW key Hammer CW key CW gun Nail clipper CW key Ballpoint pen CW key Rotary dial CW key Hammock CW key Joystick CW key Rowing boat CW key Guitar CW key Wallet CW key Radio controlled CW key Amaryllis telegraphiensis Multi-function knife with CW key Toilet paper roll CW key Table ice hockey CW key Big toe CW key Waffle iron CW key Lego straight key Lego bug Pogo stick CW key Crutch CW key Smoke signal CW key CCW key Necktie CW key Toothbrush CW key Bench press CW key Handshake CW key Chopsticks CW key Trailer hitch CW key Typewriter CW keyboard Refrigerator CW key Mobile phone CW key Paper cup iambic paddles Morsetrap CW key Fingertips CW key Vacuum cleaner semi-automatic CW key Banana CW key Rolling pin CW key Toaster CW key Cheese slicer CW key Rocking chair CW key QLF pedal for left foot CW Cross-country ski shoe CW key CW insoles QRQ paddles Onion chopper CW key Beer can CW key Egg slicer CW key Stapler CW key Bicycle pump CW key Iron bar CW key Homebrew semi-automatic bug Hacksaw blade sideswiper CW key Plywood CW key Home  |  Homebrew QRP  Page updated Google SitesReport abuse]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PSA: Libxslt is unmaintained and has 5 unpatched security bugs]]></title>
            <link>https://vuxml.freebsd.org/freebsd/b0a3466f-5efc-11f0-ae84-99047d0a6bcc.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45060004</guid>
            <description><![CDATA[Alan Coopersmith reports:]]></description>
            <content:encoded><![CDATA[
Alan Coopersmith reports:

	  On 6/16/25 15:12, Alan Coopersmith wrote:
	  
	    BTW, users of libxml2 may also be using its sibling project, libxslt,
	    which currently has no active maintainer, but has three unfixed security issues
	    reported against it according to
	    
		https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt
	  
	  2 of the 3 have now been disclosed:
	  (CVE-2025-7424) libxslt: Type confusion in xmlNode.psvi between stylesheet and source nodes
	    https://gitlab.gnome.org/GNOME/libxslt/-/issues/139
	    https://project-zero.issues.chromium.org/issues/409761909
	  (CVE-2025-7425) libxslt: heap-use-after-free in xmlFreeID caused by `atype` corruption
	    https://gitlab.gnome.org/GNOME/libxslt/-/issues/140https://project-zero.issues.chromium.org/issues/410569369
	  Engineers from Apple & Google have proposed patches in the GNOME gitlab issues,
	  but neither has had a fix applied to the git repo since there is currently no
	    maintainer for libxslt.
	

Note that a fourth vulnerability was reported on June 18, 2025, which remains undisclosed to date (GNOME libxslt issue 148, link below), see
	  
	    https://gitlab.gnome.org/Teams/Releng/security/-/wikis/2025#libxml2-and-libxslt
	
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A deep dive into Debian 13 /tmp: What's new, and what to do if you don't like it]]></title>
            <link>https://lowendbox.com/blog/a-deep-dive-into-debian-13s-tmp-whats-new-and-what-to-do-if-you-dont-like-it/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45059470</guid>
            <description><![CDATA[Debian 13 "Trixie" comes with a whole new world of tmpfs, which is memory-based, and an automated cleanup process.  Let's take a look.]]></description>
            <content:encoded><![CDATA[Debian 13 “Trixie” introduces an important change to /tmp.  Traditionally, it’s been just another filesystem, albeit with some special permissions that allows everyone on the system to use it without being able to remove each other’s files.In Trixie, it’s been moved off the disk into memory – specifically a type of memory called tmpfs.  To quote the tmpfs man page:The  tmpfs  facility  allows  the  creation of filesystems whose contents reside in virtual memory. Since the files on such filesystems typically reside in RAM, file access is extremely fast.They’re also extremely temporary…which is what you really want.  There’s an old story about a user who was assigned to work on the Transportation Management Project.  He logged into the server where he was supposed to store his work, saw the /tmp directory, found he could upload files there, and happily spent a couple months putting all his work there.  Alas, when the server was rebooted…Now that is undoubtedly an urban legend, but it illustrates the true nature of /tmp.  It’s fine if you need a disposable log fine, a PHP session file, space for sorting something, etc.  But you shouldn’t be storing anything there.This isn’t a new thing in the Linux world.  RedHat and its ilk have used tmps for /tmp for some time.A more serious problem than people losing files is people who use too much /tmp.  The system needs /tmp to do basic functions, so if it hits 100%, things will break.  It’s really easy to think “I’m going to download and untar this big zip file into /tmp, and then I’ll remove it after I pull out the one file I need”…and forget to remove it.  Now you’re hogging /tmp and over time, /tmp can be filled up with junk.Debian 13’s tmpfs Comes With…Challenges.  And SolutionsNow instead of filling up disk, you’re filling up memory.  If you download a 300MB .zip file, expand it to 1GB, and forget it, now you’re chewing up 1GB of RAM.  Ouch.There are two mitigating factors.  First, by default, Debian will only allocate a maximum of 50% of RAM to the tmpfs for /tmp.  You can change this.  To do so, typesystemctl edit tmp.mountYou’ll be popped into your editor (controlled by the EDITOR environment variable) with a form to update the settings.  At the very bottom you’ll see a template, which you can copy and edit:# [Mount]
# What=tmpfs
# Where=/tmp
# Type=tmpfs
# Options=mode=1777,strictatime,nosuid,nodev,size=50%%,nr_inodes=1m
Go back up to the part before the line “Edits below this comment will be discarded” and paste in something like this:[Mount]
What=tmpfs
Where=/tmp
Type=tmpfs 
Options=mode=1777,strictatime,nosuid,nodev,size=25%%,nr_inodes=1mto change it to 25% or if you want a number:[Mount]
What=tmpfs
Where=/tmp
Type=tmpfs 
Options=mode=1777,strictatime,nosuid,nodev,size=1G,nr_inodes=1mto change it to 1GB.For example, I have a Debian 13 VPS with 4GB of RAM.  After a fresh install, I see it’s using 2GB max for tmpfs:# findmnt --target /tmp
TARGET SOURCE FSTYPE OPTIONS
/tmp   tmpfs  tmpfs  rw,nosuid,nodev,size=2007704k,nr_inodes=1048576,inode64Note that this is a maximum.  If there’s nothing in /tmp, /tmp does not use any memory.After doing the systemctl edit, like this:I get the message:Before this change, /tmp was at 2GB (half of the 4GB RAM):Now, after reloading systemd and restarting tmp.mount, I see /tmp is limited to 1GB:CleanupThe second mitigating factor is that /tmp is now automatically cleaned up.  Quoting the release notes:The new default behavior is for files in /tmp to be automatically deleted after 10 days from the time they were last used (as well as after a reboot). Files in /var/tmp are deleted after 30 days (but not deleted after a reboot).You can modify these policies, exclude certain files (why?  they’re temporary!), or even apply it to other directories.  Consult the fine manual but I think for 99% of people, the defaults are just fine.  I might be tempted to make the cleanup a little more aggressive, like 3 days.Thinking in a LowEnd ContextOne concern is for very low-memory systems.  While 1GB has become the smallest VM for a lot of people, 512s are still sold.  Allowing /tmp to consume 256MB out of 512 (which is really only 470-480 after the kernel and vital system processes are loaded) is a lot more impactful than consuming 256MB on a 10GB or 20GB filesystem.Fortunately, opting out of the new tmpfs world is easy if you don’t like it:systemctl mask tmp.mountand reboot.  I did that on the test box above:Now I can put 17GB of junk there.  Fortunately, it will be cleaned up as described above.So how are you planning to handle Debian 13’s new tmpfs-based /tmp?Raindog308 is a longtime LowEndTalk community administrator, technical writer, and self-described techno polymath. With deep roots in the *nix world, he has a passion for systems both modern and vintage, ranging from Unix, Perl, Python, and Golang to shell scripting and mainframe-era operating systems like MVS. He’s equally comfortable with relational database systems, having spent years working with Oracle, PostgreSQL, and MySQL.As an avid user of LowEndBox providers, Raindog runs an empire of LEBs, from tiny boxes for VPNs, to mid-sized instances for application hosting, and heavyweight servers for data storage and complex databases. He brings both technical rigor and real-world experience to every piece he writes.Beyond the command line, Raindog is a lover of German Shepherds, high-quality knives, target shooting, theology, tabletop RPGs, and hiking in deep, quiet forests.His goal with every article is to help users, from beginners to seasoned sysadmins, get more value, performance, and enjoyment out of their infrastructure.You can find him daily in the forums at LowEndTalk under the handle @raindog308.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lucky 13: a look at Debian trixie]]></title>
            <link>https://lwn.net/Articles/1033474/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45059160</guid>
            <description><![CDATA[After more than two years of development, the Debian Project has released its new stable versio [...]]]></description>
            <content:encoded><![CDATA[

We're bad at marketing

We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please subscribe today to help us keep doing that, and so
we don’t have to get good at marketing.


After more than two years of development, the Debian Project has released its new stable version, Debian 13 ("trixie"). The release comes with the usual bounty of
upgraded packages and more than 14,000 new packages; it also debuts Advanced Package Tool
(APT) 3.0 as the default package manager and makes 64-bit
RISC-V a supported architecture. There are few surprises with trixie,
which is exactly what many Linux users are hoping for—a free
operating system that just works as expected.

Debian's stable
releases are aptly named; the project prioritizes stability over
shipping the latest software. The freeze
schedule for trixie called for a soft freeze in April, which meant
that (for example) the KDE Plasma 6.4
release in June was too late to make the cut—even though trixie
was not released until August. Users who prefer to live on the edge
will want to run another distribution or follow Debian development by
running the testing release
that previews the next stable version—Debian 14 ("forky"). Truly
adventurous users may take their chances with the unstable ("sid")
release.

That said, trixie is up-to-date enough for many folks; it includes
GNOME 48, KDE Plasma 6.3, Xfce 4.20, GNU
Emacs 30.1, GnuPG 2.4.7, LibreOffice 25.2, and
more. Under the hood, it includes the most recent Linux LTS kernel
(6.12.41), GNU Compiler Collection (GCC) 14.2, GNU C Library (glibc)
2.41, LLVM/Clang 19, Python 3.13, Rust 1.85, and
systemd 257. The release notes have a section
for well-known software that compares the version in Debian 12
against Debian 13. While some of the versions lag a bit behind the
upstream, they are not woefully outdated.

The project now supports
six major hardware architectures: x86-64/amd64, 32-bit Arm with a
hardware FPU (armhf), 64-bit Arm (arm64), IBM POWER8 or newer
(ppc64el), IBM S/390 (s390x), and 64-bit RISC-V. The i386 architecture
is not supported for trixie, though the project continues to
build some i386 packages to run on 64-bit systems; users with i386 systems cannot upgrade to
trixie. The MIPS
architectures (mipsel and mis64el) have also been removed in trixie.

The Arm EABI
(armel) port that targets older 32-bit Arm devices prior to Arm v7 is
still supported with trixie, but this release is the end of the
line. There is no installation media for armel systems, but users who
have bookworm installed can upgrade to trixie if they have supported
hardware: the Raspberry Pi 1, Zero, and Zero W are the
only devices mentioned in
the release notes.

Upgrades from bookworm are supported, of course. The release
notes suggest that users convert APT source files to the DEB822 format
before the upgrade. APT 3.0
includes an "apt modernize-sources" command to convert APT data
source files to DEB822, but that is not available in bookworm. Users are
also expected to remove
all third-party packages prior to running the upgrade. I tested
the upgrade on one of my servers, after taking a snapshot to roll back
to if needed, and all went smoothly. Users who are considering an
upgrade should read the release notes carefully before forging ahead;
in particular, users should be aware that it's possible (but not
certain) for network interface names to change on upgrade.

Installation

For users who want to start fresh, Debian offers a
variety of installer images and download methods; users can choose
a 64MB minimal ISO image with the netboot
installer, all the way up to a set of Blu-ray images. The project
recommends using BitTorrent or Jigsaw
Download (jigdo) for the largest images. BitTorrent probably needs
no introduction, but jigdo is not as well-known. Jigdo is a method of
downloading all of the individual packages for an image from multiple
mirrors and then assembling them into an ISO image on the user's
machine. It was a bit fiddly to use jigdo to download an image, but
not overly so—and the speed of the whole process was comparable
to simply downloading an ISO of the same size.

Debian's network
install ("netinst") image is probably the best option for server
installations and for experienced Linux users; it includes the
packages required for a base install and then fetches the remaining
software from Debian mirrors. Unlike the tiny netboot image, it
includes the option of using either the graphical installer or the
text-based installer.

The installer is a bit of a throwback to an earlier era when users
were expected to know a lot more about the workings of a Linux system. 
Users who have only worked with distributions like Fedora and Ubuntu
will notice that installing Debian requires many more steps than other
popular distributions. For example, many desktop distributions have
eliminated the step of setting a password for the root
user—instead, it is generally assumed that the primary user will
also be the system administrator, so the default is to give the
primary user sudo privileges instead. Debian does not take that
approach; in fact, there is no way to give a user sudo privileges
during installation. Setting up sudo has to be done manually after
the installation is completed Update: Users can skip creation of a root account and the installer will then set up the regular user as an administrator with sudo permissions. Apologies for the error.

For some folks, installing Debian will be a bit of a chore and may
even be confusing for users who are new to Linux. For example, the
text-mode installer requires users to specify the device for GRUB boot
loader installation, without providing a default. If one chooses an
invalid partition, the installer tells the user that the operation has
failed and drops back to a menu listing all the installation
steps. Presumably if one picks the wrong partition it will
happily install GRUB to that and render the system unbootable. This is
not insurmountable for experienced Linux users, but it would no doubt
be a hurdle for many users.

More experienced Linux users are likely to appreciate the
amount of control offered by the installer. For example, Fedora's
recent web-based installer makes it difficult to even find the option to
perform custom partitioning. Debian has a guided partitioning option
for those who do not want to fuss with it, but the option to create
custom partitions is not hidden from the user.

Debian has a better installation option for newer Linux users,
though it is easy to miss: the live install images, which
use the Calamares installer. Its
workflow is more akin to the installation process one finds with
Fedora and Ubuntu; it also sets up the primary user with sudo
privileges rather than creating a root password. Unfortunately,
the live images are not listed on the main page for installer
images—though they are mentioned, briefly, in the release
notes.







The Debian installer also has the option of using a Braille display
and/or speech synthesizer voice for the installation. I have not tried
these options, but they are available for users who need them.

X.org

Many distributions are in the process of phasing out X.org support
for GNOME and KDE as the upstream projects have started doing so.
For example, Fedora will remove X.org session support
for GNOME in Fedora 43, and the plan is for Ubuntu to do the same
in its upcoming 25.10 release. GNOME will be completely removing X.org
support in GNOME 49, which is planned for September.

Much has already been said about this, of course, and there is
likely little new left to be said or that needs to be
said. However, for users who still need or want X.org support,
Debian 13 includes X.org sessions for GNOME and KDE. In testing
trixie, I've spent some time in the GNOME and KDE X.org sessions as
well as the Wayland sessions; if there are any gotchas or horrible
bugs, I haven't encountered them (yet). This might be a compelling
reason for some folks to switch to (or stick with) Debian.

Trying trixie

I use Debian for my personal web site and blogs, but it has been
quite some time since I used it as my primary desktop operating
system. Debian (and Ubuntu) derivatives, such as Linux Mint and Pop!_OS, yes—but it's been
several years since I've used vanilla Debian on the desktop for
more than casual tinkering.

The Debian release announcement boasts about the number of packages
included in trixie: 64,419 packages total, with 14,100 added and more
than 6,000 removed as obsolete
since bookworm. That is quite a few packages, but falls short of some
other distributions. For example, "dnf repoquery --repo=fedora
--available" shows more than 76,000 packages available for
Fedora 42.

After installing Debian, I went to install some of my preferred
software, such as aerc,
Ghostty, niri, and Speech Note. The aerc
packages in trixie are current, but Ghostty and niri are not packaged
for Debian at all. Ghostty is written in Zig, which is also not
available, so users who want to build it from source will need to
install Zig separately and then build Ghostty. Speech Note is packaged
as a Flatpak, but Debian does not enable Flatpaks or Flathub in the
GNOME Software Store by default. Users who want Flatpaks on Debian via
Flathub will need to install the flatpak package and manually
add the Flathub repo:

    flatpak remote-add --if-not-exists flathub \
      https://dl.flathub.org/repo/flathub.flatpakrepo


Users will need to add the gnome-software-plugin-flatpak
package for Flatpak support in GNOME Software, and
plasma-discover-backend-flatpak to add it to
KDE Discover.

Trixie ships with the Firefox extended-support release (ESR) by
default: Firefox
128, which was released in July 2024. Happily,
Mozilla offers a Debian
repository for those who want to run more current versions. Even
better, there is a little-advertised utility called extrepo that
has a curated list of external repositories users might want to enable
for Debian. To enable the Mozilla repository, for example, a user only
needs to install extrepo, run
"extrepo enable mozilla" as root (or with
sudo), update the package cache, and look for the regular
Firefox package. In all, extrepo includes more than 160 external
repositories for applications like Docker CE, Signal, and Syncthing. Unfortunately, the
extrepo utility does not have a separate "list" command to show the
available repositories, though running "extrepo search"
with no search parameter will return all of its DEB822-formatted
repository entries. Some of the software is
in an external repository due to a non-free license, other software (like
Firefox) just has a development cycle that outpaces Debian's.

As one might expect, the Debian desktop experience is not
dramatically different from other distributions; GNOME 48 on
Debian is little different than GNOME 48 on Fedora, and the same
is true for KDE, Xfce, etc. The primary difference is that users can
expect more or less the same desktop experience running Debian stable
in two years that they have today, which is not necessarily true for
other distributions.

Miscellaneous

One of the features in Debian 13 is something that most users
won't notice or appreciate at all: a transition to
64-bit time_t on 32-bit architectures, to avoid the Year 2038 problem. The
short version is that 32-bit integers cannot hold a Unix epoch
timestamp for dates after January 19, 2038. That may seem
like a distant concern, even irrelevant for Debian trixie; after all,
Debian 13 is only supported by the project until 2030. However,
the project expects that some 32-bit embedded systems will still be running
trixie in 2038, so Debian developers did the heavy lifting to complete
the transition to 64-bit time_t now. LWN covered the early planning
for this in 2023.

By now, most users have retired their DSA
SSH keys; if not, now is the time to do so. DSA keys were disabled by
default with OpenSSH in 2015, and they are entirely disabled now with
the openssh-client and openssh-server packages in
trixie. If there is a device that can, for some reason, only be
connected to with DSA, users can install the
openssh-client-ssh1 package and use ssh1 to make the
connection.

As we covered in
June 2024, Debian 13 has switched to using a tmpfs
filesystem for the /tmp directory. By default, Debian
allocates up to 50% of memory to /tmp, but this can be
changed by following the instructions
in the release notes. Note that this also applies to systems that
are upgraded to trixie from bookworm.

Forward to forky

Debian Project Leader (DPL) Andreas Tille recently
announced "Debian's 100000th birthday", so clearly the project has a
bit of experience with putting out solid releases. Granted, he was
reporting the number in binary, but even when converted to decimal 
numbers (32 years), it's an impressive track record.

While testing, I installed trixie on a couple of systems, including
a new Framework 12-inch laptop. My original intent was to just see
whether Debian had any problems with the new hardware (it didn't), but
now I'm leaning toward sticking with Debian on this system for a while
to see if stability suits me.

With trixie out the door, the Debian Project has already turned its
attention to working on forky, which has no release date set. Debian has
stuck to a loose schedule of a new stable release roughly every two
years. Most likely we will see Debian 14 sometime in 2027. After
the forky release, trixie will still receive updates from Debian's
security team through 2028, and then from its LTS team through 2030.

As of yet, there are no major new features or changes announced for
forky; it seems likely that those will be coming to light in the
coming months now that the project has trixie out the door. LWN will,
of course, be reporting on those developments as they happen.


            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Claude Sonnet will ship in Xcode]]></title>
            <link>https://developer.apple.com/documentation/xcode-release-notes/xcode-26-release-notes</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058688</guid>
        </item>
        <item>
            <title><![CDATA[Python: The Documentary [video]]]></title>
            <link>https://www.youtube.com/watch?v=GfH4QL4VqJ0</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058171</guid>
        </item>
        <item>
            <title><![CDATA[RSS is awesome]]></title>
            <link>https://evanverma.com/rss-is-awesome</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45058024</guid>
            <description><![CDATA[Evan briefly discusses rss]]></description>
            <content:encoded><![CDATA[ 
    
      NetNewsWire
    
  
    
       is my latest most-used iPhone app. It is a simple, free RSS reader. 
    
  
    
      RSS is an old technology that it seems most people have forgotten about. Here's how it works: you enter a link to an RSS "feed", and your app pulls data from this feed every few minutes or so. When there is a new post from your feed, that post is pulled directly to your app. 
    
  
    
      RSS is really simple, so it is still very well supported. Notably, all substack publications automatically have an RSS feed included at 
    
  https://{{substack-domain}}/feed
    
      . 
    
  
    
      Blogs are great but I don't enjoy reading posts in my email, having to remember the websites each one is hosted at, or reading from each publications' different typesetting opinions with varying pop-ups and advertisements. An RSS reader centralizes all content from your blogs into a single place for reading. 
    
  
    
      Since I started using this app I spend much more of my "mindless phone time" reading blog posts, which I think is somewhere between marginally good and good. 
    
   ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rupert's Property]]></title>
            <link>https://johncarlosbaez.wordpress.com/2025/08/28/a-polyhedron-without-ruperts-property/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057561</guid>
            <description><![CDATA[You can cut a hole in a cube that’s big enough to slide an identical cube through that hole! Think about that for a minute—it’s kind of weird. Amazingly, nobody could prove any co…]]></description>
            <content:encoded><![CDATA[
				



You can cut a hole in a cube that’s big enough to slide an identical cube through that hole!   Think about that for a minute—it’s kind of weird.
Amazingly, nobody could prove any convex polyhedron doesn’t have this property!  It’s called ‘Rupert’s property’.
Until this week.
This week Steininger and Yurkevich proved there is a convex polyhedron that you can’t cut a hole in big enough to slide the entire polyhedron through the hole.  It has 90 vertices, and apparently 240 edges and 152 faces.




To prove that no such hole is possible, they had to do a computer search of 18 million different holes, plus use a lot of extra math to make sure they’d checked enough possibilities:
• Jakob Steininger and Sergey Yurkevich, A convex polyhedron without Rupert’s property.
To celebrate their discovery, they gave this polyhedron a silly name.  Since this polyhedron lacks Rupert’s property, they called it a ‘noperthedron’.
Why is this property called ‘Rupert’s property’?  Wikipedia explains:

In geometry, Prince Rupert’s cube is the largest cube that can pass through a hole cut through a unit cube without splitting it into separate pieces. Its side length is approximately 1.06, 6% larger than the side length 1 of the unit cube through which it passes. The problem of finding the largest square that lies entirely within a unit cube is closely related, and has the same solution.
Prince Rupert’s cube is named after Prince Rupert of the Rhine, who asked whether a cube could be passed through a hole made in another cube of the same size without splitting the cube into two pieces. A positive answer was given by John Wallis. Approximately 100 years later, Pieter Nieuwland found the largest possible cube that can pass through a hole in a unit cube.

Here Greg Egan shows how Rupert’s property works for the cube:


Here he shows how it works for the regular octahedron:


And finally, here’s a video by David Renshaw showing 26 polyhedra with Rupert’s property… and 5 polyhedra that might lack it:




The triakis tetrahedron is an extremely close call, but it does have Rupert’s property:




				
				
					
					This entry was posted  on Thursday, August 28th, 2025 at 7:50 am and is filed under mathematics.					You can follow any responses to this entry through the RSS 2.0 feed.
											You can leave a response, or trackback from your own site.
					
					
				

				
					Post navigation
					« Previous Post
					
				

			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[An eyecare foundation model for clinical assistance]]></title>
            <link>https://www.nature.com/articles/s41591-025-03900-7</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057513</guid>
            <description><![CDATA[In the context of an increasing need for clinical assessments of foundation models, we developed EyeFM, a multimodal vision–language eyecare copilot, and conducted a multifaceted evaluation, including retrospective validations, multicountry efficacy validation as a clinical copilot and a double-masked randomized controlled trial (RCT). EyeFM was pretrained on 14.5 million ocular images from five imaging modalities paired with clinical texts from global, multiethnic datasets. Efficacy validation invited 44 ophthalmologists across North America, Europe, Asia and Africa in primary and specialty care settings, highlighting its utility as a clinical copilot. The RCT—a parallel, single-center, double-masked study—assessed EyeFM as a clinical copilot in retinal disease screening among a high-risk population in China. A total of 668 participants (mean age 57.5 years, 79.5% male) were randomized to 16 ophthalmologists, equally allocated into intervention (with EyeFM copilot) and control (standard care) groups. The primary endpoint indicated that ophthalmologists with EyeFM copilot achieved higher correct diagnostic rate (92.2% versus 75.4%, P < 0.001) and referral rate (92.2% versus 80.5%, P < 0.001). Secondary outcome indicated improved standardization score of clinical reports (median 33 versus 37, P < 0.001). Participant satisfaction with the screening was similar between groups, whereas the intervention group demonstrated higher compliance with self-management (70.1% versus 49.1%, P < 0.001) and referral suggestions (33.7% versus 20.2%, P < 0.001) at follow-up. Post-deployment evaluations indicated strong user acceptance. Our study provided evidence that implementing EyeFM copilot can improve the performance of ophthalmologists and the outcome of patients. Chinese Clinical Trial Registry registration: ChiCTR2500095518 . Trained and validated on multimodal data from 14.5 million images from multicountry datasets, a foundation model is shown to increase diagnostic and referral accuracy of clinicians when used as an assistant in a trial involving 16 ophthalmologists and 668 patients.]]></description>
            <content:encoded><![CDATA[
                Data availabilityFor the reproduction of our algorithm code, we have also deposited a minimum dataset (https://zenodo.org/records/15546254; ref. 41), which is publicly available for scientific research and non-commercial use. The data supporting the findings of this trial are available within the paper and its supplementary information files. All requests for further data sharing will be reviewed by the data management committees from participating institutions and by the ethics committee of Shanghai Health and Medical Centre, China, to verify whether the request is subject to any intellectual property or confidentiality obligations and will be accessible with informed consents. Requests for access to deidentified individual-level data from this trial can be submitted via email to B.S. (shengbin@sjtu.edu.cn) with detailed proposals for approval and will be evaluated on a case-by-case basis and responded to within 60 days. Investigators who consent to the terms of the data transfer agreement, including, but not limited to, the use of these data only for academic purposes and to protect the confidentiality of the data and limit the possibility of identification of patients, will be granted access. Source data are provided with this paper.Code availability
            
            The code being used in the current study for developing the algorithm is provided at https://github.com/eyefm/EyeFM.
          ReferencesZhou, Y. et al. A foundation model for generalizable disease detection from retinal images. Nature 622, 156–163 (2023).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Chen, R. J. et al. Towards a general-purpose foundation model for computational pathology. Nat. Med. 30, 850–862 (2024).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Lu, M. Y. et al. A visual-language foundation model for computational pathology. Nat. Med. 30, 863–874 (2024).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. Med. 25, 44–56 (2019).CAS 
    PubMed 
    
                    Google Scholar 
                Tanno, R. et al. Collaboration between clinicians and vision–language models in radiology report generation. Nat. Med. 31, 599–608 (2025).CAS 
    PubMed 
    
                    Google Scholar 
                Norden, J, G. & Shah, N. R. What AI in health care can learn from the long road to autonomous vehicles. NEJM Catalyst https://catalyst.nejm.org/doi/abs/10.1056/CAT.21.0458 (2022).You, J. G., Hernandez-Boussard, T., Pfeffer, M. A., Landman, A. & Mishuris, R. G. Clinical trials informed framework for real world clinical implementation and deployment of artificial intelligence applications. NPJ Digit. Med. 8, 107 (2025).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Longhurst, C. A., Singh, K., Chopra, A., Atreja, A. & Brownstein, J. S. A call for artificial intelligence implementation science centers to evaluate clinical effectiveness. NEJM AI https://doi.org/10.1056/AIp2400223 (2024).Article 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Gupta, A., Savarese, S., Ganguli, S. & Fei-Fei, L. Embodied intelligence via learning and evolution. Nat. Commun. 12, 5721 (2021).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Colunga-Lozano, L. E. et al. Clinical judgment shows similar and sometimes superior discrimination compared to prognostic clinical prediction models: a systematic review. J. Clin. Epidemiol. 165, 111200 (2024).
                    Google Scholar 
                Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at https://arxiv.org/abs/2108.07258 (2022).Howell, M. D., Corrado, G. S. & DeSalvo, K. B. Three epochs of artificial intelligence in health care. JAMA 331, 242–244 (2024).
                    Google Scholar 
                Lu, M. Y. et al. A multimodal generative AI copilot for human pathology. Nature 634, 466–473 (2024).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Future of Health: The Emerging Landscape of Augmented Intelligence in Health Care. https://www.ama-assn.org/system/files/future-health-augmented-intelligence-health-care.pdf (American Medical Association, 2024).Yang, J. et al. Generalizability assessment of AI models across hospitals in a low-middle and high income country. Nat. Commun. 15, 8270 (2024).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Avram, O. et al. Accurate prediction of disease-risk factors from volumetric medical scans by a deep vision model pre-trained with 2D scans. Nat. Biomed. Eng. 9, 507–520 (2024).PubMed 
    
                    Google Scholar 
                Street, A., Kersaudy Kerhoas, M. & Ndlovu, Z. From equitable access to equitable innovation: rethinking bioengineering for global health. Nat. Rev. Bioeng. 2, 444–446 (2024).CAS 
    
                    Google Scholar 
                Matheny, M. E., Whicher, D. & Thadaney Israni, S. Artificial intelligence in health care: a report from the National Academy of Medicine. JAMA 323, 509–510 (2020).PubMed 
    
                    Google Scholar 
                van de Sande, D. et al. To warrant clinical adoption AI models require a multi-faceted implementation evaluation. NPJ Digit. Med. 7, 58 (2024).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Hadziahmetovic, M., Nicholas, P., Jindal, S., Mettu, P. S. & Cousins, S. W. Evaluation of a remote diagnosis imaging model vs dilated eye examination in referable macular degeneration. JAMA Ophthalmol. 137, 802–808 (2019).PubMed Central 
    
                    Google Scholar 
                Liu, H., Li, C., Wu, Q. & Lee, Y. J. Visual instruction tuning. In Advances in Neural Information Processing Systems 36 (eds Oh, A. et al.) https://papers.nips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf (Curran Associates, 2023).Touvron, H. et al. Llama 2: open foundation and fine-tuned chat models. Preprint at https://arxiv.org/abs/2307.09288 (2023).Bachmann, R., Mizrahi, D., Atanov, A. & Zamir, A. MultiMAE: Multi-modal Multi-task Masked Autoencoders. In Computer Vision – ECCV 2022 (eds Avidan, S. et al.) 348–367 (Springer-Verlag, 2022).Rafailov, R. et al. Direct preference optimization: your language model is secretly a reward model. In Proc. of the 37th International Conference on Neural Information Processing Systems (eds Oh, A. et al.) 53728–53741 (Curran Associates, 2023).McMahan, B., Moore, E., Ramage, D., Hampson, S. & Arcas, B. A. Y. Communication-efficient learning of deep networks from decentralized data. In Proc. of the 20th International Conference on Artificial Intelligence and Statistics (eds Singh, A. & Zhu, J.) 1273–1282 (PMLR, 2017).Chen, X. et al. FFA-GPT: an automated pipeline for fundus fluorescein angiography interpretation and question-answer. NPJ Digit. Med. 7, 111 (2024).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Singhal, K. et al. Toward expert-level medical question answering with large language models. Nat. Med. 31, 943–950 (2025).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                McDuff, D. et al. Towards accurate differential diagnosis with large language models. Nature 642, 451–457 (2025).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. JAMA 318, 2211–2223 (2017).PubMed Central 
    
                    Google Scholar 
                Wang, W. et al. Learning two-stream CNN for multi-modal age-related macular degeneration categorization. IEEE J. Biomed. Health Inform. 26, 4111–4122 (2022).PubMed 
    
                    Google Scholar 
                He, M. et al. Prevalence and clinical characteristics of glaucoma in adult Chinese: a population-based study in Liwan District, Guangzhou. Invest. Opthalmol. Vis. Sci. 47, 2782–2788 (2006).
                    Google Scholar 
                Liu, X. et al. Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension. Nat. Med. 26, 1364–1374 (2020).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Bourne, R. et al. Trends in prevalence of blindness and distance and near vision impairment over 30 years: an analysis for the Global Burden of Disease Study. Lancet Glob. Health 9, e130–e143 (2021).
                    Google Scholar 
                Trott, M. et al. Eye disease and mortality, cognition, disease, and modifiable risk factors: an umbrella review of meta-analyses of observational studies. Eye 36, 369–378 (2022).PubMed 
    
                    Google Scholar 
                Xiong, K., Mao, H., Zhang, Q., Lei, C. & Liang, Y. Associations between vision impairment and multimorbidity among older Chinese adults: results from the China health and retirement longitudinal study. BMC Geriatr. 23, 688 (2023).PubMed 
    PubMed Central 
    
                    Google Scholar 
                Zheng, D. D. et al. Patterns of chronic conditions and their association with visual impairment and health care use. JAMA Ophthalmol. 138, 387–394 (2020).PubMed Central 
    
                    Google Scholar 
                Holden, B. A. et al. Global prevalence of myopia and high myopia and temporal trends from 2000 through 2050. Ophthalmology 123, 1036–1042 (2016).PubMed 
    
                    Google Scholar 
                Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023).CAS 
    PubMed 
    PubMed Central 
    
                    Google Scholar 
                Lewis, J. R. & Sauro, J. in Human Centered Design (ed Kurosu, M.) 94–103 (Springer, 2009).Hillis, S. L. & Soh, B. P. Obuchowski-Rockette analysis for multi-reader multi-case (MRMC) readers-nested-in-test study design with unequal numbers of readers. Proc. SPIE Int. Soc. Opt. Eng. 12467, 124670F (2023).PubMed 
    PubMed Central 
    
                    Google Scholar 
                EyeFM study group EyeFM sample dataset. Zenodo https://zenodo.org/records/15546254 (2025).Download referencesAcknowledgementsWe thank H. Li and Z. Li for creating the illustrations and icons. This study was supported by the National Key R&D Program of China (2022YFC2502800), the National Natural Science Foundation of China (82388101) and the Beijing Natural Science Foundation (IS23096) to T.Y.W.; the National Natural Science Foundation of China (62272298), the Noncommunicable Chronic Diseases-National Science and Technology Major Project (2023ZD0509202 & 2023ZD0509201), the National Key Research and Development Program of China (2022YFC2407000) to B.S.; and the Noncommunicable Chronic Diseases-National Science and Technology Major Project (2023ZD0509202 & 2023ZD0509201), the Clinical Special Program of Shanghai Municipal Health Commission (20224044) and the Three-Year Action Plan to Strengthen the Construction of the Public Health System in Shanghai (2023-2025 GWVI-11.1-28) to T.C. These funders/sponsors had no role in the design or conduct of the study.Author informationAuthor notesThese authors contributed equally: Yilan Wu, Bo Qian, Tingyao Li, Yiming Qin, Zhouyu Guan, Tingli Chen, Yali Jia, Ping Zhang, Dian Zeng.These authors jointly supervised this work: Ya Xing Wang, Yih-Chung Tham, Ching-Yu Cheng, Tien Yin Wong, Bin Sheng.Authors and AffiliationsBeijing Visual Science and Translational Eye Research Institute (BERI), Beijing Tsinghua Changgung Hospital Eye Center, Tsinghua Medicine, Tsinghua University, Beijing, ChinaYilan Wu, Yiming Qin, Dian Zeng, Yixiao Jin, Hongwei Ji, Ya Xing Wang & Tien Yin WongSchool of Clinical Medicine, Tsinghua Medicine, Tsinghua University,  Beijing, ChinaYilan Wu, Yiming Qin, Dian Zeng, Hongwei Ji & Tien Yin WongShanghai Belt and Road International Joint Laboratory for Intelligent Prevention and Treatment of Metabolic Disorders, Department of Computer Science and Engineering, School of Electronic, Information, and Electrical Engineering, Institute for Proactive Healthcare, Shanghai Jiao Tong University, Department of Endocrinology and Metabolism, Shanghai Sixth People’s Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai Diabetes Institute, Shanghai Clinical Center for Diabetes, Shanghai Key Laboratory of Diabetes Mellitus, Shanghai, ChinaBo Qian, Tingyao Li, Yiming Qin, Zhouyu Guan, Huating Li, Ziyao Meng, Xiang Chen, Yuanqi Yao & Bin ShengMOE Key Laboratory of AI, School of Electronic, Information, and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, ChinaBo Qian, Tingyao Li, Yiming Qin, Ziyao Meng, Xiang Chen, Yuanqi Yao & Bin ShengCollege of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education, Nanjing, ChinaBo QianDepartment of Ophthalmology, Shanghai Health and Medical Centre, Wuxi, ChinaTingli Chen, Yi Huang, Xiyun Bian, Jing Wang, Xiaolong Yang, Haifang Zhang & Yihan LiCasey Eye Institute, Oregon Health and Science University, Portland, OR, USAYali Jia & Pengxiao ZangDepartment of Biomedical Engineering, Oregon Health and Science University, Portland, OR, USAYali Jia & Pengxiao ZangDepartment of Computer Science and Engineering, The Ohio State University, Columbus, OH, USAPing Zhang & Changchang YinDepartment of Biomedical Informatics, The Ohio State University, Columbus, OH, USAPing Zhang & Changchang YinDepartment of Ophthalmology & Visual Sciences, College of Medicine, The Ohio State University Wexner Medical Center, Columbus, OH, USASayoko Moroi, Joshua Evans, Alan Letson, Frederik Davidorf, Mona Adeli, Peter Chen & Thomas A. MendelDepartment of Ophthalmology, University of Florida College of Medicine, Jacksonville, FL, USARajiv Raman, Praveena Venkatakrishnan, Dhaivat Shah, Abhishek Kumar Tripathi, Dharshan Bhatt, Urvashi Kala & Abhishek KarraDepartment of Ophthalmology, Odense University Hospital, Odense, DenmarkBenjamin Sommer Thinggaard & Frederik PedersenOpen Patient Data Explorative Network, Odense University Hospital, Odense, DenmarkBenjamin Sommer Thinggaard, Benjamin Sommer Thinggaard, Frederik Pedersen, Bjarke Steenberg Smith, Andreas Abou Taha & Jakob GrauslundDepartment of Ophthalmology, Malabo Regional Hospital, Malabo, Equatorial GuineaJosé Alogo Obiang ÑeheDepartment of Ophthalmology, Faculty of Medicine, Universiti Malaya, Kuala Lumpur, MalaysiaTengku Ain Kamalden, José Alogo Obiang Ñehe & Celestino Edjang Nguema MikueUniversiti Malaya Medical Centre, Kuala Lumpur, MalaysiaTengku Ain Kamalden, Tengku Ain Kamalden, Tajunisah Iqbal, Penny Lott Pooi Wah, Marium Jamaluddin Ahmad, Nurul Najieha Amir & Irina Effendi-TenangNIHR Biomedical Research Centre, Moorfields Eye Hospital NHS Foundation Trust, London, UKYukun Zhou, Tengku Ain Kamalden, Tajunisah Iqbal, Penny Lott Pooi Wah, Marium Jamaluddin Ahmad, Nurul Najieha Amir & Irina Effendi-TenangInstitute of Ophthalmology, University College London, London, UKYukun Zhou, Sobha Sivaprasad & Pearse A. KeaneDepartment of Ophthalmology and Visual Sciences, The Chinese University of Hong Kong, Hong Kong Special Administrative Region, Hong Kong, ChinaAn Ran Ran, Dawei Yang, Simon K. H. Szeto, Julia Y. Y. Chan, Victor T. T. Chan, Sobha Sivaprasad & Pearse A. KeaneSingapore Eye Research Institute, Singapore National Eye Centre, Singapore, SingaporeQingsheng Peng, Chenxi Zhang, Carol Y. Cheung, Yih-Chung Tham, Ching-Yu Cheng & Tien Yin WongCentre for Innovation and Precision Eye Health and Department of Ophthalmology, Yong Loo Lin School of Medicine, National University of Singapore, Singapore, SingaporeQingsheng Peng, Gavin Siew Wei Tan, Yih-Chung Tham & Ching-Yu ChengState Key Laboratory of Ophthalmology, Zhongshan Ophthalmic Center, Sun Yat‐sen University, Guangdong Provincial Key Laboratory of Ophthalmology and Visual Science, Guangzhou, ChinaYing Feng ZhengDepartment of Ophthalmology, The Eighth Affiliated Hospital of Sun Yat-sen University, Shenzhen, ChinaDingqiao WangMedical Records and Statistics Office, Shanghai Sixth People’s Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, ChinaJie Shen & Dingqiao WangKey Laboratory of Ocular Fundus Diseases, Chinese Academy of Medical Sciences, Beijing, ChinaYouxin Chen, Weihong Yu, Chenxi Zhang & Xinyu ZhaoDepartment of Ophthalmology, Peking Union Medical College Hospital, Peking Union Medical College, Chinese Academy of Medical Sciences, Beijing, ChinaYouxin Chen, Weihong Yu, Rongping Dai, Chenxi Zhang & Xinyu ZhaoBeijing Key Laboratory of Fundus Diseases Intelligent Diagnosis & Drug/Device Development and Translation, Beijing, ChinaYouxin Chen, Weihong Yu, Rongping Dai, Chenxi Zhang, Xinyu Zhao, Shiqun Lin & Yan ZhouDepartment of Ophthalmology, Shanghai Sixth People’s Hospital Affiliated to Shanghai Jiao Tong University School of Medicine, Shanghai, ChinaXiangning Wang, Yan Chen, Qiang Wu, Shiqun Lin & Yan ZhouShenzhen Eye Hospital, Shenzhen Eye Center, Southern Medical University, Shenzhen, ChinaHongbin XieDepartment of Ophthalmology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, ChinaHua-Tao XieDepartment of Ophthalmology, The Second Affiliated Hospital of Naval Medical University (Shanghai Changzheng Hospital), Shanghai, ChinaRuili WeiDepartment of Ophthalmology, Renji Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, ChinaJin LiInstitute for AI Industry Research, Tsinghua University, Beijing, ChinaWeizhi MaROAS Thrust, Hong Kong University of Science and Technology (Guangzhou), Guangzhou, ChinaLei Zhu & Hongqiu WangHong Kong University of Science and Technology, Hong Kong Special Administrative Region, Hong Kong, ChinaLei ZhuInstitute of High Performance Computing, Agency for Science, Technology and Research, Singapore, SingaporeHuazhu FuKey Laboratory of River Basic Digital Twinning of Ministry of Water Resources, Macau University of Science and Technology, Macao Special Administrative Region, Taipa, ChinaWenxiao WangWuhan Bright Eye Hospital, Wuhan, ChinaShan Lin, Zejun Xu & Nian GuanThe People’s Hospital of Sixian County, Anhui, ChinaXiao ZhangInstitute for Research in Ophthalmology, Foundation for Ophthalmology Development, Poznan, PolandAndrzej GrzybowskiDepartment of Ophthalmology, University of Warmia and Mazury, Olsztyn, PolandAndrzej GrzybowskiDepartment of Ophthalmology, Pomeranian Hospitals, Wejherowo, PolandMonika Gołębiowska-Bogaj & Maciej GawęckiDepartment of Pediatric Ophthalmology, Faculty of Medical Sciences in Katowice, Medical University of Silesia, Katowice, PolandAdrian SmedowskiDepartment of Pediatric Ophthalmology, Kornel Gibinski University Clinical Center, Medical University of Silesia, Katowice, PolandAdrian SmedowskiGlaucoTech Co., Katowice, PolandAdrian SmedowskiDepartment of Ophthalmology, Faculty of Medical Sciences in Katowice, Medical University of Silesia, Katowice, PolandWojciech SzaraniecDepartment of Ophthalmology, Kornel Gibinski University Clinical Center, Medical University of Silesia, Katowice, PolandWojciech SzaraniecSchool of Healthcare Management, Tsinghua Medicine, Tsinghua University, Beijing, ChinaYou WuInstitute for Hospital Management, Tsinghua University, Beijing, ChinaYou WuGuangdong Provincial Key Laboratory of Intelligent Information Processing, College of Electronics and Information Engineering, Shenzhen University, Shenzhen, ChinaYang WenDepartment of Medicine, Faculty of Medicine, Universiti Malaya, Kuala Lumpur, MalaysiaLee-Ling LimDepartment of Medicine and Therapeutics, The Chinese University of Hong Kong, Prince of Wales Hospital, Hong Kong Special Administrative Region, Hong Kong, ChinaLee-Ling LimBaker Heart and Diabetes Insitute, Melbourne, Victoria, AustraliaLee-Ling LimDepartment of Ophthalmology, Rajavithi Hospital, College of Medicine, Rangsit University, Bangkok, ThailandPaisan RuamviboonsukOphthalmology and Visual Science Academic Clinical Program (Eye ACP), Duke-NUS Medical School, Singapore, SingaporeYih-Chung ThamBeijing Key Laboratory of Intelligent Diagnostic Technology and Devices for Major Blinding Eye Diseases, Tsinghua Medicine, Tsinghua University, Beijing, ChinaTien Yin WongDepartment of Ophthalmology, Peking University Third Hospital, Beijing, ChinaBo QuDepartment of Ophthalmology, The Seventh Affiliated Hospital of Sun Yat-sen University, Shenzhen, ChinaHongzhi YuanDepartment of Ophthalmology, Guangzhou Women and Children’s Medical Center, Guangzhou, ChinaMengxiang GuoDepartment of Ophthalmology, Jiangmen People’s Hospital, Jiangmen, ChinaMing ZhouDepartment of Ophthalmology, The First Affiliated Hospital of Guangzhou Medical University, Guangzhou, ChinaWen ShiDepartment of Ophthalmology, Vestfold Hospital, Tonsberg, NorwayLars Morten SkollerudAuthorsYilan WuBo QianTingyao LiYiming QinZhouyu GuanTingli ChenYali JiaPing ZhangDian ZengSayoko MoroiRajiv RamanBenjamin Sommer ThinggaardFrederik PedersenJosé Alogo Obiang ÑeheTengku Ain KamaldenYukun ZhouYixiao JinHuating LiAn Ran RanDawei YangZiyao MengQingsheng PengYing Feng ZhengDingqiao WangHongwei JiPengxiao ZangChangchang YinJie ShenYouxin ChenWeihong YuRongping DaiChenxi ZhangXinyu ZhaoXiangning WangYan ChenQiang WuHongbin XieSimon K. H. SzetoJulia Y. Y. ChanVictor T. T. ChanHua-Tao XieRuili WeiJin LiWeizhi MaLei ZhuHongqiu WangHuazhu FuWenxiao WangShan LinZejun XuNian GuanXiao ZhangAndrzej GrzybowskiMonika Gołębiowska-BogajMaciej GawęckiAdrian SmedowskiWojciech SzaraniecYou WuYang WenXiang ChenYuanqi YaoLee-Ling LimCarol Y. CheungGavin Siew Wei TanJakob GrauslundPaisan RuamviboonsukSobha SivaprasadPearse A. KeaneYa Xing WangYih-Chung ThamChing-Yu ChengTien Yin WongBin ShengConsortiaEyeFM Global Reader Study TeamJoshua Evans, Alan Letson, Frederik Davidorf, Mona Adeli, Peter Chen, Thomas A. Mendel, Yi Huang, Xiyun Bian, Jing Wang, Xiaolong Yang, Haifang Zhang, Yihan Li, Bo Qu, Hongzhi Yuan, Mengxiang Guo, Dingqiao Wang, Ming Zhou, Wen Shi, Shiqun Lin, Yan Zhou, Jakob Grauslund, Lars Morten Skollerud, Benjamin Sommer Thinggaard, Frederik Pedersen, Bjarke Steenberg Smith, Andreas Abou Taha, José Alogo Obiang Ñehe, Celestino Edjang Nguema Mikue, Praveena Venkatakrishnan, Dhaivat Shah, Abhishek Kumar Tripathi, Dharshan Bhatt, Urvashi Kala, Abhishek Karra, Tengku Ain Kamalden, Tajunisah Iqbal, Penny Lott Pooi Wah, Marium Jamaluddin Ahmad, Nurul Najieha Amir & Irina Effendi-TenangContributionsT.Y.W. and B.S. conceived and supervised the project. T.Y.W., B.S., Yilan Wu, Z.G., D.Z. and Y.F.Z. designed the study. B. Qian, Y.Q. and P.Z. designed the deep learning algorithm and the computational framework. T.Y.W., Yilan Wu, B. Qian, T.L, Y.Q., Z.G., D.Z. and Y.F.Z. contributed to the initial drafting of the manuscript. Y.J., P.Z., Y.Z., Q.P., C.Y., J.S., A.G., M.G.-B., M.G., A.S., W.S., L.Z. and You Wu helped with data collection. S.M., R.R., B.S.T., J.A.O.Ñ., T.A.K., H.L., Y.J., A.R.R., D.Y., Z.M., D.W., Y.C., W.Y., R.D., X. Zhao, C.Z., X.W., Y.C., Q.W., H.X., S.K.H.S., J.Y.Y.C., V.T.T.C., H.-T.X., R.W., J.L., Shan Lin, Z.X., N.G., J.E., A.L., F.D., MA., P.C., T.A.M., Y.H., Y.Z., Shiqun Lin, X.B., J.W., X.Y., H.Z., Y.L, B. Qu, H.Y., M.G., M.Z., W.S., L.M.S., F.P., B.S.S., A.A.T., C.E.N.M., P.V., D.S., A.K.T., D.B., U.K., A.K., T.I., P.L.P.W., M.J.A., N.N.A. and I.E.-T. participated in prospective validations. T.C., X. Zhang, Y.H., X.B., J.W., X.Y., H.Z. and Y.L. conducted the data collection and analysis in the RCT. J.G., P.R., S.S., P.A.K., L.-L.L., C.Y.C., G.S.W.T., Y.X.W., Y.-C.T., C.-Y.C., Y.F.Z., B.S. and T.Y.W. contributed to collaboration organization and provided critical revision of the manuscript for important intellectual content. All authors provided critical comments and reviewed the manuscript. All authors discussed the results and approved the final version before submission.Corresponding authorsCorrespondence to
                Tien Yin Wong or Bin Sheng.Ethics declarations
            
              Competing interests
              Y.J. is a patent holder of Optovue/Visionix, Inc., Optos plc and Genentech, Inc. She receives financial support from Genentech, Inc., and she receives financial compensation from Optovue/Visionix, Inc. and Genentech, Inc. P.A.K. is a co-founder of Cascader Ltd., has acted as a consultant for Retina Consultants of America, Roche, Boehringer Ingleheim and Bitfount and is an equity owner in Big Picture Medical. He has received speaker fees from Zeiss, Thea, Apellis and Roche. He has received travel support from Bayer and Roche. He has attended advisory boards for Topcon, Bayer, Boehringer Ingleheim and Roche. T.Y.W. is a consultant for AbbVie Pte Ltd., Aldropika Therapeutics, Bayer, Boehringer Ingelheim, Zeiss, Genentech, Iveric Bio, Novartis, Opthea Limited, Plano, Quaerite Biopharm Research Ltd., Roche, Sanofi and Shanghai Henlius. He is an inventor, holds patents and is a co-founder of start-up companies EyRiS and Visre, which have interests in, and develop digital solutions for, eye diseases. All potential conflicts of interests for consultancy, advisory boards and positions in the start-up companies and financial renumeration, if any, are managed by institutional policies under SingHealth and Tsinghua University. The other authors declare no competing interests.
            
          Peer review
            
            
              Peer review information
              Nature Medicine thanks Tae Keun Yoo and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Primary Handling Editor: Lorenzo Righetto, in collaboration with the Nature Medicine team.
            
          Additional informationPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Extended dataExtended Data Fig. 1 Envision the use case of EyeFM as a whole workflow copilot for eyecare under different clinical settings.For patients attending ocular disease screening in the primary care centre where only low-cost examination technics are available, EyeFM can use its single modality and cross-modality ability to assist disease detection, followed with screening report writing assisted by its vision-question answering ability. Then, some patients will be referred to specialty care settings for further examination, where EyeFM can use its integrated modality disease detection or even zero-shot ability to facilitate further diagnosis. The image-report writing ability and vision-question answering can also assist to improve the efficiency of clinical report and patient letter drafting in specialty care settings.Extended Data Fig. 2 The structural diagram of human-knowledge encoding in pretraining and application phase for EyeFM.a) The diagram of pretraining process for EyeFM. EyeFM was first pretrained for its image encoder with five modalities of images, then conducted vision-language joint pretraining. The image module includes one encoder and five decoders. The encoder comprises 24 Transformer blocks. Each decoder comprises two Transformer blocks. The linear projection layer is implemented with a single convolutional layer. In the vision-language module, the projection is implemented using a single linear layer, which serves to connect the image encoder with the language module. The language module is based on LLaMA 2 architecture with 7 billion parameters. b) The diagram of human-in-the-loop process for EyeFM. EyeFM human-in-the-loop utilized DPO and federated learning for distributed knowledge evolution.Extended Data Fig. 3 Glossary table and the clinical tasks related with each validation experiment.First, we conducted retrospective validations, comparing EyeFM with prior benchmarks of medical foundation models. This step serves as the foundation for evaluating the model’s performance and safety when progressing to clinical applications. Second, we conducted reader studies and real-world study prospectively to test the efficiency of EyeFM as a clinical copilot to assist ophthalmologists. This step bridged the gap between the performance of the model its own and its efficiency when applicated by clinicians. At last, we validated EyeFM with a Randomised Controlled Trial (RCT). Colored chart, above, validation experiments and their corresponding relationship with the functions of EyeFM. The x-axis represents different tasks of EyeFM and y-axis represents validation experiments. Cells were coloured if the experiments have validated corresponding functions. Below, glossary table of the tasks, clinical scenario and experiments. RCT, randomised controlled trial.Extended Data Fig. 4 Experiment 1 – Retrospective validation of EyeFM on multi-ethnic datasets.a) For disease detection on CFP, the sample sizes and P values are: DR (n = 1501, P = 0.042), glaucoma suspect (n = 405, P = 0.533), AMD suspect (n = 370, P = 0.627), MMD (n = 643, P = 0.030). For disease detection on OCT, the sample sizes and P values are: ciDME (n = 523, P = 0.002), glaucoma (n = 412, P = 0.333), AMD (n = 379, n = 0.036). The sample size for cataract detection on external eye photo was 198 and the P value was 0.102. Error bars represent 95% CI. b)Segmentation dice similarity coefficient. The sample size for segmentation on CFP are: HE (n = 13), SE (n = 14), HM (n = 27) and MA (n = 27). The P value for haemorrhages was 0.083. The sample size was 759 for OCT segmentation. Error bars represent 95% CI. c) Cross-modality disease detection of ciDME that usually need to be diagnosed by OCT with CFP inputs only (left), the sample size was 405 and the P value was <0.001. Cross-modality disease detection of wet-AMD that usually need to be diagnosed by CFP with external eye photo inputs (right) the sample size was 332 and the P value was 0.583. Boxes indicate quartile values and whiskers indicate 1.5×the interquartile range. d) Image-report generation, model performance was evaluated by automatic metrics labelled as the x-axis. The sample size was 500. Boxes indicate quartile values and whiskers indicate 1.5× the interquartile range. e) Head-to-head comparison of answers generated by EyeFM and ophthalmologists, the measurement was summed score for quality, safety and empathy, ranged from 3–15 scores. Presented as Kernel Density Plot. The sample size was 300 for EyeFM and 1200 for ophthalmologists. P values were calculated with two-sided t-test between EyeFM and the better-performed reference model. *** denotes P < 0.001, n.s. represents P > 0.05. CFP, colour fundus photo; OCT, optical coherence tomography; EEP, external eye photo; DR, diabetic retinopathy; DME, diabetic macular oedema; AMD, age-related macular degeneration; MMD, myopic macular degeneration; MA, microaneurysms; HE, hard exudates; HM, haemorrhages; SE, soft exudates.Source dataExtended Data Fig. 5 Workflow for the diagnostic study and management study in the RCT.Participants included in the trial will first receive diagnosis and report by ophthalmologists by CFP, then receive additional OCT examinations for consultant-level reviewers to assess and revise the diagnosis and report. All diagnosis and reports before revision by consultant-level reviewers will be included in the analysis for correct diagnosis rate, correct referral rate and standardization score of reports. Only participants that are correctly diagnosed as ‘with fundus abnormality’ will be included in the follow-up for patient compliance analysis. CFP, colour fundus photo; OCT, optical coherence tomography.Extended Data Table 1 Demographic characteristics of eyecare providers who participated in EyeFM validationFull size tableRights and permissionsSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.Reprints and permissionsAbout this articleCite this articleWu, Y., Qian, B., Li, T. et al. An eyecare foundation model for clinical assistance: a randomized controlled trial.
                    Nat Med  (2025). https://doi.org/10.1038/s41591-025-03900-7Download citationReceived: 24 October 2024Accepted: 16 July 2025Published: 28 August 2025DOI: https://doi.org/10.1038/s41591-025-03900-7
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Space Shuttle Columbia disaster and the over-reliance on PowerPoint (2019)]]></title>
            <link>https://mcdreeamiemusings.com/blog/2019/4/13/gsux1h6bnt8lqjd7w2t2mtvfg81uhx</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057404</guid>
            <description><![CDATA[We’ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We’re so used to it we expect it.  We accept it.  We even consider it ‘learning’. As an educator I push against ‘death by PowerPoint’ and I'm fascinated with how we]]></description>
            <content:encoded><![CDATA[

      

      

      
        
          
            
              
            
          
        
      


      
      
      

      
        
        

  
  

    

    

      

      
        
          
        
        

        
          
            
          
        

        
          
          
            The space shuttle Columbia disintegrating in the atmosphere (Creative Commons)
          
        
      
        
      

    
  We’ve all sat in those presentations.  A speaker with a stream of slides full of text, monotonously reading them off as we read along.  We’re so used to it we expect it.  We accept it.  We even consider it ‘learning’. As an educator I push against ‘death by PowerPoint’ and I'm fascinated with how we can improve the way we present and teach.  The fact is we know that PowerPoint kills.  Most often the only victims are our audience’s inspiration and interest.  This, however, is the story of a PowerPoint slide that actually helped kill seven people.January 16th 2003.  NASA Mission STS-107 is underway. The Space Shuttle Columbia launches carrying its crew of seven to low orbit.  Their objective was to study the effects of microgravity on the human body and on ants and spiders they had with them.  Columbia had been the first Space Shuttle, first launched in 1981 and had been on 27 missions prior to this one.  Whereas other shuttle crews had focused on work to the Hubble Space Telescope or to the International Space Station this mission was one of pure scientific research.  The launch proceeded as normal.  The crew settled into their mission.  They would spend 16 days in orbit, completing 80 experiments.  One day into their mission it was clear to those back on Earth that something had gone wrong.  As a matter of protocol NASA staff reviewed footage from an external camera mounted to the fuel tank.  At eighty-two seconds into the launch a piece of spray on foam insulation (SOFI) fell from one of the ramps that attached the shuttle to its external fuel tank.  As the crew rose at 28,968 kilometres per hour the piece of foam collided with one of the tiles on the outer edge of the shuttle’s left wing.  


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Frame of NASA launch footage showing the moment the foam struck the shuttle’s left wing (Creative Commons)
          
        
      
        
      

    
  It was impossible to tell from Earth how much damage this foam, falling nine times faster than a fired bullet, would have caused when it collided with the wing.   Foam falling during launch was nothing new.  It had happened on four previous missions and was one of the reasons why the camera was there in the first place.  But the tile the foam had struck was on the edge of the wing designed to protect the shuttle from the heat of Earth’s atmosphere during launch and re-entry.  In space the shuttle was safe but NASA didn’t know how it would respond to re-entry.  There were a number of options.  The astronauts could perform a spacewalk and visually inspect the hull.  NASA could launch another Space Shuttle to pick the crew up.  Or they could risk re-entry.  NASA officials sat down with Boeing Corporation engineers who took them through three reports; a total of 28 slides.    The salient point was whilst there was data showing that the tiles on the shuttle wing could tolerate being hit by the foam this was based on test conditions using foam more than 600 times smaller than that that had struck Columbia.  This is the slide the engineers chose to illustrate this point:

  NASA managers listened to the engineers and their PowerPoint.  The engineers felt they had communicated the potential risks.  NASA felt the engineers didn’t know what would happen but that all data pointed to there not being enough damage to put the lives of the crew in danger.  They rejected the other options and pushed ahead with Columbia re-entering Earth’s atmosphere as normal.  Columbia was scheduled to land at 0916 (EST) on February 1st 2003.  Just before 0900, 61,170 metres above Dallas at 18 times the speed of sound, temperature readings on the shuttle’s left wing were abnormally high and then were lost.  Tyre pressures on the left side were soon lost as was communication with the crew.  At 0912, as Columbia should have been approaching the runway, ground control heard reports from residents near Dallas that the shuttle had been seen disintegrating.  Columbia was lost and with it her crew of seven.  The oldest crew member was 48.  The shuttle programme was on lock down, grounded for two years as the investigation began.  The cause of the accident became clear: a hole in a tile on the left wing caused by the foam let the wing dangerously overheat until the shuttle disintegrated.  The questions to answer included a very simple one: Why, given that the foam strike had occurred at a force massively out of test conditions had NASA proceeded with re-entry?  Edward Tufte, a Professor at Yale University and expert in communication reviewed the slideshow the Boeing engineers had given NASA, in particular the above slide.  His findings were tragically profound.


 Firstly, the slide had a misleadingly reassuring title claiming that test data pointed to the tile being able to withstand the foam strike.  This was not the case but the presence of the title, centred in the largest font makes this seem the salient, summary point of this slide.  This helped Boeing’s message be lost almost immediately.























  Secondly, the slide contains four different bullet points with no explanation of what they mean.  This means that interpretation is left up to the reader.  Is number 1 the main bullet point?  Do the bullet points become less important or more?  It’s not helped that there’s a change in font sizes as well.  In all with bullet points and indents six levels of hierarchy were created.  This allowed NASA managers to imply a hierarchy of importance in their head: the writing lower down and in smaller font was ignored.  Actually, this had been where the contradictory (and most important) information was placed.  


Thirdly, there is a huge amount of text, more than 100 words or figures on one screen.   Two words, ‘SOFI’ and ‘ramp’ both mean the same thing: the foam.  Vague terms are used.  Sufficient is used once, significant or significantly, five times with little or no quantifiable data.  As a result this left a lot open to audience interpretation.  How much is significant?  Is it statistical significance you mean or something else?  
























Finally the single most important fact, that the foam strike had occurred at forces massively out of test conditions, is hidden at the very bottom.  Twelve little words which the audience would have had to wade through more than 100 to get to.  If they even managed to keep reading to that point.  In the middle it does say that it is possible for the foam to damage the tile.  This is in the smallest font, lost. 























  NASA’s subsequent report criticised technical aspects along with human factors.  Their report mentioned an over-reliance on PowerPoint: “The Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA.”  Edward Tufte’s full report makes for fascinating reading. Since being released in 1987 PowerPoint has grown exponentially to the point where it is now estimated than thirty million PowerPoint presentations are made every day.  Yet, PowerPoint is blamed by academics for killing critical thought.  Amazon’s CEO Jeff Bezos has banned it from meetings.   Typing text on a screen and reading it out loud does not count as teaching.  An audience reading text off the screen does not count as learning.  Imagine if the engineers had put up a slide with just: “foam strike more than 600 times bigger than test data.”  Maybe NASA would have listened.  Maybe they wouldn’t have attempted re-entry.  Next time you’re asked to give a talk remember Columbia. Don’t just jump to your laptop and write out slides of text.  Think about your message.  Don’t let that message be lost amongst text.  Death by PowerPoint is a real thing.  Sometimes literally.Thanks for reading - Jamie 


      

      
        
          
        
        

        
          
            
          
        

        
          
          
            Columbia’s final crew (from https://www.space.com/19436-columbia-disaster.html)
          
        
      
        
      

    

    

    

  

  

  

  
  
  


        
        
      

      

      

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Sometimes CPU cores are odd]]></title>
            <link>https://anubis.techaro.lol/blog/2025/cpu-core-odd/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057346</guid>
            <description><![CDATA[TL;DR: all the assumptions you have about processor design are wrong and if you are unlucky you will never run into problems that users do through sheer chance.]]></description>
            <content:encoded><![CDATA[Protected by Anubis From Techaro. Made with ❤️ in 🇨🇦.Mascot design by CELPHASE.This website is hosted by Techaro. If you have any complaints or notes about the service, please contact support@techaro.lol and we will assist you as soon as possible.
-- ImprintThis website is running Anubis version v1.22.0-pre1-8-g21c3e0c.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Expert: LSP for Elixir]]></title>
            <link>https://github.com/elixir-lang/expert</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057322</guid>
            <description><![CDATA[Official Elixir Language Server Protocol implementation - elixir-lang/expert]]></description>
            <content:encoded><![CDATA[Expert
Expert is the official language server implementation for the Elixir programming language.
Installation
You can download Expert from the releases page for your
operating system and architecture. Put the executable somewhere on your $PATH, like ~/.local/bin/expert
For editor specific installation instructions, please refer to the Installation Instructions
Nightly Builds
If you want to try out the latest features, you can download a nightly build.
Using the GH CLI, you can run the following command to download the latest nightly build:
gh release download nightly --pattern 'expert_linux_amd64' --repo elixir-lang/expert
Then point your editor to the downloaded binary.
Building from source
To build Expert from source, you need Zig 0.14.1 installed on your system.
Then you can run the following command or follow the instructions in the Installation Instructions:
just release-local
This will build the Expert binary and place it in the apps/expert/burrito_out directory. You can then point your
editor to this binary.
Sponsorship
Thank you to our corporate sponsors! If you'd like to start sponsoring the project, please read more below.






Corporate
For companies wanting to directly sponsor full time work on Expert, please reach out to Dan Janowski: EEF Chair of Sponsorship WG at danj@erlef.org.
Individual
Individuals can donate using GitHub sponsors. Team members are listed in the sidebar.
Other resources

Architecture
Development Guide
Glossary
Installation Instructions

LICENSE
Expert source code is released under Apache License 2.0.
Check LICENSE file for more information.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fuck up my site – Turn any website into beautiful chaos]]></title>
            <link>https://www.fuckupmysite.com/?url=https%3A%2F%2Fnews.ycombinator.com&amp;torchCursor=true&amp;comicSans=true&amp;fakeCursors=true&amp;peskyFly=true</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45057020</guid>
            <description><![CDATA[PARODY/ENTERTAINMENT ONLY: Transform any website into pure chaos. Add burning cursors, Comic Sans everything, fake cursors, and more chaos features to any site. A humorous parody tool - not for real use. Some people just want to watch the web burn.]]></description>
            <content:encoded><![CDATA[This tool is for parody and entertainment purposes only. It temporarily applies visual chaos effects to websites for comedic effect. We do not store, collect, or transmit any personal information.NEVER enter passwords, credit card details, or any sensitive information while using this tool. The proxied sites are not secure and should not be used for any real transactions or logins.By using this tool, you acknowledge that it's purely for entertainment and you will not enter any sensitive data. Banking, financial, healthcare, and government sites are blocked for safety.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You no longer need JavaScript: an overview of what makes modern CSS so awesome]]></title>
            <link>https://lyra.horse/blog/2025/08/you-dont-need-js/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056878</guid>
            <description><![CDATA[An overview of what makes modern CSS so awesome.]]></description>
            <content:encoded><![CDATA[
    
  
  2025-08-28  ¦ css


  So much of the web these days is ruined by the bloat that is modern JavaScript frameworks. React apps that take several seconds to load. NextJS sites that throw random hydration errors. The node_modules folder that takes up gigabytes on your hard drive.
It’s awful. And you don’t need it.




  
    
      Name
      Status
      Type
      Size
      Time
    
  
  
    app200document153.8 kB51 ms
    6920616d20612066-s.p.6f6e7421.woff2200font31.5 kB32 ms
    686579206d652074-s.p.6f6f2121.woff2200font28.5 kB116 ms
    77687920646f6573.css200stylesheet253 kB47 ms
    2074686520646566.js200script648 kB83 ms
    61756c74206e6578.js200script166 kB363 ms
    746a732074616b65.js200script83.3 kB46 ms
    turbopack-20757020302e354d.js200script38.0 kB95 ms
    423f207468617427.js200script414 B34 ms
    73206d6f72652074.js200script32.6 kB49 ms
    68616e206d792065.js200script15.1 kB71 ms
    6e7469726520626c.js200script143 kB48 ms
    6f6721 hey there!200script4.1 kB103 ms
  



The intro paragraph of this post is tongue-in-cheek. It’s there to get you to read the rest of the post. I suspect the megabytes of tracking scripts intertwined with bad code is far more likely to be the real culprit behind all the terrible sites out there. Web frameworks have their time and place. And despite my personal distaste for them, I know they are used by many teams to build awesome well-optimized apps.
Despite that, I think there’s some beauty in leaving it all behind. Not just the frameworks, but JavaScript altogether. Not every site needs JavaScript. Perhaps your e-commerce site needs it for its complex carts and data visualization dashboards, but is it really a necessity for most of what’s out there?
It’s actually pretty incredible what HTML and CSS alone can achieve.

So, what do you say?
My goal with this article is to share my perspectives on the web, as well as introduce many aspects of modern HTML/CSS you may not be familiar with. I’m not trying to make you give up JavaScript, I’m just trying to show you everything that’s possible, leaving it up to you to pick what works best for whatever you’re working on.
I think there’s a lot most web developers don’t know about CSS.
And I think JS is often used where better alternatives exist.
So, let me show you what’s out there.

“But CSS sucks”
I believe a lot of the negativity towards CSS stems from not really knowing how to use it. Many developers kind of just skip learning the CSS fundamentals in favor of the more interesting Java- and TypeScript, and then go on to complain about a styling language they don’t understand.
I suspect this is due to many treating CSS as this silly third wheel for adding borders and box-shadows to a webapp. It’s undervalued and often compared to glorified crayons, rather than what it really is - a powerful domain-specific programming language.
It’s telling when to this day the only CSS joke in the webdev circles is centering a div.


i am a div

body {  display: flex;  flex-direction: rowcolumnrow-reversecolumn-reverse;  flex-wrap: nowrapwrap;  align-content: centerflex-startflex-endspace-aroundspace-betweenstretch;  justify-content: centerflex-startflex-endspace-aroundspace-betweenspace-evenly;  align-items: centerflex-startflex-endstretchbaseline;}




Yes, the syntax isn’t the prettiest, but is it really that hard?
Besides, your devtools probably1 come with a fun little gadget that lets you fiddle with the flexbox by just clicking around. You don’t even need to remember the syntax.

I don’t think CSS is fundamentally any more difficult than JS, but if you skip the basics on one and only focus on the other, it’s no surprise it feels that way.
“But it’s painful to write”
Another source of disdain for CSS is how awful it has been to write in the past. This is very much true, and is probably why things like Sass and Tailwind2 exist.
But that’s the thing, it used to be bad.



🦊


Rebane@rebane2001
btw u should write css like

cool-thing {
    display: flex;
    &[shadow] {
        box-shadow: 1px 1px #0007;
    }
    @media (width < 480px) {
        flex-direction: column;
    }
}

and html like

<cool-thing shadow>wow</cool-thing>

because it's allowed & modern & neat!
11:58 AM · Apr 8, 2025

❤️ 1.5K


(yes! the code above is standards compliant3)
In the past few years, CSS has received a ton of awesome quality-of-life additions, making it nice to do stuff that has historically required preprocessors or JavaScript.
Nesting is definitely one of my favorite additions!
In the past, you’ve had to write code that looks like this:
:root {
  --like-color: #24A4F3;
  --like-color-hover: #54B8F5;
  --like-color-active: #0A6BA8;
}

.post {
  display: block;
  background: #EEE;
  color: #111;
}

.post .avatar {
  width: 48px;
  height: 48px;
}

.post > .buttons {
  display: flex;
}

.post > .buttons .label {
  font-size: 24px;
  padding: 8px;
}

.post > .buttons .like {
  cursor: pointer;
  color: var(--like-color);
}

.post > .buttons .like:hover {
  color: var(--like-color-hover);
}

.post > .buttons .like:active {
  color: var(--like-color-active);
}

@media screen (max-width: 800px) {
  .post > .buttons .label {
    font-size: 16px;
    padding: 4px;
  }
}

@media (prefers-color-scheme: dark) {
  .post {
    background: #222;
    color: #FFF;
  }
}


And yeah, that’s pretty awful to work with. For anything that involves multiple chained selectors, you kind of have to keep a mental map of how every parent selector relates to its children, and the more CSS you add the harder it gets.
But let’s try it with nesting:
:root {
  --like-color: #24A4F3;
  --like-color-hover: hsl(from var(--like-color) h s calc(l + 10));
  --like-color-active: hsl(from var(--like-color) h s calc(l - 20));
}

.post {
  display: block;
  background: #EEE;
  color: #111;
  @media (prefers-color-scheme: dark) {
    background: #222;
    color: #FFF;
  }
  .avatar {
    width: 48px;
    height: 48px;
  }
  & > .buttons {
    display: flex;
    .label {
      font-size: 24px;
      padding: 8px;
      @media (width <= 800px) {
        font-size: 16px;
        padding: 4px;
      }
    }
    .like {
      cursor: pointer;
      color: var(--like-color);
      &:hover { color: var(--like-color-hover); }
      &:active { color: var(--like-color-active); }
    }
  }
}

That is way nicer to read4! All the relevant parts are right next to each other, so it’s a lot easier to understand what’s going on. Seeing the &:hover and &:active right next to the .like button is especially nice imo.
And since you can sort of see the structure - the parent selectors “guarding” the child ones - it also makes it a lot easier to get away with short and simple class names (or even referring to elements themselves).
You may have noticed that I’m also making use of relative colors in the second example. I think the MDN article has a lot of awesome examples, but the jist of it is that you can take an existing color, modify it in many different ways across multiple color spaces, and mix it with other colors using color-mix().
/* remove blue from a color */
rgb(from #123456 r g 0);
/* make a color transparent */
rgb(from #123456 r g b / 0.5);
/* make a color lighter */
hsl(from #123456 h s calc(l + 10));
/* change the hue in oklch color space */
oklch(from #123456 l c calc(h + 10));
/* mix two colors in oklab color space */
color-mix(in oklab, #8CFFDB, #04593B 25%);

These snippets are really useful for when you want something to be just ever so slightly darker or brighter, such as a button hover effect or a matching border color, and they’re way nicer to use than doing all those color conversions in JavaScript. If you’re feeling particularly adventurous, you could even go ahead and generate your entire color scheme in just CSS.

100200300400500600700800900
-40°-20°0°+20°+40°
primarycomplimentarysecondary
successdangerwarninginfo

view-source


(yes! the color picker above is written in just css)
Safari is currently broken when handling of cqw/cqh units, therefore the demo above may not work correctly. If this happens, try using Firefox or Chrome instead.
There are so many cool new CSS features that make writing it just that little bit nicer. Things like letting you use (width <= 768px) instead of (max-width: 768px) in your @media query, the lh unit that matches the line-height, the scrollbar-gutter property that solves the little scrollbar-related layout shifts, or the ability to finally center stuff vertically without flex/grid.


Baseline
And all of this is brought together by the cherry on top that is Baseline. It’s a guarantee that a specific feature works in every major browser5, and it also lets you know since when - newly available features work in all the latest browsers, and widely available ones work in browsers up to 2.5 years old. Nesting, for example, has been fully supported in all browsers since December 2023, and thus will become widely available in June 2026. You can find the Baseline symbols in various places, such as the MDN docs6.
These are just a few examples of what makes modern CSS so much nicer to write than what we had even just 5 years ago. It almost feels like comparing ES37 to ECMAScript 2025 - and I wouldn’t blame your grudge if the former is what you’re used to.
Why bother?
Okay, so CSS has more quality-of-life stuff than before. Still, why would one choose to use it over something else? Doesn’t JavaScript already let us do everything just fine?
You need to disable JavaScript to run this app.
I think my reasons for using CSS fall into two main categories - because some users don’t want to use JavaScript, and because doing things in CSS can be genuinely better.
My blog, for example, focuses on infosec topics. Many security researchers (myself included) use a hardened browser configuration to protect themselves, which often means disabling JavaScript by default. I think it’s nice that they can fully experience my blog without changing their security settings or running a separate, sandboxed browser.
The same goes for privacy-conscious users, and it makes sense! As an experiment, I opened up a local Estonian news site in a web browser with JavaScript enabled. Can you guess how many js files it fetched? (answer in footnote8) That’s crazy! You do not want that running on your computer.
But surely, you are not one of the evil devs who loads a double-digit number of analytics scripts on your site - is there still any reason to reach for CSS?
Well, I think a lot of things are just plain nicer to make in HTML/CSS, both from the developer and end-user perspectives, be it for ease of use, accessibility, or performance.
Hover effects for your buttons? Toast animations? Input validation? All of these things just work in CSS, and you won’t have to reinvent the wheel, or throw kilobytes of someone else’s code at it. There will always be some cases where you do need that extra flexibility JavaScript often provides, but if you don’t need that, and doing it in CSS is easier, then why not save yourself the trouble?


  
  
  
  
  
  
  
  
  
  
  
  













And the performance of CSS is so much better! Every JavaScript interaction has to go through an event loop that wastes CPU cycles, eats some battery, and adds that tiny bit of stutter to everything.
Sure, in the grand scale of things it isn’t that bad, APIs like requestAnimationFrame are really good at keeping things smooth. But CSS animations run in the separate compositor thread, and aren’t affected by stutters and blocking in the event loop.
It makes quite a difference on low-end devices, but feels nice even on high-end ones. CSS animations on my 240hz monitor look amazing9 - JS can look pretty good too, but it has that tiny bit of stutter to it that keeps it from being perfect, especially if you plan on running other heavy code at the same time.
It also means you won’t have to worry as much about optimization, as the browser takes care of a lot more of the rendering side of things, and often runs your stuff on the GPU if possible.
Pro tip! Wanna trigger animations from JS anyways? Use the modern Web Animations API to easily play the smooth CSS animations from JS.
Transitioning

Speaking of which, I think it’s time I start showing you practical examples, and a good place to start showing the styles is well, @starting-style.
In the past it has been pretty annoying to add start animations (such as fade-ins) to elements. You’ve had to either set up an entire CSS animation with a separate @keyframes block to go with it, or do a transition using JavaScript where you first add an element to the page, then wait a frame, and then add a class to the element.
.toast {
  transition: opacity 1s, translate 1s;
  opacity: 1;
  translate: 0 0;
  @starting-style {
    opacity: 0;
    translate: 0 10px;
  }
}
Success!

But this has all changed thanks to the new @starting-style at-rule!
Pretty much all you have to do is set your properties as usual, add the initial transition states to @starting-style, and add those properties to a transition. It’s pretty simple and it kind of just works without having to trigger the animation in any way.
Lunalover
Another good example of where CSS shines is theming. Many sites need separate light and dark modes, and modern CSS makes dealing with that pretty easy.
:root {
  color-scheme: light dark;
  --text: light-dark(#000, #FFF);
  --bg: light-dark(#EEE, #242936);
}
hi there!you are awesome!i am!

By setting the color-scheme property to light dark, you are telling the browser to automatically pick the theme according to the user preference, and you can then make use of that by setting color values with the light-dark() function.
Not only does it set your own colors, but also those of the native components, such as the default buttons, form elements, and scrollbars. It kind of just makes stuff work by default, and that’s nice!
:root {
  color-scheme: light dark;
  &:has(#theme-light:checked) {
    color-scheme: light;
  }
  &:has(#theme-dark:checked) {
    color-scheme: dark;
  }
}

    Auto
    Light
    Dark
  

You can then add some way of overriding the color-scheme property to let the user pick a theme different from their system setting. Here I am using radio buttons to accomplish that.
Pro tip! CSS can’t save the theme preference, but you can still do progressive enhancement. Make the themes work CSS-only, and then add the saving/loading of preference as an optional extra in JavaScript or server-side code.
Lyres and accordions
“But those don’t look like radio buttons” I hear you cry.
Input elements such as radio buttons and checkboxes are a great foundation to build other stuff on top of - the example above consists of labels for the buttons and invisible radio buttons that can be checked for with the :checked pseudo-class.
<radio-picker aria-label="Radio buttons example" role="radiogroup">
  <label><input type="radio" name="demo" id="veni" checked>veni</label>
  <label><input type="radio" name="demo" id="vidi">vidi</label>
  <label><input type="radio" name="demo" id="vici">vici</label>
</radio-picker>
<style>
  radio-picker {
    display: flex;
    label {
      &:has(input:checked) {
        box-shadow: inset 0px 0px 8px 0px #888;
      }
      &:has(input:focus-visible) {
        outline: 2px solid #000;
      }
      box-shadow: inset 0px 0px 1.2px 0px #000;
      padding: 10px;
      cursor: pointer;
      background: #0002;
      &:hover { background: #0004; }
      &:active { background: #0006; }
    }
    input {
      /* To allow screen reader to still access these. */
      opacity: 0;
      position: absolute;
      pointer-events: none;
    }
  }
</style>


    veni
    vidi
    vici
  

This is how I made the theme selector from the previous example. I’ve made the radio buttons half-visible in the demo for clarity, but with the opacity: 0 they would not actually be visible.
There’s a whole lot going on here, so let’s break it down.
<radio-picker aria-label="Radio buttons example" role="radiogroup">

We start off with the radio-picker element - I just made it up, you can use a div instead if you’d prefer. We give it an aria-label to give the group an accessible name, and the aria role of radiogroup to make it work as a group for the radio buttons.
You could also use the fieldset element instead of doing the aria roles if that’d fit your use case better.
<label><input type="radio" name="demo" id="veni" checked>veni</label>
<label><input type="radio" name="demo" id="vidi">vidi</label>
<label><input type="radio" name="demo" id="vici">vici</label>

Next, we add the radio buttons with their respective labels - usually you’d have to use the for attribute on labels to define which element they’re referring to, but since we have the input inside the label we don’t have to do that.
All the type="radio" inputs should also have a name value set to the same thing so that they are grouped together (you still need10 the radiogroup though). And then you can give them values or ids however you want.
label {
  &:has(input:checked) {
    box-shadow: inset 0px 0px 8px 0px #888;
  }
  &:has(input:focus-visible) {
    outline: 2px solid #000;
  }
  box-shadow: inset 0px 0px 1.2px 0px #000;
  padding: 10px;
  cursor: pointer;
  background: #0002;
  &:hover { background: #0004; }
  &:active { background: #0006; }
}

We then style the labels as we wish - the :hover and :active pseudo-classes can be used to make the buttons more fun to click, the :has(input:checked) selector can be used to define the style of the selected button, and the :has(input:focus-visible) selector can be used to add an outline when someone tabs over to the button.
The difference between :focus and :focus-visible is that the former shows up even if you use your mouse, while the latter only shows up when you use keyboard navigation, so it’s often visually more clean to use the latter.
input {
  opacity: 0;
  position: absolute;
  pointer-events: none;
}

And last, we make the radio button input exist while not being visible. This is a bit hacky, but it’s how you can keep this control accessible to keyboard navigation and screen readers.
And that’s how we get the cool-looking radio buttons!
<radio-tabs>
  <div tabindex=0 id="tab-veni">veni...</div>
  <div tabindex=0 id="tab-vidi">vidi...</div>
  <div tabindex=0 id="tab-vici">vici...</div>
</radio-tabs>
<style>
  body:has(#veni:not(:checked)) #tab-veni,
  body:has(#vidi:not(:checked)) #tab-vidi,
  body:has(#vici:not(:checked)) #tab-vici {
    display: none;
  }
</style>

    veni
    vidi
    vici


  veni/ˈveɪni/(intransitive) to come
  vidi/ˈviːdi/(intransitive) to see
  vici/ˈviːt͡ʃi/(intransitive) to conquer


We can now use them in the CSS however we want by just seeing if they’re :checked. Here I made tabs with separate divs for the content by using a :has selector on a parent element to find out which radio button is currently selected.
The :has selector has to be on a parent element that contains both the radio button and the target element - you can simply use html or body if you want it to work across the entire page. You should never use something like :has(…) by itself as it’ll run the selector for every element of the page, which can cause performance issues (body:has(…) is okay).
<div>
  <details name="deets">
    <summary>What's your name?</summary>
    My name is Lyra Rebane.
  </details>
  <details name="deets">
    ...
  </details>
</div>
<style>
  div {
    border: 1px solid #AAA;
    border-radius: 8px;
    /* based on the MDN example */
    summary {
      font-weight: bold;
      margin: -0.5em -0.5em 0;
      padding: 0.5em;
      cursor: pointer;
    }
    details {
      &:last-child { border: none }
      border-bottom: 1px solid #aaa;
      padding: 0.5em 0.5em 0;
      &[open] {
        padding: 0.5em;
        summary {
          border-bottom: 1px solid #aaa;
          margin-bottom: 0.5em;
        }
      }
    }
  }
</style>

  
    
    What's your name?My name is Lyra Rebane.
    Cool name!I know ^_^
    Where can I learn more?On my website, lyra.horse!
  


Finally, before we move on, I want to give you a quick introduction to the details element. It’s great for if you want an accordion-style menu, such as for a FAQ section. The details open and close independently of each other, but you can set their name attribute to the same value to have only one open at a time.
Using them is pretty easy, put your content and a summary tag inside a details tag, and put the title inside the summary tag. The example above is a bit more convoluted for the visual flair, but all you really need is the html part of it.
The details elements are pretty stylable! You can add animations depending on the [open] state, and you can also get rid of the arrow by setting list-style: none on the summary.
Also, ctrl+f works with it, which is a big win in my book!
Validation
And lastly, I want to show you the power of input validation in HTML and CSS.
<label for="usrname">Username</label>
<input type="text" id="usrname" pattern="\w{3,16}" required>
<small>3-16 letters, only alphanum and _.</small>
<style>
 input:valid {
   border: 1px solid green;
 }
 input:invalid {
   border: 1px solid red;
 }
</style>

  Username
    
    3-16 letters, only alphanum and _.
  


This is a simple example of how you can validate an input field with a regex pattern. If you set a pattern attribute like above, a form that contains the input cannot be submitted unless the field matches the pattern. If you’re submitting something like an e-mail address, a phone number, or a url, it might make sense to use the respective input types instead of writing your own regex.
Now, where CSS comes in is styling the input to show whether its value is valid. In the example above, I’m using :valid and :invalid to set a border color, but that comes with the downside of always having your input marked, even if the user hasn’t entered anything yet.
input {
  border: none;
  border-radius: 2px;
  outline: 1px solid #000;
  &:focus { outline-width: 2px; }
  &:user-valid { outline-color: green; }
  &:user-invalid { outline-color: red; }
}

  Username
    
    3-16 letters, only alphanum and _.
  


An easy win here is to instead use :user-valid and :user-invalid - these pseudo-classes only become active once you’ve interacted with input field. I also made this example use an outline instead of a border, which I think looks a lot nicer.
It may sometimes even make sense to use a combination of :valid and :user-invalid.
And of course, you can use the :has selector to style other elements depending on the input too!


  Password
    
    The password must:
- be 8-16 characters
- contain at least ⅰ roman numeral
- not end with a letter

  


This one's just for fun ^_-! you win! yay!
I do want to mention that for some stuff, such as date pickers () or datalists (), there are built-in elements that do the job, but you may find them limited in one way or the other. If you’re making an input like that with specific requirements, you may still need to dip your feet in a bit of JavaScript.

  
  
  
  
  
  
  
  
  
  
  
  
  
  
  


Do not the vw/vh
This section is kind of random but I wanted to include it here because I think a lot of people are messing this one up and I want more people to know how to do this stuff right.
So CSS has vw/vh units that correspond to 1% of the viewport width and height respectively, which makes perfect sense for desktop browsers.





  CBSignal chatAre you feeling encrypted?
  MMaratit smells of onions in here...
  bmblackle moriwhat's the scoop in yer smacker, horseberry?
  RRhynoraterCSS go BRRRRR
  PPatTheHyrulerI just lost the game
  MMalkI can't wait to taste the sorbet!


🔒lyra.horse/blog/•••

lyra's epic blog posts tags
You no longer need JavaScript
yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap yap




Where it becomes a bit more nuanced is on mobile devices. For example, mobile versions of both Firefox and Chrome will hide the URL bar when scrolling down on a page.
This causes the vw/vh units to be a bit ambigous - do they represent the entire available screen, only the area that’s visible with the URL bar, or something in between?
If it’s the first option, you might end up with buttons or links off-screen11! If it’s the second, you may end up with a background div that doesn’t cover the entire background.




  ppingotuxcss spec so good i transitioned
  mmayahi wife!!
  ZZvitlol. lmao, sogar.
  !!!! HAND !!yap yap yap
  JJonesGlory to KuK
  SSpaxim goop
  eenscribeI need a job


  lvh
  dvh
  svh











🔒lyra.horse/blog/•••




  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh
  lvh
  svh
  dvh

Your values

  
    
      Unit
      Value
    
  
  
    
      vh
      px
    
    
      lvh
      px
    
    
      dvh
      px
    
    
      svh
      px
    
  

Above is a table of values your browser reports - if you're on mobile, try scrolling the blogpost up and down so that the URL bar hides and see how the numbers change.
The values are multiplied by 100 (eg 100vh is used instead of 1vh).




The solution to this is to use the new responsive viewport units: lvh, svh, and dvh.
lvh stands for largest viewport height, and thus is useful for things like backgrounds that you’d want to cover the entire screen with, and wouldn’t care about getting cut off.
svh stands for smallest viewport height, and should be used for things that must always fit on the screen, such as buttons and links.
And dvh stands for dynamic viewport height - this one will update to whatever the current viewport height is. It might seem like the obvious choice, but it should not be used for elements you don’t want resizing or moving around as the user scrolls the page, as it could become quite annoying and possibly even laggy otherwise.
Of course, the respective lvw, svw, and dvw units exist too :).
Keyboard cat
By default, the viewport units do not account for the keyboard overlaying the page.
There are two ways to deal with that: the interactive-widget attribute, and the VirtualKeyboard API.
The former option is widely supported across browsers, works without JS, and goes in the meta viewport tag. It makes it so that opening the keyboard will change all of the viewport units.
<meta name="viewport" content="width=device-width, interactive-widget=resizes-content">

The latter option is currently only supported in Chromium-based browsers, and requires a single line of JavaScript to use:
navigator.virtualKeyboard.overlaysContent = true;

The advantage of the second option is that it allows you to use environment variables in CSS to get the position and size of the keyboard, which is pretty cool.
floating-button {
  margin-bottom: env(keyboard-inset-height, 0px);
}

But considering the fact that it doesn’t work cross-browser, I’d avoid it.
CSS wishlist
Alright, so this is a little different from the rest of the post, but I wanted to bring up some things that I wish were in CSS. I haven’t fully fleshed out all of them, so some definitely wouldn’t fit the spec as-is, but maybe they can inspire some other stuff at least.
They are just fun ideas, don’t take them too seriously.
Reusable blocks
I wish it was possible to put classes in other classes in CSS, so that you could write something like:
.border {
  border: 2px solid;
  border-radius: 4px;
}

.button {
  @apply border;
}

.card {
  @apply border;
}

This is something that Tailwind already has, and that makes me jealous.

Combined @media selectors
We can currently do nested @media queries, and also multiple selectors at the same time:
div {
  &.foo, &.bar {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
  @media (width < 480px) {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

But we cannot combine the two into a single selector:
div {
  @media (width < 480px), &.foo {
    color: red;
    padding: 8px;
    font-size: 2em;
  }
}

Which means if you want to do that you’ll inevitably have to repeat code or do some silly variable hacks, neither of which is ideal.
n-th child variable
For many of the CSS crimes I like to commit, I often end up writing code like:
div {
  span:nth-child(1) { --nth: 1; }
  span:nth-child(2) { --nth: 2; }
  span:nth-child(3) { --nth: 3; }
  span:nth-child(4) { --nth: 4; }
  span:nth-child(5) { --nth: 5; }
  ...
  span {
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

And I think it would be a lot nicer if we could instead just do:
div {
  span {
    --nth: nth-child();
    top: calc(--nth * 24px);
    color: hsl(calc(var(--nth) * 90deg) 100 90);
  }
}

n-th letter targeting
CSS has the ability to style the ::first-letter of text. It’d be cool if were was also a ::nth-letter(…) selector, similar to :nth-child. I suspect the reason this isn’t a thing is because the ::first-letter selector is a pseudo-element, which would be a bit tricky to implement with the nth-letter idea.
/* not a real feature */
p::nth-letter(2) {
  color: red;
}

hi there~


Blackle suggested that combining the nth-child() variable with :nth-letter targeting would also be fun for certain effects, such as putting the value in the sin() function to create wavy text.
div {
  /* not a real feature */
  --nth: nth-child(nth-letter);
  will-change: transform;
  translate: 0 calc(sin(var(--nth) * 0.35 - var(--wave) * 3) * 5px);
  color: color-mix(in oklch, #58C8F2, #EDA4B2 calc(sin(var(--nth) * 0.5 - var(--wave)) * 50% + 50%));
}

  
    untucknowqueen
  
  (taphover to play animation)


Unit removal
I wish you could easily remove units from values, for example by dividing them.

div {
  /* Turns into:  (no unit) */
  --screen-width: calc(100vw / 1px);
  color: hsl(var(--screen-width) 100, 50);
}
This would allow you to use the size of the viewport or container as a numeric variable for things other than length. For example, the color picker from earlier uses it to convert the location of the color picker dot to a number to be used in a color value instead.
Uh, but wait? Does that mean this feature already exists?
Yeah, lol! We already have the ability to get unitless values in CSS, but it involves doing hacky stuff such as tan(atan2(var(--vw), 1px)) with a custom @property. It’d be nice to have this as just a division, for example.
Oh, and good news, this one we might actually be getting soon!
Also if you do something like calc(1px + sqrt(1px * 1px)) your browser will crash12.
A better image function
The image() function exists, but no browsers implement it. It’s similar to just using url(), but adds some really cool features such as a fallback color, and image fragments to crop a smaller section out of a bigger image (think spritesheets).
We can already do both fallbacks and spritesheets with the various background properties, but it’d be nice to have this pretty syntax. I’d honestly love this syntax even more for <img> tags than CSS.
style tags in body
I make heavy use of <style> tags in <body> for my projects. On my blog, for example, I write the relevant CSS close to their graphics so that you can start reading the blog before the entire page (or the entire CSS) has finished loading13. And it works great!
But what’s unfortunate is that despite browsers supporting this, and major sites using this, it’s not officially spec-compliant. I suspect it’s in the spec to avoid the FOUC footgun, but there are so many reasons you would want/need style in body that I don’t think it justifies it.
I think an HTML validator should warn for this, but not error.
The art
I want to end this article by saying that to me, web development is an art, and thus, CSS is too. I often have a hard time relating to people who do webdev solely to earn money or build a startup - web development is very different when you’re on a team and are given tasks from above instead of having free will over what you create for fun.
It’s probably most apparent with things like AI14, that for me take all the fun and creativity out of my work. But it also applies to build chain tooling such as linters and minifiers - the way I write my code is part of the art, and I don’t want a tool to erase that. I don’t even use an IDE15.
Among the practical reasons for sticking to CSS listed throughout this post, there’s a secret extra reason I like to do everything in CSS, and that’s expression and art. Art isn’t always practical, and using CSS isn’t either. But it’s how I like to express myself, and it’s why I do what I do.
I tried to keep this post approachable and practical for all web developers. But there is so much more to CSS that I’d like to talk about, so expect another post about the stuff that isn’t practical, and is instead just cool as fuck. I think CSS is a programming language, and I made a game to prove it.
But that’s a topic for another time.
afterword
it’s been almost a year since my last post, but i hope it’s been worth the wait ^_^
as usual, this post is a self-contained html file with no javascript, images, or other external resources - everything on the page is handwritten html/css, weighing in at around 49kB gzipped. it was really fun creating all the little interactive widgets and visuals this time around, i think i’ve improved in css a lot since the last time i posted.
this entire post turned out to be a bit of a fun mess (as did i!), it’s almost like a chaotic gradient of tone throughout, i hope it was still interesting and enjoyable to read though.
i have a few new posts in the works: in addition to the second css one mentioned earlier, i also have one about a new web vulnerability subclass i discovered, and one about a trans topic. i’m not sure when these posts will come out, but we’ll see! make sure to add me to your rss reader if that sounds fun.
i’ll also be giving a talk at bsides tallinn in september! i’m hoping to also do css-related talks at the next ccc and disobey, but we’ll have to see whether i get accepted and have the travel budget for those.
thank you so much for reading <3
you're awesome!! (i can tell because you checked that checkbox from earlier)

If you’d like to reach out, feel free to message me on my socials or at lyra.horse [at] gmail.com.
Discuss this post on: twitter, mastodon, lobsters





Chrome’s DevTools come with the cool flexbox widget. Firefox’s however don’t seem to for some reason? I find that weird because Firefox does have really good tools for flexbox and grid development, so this seems like an odd omission. ↩︎


While I think what I said is true, Tailwind does have more to its existence, the core of which can be found in this post by its creator. ↩︎


You are allowed to just make up elements as long as their names contain a hyphen. Apart from the 8 existing tags listed at the link, no HTML tags contain a hyphen and none ever will. The spec even has <math-α> and <emotion-😍> as examples of allowed names. You are allowed to make up attributes on an autonomous custom element, but for other elements (built-in or extended) you should only make up data-* attributes. I make heavy use of this on my blog to make writing HTML and CSS nicer and avoid meaningless div-soup. ↩︎


Still not nice to read for you? I’m personally not a fan of BEM, but I’d definitely recommend reading up on it too if you just don’t vibe with the way I’m writing my examples. Also, my example intentionally shows off a lot of the syntax at once, but in the real world it might make sense to structure things a little differently. ↩︎


Baseline browsers are Safari (macOS/iOS), Chrome (desktop/Android), Edge (desktop), and Firefox (desktop/Android). ↩︎


The MDN docs of course also list detailed browser compatibility, but the Baseline symbols are nice for just getting a quick “yeah, we can use it and it’ll work for everyone” type overview. ↩︎


ES3 (1999) is the last “classic” version of JavaScript. In 2009 we got the first major revision known as ES5, and a few years later we kicked off the yearly spec updates with ES2015. Also ES4 was abandoned which makes me feel sad :c. ↩︎


93 files!! Seems like they’re 1/3 functionality, 1/3 ads, and 1/3 analytics. The site works just fine with JavaScript disabled - only stuff like the comments section and ads won’t load. It’s no longer a laggy mess either for some reason. ↩︎


I think the x3ctf challenges page looks really smooth on my computer - the marquee text animation and clicking on the challenges is buttery. And it also runs pretty well on the low-end hardware I have. Note that some browser performance recording tools can act a bit weird with CSS animations, so make sure your tools are working as expected before using them. Unrelated, but I made some other cool x3ctf web stuff too - check out the archive. ↩︎


There’s a bug in Chrome that requires you to use a fieldset/radiogroup for the radio button index to work correctly in screenreaders. Eg if you have 3 radio buttons with the same name, selecting one of them should read “radio button 1 of 3”, which is what Firefox does, but in Chrome it will instead read it as “radio button 4 of 9” or whatever if you don’t have a fieldset/radiogroup because it kind of just combines all the radio buttons on the page into a single index. ↩︎


A certain HR platform I have to use puts its action buttons at the very bottom of a 100vh container, leading to them not being visible/interactable on my phone - not a headache you want to go through when requesting sick days. It’s a good example of how just using the wrong unit can cause a pretty bad real world accessibility problem. ↩︎


Well, probably not. This is a bug I found while writing this post that only affects Chrome, and it’ll probably get fixed before it even manages to hit stable. Update: I took so long to get this blog post out that it has been fixed now. During the writing of this blog post I found another bug in Chrome though, which is pretty funny. Update 2: I found yet another Chrome bug while writing this post, this one is kinda weird, you should read it. ↩︎


This matters for people on slow connections, such as bad mobile data, satellite internet, tor, or iodine. While my blog posts are very small in size, the CSS alone can take up more than the first 14kB of a TCP round trip, so with blocking CSS in the head you might have to wait a few extra seconds (or minutes, in the case of iodine) just to start reading the first paragraph. Now, that 14kB number isn’t completely accurate in the modern world, but testing on my own server (HTTP/2, TLS 1.3), around ~16kB of the compressed html reaches the browser in the first batch of http data. ↩︎


By this I mean tools such as Copilot, Cursor, chatbots etc. I understand there is a huge difference between full-on vibe coding and just using the tab key, but I do not want to use or interact with any of those tools. Please respect that. ↩︎


I write all my code (and blogposts) in Sublime Text, which to me is just a glorified version of Notepad. The features over Notepad it gives me are syntax highlighting, multiple cursors, keyboard shortcuts, and a better visual design. It doesn’t do that much, and yet, it’s perfect. It’s so good I paid for it. ↩︎





  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My startup banking story (2023)]]></title>
            <link>https://mitchellh.com/writing/my-startup-banking-story</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45056177</guid>
            <description><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.]]></description>
            <content:encoded><![CDATA[As a relatively new member of adult society, and an absolute infant of
the business world, I didn't think much about bank choice. I figured: you
put money in, you take money out, they're all the same. I also figured a local
branch of a global bank is just a fungible tentacle of the giant banking
machine, so also... who cares. Both incorrect assumptions, but let's relive and
rediscover the effect of these assumptions as I did.




I start my company. I am a 22 year old recent college graduate living in San
Francisco and pursuing the startup dream. I file my incorporation paperwork
and wait to receive the necessary information for one of the first
steps in the life of any new business: opening a bank account.
My filing is processed and I receive my EIN while visiting my parents
in a suburb of Los Angeles. I have time to kill during one of the days so
I drive down to the nearest Chase bank branch and open a business banking
account. We'll call the person who helped me at the local branch Alex (this
will be important later). I fund that account with a $20,000 personal loan which
was almost all of my savings. I get an account number, an online login, and
boom, we're in business!
About 6 months later, I raise a ~$1M seed round. I supply my Chase business
banking account information for the wire, and at close the funding is wired to
the account. I am sitting in a cafe in downtown San Francisco and I receive a
call from an unknown number -- it's Alex, the banker that
helped me open my account. He is being very casual, sort of like
"Hey, just wanted to check on things." "I noticed a big deposit and wanted
to make sure you had everything you needed." etc. For my side, I am
mostly confused: why is this person calling me? I mostly say things like
"yes yes I'm fine" and end the call quickly. Some wheels have started
turning in Southern California, and I just hadn't known it yet.
Someone out there is probably mentally screaming at me "you fool!"
at this point. With hindsight, I agree, but I will remind you
dear reader that I have only been legally allowed to purchase alcohol
for just over a year at this point in my life in the story.




The two years since 2012 -- from a banking perspective -- are quiet. Alex
doesn't call me again, and we have no changes in our banking setup. For two years,
the company was in heads-down building mode. We had shown significant product
traction and were now ready to ramp up hiring to continue building.
At the end of 2014, we raise a $10.2M series A. I once again provide the
same Chase business banking account and when the round closes, the funds are
wired. Surprise surprise, Alex calls me! I'm starting to realize banks get
an alert when there are major changes in account balances. Regardless,
I once again brush Alex off -- "everything is good thanks! bye!" -- and
continue on with my life.
At this point, I am bewildered that this guy I met at the random local branch
to sign some papers is the one calling me, but didn't think much more of
it at the time.




Once again, the two years since 2014 are mostly quiet from a banking
perspective. Alex called more regularly to "check in" but otherwise
nothing has changed. We still bank with Chase. I still have never gone
back into a branch. I do everything online.
In the fall of 2016, we raise a $24M series B. I once again provide the
same Chase business banking account and when the round closes, the funds
are wired. Again, Alex calls. Again, I brush him off. The bank is where I
plant money, I don't need anyone calling me. I just want to focus on building
the company.
Throughout 2016, we had been building out an executive team for the company.
And around the same time of the funding, we hire a Vice President of Finance. As he gets
up to speed with our financial footing, he notices we have ~$35M sitting in
cash in a Chase bank account. This is obviously not a smart thing to do,
so he suggests some financial plans for how to better safeguard and utilize
this mountain of cash.
As part of these plans, he suggests moving to Silicon Valley Bank (SVB).
They're local to the Bay Area, he's worked with them before, and their
bankers understand startups. It'll make accounts receivables, payables,
payroll, etc. easier. To me, a bank is a bank is a bank, and if it helps
make his job easier, I support his plan.
I log into the Chase online portal and initiate a wire for the full account
balance to SVB. I have to pay something like a $30 fee to wire $35M
(inconsequential to the story, but amusing nonetheless). Someone calls me for
verification -- not Alex -- and the wire processes. Boom, we're done with
Chase. Or so I think.
Alex calls me the next day. The day we initiated the wire was his day off.
He sounds slightly agitated. I wasn't rude to him, but I was short with him.
I switched banks, that's all there is to it. Thanks and goodbye. I never
talk to Alex ever again. A bank is a bank is a bank, you put money in,
you get money out, I don't understand why I would need to talk to someone.
I once again interrupt this story to appeal to the readers who are
screaming at me and thank you for joining me on this story recounting
my learning journey. Rest assured, at this point in the story, a professional
was now in charge of the company's finances. But the decisions of the
years leading up to this would have lingering effects for a few more years...




We now take a brief detour from the company, because this is where my
personal life becomes relevant to the story.
For the prior three years, I had been living in Los Angeles. At some
point during 2017, I had to go to a local Chase branch to make some
changes to my personal accounts. It has been close to a year since the company
stopped using Chase.
I visit the closest bank branch to my apartment. This bank branch is 20
miles north of where my parents live -- or the area with the branch where I
opened the original company business bank accounts. I'm going to Chase for
purely personal reasons, but this information is unfortunately relevant
to the story.
At my local branch, I walk up to the teller and provide some handwritten
information: my name, account number, desired transaction, etc. The teller looks at the paper,
then looks at me, then looks back at the paper, then asks "Are you the
HashiCorp guy?" What? HashiCorp is doing well but its not at all
something a random non-technical consumer would know about. What is going on?
I say yes and he acknowledges but doesn't automatically offer any more
information. I have to know, so I continue "How do you know that?" His
response is "Dude, everyone at Chase down here knows about HashiCorp." Huh?
Up to this point, everything in the story is what I know and experienced
first hand. What follows however is now second hand information as told
by this teller. I haven't verified it, but other employees (at other branches)
have said similar things to me over the years.
The teller proceeds to explain that Alex -- the guy I opened my original
company account with -- became a fast rising star in the area. He had
opened a business account in a small suburb that grew from $20,000 to
$35,000,000 in balances in just four years! Despite the business (my business)
not engaging in higher-revenue activities with the bank, the opportunity
this account represented to the small business wing of the small suburban
branch stirred up some excitement. It was just a matter of time.
And then, overnight, the account went to $0. Without talking to anyone,
without any prior warning, that account was gone. I used online banking
to transfer the entirety of the balance to another bank. The small suburban
branch viewed this as a huge loss and Alex came into work with some tough
questions and no answers. I instantly recalled feeling that Alex was agitated
when he called me the day after the transfer, and I now had an idea of why.
I don't know what happened to Alex, the teller said he was "no longer
working in the area" and said it with a noticably negative tone. I don't
know what this means and I never found out. Perhaps, he just moved.
Following this event, Chase began an educational series to other local
branches in the Los Angeles area explaining that there are these "startups"
and how their financial patterns do not match those of a typical business. This series
taught branches how to identify startups and how to consider their accounts.
The case study they used for this presentation: HashiCorp.




It has been two years since hiring our VP of Finance and our financial
department is in really healthy shape. I still have certain approval rights
but no longer directly manage the accounts of the company.
Given the recent events with Silicon Valley Bank, I feel it's important to
mention that at this point of the company, we had already begun diversifying
our balances across multiple banks. SVB will not be mentioned again for
the remainder of the story.
I'm working at my office at home in Los Angeles and I receive a phone
call from our finance department. That's weird, I rarely receive phone calls.
They tell me that during a routine internal audit, they realized there are
a few customer accounts that are still paying their bill into the old Chase
account.
I never closed that original Chase business account back in 2016. Let
me explain how that happens. To close an account, I had to do it in person at
any local Chase branch. Startups are busy, the account balance in 2016 was $0,
and so I just put it off. Well, a couple years passed, it was still open,
and a few customers were actually sending payments to it.
Worse, upon the realization that a few customers were paying into this account,
our finance team realized that there was also fraud. For over a year, someone
had been wiring thousands of dollars out every few weeks. We were short
over $100,000 due to fraud. The finance team immediately called Chase and
reported the fraud, locked down the account, and Chase started an investigation.
Meanwhile, the finance team wanted me to close the account and wire the
remaining balance to our actual business bank. With the fraud actively being
handled by Chase and the finance team, I take on the task of closing the
account. I immediately head to the nearest local Chase branch (once again
a branch I've never been to before) and explain the situation.
After waiting for 15 minutes, a manager walks up to me. I know this can't
be good. The branch manager explains that due to the actions taken to lock
down the account for fraud, electronic transfers are unavailable. It doesn't
matter that I'm provably the person who opened the account, electronic
transfers are "impossible."
I say okay, and ask how I am supposed to close the account and transfer
the remaining balance. He said I can close the account and withdraw the
remaining balance only in cash. Cash? At this point, I literally asked:
"like, green paper money cash?" He says yes. The balance in the account is
somewhere around $1M.
I spent another two hours at the bank, juggling between calling our
finance department, talking to this branch manager, and calling the Chase
business phone line. We determine that instead of literal green cash, I
can get a cashier's check. But there is a major problem: the amount the
cashier's check is made out for has to be available at that local branch
(or, whichever branch issues it).
And, well, local branches I guess don't usually have $1M cash lying around.
Or, if they do, its not enough to cover other business activities for the day
so they're not willing to part with it.
The bank manager gives me the phone number of another branch manager that
"may be able to help me." He literally writes down a phone number on a
piece of paper. This is all feeling so surreal. I call this number and
its for a slightly larger branch a few miles down the road. He says
"you're the HashiCorp guy right?" And I roll my eyes. My infamy in the
area is still well known.
This manager is very helpful, if not a bit gruff. He explains to me that
each local branch has some sort of performance metric based on inflows and
outflows at the given branch. Therefore, funding a $1M cash withdrawal was
not attractive to them. I'm learning a lot in a really condensed period of
time at this point. I don't even know if what he's telling me is true, or
legal, all I hear is "this is going to be hard to do if you want it all at
once."
But we do want it all at once. And we want to close the account. Now.
He is not happy, but he says he'll call me back in 24 to 48 hours. True
to his word, he calls me back the next day. He says that he had to coordinate
to ensure his branch had the proper funding to satisfy this transaction,
and that the funding would be available at a specific date a few days hence.
He said I have to do the withdrawal that day because his branch will not
hold that amount in cash for any longer.
He also subtly suggested I hire personal security or otherwise deposit
those funds somewhere with haste. I believe his exact words were "if you
lose that check, I can't help you." Again, this was a one time event, and
I don't know how true that all is, but it was said to me.
A few days later, I walk into the branch (I did not hire personal security).
I tell the teller my name and there is a flicker of immediate recognition.
The teller guides me to a cubicle, the account is successfully closed,
I'm issued a $1M cashier's check, and I walk out the door.
My business banking relationship with Chase is, at long last, complete.
I want to make it clear that Chase could've been an excellent
banking partner. I never gave them the chance. I never told them what
my business does or what I'd use the money for. I never talked to anyone
(besides saying what I needed to get off the phone). This story isn't
a cautionary tale about Chase, it is rather recounting my naivete
as a young, first-time startup founder.

Epilogue.
The cashier's check was uneventfully deposited into our primary business
banking account shortly after I walked out of the Chase branch.
The fraud investigation took a few months to complete but we were
able to recover all of the lost funds.
Enough time has passed and employees cycled that I'm no longer recognized at
any Los Angeles area Chase branches.
I look back on these events and there are many places I cringe. At the
same time, I can't imagine making different choices because I was acting in
good faith at all times with the knowledge I had. I think the choices I made were
reasonable for any new founder, and I know many founders who have made
similar choices.
Ultimately, there was no long term negative impact of the events that
transpired (except maybe for Alex, but I truly don't know) and I can now
look back on it with amusement.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Some thoughts on LLMs and software development]]></title>
            <link>https://martinfowler.com/articles/202508-ai-thoughts.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055641</guid>
            <description><![CDATA[a short post]]></description>
            <content:encoded><![CDATA[I’m about to head away from looking after this site for a few weeks (part vacation, part work stuff). As I contemplate some weeks away from the daily routine, I feel an urge to share some scattered thoughts about the state of LLMs and AI.

                ❄                ❄                ❄                ❄

I’ve seen a few early surveys on the effect AI is having on software development, is it really speeding folks up, does it improve or wreck code quality? One of the big problems with these surveys is that they aren’t taking into account how people are using the LLMs. From what I can tell the vast majority of LLM usage is fancy auto-complete, often using co-pilot. But those I know who get the most value from LLMs reckon that auto-complete isn’t very useful, preferring approaches that allow the LLM to directly read and edit source code files to carry out tasks. My concern is that surveys that ignore the different work-flows of using LLMs will produce data that’s going to send people down the wrong paths.

(Another complication is the varying capabilities of different models.)

                ❄                ❄                ❄                ❄

I’m often asked, “what is the future of programming?” Should people consider entering software development now? Will LLMs eliminate the need for junior engineers? Should senior engineers get out of the profession before it’s too late? My answer to all these questions is “I haven’t the foggiest”. Furthermore I think anyone who says they know what this future will be is talking from an inappropriate orifice. We are still figuring out how to use LLMs, and it will be some time before we have a decent idea of how to use them well, especially if they gain significant improvements.

What I suggest, is that people experiment with them. At the least, read about what others are doing, but pay attention to the details of their workflows. Preferably experiment yourself, and do share your experiences.

                ❄                ❄               ❇                ❄

I’m also asked: “is AI a bubble”? To which my answer is “OF COURSE IT’S A BUBBLE”. All major technological advances have come with economic bubbles, from canals and railroads to the internet. We know with near 100% certainty that this bubble will pop, causing lots of investments to fizzle to nothing. However what we don’t know is when it will pop, and thus how big the bubble will have grown, generating some real value in the process, before that happens. It could pop next month, or not for a couple of years.

We also know that when the bubble pops, many firms will go bust, but not all. When the dot-com bubble burst, it killed pets.com, it killed Webvan… but it did not kill Amazon.

                ❄                ❄                ❄                ❄

I retired from public speaking a couple of years ago. But while I don’t miss the stress of giving talks, I do miss hanging out with my friends in the industry. So I’m looking forward to catching up with many of them at GOTO Copenhagen. I’ve been involved with the GOTO conference series since the 1990s (when it was called JAOO), and continue to be impressed with how they put together a fascinating program.

                ✢                ❄                ❄                ❄

My former colleague Rebecca Parsons, has been saying for a long time that hallucinations aren’t a bug of LLMs, they are a feature. Indeed they are the feature. All an LLM does is produce hallucinations, it’s just that we find some of them useful.

One of the consequences of this is that we should always consider asking the LLM the same question more than once, perhaps with some variation in the wording. Then we can compare answers, indeed perhaps ask the LLM to compare answers for us. The difference in the answers can be as useful as the answers themselves.

Certainly if we ever ask a hallucination engine for a numeric answer, we should ask it at least three times, so we get some sense of the variation. Furthermore we shouldn’t ask an LLM to calculate an answer than we can calculate deterministically (yes, I’ve seen this). It is OK to ask an LLM to generate code to calculate an answer (but still do it more than once).

                ❄                ❄                ❄                ❄

Other forms of engineering have to take into account the variability of the world. A structural engineer builds in tolerance for all the factors she can’t measure. (I remember being told early in my career that the unique characteristic of digital electronics was that there was no concept of tolerances.) Process engineers consider that humans are executing tasks, and will sometimes be forgetful or careless. Software Engineering is unusual in that it works with deterministic machines. Maybe LLMs mark the point where we join our engineering peers in a world on non-determinism.

                ❄                ❄                ❄                ❄

I’ve often heard, with decent reason, an LLM compared to a junior colleague. But I find LLMs are quite happy to say “all tests green”, yet when I run them, there are failures. If that was a junior engineer’s behavior, how long would it be before H.R. was involved?

                ❄                ❄                ❄                ❄

LLMs create a huge increase in the attack surface of software systems. Simon Willison described the The Lethal Trifecta for AI agents: an agent that combines access to your private data, exposure to untrusted content, and a way to externally communicate (“exfiltration”). That “untrusted content” can come in all sorts of ways, ask it to read a web page, and an attacker can easily put instructions on the website in 1pt white-on-white font to trick the gullible LLM to obtain that private data.

This is particularly serious when it comes to agents acting in a browser. Read an attacker’s web page, and it could trick the agent to go to your bank account in another tab and “buy you a present” by transferring your balance to the kind attacker. Willison’s view is that “the entire concept of an agentic browser extension is fatally flawed and cannot be built safely”.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Web Bot Auth]]></title>
            <link>https://developers.cloudflare.com/bots/reference/bot-verification/web-bot-auth/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055452</guid>
            <description><![CDATA[Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.]]></description>
            <content:encoded><![CDATA[          Web Bot Auth is an authentication method that leverages cryptographic signatures in HTTP messages to verify that a request comes from an automated bot. Web Bot Auth is used as a verification method for verified bots and signed agents.
It relies on two active IETF drafts: a directory draft ↗ allowing the crawler to share their public keys, and a protocol draft ↗ defining how these keys should be used to attach crawler's identity to HTTP requests.
This documentation goes over specific integration within Cloudflare.

You need to generate a signing key which will be used to authenticate your bot's requests.



Generate a unique Ed25519 ↗ private key to sign your requests. This example uses the OpenSSL ↗ genpkey command:
openssl genpkey -algorithm ed25519 -out private-key.pem


Extract your public key.
openssl pkey -in private-key.pem -pubout -out public-key.pem


Convert the public key to JSON Web Key (JWK) using a tool of your choice. This example uses jwker ↗ command line application.
go install github.com/jphastings/jwker/cmd/jwker@latestjwker public-key.pem public-key.jwk


By following these steps, you have generated a private key and a public key, then converted the public key to a JWK.


You need to host a key directory which creates a way for your bot to authenticate its requests to Cloudflare.
This directory should follow the definition from the active IETF draft draft-meunier-http-message-signatures-directory-01 ↗.


Host a key directory at /.well-known/http-message-signatures-directory (note that this is a requirement). This key directory should serve a JSON Web Key Set (JWKS) including the public key derived from your signing key.


Serve the web page over HTTPS (not HTTP).


Calculate the base64 URL-encoded JWK thumbprint ↗ associated with your Ed25519 public key.


Sign your HTTP response using the HTTP message signature specification by attaching one signature per key in your key directory. This ensures no one else can mirror your directory and attempt to register on your behalf. Your response must include the following headers:

Content-Type: This header must have the value application/http-message-signatures-directory+json.
Signature: Construct a Signature header ↗ over your chosen components.
Signature-Input: Construct a Signature-Input header ↗ over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to http-message-signatures-directory.keyidJWK thumbprint of the corresponding key in your directory.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message.


The following example shows the annotated request and response with required headers against https://example.com.
GET /.well-known/http-message-signatures-directory HTTP/1.1Host: example.comAccept: application/http-message-signatures-directory+jsonHTTP/1.1 200 OKContent-Type: application/http-message-signatures-directory+jsonSignature: sig1=:TD5arhV1ved6xtx63cUIFCMONT248cpDeVUAljLgkdozbjMNpJGr/WAx4PzHj+WeG0xMHQF1BOdFLDsfjdjvBA==:Signature-Input: sig1=("@authority");alg="ed25519";keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U";nonce="ZO3/XMEZjrvSnLtAP9M7jK0WGQf3J+pbmQRUpKDhF9/jsNCWqUh2sq+TH4WTX3/GpNoSZUa8eNWMKqxWp2/c2g==";tag="http-message-signatures-directory";created=1750105829;expires=1750105839Cache-Control: max-age=86400{  "keys": [{    "kty": "OKP",    "crv": "Ed25519",    "x": "JrQLj5P_89iXES9-vFgrIy29clF9CC_oPPsw3c5D0bs", // Base64 URL-encoded public key, with no padding  }]}



You can use the Cloudflare-developed http-signature-directory CLI tool ↗ to assist you in validating your directory.

You need to register your bot and its key directory to add your bot to the list of verified bots.

Log in to the Cloudflare dashboard ↗, and select your account and domain.
Go to Manage Account > Configurations.
Go to the Verified Bots tab.
For Verification Method: select Request Signature.
For Validation Instructions: enter the URL of your key directory. You can additionally supply User Agents values (and their match patterns) that will be sent by your bot.
Select Submit.

Cloudflare accepts all valid Ed25519 keys found in your key directory. In the event a key already exists in Cloudflare's registered database, Cloudflare will work with you to supply a new key, or rotate your existing key.


After your bot has been successfully verified, your bot is ready to sign its requests. The signature protocol is defined in draft-meunier-web-bot-auth-architecture-02 ↗

Choose a set of components to sign.
A component is either an HTTP header, or any derived components ↗ in the HTTP Message Signatures specification. Cloudflare recommends the following:

Choose at least the @authority derived component, which represents the domain you are sending requests to. For example, a request to https://example.com will be interpreted to have an @authority of example.com.
Use components that only contain ASCII values. HTTP Message Signature specification disallows non-ASCII characters, which will result in failure to validate your bot's requests.




Calculate the base64 URL-encoded JWK thumbprint ↗ from the public key you registered with Cloudflare.

Construct the three required headers for Web Bot Auth.

Construct a Signature-Input header ↗ over your chosen components. The header must meet the following requirements.

























Required component parameterRequirementtagThis should be equal to web-bot-auth.keyidThis should be equal to the thumbprint computed in step 2.createdThis should be equal to a Unix timestamp associated with when the message was sent by your application.expiresThis should be equal to a Unix timestamp associated with when Cloudflare should no longer attempt to verify the message. A short expires reduces the likelihood of replay attacks, and Cloudflare recommends choosing suitable short-lived intervals.

Construct a Signature header ↗ over your chosen components.

Construct a Signature-Agent header ↗ that points to your key directory. Note that Cloudflare will fail to verify a message if:

The message includes a Signature-Agent header that is not an https://.
The message includes a valid URI but does not enclose it in double quotes. This is due to Signature-Agent being a structured field.
The message has a valid Signature-Agent header, but does not include it in the component list in Signature-Input.


Attach these three headers to your bot's requests.
An example request may look like this:
Signature-Agent: "https://signature-agent.test"Signature-Input: sig2=("@authority" "signature-agent") ;created=1735689600 ;keyid="poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U" ;alg="ed25519" ;expires=1735693200 ;nonce="e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==" ;tag="web-bot-auth"Signature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:


You may wish to refer to the following resources.

Bots FAQs.
Cloudflare blog: Message Signatures are now part of our Verified Bots Program ↗.
Cloudflare blog: Forget IPs: using cryptography to verify bot and agent traffic ↗.
Cloudflare's web-bot-auth library in Rust ↗.
Cloudflare's web-bot-auth npm package in Typescript ↗.
        Resources     API     New to Cloudflare?     Directory     Sponsorships     Open Source     Support     Help Center     System Status     Compliance     GDPR     Company     cloudflare.com     Our team     Careers     Tools     Cloudflare Radar     Speed Test     Is BGP Safe Yet?     RPKI Toolkit     Certificate Transparency     Community     X     Discord     YouTube     GitHub      ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Building your own CLI coding agent with Pydantic-AI]]></title>
            <link>https://martinfowler.com/articles/build-own-coding-agent.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45055439</guid>
            <description><![CDATA[How to build a CLI coding agent]]></description>
            <content:encoded><![CDATA[

The wave of CLI Coding Agents

If you have tried Claude Code, Gemini Code, Open Code or Simon
      Willison’s LLM CLI, you’ve experienced something fundamentally
      different from ChatGPT or Github Copilot. These aren’t just chatbots or
      autocomplete tools - they’re agents that can read your code, run your
      tests, search docs and make changes to your codebase async.

But how do they work? For me the best way to understand how any tool
      works is to try and build it myself. So that’s exactly what we did, and in
      this article I’ll take you through how we built our own CLI Coding Agent
      using the Pydantic-AI framework and the Model Context Protocol (MCP).
      You’ll see not just how to assemble the pieces but why each capability
      matters and how it changes the way you can work with code.

Our implementation leverages AWS Bedrock but with Pydantic-AI you could
      easily use any other mainstream provider or even a fully local LLM.



Why Build When You Can Buy?

Before diving into the technical implementation, let's examine why we
      chose to build our own solution.

The answer became clear very quickly using our custom agent, while
      commercial tools are impressive, they’re built for general use cases. Our
      agent was fully customised to our internal context and all the little
      eccentricities of our specific project. More importantly, building it gave
      us insights into how these systems work and the quality of our own GenAI
      Platform and Dev Tooling.

Think of it like learning to cook. You can eat at restaurants forever
      but understanding how flavours combine and techniques work makes you
      appreciate food differently - and lets you create exactly what you
      want.



The Architecture of Our Development Agent

At a high level, our coding assistant consists of several key
      components:


Core AI Model: Claude from Anthropic accessed through AWS Bedrock 

Pydantic-AI Framework: provides the agent framework and many helpful
        utilities to make our Agent more useful immediately 

MCP Servers: independent processes that give the agent specialised
        tools, MCP is a common standard for defining the servers that contain these
        tools. 

CLI Interface: how users interact with the assistant


The magic happens through the Model Context Protocol (MCP), which
      allows the AI model to use various tools through a standardized interface.
      This architecture makes our assistant highly extensible - we can easily
      add new capabilities by implementing additional MCP servers, but we’re
      getting ahead of ourselves.



Starting Simple: The Foundation

We started by creating a basic project structure and installing the
      necessary dependencies:

uv init
uv add pydantic_ai
uv add boto3


Our primary dependencies include:


pydantic-ai: Framework for building AI agents

boto3: For AWS API interactions


We chose Claude Sonnet 4 from Anthropic (accessed via AWS Bedrock) as
      our foundation model due to its strong code understanding and generation
      capabilities. Here's how we configured it in our main.py:

import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider


bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


if __name__ == "__main__":
  agent.to_cli_sync()


At this stage we already have a fully working CLI with a chat interface
      which we can use as you would a GUI chat interface, which is pretty cool
      for how little code this is! However we can definitely improve upon
      this.



First Capability: Testing!

Instead of running the tests ourselves after each coding iteration why
      not get the agent to do it? Seems simple right?

import subprocess


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


Here we use the same pytest command you would run in the terminal (I’ve
      shortened ours for the article). Now something magical happened. I could
      say “X isn’t working” and the agent would:


1. Run the test suite

2. Identify which specific tests were failing

3. Analyze the error messages

4. Suggest targeted fixes.


The workflow change: Instead of staring at test failures or copy
      pasting terminal outputs into ChatGPT we now give our agent super relevant
      context about any issues in our codebase.

However we noticed our agent sometimes “fixed” failing tests by
      suggesting changes to the tests, not the actual implementation. This led
      to our next addition.



Adding Intelligence: Instructions and intent

We realised we needed to teach our agent a little more about our
      development philosophy and steer it away from bad behaviours.

instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


agent = Agent(
instructions=instructions,
model=model,
)


The workflow change: The agent now understands our values around
      Test Driven Development and minimal changes. It stopped suggesting large
      refactors where a small fix would do (Mostly).

Now while we could continue building everything from absolute scratch
      and tweaking our prompts for days we want to go fast and use some tools
      other people have built - Enter Model Context Protocol (MCP).



The MCP Revolution: Pluggable Capabilities

This is where our agent transformed from a helpful assistant to
      something approaching the commercial CLI agents. The Model Context
      Protocol (MCP) allows us to add sophisticated capabilities by running
      specialized servers.


MCP is an open protocol that standardizes how applications provide
        context to LLMs. Think of MCP like a USB-C port for AI applications.
        Just as USB-C provides a standardized way to connect your devices to
        various peripherals and accessories, MCP provides a standardized way to
        connect AI models to different data sources and tools. 

-- MCP Introduction


We can run these servers as a local process, so no data sharing, where
      we interact with STDIN/STDOUT to keep things simple and local. (More details on tools and MCP)



Sandboxed Python Execution

Using large language models to do calculations or executing arbitrary code they create is not effective and potentially very dangerous! To make our Agent more accurate and safe our first MCP addition was Pydantic Al’s default server for sandboxed Python code execution:

run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)


agent = Agent(
    ...
    mcp_servers=[
        run_python
    ],
)


This gave our agent a sandbox where it could test ideas, prototype
      solutions, and verify its own suggestions.

NOTE: This is very different from running the tests where we need the
      local environment and is intended to be used to make calculations much
      more robust. This is because writing the code to output a number and then
      executing that code is much more reliable and understandable, scalable and
      repeatable than just generating the next token in a calculation. We have
      seen from frontier labs (including their leaked instructions) that this is
      a much better approach.

The workflow change: Doing calculations, even more complex ones,
      became significantly more reliable. This is useful for many things like
      dates, sums, counts etc. It also allows for a rapid iteration cycle of
      simple python code.



Up-to-Date library Documentation

LLMs are mostly trained in batch on historical data this gives a fixed
      cutoff while languages and dependencies continue to change and improve so
      we added Context7 for access to up to date python
      library documentation in LLM consumable format:

context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)


The workflow change: When working with newer libraries or trying to
      use advanced features, the agent could look up current documentation
      rather than relying on potentially outdated training data. This made it
      much more reliable for real-world development work.



AWS MCPs

Since this particular agent was built with an AWS platform in mind, we
      added the AWS Labs MCP servers for comprehensive cloud docs and
      integration:

awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)


The workflow change: Now when I mentioned “Bedrock is timing out”
      or “the model responses are getting truncated,” the agent could directly
      access AWS documentation to help troubleshoot configuration issues. While
      we've only scratched the surface with these two servers, this is the tip
      of the iceberg—the AWS Labs MCP
      collection includes servers for
      CloudWatch metrics, Lambda debugging, IAM policy analysis, and much more.
      Even with just documentation access, cloud debugging became more
      conversational and contextual.



Internet Search for Current Information

Sometimes you need information that's not in any documentation—recent
      Stack Overflow discussions, GitHub issues, or the latest best practices.
      We added general internet search:

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])


The workflow change: When encountering obscure errors or needing to
      understand recent changes in the ecosystem, the agent could search for
      current discussions and solutions. This was particularly valuable for
      debugging deployment issues or understanding breaking changes in
      dependencies.



Structured Problem Solving

One of the most valuable additions was the code reasoning MCP, which
      helps the agent think through complex problems systematically:

code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)


The workflow change: Instead of jumping to solutions, the agent
      would break down complex problems into logical steps, explore alternative
      approaches, and explain its reasoning. This was invaluable for
      architectural decisions and debugging complex issues. I could ask “Why is
      this API call failing intermittently?” and get a structured analysis of
      potential causes rather than just guesses.



Optimising for Reasoning

As we added more sophisticated capabilities, we noticed that reasoning
      and analysis tasks often took much longer than regular text
      generation—especially when the output wasn't correctly formatted on the
      first try. We adjusted our Bedrock configuration to be more patient:

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)


The workflow change: The longer timeouts meant our agent could work
      through complex problems without timing out. When analyzing large
      codebases or reasoning through intricate architectural decisions, the
      agent could take the time needed to provide thorough, well-reasoned
      responses rather than rushing to incomplete solutions.



Desktop Commander: Warning! With great power comes great responsibility!

At this point, our agent was already quite capable—it could reason
      through problems, execute code, search for information, and access AWS
      documentation. This MCP server transforms your agent from a helpful
      assistant into something that can actually do things in your development
      environment:

desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)


Desktop Commander provides an incredibly comprehensive toolkit: file
      system operations (read, write, search), terminal command execution with
      process management, surgical code editing with edit_block, and even
      interactive REPL sessions. It's built on top of the MCP Filesystem Server
      but adds crucial capabilities like search-and-replace editing and
      intelligent process control.

The workflow change: This is where everything came together. I
      could now say “The authentication tests are failing, please fix the issue”
      and the agent would:


1. Run the test suite to see the specific failures

2. Read the failing test files to understand what was expected

3. Examine the authentication module code

4. Search the codebase for related patterns

5. Look up the documentation for the relevant library

6. Make edits to fix the implementation

7. Re-run the tests to verify the fix

8. Search for similar patterns elsewhere that might need updating


All of this happened in a single conversation thread, with the agent
      maintaining context throughout. It wasn't just generating code
      suggestions—it was actively debugging, editing, and verifying fixes like a
      pair programming partner.

The security model is thoughtful too, with configurable allowed
      directories, blocked commands, and proper permission boundaries. You can
      learn more about its extensive capabilities at the Desktop Commander
      documentation.



The Complete System

Here's our final agent configuration:

import asyncio


import subprocess
import boto3
from pydantic_ai import Agent
from pydantic_ai.mcp import MCPServerStdio
from pydantic_ai.models.bedrock import BedrockConverseModel
from pydantic_ai.providers.bedrock import BedrockProvider
from botocore.config import Config as BotocoreConfig

bedrock_config = BotocoreConfig(
    read_timeout=300,
    connect_timeout=60,
    retries={"max_attempts": 3},
)
bedrock_client = boto3.client(
    "bedrock-runtime", region_name="eu-central-1", config=bedrock_config
)
model = BedrockConverseModel(
    "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    provider=BedrockProvider(bedrock_client=bedrock_client),
)
agent = Agent(
    model=model,
)


instructions = """
You are a specialised agent for maintaining and developing the XXXXXX codebase.

## Development Guidelines:

1. **Test Failures:**
   - When tests fail, fix the implementation first, not the tests
   - Tests represent expected behavior; implementation should conform to tests
   - Only modify tests if they clearly don't match specifications

2. **Code Changes:**
   - Make the smallest possible changes to fix issues
   - Focus on fixing the specific problem rather than rewriting large portions
   - Add unit tests for all new functionality before implementing it

3. **Best Practices:**
   - Keep functions small with a single responsibility
   - Implement proper error handling with appropriate exceptions
   - Be mindful of configuration dependencies in tests

Remember to examine test failure messages carefully to understand the root cause before making any changes.
"""


run_python = MCPServerStdio(
    "deno",
    args=[
        "run",
        "-N",
        "-R=node_modules",
        "-W=node_modules",
        "--node-modules-dir=auto",
        "jsr:@pydantic/mcp-run-python",
        "stdio",
    ],
)

internet_search = MCPServerStdio(command="uvx", args=["duckduckgo-mcp-server"])
code_reasoning = MCPServerStdio(
    command="npx",
    args=["-y", "@mettamatt/code-reasoning"],
    tool_prefix="code_reasoning",
)
desktop_commander = MCPServerStdio(
    command="npx",
    args=["-y", "@wonderwhy-er/desktop-commander"],
    tool_prefix="desktop_commander",
)
awslabs = MCPServerStdio(
    command="uvx",
    args=["awslabs.core-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR"},
    tool_prefix="awslabs",
)
aws_docs = MCPServerStdio(
    command="uvx",
    args=["awslabs.aws-documentation-mcp-server@latest"],
    env={"FASTMCP_LOG_LEVEL": "ERROR", "AWS_DOCUMENTATION_PARTITION": "aws"},
    tool_prefix="aws_docs",
)
context7 = MCPServerStdio(
    command="npx", args=["-y", "@upstash/context7-mcp"], tool_prefix="context"
)

agent = Agent(
    instructions=instructions,
    model=model,
    mcp_servers=[
        run_python,
        internet_search,
        code_reasoning,
        context7,
        awslabs,
        aws_docs,
        desktop_commander,
    ],
)


@agent.tool_plain()
def run_unit_tests() -> str:
    """Run unit tests using uv."""
    result = subprocess.run(
        ["uv", "run", "pytest", "-xvs", "tests/"], capture_output=True, text=True
    )
    return result.stdout


async def main():
    async with agent.run_mcp_servers():
        await agent.to_cli()


if __name__ == "__main__":
    asyncio.run(main())


How it changes our workflow:


Debugging becomes collaborative: you have an intelligent partner
        that can analyze error messages, suggest hypotheses, and help test
        solutions.

Learning accelerates: when working with unfamiliar libraries or
        patterns, the agent can explain existing code, suggest improvements, and
        teach you why certain approaches work better.

Context switching reduces: rather than jumping between
        documentation, Stack Overflow, AWS Console, and your IDE, you have a
        single interface that can access all these resources while maintaining
        context about your specific problem.

Problem-solving becomes structured: rather than jumping to
        solutions, the agent can break down complex issues into logical steps,
        explore alternatives, and explain its reasoning. Like having a real life talking rubber duck!

Code review improves: the agent can review your changes, spot
        potential issues, and suggest improvements before you commit—like having a
        senior developer looking over your shoulder.




What We Learned About CLI Agents

Building our own agent revealed several insights about this emerging
      paradigm:


MCP is (almost) all you need: the magic isn't in any single
        capability, but in how they work together. The agent that can run tests,
        read files, search documentation, execute code, access AWS services, and
        reason through problems systematically becomes qualitatively different
        from one that can only do any single task.

Current information is crucial: having access to real-time search
        and up-to-date documentation makes the agent much more reliable for
        real-world development work where training data might be outdated.

Structured thinking matters: the code reasoning capability
        transforms the agent from a clever autocomplete into a thinking partner
        that can break down complex problems and explore alternative
        solutions.

Context is king: commercial agents like Claude Code are impressive
        partly because they maintain context across all these different tools.
        Your agent needs to remember what it learned from the test run when it's
        making file changes.

Specialisation matters: our agent works better for our specific
        codebase than general-purpose tools because it understands our patterns,
        conventions, and tool preferences. If it falls short in any area then we
        can go and make the required changes.




The Road Ahead

The CLI agent paradigm is still evolving rapidly. Some areas we're
      exploring:


AWS-specific tooling: the AWS Labs MCP servers
        (https://awslabs.github.io/mcp/) provide incredible depth for cloud-native
        development—from CloudWatch metrics to Lambda debugging to IAM policy
        analysis.

Workflow Enhancements: teaching the agent our common development
        workflows so it can handle routine tasks end-to-end. Connecting the agent
        to our project management tools so it can understand priorities and
        coordinate with team processes.

Benchmarking: Terminal Bench
        looks like a great dataset and leaderboard to test this toy agent against
        the big boys!




Why This Matters

CLI coding agents represent a fundamental
      shift from AI as a writing assistant to AI as a development partner.
      Unlike Copilot's autocomplete or ChatGPT's Q&A, these agents can:


Understand your entire project context

Execute tasks across multiple tools

Maintain state across complex workflows

Learn from your specific codebase and patterns


Building one yourself—even a simple version—gives you insights into
      where this technology is heading and how to make the most of commercial
      tools when they arrive.

The future of software development isn't just about writing code
      faster. It's about having an intelligent partner that understands your
      goals, your constraints, and your codebase well enough to help you think
      through problems and implement solutions collaboratively.

And the best way to understand that future? Build it yourself.



]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Uncertain<T>]]></title>
            <link>https://nshipster.com/uncertainty/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45054703</guid>
            <description><![CDATA[GPS coordinates aren’t exact. Sensor readings have noise. User behavior is probabilistic. Yet we write code that pretends uncertainty doesn’t exist, forcing messy real-world data through clean Boolean logic.]]></description>
            <content:encoded><![CDATA[
              You know what’s wrong with people?
                They’re too sure of themselves.
              Better to be wrong and own it than be right with caveats.
                Hard to build a personal brand out of nuance these days.
                People are attracted to confidence — however misplaced.
              But can you blame them? (People, that is)
                Working in software,
                the most annoying part of reaching Senior level
                is having to say “it depends” all the time.
                Much more fun getting to say
                “let’s ship it and iterate” as Staff or
                “that won’t scale” as a Principal.
              Yet, for all of our intellectual humility,
                why do we write vibe code like this?
              if currentLocation.distance(to: target) < 100 {
    print("You've arrived!") // But have you, really? 🤨
}

              GPS coordinates aren’t exact.
                They’re noisy. They’re approximate. They’re probabilistic.
                That horizontalAccuracy property tucked away in your CLLocation object
              is trying to tell you something important:
              you’re probably within that radius.
              Probably.
            A Bool, meanwhile, can be only true or false.
              That if statement needs to make a choice one way or another,
              but code like this doesn’t capture the uncertainty of the situation.
              If truth is light,
              then current programming models collapse the wavefunction too early.
            
              Picking the Right Abstraction
            In 2014, researchers at the University of Washington and Microsoft Research
              proposed a radical idea:
              What if uncertainty were encoded directly into the type system?
              Their paper,
              Uncertain<T>: A First-Order Type for Uncertain Data
              introduced a probabilistic programming approach that’s both
              mathematically rigorous and surprisingly practical.
            
            As you’d expect for something from Microsoft in the 2010s,
              the paper is implemented in C#.
              But the concepts translate beautifully to Swift.
            You can find my port on GitHub:
            import Uncertain
import CoreLocation

let uncertainLocation = Uncertain<CLLocation>.from(currentLocation)
let nearbyEvidence = uncertainLocation.distance(to: target) < 100
if nearbyEvidence.probability(exceeds: 0.95) {
    print("You've arrived!") // With 2σ confidence 🤓
}

            When you compare two Uncertain values,
              you don’t get a definitive true or false.
              You get an Uncertain<Bool> that represents the probability of the comparison being true.
            
            The same is true for other operators, too:
            // How fast did we run around the track?
let distance: Double = 400 // meters
let time: Uncertain<Double> = .normal(mean: 60, standardDeviation: 5.0) // seconds
let runningSpeed = distance / time // Uncertain<Double>

// How much air resistance?
let airDensity: Uncertain<Double> = .normal(mean: 1.225, standardDeviation: 0.1) // kg/m³
let dragCoefficient: Uncertain<Double> = .kumaraswamy(alpha: 9, beta: 3) // slightly right-skewed distribution
let frontalArea: Uncertain<Double> = .normal(mean: 0.45, standardDeviation: 0.05) // m²
let airResistance = 0.5 * airDensity * frontalArea * dragCoefficient * (runningSpeed * runningSpeed)

            This code builds a computation graph,
              sampling only when you ask for concrete results.
              The library uses
              Sequential Probability Ratio Testing (SPRT)
              to efficiently determine how many samples are needed —
              maybe a few dozen times for simple comparisons,
              scaling up automatically for complex calculations.
            // Sampling happens only when we need to evaluate
if ~(runningSpeed > 6.0) {
    print("Great pace for a 400m sprint!")
}
// SPRT might only need a dozen samples for this simple comparison

let sustainableFor5K = (runningSpeed < 6.0) && (airResistance < 50.0)
print("Can sustain for 5K: \(sustainableFor5K.probability(exceeds: 0.9))")
// Might use 100+ samples for this compound condition

            Using an abstraction like Uncertain<T> forces you to deal with uncertainty as a first-class concept
              rather than pretending it doesn’t exist.
              And in doing so, you end up with much smarter code.
            To quote Alan Kay:
            
              Point of view is worth 80 IQ points
                
            
            Before we dive deeper into probability distributions,
              let’s take a detour to Monaco and talk about
              Monte Carlo sampling.
            
              The Monte Carlo Method
            Behold, a classic slot machine (or “fruit machine” for our UK readers 🇬🇧):
            enum SlotMachine {
    static func spin() -> Int {
        let symbols = [
            "◻️", "◻️", "◻️",  // blanks
            "🍒", "🍋", "🍊", "🍇", "💎"
        ]

        // Spin three reels independently
        let reel1 = symbols.randomElement()!
        let reel2 = symbols.randomElement()!
        let reel3 = symbols.randomElement()!

        switch (reel1, reel2, reel3) {
        case ("💎", "💎", "💎"): return 100  // Jackpot!
        case ("🍒", "🍒", "🍒"): return 10
        case ("🍇", "🍇", "🍇"): return 5
        case ("🍊", "🍊", "🍊"): return 3
        case ("🍋", "🍋", "🍋"): return 2
        case ("🍒", _, _), // Any cherry
             (_, "🍒", _),
             (_, _, "🍒"):
            return 1
        default:
            return 0  // Better luck next time
        }
    }
}

            Should we play it?
            
            Now, we could work out these probabilities analytically —
              counting combinations,
              calculating conditional probabilities,
              maybe even busting out some combinatorics.
            Or we could just let the computer pull the lever a bunch and see what happens.
            
            let expectedPayout = Uncertain<Int> {
    SlotMachine.spin()
}.expectedValue(sampleCount: 10_000)
print("Expected value per spin: $\(expectedPayout)")
// Expected value per spin: ≈ $0.56

            At least we know one thing for certain:
              The house always wins.
            
              Beyond Simple Distributions
            While one-armed bandits demonstrate pure randomness,
              real-world applications often deal with more predictable uncertainty.
            Uncertain<T> provides a
              rich set of probability distributions:
            // Modeling sensor noise
let rawGyroData = 0.85  // rad/s
let gyroReading = Uncertain.normal(
    mean: rawGyroData,
    standardDeviation: 0.05  // Typical gyroscope noise in rad/s
)

// User behavior modeling
let userWillTapButton = Uncertain.bernoulli(probability: 0.3)

// Network latency with long tail
let apiResponseTime = Uncertain.exponential(rate: 0.1)

// Coffee shop visit times (bimodal: morning rush + afternoon break)
let morningRush = Uncertain.normal(mean: 8.5, standardDeviation: 0.5)  // 8:30 AM
let afternoonBreak = Uncertain.normal(mean: 15.0, standardDeviation: 0.8)  // 3:00 PM
let visitTime = Uncertain.mixture(
    of: [morningRush, afternoonBreak],
    weights: [0.6, 0.4]  // Slightly prefer morning coffee
)

            
          Uncertain<T> also provides comprehensive
            statistical operations:
          // Basic statistics
let temperature = Uncertain.normal(mean: 23.0, standardDeviation: 1.0)
let avgTemp = temperature.expectedValue() // about 23°C
let tempSpread = temperature.standardDeviation() // about 1°C

// Confidence intervals
let (lower, upper) = temperature.confidenceInterval(0.95)
print("95% of temperatures between \(lower)°C and \(upper)°C")

// Distribution shape analysis
let networkDelay = Uncertain.exponential(rate: 0.1)
let skew = networkDelay.skewness() // right skew
let kurt = networkDelay.kurtosis() // heavy tail

// Working with discrete distributions
let diceRoll = Uncertain.categorical([1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1])!
diceRoll.entropy()  // Randomness measure (~2.57)
(diceRoll + diceRoll).mode() // Most frequent outcome (7, perhaps?)

// Cumulative probability
if temperature.cdf(at: 25.0) < 0.2 {  // P(temp ≤ 25°C) < 20%
    print("Unlikely to be 25°C or cooler")
}

          The statistics are computed through sampling.
            The number of samples is configurable, letting you trade computation time for accuracy.
          
            Putting Theory to Practice
          Users don’t notice when things work correctly,
            but they definitely notice impossible behavior.
            When your running app claims they just sprinted at 45 mph,
            or your IRL meetup app shows someone 500 feet away when GPS accuracy is ±1000 meters,
            that’s a bad look 🤡
          So where do we go from here?
            Let’s channel our Senior+ memes from before for guidance.
          That Staff engineer saying “let’s ship it and iterate”
            is right about the incremental approach.
            You can migrate uncertain calculations piecemeal
            rather than rewriting everything at once:
          extension CLLocation {
    var uncertain: Uncertain<CLLocation> {
        Uncertain<CLLocation>.from(self)
    }
}

// Gradually migrate critical paths
let isNearby = (
    currentLocation.uncertain.distance(to: destination) < threshold
).probability(exceeds: 0.68)

          And we should consider the Principal engineer’s warning of “that won’t scale”.
            Sampling has a cost, and you should understand the
            computational overhead for probabilistic accuracy:
          // Fast approximation for UI updates
let quickEstimate = speed.probability(
    exceeds: walkingSpeed,
    maxSamples: 100
)

// High precision for critical decisions
let preciseResult = speed.probability(
    exceeds: walkingSpeed,
    confidenceLevel: 0.99,
    maxSamples: 10_000
)

          
          Start small.
            Pick one feature where GPS glitches cause user complaints.
            Replace your distance calculations with uncertain versions.
            Measure the impact.
          Remember:
            the goal isn’t to eliminate uncertainty —
            it’s to acknowledge that it exists and handle it gracefully.
            Because in the real world,
            nothing is certain except uncertainty itself.
          And perhaps,
            with better tools,
            we can finally stop pretending otherwise.
        ]]></content:encoded>
        </item>
    </channel>
</rss>