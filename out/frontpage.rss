<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 11 Sep 2025 07:09:25 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[The Four Fallacies of Modern AI]]></title>
            <link>https://blog.apiad.net/p/the-four-fallacies-of-modern-ai</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45207008</guid>
            <description><![CDATA[And Why Believing in Them Hinders Progress]]></description>
            <content:encoded><![CDATA[I've spent the last few years trying to make sense of the noise around Artificial Intelligence, and if there's one feeling that defines the experience, it's whiplash. One week, I'm reading a paper that promises AI will cure disease and unlock unimaginable abundance; the next, I'm seeing headlines about civilizational collapse. This dizzying cycle of AI springs, periods of massive investment and hype, followed by the chilling doubt of AI winters isn't new. It's been the engine of the field for decades.After years of this, I've had to develop my own framework just to stay grounded. It’s not about being an optimist or a pessimist; it’s about rejecting both extremes. For me, it’s a commitment to a tireless reevaluation of the technology in front of us; to using reason and evidence to find a path forward, because I believe we have both the power and the responsibility to shape this technology’s future. That begins with a clear-eyed diagnosis of the present.One of the most useful diagnostic tools I've found for this comes from computer scientist Melanie Mitchell. In a seminal paper back in 2021, she identified what she claims are four foundational fallacies, four deeply embedded assumptions that explain to a large extent our collective confusion about AI, and what it can and cannot do.My goal in this article isn't to convince you that Mitchell is 100% right. I don't think she is, either, and I will provide my own criticism and counter arguments to some points. What I want is to use her ideas as a lens to dissect the hype, explore the counterarguments, and show why this intellectual tug-of-war has real-world consequences for our society, our economy, and our safety.For me, the most important test of any idea is its empirical validation. No plan, no matter how brilliant, survives its first encounter with reality. I find that Mitchell’s four fallacies are the perfect tool for this. They allow us to take the grand, sweeping claims made about AI and rigorously test them against the messy, complicated reality of what these systems can actually do.The most common and seductive fallacy is the assumption that every impressive feat of narrow AI is an incremental step on a smooth path toward human-level Artificial General Intelligence (AGI). That is, that intelligence is a single, unidimensional metric on a continuum that goes from narrow to general.We see this everywhere. When IBM's Deep Blue beat Garry Kasparov at chess, it was hailed as a first step towards AGI. The same narrative emerged when DeepMind's AlphaGo defeated Lee Sedol. This way of thinking creates, according to Mitchell, a flawed map of progress, tricking us into believing we are much closer to AGI than we are. It ignores the colossal, unsolved challenge known as the commonsense knowledge problem—the vast, implicit understanding of the world that humans use to navigate reality.As philosopher Hubert Dreyfus famously said, this is like claiming that the first monkey that climbed a tree was making progress towards landing on the moon. Well, in a sense, maybe it is, but you get the point. We didn't get to the moon until we invented combustion rockets. Climbing ever taller trees gets us nowhere closer, it's just a distraction. In the same sense, mastering a closed-system game may be a fundamentally different challenge than understanding the open, ambiguous world.But here's the nuance. While beating Kasparov isn't a direct step to having a conversation, the methods developed can be surprisingly generalizable. The architecture that powered AlphaGo was later adapted into MuZero, a system that mastered Go, chess, and Atari games without being told the rules. Furthermore, can we really call a Large Language Model narrow in the same way? Its ability to write code and summarize text feels like a qualitative leap in generality that the monkey-and-moon analogy doesn't quite capture.This leaves us with a forward-looking question: How do recent advances in multimodality and agentic AI test the boundaries of this fallacy? Does a model that can see and act begin to bridge the gap toward common sense, or is it just a more sophisticated version of the same narrow intelligence? Are world models a true step towards AGI or just a higher branch in a tree of narrow linguistic intelligence?We have a terrible habit of projecting our own cognitive landscape onto machines, assuming that what's hard for us is hard for them, and what's easy for us is easy for them. For decades, the opposite has been true.This is Moravec's Paradox, named after the roboticist Hans Moravec, who noted it's easier to make a computer exhibit adult-level performance on an IQ test than to give it the sensory and motor skills of a one-year-old.This explains why we have AI that can master the ridiculously complex game of Go, while a fully self-driving car remains stubbornly just over the horizon. The "easy" things are built on what Mitchell calls the "invisible complexity of the mundane." This paradox causes a chronic mis-calibration of our progress and priorities, leading us to be overly impressed by performance in formal domains while underestimating the staggering difficulty of the real world.Of course, some would argue this isn't a fundamental barrier, but a temporary engineering hurdle. They’d say that with enough data and compute, the "invisible complexity" of the real world can be learned, just like the complexity of Go was.From this perspective, the problem isn't one of kind, but of scale. This forces us to ask: as sensor technology and robotics improve, are we finally starting to overcome Moravec's Paradox? Or are we just discovering even deeper layers of complexity we never knew existed?Language doesn't just describe reality; it creates it. In AI, we constantly use anthropomorphic shorthand, saying a system "learns," "understands," or has "goals." Mitchell argues this practice of using "wishful mnemonics" is deeply misleading, fooling not just the public but the researchers themselves.When a benchmark is called the "General Language Understanding Evaluation" (GLUE) and a model surpasses the human baseline, headlines declare that AI now understands language better than humans. But does it?The term "stochastic parrot" was coined as a powerful antidote, reframing what LLMs do as sophisticated mimicry rather than comprehension. This isn't just a semantic game, Mitchell argues; it creates a flawed mental model that leads to misplaced trust, encouraging us to deploy systems in high-stakes situations where a lack of true understanding can have serious consequences.A fair critique is that these terms are a necessary cognitive shorthand. At a certain level of complexity, a system's emergent behavior becomes functionally indistinguishable from "understanding," and arguing about whether it really understands is an unprovable philosophical distraction.But that still leaves a crucial question: can we develop a more precise, less anthropomorphic vocabulary to describe AI capabilities? Or is our human-centric language the only tool we have to reason about these new forms of intelligence, with all the baggage that entails?This is the most philosophical, and in my opinion, the most important fallacy. It's the deep-seated assumption that intelligence is, like software, a form of pure information processing that can be separated from its body.This "brain-as-computer" metaphor leads to the belief that AGI is simply a matter of scaling up compute to match the brain's raw processing power. It's challenged by Mitchell and many others with the thesis of embodied cognition, a view from cognitive science which holds that intelligence is inextricably linked to having a body that interacts with the world. If this is correct, then our current approach may just be creating ever-more-sophisticated systems that are fundamentally brittle because they lack grounded understanding.This is where we hit the great intellectual battle line in modern AI. The primary counterargument can be framed in terms of Rich Sutton's famous essay, "The Bitter Lesson," which argues that the entire history of AI has taught us that attempts to build in human-like cognitive structures (like embodiment) are always eventually outperformed by general methods that just leverage massive-scale computation.From this viewpoint, embodiment isn't a magical prerequisite for intelligence; it's just another fiendishly complex problem that will yield to more data and processing power.This tension poses a critical question for the future: do multimodal models that can process images and text represent a meaningful step toward solving the embodiment problem? Or are they just a more sophisticated version of the same disembodied mind, a brain in a slightly larger digital vat?As we dig into these fallacies, a deeper pattern emerges. They aren't just four isolated mistakes; they're symptoms of a fundamental schism in how the AI world thinks about intelligence itself. Again, my goal isn't to pick a side but to avoid falling prey to cheap heuristics or ideological banners, and instead evaluate which of these paradigms gives us a more useful map of reality.On one side, you have what I’ll call the Cognitive Paradigm, championed by thinkers like Mitchell and her mentor, superstar AI researcher and philosopher Douglas Hofstadter. This view sees intelligence as a complex, integrated, and embodied phenomenon. It assumes that the things we associate with human intelligence—common sense, emotions, values, a sense of self—are likely inseparable components of the whole, emerging from rich interaction with a physical and social world.From this perspective, the path to AGI requires a deep, scientific understanding of these integrated components, not just more processing power.On the other side is the Computationalist Paradigm, which is the implicit philosophy behind many of today's leading labs, and best captured by The Bitter Lesson. This posits that the biggest breakthroughs have always come from general methods that leverage massive-scale computation—in other words, from scaling things up.In this paradigm, intelligence is a more abstract, substrate-independent quality of optimization. Problems like embodiment aren't fundamental barriers; they are just incredibly complex computational tasks that will eventually be solved by ever-larger models and ever-faster chips.Of course, it's not a perfect binary. Most researchers are pragmatists, like me, working somewhere in the messy middle. But these two paradigms represent the poles of the debate, and the tension between them defines the entire field. It shapes which research gets funded, which systems get built, and ultimately, which vision of the future we are collectively racing toward.This debate isn't just an academic parlor game. These fallacies have a massive ripple effect across society because they obscure a fundamental rule of technology and economics: there's no free lunch, only trade-offs.The hype generated by fallacious thinking isn't just an innocent mistake; it's the fuel for a powerful economic engine. The intense competition between tech giants, the flood of venture capital, and the geopolitical AI race all depend on a constant narrative of imminent, world-changing breakthroughs. This political economy of hype forces us into a series of dangerous trade-offs.First, we trade long-term progress for short-term hype.The fallacies create an unstable, boom-and-bust funding cycle. During an AI spring, capital flows to projects that can produce impressive-looking demos, often based on narrow benchmarks. This starves the slow, methodical, foundational research needed to solve the hard problems like common sense and reasoning. The result is a field that lurches from one hype bubble to the next, leaving a trail of abandoned projects and unfulfilled promises that trigger the inevitable AI winter.Second, we trade public trust for market excitement.The cycle of over-promising and under-delivering is deeply corrosive. When we use wishful mnemonics to describe a system that "understands," and it then fails in spectacular, nonsensical ways in the real world, it breeds public anxiety and skepticism. Recent studies show the public perceives AI scientists more negatively than almost any other field, specifically because of a perceived lack of prudence. This isn't a vague feeling; it's a direct reaction to the unintended consequences of deploying brittle, overhyped systems.Finally, and most critically, we trade responsible validation for speed to market.This is where the consequences become most severe. Believing a system is on a continuum with general intelligence, or that it truly "understands" language, leads to its premature deployment in high-stakes domains.When a mental health chatbot, which is fundamentally, at least today, a sophisticated pattern-matcher, gives harmful advice to a person in crisis, it’s a direct result of these fallacies. When we over-rely on brittle systems in healthcare, finance, or autonomous vehicles, we are making a dangerous bet, trading real-world safety for the illusion of progress.So where does this leave us? The value of Mitchell's fallacies isn't just in spotting hype, but in exposing the deep, productive tension between these two powerful ways of thinking about intelligence. We can't ignore the fallacies, but we also can't deny the incredible, world-altering power of the scaling paradigm that fuels them.Mitchell in her paper compares modern AI to alchemy. It produces dazzling, impressive results but it often lacks a deep, foundational theory of intelligence.It’s a powerful metaphor, but I think a more pragmatic conclusion is slightly different. The challenge isn't to abandon our powerful alchemy in search of a pure science of intelligence. The goal, at least from a pragmatist point of view, should be to infuse our current alchemy with the principles of science, to make scaling smarter, safer, and more grounded by integrating the hard-won insights about how intelligence actually works.The path forward, I believe, requires more than just intellectual humility. It also requires a willingness to synthesize these seemingly opposed worldviews, and a commitment to a tireless reevaluation of the technology before us. The ultimate question is not if we should choose the path of scaling or the path of cognitive science, but how we can weave them together to guide the raw power of our modern AI alchemy with the deep understanding of a true science of intelligence.No posts]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Court rejects Verizon claim that selling location data without consent is legal]]></title>
            <link>https://arstechnica.com/tech-policy/2025/09/court-rejects-verizon-claim-that-selling-location-data-without-consent-is-legal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45206567</guid>
            <description><![CDATA[Verizon and T-Mobile lost but AT&T beat the FCC. SCOTUS may have to step in.]]></description>
            <content:encoded><![CDATA[
          
          
Instead of providing notice to customers and obtaining or verifying customer consent itself, Verizon "largely delegated those functions via contract," the court said. This system and its shortcomings were revealed in 2018 when "the New York Times published an article reporting security breaches involving Verizon's (and other major carriers') location-based services program," the court said.
Securus Technologies, a provider of communications services to correctional facilities, "was misusing the program to enable law enforcement officers to access location data without customers' knowledge or consent, so long as the officers uploaded a warrant or some other legal authorization," the ruling said. A Missouri sheriff "was able to access customer data with no legal process at all" because Securus did not review the documents that law enforcement uploaded.
Verizon claimed that Section 222 of the Communications Act covers only call-location data, as opposed to device location data. The court disagreed, pointing to the law's text stating that customer proprietary network information includes data that is related to the location of a telecommunications service, and which is made available to the carrier "solely by virtue of the carrier-customer relationship."
"Device-location data comfortably satisfies both conditions," the court said.
Verizon chose to pay fine, giving up right to jury trial
As for Verizon's claim that the FCC violated its right to a jury trial, the court said that "Verizon could have gotten such a trial" if it had "declined to pay the forfeiture and preserved its opportunity for a de novo jury trial if the government sought to collect." Instead, Verizon chose to pay the fine "and seek immediate review in our Court."
By contrast, the 5th Circuit decision in AT&T's favor said the FCC "acted as prosecutor, jury, and judge," violating the right to a jury trial. The 5th Circuit said it was guided by the Supreme Court's June 2024 ruling in Securities and Exchange Commission v. Jarkesy, which held that "when the SEC seeks civil penalties against a defendant for securities fraud, the Seventh Amendment entitles the defendant to a jury trial."
The 2nd Circuit ruling said there are key differences between US telecom law and the securities laws considered in Jarkesy. It's because of those differences that Verizon had the option of declining to pay the penalty and preserving its right to a jury trial, the court said.
In the Jarkesy case, the problem "was that the SEC could 'siphon' its securities fraud claims away from Article III courts and compel payment without a jury trial," the 2nd Circuit panel said. "The FCC's forfeiture order, however, does not, by itself, compel payment. The government needs to initiate a collection action to do that. Against this backdrop, the agency's proceedings before a § 504(a) trial create no Seventh Amendment injury."


          
                  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Where did the Smurfs get their hats (2018)]]></title>
            <link>https://www.pipelinecomics.com/beginning-bd-smurfs-hats-origin/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45206311</guid>
            <description><![CDATA[What's that hat the Smurfs wear? And what's it covering up? Do Smurfs have luxurious locks of hair? Are they bald? And, most crucial of all, is it the wrong hat?!?]]></description>
            <content:encoded><![CDATA[
	
	
That white floppy hat on top of every Smurf’s head.



What is it?  Where did it come from?



Do Smurfs have bald spots?



We’ve got a history lesson coming up here, folks.  Sit back.



But, first, let’s answer a side question:



What’s Under a Smurf’s Hat?



There’s only one true source for information on this question, and that’s Peyo’s comics.  Accept no substitutes. Yes, there are probably moments in various movies and cartoons where a Smurf can been seen without their hat on.  That’s not Peyo.



Peyo showed us one example of a Smurf without a hat.  Go to the classic story, “The Purple Smurf.”  At one point, Papa Smurf’s lab blows up, along with his hat.







Papa Smurf is bald. This doesn’t necessarily mean that Brainy Smurf isn’t sporting a pompadour under his hat or that Hefty Smurf doesn’t have a buzz cut, but…







We know Smurfs are capable of having hair.  Smurfette has luxurious blonde locks, but she’s also not really a natural born Smurf.  Technically, she’s a concoction of Gargamel’s.  But let’s give his spell-making the benefit of the doubt.  Maybe Smurfs can have hair.



I mean, where do Smurfs come from?  What’s the genetics at work that would determine–



–wait, no, I’m not going down that rabbit hole.



Papa Smurf is definitely bald.  I bet you already assumed that, though. Let’s move on.



Where Do Smurfs Get Such Wonderful Hats?



It’s called a Phrygian cap, which is about as much fun to type as Vercingetorix, who is an amazing story for another day.



The headgear is over 2000 years old.  Here’s proof:



By Jastrow (2006), Public Domain, https://commons.wikimedia.org/w/index.php?curid=647031



Handsome fella, isn’t he?  But check out that hat!  He’s totally a Smurf, right?



His name is Attis, and this sculpture comes from somewhere in the 100s AD, though Attis lived even further back, in the 4th century BC.



In that time period, you know who else worse a Smurfs hat?  King Midas.  Yes the guy with the thing for gold. (Croesus is also involved in this story, but let’s not get completely off topic.)



Phrygis, in case your curious, is the name of an ancient group of people who lived in the Balkans region of eastern Europe — Greece, Turkey, Romania, etc.  Their language and culture went extinct by the 5th century AD. Near the end, the Romans thought of them as being lazy and dull.



Those Phrygian caps do kind of resemble a dunce cap, don’t they?  It’s completely unrelated, though.  That’s a dead end.



Liberty and Freedom



The hat is often associated with liberty and freedom. Why?



It was adopted during the French Revolution (at the end of the 18th century AD) as “the red cap of liberty” by the revolutionaries.



You can see one such cap on the French personification of “Liberté”.  Here she is:







Looks just like Papa Smurf’s hat.  And while Peyo was Belgian, he was working with French publishers, so drawing inspiration from a French symbol isn’t too crazy.



The thing that Peyo maybe didn’t realize and the French revolutionaries definitely didn’t realize, though, is that it’s the wrong hat.



The Right Hat







This is the pileus hat.  It’s a brimless hat, often made of felt or maybe wool.  It started in Ancient Greece as a sailor’s hat, and eventually found its way to Ancient Rome, too.



In Rome, a freed slave had his head shaved.  Then, they would wear a pileus, in part to keep their head warm.  The hat was a sign of the slave’s freedom/liberty.



Somewhere along the line in the French Revolution, they adopted the freed slaves’ head gear as their own symbol of freedom, but picked the wrong one.



They didn’t have Google Images back then, so don’t be too hard on them.



I’m sure Peyo picked that hat because it looked good on the Smurfs, helped further set them apart from all the other characters, and would someday make for an awesome free giveaway at Angouleme.



Side By Side



Wikipedia has an image of the Phrygian and Pileus next to each other on the bottom shelf here:



By MisterPlus65 – Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=46040581



On the bottom shelf, the Smurfs’ Phrygian cap on the left is next to the pileus in the center there.



Yes, there were metal versions of both that were used as head gear during war times.  Imagine an army of Smurfs coming over the hill to attack!



Glad You Asked?



To sum it all up: Some Greeks had a hat that the Romans borrowed and that their slaves used to represent their freedom.  2000 years later, some French revolutionaries confused that hat for a different one from the Phrygis folks and made it their own sign of freedom.



150 years after that, Peyo created the Smurfs and gave them that hat, but in white.



Here endeth the lesson.





				
		Augie De Blieck Jr.Comic fan since 1989.  Letterhack from 1991.  Pipeline writer since 1997.
		
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DOOMscrolling: The Game]]></title>
            <link>https://ironicsans.ghost.io/doomscrolling-the-game/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45205232</guid>
            <description><![CDATA[Can a game work where all you do is scroll?]]></description>
            <content:encoded><![CDATA[
        We’re all familiar with doomscrolling, spending too much time scrolling endless feeds of content that make you feel bad about everything.But sometimes when I hear the word “doomscrolling” it makes me think of two other things: the classic video game Doom and, well, scrolling.That got me wondering if I could make a Doom-inspired game in a web browser where the only thing you do to play is scroll. No moving your character laterally, no jumping. Just scrolling.So I made it. And it’s fun! Here’s a small clip:That’s what the game looks like. But here’s what playing it feels like:You can go play it right now on desktop or mobile. The rest of this newsletter is the non-technical story of how I made the game.The first time was a failureAs readers know, I’m not a coder, but I enjoy how vibe coding lets me turn an idea into something real. So naturally, I turned to vibe coding for this.It didn’t work.This was around nine months ago. I tried and tried, but none of the LLMs were able to help me go from idea to a playable game at all. Like, not even close. GPT-4 absolutely refused to understand that scrolling down a page means that the background should move up the page. I ended up with something kinda pathetic that didn’t even resemble a game. So I gave up, figuring this was either beyond an LLM’s skills, beyond my skills, or both.But then GPT-5 came out a few weeks ago, and I tried again to see how much better it might be at coding. In just two hours I had a very good prototype. I even made a little title screen for my prototype so it felt more like a real game:I described the game design to ChatGPT very broadly at first. I said it should work kinda like Galaga turned upside-down. But I explained that unlike Galaga, the player moves forward and backward rather than side to side, and that the monster’s position should remain relative to the floor. That and a few more details got me surprisingly far as a first step.For prototyping purposes, I asked ChatGPT to just come up with five different monsters, each one with a two-frame animation, like Space Invaders aliens. They were little more than basic shapes, and I figured at some point I’d replace them with actual pre-rendered monster sprites. But this worked for now.The original 5 monstersThen I went on vacation. I spent an hour or two each morning over coffee working on this game until the kids woke up, gradually adding and refining features one at a time.Improving the gameI focused on making the game more fun to play, with incentives to keep moving but also things to stop you from racing through it too fast. I added things like weapon upgrades for every 100 monsters you kill, a wall of fire that chases you if you stay in one place too long, and obstacles to slow you down, like brick walls and spider webs.Don’t let the wall of fire get youSome other little quality-of-life features I came up with include:Five different background textures so you can have a different visual experience each time you play.Health potions. The first one appears when you’re down to three health points. After that, they are more rare and may require weapon upgrades to reach.A visual marker when you pass your record distanceA pause screen with some statsMaking it really DoomscrollingI was pretty happy with the game and ready to share it. But then at the last minute I got another nagging idea in the back of my mind: What if it was somehow more like actual doomscrolling?It would be easy to get an RSS Feed of headlines from a news site. Could I make them appear in the game as you scroll, in a way that felt integrated with the game?First I tried having headlines just appear on the floor as you play, but it felt very tacked-on. So I came up with some lore for the game that helped.I decided that the game takes place in the future, in a lair that was sealed up today, whatever day you happen to be playing the game. And the civilization that sealed up the cave left behind plaques featuring today’s headlines for some unexplained reason. So as you play, you encounter these plaques that each has a bit of news from when the cave was sealed up. Today’s actual news.It’s not really doomscrolling if there isn’t awful news to readThe plaques have no effect on the game, except to the extent that they tempt you to read them and get distracted from the gameplay. But they’re just decorative elements. Feel free to ignore them. If you can.The headlines all come from the New York Times front page RSS feed. So in a sense, this game is actually an extremely complex single-feed RSS reader.Working with AI is still a pain. This was my solution.If you’ve ever tried to work with AI, you’ve likely run into a roadblock where you’re describing something over and over and it’s simply not getting it. “No, don’t do it that way, for the umpteenth time, do it this way.”I still planned on making the monsters as pre-rendered sprites, but the background textures, plaques, and decorative items like torches could still be rendered in-game if I could just get GPT-5 to understand what I want them to look like. An LLM isn’t really good at making artwork like this.So I simplified things. I had the AI set up simple “labs,” standalone test pages where we could work on different designs, using the style from the game. For example, here’s one “lab” I made for testing how some in-world elements would look on different backgrounds:Everything above is rendered on the fly. One big advantage of that approach is that I could randomize some visual elements in the game. For example, look at the spider webs above. They all follow the same rules but they’re all slightly different. The game’s background textures are also slightly different each time you play.Next, I set about making pre-rendered monsters. But wow, small-scale pixel art is hard. I went through a lot of versions of different monsters. Eventually, I had a few I felt were good enough. The game looked like this:It had its own charm, but in the end, I didn’t love it. Somehow, my pre-rendered monsters made the game feel worse. Maybe it’s because I just am not a good pixel artist.So I decided to see what I could do with ChatGPT in a “lab” like I did for other in-game items, but focused on monster designs. After a lot of experimentation, I settled on the simple monsters that ended up in the game:I assume doing all this computationally is more processor-intensive than using pre-rendered monsters, but it’s very smooth for me on both desktop and phone, so it must not be too intensive. I guess I’ll hear from people if it’s choppy on their device.Sometimes I still needed a little more control over how something looked. So in those cases I had ChatGPT build labs with sliders that I could adjust to decide how I want things to appear, instead of getting frustrated with the chatbot. This way I could quickly settle on a look and feel.Here for example is the lab page for the plaques. I wanted them to look integrated into the game world, so I described the parameters I wanted to play with for the text styling and plaque itself. We put a “copy settings” button that I could use to report back to the LLM once I liked it, so I could say “Okay, let’s go with these settings in the game.”I’ve made this lab page live for you to play with if you’re curious.Ship itThere are still features and adjustments I’d like to add, but I’m not on vacation anymore, so think I just need to stop here. I may still tweak it, but for now I’m calling version 1.0 ready to ship. It has successfully scratched my itch to see if I could make a fun scrolling-only game, which was really all I wanted.It should play equally well on mobile and desktop. The only difference is that with a taller device you can see more of the world at a time, which makes it a little easier.Oh, and you can save it to your home screen so it acts like a standalone app rather than a browser game. That’s how I play.Happy Doomscrolling!And that’s it for another newsletter!Here’s where I remind you that you can support my endeavors by becoming a paid member, or making a one-time donation. Every little bit matters.And if not, that’s okay, too. But maybe you can share the newsletter? Moving from Beehiiv to Ghost resulted in a subscriber hit, which I anticipated because the same thing happened when I first left Substack. It took a while to begin growing again after the first move, and you can help me get back to positive growth by spreading the word now that I’ve landed at Ghost. It would mean the world to me.Until next time, thanks as always for reading!David
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Intel's E2200 "Mount Morgan" IPU at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/intels-e2200-mount-morgan-ipu-at</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204838</guid>
            <description><![CDATA[Intel’s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities.]]></description>
            <content:encoded><![CDATA[Intel’s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities. IPUs take things a step further, aiming to take on a wide variety of infrastructure services in a cloud environment in addition to traditional software defined networking functions. Infrastructure services are run by the cloud operator and orchestrate tasks like provisioning VMs or collecting metrics. They won’t stress a modern server CPU, but every CPU core set aside for those tasks is one that can’t be rented out to customers. Offloading infrastructure workloads also provides an extra layer of isolation between a cloud provider’s code and customer workloads. If a cloud provider rents out bare metal servers, running infrastructure services within the server may not even be an option.Intel’s incoming “Mount Morgan” IPU packs a variety of highly configurable accelerators alongside general purpose CPU cores, and aims to capture as many infrastructure tasks as possible. It shares those characteristics with its predecessor, “Mount Evans”. Flexibility is the name of the game with these IPUs, which can appear as a particularly capable network card to up to four host servers, or run standalone to act as a small server. Compared to Mount Evans, Mount Morgan packs more general purpose compute power, improved accelerators, and more off-chip bandwidth to support the whole package.Intel includes a set of Arm cores in their IPU, because CPUs are the ultimate word in programmability. They run Linux and let the IPU handle a wide range of infrastructure services, and ensure the IPU stays relevant as infrastructure requirements change. Mount Morgan’s compute complex gets an upgrade to 24 Arm Neoverse N2 cores, up from 16 Neoverse N1 cores in Mount Evans. Intel didn’t disclose the exact core configuration, but Mount Evans set its Neoverse N1 cores up with 512 KB L2 caches and ran them at 2.5 GHz. It’s not the fastest Neoverse N1 configuration around, but it’s still nothing to sneeze at. Mount Morgan of course takes things further. Neoverse N1 is a 5-wide out-of-order core with a 160 entry ROB, ample execution resources, and a very capable branch predictor. Each core is already a substantial upgrade over Neoverse N1. 24 Neoverse N2 cores would be enough to handle some production server workloads, let alone a collection of infrastructure services.Mount Morgan gets a memory subsystem upgrade to quad channel LPDDR5-6400 to feed the more powerful compute complex. Mount Evans had a triple channel LPDDR4X-4267 setup, connected to 48 GB of onboard memory capacity. If Intel keeps the same memory capacity per channel, Mount Morgan would have 64 GB of onboard memory. Assuming Intel’s presentation refers to 16-bit LPDDR4/5(X) channels, Mount Morgan would have 51.2 GB/s of DRAM bandwidth compared to 25.6 GB/s in Mount Evans. Those figures would be doubled if Intel refers to 32-bit data buses to LPDDR chips, rather than channels. A 32 MB System Level Cache helps reduce pressure on the memory controllers. Intel didn’t increase the cache’s capacity compared to the last generation, so 32 MB likely strikes a good balance between hitrate and die area requirements. The System Level Cache is truly system level, meaning it services the IPU’s various hardware acceleration blocks in addition to the CPU cores.A Lookaside Crypto and Compression Engine (LCE) sits within the compute complex, and shares lineage with Intel’s Quickassist (QAT) accelerator line. Intel says the LCE features a number of upgrades over QAT targeted towards IPU use cases. But perhaps the most notable upgrade is getting asymmetric crypto support, which was conspicuously missing from Mount Evans’s LCE block. Asymmetric cryptography algorithms like RSA and ECDHE are used in TLS handshakes, and aren’t accelerated by special instructions on many server CPUs. Therefore, asymmetric crypto can consume significant CPU power when a server handles many connections per second. It was a compelling use case for QAT, and it’s great to see Mount Morgan get that as well. The LCE block also supports symmetric crypto and compression algorithms, capabilities inherited from QAT.A programmable DMA engine in the LCE lets cloud providers move data as part of hardware accelerated workflows. Intel gives an example workflow for accessing remote storage, where the LCE helps move, compress, and encrypt data. Other accelerator blocks located in the IPU’s network subsystem help complete the process.Networking bandwidth and offloads are a core function of the IPU, and its importance can’t be understated. Cloud servers need high network and storage bandwidth. The two are often two sides of the same coin, because cloud providers might use separate storage servers accessed over datacenter networking. Mount Morgan has 400 Gbps of Ethernet throughput, double Mount Evans’s 200 Gbps.True to its smart NIC lineage, Mount Morgan uses a large number of inline accelerators to handle cloud networking tasks. A programmable P4-based packet processing pipeline, called the FXP, sits at the heart of the network subsystem. P4 is a packet processing language that lets developers express how they want packets handled. Hardware blocks within the FXP pipeline closely match P4 demands. A parser decodes packet headers and translates the packet into a representation understood by downstream stages. Downstream stages can check for exact or wildcard matches. Longest prefix matches can be carried out in hardware too, which is useful for routing.The FXP can handle a packet every cycle, and can be configured to perform multiple passes per packet. Intel gives an example where one pass processes outer packet layers to perform decapsulation and checks against access control lists. A second pass can look at the inner packet, and carry out connection tracking or implement firewall rules.An inline crypto block sits within the network subsystem as well. Unlike the LCE in the compute complex, this crypto block is dedicated to packet processing and focuses on symmetric cryptography. It includes its own packet parsers, letting it terminate IPSec and PSP connections and carry out IPSec/PSP functions like anti-replay window protection, sequence number generation, and error checking in hardware. IPSec is used for VPN connections, which are vital for letting customers connect to cloud services. PSP is Google’s protocol for encrypting data transfers internal to Google’s cloud. Compared to Mount Evans, the crypto block’s throughput has been doubled to support 400 Gbps, and supports 64 million flows.Cloud providers have to handle customer network traffic while ensuring fairness. Customers only pay for a provisioned amount of network bandwidth. Furthermore, customer traffic can’t be allowed to monopolize the network and cause problems with infrastructure services. The IPU has a traffic shaper block, letting it carry out quality of service measures completely in hardware. One mode uses a mutli-level hierarchical scheduler to arbitrate between packets based on source port, destination port, and traffic class. Another “timing wheel” mode does per-flow packet pacing, which can be controlled by classification rules set up at the FXP. Intel says the timing wheel mode gives a pacing resolution of 512 nanoseconds per slot.RDMA traffic accounts for a significant portion of datacenter traffic. For example, Azure says RDMA accounts for 70% of intra-cloud network traffic, and is used for disk IO. Mount Morgan has a RDMA transport option to provide hardware offload for that traffic. It can support two million queue pairs across multiple hosts, and can expose 1K virtual functions per host. The latter should let a cloud provider directly expose RDMA acceleration capabilities to VMs. To ensure reliable transport, the RDMA transport engine supports the Falcon and Swift transport protocols. Both protocols offer improvements over TCP, and Intel implements congestion control for those protocols completely in hardware. To reduce latency, the RDMA block can bypass the packet processing pipeline and handle RDMA connections on its own.All of the accelerator blocks above are clients of the system level cache. Some hardware acceleration use cases, like connection tracking with millions of flows, can have significant memory footprints. The system level cache should let the IPU keep frequently accessed portions of accelerator memory structures on-chip, reducing DRAM bandwidth needs.Mount Morgan’s PCIe capabilities have grown far beyond what a normal network card may offer. It has 32 PCIe Gen 5 lanes, providing more IO bandwidth than some recent desktop CPUs. It’s also a huge upgrade over the 16 PCIe Gen 4 lanes in Mount Evans.Traditionally, a network card sits downstream of a host, and thus appears as a device attached to a server. The host fabric and PCIe subsystem is flexible to let the IPU wear many hats. It can appear as a downstream device to up to four server hosts, each of which sees the IPU as a separate, independent device. Mount Evans supported this “multi-host” mode as well, but Mount Morgan’s higher PCIe bandwidth is necessary to utilize its 400 Gigabit networking.Mount Morgan can run in a “headless” mode, where it acts as a standalone server and a lightweight alternative to dedicating a traditional server to infrastructure tasks. In this mode, Mount Morgan’s 32 PCIe lanes can let it connect to many SSDs and other devices. The IPU’s accelerators as well as the PCIe lanes appear downstream of the IPU’s CPU cores, which act as a host CPU.A “converged” mode can use some PCIe lanes to connect to upstream server hosts, while other lanes connect to downstream devices. In this mode, the IPU shows up as a PCIe switch to connected hosts, with downstream devices visible behind it. A server could connect to SSDs and GPUs through the IPU. The IPU’s CPU cores can sit on top of the PCIe switch and access downstream devices, or can be exposed as a downstream device behind the PCIe switch.The IPU’s multiple modes are a showcase of IO flexibility. It’s a bit like how AMD uses the same die as an IO die within the CPU and a part of the motherboard chipset on AM4 platforms. The IO die’s PCIe lanes can connect to downstream devices when it’s serving within the CPU, or be split between an upstream host and downstream devices when used in the chipset. Intel is also no stranger to PCIe configurability. Their early QAT PCIe cards reused their Lewisburg chipset, exposing it as a downstream device with three QAT devices appearing behind a PCIe switch.Cloud computing plays a huge role in the tech world today. It originally started with commodity hardware, with similar server configurations to what customers might deploy in on-premise environments. But as cloud computing expanded, cloud providers started to see use cases for cloud-specific hardware accelerators. Examples include "Nitro" cards in Amazon Web Services, or smart NICs with FPGAs in Microsoft Azure. Intel has no doubt seen this trend, and IPUs are the company's answer.Mount Morgan tries to service all kinds of cloud acceleration needs by packing an incredible number of highly configurable accelerators, in recognition of cloud providers’ diverse and changing needs. Hardware acceleration always runs the danger of becoming obsolete as protocols change. Intel tries to avoid this by having very generalized accelerators, like the FXP, as well as packing in CPU cores that can run just about anything under the sun. The latter feels like overkill for infrastructure tasks, and could let the IPU remain relevant even if some acceleration capabilities become obsolete.At a higher level, IPUs like Mount Morgan show that Intel still has ambitions to stretch beyond its core CPU market. Developing Mount Morgan must have been a complex endeavor. It’s a showcase of Intel’s engineering capability even when their CPU side goes through a bit of a rough spot. It’ll be interesting to see whether Intel’s IPUs can gain ground in the cloud market, especially with providers that have already developed in-house hardware offload capabilities tailored to their requirements.If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KDE launches its own distribution]]></title>
            <link>https://lwn.net/SubscriberLink/1037166/caa6979c16a99c9e/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204393</guid>
            <description><![CDATA[At Akademy 2025, the KDE Project released an alpha version of KDE Linux, a distribution built b [...]]]></description>
            <content:encoded><![CDATA[

At Akademy 2025, the
KDE Project released an
alpha version of KDE Linux, a
distribution built by the project to "include the best
implementation of everything KDE has to offer, using the most advanced
technologies". It is aimed at providing an operating system
suitable for home use, business use, OEM installations, and more
"eventually". For now there are many rough edges and missing
features that users should be aware of before taking the plunge; but
it is an interesting look at the kind of complete Linux system that
KDE developers would like to see.

Development and goals

KDE contributor Nate Graham wrote an announcement
blog post on September 6 to accompany the release of KDE
Linux. Harald Sitter had introduced the project as "Project Banana"
during a talk (video, slides)
at Akademy in 2024, and has
been leading its development along with major contributions from Hadi Chokr, Lasath Fernando,
Justin Zobel, Graham, and others.

KDE Linux is an immutable distribution that uses Arch Linux
packages as its base, but Graham notes that it is "definitely not
an 'Arch-based distro!'" Pacman is not included, and Arch is used
only for the base operating system. Everything else, he said, is
either compiled from source using KDE Builder or installed
using Flatpak.

Some may wonder why another Linux distribution is needed; Graham
said that he has expressed that sentiment himself in the past
regarding other distributions, but he thinks that KDE Linux is justified:


KDE is a huge producer of software. It's awkward for us to not have
our own method of distributing it. Yes, KDE produces source code that
others distribute, but we self-distribute our apps on app stores like
Flathub and the Snap and Microsoft stores, so I think it's a natural
thing for us to have our own platform for doing that distribution too,
and that's an operating system. I think all the major producers of
free software desktop environments should have their own OS, and many
already do: Linux Mint and ElementaryOS spring to mind, and GNOME is working on one too.

Besides, this matter was settled 10 years ago with the creation of
KDE neon, our first bite at the "in-house OS" apple. The sky did not
fall; everything was beautiful and nothing hurt.


Speaking of neon, Graham points
out that it is "being held together by a heroic volunteer"
(singular) and that no decision has been made as of yet about its
future. Neon has "served admirably for a decade", he said, but
it "has somewhat reached its limit in terms of what we can do with
it" because of its Ubuntu base. According to the wiki
page, neon's Ubuntu LTS base is built on old technology and
requires "a lot of packaging busywork". It also becomes less
stable as time goes on, "because it needs to be tinkered with to
get Plasma to build on it, breaking the LTS promise".

Architecture and plans

KDE Linux, on the other hand, is designed to be a greenfield
project that allows KDE to make use of newer technologies and more
modern approaches to a Linux distribution unhampered by the needs of a
general-purpose distribution. If KDE Linux's technology choices are
not appealing, Graham says, "feel free to ignore KDE Linux and
continue using the operating system of your choice. There are plenty
of them!"

KDE Linux is Wayland-only; there is no X.org session and no plan
to add one. Users with some of the older NVIDIA cards will need to
manually
configure the system to work properly with KDE Linux. The
distribution also only supports UEFI systems, and there are no plans
to add support for BIOS-only systems.

The root filesystem (/) is a read/write Btrfs
volume, while /usr is a read-only Enhanced
Read-Only File System (EROFS) volume backed by a single file. The
system is updated atomically by swapping out the EROFS volume;
currently KDE Linux caches up to five of the files to allow users to
roll back to previous versions if the most recent updates are
broken.

The files have names like kde-linux_202509082242.erofs and
are stored in /system. The most recent releases are about
4.8GB in size. The project uses systemd-sysupdate
under the hood, which does not have
support for delta updates yet. Users should expect to set aside at least 30GB
just to cache the EROFS files for now.

Unlike Fedora's image-based Atomic Desktops,
KDE Linux does not supply a way for users to add packages to the base
system. So, for example, users have no way to add packages with
additional kernel modules. Users can add applications packaged as 
Flatpaks using KDE's Discover graphical software manager; the 
Snap format is also supported, but it is not integrated with
Discover—the snap command-line
utility can be used to do install Snaps for now. KDE Linux also includes Distrobox, which allows users to set
up a container with the distribution of their choice and install
software in the container that is integrated with the system. LWN touched on Distrobox in
our coverage of the Bluefin image-based operating system in December
2023.

Unfortunately, it looks
like users are not set up correctly for Podman, which Distrobox
needs, on KDE Linux; trying to set up a new container gives a
"potentially insufficient UIDs or GIDs available in user namespace"
error when trying to test Distrobox on the latest KDE Linux build. This
comment in the Podman repository on GitHub set me on the right
path to fix the problem. This kind of bug is to be expected in an
alpha release; no doubt it will be ironed out in the coming weeks or
months.

System updates are also performed using Discover: when a new system
image is available, it will show up in the Updates tab and can be
installed from there. (Or using "sudo updatectl update" from
the command line, for those who prefer doing it that way.) Likewise,
installed Flatpaks with updates will show up in the Updates tab. For
now, at least, users will have to manually manage any applications
installed in a Distrobox container.

The default software selection is a good start for a desktop
distribution; it includes the Gwenview image viewer, Okular document
viewer, Haruna media player, Kate text editor, and Konsole for
terminal emulation. Firefox is the only prominent non-KDE application
included with the default install. The base system currently includes
GNU Bash 5.3.3, curl 8.15, Linux 6.16.5, GCC 15.2.1, Perl 5.42, Python 3.13.7, Vim
9.1, and wget 1.25. It does not include some utilities users might
want or expect, such as GNU Screen, Emacs, tmux, pip, or alternate shells like Fish.







KDE Linux's base packages are not meant to be user-customizable,
but it should be possible to create custom images using systemd's mkosi tool, which is what is used
by the project itself. The mkosi.conf.d
directory in the KDE Linux repository contains the various
configuration files for managing the packages included in the system image.


No AI slop, all substance: subscribe to LWN today

LWN has always been about quality over quantity; we need your help
to continue publishing in-depth, reader-focused articles about Linux
and the free-software community. Please subscribe today to support our work
and keep LWN on the air; we are offering a free one-month trial subscription to get you started.



Development and what's next

The plan, longer term, is to have three editions of KDE Linux: the
testing edition, which is what is available now, an enthusiast
edition, and a stable edition. The testing edition is meant for
developers and quality assurance folks; it is to be built daily from
Git and to be similar in quality to KDE neon's unstable release. The
enthusiast edition will include beta or released software, depending
on the status of a given application at the time; this edition is
aimed at "KDE enthusiasts, power users, and influencers". The
stable edition, as the name suggests, will include only released
software that meets quality metrics (which are not yet defined),
indicating it's ready for users not in the other categories.

KDE Linux can be installed
on bare metal or in a virtual
machine using virt-manager. Support
for UEFI Secure Boot is currently missing. Since KDE Linux uses a lot
of space for cached images, users should provision more disk space for
a virtual machine than they might ordinarily; I allocated 50GB, but
probably should have gone with 75GB or more.

Those wishing to follow along with KDE Linux development can check
out the milestone trackers for the enthusiast
and stable
editions. All of the milestones
have been reached for the testing edition. There are quite a few items
to complete before KDE Linux reaches beta status; for example, the
project is currently using packages from the Arch User Repository (AUR) but
the plan is to move
away from using AUR soon. The project also needs to move
production to official KDE infrastructure rather than depending on
Sitter's personal systems.

At the moment, the project does not have a security announcement mailing
list or other notification mechanism; those using KDE Linux for more
than testing should keep an eye on Arch's security tracker and
KDE security advisories.
Since KDE Linux is an immutable derivative of Arch Linux, with no
way to immediately pull updated Arch packages, users should remember
that they will be at a disadvantage when there are security
vulnerabilities in the base operating system. Any security update would
need to be created by Arch Linux, pushed out as an Arch package, and
then incorporated into a build for KDE Linux. Conservatively, that
will add at least a day for any security updates to reach KDE Linux
users.

One of the downsides of having no package manager is that there is
no easy way to take stock of what is installed on the system. Normally,
one might do an inventory of software using a package manager's query
tools; a quick "rpm -qa" shows all of the system software on
my desktop's Fedora 42 install. There is no such mechanism for
KDE Linux, and it's not clear that there are any plans for that type
of feature long term. To be suitable for some of the target audiences,
KDE Linux will need (for example) ways to manage the base operating
system and easily query what is installed.

The project's governance is described
as a "'Council of elders' model with major contributors being
the elders". Sitter has final decision-making authority in cases
of disagreement.

Obviously the team working on KDE Linux wants the project to
succeed, but it has put some thought into what will happen if the
distribution is put out to pasture at some point. There is an end-of-life
contingency plan to "push a final update shipping an OS image
that transforms the system into a completely different
distro". The successor distribution has not been chosen yet; it 
would be picked based on the KDE Linux team's relationship with the other
distribution and its ability to take on all of the new users.

Part of the rationale for KDE Linux is to satisfy an impulse that
is common to many open-source developers: the desire to ship software
directly to users without an intermediary tampering with it.
The process of creating and refining KDE Linux will satisfy
that for KDE developers, but it may also serve another purpose: to
demonstrate just how difficult it is to create and maintain a
desktop distribution for widespread use. Whether KDE Linux succeeds as a
standalone distribution or not, it may be a useful exercise to
illustrate why projects like Debian, Fedora, openSUSE, Ubuntu,
and others make choices that ultimately frustrate application
developers.


               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fraudulent Publishing in the Mathematical Sciences]]></title>
            <link>https://arxiv.org/abs/2509.07257</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45203935</guid>
            <description><![CDATA[This report is the first of two publications of a joint Working Group of the International Mathematical Union (IMU) and the International Council of Industrial and Applied Mathematics (ICIAM). In it, we shall analyze the current state of publishing in the mathematical sciences and explain the resulting problems. Our second publication will offer concrete recommendations, guidelines, and best practices for researchers, policymakers, and evaluators of mathematical research. It will explain how to detect and counteract attempts to game bibliometric measures, empowering the community to reclaim control over research evaluation and drive necessary change.]]></description>
            <content:encoded><![CDATA[
    
    
                
    View PDF
    HTML (experimental)
            Abstract:This report is the first of two publications of a joint Working Group of the International Mathematical Union (IMU) and the International Council of Industrial and Applied Mathematics (ICIAM). In it, we shall analyze the current state of publishing in the mathematical sciences and explain the resulting problems. Our second publication will offer concrete recommendations, guidelines, and best practices for researchers, policymakers, and evaluators of mathematical research. It will explain how to detect and counteract attempts to game bibliometric measures, empowering the community to reclaim control over research evaluation and drive necessary change.
    

    
    
              
          Comments:
          Full version with complete clickable references. A print version with a selection of the most important references appeared in the October 2025 issue of the Notices of the AMS
        

          Subjects:
          
            History and Overview (math.HO)
        
          Cite as:
          arXiv:2509.07257 [math.HO]
        
        
           
          (or 
              arXiv:2509.07257v1 [math.HO] for this version)
          
        
        
           
                        https://doi.org/10.48550/arXiv.2509.07257
              
                                arXiv-issued DOI via DataCite (pending registration)
            
          
        
    
  
      Submission history From: Ilka Agricola [view email]          [v1]
        Mon, 8 Sep 2025 22:19:40 UTC (34 KB)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mux (YC W16) Is Hiring Engineering ICs and Managers]]></title>
            <link>https://mux.com/jobs</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45203643</guid>
            <description><![CDATA[Mux is video for developers. Our mission is to democratize video by solving the hard problems developers face when building video. Video is a huge part of people’s lives, and we want to help make it better.]]></description>
            <content:encoded><![CDATA[Mux is video for developers. Our mission is to democratize video by solving the hard problems developers face when building video: video encoding and streaming (Mux Video), video monitoring (Mux Data), and more. Video is a huge part of people’s lives, and we want to help make it better.We’re committed to building a healthy team that welcomes a diverse range of backgrounds and experiences. We want people who care about our mission, are ready to grow, believe in our values (from Be Human to Turn Customers Into Fans), and want to make the people around them better.You’ll be joining a tight-knit team with experience at places like Google, YouTube, Twitch, Zencoder, Fastly, and more. Our founders previously started (and sold) Zencoder, an early leader in cloud video technology, and authored Video.js, the biggest HTML5 video player on the web. We organize Demuxed, the premiere conference for video engineers in the world.We’re backed by top investors like Coatue, Accel, Andreessen Horowitz, and Y Combinator. You’ll get to work with amazing companies: hundreds of startups, plus Reddit, Vimeo, Robinhood, CBSi, Discovery, PBS, and TED. Customers large and small love working with us and love our team.We are building something big together. We’d love to hear from you!]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Haystack – Review pull requests like you wrote them yourself]]></title>
            <link>https://haystackeditor.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45201703</guid>
        </item>
        <item>
            <title><![CDATA[Defeating Nondeterminism in LLM Inference]]></title>
            <link>https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200925</guid>
            <description><![CDATA[Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).]]></description>
            <content:encoded><![CDATA[
    
    
    
    Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).
But why aren’t LLM inference engines deterministic? One common hypothesis is that some combination of floating-point non-associativity and concurrent execution leads to nondeterminism based on which concurrent core finishes first. We will call this the “concurrency + floating point” hypothesis for LLM inference nondeterminism. For example, a recent arXiv preprint writes:

Floating-point arithmetic in GPUs exhibits non-associativity, meaning $(a + b) + c \neq a + (b + c)$ due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.


You can also find the “concurrency + floating point” hypothesis repeated by others, like here (“There are speed tradeoffs, and in order to make the endpoints fast GPUs are used, which do parallel [nondeterministic] calculations. Any modern GPU neural net calculations will be subject to these."), or here (“Because GPUs are highly parallelized, the ordering of additions or multiplications might be different on each execution, which can cascade into small differences in output.").
While this hypothesis is not entirely wrong, it doesn’t reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. We’re definitely using floating-point numbers. And our GPU definitely has a lot of concurrency. Why don’t we see nondeterminism in this test?
A = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
B = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
ref = torch.mm(A, B)
for _ in range(1000):
    assert (torch.mm(A, B) - ref).abs().max().item() == 0
To understand the true cause of LLM inference nondeterminism, we must look deeper.
Unfortunately, even defining what it means for LLM inference to be deterministic is difficult. Perhaps confusingly, the following statements are all simultaneously true:

Some kernels on GPUs are nondeterministic.
However, all the kernels used in a language model’s forward pass are deterministic.
Moreover, the forward pass of an LLM inference server (like vLLM) can also be claimed to be deterministic.
Nevertheless, from the perspective of anybody using the inference server, the results are nondeterministic.

In this post, we will explain why the “concurrency + floating point” hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.
The original sin: floating-point non-associativity
Before talking about nondeterminism, it’s useful to explain why there are numerical differences at all. After all, we typically think of machine learning models as mathematical functions following structural rules such as commutativity or associativity. Shouldn’t there be a “mathematically correct” result that our machine learning libraries should provide us?
The culprit is floating-point non-associativity. That is, with floating-point numbers:
$$ (a + b) + c \neq a + (b + c) $$(0.1 + 1e20) - 1e20
>>> 0
0.1 + (1e20 - 1e20)
>>> 0.1
Ironically, breaking associativity is what makes floating-point numbers useful.
Floating-point numbers are useful because they allow for a “dynamic” level of precision. For the purposes of explanation, we will use base 10 (instead of binary), where floating-point numbers are in the format $\text{mantissa} * 10^\text{exponent}$. We will also use 3 digits for the mantissa and 1 digit for the exponent.
For example, for the value 3450, we can represent it exactly as $3.45 * 10^3$. We can also represent much smaller values like 0.486 as $4.86 * 10^{-1}$. In this way, floating point allows us to represent both very small as well as very large values. In the sciences, we might say that floating point allows us to maintain a constant number of “significant figures”.
If you add together two floating-point numbers with the same exponent, it looks similar to integer addition. For example, 123 ($1.23 * 10^2$) + 456 ($4.56 * 10^2$) results in 579 ($5.79 * 10^2$).
But what happens when we add two floating-point numbers with different exponents, such as 1230 and 23.4?  In this case, the exact result is 1253.4. However, we can only maintain 3 digits of precision at a time. Floating-point addition will thus drop the last 2 digits and obtain the value $1.25 * 10^3$ (or 1250).


        
            1.23 × 10²
        
        +
        
            3.45 × 10¹
        
        =
        
            1.575 × 10²
            Exact: 1575
        
    



We require 3 digits of precision to represent 1230 and 3 digits of precision to represent 23.4. However, adding these 2 numbers together results in a number that requires 5 digits of precision to represent (1253.4). Our floating-point format must then drop the 34 off the end. In some sense, we have effectively rounded our original 23.4 to 20.0 before adding it. 

At this point, however, we’ve destroyed information. Note that this can happen every time we add two floating-point numbers with different “scales” (i.e. different exponents). And adding together floating-point numbers with different exponents happens all of the time. In fact, if we could guarantee that we never needed different exponents, we could just use integers!
In other words, every time we add together floating-point numbers in a different order, we can get a completely different result. To take an extreme example, there are 102 possible different results for summing this array depending on the order.
import random

vals = [1e-10, 1e-5, 1e-2, 1]
vals = vals + [-v for v in vals]

results = []
random.seed(42)
for _ in range(10000):
    random.shuffle(vals)
    results.append(sum(vals))

results = sorted(set(results))
print(f"There are {len(results)} unique results: {results}")

# Output:
# There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]
Although this is the underlying cause for non-identical outputs, it does not directly answer where the nondeterminism comes from. It doesn’t help us understand why floating-point values get added in different orders, when that happens, nor how it can be avoided.
The answers lie in how kernels are implemented.
Why don’t kernels always add numbers in the same order?
As mentioned above, one common explanation for why kernels add numbers in different orders is the “concurrency + floating point” hypothesis. The hypothesis states that if the order in which concurrent threads finish is nondeterministic and the accumulation order depends on the order in which concurrent threads finish (such as with an atomic add), our accumulation order will be nondeterministic as well.
Confusingly, although this can lead to nondeterministic kernels, concurrency (and atomic adds) end up being completely uninvolved in LLM inference nondeterminism! To explain what the real culprit is, let’s first understand why modern GPU kernels rarely need atomic adds.
When are atomic adds needed?
Typically a GPU launches a program concurrently across many “cores” (i.e. SMs). As the cores have no inherent synchronization among them, this poses a challenge if the cores need to communicate among each other. For example, if all cores must accumulate to the same element, you can use an “atomic add” (sometimes known as a “fetch-and-add”). The atomic add is “nondeterministic” — the order in which the results accumulate is purely dependent on which core finishes first.
Concretely, imagine that you are reducing a 100-element vector with 100 cores (e.g. torch.sum()). Although you can load all 100 elements in parallel, we must eventually reduce down to a single element. One way to accomplish this is with some kind of “atomic add” primitive, where the hardware guarantees that all additions will be processed but does not guarantee the order.




 The atomic add ensures that every core's contributions will be reflected in the final sum. However, it makes no guarantee about what order the contributions will be added. The order depends entirely on which core finishes first, a nondeterministic property. Thus, executing the same parallel program multiple times can result in nondeterministic outputs. 

This is usually what folks mean by “nondeterminism” — you execute the same kernel twice with exactly the same inputs and you get a different result out. This is known as run-to-run nondeterminism, where you run the same python script twice with the exact same dependencies but get a different result.
Although concurrent atomic adds do make a kernel nondeterministic, atomic adds are not necessary for the vast majority of kernels. In fact, in the typical forward pass of an LLM, there is usually not a single atomic add present.
This may be surprising, given that parallelizing a reduction can benefit from atomic adds. There are two main reasons why atomic adds do not end up being needed.

There is often sufficient parallelism along the “batch” dimension that we don’t need to parallelize along the reduction dimension. For example, let’s say that instead of reducing a single 100-dim vector we were reducing 500 vectors in parallel. In this case, we can reduce an entire vector in each core and allow every core to operate on a different vector.
Over time, most neural network libraries have adopted a variety of strategies for achieving determinism without sacrificing performance. For example, we can perform a “split” (or tree) reduction, where we split the 100-element reduction into five 20-element reductions (thus achieving five-way parallelism). Then, to combine the remaining five elements, we can either perform a separate “clean-up” reduction (which isn’t parallelized, but operates over few enough elements to be cheap) or utilize a semaphore (which ensures that each concurrent thread-block will accumulate in a deterministic order).The semaphore strategy can be found described here.

Due to these two factors, avoiding atomics adds is a negligible performance penalty for the vast majority of neural network operations.
There are still a couple of common operations that have significant performance penalties for avoiding atomics. For example, scatter_add in PyTorch (a[b] += c). The only one commonly used in LLMs, however, is FlashAttention backward.Fun fact: did you know that the widely used Triton implementations of FlashAttention backward actually differ algorithmically from Tri Dao’s FlashAttention-2 paper? The standard Triton implementation does additional recomputation in the backward pass, avoiding atomics but costing 40% more FLOPs!
However, the forward pass of an LLM involves no operations that require atomic adds. Thus, the forward pass in an LLM is in fact “run-to-run deterministic.”











Model

Deterministic



User requests




Other user requests




Output




















































From the perspective of the inference server, it is deterministic. Given the exact same user requests, it will always provide the same deterministic output. 

Wikipedia writes that “a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.” And in this case, given the exact same inputs (i.e. the exact requests the inference server is processing), the forward pass always produces the exact same outputs.
However, the forward pass itself being “deterministic” is not sufficient to ensure that a system that includes it is deterministic. For example, what if our request’s output depended on the parallel user requests (e.g. batch-norm)? Since each individual request has no way of knowing what the parallel requests will be, from their perspective our overall LLM inference is also nondeterministic!
As it turns out, our request’s output does depend on the parallel user requests. Not because we’re somehow leaking information across batches — instead, it’s because our forward pass lacks “batch invariance”, causing our request’s output to depend on the batch size of our forward pass.
Batch invariance and “determinism”
To explain batch invariance, let’s simplify the system and look solely at matmuls. You can assume that all matmul implementations are “run-to-run deterministic."This is not totally true, but most common matmul implementations do have this property.  However, they are not “batch-invariant.” In other words, when the batch size changes, each element in the batch can get different results.
This is a fairly unusual property from a mathematical perspective. Matrix multiplication should be “independent” along every element in the batch — neither the other elements in the batch nor how large the batch is should affect the computation results of a specific element in the batch.
However, as we can observe empirically, this isn’t true.
import torch
torch.set_default_device('cuda') 

B = 2048
D = 4096
a = torch.linspace(-1000, 1000, B*D).reshape(B, D)
b = torch.linspace(-1000, 1000, D*D).reshape(D, D)
# Doing a matrix vector multiplication by taking
# the first element of the batch
out1 = torch.mm(a[:1], b)
# Doing a matrix matrix multiplication and then taking
# the first element of the batch
out2 = torch.mm(a, b)[:1]
print((out1 - out2).abs().max()) # tensor(1669.2500, device='cuda:0')
Note that this is “run-to-run deterministic.” If you run the script multiple times, it will deterministically return the same result.It is not “hardware/software version invariant” — your GPU/PyTorch version may return a different value, but it should deterministically return the same value.
However, when a non-batch-invariant kernel is used as part of a larger inference system, the system can become nondeterministic. When you make a query to an inference endpoint, the amount of load the server is under is effectively “nondeterministic” from the user’s perspective. The load determines the batch size that the kernels are run under, and thus changes the eventual result of each individual request!


















Model

Deterministic
Nondeterministic



User requests




Other user requests




Output


































































Although the inference server itself can be claimed to be "deterministic", the story is different for an individual user. From the perspective of an individual user, the other concurrent users are not an "input" to the system but rather a nondeterministic property of the system. This makes LLM inference "nondeterministic" from the perspective of each user.

If you compose some property under which the kernel is not invariant (i.e. batch-size) with nondeterminism of that property (i.e. the load the server is under), you get a nondeterministic system.
In other words, the primary reason nearly all LLM inference endpoints are nondeterministic is that the load (and thus batch-size) nondeterministically varies! This nondeterminism is not unique to GPUs — LLM inference endpoints served from CPUs or TPUs will also have this source of nondeterminism.
So, if we’d like to avoid nondeterminism in our inference servers, we must achieve batch invariance in our kernels. In order to understand how that can be achieved, let’s first take a look at why kernels don’t have batch invariance in the first place.
How do we make kernels batch-invariant?
In order to make a transformer implementation batch-invariant, we must make every kernel batch-invariant. Luckily, we can assume that every pointwise operation is batch-invariant.Although this is true for all kernels in say, PyTorch, it’s not inherently true. For example, there are some kernel implementations on CPU that will use vectorized intrinsics on some parts of the array and non-vectorized intrinsics on other parts, and these intrinsics don’t necessarily always have bitwise identical numerics. Thus, we only need to worry about the 3 operations that involve reductions — RMSNorm, matrix multiplication, and attention.Reductions related to parallelism are out of the scope of this discussion, but the same principles apply. One factoid that may be useful is that NVLink-Sharp in-switch reductions are deterministic on Blackwell as well as Hopper with CUDA 12.8+. As is the case with many things, this information can be found on NCCL’s github issues
Conveniently, these are also ordered in ascending levels of difficulty. Each one requires some additional considerations to achieve batch invariance with reasonable performance. Let’s talk about RMSNorm first.
Batch-invariant RMSNorm











































Data Parallel RMSNorm Ideally, we'd like to avoid communication between cores in our parallelization strategy. One way to achieve that is by assigning one batch-element to each core, thus guaranteeing that each reduction is done entirely within a single core. This is what's known as a "data-parallel" strategy, since we're simply parallelizing along a dimension that doesn't require communication. In this example, we have four rows and four cores, saturating our cores. 

RMSNorm can be implemented as:
# x: [batch_size, hidden_dim]
# weight: [hidden_dim]
def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
The requirement for batch invariance is that the reduction order for each element must be fixed regardless of the batch-size of the kernel. Note that this doesn’t mean we must always use the same reduction strategy. For example, if we change the number of elements we’re reducing over, we can still be batch-invariant even if our reduction strategy changes.The Quack blog post has some nice examples showing the hierarchy of various reduction strategies you can do (e.g. thread reduction, warp reduction, block reduction, cluster reduction).
Thus, we only break batch invariance when our batch-size affects the reduction strategy.
Let’s look at the standard parallelism strategy for RMSNorm. Generally, parallel algorithms benefit from minimizing communication across cores. For the purpose of this discussion you can assume that when we refer to “cores” we mean SMs. More specifically, the property here that’s important is that the # of threadblocks our kernel launches is greater than the # of SMs. So, one strategy we can start with is to assign each batch element to one core, as seen in the above figure.
Increasing our batch size doesn’t affect our reduction strategy; if a batch size of 200 provides sufficient parallelism to our kernel then a batch size of 2000 will definitely provide sufficient parallelism.



































































Data Parallel RMSNorm for larger batches Extending the data-parallel strategy to larger batches is fairly straightforward --- instead of having each core handle one row you allow each core to handle different rows sequentially. This preserves batch invariance as the reduction strategy for each batch element remains identical. 

On the other hand, decreasing the batch size can pose challenges. Because we assign each batch element to one core, decreasing our batch size will eventually lead to having more cores than batch elements, leaving some cores idle.
Upon encountering this situation, a good kernel engineer would reach for one of the solutions mentioned in the prior section (atomic adds or split reductions), maintaining good parallelism and thus, good performance. Unfortunately, this changes the reduction strategy, preventing this kernel from being batch-invariant.































Split-Reduction RMSNorm If we have a small batch size, our data-parallel strategy may no longer have sufficient parallelism to saturate our cores. In this case, it may be more efficient to "split" a reduction among multiple cores, allowing us to fully utilize our GPU. However, this loses batch invariance, as we are no longer reducing each element in the same order.

The easiest solution is to simply ignore these cases altogether. This is not completely unreasonable — a small batch size means that the kernel is likely to execute quickly anyways, and so a slowdown may not be catastrophic.
If we were compelled to optimize this use case, one approach would be to consistently use a reduction strategy that has enough parallelism even for very small batch sizes. Such a reduction strategy would lead to an excess amount of parallelism for larger batch sizes but would allow us to achieve decent (but not peak) performance across the entire range of sizes.
Batch-invariant matrix multiplication














































































Data Parallel Matmul Similar to RMSNorm, the standard parallelism strategy for matmuls is a "data-parallel" strategy, keeping the entire reduction in one core. It is most straightforward to think about splitting the output tensor into 2D tiles and assigning each tile to a different core. Each core then computes the dot products that belong to that tile, once again performing the entire reduction within one core.
Unlike for RMSNorm, additional constraints around arithmetic intensity and utilizing tensorcores force us to split 2D tiles instead of individual output elements for efficient matmul kernels.


At its core, you can view matrix multiplication as simply a pointwise operation followed by a reduction. Then, if we parallelize our matrix multiplication by chunking the output into tiles, we have an analogous “data-parallel” kernel strategy that keeps each reduction within one core.
Also similar to RMSNorm, it is possible for our “batch” dimensions (M and N) to become too small, forcing us to split along the reduction dimension (K). Despite having two “batch” dimensions, matmuls also require us to have much more “work” per core in order to leverage tensorcores effectively. For example, if you have a [1024, K] x [K, 1024] matmul and a standard 2D tile size of [128, 128], a data-parallel strategy would only be able to split this matmul into 64 cores, insufficient to saturate the GPU.
Splitting along the reduction dimension in a matmul is known as a Split-K Matmul. And just like RMSNorm, using this strategy breaks batch invariance.

Another interesting parallelism strategy for matmuls is stream-k. Stream-k is interesting because it has even less invariance than typical matmuls. As discussed, most matmul libraries are not batch-invariant, but they’re at least what you could call batch-position-invariant (i.e. changing the position of the element within the batch does not affect numerics). However, stream-k is not batch-position-invariant either! Its core insight is that you can get cleaner load-balancing by splitting along k in different ways for different output tiles, but taking advantage of this makes our kernel not batch-position-invariant either. 






















































Split-K Matmul If our batch dimension is fairly small we may not have enough parallelism and require a split-k matmul. In this example, we split each reduction across two cores, which would accumulate separately and then combine their results at the end. However, splitting each reduction across two cores allows us to still leverage eight cores.



There’s an additional complexity with matmuls — tensor core instructions. Whereas with reductions we could simply operate on one row at a time, efficient matrix multiplication kernels must operate on an entire “tile” at a time.
Each tensor-core instruction (like say, wgmma.mma_async.sync.aligned.m64n128k16) may have a different reduction order internally. One reason to use a different tensor-core instruction might be that the batch size is very small. For example, if we use a tensor-core PTX instruction that operates on a tile of length 256 but the batch size is only 32, we’re wasting almost all of that compute! At a batch-size of 1, the fastest kernels usually don’t use tensor cores at all.







































































Padded Tensor-Core Instructions If the batch size is too small, we may be in our situation where we can't fit even one of our 2D tiles in the output. In this case, it is most efficient to switch to a smaller tensor-core instruction or eschew tensor-cores altogether! However, both of these options prevent our kernel from being batch-invariant.

So, the easiest way to ensure batch invariance for matmuls is to compile one kernel configuration and use that for all shapes. Although we will lose some performance, this isn’t typically disastrous in LLM inference. In particular, split-k is most needed when both M and N are small, and luckily in our case, N (i.e. the model dim) is usually pretty large!

  Despite obtaining batch invariance, we only lose about 20% performance compared to cuBLAS. Note that this is not an optimized Triton kernel either (e.g. no TMA). However, some of the patterns in performance are illustrative of where our batch-invariant requirement loses performance. First, note that we lose a significant amount of performance at very small batch sizes due to an overly large instruction and insufficient parallelism. Second, there is a "jigsaw" pattern as we increase the batch-size that is caused by quantization effects (both tile and wave) that are typically ameliorated through changing tile sizes. You can find more on these quantization effects here.
  
  

Batch-invariant attention





















































FlashAttention2 Strategy We parallelize along Q, and reduce along K/V simultaneously. This means that our entire reduction can be kept within a single core, making it another data-parallel strategy.

After obtaining batch invariance for matmuls, attention introduces two additional wrinkles — fittingly,  because it contains two matmuls.

As opposed to only reducing over the feature dimension like both RMSNorm and matmuls, we now reduce over the feature dimension and sequence dimension.
Due to the above, attention must deal with a variety of inference optimizations that affect how sequences get processed (chunked prefill, prefix caching, etc.).

Thus, to achieve determinism in LLM inference our numerics must be invariant to both how many requests are processed at once and how each request gets sliced up in the inference engine.
Let’s first walk through the standard parallelism strategy for attention, first introduced in FlashAttention2. Similar to RMSNorm and Matmul, the default strategy is a “data-parallel” strategy. Since we reduce along the key/value tensors, a data-parallel strategy can only parallelize along the query tensor.
For example, depending on the inference engine’s choices, it’s possible that a sequence might get processed in several parts (such as in chunked prefill) or perhaps all at once (if the prefill isn’t split up). In order to achieve “batch invariance”, it’s necessary that the reduction order for a given token does not depend on how many other tokens from its sequence are being simultaneously processed. If you reduce over the K/V values in the KV cache separately from the K/V values in the current tokens being processed (like in vLLM’s Triton attention kernel), this can’t be achieved. For example, when processing the 1000th query token in a sequence, the reduction order must be identical regardless of whether 0 tokens are in the KV cache (prefill) or 999 tokens are in the KV cache (decoding).






























































FlashAttention with a KV Cache The reason why explicitly handling the KV cache separately from the current KV values breaks batch invariance is a bit subtle and is related to "boundary conditions". In particular, imagine your block size is 32 but we currently have 80 elements in our KV cache. We then compute an additional 48 elements that aren't cached. In this case, we need three blocks (two full and one masked) to compute "P cache" and another two blocks (one full and one masked) to compute "P". This is therefore five total blocks to compute our reduction when we only have four total blocks (i.e. 128) of elements to compute, which will definitely change our reduction order. 
For example, if we instead had no elements in our KV Cache and were processing 128 elements altogether, we need to have identical numerics in both of these situations to ensure “batch invariance” for attention.
 

To resolve this, we can just update the KV cache and page table before the attention kernel itself, ensuring that our keys and values are always consistently laid out regardless of how many tokens are being processed.
With this additional detail (as well as all the things mentioned in the previous section, like consistent tile sizes), we are able to achieve a batch-invariant attention implementation!
However, there is a significant problem here. Unlike with matrix multiplication, the attention shapes we see in LLM inference often do require a split-reduction kernel, often known as Split-KV or FlashDecoding. This is because if we don’t parallelize along the reduction, we can only parallelize along the batch dimension, head dimension, and “query length” dimension. In the decode stage of attention, query length is very small, and so unless we have a very large batch size we are often unable to saturate the GPU.
Unfortunately, it’s not as easy to ignore this case as it was for RMSNorm and Matmuls. For example, if you have a very long KV cache, the attention kernel may take a very long time despite only processing one request.



































Fixed # Split-KV Strategy (i.e. FlashDecode) If our query length becomes very small (like it does during decoding), we may end up in a situation where there is very little parallelism in our kernel at all. In these cases, we'll need to once again split along the reduction dimension --- the KV dimension this time. The typical strategy for how to split along the KV dimension is to figure out how much parallelism we need and then divide the KV dimension evenly. For example, if our KV length was 1000 and we needed 4 splits, each core would handle 250 elements.
This unfortunately also breaks batch invariance, as our precise reduction strategy depends on how many query tokens from the sequence we’re processing in any given request.


Furthermore, the split-reduction strategies commonly used for attention also pose challenges for batch invariance. For example, FlashInfer’s “balanced scheduling algorithm” chooses the largest split-size that can still saturate all the GPU’s cores, thus making the reduction strategy not “batch-invariant”. However, unlike with RMSNorm/Matmuls, it’s not sufficient to choose a fixed number of splits regardless of the batch size.
Instead, to achieve batch invariance, we must adopt a “fixed split-size” strategy. In other words, instead of fixing the # of splits, we fix the size of each split and then end up with a varying number of splits. In this manner, we can guarantee that regardless of how many tokens we’re processing, we always perform the identical reduction order. This requires some internal FlexAttention changes that are not included in our code release. We will upstream them in the near future!




































Fixed Size Split-KV Strategy 
The only difference between this strategy and the previous strategy is that our splits are now "fixed size". For example, if our KV length was 1000, instead of splitting it into four even length 250 splits, we would split it into three fixed-size length 256 splits and one length 232 split.
This allows us to preserve batch invariance as our reduction strategy is no longer dependent on how many query tokens we’re processing at once!
 

Implementation
We provide a demonstration of deterministic inference on top of vLLM by leveraging its FlexAttention backend as well as torch.Library.
Through torch.Library, we’re able to substitute out most of the relevant PyTorch operators in an unintrusive way. You can find the library of “batch-invariant” kernels at thinking-machines-lab/batch-invariant-ops, as well as the vLLM example of running in “deterministic” mode.
Experiments
How nondeterministic are completions?
We use Qwen/Qwen3-235B-A22B-Instruct-2507 and sample 1000 completions at temperature 0 with the prompt “Tell me about Richard Feynman” (non-thinking mode), generating 1000 tokens each. Surprisingly, we generate 80 unique completions, with the most common of these occuring 78 times.
Looking at where the completions differ, we see that the completions are actually identical for the first 102 tokens! The first instance of diverging completions occurs at the 103rd token. All completions generate the sequence “Feynman was born on May 11, 1918, in” However, 992 of the completions go on to generate “Queens, New York” whereas 8 of the completions generate “New York City”.

On the other hand, when we enable our batch-invariant kernels, all of our 1000 completions are identical. This is what we would mathematically expect from our sampler, but we aren’t able to achieve deterministic results without our batch-invariant kernels.

Performance
We have not put a significant effort into optimizing the performance of the batch-invariant kernels here. However, let’s run some experiments to verify that our performance remains usable.
We will set up an API server with one GPU running Qwen-3-8B, and request 1000 sequences with an output length of between 90 and 110.

  
      
          Configuration
          Time (seconds)
      
  
  
      
          vLLM default
          26
      
      
          Unoptimized Deterministic vLLM
          55
      
      
          + Improved Attention Kernel
          42
      
  

Much of the slowdown comes from the fact that the FlexAttention integration in vLLM has not been heavily optimized yet. Nevertheless, we see that performance is not disastrous.
True on-policy RL
As researchers have noted, the different numerics between training and inference implicitly turns our on-policy RL into off-policy RL.
Of course, it is impossible to get bitwise identical results between training and inference if we can’t even get bitwise identical results from two identical inference requests. Then, deterministic inference enables us to also modify our training stack to obtain bitwise identical results between sampling and training, thus resulting in true on-policy RL.
We run experiments in a RLVR setup on Bigmath with the RL policy initialized from the Qwen 2.5-VL instruct 8B with a max rollout length of 4096.
If we train without off-policy correction (i.e. importance weighting), our reward collapses partway through training, whereas adding an off-policy correction term  allows training to proceed smoothly. But, if we achieve bitwise identical results between our sampler and trainer, we are fully on policy (i.e. 0 KL divergence) and can also train smoothly.
We can also plot the KL-divergence in logprobs between our sampler and trainer, where all 3 runs have notably different behavior. When running with importance weighting, it stays around 0.001 with occasional spikes. However, running without importance weighting eventually leads to a spike in KL-divergence around the same time that reward crashes. And, of course, when running “True On-Policy RL”, our KL-divergence stays flat at 0, indicating that there is no divergence between the training policy and sampling policy.



  Note that the run without importance weighting has a significant loss spike around Step 318, and this comes with a correspond ing spike in KL-divergence of logprobs. Meanwhile, either using an off-policy correction or running with "True On-Policy" allows RL to continue smoothly. The blue line showing "True On-Policy" is not a bug - it's just a flat line at 0. 
  
  

Conclusion
Modern software systems contain many layers of abstractions. In machine learning, when we run into nondeterminism and subtle numerical differences it can often be tempting to paper over them. After all, our systems are already “probabilistic”, so what’s wrong with a little more nondeterminism? What’s wrong with bumping up the atol/rtol on the failing unit test? The difference in logprobs between the trainer and the sampler probably isn’t a real bug, right?
We reject this defeatism. With a little bit of work, we can understand the root causes of our nondeterminism and even solve them! We hope that this blog post provides the community with a solid understanding of how to resolve nondeterminism in our inference systems and inspires others to obtain a full understanding of their systems.
Citation
Please cite this work as:
He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference", 
Thinking Machines Lab: Connectionism, Sep 2025.
Or use the BibTeX citation:
@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}

    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[“No Tax on Tips” Includes Digital Creators, Too]]></title>
            <link>https://www.hollywoodreporter.com/business/business-news/no-tax-on-tips-guidance-creators-trump-treasury-1236366513/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200024</guid>
            <description><![CDATA[A U.S. Treasury Department document includes podcasters, social media influencers and streamers among occupations eligible for tax-free tips.]]></description>
            <content:encoded><![CDATA[
	President Trump’s One Big Beautiful Bill Act may have quietly changed the economics of the creator economy.




	The U.S. Treasury Department this past week released a list of occupations “that customarily and regularly received tips” and thus will be eligible for the administration’s flagship “no tax on tips” policy, which will let eligible taxpayers deduct their tipped income, within certain limits.






	And while the list includes the obvious (bartenders, food servers, casino dealers and housekeepers are all there) it also includes some surprising jobs that could alter the economics of the creator economy.

	







	That’s because the Treasury Department has determined that “digital content creators” are eligible, including podcasters, social media influencers and streamers.




	Comedians, singers, musicians, DJs and magicians are also included, though that is more relevant to the wedding performer crowd than Grammy-winners.




	The change could cause digital creators to rethink how they seek income. Platforms like TikTok, YouTube, Twitch and Snapchat all offer a variety of ways for creators to generate income, be it a share of advertising revenue or creator funding programs, or options to launch subscription tiers for their channels or profiles. But they also give creators the option to turn on tips or gifts. If revenue from user tips or gifts is eligible, while recurring subscription revenue is not, it could shift how streamers, podcasters or influencers ask their followers to support them.




	To be sure, there are limitations: The tax deduction is capped at $25,000 per year, and it begins to phase out at $150,000 in income for single filers and $300,000 for married joint filers. The act also provides that tips do not qualify for the deduction if they are received “in the course of certain specified trades or businesses — including the fields of health, performing arts, and athletics,” Treasury says, further limiting the deduction opportunity for some in entertainment-adjacent lines of work.

	





	But by making influencers, Twitch streamers and podcasters eligible, the administration has nonetheless changed the incentive structure for digital creators, and the ramifications could be felt across the creator economy in the name of tax efficiency (Don’t be surprised if users are asked to like, subscribe, and tip).




	Platforms may also develop more ways to more prominently feature tips and gifts, pushing creators to add more opportunities for that income.




	But the inclusion of digital creators is also a recognition of how the power dynamics have shifted in media.




	Podcasters and creators, as everyone knows by now, have emerged as a driving force in today’s political climate, and the classification by Treasury could push more people to consider joining the fray or ramping up their content, as long as it is tipped, of course.














]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[I didn't bring my son to a museum to look at screens]]></title>
            <link>https://sethpurcell.com/writing/screens-in-museums/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199931</guid>
            <description><![CDATA[When I was a kid in the ’80s, one of my two favorite places on Earth was The Franklin Institute (TFI) in downtown Philadelphia. We lived a couple hours away so a visit was a rare and precious thing. I think I only visited two or three times but it left an indelible impression on me. I remember wandering in amazement through its enormous spaces getting to actually play with amazing and interesting things. I remember sweeping off a table and then filling an overhanging funnel pendulum with sand, setting it going, and watching it create unexpected patterns on the table. I remember running through the gigantic model heart with other kids. I remember the overpowering joy of being in an actual monumental marble temple of curiosity and fascination. So I was filled with anticipation a couple weeks ago when, during a family trip to the East Coast, we managed to squeeze in a visit to TFI with our six-year-old son.]]></description>
            <content:encoded><![CDATA[ When I was a kid in the ’80s, one of my two favorite places on Earth was The Franklin Institute (TFI) in downtown Philadelphia. We lived a couple hours away so a visit was a rare and precious thing. I think I only visited two or three times but it left an indelible impression on me. I remember wandering in amazement through its enormous spaces getting to actually play with amazing and interesting things. I remember sweeping off a table and then filling an overhanging funnel pendulum with sand, setting it going, and watching it create unexpected patterns on the table. I remember running through the gigantic model heart with other kids. I remember the overpowering joy of being in an actual monumental marble temple of curiosity and fascination. So I was filled with anticipation a couple weeks ago when, during a family trip to the East Coast, we managed to squeeze in a visit to TFI with our six-year-old son.
We parked and ran in, paid close to ninety bucks (ouch! but I love you, so take my money), and started off on the top floor with the Wondrous Space exhibit.
And were met with screens.
Design your own rocket! it said (or something like that). No, I thought, this isn’t designing a rocket, this is playing a lame video game on a touchscreen. Yes, there were space-related artifacts around the walls, and a spacesuit in its own large case, but you couldn’t touch any of this stuff, you couldn’t play with it, you could just look at it for a few seconds, read the placard, say “huh”, and maybe point out some interesting feature to the kiddo.
But the screens were given pride of place, dead center in the dimly-lit space. And so they beckoned. My wife — a science writer who used to be the only staff writer covering space for New Scientist and before that, worked at NASA — poked at one of these with my son, added too many boosters to their launch vehicle, and were told it failed “for reasons” in a way she found totally unhelpful and pointless. She led our son gently but firmly away to the glorious four-story Foucault pendulum which hangs in a stairwell.
Here are some images from the website showing patrons interacting with (or running past) screens so you can see what I’m talking about:



But the screens were all over the place. There were on the main floor, in another section of Wondrous Space, and in the Body Odyssey exhibit. They were all over the SportsZone exhibit on the top floor. Many of them are connected to body motion sensors a la Xbox Kinect so you don’t need to touch them, but they’re still just video games, where the action-response feedback loop is provided by software, not the universe itself.
And the wonderful hands-on physical stuff that I loved as a kid? Jammed into out-of-the-way spaces in the Sir Isaac’s Loft and Air Show rooms. These rooms are terrific, and I was delighted to see they were absolutely packed with kids playing with stuff. No screens, just objects and forces — you don’t even need to read anything to enjoy many of the exhibits, such as the one where you sit in one of two chairs hanging from different configurations of block-and-tackle and haul yourself up — and then just let yourself drop, cushioned by a damping piston. Tucked far away in a desolate corner by a hallway we discovered an engaging exhibit where you pluck rods with your finger to generate Lissajous curves from the vibrations. My son was fascinated; he had never seen anything behave like that. And in the Air Show room he liked many of the exhibits, like the one where you (apparently) evacuate a cylinder to see the effect that has on objects moving through it (versus a control). And the “shimmer wall”, where kids generate sound waves using a variety of devices and can then see the sound waves impacting on a reflective and reactive surface was wonderful and really conveys the mechanical nature of sound.
But these physical exhibits require maintenance, and I was dismayed to see that several are in bad repair; some of them weren’t even working anymore, some seemed worn out, or didn’t seem well-designed to begin with. For instance, they have the classic “bicycle wheel and rotating stool” gyro effect demonstration, but the wheel was too large a diameter for my son to hold, and the stool seemed to have too much friction to work properly for my wife. There was no one trying to use it before or after us; I’d be curious to see the data on how many visitors attempt it, and how well it usually works for them. And that one should be trivial to design and implement properly: for crying out loud, our local ice cream shop has stools that spin on ball bearings, and I think that would be a big improvement. Every time something didn’t work right I couldn’t help thinking: we paid almost ninety bucks to visit this place. TFI doesn’t seem poor; it seems like its budgetary priorities lie elsewhere.
And where it looks like the budget has been going are the screen rooms. They occupy the huge central spaces on the main floor of the museum, and I’m sure a lot of time, money, and passion went into these things. But it’s misguided.
I believe museums exist to present the real thing for the visitor to experience with their own senses. Here’s the sculpture — the actual piece of stone, two thousand years old, Greek sculptor unknown — now go ahead and form your impressions. Come back to it when you’re an old man or woman, it will still be here, and you will see it with different eyes. This is a tiny but, to me, beautiful part of the human condition. And what made the Franklin Institute so amazingly special to me as a kid was that the exhibits sat at the intersection of things that kids want to play with, things that kids are allowed to play with, and things that demonstrate some “hey, that’s cool!” scientific phenomenon. And it was all real.
But a lame video game? I can do that on my goddamn phone. TFI “was one of the first museums in the nation to offer a hands-on approach to learning about the physical world”, but — and I can’t emphasize this enough, it’s my whole reason for writing this — touchscreens are not actually hands-on. Digital representations aren’t tangible, and touchscreen experiences just don’t activate a kid’s brain (including, I’d say, a sense of delight) the same way a genuinely hands-on experience, like pulling hard on a rope to raise your chair, does.
I don’t know why museums are doing this; my idle speculation is that they see themselves as competing with screens for attention, so in a kind of experiential race to the bottom, they feel compelled to bring screens into their exhibits (see: Amusing Ourselves to Death). But now more than ever in history, kids need a break from the screens that all too many of them are sadly often plugged into by default, and connection to the real world instead. Now is the time for TFI — and all museums — to take a stand against the tidal wave of digital garbage that is consuming humanity, especially kids, by eliminating all of their touchscreen “exhibits”.
To be fair, TFI is still pretty damn great if you just ignore all the screens. The Franklin Memorial rotunda (free to visit!) is gorgeous. The hands-on stuff, tucked away and apparently suffering from neglect though it is, should be replicated in every city in the world. But it would be so much better if they removed all the screens and put that budget and real estate toward the real, tangible, interactive science exhibits that were the reason the museum was created in the first place, and what made me love it as a child. ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ChatGPT Developer Mode: Full MCP client access]]></title>
            <link>https://platform.openai.com/docs/guides/developer-mode</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199713</guid>
        </item>
        <item>
            <title><![CDATA[Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts]]></title>
            <link>https://news.ycombinator.com/item?id=45199648</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199648</guid>
            <description><![CDATA[Hey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today we’re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. It’s our biggest release in quite a while so we thought we’d finally do our Launch HN :)]]></description>
            <content:encoded><![CDATA[Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts86 points by davidgu 13 hours ago  | hide | past | favorite | 34 commentsHey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today we’re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. It’s our biggest release in quite a while so we thought we’d finally do our Launch HN :)Here’s a demo that shows it producing a transcript from a meeting, followed by examples in code: https://www.youtube.com/watch?v=4croAGGiKTA . API docs are at https://docs.recall.ai/.Back in W20, our first product was an API that lets you send a bot participant into a meeting. This gives developers access to audio/video streams and other data in the meeting. Today, this API powers most of the meeting recording products on the market.Recently, meeting recording through a desktop form factor instead of a bot has become popular. Many products like Notion and ChatGPT have added desktop recording functionality, and LLMs have made it easier to work with unstructured transcripts. But it’s actually hard to reliably record meetings at scale with a desktop app, and most developers who want to add recording functionality don’t want to build all this infrastructure.Doing a basic recording with just the microphone and system audio is fairly straightforward since you can just use the system APIs. But it gets a lot harder when you want to capture speaker names, produce a video recording, get real-time data, or run this in production at large scale:- Capturing speaker names involves using accessibility APIs to screen-scrape the video conference window to monitor who is speaking at what time. When video conferencing platforms change their UI, we must ship a change immediately, so this keeps working.- Producing a video recording that is clean, and doesn’t capture the video conferencing platform UI involves detecting the participant tiles, cropping them out, and compositing them together into a clean video recording.- Because the desktop recording code runs on end-user machines, we need to make it as efficient as possible. This means writing highly platform-optimized code, taking advantage of hardware encoders when available, and spending a lot of time doing profiling and performance testing.Meeting recording has zero margin for failure because if anything breaks, you lose the data forever. Reliability is especially important, which dramatically increases the amount of engineering effort required.Our Desktop Recording SDK takes care of all this and lets developers build meeting recording features into their desktop apps, so they can record both video conferences and in-person meetings without a bot.We built Recall.ai because we experienced this problem ourselves. At our first startup, we built a tool for product managers that included a meeting recording feature. 70% of our engineering time was taken up by just this feature! We ended up starting Recall.ai to solve this instead. Since then, over 2000 companies use us to power their recording features, e.g. Hubspot for sales call recording, Clickup for their AI note taker. Our users are engineering teams building commercial products for financial services, telehealth, incident management, sales, interviewing, and more. We also power internal tooling for large enterprises.Running this sort of infrastructure has led to unexpected technical challenges! For example, we had to debug a 1 in 36 million segfault in our audio encoder (https://www.recall.ai/blog/debugging-a-1-in-36-000-000-segfa...), we encountered a Postgres lock-up that only occurs when you have tens of thousands of concurrent writers (https://news.ycombinator.com/item?id=44490510), and we saved over $1M a year on AWS by optimizing the way we shuffle data around between our processes (https://news.ycombinator.com/item?id=42067275).You can try it here: https://www.recall.ai. It's self-serve with $5 of free credits. Pricing starts at $0.70 for every hour of recording, prorated to the second. We offer volume discounts with scale.All data recorded through Recall.ai is the property of our customers, we support 0-day retention, and we don’t train models on customer data.We would love your feedback!
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Jiratui – A Textual UI for interacting with Atlassian Jira from your shell]]></title>
            <link>https://jiratui.sh/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45198481</guid>
            <description><![CDATA[JiraTUI revolutionizes task management for developers by enabling seamless interaction with Jira from the terminal. Create, update, and track tasks efficiently, all while maintaining focus on your code and workflow.]]></description>
            <content:encoded><![CDATA[
      
        
      

      
            
            JiraTUI revolutionizes task management for developers by enabling seamless interaction with Jira from the terminal. Create, update, and track tasks efficiently, all while maintaining focus on your code and workflow.
          


      
        
        
      

      
          
            Features
          
          
            
                  
                  
                  
                  
                
            
              Search Tasks
              Quickly locate your Jira tasks using the powerful search functionality in JiraTUI. With just a few commands, you can filter tasks by status, assignee, or priority. This feature saves time and enhances productivity, allowing you to focus on what matters most in your projects.
            
          
          
            
            
            
              Create Tasks
               Easily create new Jira tasks directly from the terminal with JiraTUI. This feature simplifies the task creation process, enabling you to specify details like title, description, and priority in a streamlined manner. Spend less time navigating interfaces and more time getting things done.
            
          
          
            
                  
                  
                  
                  
                
            
              Update Tasks
              Keep your tasks up to date effortlessly with JiraTUI's update feature. Modify task details such as status, assignee, summary, labels and due dates directly from the command line. This functionality ensures that your project remains organized and current, enhancing collaboration and workflow efficiency.
            
          
          
            
            
            
              Comments
              Engage with your team by managing comments on tasks through JiraTUI. Add or delete comments directly from the terminal, fostering clear communication and collaboration. This feature helps keep discussions organized and accessible, ensuring everyone stays informed about task progress.
            
          
          
              Manage Related Tasks
              Easily manage related tasks with JiraTUI, allowing you to link and unlink tasks directly from the terminal. This feature helps you visualize dependencies and relationships between tasks, ensuring a more cohesive project management experience and improving overall workflow.
            
          
            
                  
                  
                  
                  
                
            
              JQL Search
              Leverage the power of Jira Query Language (JQL) with JiraTUI to perform advanced searches. This feature allows you to create complex queries to filter tasks based on specific criteria, providing greater flexibility and precision in managing your projects and enhancing your productivity. Expressions can be saved to use at any time.
            
          
        

      
          Advantages
          Discover the key advantages of JiraTUI that make it an essential tool for developers seeking efficient and effective task management.
          
            
                  
                    
                  
                  Configurable
                  JiraTUI is highly configurable, allowing users to tailor the tool to their specific needs. Customize command shortcuts, settings, and preferences to create a personalized experience that enhances productivity. This flexibility ensures that JiraTUI adapts to your workflow, making it a perfect fit for any development environment.
                
            
                  
  

                  Simplicity
                  JiraTUI offers a straightforward command-line interface that simplifies task management. By eliminating unnecessary clicks and navigation, it allows developers to focus on their work. The intuitive commands make it easy to perform actions quickly, ensuring that managing Jira tasks is a seamless part of your development workflow.
                
          
          
            
                  
                    
                    
                  
                  Speed
                  Experience unparalleled speed with JiraTUI, designed for efficiency in task management. The command-line interface allows for rapid execution of commands, enabling developers to create, update, and search tasks in seconds. This speed not only saves time but also enhances overall productivity, allowing you to focus on delivering quality work.
                
            
                  
  

                  Easy to Use
                  JiraTUI is designed with user-friendliness in mind, making it accessible for developers of all skill levels. The clear command structure and helpful prompts guide users through task management effortlessly. With minimal learning curve, you can quickly harness the power of JiraTUI and integrate it into your daily workflow.
                
          
        

      
            
            © 2025 Gaston Tagni.
            
            
          

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kerberoasting]]></title>
            <link>https://blog.cryptographyengineering.com/2025/09/10/kerberoasting/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45196437</guid>
            <description><![CDATA[I learn about cryptographic vulnerabilities all the time, and they generally fill me with some combination of jealousy (“oh, why didn’t I think of that”) or else they impress me w…]]></description>
            <content:encoded><![CDATA[
	
	
		
I learn about cryptographic vulnerabilities all the time, and they generally fill me with some combination of jealousy (“oh, why didn’t I think of that”) or else they impress me with the brilliance of their inventors. But there’s also another class of vulnerabilities: these are the ones that can’t possibly exist in important production software, because there’s no way anyone could still do that in 2025.



Today I want to talk about one of those ridiculous ones, something Microsoft calls “low tech, high-impact”. This vulnerability isn’t particularly new; in fact the worst part about it is that it’s had a name for over a decade, and it’s existed for longer than that. I’ll bet most Windows people already know this stuff, but I only happened to learn about it today, after seeing a letter from Senator Wyden to Microsoft, describing how this vulnerability was used in the May 2024 ransomware attack on the Ascension Health hospital system.



The vulnerability is called Kerberoasting, and TL;DR it relies on the fact that Microsoft’s Active Directory is very, very old. And also: RC4. If you don’t already know where I’m going with this, please read on.



A couple of updates: The folks on HN pointed out that I was using some incorrect terms in here (sorry!) and added some good notes, so I’m updating below.



What’s Kerberos, and what’s Active Directory?



Microsoft’s Active Directory (AD) is a many-tentacled octopus that controls access to almost every network that runs Windows machines. The system uses centralized authentication servers to determine who gets access to which network resources. If an employee’s computer needs to access some network Service (a file server, say), an Active Directory server authenticates the user and helps them get securely connected to the Service.



This means that AD is also the main barrier ensuring that attackers can’t extend their reach deeper into a corporate network. If an attacker somehow gets a toehold inside an enterprise (for example, because an employee clicks on a malicious Bing link), they should absolutely not be able to move laterally and take over critical network services. That’s because any such access would require the employee’s machine to have access to specialized accounts (called “Service accounts”) with privileges to fully control those machines. A well-managed network obviously won’t allow this. This means that AD is the “guardian” that stands between most companies and total disaster.



Unfortunately, Active Directory is a monster dragged from the depths of time. It uses the Kerberos protocol, which was first introduced in early 1989. A lot of things have happened since 1989! In fairness to Microsoft, Active Directory itself didn’t actually debut until about 1999; but (in less fairness), large portions of its legacy cryptography from that time period appear to still be supported in AD. This is very bad, because the cryptography is exceptionally terrible. 



Let me get specific.



When you want to obtain access to some network resource (a “Service” in AD parlance), you first contact an AD server (called a KDC) to obtain a “ticket” that you can send to the Service to authenticate. This ticket is encrypted using a long-term Service “password” established at the KDC and the Service itself, and it’s handed to the user making the call.



Now, ideally, this Service password is not really a password at all: it’s actually a randomly-generated cryptographic key. Microsoft even has systems in place to generate and rotate these keys regularly. This means the encrypted ticket will be completely inscrutable to the user who receives it, even if they’re malicious. But occasionally network administrators will make mistakes, and one (apparently) somewhat common mistake is to set up a Service that’s connected to an ordinary user account, complete with a human-generated password.



Since human passwords probably are not cryptographically strong, the tickets encrypted using them are extremely vulnerable to cracking. This is very bad, since any random user — including our hypothetical laptop malware hacker — can now obtain a copy of such a ticket, and attempt to crack the Service’s password offline by trying many candidate passwords using a dictionary attack. The result of this is that the user learns an account password that lets them completely control that essential Service. And the result of that (with a few extra steps) is often ransomware.



Isn’t that cute?



That doesn’t actually seem very cute?



Of course, it’s not. It’s actually a terrible design that should have been done away with decades ago. We should not build systems where any random attacker who compromises a single employee laptop can ask for a message encrypted under a critical password! This basically invites offline cracking attacks, which do not need even to be executed on the compromised laptop — they can be exported out of the network to another location and performed using GPUs and other hardware.



There are a few things that can stop this attack in practice. As we noted above, if the account has a long enough (random!) password, then cracking it should be virtually impossible. Microsoft could prevent users from configuring services with weak human-generated passwords, but apparently they don’t — at least because this is something that’s happened many times (including at Ascension Health.) 



So let’s say you did not use a strong cryptographic key as your Service’s password. Where are you?



Your best hope in this case is that the encrypted tickets are extremely challenging for an attacker to crack. That’s because at this point, the only thing preventing the attacker from accessing your Service is computing power. But — and this is a very weak “but” — computing power can still be a deterrent! In the “standard” authentication mode, tickets are encrypted with AES, using a key derived using 4,096 iterations of PBKDF2 hashing, based on the Service password and a per-account salt (Update: which is not truly random salt, it’s a combination of domain and principal name.) The salt means an attacker cannot easily pre-compute a dictionary of hashed passwords, and while the PBKDF2 (plus AES) isn’t an amazing defense, it puts some limits on the number of password guesses that can be attempted in a given unit of time.



This page by Chick3nman gives some excellent password cracking statistics computed using an RTX 5090. It implies that a hacker can try 6.8 million candidate passwords every second, using AES-128 and PBKDF2.



So that’s not great. But also not terrible, right?



This isn’t the end of the story. In fact it’s self-evident that this is not the end of the story, because Active Directory was invented in 1999, which means at some point we’ll have to deal with RC4.



Here’s the thing. Anytime you see cryptography born in the 1990s and yet using AES, you cannot be dealing with the original. What you’re looking at is the modernized, “upgraded” version of the original. The original probably used an abacus and witchcraft, or (failing that) at least some combination of unsalted hash functions and RC4. And here’s the worst part: it turns out that in Active Directory, when a user does not configure a Service account to use a more recent mode, then Kerberos will indeed fall back to RC4, combined with unsalted NT hashes (basically, one iteration of MD4.)



The main implication of using RC4 (and NT hashing) is that tickets encrypted this way become hilariously, absurdly fast to crack. According to our friend Chick3nman, the same RTX 5090 can attempt 4.18 billion (with a “b”) password guesses every second. That’s roughly 1000x faster than the AES variant.



As an aside, the NT hashes are not salted, which means they’re vulnerable to pre-computation attacks that involve rainbow tables. I had been meaning to write about rainbow tables recently on this blog, but had convinced myself that they mostly don’t matter, given that these ancient unsalted hash functions are going away. I guess maybe I spoke too soon? Update: see Tom Tervoort’s excellent comment below, which mentions that there is a random 8-byte “confounder” acting as a salt during key derivation.



So what is Microsoft doing about this?



Clearly not enough. These “Kerberoasting” attacks have been around for ages: the technique and name is credited to Tim Medin who presented it in 2014 (and many popular blogs followed up on it) but the vulnerabilities themselves are much older. The fact that there are practical ransomware attacks using these ideas in 2024 indicates that (1) system administrators aren’t hardening things enough, but more importantly, (2) Microsoft is still not turning off the unsafe options that make these attacks possible.







To give some sense of where we are, in October 2024, Microsoft published a blog post on how to avoid Kerberos-based attacks (NB: I cannot say Kerberoasting again and take myself seriously). 



The recommendations are all kind of dismal. They recommend that administrators should use proper automated key assignment, and if they can’t do that, then to try to pick “really good long passwords”, and if they can’t do that, to pretty please shut off RC4. But Microsoft doesn’t seem to do anything proactive, like absolutely banning obsolete legacy stuff, or being completely obnoxious and forcing admins to upgrade their weird and bad legacy configurations. Instead this all seems much more like a reluctant and half-baked bit of vulnerability management.



I’m sure there are some reasons why this is, but I refuse to believe they’re good reasons, and Microsoft should probably try a lot harder to make sure these obsolete services go away. It isn’t 1999 anymore, and it isn’t even 2014.



If you don’t believe me on these points, go ask Ascension Health.
	

			
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pontevedra, Spain declares its entire urban area a "reduced traffic zone"]]></title>
            <link>https://www.greeneuropeanjournal.eu/made-for-people-not-cars-reclaiming-european-cities/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45195520</guid>
            <description><![CDATA[By prioritising residents over private vehicles, a Spanish municipality has overcome some of the biggest challenges facing Europe’s cities.]]></description>
            <content:encoded><![CDATA[
					
	
		
With the number of passenger vehicles rising across Europe, cities are grappling with air pollution, traffic accidents, and the loss of public space. In Spain, the city of Pontevedra has managed to overcome these challenges, surpassing national air quality standards and creating safer streets.  The key, according to the Galician municipality’s mayor, is an urban model that prioritises residents over cars – without imposing an outright ban on private vehicles. 



It is a bright summer evening in Pontevedra, a Galician city in the northwest of Spain. The air is filled with a contralto accompanying a live jazz show in a corner of the big town square. A few metres away, four teenagers play soccer with an orange ball that two younger children try to touch in vain. A family takes a selfie while, seated on a nearby bench, four elderly women are engaged in a lively conversation. The intervals between one jazz piece and another are filled with the chirping of birds that are attracted to the greenery around the fountain.  



Looking at the images of Pontevedra from the 1990s, with lines of cars stretching as far as the eye could see, it would be very difficult to predict a future like this. But since family doctor Miguel Anxo Fernández Lores was elected mayor in 1999, the Galician city has been implementing policies that go well beyond regulating vehicles in its streets. The goal, according to the 71-year-old mayor, is to recover public space for the people.  



“When we reclaim public space and guarantee universal accessibility, then people have autonomy,” the mayor says. A politician of the Galician Nationalist Bloc party (Bloque Nacionalista Galego, BNG), Lores is now serving his seventh mandate and is willing to run for an eighth in 2027. Galicia is mainly and historically ruled by the right-wing Popular Party, and is the birthplace of several of its national leaders, which makes the local leftist and nationalist BNG’s long rule in Pontevedra an exception in the region.  




When we reclaim public space and guarantee universal accessibility, then people have autonomy.




In December 2022, the Spanish government approved a royal decree requiring all municipalities with more than 50,000 inhabitants to have a Low Emission Zone (LEZ) in operation. To improve air quality for citizens and reduce carbon emissions, the decree recommends measures such as restricting access for more polluting vehicles based on their environmental label and introducing traffic-restricted areas where entry charges apply.  



Spain adopted this measure to comply with the legally binding requirements of the Paris Agreement – the international treaty on climate change – more than six years after it entered into force in November 2016.  



Nevertheless, since Pontevedra was already fully complying with the air quality parameters laid out in the national Law 7/2021 on Climate Change, the city council decided to take a much more ambitious step: declaring the entire urban area (about 490 hectares) as a “reduced traffic zone”. 



On a sunny, fresh noon at the end of June in his city-centre office, Lores recalls what Pontevedra looked like when he first took up his duties: “It was literally a cars’ warehouse and people, especially those with disabilities and the elderly, couldn’t go out on the streets, because everything was occupied by vehicles.” 



While he speaks, the mayor mentions the 19th-century Catalan engineer Ildefons Cerdà i Sunyer, best known for the urban reform of Barcelona’s central Eixample neighbourhood, with its distinctive grid layout and symmetrical structure. Just like Cerdà, he views public space as an extension of the home.



Cars: a continental problem



More than 75 per cent of the EU population lives in urban areas, and this figure is expected to rise to approximately 83 per cent by 2050. With air pollution widely recognised as the most pressing health risk in Europe, curbing road transport emissions – responsible for 37 per cent of nitrogen oxide pollution – is crucial.  



The European Union has launched a number of initiatives to encourage cities to become cleaner and healthier. One of the most notable is the Green City Accord, which invites towns with a population of 20,000 inhabitants or more to commit to improvements in areas such as air and water quality, noise reduction, biodiversity conservation, and advancing towards a circular economy. The initiative also calls on cities to connect with a wider European network to facilitate knowledge sharing.  



Another initiative is the Climate-Neutral and Smart Cities EU Mission, which is supporting 100 cities in the EU and 12 in countries associated with the Horizon Europe programme. The goal of the mission is to develop a pilot project to achieve climate neutrality by 2030. The solutions and models tested in these cities could then serve as an example, helping all European cities to follow suit by 2050, the year by which the EU has committed to achieving net-zero greenhouse gas emissions. 



There are also informal initiatives such as the Ciudades que caminan (“Walking Cities”) network in Spain, which is a non-profit open to city councils and other public administrations committed to walkability. The network provides participant cities with training and a forum for the exchange of information and experiences. It includes an online school of public space, and is responsible for managing and promoting Entornos escolares (“school environments”), Spain’s most important website dedicated to promoting child autonomy and urban mobility.  



Still, in spite of both top-down and bottom-up initiatives, the number of cars in our continent is growing: in 2024, the number of vehicles in the European Union exceeded 259 million, an increase of 5.9 per cent as compared to 2019. The country with the highest motorisation rate – that is, the number of cars per 1,000 inhabitants – is Italy (701), followed by Luxembourg (670) and Finland (666 cars). With a rate of 544 (although this is based on provisional data), Spain is below the EU average of 576. 







Defying the norm  



Meanwhile, in Pontevedra, the number of cars has consistently diminished in recent years. The city has also introduced regulations both restricting the purposes for which cars may circulate and adjusting the times when they are permitted to do so. 



When asked to name one of the best examples of what his administration has achieved in terms of prioritising pedestrians over vehicles, the mayor does not hesitate to mention the transformation of one of the main squares in Pontevedra. Next to the remains of the 14th-century Gothic convent of Santo Domingo lies the wide Praza de España, regarded as the city’s “kilometre zero” – the starting point of its main natural and historical routes. Nowadays, the site serves as a lively hub where on any summer day walkers, drawn by the frequent cultural events hosted in the city, meet with pilgrims of the St. James’ Way.  



Lores points out that at the end of the 1990s, about 26,000 cars passed by this square every day. Remarkably, many of these vehicles were simply trying to reach destinations out of town, such as the beaches of Sanxenxo – a touristic municipality some 30 kilometres away, as well as the mayor’s birthplace – and to do so they passed through Praza de España. But today, transit traffic and circling for parking are not allowed anywhere in the city, and according to Lores, only 800 cars reach the square on a daily basis. As he explains, the decision to prohibit passing traffic and divert it to beltways has led to a 40-per cent reduction in the city’s overall traffic.   



Only “necessary traffic” is allowed in Pontevedra: vehicles used for emergency and public safety, public services (including garbage and water trucks, etc.), transportation of people with reduced mobility, and accessing private garages are permitted 24/7. However, loading and unloading for commercial supplies, home delivery, transporting bulky objects, and house moving and related activities, are permitted only during certain hours. Free parking spaces are scattered all over the city so that people can temporarily stop while carrying out any of these activities. In case of violations, police have the option to issue fines of up to several hundred euros.  



“There is no place in town that can’t be reached by car, but only by those who need to, not those who feel like it,” Lores explains. And with his characteristic tone, direct yet polite, he adds: “The fact that you park your private car in a public space is crazy: if you don’t have room for your freezer, do you put it on the sidewalk?” 



In the order of priorities, all cars, including electric ones, come last in Pontevedra: “The entire pyramid of preferences was changed: pedestrians come first, then bicycles, scooters, public transportation, and only lastly private transportation.” Inspired by Jane Jacobs’ 1961 book The Death and Life of Great American Cities, Lores has placed maintaining a “compact city” at the heart of his administration’s policies on urban expansion: concentrating most activities in the city centre, discouraging large department stores on the outskirts, encouraging mixed-use neighbourhoods instead of single-purpose zones (such as a separate ‘city of justice’), like Barcelona’s law courts complex and improving public transport. The aim is not only to reduce unnecessary car travel, but also to bring life to neighbourhoods and strengthen social cohesion. 




The entire pyramid of preferences was changed: pedestrians come first, then bicycles, scooters, public transportation, and only lastly private transportation.




Wandering through the downtown area on any working day, it is surprising to see the large number and diversity of shops, especially for a provincial city. Local boutiques, jewellers, florists, and bookstores all thrive side by side. 



In addition to the Galician municipality, some other European cities are also taking steps towards more sustainability. Freiburg, for instance, is well known for its sustainable policies, including its regulation of car traffic. In the vibrant university city in southwest Germany, the urban planning process involves local people, bikes account for about 30 per cent of all journeys, and one district – Vauban – is almost entirely car-free. Moreover, in 2019, Oslo became the first European capital to completely ban cars in its central areas. The city has expanded the public transportation network and eliminated hundreds of parking spaces, replacing them with benches, green spaces, and bike paths. 







In Pontevedra, pedestrians clearly take precedence in the centre – and in roughly a third of the city overall. There, the roads and the sidewalks are indistinguishable. For those who see it for the first time, it is startling to witness people walking in the middle of the street, apparently unconcerned, while cars slowly wait behind them until they spontaneously move and let them pass. No one honks; no traffic light is there to tell people when they can or can’t move. And no parking is allowed in the whole area between 6 PM and 8 AM. 



In the rest of the city, streets are single-track roads with wide sidewalks. Cars can stop during working hours for a limited time (15 minutes for services and 30 for loading and unloading), and parking is allowed between 9 PM and 9 AM. Traffic lights can only be found on the two-lane avenues on the city’s external ring, where pedestrian overpasses and roundabouts remind vehicles to slow down. Only ten minutes away from the city centre, an open-air parking area offers an alternative to the roughly 4,500 private underground parking spaces available in the city. In total, Pontevedra has more than ten free municipal parking sites and approximately 3,500 free parking spaces, all located within a walking distance of 10-15 minutes maximum from the centre. This is especially useful for people who don’t live in the city but have to commute from surrounding areas for work. 



Safer, healthier, and more accessible  



In 2010, Pontevedra was the first Spanish city to enact a speed limit of 30 km/h on all roads in its urban territory. At present, the limit is 10 km/h in the downtown area, “but only if there is no one around”, explains Daniel Macenlle, an ex-local policeman and the current director general for protection of citizens at the city council. “If there are people, then it goes down to six.” In the rest of the centre, the maximum speed is 20 kilometres per hour, while in other neighbourhoods, vehicles can go as fast as 30. 



The result is that there have been no fatal accidents on municipal roads for a decade. Today, according to municipality data, 73 per cent of children walk to school (including 44 per cent who go accompanied, and 29 who take the journey on their own). A 2012 study conducted in Denmark, involving 20,000 children and part of a bigger project on the links between concentration, diet, and exercise, found that those who walk or bike to school have higher concentration levels than their peers up to four hours later.  In Pontevedra, as in other Spanish cities like Barcelona, “school routes” have been implemented for years. These spaces with specific signposts are located around schools to offer students the option of walking to and from school, alone or in groups. 



And the benefits extend to all residents: overall, the number of people who choose to walk or bike in the Galician municipality has risen from 66 per cent in 2011 to 90 per cent in 2021. Moreover, today about 70 per cent of all trips take place on foot or by bicycle. The city council also estimates that, since the end of the 1990s, CO2 emissions have been reduced by approximately 67 per cent. 



Road and the sidewalk are indistinguishable in the city center Credit: ©Elena Ledda



In order to know not only the distance by foot, but also the number of steps and the calories burnt to reach a location, people in Pontevedra can try to use Metrominuto.1 The synoptic map includes information about the city’s historic sites, news (e.g. on recent international studies on mobility), and service information (like national subsidies for electric cars).  



Though it may be assumed that geo-referenced camera checks, fines and towing in place are extensively used to ensure road rules are respected in Pontevedra, most people interviewed said that the system works simply because people find that it is convenient. 



This is also true for businesses: “A city that is kind to you invites you to explore it and enjoy it. And when you walk around, you also consume,” says Andrés Martínez, 48 and owner of an optical shop in calle de la Oliva, one of the city’s more commercial streets. Andrés lives in the same building where the shop is located and parks his private car in its communal parking. 




A city that is kind to you invites you to explore it and enjoy it. And when you walk around, you also consume.




Another resident is 57-year-old Santi Cachadas, who has been selling fish at the municipal market for 30 years. He points out a different benefit of the city’s model: “People who come to buy pick up and leave, and that means an empty space where a new client can park, and so on, which leads to more movement.” Santi lives about three kilometres away from where he works, and every morning he leaves his car in a free parking lot, built by the Lérez river, and walks for 500 metres to reach his market stall. 



Still, it must be noted that two out of five business owners interviewed for this story, one managing a newsstand and one running an eco-products shop, say that their clients often complain about the fact that they cannot find parking spots.  



When asked why he thinks Pontevedra’s policies have succeeded, the mayor points to several factors: clear communication and education about the city’s goals, networking “with the most proactive and dynamic people in town”, carrying out participatory processes, and the decision not to ban cars outright. And he adds: “We carried out many projects to expand sidewalks and pedestrian spaces; as soon as the project was completed, the space was automatically occupied by activities. We also allowed cars that needed it to enter, and people who had doubts were relieved.” 



Replicating Pontevedra’s success 



Over the years, the northwestern Spanish municipality has received numerous awards for its good practices, including the UN-Habitat’s 2014 Dubai International Best Practices Award for Sustainable Development for its city model centred on people, and the 2020 European Commission’s EU Urban Road Safety Award for “impressively achieving zero road deaths between 2011 and 2018”.  



When invited to answer whether he thinks Pontevedra’s success is exportable, Lores quickly replies that each city has to find its own model and strategy, and that every external example needs to be translated. And when asked if there is at least some lesson he believes could serve as an inspiration for other cities, and if so, for what type of city, without hesitation he states: “Changing the paradigm by moving away from a town imagined for the cars back to one for the people”.  



This article was produced as part of the PULSE collaborative cross-border journalism initiative (coordinated by n-ost and OBCT) and in collaboration with journalist Alice Facchini.






Although I sometimes experienced delays when I tried to use the platform.  ↩︎	

				]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: TailGuard – Bridge your WireGuard router into Tailscale via a container]]></title>
            <link>https://github.com/juhovh/tailguard</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45192963</guid>
            <description><![CDATA[Easy Tailscale to WireGuard bridge in a container. Contribute to juhovh/tailguard development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[TailGuard
A simple Docker container app which allows connecting existing WireGuard
servers to the Tailscale network, in case the device running WireGuard is
locked in and/or does not support Tailscale binaries.
The network topology will look roughly like this:
  +---------+
  | device1 |\
  +---------+ \                      VPS
  +---------+  \ +---------+    +-----------+      +-----------+
  | device2 |----| tailnet |----| TailGuard |<---->| WireGuard |
  +---------+  / +---------+    +-----------+      +-----------+
  +---------+ /
  | device3 |/
  +---------+

As usual, the tailnet is virtual and in reality connections are point-to-point,
but all connections to WireGuard are tunneled through the TailGuard server with
a fixed and persistent connection. As long as you have access to a server as
close to the WireGuard server as possible (ideally with a minimal ping), for
example a VPS, you can connect any WireGuard device to your tailnet.
Benefits
Why would you want to do this? For most use cases it may be easier to connect
your device with WireGuard directly, but there are a couple of benefits with
this bridged approach:

the WireGuard tunnel private key is stored only on a single machine, making
the key management less work
if you have a new device, you can simply log in to your tailnet with SSO,
without having to transfer keys
it's easier to switch between exit nodes in your tailnet, without having to
reconnect to different VPNs
you can have access to both your tailnet and WireGuard concurrently on your
mobile device, which doesn't support multiple VPNs
you can connect your home network to your tailnet using your router, which
only supports WireGuard but not Tailscale

Installation
The simplest way to start TailGuard is to simply download a WireGuard client
config and save it as wg0.conf. After that you can create an IPv6 network
(optional, but recommended) and start the container:
docker network create --ipv6 ip6net
docker run -it \
  -v ./wg0.conf:/etc/wireguard/wg0.conf -v ./state:/tailguard/state \
  --cap-add NET_ADMIN --device /dev/net/tun \
  --sysctl net.ipv4.ip_forward=1 --sysctl net.ipv6.conf.all.forwarding=1 \
  --sysctl net.ipv4.conf.all.src_valid_mark=1 \
  --network ip6net -p 41641:41641/udp \
  --name tailguard juhovh/tailguard:latest

Docker will print you an URL where you need to log in to your tailnet, and after
that you should be good to go.
If you want to build the latest version of the image yourself, it might be best
to use docker compose. In that case you should store the wg0.conf file under
config/, build the latest image with docker compose build and finally run it
with docker compose up.
That's it, happy networking!
Advanced settings
Let's imagine you have a WireGuard server running on 10.1.0.1 that is able to
accept any routes, and its local LAN network is 192.168.8.0/24. You have already
downloaded the WireGuard client config for this tunnel and saved it.  Make sure
that the subnet 192.168.8.0/24 is explicitly mentioned in the AllowedIPs section
in addition to 0.0.0.0/0, for TailGuard to pick it up. It should look something
like this:
[Interface]
PrivateKey = <REDACTED>
Address = 10.1.0.2/24,fd00:ed7c:a960:6e9b::2/64
DNS = 10.1.0.1,fd00:ed7c:a960:6e9b::1
MTU = 1420

[Peer]
PublicKey = <REDACTED>
PresharedKey = <REDACTED>
AllowedIPs = 0.0.0.0/0, ::/0, 192.168.8.0/24
Endpoint = <REDACTED>:51820
PersistentKeepalive = 25

Next you can either add -e TS_DEST_IP=10.1.0.1 if running directly, or open
the docker-compose.yml and modify it as follows:
    environment:
      - TS_DEST_IP=10.1.0.1

This will use the device wg0 and therefore the wg0.conf file for WireGuard. It
will connect to the tailnet, forward all connections targeting itself to the
router behind the tunnel, advertise the "192.168.8.0/24" route to other tailnet
hosts, advertise itself as an exit node, and authenticate with the given
authkey.
Supported configuration parameters through environment:

WG_DEVICE - WireGuard device name, must be valid and match config file name (default: wg0)
TS_DEVICE - Tailscale device name, must be a valid device name (default: tailscale0)
TS_PORT - Tailscale port number, should be exposed by Docker (default: 41641)
TS_AUTHKEY - Tailscale auth key for authentication if used
TS_DEST_IP - Destination IP to route Tailscale traffic to
TS_HOSTNAME - Tailscale hostname for this device if used

Two-way routing between the networks
Unlike Tailscale, WireGuard itself does not handle any routing. Therefore, the
WireGuard subnets and routes are automatically advertised on the Tailscale
network, but it doesn't work the other way around.
Let's say your TailGuard node has IP addresses 10.1.0.2 and
fd00:ed7c:a960:6e9b::2 for the WireGuard tunnel, like in the above config. You
likely want to add at least routes 100.64.0.0/10 and fd7a:115c:a1e0::/48
(Tailscale private address spaces) to be routed through 10.1.0.2. You can do
this with something along the lines of:
ip route add 100.64.0.0/10 via 10.1.0.2 dev wgserver
ip route add fd7a:115c:a1e0::/48 via fd00:ed7c:a960:6e9b::2 dev wgserver

If you have additional subnets in your tailnet (e.g. 192.168.1.0/24) that
you'd like to access, just add similar routing rules for those. TailGuard should
take care of forwarding all the published subnets to the tailnet, as long as it
is able to receive packets through the WireGuard tunnel first.
License
The MIT License (MIT)
Copyright (c) 2025 Juho Vähä-Herttua
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Term.everything – Run any GUI app in the terminal]]></title>
            <link>https://github.com/mmulet/term.everything</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45181535</guid>
            <description><![CDATA[Run any GUI app in the terminal❗. Contribute to mmulet/term.everything development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[
Run every GUI app in the terminal!

Even over ssh!
Behold as I play a video game in a font in a web browser in a terminal transmitted over ssh (with one hand tied behind my back)!

Read about how it works!
Check out HowIDidIt.md
More Examples
The quality of the window is limited to the number of rows and columns in your
terminal. If you increase the resolution (ctrl - in alacritty, check your
terminal) the quality will go up, (but performance may go down).
Here I open up the Wing It! movie, and increase the quality until I get both
a good frame rate and resolution:


If your terminal supports images (like kitty
or iterm2) you can render windows at full resolution
(performance may degrade).
In this example, on my mac, I open iTerm2 ssh into ubuntu and open firefox
at full resolution:


I feel like every single day I hear about another terminal file viewer. I say, stop making terminal file viewers because you can just use the file viewer you already have! In your terminal!


Terminal in a terminal in a terminal in a terminal in a terminal.... it's terminals all the way down.


With only a small amount hacking, it can run Doom (shareware episode)!

About
term.everything❗ is a Linux CLI program to run GUI windows in your terminal. Specifically, term.everything❗ is a built-from-scratch Wayland compositor that outputs to a terminal rather than your monitor.

Don't know what Wayland is or just want to know more about how this works? Then, head over to HowIDidIt.md where I will explain how everything works in detail.

Try it out!
Download the beta test now!
Roadmap

 Term some things <--- This is where we are at


Some apps or (even most apps) may fail to launch or even crash! Please create an issue if you have problems.


 Term most things
 Term everything❗

Help and Usage
Check out the help file here for a usage guide on how to use term.everything❗
Contributing
term.everything❗ is written in developer friendly Typescript using the bun engine, with a just a smidge of C++.
See ./Contributing.md.
Legal:
term.everything❗ copyright 2025 Late for Dinner Studios, LLC
Fontemon copyright 2021 Late for Dinner Studios, LLC
Wing It! movie is licensed under the Creative Commons Attribution 4.0 license
Wing it licensing page
Attribution:
(CC) Blender Foundation | studio.blender.org
Doom shareware episode is copyright 1993 id Software
Bonus:
This is Gwerm the Term Worm.

He is doing okay. Thanks for asking.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Formally verifying a floating-point division routine with Gappa – part 1]]></title>
            <link>https://community.arm.com/arm-community-blogs/b/embedded-and-microcontrollers-blog/posts/formally-verifying-a-floating-point-division-routine-with-gappa-p1</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45174328</guid>
            <description><![CDATA[Learn the basics of using Gappa for numerical error analysis, using floating-point division in Arm machine code as a case study.]]></description>
            <content:encoded><![CDATA[



	Formally verifying a floating-point division routine with Gappa – part 1

		We have recently released a set of optimized assembly-language routines for basic floating-point arithmetic, in Arm Optimized Routines, under an open-source license.
These functions perform the same operations as hardware floating point instructions, for example addition, multiplication, and division. However, they are implemented using only integer 32-bit Arm instructions, for Arm CPUs without hardware floating point. Existing open-source libraries such as libgcc and compiler-rt offer similar functions. Our optimized versions were previously part of the Arm Compiler for Embedded toolchain, which has reached end of life. We are now making them available as open source.
Preparing these functions for publication mainly involved polishing the code and rewriting the comments to improve clarity. However, one of them was much more interesting: polishing up the double-precision division function, “ddiv”, as we call it. The task turned into an adventure in formal verification, and ended up finding a bug in the original version.
In this two-part blog, I’ll tell the story, and share my learning experiences using Gappa and the Rocq prover.
What is “ddiv” and how does it work?
The “ddiv” function is implements double-precision floating-point division in accordance with the IEEE 754 standard. It accepts two (64-bit) floating-point numbers as input and gives their quotient as output. Since it only uses integer arithmetic, it treats the inputs and outputs as 64-bit integers. Each integer is divided into three fields: sign, exponent, and mantissa.


Floating-point functions require handling many special cases such as NaNs, infinities, denormalized numbers, overflow, underflow, and signed zeroes. Division also requires handling dividing by zero. However, this discussion focuses on ordinary cases, where nothing strange happens.
In these ordinary cases, the signs and exponents of the input number are easy to deal with. The primary challenge is the mantissas. Each input mantissa is effectively a 53-bit binary number, made by putting a 1 bit on the front of the 52 bits stored in the input value. Given two such mantissas, n (numerator) and d (denominator), the goal is to compute their quotient and round it to the nearest 53-bit binary number, efficiently.
There are many ways to approach this challenge. The most obvious is the binary analogue of ordinary pencil-and-paper long division: loop round 53 times, generating one output bit every time. However, we can go significantly faster than that.
The approach we took in “ddiv” is to use an approximate method. We start by calculating an approximation to 1 / d (the reciprocal of the denominator). Then we multiply that by n, to get an approximation to the quotient n / d. This strategy can be implemented a lot faster than the bit-by-bit loop, because the CPU’s multiplication instruction does a lot more of the work for us.
The approximations are more precise than the output needs to be. In the version of the code I started with, the approximate reciprocal occupies 61 bits, rather than 53. So, 8 extra bits are generated, which helps us to know which way to round the result, but will be discarded in the final output. However, those extra bits are not all accurate. The technique we use to calculate an approximate reciprocal has an error.
IEEE 754 requires that the result returned from the division operation is the closest possible floating-point number to the true mathematical result. If you round any number in the wrong direction, then you are in violation of the standard. Therefore, we need to be careful about that approximation error. Is it possible that it makes us round a number the wrong way?
To illustrate this, here is a diagram of a small piece of the number line:

The labels show numbers that are exactly representable in double-precision floating point. In the center is an arbitrary number x. The values to the left and right change by the “machine epsilon” ε at each step. This is the minimum separation between floating-point numbers with a particular exponent.
The dotted lines, located halfway between representable numbers, show the rounding boundaries. In the default IEEE 754 mode, numbers are rounded to the nearest output value. This may be either rounding up or down. A number just left of a dotted line rounds down, and one just right rounds up.
The green interval on the left shows one possible output from ddiv’s main division code. The actual value returned is the circle at the left end of the interval. The code uses an approximate method. but it only errs in one direction, so the true quotient is never further left than the green blob. It might be further right, anywhere within the marked interval.
Fortunately, it does not matter. The whole interval lies between the same pair of dotted lines, so any value in that interval will round to the same output value, namely x − ε. Therefore, the uncertainty in our approximate quotient does not affect the final output.
The red line on the right shows what happens if we are not so lucky. In this case, the interval crosses a rounding boundary. The smaller values on the left side of the dotted line round down to x. The larger values on the right side round up to x + ε. The correct output value is uncertain. Our fast but approximate calculation has not managed to narrow down to just one answer. In this case, ddiv runs correction code that performs additional calculation to determine the correct output.
This article does not cover the correction code in detail. The key point is that correction takes significant effort, so it should be avoided where possible.
We calculate an upper bound on the error in ddiv’s approximate method. That tells us the width of one of those intervals of uncertainty, a range of numbers where the true quotient must lie. In most cases, this means we can skip the slower correction code. We simply round and return the approximate value.
ddiv stands or falls on its error bound
The upper bound on the approximation error is central to the ddiv implementation strategy.
The narrower we make that interval of uncertainty, the faster the function will run on average. Fewer cases require the slower correction code and more can use the “fast path” which rounds and returns the approximate value.
This is a rare case where improved mathematical analysis improves performance without changing the algorithm. By tightening the error bound of the existing algorithm, we only need to tweak a couple of constants in the code. As a result, the entire function becomes faster.
An error bound that is too large reduces performance. An error bound that is too small can cause an incorrect answer to be returned. The error bound must be as tight as possible, but no tighter.
In single precision floating point, the problem is simpler. There are only 223 possible values of the denominator d. You can just iterate over all of them and find out how inaccurate your approximation to 1 / d can be. In double precision, there are 252 possible values, and that is too many to loop over. We must rely on mathematical proof, rather than brute-force computation.
When this ddiv function was first written, I calculated the error bound by hand and recorded all my working in a giant comment in the code. A proof in that form requires an expert mathematician to check that it is correct. While I was polishing up the code and the comments, this seemed like a good moment to try to improve the situation.
What is the alternative to a proof that requires a human to check it? A proof checked by a computer. By formalizing the original error proof or generating a replacement one that is formalized already, we can confirm it was correct by re-running the automated checker.
Introducing Gappa
The tool I selected for this task is Gappa. I had heard of it before, but this was the first time I had tried using it.
Gappa is not a general-purpose theorem prover. It is specialized for numerical analysis. Gappa finds or verifies an interval of possible values for a numerical calculation, based on similar intervals for its input values.
As a quick introduction, here is a simple example of Gappa input:
y = x * x;
{ x in [-2, +2] -> y in ? }
The first line defines a calculation. We have some unspecified input x, and we compute a value y = x2. The second line defines the question we are asking Gappa about this calculation. If x is in the interval [−2, +2], what interval can you prove y is in?
Gappa reports this:
Results:
  y in [0, 4]
As expected, a squared real number is never negative. Therefore, x2 cannot be less than 0. If the absolute value of x is at most 2, then x2 is at most 4.
However, Gappa does not do magic all by itself. There is an optional third component of the input file, coming after the calculation and the question. You can also provide hints to Gappa about how it should rearrange its algebraic expressions. For example, here is a second input file where Gappa needs some help:
y = x * (10 - x);
{ x in [4,6] -> y in ? }
In this case, Gappa will report that y is in the range [16, 36]. Why? It knows that x is between 4 and 6, and therefore 10 − x is also between 4 and 6. Multiplying two values of at least 4 gives a product of at least 42 = 16. If they are both at most 6, then the product is at most 62 = 36.
That is all true, of course, but it is not the full picture. The output is close to 16 only if both the values being multiplied, x and 10 − x, are close to 4. The result is close to 36 only if both those values are close to 6. However, they cannot both do that at once, because x and 10 − x are not independent.
When one value is small, the other one is large. They cannot both be close to 4 or both close to 6. Therefore, Gappa’s lower bound of 16 and upper bound of 36 cannot actually be attained.
The value of y can therefore be bounded more tightly. If you plot the function on a graph, you can see what is really going on:

The value of x(10 − x) is guaranteed to fall in the much smaller interval [24, 25]. It hits its maximum value at x = 5, where x(10 − x) = 52 = 25. On both sides, it decreases the further away x gets from 5, so that the smallest value it takes is when x is as far away as possible from 5: when x is either 4 or 6, so that the calculation computes 4 × 6 = 24.
Gappa can prove this result, but it needs help. We can provide it with this input file:
y = x * (10 - x);
{ x in [4,6] -> y in ? }
x * (10 - x) -> 25 - (x - 5)*(x - 5);
This is identical to the previous input, except that I have added a third line, after the braced question section. That third line is a hint. It tells Gappa that when it sees the algebraic expression x(10 − x), it should try rearranging it into 25 − (x − 5)2.
Gappa first checks that the two expressions are equivalent by multiplying them out. Both come to 10x − x2. Once convinced our hint is true, Gappa applies it, and then it can find a better bound. It reasons that if x is in the range [4, 6], then x − 5 is in the range [−1, +1], so (x − 5)2 is in the range [0, 1], and therefore 25 minus that is in the range [24, 25]:
Results:
  y in [24, 25]
These are simplified examples. The normal use of Gappa is to calculate a reliable bound on the error in an approximate calculation. The ‘calculation’ section will normally perform two separate calculations. One delivers the true mathematical value of the result you want, and the other delivers the approximate result from your particular algorithm. Then you ask Gappa to bound the difference between those two values.
In a real-world case, your approximate algorithm is probably written in floating point, fixed point, or integers. As a result, intermediate values are not perfectly accurate real numbers. At many steps, the answers will be implicitly rounded to the nearest number that fits in whatever format you are using. Gappa’s input language comes with a range of ways to specify that rounding explicitly. This allows Gappa to account of rounding errors introduced at each step when calculating the maximum overall error.
A natural question is whether this replaces one problem of formal verification with another. It is useful to apply Gappa to check the calculation in your function, but what happens if Gappa itself has a bug? Gappa performs complicated analysis, particularly for realistic calculations which have implicit roundings between most operations.
Gappa has a solution to this concern. Gappa can run in a mode where it calculates bounds on the query value and produces a formal proof of those bounds. The proof is generated in a format suitable for the Rocq theorem prover, formerly known as ‘Coq’, renamed in 2025. If Gappa’s output is accepted by Rocq, you can be confident it contains a complete and correct proof.
You do not have to trust Gappa itself to have got all the details of its numerical calculations right. You only need to trust the Rocq verifier which checks every step of Gappa’s working. Theorem proving is complex, but once a proof is expanded into simple enough steps, proof checking is much simpler. This approach minimizes the amount of code that must be trusted.
Translating ddiv’s mantissa division into Gappa
Once I selected Gappa to verify a bound on ddiv’s calculation, the first step is to explain to Gappa what ddiv’s calculation is.
I mentioned earlier that the basic strategy of ddiv’s calculation is to approximate the reciprocal 1 / d, and then multiply the result by n. More specifically, the reciprocal approximation is calculated using the standard Newton-Raphson iteration from calculus textbooks:
x → x − f(x) / f'(x)
Here, f is a function that is zero at the value you are trying to find, and f' is its derivative.
One difficulty with this technique for division is that the formula itself involves a division. It is easy to think of a function f that has a zero at 1 / d, but several of the obvious choices (such as f(x) = dx − 1) produce an iteration formula that requires you to divide by something. If we already knew how to divide, we would not be trying to write a division routine in the first place.
The secret is to use this function:
f(x) = 1/x − d
When you apply the Newton-Raphson formula to that function, the division by f' turns into a multiplication by x2. This cancels out the division by x in the numerator. You are left with an iteration involving only multiplications and subtractions:
x → x(2 − dx)
Like any Newton-Raphson iteration, this formula doubles the number of accurate bits in every iteration. To see that without calculus, suppose that x is already close to d, so that dx is close to 1, say 1 − ε. Then d times the next approximation is dx(2 − dx) = (1 − ε)(1 + ε) = 1 − ε2. The error is the square of the previous error.
Our ddiv function begins with an 8-bit approximation. In three iterations, it increases it to 16, 32 and then (very nearly) 64 bits of precision. Each of these iterations is done in handwritten Arm machine code. They are all different from each other, because when you are converting an 8-bit value to a 16-bit one, it is not necessary to compute 64 bits of any intermediate value.
For example, here is the machine code that computes the first iteration, turning an 8-bit approximation into 16 bits:
LSR     r5, r3, #16
MUL     r7, r6, r5
RSB     r7, r7, #1<<24
MUL     r7, r6, r7
LSR     r7, r7, #15
The three middle instructions, two MUL and an intervening RSB, are the main part of the formula. They multiply x by d, then subtract from 2, then multiply by x again. As all of this is scaled up to fit in integer registers, “subtract from 2” becomes “subtract from 224”.
The initial LSR prepares the value of d we will be using, by extracting just its top 16 bits. That is all we really need to make the output of this iteration accurate to 16 bits. By not using any more bits than we need, we avoid multiplications overflowing.
The final LSR discards the unwanted extra bits in the output from the second MUL. Otherwise, we would have a 32-bit value with only 16 of the bits being worthwhile.
To model those five instructions in Gappa, I ended up writing this, where recip08 represents the 8-bit approximation the iteration starts from, and recip16 is the 16-bit approximation it outputs:
recip16 = floor((recip08 * (0x1p24 - recip08 * floor(d / 0x1p48))/ 0x1p15);
The output looks different, both in syntax and semantics. The Gappa version of the code is thinking of d as a single 64-bit value. In the machine code it has to be split between two 32-bit Arm registers. Therefore, to extract its top 16 bits the Gappa code divides by 248 where the machine code divides by 216.
Each right shift operation is written as a division by a power of 2 followed by a call to the floor function (“round down to the next smaller integer”). That is how the machine code right-shift works. If I had not written floor explicitly, Gappa would assume that each of those divisions gave a fractional result, with all the low-order bits still there. The error analysis would then be based on that wrong assumption.
Making Gappa simulate a lookup table
Earlier, I stated that ddiv runs three Newton-Raphson iterations, starting from an 8-bit approximation to 1 / d. Where did that first 8-bit approximation come from?
In the ddiv code, it comes from a lookup table. We use the top 8 bits of d itself (including the leading bit which is always 1) as an index into a 128-entry lookup table. Each entry provides the best approximation to the reciprocal of each of those possible 8-bit values.
Gappa’s input language is good at describing arithmetic, and rounding, but not lookup tables. So how do we tell Gappa about this part of the algorithm?
I had hoped to achieve this by writing a complicated expression in the ‘question’ section of the input. When you specify the ranges of the input values, Gappa lets you do it with a Boolean expression, using the unusual ASCII operators /\ and \/ for ‘AND’ and ‘OR’ respectively (mimicking the mathematical logic operators ∧ and ∨). I thought perhaps I could treat my variable recip08 as one of the inputs to the whole calculation, and then write a large boolean expression with one clause per range of the input:
{ (d in [0x8000000000000000,0x80FFFFFFFFFFFFFF] /\ recip08 = 0xFF) \/
  (d in [0x8100000000000000,0x81FFFFFFFFFFFFFF] /\ recip08 = 0xFD) \/
  (d in [0x8200000000000000,0x82FFFFFFFFFFFFFF] /\ recip08 = 0xFB) \/
  ...
  (d in [0xFD00000000000000,0xFDFFFFFFFFFFFFFF] /\ recip08 = 0x81) \/
  (d in [0xFE00000000000000,0xFEFFFFFFFFFFFFFF] /\ recip08 = 0x81) \/
  (d in [0xFF00000000000000,0xFFFFFFFFFFFFFFFF] /\ recip08 = 0x80) -> ... }
Which reads as: “Either d  is in this range and recip08 has this value, or d is in that range and recip08 has that value, or” and so on for 128 cases.
This was too much for Gappa. It could not do anything useful with an expression this complicated. It thought for a long time and reported that it could not find a bound on the division error at all.
I resorted to Plan B: separate each of these cases into its own run of Gappa and just run Gappa 128 times. Each run is given a single range of d covering the catchment area of one entry in the lookup table, and a single fixed value of recip08 giving the appropriate value from the table. Then we record the error bound Gappa gave from each of those runs and find the worst case out of all of them.
Giving Gappa hints about the algebra
Even with the input ranges narrowed down to a single lookup table entry, Gappa did not output any useful bound on the error. The expectation was that the final 64-bit integer value of the scaled-up approximate quotient should differ from the true value by at most a few tens of units. Instead Gappa reported that the error could be anything in the range [−256, +256].
This does not mean I made a mistake in my Gappa input file. The cause was the same reason that my introductory example earlier derived the range [16, 36] for a value that was bounded in the much smaller range [24, 25]. In this case the same issue occurred on an even more extreme scale. The problem is that Gappa needed help rearranging the algebraic expressions into a form that would let it see why the error was small.
There is no easy way to know what you should do in this kind of situation. You can tell there is a problem in the first place because you get an absurdly large error interval as output. You can narrow down to a specific part of the calculation by asking Gappa to analyze the error in a particular intermediate value instead of the final output. Working back, you find the first intermediate value where it had trouble.
However, I could not find any way to get it to output the actual algebraic expression it was considering at the point where it failed. If I had known that, it would have been easy to suggest how it should rewrite that expression. Instead, I had to use a lot of trial and error, until I found a combination which worked.
The biggest problem is that Gappa did not recognize that the Newton-Raphson iteration converges quadratically. That is, the error after an iteration is proportional to the square of the error before, and therefore much smaller, if the initial error was already small. Here I had a piece of luck, because Newton-Raphson based division is one of the examples in Gappa’s own manual. There was already a worked example of how to give Gappa the right kind of hint: you tell it to rewrite:
x(2 − dx) − 1/d → −d(x − 1/d)2
The left-hand expression represents the absolute error in the output approximation, the difference between it and the true value of 1 / d). The rewritten expression includes the absolute error in the input approximation, squared. Gappa can then use the bound it has on the input error x − 1 / d to compute a usefully small bound on the output error.
It is one thing to see a worked example applying this to a simple use of Newton-Raphson division. It is another to get it to work in this much more complicated case involving lots of intermediate roundings and scalings. After some struggle, I found I could write a hint like this:
(recip08 * (0x1p24 - recip08 * (d/0x1p48))) / 0x1p15 - Recip16 ->
  (-(recip08 - Recip08) * (recip08 - Recip08) * d / 0x1p63)
  { d <> 0 };
In this case, the starting expression on the first line looks very like the calculation I showed earlier, without the floor operations. It describes a mathematically simpler calculation. Gappa works out that the expression we are really computing, with those floors, differs by only a small amount from the one shown in the hint. It takes account of that extra error by itself.
The capitalized variables Recip08 and Recip16 both represent the true mathematical value of 1 / d. However, each one is scaled by a different power of 2 to match what they look like in the machine code.
The hint must declare that d is not zero, to prevent Gappa getting confused by any attempt to divide by zero. It seemed odd to have to do that when I had already specified a range of values for d that did not include zero. However, the manual warned to do it anyway, and it was right.
There was one other problem. In the very last part of the calculation, I was surprised to have to give Gappa another hint. In ddiv’s final multiplication of the approximate value of 1 / d by n, some low-order terms are discarded to save a few instructions. The output is q + ε, where q is the accurate version of that product and ε is the error introduced by discarding terms. When I asked Gappa to tell me the difference between that and the true quotient Q = n/d, it could not do it until provided with a hint. The hint required rearranging the expression to take that error ε out from between the two values:
(q + ε) − Q → (q − Q) + ε
I was not surprised that Gappa needed help with Newton-Raphson. It is not obvious to humans either (we must be taught it). However, it was a surprise to have to help it with something this simple.
To be continued
In part 2, I talk about one of the most difficult parts of the problem. After writing this Gappa description of the calculation, how can we be sure that it really matches the version in the machine code?
I also describe the way this verification exercise revealed a bug in the original code, and the way it made it easy to fix.
Part 2]]></content:encoded>
        </item>
    </channel>
</rss>