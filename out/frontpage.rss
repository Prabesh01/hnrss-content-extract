<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 07 Sep 2025 10:07:44 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[I am giving up on Intel and have bought an AMD Ryzen 9950X3D]]></title>
            <link>https://michael.stapelberg.ch/posts/2025-09-07-bye-intel-hi-amd-9950x3d/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155986</guid>
            <description><![CDATA[The Intel 285K CPU in my high-end 2025 Linux PC died again! ðŸ˜¡ Notably, this was the replacement CPU for the original 285K that died in March, and after reading through the reviews of Intel CPUs on my electronics store of choice, many of which (!) mention CPU replacements, I am getting the impression that Intelâ€™s current CPUs just are not stable ðŸ˜ž. Therefore, I am giving up on Intel for the coming years and have bought an AMD Ryzen 9950X3D CPU instead.]]></description>
            <content:encoded><![CDATA[
  
  
    Table of contents
    
  
    What happened? Or: the batch job of death
    No, it wasnâ€™t the heat wave
    Which AMD CPU to buy?
    Performance
    Power consumption
    Conclusion
  

  
  The Intel 285K CPU in my high-end 2025 Linux
PC died again! ðŸ˜¡ Notably,
this was the replacement CPU for the original 285K that died in
March, and
after reading through the reviews of Intel CPUs on my electronics store of
choice, many of which (!) mention CPU replacements, I am getting the impression
that Intelâ€™s current CPUs just are not stable ðŸ˜ž. Therefore, I am giving up on
Intel for the coming years and have bought an AMD Ryzen 9950X3D CPU instead.
What happened? Or: the batch job of death
On the 9th of July, I set out to experiment with
layout-parser and
tesseract in order to
convert a collection of scanned paper documents from images into text.
I expected that offloading this task to the GPU would result in a drastic
speed-up, so I attempted to build layout-parser with
CUDA. Usually, itâ€™s not required to
compile software yourself on NixOS, but CUDA is non-free,
so the default NixOS cache does not compile software with CUDA. (Tip: Enable the
Nix Community Cache, which contains prebuilt
CUDA packages, too!)
This lengthy compilation attempt failed with a weird symptom: I left for work,
and after a while, my PC was no longer reachable over the network, but fans kept
spinning at 100%! ðŸ˜³ At first, IÂ suspected a Linux
bug, but now I am thinking this was
the first sign of the CPU being unreliable.
When the CUDA build failed, I ran the batch job without GPU offloading
instead. It took about 4 hours and consumed roughly 300W constantly. You can see
it on this CPU usage graph:


































On the evening of the 9th, the computer still seemed to work fine.
But the next day, when I wanted to wake up my PC from suspend-to-RAM as usual,
it wouldnâ€™t wake up. Worse, even after removing the power cord and waiting a few
seconds, there was no reaction to pressing the power button.
Later, I diagnosed the problem to either the mainboard and/or the CPU. The Power
Supply, RAM and disk all work with different hardware. I ended up returning both
the CPU and the mainboard, as I couldnâ€™t further diagnose which of the two is
broken.
To be clear: I am not saying the batch job killed the CPU. The computer was
acting strangely in the morning already. But the batch job might have been what
really sealed the deal.
No, it wasnâ€™t the heat wave
Tomâ€™s Hardware recently
reported
that â€œIntel Raptor Lake crashes are increasing with rising temperatures in
record European heat waveâ€, which prompted some folks to blame Europeâ€™s general
lack of Air Conditioning.
But in this case, I actually did air-condition the room about half-way
through the job (at about 16:00), when I noticed the room was getting
hot. Hereâ€™s the temperature graph:















I would say that 25 to 28 degrees celsius are normal temperatures for computers.
I also double-checked if the CPU temperature of about 100 degrees celsius is too
high, but no: this Tomâ€™s Hardware
article
shows even higher temperatures, and Intel specifies a maximum of 110
degrees. So, running at â€œonlyâ€ 100 degrees for a few hours should be fine.
Lastly, even if Intel CPUs were prone to crashing under high heat, they should
never die.
Which AMD CPU to buy?
I wanted the fastest AMD CPU (for desktops, not for servers), which currently is
the Ryzen 9 9950X, but there is also the Ryzen 9 9950X3D, a variant with 3D
V-Cache. Depending on the use-case, the variant with or without 3D V-Cache is
faster, see the comparison on
Phoronix.
Ultimately, I decided for the 9950X3D model, not just because it performs better
in many of the benchmarks, but also because Linux 6.13 and newer let you
control whether to prefer the CPU cores with larger V-Cache or higher
frequency,
which sounds like an interesting capability: By changing this setting, maybe one
can see how sensitive certain workloads are to extra cache.
Aside from the CPU, I also needed a new mainboard (for AMDâ€™s socket AM5), but I
kept all the other components. I ended up selecting the ASUS TUF
X870+
mainboard. I usually look for low power usage in a mainboard, so I made sure to
go with an X870 mainboard instead of an X870E one, because the X870E has two
chipsets (both of which consume power and need cooling)! Given the context of
this hardware replacement, I also like the TUF lineâ€™s focus on enduranceâ€¦
Performance
The performance of the AMD 9950X3D seems to be slightly better than the Intel
285K:

  
      
          Workload
          12900K (2022)
          285K (2025)
          9950X3D (2025)
      
  
  
      
          build Go 1.24.3
          â‰ˆ35s
          â‰ˆ26s
          â‰ˆ24s
      
      
          gokrazy/rsync tests
          â‰ˆ0.5s
          â‰ˆ0.4s
          â‰ˆ0.5s
      
      
          gokrazy Linux compile
          3m 13s
          2m 7s
          1m 56s
      
  

In case youâ€™re curious, the commands used for each workload are:

cd src; ./make.bash
make test
gokr-rebuild-kernel -cross=arm64

(I have not included the gokrazy UEFI integration tests because I think there is
an unrelated difference that prevents comparison of my old results with how the
test runs currently.)
Power consumption
In my high-end 2025 Linux PC I
explained that I chose the Intel 285K CPU for its lower idle power consumption,
and some folks were skeptical if AMD CPUs are really worse in that regard.
Having switched between 3 different PCs, but with identical peripherals, I can
now answer the question of how the top CPUs differ in power consumption!
I picked a few representative point-in-time power values from a couple of days
of usage:

  
      
          CPU
          Mainboard
          idle power
          idle power with monitor
      
  
  
      
          Intel 12900k
          ASUS PRIME Z690-A
          40W
          60W
      
      
          Intel 285k
          ASUS PRIME Z890-P
          46W
          65W
      
      
          AMD 9950X3D
          ASUS TUF GAMING X870-PLUS WIFI
          55W
          80W
      
  

Looking at two typical evenings, here is the power consumption of the Intel 285K:















â€¦and here is the same PC setup, but with the AMD 9950X3D:















I get the general impression that the AMD CPU has higher power consumption in
all regards: the baseline is higher, the spikes are higher (peak consumption)
and it spikes more often / for longer.
Looking at my energy meter statistics, I usually ended up at about 9.x kWh per
day for a two-person household, cooking with induction.
After switching my PC from Intel to AMD, I end up at 10-11 kWh per day.
Conclusion
I started buying Intel CPUs because they allowed me to build high-performance
computers that ran Linux flawlessly and produced little noise. This formula
worked for me over many years:

Back in 2008, I bought a mobile Intel CPU in a desktop case (article in
German).
Then, in 2012, I could just buy a regular Intel CPU (i7-2600K) for my Linux
PC, because they had gotten so
much better in terms of power saving.
Over the years, I bought an i7-8700K, and later an i9-9900K.
The last time this formula worked out for me was with my 2022 high-end Linux
PC.

On the one hand, Iâ€™m a little sad that this era has ended. On the other hand, I
have had a soft spot for AMD since I had one of their K6 CPUs in one of my early
PCs and in fact, I have never stopped buying AMD CPUs (e.g. for my Ryzen
7-based Mini
Server).
Maybe AMD could further improve their idle power usage in upcoming models? And,
if Intel survives for long enough, maybe they succeed at stabilizing their CPU
designs again? I certainly would love to see some competition in the CPU market.

  
    Did you like this
    post? Subscribe to this
      blogâ€™s RSS feed to not miss any new posts!
  
  
    I run a blog since 2005, spreading knowledge and experience for over 20 years! :)
  
  
    If you want to support my work, you
    can buy me a coffee.
  
  
    Thank you for your support! â¤ï¸
  


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The "impossibly small" Microdot web framework]]></title>
            <link>https://lwn.net/Articles/1034121/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155682</guid>
            <description><![CDATA[The Microdot web framework is quite small, as its name would imply; it supports both standard C [...]]]></description>
            <content:encoded><![CDATA[

Benefits for LWN subscribers

The primary benefit from subscribing to LWN
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!



The Microdot
web framework is quite small, as its name would imply; it supports both
standard CPython and MicroPython,
so it can be used on systems ranging from internet-of-things (IoT) devices
all the way up to large, cloudy servers.  It was developed by Miguel
Grinberg, who gave a presentation about it at EuroPythonÂ 2025.  His name
may sound familiar from his well-known Flask
Mega-Tutorial, which has introduced many to the Flask lightweight Python-based
web framework.  It should come as no surprise, then, that Microdot is
inspired by its rather larger cousin, so Flask enthusiasts will find much
to like in Microdotâ€”and will come up to speed quickly should their needs turn
toward smaller systems.



We have looked at various pieces of this software stack along the way: Microdot itself in January 2024, MicroPython in 2023, and Flask as part of a look at Python microframeworks in
2019.



Grinberg began his talk with an introduction.  He has been living in
Ireland for a few years and "I make stuff".  That includes open-source projects, blog posts (on a
Flask-based blog platform that he wrote), and "a bunch of books".
He works for Elastic and is one of the maintainers of the Elasticsearch
Python client, "so maybe you have used some of the things that I
made for money".


Why?


With a chuckle, he asked: "Why do we need another web framework?  We
have so many already."  The story starts with a move that he made to
Ireland from the US in 2018; he rented a house with a "smart" heating
controller and was excited to use it.  There were two thermostats, one for
each level of the house, and he was "really looking forward to the
winter" to see the system in action.



As might be guessed, he could set target temperatures in each thermostat;
they would communicate with the controller that would turn the heating on
and off as needed.  In addition, the system had a web server that could be
used to query various parameters or to start and stop the heaters.  You
could even send commands via SMS text messages; "there's a SIM card
somewhere in that box [...] very exciting stuff".



When winter rolled around, it did not work that well, however; sometimes
the house was too chilly or warm and he had to start and stop the heaters
himself. He did some debugging and found that the thermostats were
reporting temperatures that were off by Â±3Â°C, "which is too much for
trying to keep the house at 20Â°".  The owner of the house thought that
he was too used to the US where things just work; "at least she thinks that in America everything is super-efficient,
everything works, and she thought 'this is the way things work in
Ireland'".  So he did not make any progress with the owner.



At that point, most people would probably just give up and live with the
problem; "I hacked my heating controller instead".  He set the
temperatures in both thermostats to zero, which effectively disabled their
ability to affect the heaters at all, and built two small boards running
MicroPython, each connected to a temperature and humidity sensor device.
He wrote code that would check the temperature every five minutes and send
the appropriate commands to start or stop the heaters based on what it
found.



So the second half of his first winter in Ireland went great.  The sensors
are accurate to Â±0.5Â°C, so "problem solved".  But, that led to a new
problem for him.  "I wanted to know things: What's the temperature right
now?  Is the heating running right now or not?  How many hours did it run
today compared to yesterday?"  And so on.



He added a small LCD screen to display some information, but he had to
actually go to the device and look at it; what he really wanted was to be
able to talk to the device over WiFi and get information from the couch
while he was watching TV. "I wanted to host a web server [...]  that
will show me a little dashboard".



So he searched for web frameworks
for MicroPython; in the winter of 2018-2019, "there were none".
Neither Flask nor Bottle,
which is a good bit smaller, would run on MicroPython; both are too large
for the devices,
but, in addition, the standard library for MicroPython is a subset of that of
CPython, so many things that they need are missing. A "normal
person" would likely have just accepted that and moved on; "I
created a web framework instead."


Demo


He brought one of his thermostat devices to Prague for the conference and
did a small demonstration of it operating during the talk.  The device was
connected to his laptop using USB, which provided power, but also a serial
connection to the board.  On the laptop, he used the rshell
remote MicroPython shell to talk to the board, effectively using the laptop
as a terminal.





He started the MicroPython read-eval-print loop (REPL) on the board in
order to simulate the normal operation of the board.  When it is plugged
into the wall, rather than a laptop, it will boot to the web server, so he
made that happen with a soft-reboot command.  The device then connected to
the conference WiFi and gave him the IP address (and port) where the server
was running.



He switched over to Firefox on his laptop and visited the site, which showed a
dashboard that had the current temperature (24.4Â°) and relative humidity
(56.9%) of the room.  He also used curl from the laptop to contact the
api endpoint of the web application, which returned JSON with the
two values and the time.  There is no persistent clock on the board, so the
application contacts an NTP server to pick up the time when it boots; that
allows it to report the last time a measurement was taken.



Grinberg said that he wanted to set the expectations at the right level by
looking at the capabilities of the microcontrollers he often uses with
Microdot.  For example, the ESP8266 in his thermostat device has 64KB of
RAM and up to 4MB of flash.  The ESP8266 is the smallest and least expensive (around â‚¬5)
device with WiFi that
he has found; there are many even smaller devices, but they lack
the networking required for running a web server.  The other devices
he uses are the Raspberry Pi Pico W with 2MB of flash and 256KB of RAM and
the ESP32 with up to 8MB of flash and 512KB of RAM.  He contrasted those
with his laptop, which has 32GB of RAM, so "you need 500,000
ESP8266s" to have the same amount of memory.


Features


The core framework of Microdot is in a single microdot.py
file.  It is fully asynchronous, using the MicroPython
subset of the CPython
asyncio module, so it can run on both interpreters.  It uses
asyncio because that is the only way to do concurrency on the
microcontrollers; there is no support for processes or threads on those devices.



Microdot has Flask-style route
decorators to define URLs for the application.  It has Request
and Response
classes, as well as hooks
to run before and after requests, he said.  Handling query strings,
form data, and JSON are all available in Microdot via normal Python
dictionaries.  Importantly, it can handle streaming requests and responses;
because of the limited memory of these devices, it may be necessary to split
up the handling of larger requests or responses.



It supports setting
cookies and sending static
files.  Web applications can be constructed from a set of modules, using sub-applications,
which are similar to Flask
blueprints.  It also has its own web
server with TLS support.  "I'm very proud of all the stuff I was
able to fit in the core Microdot framework", Grinberg said.



He hoped that attendees would have to think for a minute to come up with
things that are missing from Microdot, but they definitely do exist.  There
are some officially
maintained extensions, each in its own single .py file, to
fill some of those holes.  They encompass functionality that is important,
but he did not want to add to the core because that would make it too large
to fit on the low-end ESP8266 that he is using.



There is an extension for multipart
forms, which includes file uploads; "this is extremely complicated
to parse, it didn't make sense to add it into the core because most people
don't do this".  There is support for WebSocket
and server-sent
events (SSE).  Templates
are supported using utemplate
for both Python implementations or Jinja, which only
works on CPython.  There are extensions for basic
and token-based authentication and for secure
user logins with session data; the latter required a replacement for
the CPython-only PyJWT, which Grinberg
wrote and contributed to MicroPython as jwt.py.
There is a small handful of other extensions that he quickly mentioned as well.



"I consider the documentation as part of the framework"; he is
"kind of fanatical" about documenting everything.  If there is
something missing or not explained well, "it's a bug that I need to
fix".  He writes books, so the documentation is organized similarly;
it comes in at 9,267 words, which equates to around 47 minutes of reading
time.  There is 100% test coverage, he said, and there are around 30
examples, with more coming.



A design principle that he follows is "no dark magic".  An example
of dark magic to him is the Flask
application context, "which very few people understand".  In
Microdot, the request object is explicitly passed to each route function.
Another example is the dependency
injection that is used by the FastAPI framework to add
components; Microdot uses explicit decorators instead.



He used the cloc
utility to count lines of code, while ignoring comments and blank
lines.  Using that, Django
comes in at 110,000 lines, Flask plus its essential Werkzeug library
is 15,500 lines, FastAPI with Starlette is 14,900 lines, Bottle is
around 3,000 lines, while the Microdot core has 765 lines ("believe it
or not") and a full
install with all the extensions on MicroPython comes in at just shy of 1,700
lines of code.



He ended with an example of how Microdot can be so small by comparing the
URL matching in Flask with Microdot.  The Flask version does lots more than
Microdot, with more supported types of arguments in a URL and multiple classes
in the werkzeug.routing
module; it has 1,362 lines of code.  For Microdot, there is a more
limited set of URL arguments, though there is still the ability to define
custom types, and a single
URLPattern class; all of that is done in 63 lines of
code. "I don't intend to support everything that Flask supports, in
terms of routing, but I intend to support the 20% that covers 80% of the
use cases."  That is the overall mechanism that he has used to get to
something that is so small.



An audience member asked about whether the Microdot code was minified in
order to get it to fit.  Grinberg said that doing so was not all that
useful for MicroPython, but the code is smaller on the board because it is
precompiled on another system; that results in a
microdot.mpy file, which is bytecode for MicroPython.  For
example, on the
low-end device he is using for his thermostats, Microdot would not be able
to be compiled on the device itself.  There are some other tricks that can
also be used for reducing the RAM requirements, like putting the code into
the firmware as part of the MicroPython binary.



The final question was about performance, and how many requests per second
could be handled. Grinberg said that he did not
have any real numbers, but that the device he demonstrated is "really
really slow".  That question led to a blog
post in late July where Grinberg tried to more fully answer it.



[I would like to thank the Linux Foundation, LWN's travel sponsor, for
travel assistance to Prague for EuroPython.]

           Index entries for this article
           ConferenceEuroPython/2025
            PythonWeb
            

            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Unofficial Windows 11 requirements bypass tool allows disabling all AI features]]></title>
            <link>https://www.neowin.net/news/unofficial-windows-11-requirements-bypass-tool-now-allows-you-to-disable-all-ai-features/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155398</guid>
        </item>
        <item>
            <title><![CDATA[The Claude Code Framework Wars]]></title>
            <link>https://shmck.substack.com/p/claude-code-framework-wars</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155302</guid>
        </item>
        <item>
            <title><![CDATA[RFC 3339 vs. ISO 8601]]></title>
            <link>https://ijmacd.github.io/rfc3339-iso8601/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45155179</guid>
        </item>
        <item>
            <title><![CDATA[The world has a running Rational R1000/400 computer again (2019)]]></title>
            <link>https://datamuseum.dk/wiki/Rational/R1000s400/Logbook/2019#2019-10-28</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154947</guid>
            <description><![CDATA[SCSI command 0x0d seems to be explained now:]]></description>
            <content:encoded><![CDATA[

2019-12-11
SCSI command 0x0d seems to be explained now:
SÃ¸ren Roug has spotted SASI command 0x0d in the manual for the Xebec S1410 diskcontroller:
It returns the length of burst errors corrected with ECC.
That matches the CMD_CORRECTION Tollef found.
Thanks everybody!

2019-12-08
About that 0x0d SCSI command...
My good friend Tollef Fog Heen replied on twitter that this source file on github calls the command CMD_CORRECTION.
That github repository contains a NeXT Station emulation.
The NeXT Station had a Magneto-Optical drive.
The first two pages of this bunch of papers that came with the machine, marked "aflasting" ("off-loading"), are jumper settings for Fujitsu's M2512A MO drive.
So the best theory now is that 0x0D is a way to probe for a MO drive configured as a disk drive.
I'll leave it at that for now, but I'm still interested to hear from anybody who spots 0x0d in old documents.
/phk

2019-12-07
Now it works!
My hacked up SCSI2SD firmware provided a log of all SCSI commands during boot
and the two crucial ones turns out to be:

   1a 00 03 00 24 00
   1a 00 04 00 20 00

Those are "MODE SENSE(6)" commands, asking for page 3 and 4 respectively.
Page 3 is "Format Device" and the FUJITSU M2266 returns:

   00 0f 00 00 00 00 00 2d 00 2d 04 00 00 01 00 05 00 0b 40 00 00 00
   
   Tracks per Zone:  15
   Alternate Sectors per Zone:  0
   Alternate Tracks per Zone:  0
   Alternate Tracks per Logical Unit:  45
   Sectors per Track:  45
   Data Bytes per Physical Sector:  1024
   Interleave:  1
   Track Skew Factor:  5
   Cylinder Skew Factor:  11
   SSEC:  0
   HSEC:  1
   RMB:  0
   SURF:  0

Page 4 is "Rigid Disk Geometry" where the FUJITSU M2266 returns:

   00 06 7a 0f 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
   
   Number of Cylinders:  1658
   Number of Heads:  15
   Starting Cylinder-Write Precompensation:  0
   Starting Cylinder-Reduced Write Current:  0
   Drive Step Rate:  0
   Landing Zone Cylinder:  0
   RPL:  0
   Rotational Offset:  0
   Medium Rotation Rate:  0

After I hacked the SCSI2SD firmware to return those values, the
R1000 booted flawlessly on the saved disk-images from PAM's machine.
Conclusion:  The IOC bootstrap uses "modern SCSI" with linear block-numbers, the real system works in terms of Cylinders, Heads and Sectors.
/phk

2019-12-05
A little bit more SCSI2SD progress today.
I compiled a custom firmware for the SCSI2SD to get more logging, still sampling, but it should capture all failing commands now.
The picture from the log is the following:
First some SCSI READ(10) commands, which are compatible with the loading of KERNEL, FS and PROGRAM.
Then a SCSI command 0x0d, which is unknown everywhere I have looked.
This command sets a flag in the KERNEL if successful, I have not investigated that flag further.
Next a SCSI MODE SENSE (RIGID DISK GEOMETRY)
Finally SCSI READ(6) commands which are past the DFS filesystem.
At the same time the console shows this:

   Disk  0 is ONLINE and WRITE ENABLED
   Disk  1 is ONLINE and WRITE ENABLED
   IOP Kernel is initialized
   Initializing diagnostic file system ... File does not exist ERROR_LOG
   [OK]

Working theory right now is that the R1000 uses the RIGID DISK GEOMETRY data to calculate disk access, and what the SCSI2SD returns does not work for this.
I queried one of the blank Fujitsu disks we got from Terma in our "dumper machine" and it returns:

   00 06 7a 0f 00 00 00 00
   00 00 00 00 00 00 00 00
   00 00 00 00 00 00
   
   Number of Cylinders:  1658
   Number of Heads:  15
   Starting Cylinder-Write Precompensation:  0
   Starting Cylinder-Reduced Write Current:  0
   Drive Step Rate:  0
   Landing Zone Cylinder:  0
   RPL:  0
   Rotational Offset:  0
   Medium Rotation Rate:  0

Whereas the SCSI2SD seems to return:

   00 38 30 06 00 00 00 00
   00 00 00 00 00 00 00 00
   00 00 1c 05 00 00
   
   Number of Cylinders:  14384
   Number of Heads:  6
   Starting Cylinder-Write Precompensation:  0
   Starting Cylinder-Reduced Write Current:  0
   Drive Step Rate:  0
   Landing Zone Cylinder:  0
   RPL:  0
   Rotational Offset:  0
   Medium Rotation Rate:  7173

My next experiment will be to make the SCSI2SD firmware emit the same RIGID GEOMETRY as the Fujitsu disk.

2019-11-20
I made a DFS backup from PAM's Fujitsu disks, and read the remaining 8mm tapes.
/phk

2019-11-14
Got a bit further with the SCSI2SD:  1024 byte sector size helps, still get a DFS kernel panic though:

   Initializing M400S I/O Processor Kernel 4_2_18
   Disk  0 is ONLINE and WRITE ENABLED
   Disk  1 is ONLINE and WRITE ENABLED
   IOP Kernel is initialized
   Initializing diagnostic file system ... File does not exist ERROR_LOG
   [OK]

   I/O Processor Kernel Crash: error 0614 (hex) at PC=0000849E
   ************************************************

Next, tried to restore the backup of PAM's disks onto the Seagate disks, looks like success:

   --- Booting the R1000 Environment ---
     Loading from file M207_44.M200_UCODE  bound on November 5, 1992 at 6:08:17 PM
     Loading Register Files .... [OK]
   LoadingÂ : KAB.11.0.1.MLOAD
   LoadingÂ : KMI.11.0.0.MLOAD
   LoadingÂ : KKDIO.11.0.3.MLOAD
   LoadingÂ : KKD.11.0.0S.MLOAD
   LoadingÂ : KK.11.5.9K.MLOAD
   LoadingÂ : EEDB.11.2.0D.MLOAD
   LoadingÂ : UOSU.11.3.0D.MLOAD
   LoadingÂ : UED.10.0.0R.MLOAD
   LoadingÂ : UM.11.1.5D.MLOAD
   LoadingÂ : UAT.11.2.2D.MLOAD
   851/1529 wired/total pages loaded.
       The use of this system is subject to the software license terms and
       conditions agreed upon between Rational and the Customer.
   
                            Copyright 1992 by Rational.
   
                             RESTRICTED RIGHTS LEGEND
   
       Use, duplication, or disclosure by the Government is subject to
       restrictions as set forth in subparagraph (c)(1)(ii) of the Rights in
       Technical Data and Computer Software clause at DoD FAR Supplement
       252.227-7013.
   
                   Rational
                   3320 Scott Blvd.
                   Santa Clara, California 95054-3197
   
   Starting R1000 Environment - it now owns this console.
   
   ====>> Kernel.11.5.8 <<====
   Kernel: CHANGE_GHOST_LOGGING
   WANT TRACING: FALSE
   WANT LOGGING: FALSE
   Kernel: START_VIRTUAL_MEMORY
   ALLOW PAGE FAULTS: YES
   
   ====>> ERROR_LOG <<====
   22:55:05 --- TCP_IP_Driver.Worker.Finalized
   
   ====>> CONFIGURATOR <<====
   starting diagnosis of configuration
   recovery is needed
   WANT TO BUILD NEW SYSTEM (YES/NO): yes
   starting creation of new system
   VOLUME NAME FOR unit 0: volume 1
   VOLUME NAME FOR unit 1: volume 2
   creating root volume: 1
   adding volume 2 to virtual memory system
   creation of new system is complete
   starting virtual memory system
   the virtual memory system is up
   
   ====>> Kernel.11.5.8 <<====
   Kernel: START_NETWORK_IO
   Kernel: START_ENVIRONMENT
   TRACE LEVEL: INFORMATIVE
   Kernel:
   
   ====>> Environment Elaborator <<====
   Elaborating subsystem:  ENVIRONMENT_DEBUGGER
   Elaborating subsystem:  ABSTRACT_TYPES
   Elaborating subsystem:  MISCELLANEOUS
   Elaborating subsystem:  OS_UTILITIES
   
   ====>> Recovery <<====
   Recovery Is Needed, Should I fake it? no
   Please tell me Volume Id for the Backup Index Tape:
   
   ====>> SYSTEMÂ % RECOVERY <<====
   
   Please Load Tape
     (Kind of Tape    => CHAINED_ANSI,
      Direction       => READING)
   Is Tape mounted and ready to read labels? yes
   Info on tape mounted on drive 0 is:
     (Kind of Tape    => CHAINED_ANSI,
      Writeable       => FALSE,
      Volume Id       => 028600,
      Volume Set Name => BACKUP, 09-MAR-01 16:01:45 3)
   OK to read volume? [YES]
   
   ====>> Recovery <<====
   Positioning tape to Backup Index
   Processing Backup Index
      Processing Tape File: Vol Info
      Processing Tape File: VP Info
      Processing Tape File: DB Backups
      Processing Tape File: DB Processors
      Processing Tape File: DB Disk Volumes
      Processing Tape File: DB Tape Volumes
   Positioning tape to Backup Data
   Processing Backup Data
      Processing Tape File: Space Info Vol 1
      Processing Tape File: Block Info Vol 1
      Processing Tape File: Block Data Vol 1
      Processing Tape File: Space Info Vol 2
      Processing Tape File: Block Info Vol 2
      Processing Tape File: Block Data Vol 2
   
   ====>> SYSTEMÂ % RECOVERY <<====
   
   Please Dismount Tape on Drive 0
     (Kind of Tape    => CHAINED_ANSI,
      Volume Id       => 028600,
      Volume Set Name => BACKUP, 09-MAR-01 16:01:45 3)
   
   ====>> Recovery <<====
   Tape Processing Complete.
   Restoring Data
   Restoring Spaces
   Taking 1st snapshot
   
   ====>> CONFIGURATOR <<====
   starting snapshot
   snapshot is finished
   
   ====>> Recovery <<====
   Updating Databases
   Taking 2nd snapshot
   
   ====>> CONFIGURATOR <<====
   starting snapshot
   snapshot is finished
   
   ====>> Recovery <<====
   Recovery Is Complete
   Garbage Collection can't run until the machine is rebooted
   Rebooting to enable Garbage Collection
   
   ====>> CONFIGURATOR <<====
   starting virtual memory shutdown
   starting snapshot
   snapshot is finished
   virtual memory shutdown at ( 3, 26-MAR-01 04:22:46)
   system shutdown is complete
   
   ***************************************
   Sequencer has detected a machine check.
   
   ************************************************
   Booting R1000 IOP after R1000 Halt or Machine Check detected
     Boot Reason code = 0C, from PC 0001ADA2
   
   Restarting R1000-400S March 26th, 1901 at 04:23:01
   
   OPERATOR MODE MENU - options are:
       1 => Change BOOT/CRASH/MAINTENANCE options
       2 => Change IOP CONFIGURATION
       3 => Enable manual crash debugging (EXPERTS ONLY)
       4 => Boot IOP, prompting for tape or disk
       5 => Boot SYSTEM
   
   Enter option [Boot SYSTEM]Â :

The disks were formatted yesterday, so I do not have a total time for the restore, but reading the backup tape took 7 hours.

2019-11-13
Tried if the R1000 would accept a SCSI2S without much luck.
The RESHA EEPROM issues the usual terse style error message saying simply "SCSI Error", giving no usable details.
Formatted the Seagate disks so they are ready for an attempt to restore from the Backup-Tape.
If the SCSI2SD is in the machine along with disks, some DFS operations work, but since no defect list can be read, preparing the disk fails.
The two Fujitsu disks with the working image was never powered up today.
/phk

2019-10-29

R1000-400 with Facit A-4600 terminal showing the Rational Environment. 
Note the keyboard overlay that extends a couple of inches beyond the keyboard.


Keyboard overlay for the Rational Environment.


The front plate of a running R1000-400.
==

2019-10-28
The world has a running Rational R1000/400 computer again:

Thanks a LOT to Pierre-Alain Muller for driving all the way here to help make this happen!

2019-10-24
On suggestion by Pierre-Alain todays aim was an FRU diagnostics.
First the faulty Fujitsu disk was removed and the Seagate promoted to SCSI ID 0.
Then we got a "File does not exist HARDWARE.M200_CONFIG". M200, hmm... - We did the following:

  OPERATOR MODE MENU - options are:
      1 => Change BOOT/CRASH/MAINTENANCE options
      2 => Change IOP CONFIGURATION
      3 => Enable manual crash debugging (EXPERTS ONLY)
      4 => Boot IOP, prompting for tape or disk
      5 => Boot SYSTEM
  
  Enter option [Boot SYSTEM]Â : 1
  Enable Modem DIALOUT [N]Â ? 
  Enable Modem ANSWER [N]Â ? 
  Enable IOP (IOC 68K) Auto Boot [N]Â ? y
  Enable R1000 CPU Auto Boot [N]Â ? n
  Enable AUTO CRASH RECOVERY [N]Â ? y
  Enable CONSOLE BREAK KEY [N]Â ? y
  Are these new defaults [N]Â ? y

then

  CLI/CRASH MENU - options are:
    1 => enter CLI
    2 => make a CRASHDUMP tape
    3 => display CRASH INFO
    4 => Boot DDC configuration
    5 => Boot EEDB configuration
    6 => Boot STANDARD configuration
  Enter option [Boot EEDB configuration]Â : 1
  CLI> x cedit
  Change hardware configuration [N]Â ? y
  File does not exist HARDWARE.M200_CONFIG
  Does this processor have 32 MB memory boards [Y]Â ? 
  NOTE: 32 MB boards must be installed as MEM 0 or MEM 2 only.
        8 MB boards cannot be in the same CPU as 32 MB boards.
  Does memory board 0 exist [Y]Â ? y
  Does memory board 2 exist [Y]Â ? n
  CLI> x expmon
  0  32MB MEMORY BOARDS IN PROCESSOR - TOTAL OF 0 MEGABYTES. 
  EM> bye
  CLI> quit

At this point we rebooted, but the memory board was still not detected. We then decided to take all 8 boards out of the machine starting with the two MEM 0/2 boards, photographing and then re-seating them. The 2 memory boards looked identical, so we switched their position. At the following boot we got: "Memory 2 exists but is not in configuration.  Board will not be used.", so we changed the HW config again and included both MEM boards. We don't believe the changed positions did anything, but perhaps the power-cycle was needed to get them detected right?!
At this point we attempted the FRU-diagnostics:

  CLI> x expmon
  2  32MB MEMORY BOARDS IN PROCESSOR - TOTAL OF 64 MEGABYTES. 
  EM> rd
  ...
  EM> poll_all
  NO MACHINE CHECKS DETECTED 
  EM> poll_all
  SEQ HAS DETECTED A MACHINE CHECK 
  EM> sm
  ...
  UCODE HALT AT 0102 
  ...
  EM> bye
  CLI> 
  CLI> x rdiag
  ...
  DIAG> test/3 all
  Running FRU P1DCOMM
  Running FRU P1IOC
  Running FRU P1VAL
  Running FRU P1TYP
  Running FRU P1SEQ
  Running FRU P1FIU
  Running FRU P1MEM
  Running FRU P1SF
  Running FRU P2IOC
  Running FRU P2VAL
    Loading from file PHASE2_MULT_TEST.M200_UCODE  bound on July 16, 1986  14:31:44
    Loading Register Files and Dispatch Rams .... [OK]
    Loading Control Store  [OK]
  Running FRU P2TYP
  Running FRU P2SEQ
  Running FRU P2FIU
  Running FRU P2MEM
  Running FRU P2UADR
  Running FRU P2FP
    Loading from file FPTEST.M200_UCODE  bound on January 29, 1990 17:26:52
    Loading Register Files and Dispatch Rams .... [OK]
    Loading Control Store  [OK]
  Running FRU P2EVNT
  Running FRU P2STOP
  Running FRU P2ABUS
    Loading from file ABUS_TEST.M200_UCODE  bound on July 22, 1986  13:27:21
    Loading Register Files and Dispatch Rams .... [OK]
    Loading Control Store  [OK]
  Running FRU P2CSA
  Running FRU P2MM
  Running FRU P2COND
  Running FRU P2UCODE
    Loading from file P2UCODE.M200_UCODE  bound on August 6, 1986  16:16:27
    Loading Register Files and Dispatch Rams .... [OK]
    Loading Control Store .......... [OK]
  Running FRU P3RAMS
  Running FRU P3UCODE
    Loading from file P3UCODE.M200_UCODE  bound on August 6, 1986  13:50:42
    Loading Register Files and Dispatch Rams .... [OK]
    Loading Control Store . [OK]
  PASSED

"PASSED"!!!
And a bit later the following line: 

  Starting R1000 Environment - it now owns this console.

Cleaned up log from the most interesting part of the session: Fil:20191024 1915 R1000.pdf

2019-10-17
Peter has returned with a working PSUÂ :-) - The R1000 is now up and running again drawing 155 amps.
Almost full log of the session: Fil:20191017 1954 R1000.pdf
The Fujitsu 2266 disk appears to be faulty, R1000 complains about disk errors (our old acquaintance General Error?):

  Options are:                                                                          
      0 => Exit                                                                         
      1 => Initialize disk (for experts only)
      2 => Initialize disk, drop USR defects (internal use only)
      3 => Show MFG and USR bad block locations
      4 => Show only USR bad block locations
      5 => Install new DFS only
      6 => Show bad block count and DOS limits
  Enter optionÂ : 3
  Enter unit number of disk to format/build/scan (usually 0)Â : 0
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=4038 CS2=0040 DS=11C0 ER1=0000 ER2=0000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  CS1=0038 CS2=0040 DS=11C0 ER1=0100 ER2=4000 EC1=0000 EC2=0000 DC=0000 DA=0005
  ** ABORT: Can't retrieve labels due to disk errors.

A Seagate ST41200N is now installed as DISK 1, the Fujitsu remains as DISK 0 for now.
R1000 recognizes the Seagate but wants to format it:

  Initializing M400S I/O Processor Kernel 4_2_16
  Spinning up disk 1
  Spinning up disk 0
  Disk  1 is ONLINE and WRITE ENABLED
  IOP Kernel is initialized
  Enable line printer for console output [N]Â ? 
      RECOVERY 14.04 92/09/17 10:00:00\
  Options are:
      0 => Exit
      1 => Initialize disk (for experts only)
      2 => Initialize disk, drop USR defects (internal use only)
      3 => Show MFG and USR bad block locations
      4 => Show only USR bad block locations
      5 => Install new DFS only
      6 => Show bad block count and DOS limits
  Enter optionÂ : 3
  Enter unit number of disk to format/build/scan (usually 0)Â : 1
  ** ABORT: Disk has no labels.
  Options are:
      0 => Exit
      1 => Initialize disk (for experts only)
      2 => Initialize disk, drop USR defects (internal use only)
      3 => Show MFG and USR bad block locations
      4 => Show only USR bad block locations
      5 => Install new DFS only
      6 => Show bad block count and DOS limits
  Enter optionÂ : 1
  Enter unit number of disk to format/build/scan (usually 0)Â : 1
  Disk has no labels.
  Drive types are:
      1 - Fujitsu 2263
      2 - Fujitsu 2266
      3 - SEGATE ST41200N
      0 - Other
  Enter drive typeÂ : 3
  Enter HDA serial numberÂ : TJ617458
  Disk must be formated.
  Formatting the drive will take about 35 minutes.
  Elapsed time is 00:32:32
  Writing bad block information.
  Writing boot label.
  Writing DFS label.
  Do you want to build a diagnostic file system on this unit [Y]Â ? 
  Enter last cylinder to be used by the DFS [ Hint => 76 ]:76
  Enter first cylinder to be used for read/write diagnostics [ Hint => 1889 ]:1889
  Writing shared label.
  Constructing free list.
  Writing free list.
  Allocating and initializing directory.
  Creating predefined files.
  KERNEL_0.M200
  KERNEL_1.M200
  KERNEL_2.M200
  FS_0.M200
  FS_1.M200
  FS_2.M200
  PROGRAM_0.M200
  PROGRAM_1.M200
  PROGRAM_2.M200
  DFS_BOOTSTRAP.M200
  ERROR_LOG
  Do you want to load files into the DFS on this unit [Y]Â ? y
  Tape drive unit numberÂ : 0
  Do you want to display filenames as they are loaded [Y]Â ? y
  Reading -> DFS_BOOTSTRAP.M200
  Reading -> KERNEL_0.M200
  ... 3160 files later ...
  Reading -> DDC.M200_CONFIG
  Elapsed time is 00:10:40
  Options are:
      0 => Exit
      1 => Initialize disk (for experts only)
      2 => Initialize disk, drop USR defects (internal use only)
      3 => Show MFG and USR bad block locations
      4 => Show only USR bad block locations
      5 => Install new DFS only
      6 => Show bad block count and DOS limits
  Enter optionÂ : 0
  Boot disk has been rebuilt or the IOP was booted from tape.
  You must crash the machine to exit.

Next week, boot from disk and see how far we get.
PSU got *hot*, but survived the 1Â½ hour session.

2019-10-03
After rumaging through our entire workshop, it transpires that we have no solder-iron with sufficient power to unsolder the capacitors from the thick copper on the PCB.
One of our members, Peter, has offered to attempt the repair in his own workshop, and he picked it up tonight.

2019-09-12
PSU dismantled, and the visible defective Electrolyte has been soldered out together with two Tantalum. Unfortunately, when mounted, the leads were pinched and cut, making them difficult to pull through the PCB today since the holes are almost exactly the size of the leads. The insulation on the wires to the transformer has deteriorated quite a bit and will need some repair.
Some better images of the PSU as a whole:

Overview


One half of the PSU


Other half of the PSU


Defect insulation on some wires


One of the two capacitor boards on the 5V rails.


Backside of PCB carrying the damaged capacitor.
Each of the two PCBs carry: 5 Tantalum capacitors 15ÂµF, and 5 6800ÂµF SXF 30mm x 18mm, lead spacing 7.5mm

2019-09-05
Arriving today, expecting a fight, armed with various debugging plans, the Rational just started, booted and were happy as could be?!? - After some configuration, and booting the kernel "M400S_KERNEL_0.M200" (thanks to Pierre-Alain for supplying that information), the R1000 now responds with:

   R1000-400 IOC SELFTEST 1.3.2 
      512 KB memory ... [OK]
      Memory parity ... [OK]
      I/O bus control ... [OK]
      I/O bus map ... [OK]
      I/O bus map parity ... [OK]
      I/O bus transactions ... [OK]
      PIT ... [OK]
      Modem DUART channel ... [OK]
      Diagnostic DUART channel ... [OK]
      Clock / Calendar ... [OK]
  Checking for RESHA board
      RESHA EEProm Interface ... [OK]
  Downloading RESHA EEProm 0 - TEST
  Downloading RESHA EEProm 1 - LANCE 
  Downloading RESHA EEProm 2 - DISK  
  Downloading RESHA EEProm 3 - TAPE  
      DIAGNOSTIC MODEM ... DISABLED
      RESHA VME sub-tests ... [OK]
      LANCE chip Selftest ... [OK]
      RESHA DISK SCSI sub-tests ... [OK]
      RESHA TAPE SCSI sub-tests ... [OK]
      Local interrupts ... [OK]
      Illegal reference protection ... [OK]
      I/O bus parity ... [OK]
      I/O bus spurious interrupts ... [OK]
      Temperature sensors ... [OK]
      IOC diagnostic processor ... [OK]
      Power margining ... [OK]
      Clock margining ... [OK]
  Selftest passed
  
  Restarting R1000-400S January 14th, 1901 at 22:56:43
  
  OPERATOR MODE MENU - options are:
      1 => Change BOOT/CRASH/MAINTENANCE options
      2 => Change IOP CONFIGURATION
      3 => Enable manual crash debugging (EXPERTS ONLY)
      4 => Boot IOP, prompting for tape or disk
      5 => Boot SYSTEM
  
  Enter option [Boot SYSTEM]Â : 5
  
  Logical tape drive 0 is an 8mm cartridge tape drive.
  Logical tape drive 1 is declared non-existent.
  Logical tape drive 2 is declared non-existent.
  Logical tape drive 3 is declared non-existent.
  Booting I/O Processor with Bootstrap version 0.4
  
  Boot from (Tn or Dn)  [D0]Â : T0
  
  Tape_Boot_1.2.0  920401
  Waiting for tape unit ready.
  Strike any key to abort.....................
  End of Tape Reached.rewinding
  
  Select files to boot [D=DEFAULT, O=OPERATOR_SUPPLIED]Â : [D]
  Skipping..
  Loading FS_0.M200
  
  Loading RECOVERY.M200
  Skipping.................
  Loading M400S_KERNEL_0.M200
  
  Initializing M400S I/O Processor Kernel 4_2_16
  Spinning up disk 0
  IOP Kernel is initialized
  Enable line printer for console output [N]Â ? 
      RECOVERY 14.04 92/09/17 10:00:00\
  Options are:
      0 => Exit
      1 => Initialize disk (for experts only)
      2 => Initialize disk, drop USR defects (internal use only)
      3 => Show MFG and USR bad block locations
      4 => Show only USR bad block locations
      5 => Install new DFS only
      6 => Show bad block count and DOS limits
  Enter optionÂ : 
  *** AC power is L

The last line is probably from the time where I cut the power after seeing significant white-gray smoke coming up from the machine...
The following smell-test suggested that the PSU should be checked:

The capacitors are located between the 5V power rails (the two black blocks on each side of the cap).
Next job: Acquire and exchange 10 x 6800ÂµF 6.3V capacitors (L<31, D<=18)

2019-08-29
Disappointment! - We had hoped to get further in the boot process, but met an "unwilling" machine that didn't even presented itself. The PSU powered up with its fan, but that was it - No lights, no 5V, -12V or +12V.
During power-off, these lights turned for a very short period, indicating the PSU is capable but unwilling (Inhibit line active?).
We are not completely sure of the reason, but we will debug the issue next week, starting with checking the RESHA diagrams, followed up by checking the IOC RTC-battery (which were replaced last week).

2019-08-22
After further tests we finally took the leap and grabbed the cutter and solder iron, and replaced the three suspected memory chips. Boot sequence with the original BIOS now gives:

   R1000-400 IOC SELFTEST 1.3.2 
      512 KB memory ... [OK]
      Memory parity ... [OK]
      I/O bus control ... [OK]
      I/O bus map ... [OK]
      I/O bus map parity ... [OK]
      I/O bus transactions ... [OK]
      PIT ... [OK]
      Modem DUART channel ... [OK]
      Diagnostic DUART channel ... [OK]
      Clock / Calendar ... [OK]
  Checking for RESHA board
    --  Bench mode (ID 7) detected Skipping RESHA tests
      Local interrupts ... [OK]
      Illegal reference protection ... [OK]
      I/O bus parity ... [OK]
      I/O bus spurious interrupts ... [OK]
      Temperature sensors ... [OK]
      IOC diagnostic processor ... [OK]
      Power margining ... [OK]
      Clock margining ... [OK]
  Selftest passed
  
  Restarting R1000-400S January 1st, 1901 at 00:03:56
  
  Logical tape drive 0 is an 8mm cartridge tape drive.
  Logical tape drive 1 is declared non-existent.
  Logical tape drive 2 is declared non-existent.
  Logical tape drive 3 is declared non-existent.
  Booting I/O Processor with Bootstrap version 0.4
  
  Boot from (Tn or Dn)  [D0]Â : 

Success!
Next step: Mount the board into its rightful place and see how far it gets now...

2019-06-06
Decided to make further measurements with the oscilloscope in order to rule out other causes.
Probing all pins on a good and a bad RAM chip did not reveal anything.
Tried to piggyback H11 with a new RAM-chip, and the questionable bit went mid between VCC and GND. It could be that the good chip tried to pull down while the bad chip pulled up. Double-checked chip-select on H3, the only other identified chip that may drive the same data line (D30), and chip-select is completely passive during the troublesome period.
Previous tests went between [0x00000000..0x00040000[ and [0x00040000..0x00080000[. Now tried to run the tests between [0x00001000..0x00021000[ and [0x00041000..0x00061000[ as well as between [0x00001200..0x00021200[ and [0x00041200..0x00061200[ to test whether run-length could be a factor. The exact same failing addresses indicate run-length is not a factor.
Tried a reverse scan, Set(addr), Clr(addr), Set(addr) then Get(addr-4). The same data bits are affected in both banks, but the failing addresses are not identical. Some patterns are similar, and some other patterns appears.
Tried another test with: Set(addr), Clr(addr), Set(addr), Get(addr+4), branch-test delay, then another Get(addr+4) - The second Get reads out correctly. this indicates that the chip does have the right value, but that it is incorrectly read out in some circumstances.

2019-05-23
Previous software tests indicated problems with certain bits at some address patterns: Bits 7 and 23 in the low bank and bit 30 in the high bank showed issues. The fault manifests itself at some addresses when the following memory accesses are done in quick succession: Set(addr), Clr(addr), Set(addr) then Get(addr+4) - The Get(addr+4) returns incorrect values only on these bits.
Today all RAM chips were checked with oscilloscope to verify and possibly identify the problem. H11, G10 and G41 showed different behavior on the oscilloscope, and these chips happens to map to the exact bits identified at the software test.
H40 did show a little flickering on the DC levels, but the flanks seemed OK.
The above input to the RAM-chips looks like this:

The output of a healthy chip looks like this:

The output of the sick chips looks like this:


Next step will be to replace them. We have replacements ready, so stay tuned...

2019-03-07
Tried to patch the EEPROM in various ways, and learned a lot more.
If we skip the offending memory check, (and the EEPROM checksum because we're lazy) we get all
the way to the boot device prompt (tape/disk).
We got two-way serial connection to the console port: TX is TTL level, RX is RS-232 level.
In trivial homebrew tests, the RAM does not fail, but what we call "ramtest_5" repeatedly does.
Big discovery of the night:  The two top address bits of the EEPROM are swapped on the PCB, so the middle two quarters are swapped in the image we try to reverse-engineer.  After fixing that, the contents make a lot more sense.

2019-02-28
Managed to power up the IOC board stand-alone. 5V @ 35A required (30A didn't seem to be quite enough).
Procedure:

5V @ 35A connected to 3 Capacitors at edge (to distribute load).
Reset (GB113) to Ground (page 23 of R1000_SCHEM_IOC.pdf).
CTS# (GB055) to 5V (page 25)
Power On
Release Reset (GB113)
TTL Serial output read from CPDRV0 @N1 (pin 2 or 3).
As expected output is still:

  R1000-400 IOC SELFTEST 1.3.2 
     512 KB memory ... * * * * * * * FAILED

EEPROM 28256 is not compatible with EPROM 27256!





]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: Lightweight tool for managing Linux virtual machines]]></title>
            <link>https://github.com/ccheshirecat/flint</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154857</guid>
            <description><![CDATA[Lightweight tool for managing linux virtual machines - ccheshirecat/flint]]></description>
            <content:encoded><![CDATA[â¸»
ðŸŒ€ Flint â€” Lightweight KVM Management, Reimagined

  


  
    A single <8 MB binary with a modern Web UI, CLI, and API for KVM.
    No XML. No bloat. Just VMs.
  


  
    
  
  
    
  
  
    
  

â¸»
ðŸš€ Install Flint
curl -fsSL https://raw.githubusercontent.com/ccheshirecat/flint/main/install.sh | bash
â€¢	Auto-detects OS + arch
â€¢	Pulls the latest release
â€¢	Installs to /usr/local/bin/flint
â€¢	Ready in seconds

â¸»
âœ¨ Why Flint?
Flint is a modern KVM manager in a single binary.
Built for developers, sysadmins, and home labs who want zero bloat.
â€¢	ðŸ–¥ï¸ Modern UI â€” Next.js + Tailwind, fully embedded.
â€¢	âš¡ Single Binary â€” No containers, no XML hell, <8 MB.
â€¢	ðŸ› ï¸ Non-Intrusive â€” Lives on your host, never inside it.
â€¢	ðŸ“¦ Fast Provisioning â€” Built-in Cloud-Init + managed image library.
â€¢	ðŸ”Œ Full API + CLI â€” Automate everything without touching YAML.

  

â¸»
ðŸŽï¸ Quickstart

Start the server

flint serve
â€¢	Web UI â†’ http://localhost:5550
â€¢	CLI â†’ flint vm list
â€¢	API â†’ GET http://localhost:5550/api/vms

â¸»

CLI examples

List VMs
flint vm list
Start a VM
flint vm start ubuntu-dev
Snapshot before upgrading
flint snapshot create ubuntu-dev --tag before-upgrade
Launch a VM with custom resources
flint launch ubuntu-24.04 --name api --vcpus 4 --memory 8192
â¸»
ðŸ“¦ Feature Matrix
Feature	Flint	virt-manager / cockpit	OpenStack / Proxmox
Web UI	âœ… Embedded	âœ… GUI-based	âœ…
Single Binary	âœ… <8 MB	âŒ Heavy GUI stack	âŒ Requires cluster
CLI	âœ… Built-in	âŒ Limited	âŒ External tooling
API	âœ… Built-in	âŒ	âœ…
Cloud-Init	âœ… Native	âš ï¸ Limited	âœ…
Dependencies	libvirt only	Heavy desktop deps	Heavy orchestration
â¸»
ðŸ”§ Tech Stack
â€¢	Backend: Go 1.25
â€¢	Web UI: Next.js + Tailwind + Bun
â€¢	Embedding: go:embed
â€¢	KVM Integration: libvirt-go
â€¢	Binary Size: ~8.4 MB stripped
â¸»
ðŸ“¥ Downloads
ðŸ“¦ Prebuilt binaries available for:
â€¢	Linux AMD64 âœ…
â€¢	Linux ARM64 âœ…
â€¢	macOS Intel/Apple Silicon âœ…
âž¡ï¸ Get the latest release
â¸»
ðŸ§  Philosophy
â€œJust give me a binary.
Let me manage VMs without XML or Kubernetes.â€
Flint isnâ€™t trying to be Proxmox or OpenStack.
Itâ€™s lightweight, self-contained, and hackable.
â¸»
ðŸ› ï¸ Roadmap
â€¢	ðŸ”Œ Plugin system for storage + networking
â€¢	ðŸ—„ï¸ SeaweedFS & Dragonfly storage drivers
â€¢	ðŸ£ MicroVM & Firecracker integration
â€¢	âš¡ Hot snapshot boot + tmpfs acceleration
â¸»

  ðŸš€ Flint is young, fast-moving, and designed for hackers.
  Try it. Break it. Star it. Contribute.

â¸»
TL;DR â€” One Liner Install
curl -fsSL https://raw.githubusercontent.com/ccheshirecat/flint/main/install.sh | bash
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I recreated Windows XP as my portfolio]]></title>
            <link>https://mitchivin.com/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154609</guid>
            <description><![CDATA[A Windows XPâ€“style interactive portfolio showcasing design, video, and UI work by Mitch Ivin.]]></description>
            <content:encoded><![CDATA[To begin, click on Mitch Ivin to log inMitch IvinGraphic DesignerRestart MitchIvin XPAfter you log on, the system's yours to explore. Every detail has been designed with a purpose. Tap on the user icon to begin]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The key to getting MVC correct is understanding what models are]]></title>
            <link>https://stlab.cc/tips/about-mvc.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45154501</guid>
            <description><![CDATA[stlab hosts modern, modular C++ algorithms and data structures.]]></description>
            <content:encoded><![CDATA[
        Smalltalk MVC is defined in Design Pattern as:


  MVC Consists of three kinds of objects. The Model is the application object, the View is its screen presentation, and the Controller defines the way the user interface reacts to user input.1


However this definition has been abused over the years - Back in 2003 I gave a talk citing how bad Appleâ€™s definition was. At the time it stated:


  A view object knows how to display and possibly edit data from the applicationâ€™s modelâ€¦ A controller object acts as the intermediary between the applicationâ€™s view objects and its model objectsâ€¦ Controllers are often the least reusable objects in an application, but thatâ€™s acceptableâ€¦2


Of course it isnâ€™t acceptable and, over the years, Apple has refined their definition and now acknowledge the distinction between the traditional Smalltalk version of MVC and the Cocoa version.3 But the Cocoa version is still defined much as it was before:


  A view object knows how to display, and might allow users to edit, the data from the applicationâ€™s modelâ€¦ A controller object acts as the intermediary between the applicationâ€™s view objects and its model objectsâ€¦3


In looking at how iOS applications are written the sentiment that controllers (and now view-controllers) are often the least reusable components in an application still flourishes, even if it is now unstated.

MVC (Iâ€™ll always use that term to refer to the Smalltalk form) has the following structure:















figure: Smalltalk MVC4


Here the solid lines imply a direct association. And the dashed lines an indirect association by an observer. So what we see is that the model is unaware of the view and controller, except indirectly through notifications, and hence the code in the Model is reusable. The controller and view bind to the model, not the other way around.

Often the function of the Controller and View are tightly coupled into a â€œwidgetâ€ or â€œcontrolâ€. When Apple talks about a View-Controller in their model they are talking about a grab-bag of an uber-widget that is a composite of UIView widgets and multiple models. From what Iâ€™ve seen, including in Appleâ€™s example code, it is usually a pretty big mess.

The key to getting MVC correct is understanding what models are. A model is simply an object5 which can be observed (a requirement for attaching views). For example, in ObjC an int is an object, but it is not observable. However, an ObjC object with an int property is observable using Key-Value Observing6.  A model may encapsulate complex relationships between the modelâ€™s properties. A trivial model is one where each property is completely independent (think C struct vs. C++ class). From a notification the view should be able to determine, at a minimum:


  What changed. It may be as simple as â€œthe model bound to the viewâ€.
  The new value to display.


For example, letâ€™s say our model is a trivial observable boolean (I canâ€™t imagine a simpler model). What we want is a checkbox that binds to the observable boolean. When the controller requests a change in value, the boolean is updated, and the view is notified of the new state of the model. The model is unaware of what UI is attached to it, and in fact there could be multiple UIs, including something like a scripting system, attached to the same instance of the model. This is a form of data binding - though most data binding systems replicate the problems of their underlying widget set by treating the model as if it were observing the view, not the other way around.

Contrast this with most UI frameworks where you have a checkbox widget from which you can query the value and you receive a notification when the value has changed. This is pushing a model into the widget. With MVC you never ask a question like â€œwhat is the default state of this checkbox?â€ - the default state of the view is always the current state of the model. You would also never get the state of the checkbox - the state of the checkbox is simply a reflection of the state of the model. In a system where you get the state of a checkbox you are binding two models together by treating one as a view/controller of the other. Such a pattern doesnâ€™t scale beyond trivial models, and even for those it introduces some ambiguity.

I conjecture that one of the reasons why MVC has been so screwed up is because, unlike in Smalltalk, writing something as simple as an observable boolean is a bit of a pain in a language like Pascal or C. You quickly get into object ownership and lifetime issues and how to write bind expressions. If one also assumes that you have a 1:1 mapping from UI to model then there is some inherent inefficiency in the generalization. The Lisa team made some major compromises and the rest of the industry followed along.7

To support more complex views, the notification may need to specify what parts of the model changed and how those parts changed. For example, â€œimage 58 was removed from the sequenceâ€. A complete model is one that can support any view of that model type efficiently (related to the notion of a complete type and a typeâ€™s efficient basis).

One additional attribute of MVC is that it is a composite pattern. This is hinted at by the direct connection between the Controller and the View. As I said early, the view may contain state, this state is itself an object, and because this state is also displayed within the view it is observable. It is another model. I refers to this as the viewâ€™s model. This model may include things such as the visibility of a window, the tab the user was last looking at, and the portion of the model being viewed.

Identifying what the models are in your system is important. We usually do pretty good at identifying the major models. Such as â€œthis is an imageâ€ - but often fall short of identifying the complete model, i.e. â€œthis is an image with a collection of settings.â€ We end up with our model spread out within the code (an incidental type) and it makes it more difficult to deal with it.

A common model that is often completely overlooked is the model for function arguments. When you have a command, button, gesture, or menu item in your application, these are bound to a function. The function itself is not typically a zeroary function but rather has a set of arguments that are constructed through other parts of the UI. For example, if I have a list of images in my application, I might have a button to delete the selected images. Here the current selection is the argument to my delete command. To create a UI for the selection I must create a model of the arguments to my function. A precondition of delete is that the selection is not empty. This precondition must be observable in the argument model so it can be reflected in the view by disabling or hiding the button and in the controller be disallowing the user to click the button and issue the command. The same argument model can be shared for multiple commands within an application.


  
    

      Gamma, Erich. â€œ1.2 Design Patterns in Smalltalk MVC.â€ Design Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-Wesley, 1995. N. pag. Print.Â â†©
    
    

      http://smartfriends.com/U/Presenters/untangling_software.pdf (Donâ€™t bother reading, this was an incomprehensible talk.)Â â†©
    
    

      https://developer.apple.com/library/content/documentation/General/Conceptual/CocoaEncyclopedia/Model-View-Controller/Model-View-Controller.htmlÂ â†©Â â†©2
    
    

      https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controllerÂ â†©
    
    

      Stepanov, Alexander A., and Paul McJones. â€œ1.3 Objects.â€ Elements of Programming. Upper Saddle River, NJ: Addison-Wesley, 2009. N. pag. Print.Â â†©
    
    

      https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/KeyValueObserving/KeyValueObserving.htmlÂ â†©
    
    

      https://en.wikipedia.org/wiki/Object-oriented_user_interfaceÂ â†©
    
  


    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A history of metaphorical brain talk in psychiatry]]></title>
            <link>https://www.nature.com/articles/s41380-025-03053-6</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45153428</guid>
            <description><![CDATA[From the very beginnings of our field in the late 18th century, psychiatrists have engaged, often extensively, in â€œmetaphorical brain talkâ€ â€“ rephrasing descriptions of mental processes in unconfirmed brain metaphors (e.g., â€œdiseased working of the brain convolutionsâ€). In the late 19th century, Kraepelin criticized the later developments of such approaches, termed â€œbrain mythologyâ€ by the philosopher/psychiatrist Jaspers in 1913. In this essay, I review the history, meaning, and significance of this phenomenon and reach four conclusions. First, this trend has continued to the present day in metaphors such as the â€œbroken brainâ€ and the use of simplistic and empirically poorly supported explanations of psychiatric illness, such as depression being â€œdue to an imbalance of serotonin in the brain.â€ Second, our language stems from the tension in our profession that seeks to be a part of medicine yet declares our main focus as treatment of the mental. We feel more comfortable with the reductionist approach of brain metaphors, which, even though at times self-deceptive, reinforce our commitment to and membership in a brain-based medical specialty. Third, metaphorical brain talk can also be seen as the â€œpromissory noteâ€ of our profession, a pledge that the day will come when we can indeed explain accurately to ourselves and to our patients the brain basis of the psychiatric disorders from which they suffer. Finally, moving away from metaphorical brain talk would reflect an increasing maturity of both the research and clinical aspects of our profession.]]></description>
            <content:encoded><![CDATA[
                    
                        Psychiatry emerged as a medical specialty between 1780 and 1830 committed both to the care of the mentally ill and to the brain as the seat of these disturbances [1]. This created an inherent tension, because for all of our history up to the present day, with rare exceptions, such as general paresis of the insane, we have had little insight into exactly how disturbances in the brain cause psychiatric disorders. This is not the case with most other organ-based medical specialties, like obstetrics and ophthalmology, where organ-specific pathologies could be related to many observed clinical syndromes. This article seeks to describe how the tension between the mental, clinical phenomenon we treat and our commitment to brain-based explanations as a medical specialty have played out over the history of our discipline.We proceed historically and focus on a concept I call â€œmetaphorical brain talk,â€ defined as describing the disturbed mental processes in psychiatric illness in terms of brain function in ways that appear to be explanatory but actually have little to no explanatory power. I will work my way through (i) 19th century asylum psychiatry, (ii) the rise and then decline of the first biological psychiatric revolution in the 1860sâ€“70s and 1880â€“1910 respectively, (iii) meet the key figure of Meynert and the application to his work of the concept of â€œbrain mythologyâ€ by Karl Jaspers, (iv) trace a few examples of metaphorical brain talk into the 20th century, and (v) review a more positive view of brain talk â€“ as instantiating a promissory note of our profession to one day clarify the brain mechanisms underlying psychiatric illness. I conclude by summarizing the significance of this phenomenon, postulating that it addresses a foundational conflict in our medical profession of psychiatry between our commitment to the study and treatment of the disturbed mental phenomenon and our identification with the brain as our organ of interest.Metaphorical brain talk in asylum psychiatry 1780â€“1900Here are twelve examples of metaphorical brain talk sampled from over a century from physicians who cared for and wrote, largely in asylums, about the nature and origins of what they would have called madness or insanity. Italics are added throughout this essay for emphasis:Cullen 1784 â€“ Delusions, he suggests â€œmay depend â€¦ upon some inequality in the excitement of the brain â€¦[for] the proper exercise of our intellectual functions, the excitement must be complete, and equal in every part of the brain â€¦. so, if any part of the brain is not excited, or not excitable, that recollection cannot properly take place, while at the same time other parts of the brain, more excited and excitable, may give false perceptions, associations, and judgmentsâ€ [2] pp. 130â€“131.Hartley 1834 â€“ Who, in describing a potential mechanism for delusions, writes: â€œthus suppose a person, whose nervous system is disordered, to turn his thoughts accidentally to some barely possible good or evil. If the nervous disorder falls in with this, it increases the vibrations belonging to its ideas so much, as to give it a realityâ€¦â€ [3] p. 252.Laycock 1845 â€“ He explains the origin of obsessions â€œ..by which is termed the association of ideas, the morbid action of vesicular neurine be brought within the current of his thought, he becomes utterly powerless to resist itâ€¦â€ [4]. [Neurine, an alkaloid found in egg yolk, brain, bile and decaying flesh arises from the putrefaction of biological tissues and is a syrupy liquid with a fishy odor.]Monro 1851 â€“ He proposes a theory of insanity: â€œ1. That it is an affection consequent on depressed vitality [which] manifest itself with peculiar and specific force in the cerebral masses, owing to a congenital, and frequently hereditary, tendency in the brain thus to succumb when oppressed by any exciting cause. 2. That when the cerebral masses are suffering from this condition of depressed vitality, they lose that static equilibrium of the nervous energies which we call toneâ€ [5] p. 76.Dickson 1874 â€“ The characteristic of some forms of insanity is excitement and vividness of impression, but this excitement and vividness always emanate from one portion or spot of the brain which pays out its functional activity rapidly and unchecked or uncorrected by the [brain] portions whose functions are arrested or in abeyance [6] p. 11.Ball and Ritti 1881 â€“ the cells of the cortex we are told are the organ of intelligence; it is therefore right that to their disorder, whether anatomical or physiological, the production of delusions should be attributed [7] p. 337.Stearns 1883 â€“ The Professor takes notice of two states of the brain; the one he terms excitementâ€”the other collapse. Collapse may be defined a morbid diminution of the tone of the brain, and of the motion of the nervous fluid. The term excitement must be obvious to everyone â€¦ sometimes a collapse of one part of the brain interrupts the communication of the due excitement of the whole, and thus induces delirium [delusions] [8] p. 6â€“8.Savage 1884 â€“ With melancholia we meet with a slowing of all vital processes. What the pathological basis of melancholia may be one cannot at present tell. It however seems, in most cases, that it must be associated with impaired nutrition of the nervous centres and the conducting system [9] p. 130.Clouston 1892 â€“ An â€œinsane delusionâ€ may therefore be defined to be â€œa belief in something that would be incredible to people of the same class, education, or race as the person who expresses it, the belief persisting in spite of proof to the contrary, this resulting from diseased working of the brain convolutionsâ€ [10] p. 244.Clark 1895 â€“ The factor which is inherited [in severe mental illness] cannot be insanity per se, but may be an instability or disordered arrangement of nerve tissue [11] p. 238.Maudsley 1895 â€“ In a description of melancholia, he writes â€œThe probable pathological condition of things is an exorbitant and predominant, almost exclusive, activity of certain brain-tracts charged with sad feeling not unlike the sort of activity which has motor issue otherwise in spasm or convulsion of musclesâ€ [12] p. 195.Kellogg 1897 â€“ In explaining hallucinations, he write that â€œThe main pathogenic fact is that the sensory image is aroused in the higher brain centres with such unnatural force as to be projected outwardly as an objective realityâ€ [13] p. 152.The goal of each of these examples of brain talk is to explain insanity itself or some more specific features like delusions or melancholia in terms of brain function or structure. But the descriptions are non-specific and/or metaphorical and lacked substantial biological meaning. We find general constructs such as â€œinequality of excitement,â€ â€œmorbid action of vesicular neurine,â€ â€œdiminution of the tone of the brain,â€ â€œimpaired nutrition of the nervous centres,â€ â€œdiseased workings of the brain convolutions,â€ and â€œdisordered arrangement of nerve tissue.â€ Nearly all these observations could have been stated in mental language, but were not.Brain talk and the first biological revolution in psychiatry 1870sâ€“1880sThe first biological revolution in psychiatry, the turn to neuroanatomy and neuropathology, was largely stimulated by Wilhelm Griesinger (1817â€“1868) [14, 15], the first Professor of Psychiatry in German speaking Europe and the authors of an influential textbook of psychiatry [16,17,18]. He established the important journal Archives of Psychiatry and Nervous Diseases in 1868 and in his preface to the first issue wrote:Psychiatry has undergone a transformation in its relation to the rest of medicine.â€¦ This transformation rests principally on the realization that patients with so-called â€˜mental illnessesâ€™ are really individuals with illnesses of the nerves and brain [14] p. 76.Eric Engstrom, a leading historian of German Psychiatry, captures the critical change that Griesinger initiated in German psychiatry:During the 1850s and 1860s, his textbook had served as a powerful catalyst to an entire generation of students â€¦ who wholeheartedly adopted Griesingerâ€™s conviction that â€œmental disease were brain diseasesâ€ and who went about applying anatomic and physiologic methods of inquiryâ€ [15] p. 90.His students, particularly Westphal, Meynert, and Wernicke, became the first generation of academic psychiatrists in the world, pursuing biological/neuropathological research careers outside of asylums. Griesinger strongly advocated for the establishment of psychiatric autopsies as the key research method. Disciplined science applied to the brains of the diseased mentally ill would, he asserted, reveal an anatomical diagnosis, the true essence of mental illness [15]. In the increasing number of university-based departments of psychiatry in German-speaking Europe established in this era, the leading scientific paradigm, the high prestige research programs, were in neuroanatomy and neuropathology.The excitement of this era in psychiatry was well captured by Otto Binswanger in 1892:Under the influence of the tremendous progress of medical science in anatomy and physiology [and] â€¦ the increase in the use of the microscopeâ€¦ a degree of disdain for imponderable psychic influences became the norm. This progress literally intoxicated the heads of many; in heated efforts to derive the cause â€¦ of all pathological life processes from the fundamental precepts of biological [and anatomical] workâ€¦ [19] p. 53.But the excitement was short lived. The hope that gross and/or microscopic examination of the brains of the mentally ill would yield insights into etiology of the classical psychiatric disorders was not fulfilled. But something else happened in the wake of these failures, a proliferation of more elaborate metaphorical brain talk.The reaction to this â€œRevolutionâ€In a section of his history of this era, roughly 1880â€“1910, entitled â€œNeuropathology in Retreat,â€ Engstrom begins:At the close of the century, a number of critics of neuropathology emerged within the ranks of the profession to add their voice to [other] longstanding â€¦ objections â€¦ the fruits of anatomical research turned out to be less than originally expectedâ€¦. The potential of anatomical research had been decidedly overestimated and â€¦ researchers had fallen victim to speculative interpretations of their laboratory resultsâ€¦.Research had stagnated in what had become an era of abstract theories and schematic models of brain function in which researchers appears to take little notice of alternative theories or of contradictory evidence [15] p. 123.One early critique of this new neuropathological biological psychiatry was provided by a 31-year-old Emil Kraepelin. In the inaugural lecture for his first professorship at the University of Dorpat in 1887 [20], he summarized the current progress in psychiatry and provided a strong critique of the biological theorizing then occurring from prominent neuroanatomical psychiatric researchers, especially Meynert [21], who was then working at the University of Vienna. He began by noting that â€œthe impossibility of a satisfactory solution [to the mind-body problem] â€¦ has led to numerous attempts to bridge the gap separating events of the body and mind by means of airy constructions of speculative fantasyâ€ p. 351. This focus on highly speculative brain-based theories â€œâ€¦ explain the strange fact that psychiatry still finds itself in an era of fruitless hypotheses and theories â€“ an era long surpassed by the rest of medical scienceâ€¦â€ [21] p. 351.Then, he directly confronts the reductionist biological theories of Meynert, in which he defined his science as a clinical science of the diseases of the anterior brain, its structure, capabilities and nutriments. In the field of cerebral anatomy, in spite of wholly inadequate factual grounding, he ingeniously understood and described the general blueprint of the organs of the central nervous system [21] p. 353.Meynert, according to Kraepelin, took the concept of localization of cerebral functions to implausible extremes.He does not even shy away from the shocking implication â€¦ that every cell â€¦ must be viewed as the seat of a specific and discrete idea; and that the associative linkage of that idea with other mental elements is mediated through connective pathways that he calls associative strands. The second key idea behind Meynertâ€™s theories is taken from neural physiology and is expressed in the terms â€˜stimulationâ€™ and â€˜inhibition.â€™ As soon as each individual mental function has been associated with the various parts of the brain, then naturally it suffices to posit an inhibition here and a stimulation there in order to explain the most diverse combinations of phenomena [21] p. 353.Kraepelin continues to his key point, noting that some of Meynertâ€™s anatomical claims had already been contradicted by recent research.It is possible that specific aspects of this system will establish themselves as an enduring part of our science. But if that is to be the case, then it will happen only after much long, arduous and detailed work has been completedâ€¦ Only time will tell whether, as in the case of anatomy, the basic idea represents an intuitive knowledge of reality and proves to be true. But it is dangerous â€“ and Meynertâ€™s school has not entirely escaped the ruinous consequences â€“ to want immediately to move into and live in Meynertâ€™s airily-constructed house, before its foundation has been securely laid and hard work has tightened up, one by one, the loose joints of his theoryâ€¦ [21] p. 354.He then generalizes his criticism to a number of the highly speculative reductionist physiological theories postulated during this time period:While Meynertâ€™s constructions are based on assumptions that appear to be open to empirical inquiry at some point in time, there is no shortage of [other] theories built on ground that will probably forever remain in the realm of hypothesis. Recall for example the attempts to transcribe simple [mental] processes into the language of psychophysics, or the efforts to trace the origins of mental disorder back to the gradual death of the core neural material, or finally the ingenious essays that plumb the depths of molecular mechanics in order to arrange the physiological processes that supposedly occur in the disordered brain in terms of the interaction of elementary forces, the exchange of kinetic into potential energy and vice versa, or even the movement of atoms. The impartial observer will barely be able to contain his astonishment when he sees how, in the very medical discipline most lacking in factual and scientifically useful empirical evidence, there exists a flourishing tendency to extend theory all the way back to the most primal state of the phenomena [21] p. 354.More on MeynertAs a major figure in the history of neuroanatomy and chair of the department of psychiatry in Vienna from 1873â€“1892, Theodor Meynert (1833â€“1892) is worthy of more attention as he typifies in this historical era, the excess exuberance of metaphorical brain talk. Here is an extended quote from a careful review of his career and research by the eminent Austrian neurologist and historian â€“ Franz Seitelberger [22]:Meynert carried his morphological research to its attainable limits and then transgressed them by hypotheses â€¦ [His] model, while partly based on empirical morphological findings, also involved some deductive speculative thinking, devoid of proof, yet fused into a coherent whole. The structural element of the brain, the nerve cell is, in Meynertâ€™s view, endowed with a â€˜soulâ€™â€¦ Meynert conceives of the function of the cerebral hemispheres as analogous to â€œcolonies of living beings, capable of consciousness, connected with each other with feelings threads â€¦ and controlling their image of the worldâ€ [23] p. 269.We will not detail with other aspects of his system except to note that some tracts in the brain reflected â€œunconscious animal life,â€ while others, especially the cortex, constituted the â€œego-forming functional center of the brain.â€His developed distinct theories of psychotic and mood-based psychopathologies, with the former arising fromDisturbed cooperation between the various parts of the brain, a disturbance of association coordination often caused by â€œfunctional disturbances of the social nature of the brainâ€ [23] p. 270.By contrast, his theory of affective disturbances, was an elaboration of a very old view, based in humoral medicine, of excess or deficient blood flow. Seitelberger summarizes his theory:They combined the assumption of an aggressive nature of neuronal activity with the functional role of the cerebral blood flow and the antagonism between cortex and subcortical centersâ€¦. The feeling of unhappiness is linked with an inhibition or suppression of the neuronal activity â€¦ and corresponding reduction in blood flow. Affects and therefore manifestations of the nutritional state of the cortical neurons [23] pp. 270â€“271.Another psychiatric historian, Janzarik, summarized Meynertâ€™s neuropathological work as followsParadigmatic for the spirit of the time is the highly speculative fusion of psychopathological findings with anatomical findings and pathogenetic claims, which in Meynert, following an old tradition, relate in particular to vascular-dependent differences in brain nutrition [24] pp. 598â€“599.Meynertâ€™s own student, Auguste Forel, called his brain pathways â€œfantastical constructionsâ€ [25] p. 177. We conclude with one further critique from Phelps:What about Meynertâ€™s work made it cogent and compelling? What about it seemed capable of dispelling the mystery of the nervous system and dispensing with the unique inwardness of the mind? Part of the answer â€¦ lies with Meynertâ€™s images of the brainâ€™s material and the collocation of its fleshy, fibrous inner stuff with the interiority of the mindâ€¦ By delineating various â€˜fibre-systemsâ€™ in the brain and nerves, then deducing their different functions on the basis of their winding â€˜pathwaysâ€™, Meynert elaborated new shapes and textures inside the brain and by doing so, he elaborated â€¦ [and] fleshed-out functions of the mindâ€¦ But even as he imagined these fibres as pathways or tracks inside the brain, he co-extended them and co-located them as pathways somehow equally inside the mind [26]. P. 389â€“390His success, Phelps suggests, was due to his ability â€œto connect what he saw inside the brain with an image of what he described as â€˜insideâ€™ the mind [by] â€¦ his combination of material and metaphorical techniquesâ€ [26] pp. 394.Metaphorical brain talk in the 20th centuryWe first turn to Adolf Meyer, the most influential psychiatrist in the US over the first 3rd of the 20th century [27]. It should be recalled that until World War II, American psychiatry was a rather small profession, largely composed of superintendents of mental hospitals who largely had a biologically orientation to their work. In 1907, while the director of the New York Psychiatric Institute, Meyer wrote about his concerns of the narrow views that US physicians would typically take in their approach to psychiatric illness that likely reflected his views about the excesses of earlier authors like Meynert:Instead of analyzing the facts in an unbiased way and using the great extension of our experience with mental efforts to get square with things â€¦ they pass at once to a one-sided consideration of the extra-psychological components of the situation, abandon the ground of controllable observation, translate what they see into a jargon of wholly uncontrollable brain-mythology, and all that with the conviction that this is the only admissible and scientific way [28] p. 172.Next, we examine a text on a similar theme from a quite different source - the psychiatrist-philosopher Karl Jaspers. In his the introduction to the first (1913) edition of General Psychopathology [29], he writes:The still widespread â€œsomatic prejudiceâ€ is: everything mental cannot be examined as such, it is merely subjective. If it is to be discussed scientifically, it must be presented anatomically, physically, as a physical function; for this it is better to have a preliminary anatomical construction, which is considered heuristic, than a direct psychological investigation. Such anatomical constructions are quite fantastic (Meynertâ€¦) and are rightly called â€œbrain mythologies.â€ Things that have no connection to one another, such as cortical cells and memory images, brain fibers and psychological associations, are brought together. There is also no basis for these mythologies insofar as not a single specific brain process is known that could be assigned to a specific mental process as a direct parallel phenomenon [29] p. 8 (KSK translation).We might think that recent scientific developments over the rest of the 20th century eliminated the need for metaphorical brain talk. We provide three examples suggesting that this is not the case.First, in a series of articles published over several decades, the distinguished psychologist Paul Meehl proposed a cognitively and psychometrically sophisticated genetic single-locus model for schizophrenia spectrum disorders. A core part of this theory was equating cognitive and neurobiological parts of his theory, as expressed here in 1962:The cognitive slippage is here conceived as a direct molar consequence of synaptic slippage, potentiated by the disruptive effects of aversive control and inadequate development of interpersonal communication sets [30] p. 834.His phrase, â€œsynaptic slippage,â€ has commonly been repeated in the subsequent literature [31, 32].Second, in 1985, a leading biological psychiatrist, Nancy Andreasen, published a widely-cited book whose title was a paradigmatic example of metaphorical brain talk: The Broken Brain [33]. She writes, for example, that recent advances in research have â€œtaught us that many forms of mental illness are due to abnormalities in brain structure or chemistry. Psychiatry is moving from the study of the â€œtroubled mindâ€ to the â€œbroken brainâ€ [33] p. VIII. In a later section, she describes, using broad metaphors, the kinds of brain abnormalities that occur in psychiatric disorders.The various forms of mental illness are due to many different types of brain abnormalities â€¦ sometimes the fault maybe in the pattern of the wiring or circuitry, sometimes in the command centers, and sometimes in the way messages move along the wiresâ€ [33] p. 221.Third, in an important development in the history of neuroscience, in the early 1960s, cell bodies and neuronal pathways of the putative monoamine neurotransmitters dopamine, norepinephrine, and serotonin were demonstrated in mammalian brains [34,35,36]. Within a few years, prominent psychiatric researchers proposed that abnormalities of function in these neurotransmitters were the major cause of three of the most important of psychiatric disorders: schizophrenia, mania and depression [37,38,39,40]. I suggest that these theories reflect, in more subtle ways than prior examples, metaphorical brain talk. Or, perhaps these monoamine hypotheses could be seen as sitting somewhere on a continuum of naively enthusiastic scientific theories and metaphorical brain talk. They were grounded in solid basic neuroscience, and had support from pharmacologic studies of mechanisms of action of antipsychotic and antidepressant medication [41]. However, trying to clarify disease etiology through the mechanism of action of pharmacologic treatments is deeply problematic as illustrated by the now common phrase: â€œheadache is not an aspirin-deficiency diseaseâ€ [42]. Given the more than 100 neurotransmitters in the mammalian brain, the plausibility that dysfunctions in the first three to be traced in the brain caused the major psychiatric disorders strains to the breaking point any sense of credulity. Furthermore, these theories, for example, the serotonin hypothesis of depression [43,44,45,46,47], have not fared well over time. Large-sample, genome-wide association studies are now available on all three disorders, and none support a major role for genetic variants involved in the dopamine, norepinephrine, and serotonin systems for, respectively, schizophrenia [48], bipolar illness [49], and depression [50]. A recent widely-cited umbrella review concludes, â€œThe main areas of serotonin research provide no consistent evidence of there being an association between serotonin and depression, and no support for the hypothesis that depression is caused by lowered serotonin activity or concentrationsâ€ [46] p. 3243. While there remains substantial controversy about the precise etiological relationship between serotonin and depression [43, 51, 52], I do not seek to deny that serotonin may play some role in the complex pathophysiology of this syndrome. Rather, I suggest, less controversially, I hope, that the grand monocausal theory of depression resulting primarily from serotonin dysfunction (as I have argued earlier, regarding the dopamine hypothesis of schizophrenia [42]) is almost certainly false.Importantly, these monoamine theories have had impacts outside of our research world. I recently heard the following story from friend of a friend, who knew I was a psychiatrist:I was feeling really down and my family doctor referred me to a local psychiatrist, Dr. C. We talked for 30â€‰min. He said I had what he called â€œmajor depressionâ€ and this was due to an imbalance in my brain serotonin. He said I should take the medicine he prescribed and it would correct that imbalance. Three weeks later I was feeling a lot better. I was really impressed.While certainly more false than true, this example of metaphorical brain talk remains a common story in psychiatric culture. Some of us tell stories like this to our patients, in part because it may make them feel better and also perhaps because it is a story we like to tell. Metaphorical brain talk can be popular.While my focus in this essay is the internal history of the psychiatric profession, there remains one elephant in the room that needs brief attention. When psychiatric research came to seek external financial support, simplistic metaphorical brain talk often appealed to prospective funders. This continues to the present day. Even more powerfully, advertising of psychopharmacological drugs, rising to prominence in the last third of the 20th century, often revolved centrally around metaphorical brain talk. While we like to claim that such advertising has no effect on us, this is naÃ¯ve. An empirical review of this question, while far outside my remit, would, I suspect, find such advertisements have significantly increased the modern popularity of metaphorical brain.Why brain talk?Metaphorical brain talk has arisen out of a foundational feature of our profession. Psychiatry began and remains a profession that treats disorders whose major clinical manifestations are in mental space â€“ symptoms â€“ as well as resulting signs and disturbed behaviors. But, we are also a medical specialty and consider our association to medicine central to our mission and professional identity. Most other medical specialists have organs of special focus â€“ ophthalmology the eye, cardiologists the heart, gynecologists the uterus and ovaries. In its first decades in the late 18th and early 19th centuries, psychiatry, for quite logical reasons, chose the brain [1]. But, some 80 years later, neurology began to develop and took with it nearly all the diseases where, given the methods of gross and histological pathology then available, one could track those disorders back to definable brain or nervous system lesions. Ever since, our relationship with the brain has been an ambivalent one, the reasons for which can be simply explained. The indirect evidence that the symptoms of our disorders are instantiated in brains is overwhelming and new genetic studies are providing a further sound footing to this long widely accepted belief [53]. However, despite decades of research, and tremendous progress in basic neuroscience, imaging technologies, and molecular genetics, we still have no good idea exactly how psychiatric disorders emerge from disturbances in brain function. This is an uncomfortable position for our field to be in. Metaphorical brain talk has, for now more than two centuries, been one way to patch over this discomfort.The medical historian Rosenberg can help us here, first in a â€œstatusâ€ report on our discipline:Since its origins as a specialty in the 19th century, psychiatry has â€¦ suffered from a recurrent status anxietyâ€”one might call it procedure envy, or organic inferiority. Psychiatry has been chronically sensitive to its inability to call upon a repertoire of tightly bounded, seemingly objective, and generally agreed-upon diagnostic categories based firmly on biopathological mechanisms â€¦ Psychiatry remains the legatee of the emotional, the behavioral, and the imperfectly understood. In this sense it has been a poor relation of its specialist peers in surgery and internal medicine [54] p. 124.Second, in reviewing the history of 19th century medicine and the success of its system of primitive etiologic theories, he makes the following point relevant to my argumentâ€¦the [medical] system provided a rationalistic framework in which the physician could at once reassure the patient and legitimate his own ministrationsâ€¦. The physicianâ€™s own self-image and his social plausibility depended on the creation of a shared faith â€“ a conspiracy to believe - in his ability to understand and rationally manipulate the elements in this speculative system [55] P. 489.Even though not true in any substantive scientific sense, since the beginning of our discipline, metaphorical brain talk has, as part of a professional â€œconspiracy to believe,â€ helped our own self-image as a â€œpoor relationâ€ in medicine and given us plausible metaphors to communicate about the disease we treat with ourselves and with our patients. This conspiracy, I suggest, was further reinforced through the large advertising budgets of modern drug companies.Another take on metaphorical brain talkHaving read and pondered many of the 19th century examples from asylum psychiatrists of metaphorical brain talk, and the more elaborate high speculative system of Meynert, I realized that there is a more positive view one might take of these efforts. It complements the more critical perspective outline above. These examples of brain talk also speak of the deep desire of psychiatrists to truly understand the brain basis of mental illness. For many of them, their constant pattern of framing mental descriptions of psychiatric illness in brain metaphors likely bespeaks their wish this could one day be done in earnest. Their use of brain talk was indirectly an expression of a promissory note for the future, a long-term aspiration to one day be able to both be a clinician, familiar with the description and treatment of the deeply disturbing mental symptoms of their patients, and, like their colleagues in other branches of medicine, have the ability to identify, by clinical signs, laboratory or radiological tests, or even biopsy, a definitive underlying pathophysiology in the brain.I present only one example of this phenomenon from the influential psychiatrist Kurt Schneider (1887â€“1967). In the fifth edition of his short textbook Clinical Psychopathology [56], in his section on classification, he describes two groups of disorders â€“ those termed â€œabnormal variations of psychic lifeâ€ and the other â€œeffects of Illness.â€ All the latter have clear somatic etiologies (e.g., general paresis of the insane, cerebral damage), except schizophrenia and what Schneider called â€œcyclothymia,â€ equivalent to Kraepelinâ€™s manic-depressive illness. Why were they classified differently from the other psychiatric syndromes? He explains:There is no question at this point whether some morbid condition does in fact underlie these psychopathological forms â€¦ Here we are firmly postulating that cyclothymia and schizophrenia are psychopathologic symptoms of some unknown illness. If â€œpostulateâ€ seems too strong, we can say â€œworking hypothesisâ€ instead [56] p. 5.In the absence of clear evidence, Schneider was willing to take a â€œpromissory noteâ€ on the hypothesis that Kraepelinâ€™s two major disorders of schizophrenia and manic-depressive illness were indeed the result of some as yet â€œunknown illness.â€ Perhaps Schneider was expressing overtly what many of the earlier brain talkers were feeling covertly â€“ â€œthere are real diseases down there â€“ give us time, and we will find them.â€Brain talk and reductionismMetaphorical brain talk is the soft underbelly of the reductionist agenda of modern science, which, since the dawn of the Enlightenment, has sought to explain the complex phenomena we confront in our biological and physical environment on the basis of simpler physical constituents [57]. This approach has been spectacularly successful in physics, chemistry, and molecular biology, as driven by highly quantitative, replicable science. Its application to the world of the mental has been less successful and more problematic. Metaphorical psychiatric brain talk adopted the language of reductionism without its empirical scientific underpinning.To be clear, my criticism of metaphorical psychiatric brain talk is not a critique of the rigorous scientific reductive agenda in modern psychiatric research that seeks to understand the etiology of psychiatric illness. Indeed, I have spent most of my career in the study of psychiatric genetics, clearly, at least in part, a reductive research project. My concern is rather the degree to which the profession of psychiatry, over its long history, has impoverished its conceptual foundations by a strong brain-focused bias in how we talk, and, more importantly, think about mental disorders. We are at risk of underappreciating efforts to understand the first-person experiences of our patients. If our reductionist neuroscience efforts are to succeed, we will have to be able to link our scientific explanations with incisive understanding of the states of mental illness we treat. We could then provide our patients not with empty, metaphorical brain-talk, but a real explanation of the ways in which brain dysfunctions produce their experiences. This will, in turn, hopefully expand our ability to emphasize with their symptoms in the process of â€œexplanation-aided understandingâ€ [58].So what?We have a very imperfect sense of the brain-based disturbances that predispose our disorders. We should not be ashamed of this. We have been working hard for a very long time on a set of problems of extraordinary complexity. Talking about our profession and our disorders to ourselves, our colleagues and patients using metaphorical brain talk is scientifically immature and ultimately disrespectful to our patients. Unless we know otherwise, we should assume that our patients want us to â€œTell it like it is, Doc,â€ even when that means we cannot tell pretty stories about serotonin imbalances. When we describe the suffering of our patients, we donâ€™t have to â€œdress upâ€ the descriptions of their mental anguish with problematic brain metaphors. We should tell them what we know, with all the uncertainty. We should take pride in being the only specialists in medicine that have chosen to treat the disorders of the mind, conditions that account for a large proportion of aggregate human suffering [59]. We need not try to hide the large amount we still do not know about the causes of mental illness behind metaphorical brain talk.
                    
                ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rust tool for generating random fractals]]></title>
            <link>https://github.com/benjaminrall/chaos-game</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45153237</guid>
            <description><![CDATA[A Rust implementation of the 'Chaos Game' algorithm for generating fractal images from randomness. - benjaminrall/chaos-game]]></description>
            <content:encoded><![CDATA[Chaos Game Fractal Generator

A simple command-line application written in Rust for generating fractals using the 'Chaos Game' algorithm.
Table of Contents

Algorithm Description
Gallery
Installation and Usage

Basic Example
Command-Line Arguments


Development Setup

Adding Custom Rules


License

Algorithm Description
This application generates fractals using the following simple, iterative algorithm:

Define the $n$ vertices of a regular polygon
Choose a random initial point within the polygon
Select one of the polygon's vertices at random
Move the current point a specific ratio, $r$, of the distance towards the chosen vertex
Repeat steps 3 and 4 for some large number of iterations, plotting each new point

By adjusting the number of vertices, the distance ratio, and optionally adding additional
restrictions to the choice of vertices, a huge variety of intricate fractal patterns can be generated. More details on the algorithm can be found here.
Gallery
Below are some example fractals generated with this application.



Fractal
Parameters
Image




SierpiÅ„ski Triangle

$n=3$, $r=0.5$




Rainbow Hex Fractal

$n=6$, $r=0.5$




Spirals Fractal

$n=5$, $r=0.5$, Rule: Cannot pick the same vertex twice in a row.



Star Fractal

$n=5$, $r=0.5$, Rule: If a vertex is picked twice in a row, the next pick cannot be a direct neighbour of it.




Installation and Usage
To use the application, you can simply install it using Cargo:
cargo install chaos-game
You can then run it from the command line, providing arguments to customise the generated fractal.
Basic Example
This basic example generates the SierpiÅ„ski triangle, one of the most well-known fractals
produced by the Chaos Game.
chaos-game -n 3 -r 0.5 -o sierpinski.png
Command-Line Arguments
You can see all available options by running the application with the --help flag.



Option
Short Flag
Description
Default Value




--sides
-n
The number of sides of the fractal polygon.
3


--ratio
-r
The distance ratio for point interpolation (0.0 to 1.0).
0.5


--iterations
-i
The total number of iterations to run the algorithm for.
100,000,000


--output
-o
The output filename for the final PNG image.
output.png


--coloured
-c
A flag to generate coloured fractals based on vertex angle.
false


--colour-scale

An aesthetic parameter to control image brightness.
4.0


--image-size

The width and height of the square image in pixels.
1000


--rotation-offset

A rotation offset for the polygon in degrees.
0.0


--rule

The name of the rule to use for selecting vertices.
"default"



Development Setup
To set up the project for development and add your own custom rules, follow these steps:


Clone the repository
git clone https://github.com/benjaminrall/chaos-game.git
cd chaos-game


Run the application:
You can build and run the project directly with Cargo. Using the --release flag is recommended for
performance.
cargo run --release -- -n 6 -c -o example.png


Adding Custom Rules
The application is designed to be easily extensible with custom rules for generating more complex fractals.
Rules consist of a function acting on some history of previous points and a proposed new point, and must
return a boolean indicating whether the new point is valid.
To create your own rule, follow these steps:


Create a file for your new rule:
Create a new file in the chaos-game/src/rules/ directory (e.g., my_rule.rs)


Write your rule function:
Inside the new file, write a function that takes the history of previous points and a proposed new point, and
returns whether the point is valid. Decorate it with the #[rule] attribute, giving it a unique name and
specifying how much history it needs.
// chaos-game/src/rules/my_rule.rs
use std::collections::VecDeque;
use chaos_game_macros::rule;
use crate::types::Vertex;

// An example rule that doesn't allow the same vertex to be selected twice in a row
#[rule("my-rule", history = 1)]
fn no_repeats(previous_points: &VecDeque<&Vertex>, new_point: &Vertex) -> bool {
  if previous_points.len() == 0 {
    return true;
  }
  previous_points[0].index != new_point.index
}


Add your rule to the rules module:
Inside chaos-game/src/rules/mod.rs, define your rule as a module by its filename (e.g. mod my_rule;)


Use your new rule:
After completing this setup, the rule will be automatically registered
and available to use from the command line:
cargo run --release -- -n 5 -c --rule my-rule


License
This project is licensed under the MIT License. See the LICENSE file for details.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Show HN: I'm making an open-source platform for learning Japanese]]></title>
            <link>https://kanadojo.com</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152940</guid>
        </item>
        <item>
            <title><![CDATA[A Navajo weaving of an integrated circuit: the 555 timer]]></title>
            <link>https://www.righto.com/2025/09/marilou-schultz-navajo-555-weaving.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152779</guid>
            <description><![CDATA[The noted DinÃ© (Navajo) weaver Marilou Schultz recently completed an intricate weaving composed of thick white lines on a black background, ...]]></description>
            <content:encoded><![CDATA[
The noted DinÃ© (Navajo) weaver Marilou Schultz recently completed an intricate weaving
composed of
thick white lines on a black background, punctuated with reddish-orange diamonds.
Although this striking rug may appear abstract, it shows the internal circuitry of a tiny silicon chip known
as the 555 timer.
This chip has hundreds of applications in
everything from a sound generator to a windshield wiper controller.
At one point, the 555 was the world's best-selling integrated circuit with billions sold.
But how did the chip get turned into a rug?

The 555 chip is constructed from a tiny flake of silicon with a layer of metallic wiring on top.
In the rug, this wiring is visible as the thick white lines, while the silicon forms the black background.
One conspicuous feature of the rug is the reddish-orange diamonds around the perimeter.
These correspond to the connections between the silicon chip and its eight pins. Tiny golden bond wiresâ€”thinner than a human hairâ€”are attached to the square bond pads to provide these connections.
The circuitry of the 555 chip contains 25 transistors, silicon devices that can switch
on and off.
The rug is dominated by three large transistors, the filled squares with a çŽ‹ pattern inside, while the remaining transistors are represented by small dots.
The weaving was inspired by a photo of the 555 timer die taken by
Antoine Bercovici
(Siliconinsider); I suggested this photo to Schultz as a possible subject
for a rug.  The diagram below compares the 
weaving (left) with the die photo (right).
As you can see, the weaving closely follows the actual chip, but there are a few artistic differences.
For instance, two of the bond pads have been removed, the circuitry at the top has been simplified,
and the part number at the bottom has been removed.
A comparison of the rug (left) and the original photograph (right).
Dark-field image of the 555 timer is courtesy of Antoine Bercovici.
Antoine took the die photo with a dark field microscope, a special type of microscope that
produces an image on a black background.
This image emphasizes the metal layer on the top of the die.
In comparison, a standard bright-field microscope produced the image below.
When a chip is manufactured, regions of silicon are "doped" with impurities to create transistors
and resistors.
These regions are visible in the image below as subtle changes in the color of the silicon.

In the weaving, the chip's design appears almost monumental, making it easy to forget that the
actual chip is microscopic.
For the photo below,
I obtained a version of the chip packaged in a metal can, rather than the typical rectangle of
black plastic.
Cutting the top off the metal can reveals the tiny chip inside, with eight gold bond wires connecting the
die to the pins of the package.
If you zoom in on the photo, you may recognize the three large transistors that dominate the rug.
The 555 timer die inside a metal-can package, with a penny for comparison. Click this image (or any other) for a larger version.
The artist, Marilou Schultz, has been creating chip rugs since 1994, when Intel commissioned a
rug based on the Pentium as a gift to AISES (American Indian Science & Engineering Society).
Although Schultz learned weaving as a child, the Pentium rug was a challenge due to its complex pattern
and lack of symmetry; a day's work might add just an inch to the rug.
This dramatic weaving was created with wool from the long-horned Navajo-Churro sheep, colored with
traditional plant dyes.
"Replica of a Chip", created by Marilou Schultz, 1994. Wool. Photo taken at the National Gallery of Art, 2024.
For the 555 timer weaving, Schultz experimented with different materials. Silver and gold metallic threads
represent the aluminum and copper in the chip.
The artist explains that "it took a lot more time to incorporate the metallic threads," but it was
worth the effort because "it is spectacular to see the rug with the metallics in the dark with a little light hitting it."
Aniline dyes provided the black and lavender colors.
Although natural logwood dye
produces a beautiful purple, it fades over time, so Schultz used an aniline dye instead.
The lavender colors are dedicated to the weaver's mother, who passed away in February;
purple was her favorite color.
Inside the chip
How does the 555 chip produce a particular time delay?
You add external componentsâ€”resistors and a capacitorâ€”to select the time.
The capacitor is filled (charged) at a speed controlled by the resistor. When the capacitor get "full",
the 555 chip switches operation and starts emptying (discharging) the capacitor.
It's like filling a sink: if you have a large sink (capacitor) and a trickle of water (large resistor),
the sink fills slowly. But if you have a smal sink (capacitor) and a lot of water (small resistor),
the sink fills quickly.
By using different resistors and capacitors, the 555 timer can provide time intervals from microseconds
to hours.
I've constructed an interactive chip browser that shows how the regions of the rug correspond to specific
electronic components in the physical chip. Click on any part of the rug to learn the function of
the corresponding component in the chip.


Click the die or schematic for details...











For instance, two of the large square transistors turn the chip's output on or off, while the third
large transistor discharges the capacitor when it is full. (To be precise, the capacitor goes between 1/3 full
and 2/3 full to avoid issues near "empty" and "full".)
The chip has circuits called comparators that detect when the capacitor's voltage reaches 1/3 or 2/3,
switching between emptying and filling at those points.
If you want more technical details about the 555 chip, see my previous articles:
an early 555 chip,
a 555 timer similar to the rug,
and a more modern CMOS version of the 555.
Conclusions
The similarities between Navajo weavings and the patterns in integrated circuits have long been recognized.
Marilou Schultz's weavings of integrated circuits make these visual metaphors into concrete works of art.
This connection is not just metaphorical, however; in the 1960s, the semiconductor company Fairchild employed numerous Navajo workers to assemble chips in Shiprock, New Mexico.
I wrote about this complicated history in The Pentium as a Navajo Weaving.


This work is being shown at SITE Santa Fe's Once Within a Time exhibition (running until January 2026).
I haven't seen the exhibition in person, so let me know if you visit it.
For more about Marilou Schultz's art, see The DinÃ© Weaver Who Turns Microchips Into Art, or
A Conversation with Marilou Schultz on YouTube.
Many thanks to Marilou Schultz for discussing her art with me.
Thanks to First American Art Magazine for providing the photo of her 555 rug.
Follow me on Mastodon (@[emailÂ protected]),
Bluesky (@righto.com),
or RSS for updates.

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Shipping textures as PNGs is suboptimal]]></title>
            <link>https://gamesbymason.com/blog/2025/stop-shipping-pngs/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152648</guid>
            <description><![CDATA[Are you shipping textures to players as PNGs? The goal of this post is to convince you that this is suboptimal, and walk you through a better approach.]]></description>
            <content:encoded><![CDATA[
    

    
      
        Are you shipping textures to players as PNGs? The goal of this post is to convince you that this is suboptimal, and walk you through a better approach.Iâ€™ll also share my implementation of the suggested approach, but if youâ€™d rather do it yourself Iâ€™ll also provide you with the information you need to get started.If youâ€™re using a game engine, it is almost certainly doing what this post suggests automatically, but it doesnâ€™t hurt to double check!
      
      
    
      
      
        
          Table of Contents
          
Table of ContentsWhatâ€™s wrong with PNGs?What texture formats are out there?Exporting to KTX2Extras
Texture ViewersPreserving Alpha CoverageAutomation
        
      
    
      
        Whatâ€™s wrong with PNGs? sourcePNGs are great for interchange. Theyâ€™re lossless, they compresses well, and support is ubiquitous. PNG is my image interchange format of choice.This post isnâ€™t a criticism of PNGsâ€“itâ€™s just that the PNG format is designed for image data, not texture data.Here are some examples of features you would expect out of a texture format that youâ€™re not going to find in an image format:Pregenerated mipmapsCubemapsPremultiplied alphaTechnically PNGs can be premultplied, but yours probably arenâ€™t.Can you work around all these issues? Sure.You can premultiply and generate your mipmaps at load time. You can ship separate images for each cuebmap face. But now youâ€™re resigned to cheap mipmap generation, and cubemaps that are difficult to downsample correctly.You can certainly make it work, but youâ€™re making things unnecessarily difficult for yourself by using the wrong tool for the job.Furthermore, texture formats have a killer feature not mentioned aboveâ€“support for GPU compatible texture compression like BCn.An in-depth explanation of GPU compression formats it out of scope for this post, but at a high level, these formats store each block of pixels as a couple of endpoints and a method for interpolating between those endpoints.This trades mild degradation of image quality for improvements in storage, VRAM usage, and sampling performance. Itâ€™s so good it feels like youâ€™re cheating thermodynamics.GPUs canâ€™t decompress PNGs on the fly, so as a result, if you ship PNGs you either canâ€™t take advantage of this compression, or you have to first decompress the PNGs and then do an extremely expensive compression step to convert to the desired block based format every time a player loads the game.Thatâ€™s a little goofy, right?(EDIT: Well, itâ€™s goofy when done naivelyâ€“see discussion w/ Ignacio CastaÃ±o here, something along these lines can become viable if you can transcode quickly.)
      
      
    
      
        What texture formats are out there?Texture formats like Khronosâ€™ KTX2 and Microsoftâ€™s DDS are designed for exactly our use case. Theyâ€™re just headers followed by some image data that you can upload directly to the GPU without any additional processing.Well, unless you use supercompression. GPU compression formats donâ€™t provide great compression ratios, so itâ€™s typical to apply lossless compression as well (think zlib or lz4.) In that case youâ€™ll decompress, and then upload.The meta here is to design your lossy compressor to be aware that its output is going to be losslessly compressed afterwards. This lets it make decisions that reduce entropy, improving the effectiveness of the lossless step.I used DXT5 + lz4 compressed DDS files for Way of Rhea, Iâ€™m switching to BC7 + zlib compressed KTX2 files for my next game. Both approaches are reasonable.Note: I primarily develop games for desktop platforms. IIUC, on mobile, hardware support for various types of GPU compression varies but the formats are similar-ish, so the meta is to use something like Basis Universal to quickly transcode to the correct format on load.
      
      
    
      
        Exporting to KTX2At this point, youâ€™re likely looking through the export menu of your image editor of choice for KTX2 and DDS, and not seeing any results.Unfortunately, AFAICT most people end up rolling their own exporters. People used to use Nvidia Texture Tools, but itâ€™s archived as there wasnâ€™t funding to maintain it. Itâ€™s still a great reference. Nvidia has a closed source fork, but I donâ€™t love having a closed source dependency for such an integral part of my engine.Iâ€™ve implemented an open source texture tool that youâ€™re welcome to use directly or as a reference for your own implementation: Zex.It can be used as a command line tool, or as a Zig library. It reads PNGs using stb_image, and converts them to KTX2, with support BC7 compression + rate distortion optimization from bc7enc_rdo, and supercompression via zlib.It supports most standard features, such as mipmap generation with configurable filters and address modes.I havenâ€™t implemented cubemap exports yet as my current game isnâ€™t using them. If you need support before I get around to it, PRs are welcomeâ€“it should be a pretty straightforward addition.If you want to implement your own exporter, here are some useful references. Keep in mind that you donâ€™t need to support all possible features, just the ones your engine uses:KTX2 (Format Specification / DFD Specification)DDS
      
      
    
      
        
      
      
    
      
        Texture Viewers sourceMost image viewers wonâ€™t be able to open texture formats like DDS/KTX2. This sorta makes senseâ€“image viewers are typically designed to show a single image, whereas a texture may be comprised of multiple mipmaps and cubemap faces and such, and may be HDR. This requires a fancier UI.Iâ€™m personally a fan of Tacentview for this use case. Itâ€™s open source, cross platform, and supports a large number of formats.
      
      
    
      
        Preserving Alpha Coverage source: firewatch inspired me so I made a tree and then never used it for anythingPregenerating your mipmaps gives you a chance to be a little more â€œcorrectâ€ about them.For example, if youâ€™ve ever tried to render a tree or a chain link fence in-game as a cutout (or with alpha to coverage) but found that it vanishes when you get far away, your mipmap filtering likely isnâ€™t taking into account the alpha test.You can see Zexâ€™s alpha test aware resize here. This isnâ€™t battle tested yet, compare results visually in-engine to see if it provides a benefit for your artwork.
      
      
    
      
        Automation sourceYou probably donâ€™t want to convert all your images by hand. I did this for Way of Rhea for a while, but eventually realized that it was a waste of time. Every time a texture changes you have to go back and figure out what settings you used last time. Just automate it.Iâ€™ll probably write a follow up post describing my strategy for automating this at some point in the future, but if you want a sneak peak, check out Oven. Itâ€™s not exactly general purpose right now, but might be an interesting reference.
      
      
    
    

    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Being good isn't enough]]></title>
            <link>https://joshs.bearblog.dev/being-good-isnt-enough/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152402</guid>
            <description><![CDATA[Giving good career advice is hard. Maybe itâ€™s because careers can look more alike than they really are. Two people can have the same title but what helps one...]]></description>
            <content:encoded><![CDATA[
    

    
        
    

    
        

        
            
                
                    06 Sep, 2025
                
            
        
    

    Giving good career advice is hard. Maybe itâ€™s because careers can look more alike than they really are. Two people can have the same title but what helps one could be rubbish for another.
Or maybe itâ€™s that â€œgood adviceâ€ itself is fuzzy. It depends entirely on the person receiving it. For some people it means finding work they love. For others itâ€™s about meaning. For many itâ€™s just getting promoted. Still, hereâ€™s what I usually say.
You have to be good at the technical work first, whatever that means for you. Itâ€™s the thing you were hired for and this has to be your first priority. And the better you are, the further this can take you. You write better code, or better reports, or better designs, and people notice. Thatâ€™s enough for a while.
But eventually itâ€™s not. Everyone around you is technically strong too. So for most of us, you wonâ€™t stand out anymore. You need to increase your impact in other ways.
The biggest gains come from combining disciplines. There are four that show up everywhere: technical skill, product thinking, project execution, and people skills. And the more senior you get, the more youâ€™re expected to contribute to each.
Technical skill is your chosen craft. Product thinking is knowing whatâ€™s worth doing. Project execution is making sure it happens. People skills are how you work with and influence others.
Every successful effort needs all four. Try to imagine an endeavor that wouldnâ€™t benefit from improving in these areas. I canâ€™t think of one. If you squint, together they mean one thing: making stuff that matters actually happen. Thatâ€™s how you increase your impact.
Youâ€™ll naturally pick them up over time, but slowly. You can go faster if you push yourself.
This is harder than it sounds because the less competent we are at something, the more likely we are to overestimate ourselves. Itâ€™s easy to think youâ€™re working on what matters, or that youâ€™re doing great technical work, but that might not be true. So how do you find your weaknesses?
Iâ€™m pretty confident you only need two things. Feedback and humility, and they work best together. Feedback shows you what to work on, and humility lets you actually hear it.
So find your weakest discipline and work on that. The fastest way is to get feedback from someone you admire and then act on it. Donâ€™t wait for the perfect plan, doing something is almost always better than doing nothing.
Find a mentor, be a mentor. Lead a project, propose one. Do the work, present it. Create spaces for others to do the same. Do whatever it takes to get better.
And do it in the open. A common mistake is assuming work speaks for itself. It rarely does.
But all of this requires maybe the most important thing of all: agency. Itâ€™s more powerful than smarts or credentials or luck. And the best part is you can literally just choose to be high-agency. High-agency people make things happen. Low-agency people wait. And if you want to progress, you canâ€™t wait.
This advice is like any other, fuzzy. But it does go further than simply â€œwork harderâ€. It will take work and it will be hard, but it might be the difference between effort and progress.
And in the long run, the best way to get what you want is to deserve it.
The Staff Engineers Path by Tanya Reilly heavily shaped my views. I read it every year.
Thanks for the Feedback by Douglas Stone and Sheila Heen


    

    
        
            
                
                    #career
                
            
        

        
            


        
    


  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Over 80% of sunscreen performed below their labelled efficacy (2020)]]></title>
            <link>https://www.consumer.org.hk/en/press-release/528-sunscreen-test</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152374</guid>
            <description><![CDATA[The Consumer Council keens to be the trusted voice of consumers in striving to build in the market an environment of safe, fair and sustainable consumption.]]></description>
            <content:encoded><![CDATA[The use of effective sunscreen can reduce the harm caused to the skin by ultraviolet rays (UV) and slow down skin aging. The Consumer Council tested 30 models of sunscreen for daily use and over 80% of them were found to perform below their respective labelled efficacy. The measured sunscreen efficacy of 4 models were below SPF15, of which 2 were sunscreen products with very high protection i.e. labelled with SPF50+. Among the 23 models using the â€œPA Systemâ€ which is commonly adopted by Asian countries to denote the UVA protection efficacy, only 7 were measured with an UVA Protection Factor (UVAPF) value met with their labelled PA levels. In addition, only 19 models stated the major ingredients on their packaging and consumers may not be able to identify possible allergens as a result. The Council urges manufacturers to critically review their production technology and processes, and to accurately label its product efficacy as well as to provide clear product information and usage guidelines. If consumers engage in outdoor activities for a prolonged period and use sunscreens with insufficient protection will possibly increase their risks of skin darkening or sunburn, and even skin cancer.Â UVA emits from the sun may lead to skin aging, create wrinkles, darken skin colour, and may even induce skin cancer. However, internationally there is no unified system for product labelling of UVA protection, yet â€œPA Systemâ€ is commonly adopted by Asian countries. UVB as ultraviolet rays with a higher energy level, can destroy DNA on skin surface, causing sunburn and is one of the main reasons of skin cancer. Currently, the Sun Protection Factor (SPF) index is an internationally recognised system to indicate the level of UVB protection in sunscreen products, the higher the value, the longer the protection offered against UVB.Among the 30 daily-use sunscreen models tested, their price ranged from $80 to $550, i.e. $0.7 to $16.1 per g/ml, marking a difference of 23 times. 14 of them belonged to high protection and were labelled from SPF30 to SPF50 while the remaining 16 models belonged to very high protection and were labelled as SPF50+. 23 models showed their UVA protection ratings by â€œPA Systemâ€. The test result revealed the second cheapest model ($85) scored the highest 5 points in overall performance but the most expensive model ($550) only rated 3.5 points, indicating once again that there is no correlation between the price and product quality.Currently, Hong Kong has no legislation or standard in regulating both SPF and UVA efficacy in products. Taking reference to the Cosmetics Regulation in the European Union (EU), this test covered SPF test and UVA protection test, as well as reviewing the labelling of each model.According to the product labelling requirements of the EU Cosmetics Regulation, SPF labelling on sunscreens must meet 3 criteria, including passing the in vivo test of the related SPF; the measured UVAPF value reaching one-third or above of SPF; and the measured critical wavelength should be 370nm or above. The in vivo SPF test applied a fixed amount of the models on the skin of the back of 10 trial users before they were exposed to UV light. The SPF value of each model was calculated based on the erythema reactions measured on skin surface within 24 hours. Sunscreen labelled with SPF50+ should reach a measured SPF value of 60 or above, whereas products of SPF30 should reach a measured SPF value between 30 to 49.9. For the UVA blocking protection test, the UVA efficacy and the critical wavelength were calculated by detecting the penetration rate of UV light source through the special plastic film simulating human skin after applying the sunscreen models.SPF test results revealed only 4 sunscreen models labelled with high protection (SPF30 to SPF50) fully complied with the efficacy labelling requirement under the EU Cosmetic Regulation. In the 14 models, 8 were measured with SPF value below their claims in the in vivo test. 1 model labelled as SPF30 had the largest discrepancy with its measured SPF value of only 9.8. Although the SPF values measured in the other 6 models were higher than or equal to their claims, the UVAPF value in 2 of them were only 8.0 and 4.0 respectively, failing to meet the requirement that UVAPF value need to be one-third of its SPF, and were therefore not in compliance with the labelling requirements of the EU.In the 16 models labelled with very high protection (SPF50+), only 1 fully complied with the EU requirement. The measured SPF value in 14 of them were below SPF60, of which the lowest performing 2 were recorded with a measured value of just 11.7 and 14.3 respectively. The 2 models with the highest SPF values reached 87.2 and 61.7 respectively, but the UVAPF and critical wavelength of 1 of them could not meet the relevant criteria.Unlike UVB, there is no unified international system for labelling UVA protection efficacy in products. The Council thus rated such efficacy of all models by converting the UVAPF values measured into the â€œPA systemâ€ which is commonly adopted by Asian countries. All 30 tested models were detected with different degrees of UVA protection with the measured UVAPF values ranging between 3.3 to 67.3, whereas UVAPF values of 9 of them were above 16, which were roughly the highest level in the PA system (i.e. PA++++) while another 10 models were rated at PA+++.As for product labelling, 6 models listed their ingredients in Japanese only and general consumers may not be able to identify possible allergens or apply the products correctly. Suggested usage quantity cannot be found in 21 models. If consumers apply insufficient amount of sunscreens, they may incur the risk of inadequate protection. Moreover, 3 models were not marked with any expiry date. The Council urges manufacturers to improve product labelling. On the other hand, the Council reminds that some sunscreen products may have high water content level, once the product has reached the expiry date, preservative may lose its effectiveness, and this could accelerate bacterial and microbial growth. These products should be used up well before the expiry date after opening.Consumers should try to avoid exposing their skin under direct sunlight to minimise the harm to the skin caused by UV radiation. When purchasing and using sunscreens, consumers need to be aware of the following:In selecting sunscreen products, read the labels carefully to check the presence of allergens. Consumers with skin allergies or eczema should consider sunscreens with physical filters to reduce the risk of allergy;Sunscreens of physical filters are relatively mild and less likely to cause allergy but are relatively whiter in colour and more viscous in texture, thus it is harder to be applied evenly. While those with chemical filters are thinner and give a lighter feeling after application, they may pose a greater risk in skin and eyes irritation, thus resulting in allergy more easily;Sunscreens with SPF50 are basically adequate in providing 98% protection to the skin while those with a larger SPF value may instead clog up pores or cause skin allergy. Thus, for normal use, it is not necessary to look for sunscreen products with a very high SPF value;Make reference to the UV index announced by the Hong Kong Observatory before going out for outdoor activities, and choose appropriate sunscreens according to the UV index, type and duration of their activities;Apply sunscreen according to the product label, normally it is around 1 teaspoon for face and should be re-applied every 2 to 3 hours to ensure sufficient protection to the skin;Sunscreen should be cleansed by make-up remover or facial cleanser according to the packaging instruction to prevent any residue from affecting skinâ€™s health;Pay attention to the product expiry date, disposal is necessary if the product is expired to avoid the risks of microbial growth upon contact with air or skin.The Consumer Council reserves all its right (including copyright) in respect of CHOICE magazine and Online CHOICE]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How the â€œKimâ€ dump exposed North Korea's credential theft playbook]]></title>
            <link>https://dti.domaintools.com/inside-the-kimsuky-leak-how-the-kim-dump-exposed-north-koreas-credential-theft-playbook/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45152066</guid>
            <description><![CDATA[A rare and revealing breach attributed to a North Korean-affiliated actor, known only as â€œKimâ€ as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actor's operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The â€œKimâ€ dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.]]></description>
            <content:encoded><![CDATA[
                                
Contents:Part I: Technical AnalysisPart II: Goals AnalysisPart III: Threat Intelligence Report



Executive Summary



A rare and revealing breach attributed to a North Korean-affiliated actor, known only as â€œKimâ€ as named by the hackers who dumped the data, has delivered a new insight into Kimsuky (APT43) tactics, techniques, and infrastructure. This actorâ€™s operational profile showcases credential-focused intrusions targeting South Korean and Taiwanese networks, with a blending of Chinese-language tooling, infrastructure, and possible logistical support. The â€œKimâ€ dump, which includes bash histories, phishing domains, OCR workflows, compiled stagers, and rootkit evidence, reflects a hybrid operation situated between DPRK attribution and Chinese resource utilization.



Screen shot of the adversaryâ€™s desktop VM



This report is broken down into three parts:Â 




Technical Analysis of the dump materials



Motivation and Goals of the APT actor (group)



A CTI report compartment for analysts




While this leak only gives a partial idea of what the Kimusky/PRC activities have been, the material provides insight into the expansion of activities, nature of the actor(s), and goals they have in their penetration of the South Korean governmental systems that would benefit not only DPRK, but also PRC.



Phrack article



Without a doubt, there will be more coming out from this dump in the future, particularly if the burned assets have not been taken offline and access is still available, or if others have cloned those assets for further analysis. We may revisit this in the future if additional novel information comes to light.



Part I: Technical Analysis



The Leak at a Glance



The leaked dataset attributed to the â€œKimâ€ operator offers a uniquely operational perspective into North Korean-aligned cyber operations. Among the contents were terminal history files revealing active malware development efforts using NASM (Netwide Assembler), a choice consistent with low-level shellcode engineering typically reserved for custom loaders and injection tools. These logs were not static forensic artifacts but active command-line histories showing iterative compilation and cleanup processes, suggesting a hands-on attacker directly involved in tool assembly.



File list of dump



In parallel, the operator ran OCR (Optical Character Recognition) commands against sensitive Korean PDF documents related to public key infrastructure (PKI) standards and VPN deployments. These actions likely aimed to extract structured language or configurations for use in spoofing, credential forgery, or internal tool emulation.



Privileged Access Management (PAM) logs also surfaced in the dump, detailing a timeline of password changes and administrative account use. Many were tagged with the Korean string ë³€ê²½ì™„ë£Œ (â€œchange completeâ€), and the logs included repeated references to elevated accounts such as oracle, svradmin, and app_adm01, indicating sustained access to critical systems.



The phishing infrastructure was extensive. Domain telemetry pointed to a network of malicious sites designed to mimic legitimate Korean government portals. Sites like nid-security[.]com were crafted to fool users into handing over credentials via advanced AiTM (Adversary-in-the-Middle) techniques.



nid-security[.]com phishing domain (anon reg 2024)



Finally, network artifacts within the dump showed targeted reconnaissance of Taiwanese government and academic institutions. Specific IP addresses and .tw domain access, along with attempts to crawl .git repositories, reveal a deliberate focus on high-value administrative and developer targets.



Perhaps most concerning was the inclusion of a Linux rootkit using syscall hooking (khook) and stealth persistence via directories like /usr/lib64/tracker-fs. This highlights a capability for deep system compromise and covert command-and-control operations, far beyond phishing and data theft.



Artifacts recovered from the dump include:




Terminal history files demonstrating malware compilation using NASM



OCR commands parsing Korean PDF documents related to PKI and VPN infrastructure



PAM logs reflecting password changes and credential lifecycle events



Phishing infrastructure mimicking Korean government sites



IP addresses indicating reconnaissance of Taiwanese government and research institutions



Linux rootkit code using syscall hooking and covert channel deployment




Credential Theft Focus



The dump strongly emphasizes credential harvesting as a central operational goal. Key files such as 136ë°±ìš´ê·œ001_env.key (The presence of 136ë°±ìš´ê·œ001_env.key is a smoking gun indicator of stolen South Korean Government PKI material, as its structure (numeric ID + Korean name + .key) aligns uniquely with SK GPKI issuance practices and provides clear evidence of compromised, identity-tied state cryptographic keys.) This was discovered alongside plaintext passwords, that indicate clear evidence of active compromise of South Koreaâ€™s GPKI (Government Public Key Infrastructure). Possession of such certificates would allow for highly effective identity spoofing across government systems.











PAM logs further confirmed this focus, showing a pattern of administrative account rotation and password resets, all timestamped and labeled with success indicators (ë³€ê²½ì™„ë£Œ: Change Complete). The accounts affected were not low-privilege; instead, usernames like oracle, svradmin, and app_adm01, often used by IT staff and infrastructure services, suggested access to core backend environments.



These findings point to a strategy centered on capturing and maintaining access to privileged credentials and digital certificates, effectively allowing the attacker to act as an insider within trusted systems.




Leaked .key files (e.g., 136ë°±ìš´ê·œ001_env.key) with plaintext passwords confirm access to GPKI systems



PAM logs show administrative password rotations tagged with ë³€ê²½ì™„ë£Œ (change complete)



Admin-level accounts such as oracle, svradmin, and app_adm01 repeatedly appear in compromised logs




Phishing Infrastructure



The operatorâ€™s phishing infrastructure was both expansive and regionally tailored. Domains such as nid-security[.]com and webcloud-notice[.]com mimicked Korean identity and document delivery services, likely designed to intercept user logins or deploy malicious payloads. More sophisticated spoofing was seen in sites that emulated official government agencies like dcc.mil[.]kr, spo.go[.]kr, and mofa.go[.]kr.



Whoisof domains created by dysoni91@tutamail[.]com



Historical Whois of webcloud-notice[.]com



Burner email usage added another layer of operational tradecraft. The address jeder97271[@]wuzak[.]com is likely linked to phishing kits that operated through TLS proxies, capturing credentials in real time as victims interacted with spoofed login forms.



These tactics align with previously known Kimsuky behaviors but also demonstrate an evolution in technical implementation, particularly the use of AiTM interception rather than relying solely on credential-harvesting documents.



Domain connections map




Domains include: nid-security[.]com, html-load[.]com, webcloud-notice[.]com, koala-app[.]com, and wuzak[.]com



Mimicked portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Burner email evidence: jeder97271[@]wuzak[.]com



Phishing kits leveraged TLS proxies for AiTM credential capture




Malware Development Activity



Kimâ€™s malware development environment showcased a highly manual, tailored approach. Shellcode was compiled using NASM, specifically with flags like -f win32, revealing a focus on targeting Windows environments. Commands such as make and rm were used to automate and sanitize builds, while hashed API call resolution (VirtualAlloc, HttpSendRequestA, etc.) was implemented to evade antivirus heuristics.



The dump also revealed reliance on GitHub repositories known for offensive tooling. TitanLdr, minbeacon, Blacklotus, and CobaltStrike-Auto-Keystore were all cloned or referenced in command logs. This hybrid use of public frameworks for private malware assembly is consistent with modern APT workflows.



A notable technical indicator was the use of the proxyres library to extract Windows proxy settings, particularly via functions like proxy_config_win_get_auto_config_url. This suggests an interest in hijacking or bypassing network-level security controls within enterprise environments.




Manual shellcode compilation via nasm -f win32 source/asm/x86/start.asm



Use of make, rm, and hash obfuscation of Win32 API calls (e.g., VirtualAlloc, HttpSendRequestA)



GitHub tools in use: TitanLdr, minbeacon, Blacklotus, CobaltStrike-Auto-Keystore



Proxy configuration probing through proxyres library (proxy_config_win_get_auto_config_url)




Rootkit Toolkit and Implant Structure



The Kim dump offers deep insight into a stealthy and modular Linux rootkit attributed to the operatorâ€™s post-compromise persistence tactics. The core implant, identified as vmmisc.ko (alternatively VMmisc.ko in some shells), was designed for kernel-mode deployment across multiple x86_64 Linux distributions and utilizes classic syscall hooking and covert channeling to maintain long-term undetected access.







Google Translation of Koh doc: Rootkit Endpoint Reuse Authentication Tool



â€œThis tool uses kernel-level rootkit hiding technology, providing a high degree of stealth and penetration connection capability. It can hide while running on common Linux systems, and at the kernel layer supports connection forwarding, allowing reuse of external ports to connect to controlled hosts. Its communication behavior is hidden within normal traffic.



The tool uses binary merging technology: at compile time, the application layer program is encrypted and fused into a .ko driver file. When installed, only the .ko file exists. When the .ko driver starts, it will automatically decompress and release the hidden application-layer program.



Tools like chkrootkit, rkhunter, and management utilities (such as ps, netstat, etc.) are bypassed through technical evasion and hiding, making them unable to detect hidden networks, ports, processes, or file information.



To ensure software stability, all functions have also passed stress testing.



Supported systems: Linux Kernel 2.6.x / 3.x / 4.x, both x32 and x64 systemsâ€.



Implant Features and Behavior



This rootkit exhibits several advanced features:




Syscall Hooking: Hooks critical kernel functions (e.g., getdents, read, write) to hide files, directories, and processes by name or PID.



SOCKS5 Proxy: Integrated remote networking capability using dynamic port forwarding and chained routing.



PTY Backdoor Shell: Spawns pseudoterminals that operate as interactive reverse shells with password protection.



Encrypted Sessions: Session commands must match a pre-set passphrase (e.g., testtest) to activate rootkit control mode.




Once installed (typically using insmod vmmisc.ko), the rootkit listens silently and allows manipulation via an associated client binary found in the dump. The client supports an extensive set of interactive commands, including:



+pÂ  Â  Â  Â  Â  Â  Â  # list hidden processes



+fÂ  Â  Â  Â  Â  Â  Â  # list hidden files



callrkÂ  Â  Â  Â  Â  # load client â†” kernel handshake



exitrkÂ  Â  Â  Â  Â  # gracefully unload implant



shell Â  Â  Â  Â  Â  # spawn reverse shell



socks5Â  Â  Â  Â  Â  # initiate proxy channel



upload / download # file transfer interface



These capabilities align closely with known DPRK malware behaviors, particularly from the Kimsuky and Lazarus groups, who have historically leveraged rootkits for lateral movement, stealth, persistence, and exfiltration staging.



Observed Deployment



Terminal history (.bash_history) shows the implant was staged and tested from the following paths:



.cache/vmware/drag_and_drop/VMmisc.ko

/usr/lib64/tracker-fs/vmmisc.ko

Execution logs show the use of commands such as:

insmod /usr/lib64/tracker-fs/vmmisc.ko

./client 192.168.0[.]39 testtest



These paths were not randomâ€”they mimic legitimate system service locations to avoid detection by file integrity monitoring (FIM) tools.



Deployment map



This structure highlights the modular, command-activated nature of the implant and its ability to serve multiple post-exploitation roles while maintaining stealth through kernel-layer masking.



Strategic Implications



The presence of such an advanced toolkit in the â€œKimâ€ dump strongly suggests the actor had persistent access to Linux server environments, likely via credential compromise. The use of kernel-mode implants also indicates long-term intent and trust-based privilege escalation. The implantâ€™s pathing, language patterns, and tactics (e.g., use of /tracker-fs/, use of test passwords) match TTPs previously observed in operations attributed to Kimsuky, enhancing confidence in North Korean origin.



OCR-Based Recon



A defining component of Kimâ€™s tradecraft was the use of OCR to analyze Korean-language security documentation. The attacker issued commands such as ocrmypdf -l kor+eng â€œfile.pdfâ€ to parse documents like ë³„ì§€2)í–‰ì •ì „ìžì„œëª…_ê¸°ìˆ ìš”ê±´_141125.pdf (â€œAppendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdfâ€) and SecuwaySSL U_ì¹´ë‹¬ë¡œê·¸.pdf (â€œSecuwaySSL U_Catalog.pdfâ€). These files contain technical language around digital signatures, SSL implementations, and identity verification standards used in South Koreaâ€™s PKI infrastructure.



This OCR-based collection approach indicates more than passive intelligence gathering â€“ it reflects a deliberate effort to model and potentially clone government-grade authentication systems. The use of bilingual OCR (Korean + English) further confirms the operatorâ€™s intention to extract usable configuration data across documentation types.



OCR run on Korean PDFs




OCR commands used to extract Korean PKI policy language from PDFs such as (ë³„ì§€2)í–‰ì •ì „ìžì„œëª…_ê¸°ìˆ ìš”ê±´_141125.pdf and SecuwaySSL U_ì¹´ë‹¬ë¡œê·¸.pdf

ë³„ì§€2)í–‰ì •ì „ìžì„œëª…_ê¸°ìˆ ìš”ê±´_141125.pdf â†’ (Appendix 2: Administrative Electronic Signature_Technical Requirements_141125.pdf



SecuwaySSL U_ì¹´ë‹¬ë¡œê·¸.pdf â†’ SecuwaySSL U_Catalog.pdf





Command examples: ocrmypdf -l kor+eng â€œfile.pdfâ€




SSH and Log-Based Evidence



The forensic evidence contained within the logs, specifically SSH authentication records and PAM outputs, provides clear technical confirmation of the operatorâ€™s tactics and target focus.



Several IP addresses stood out as sources of brute-force login attempts. These include 23.95.213[.]210 (a known VPS provider used in past credential-stuffing campaigns), 218.92.0[.]210 (allocated to a Chinese ISP), and 122.114.233[.]77 (Henan Mobile, China). These IPs were recorded during multiple failed login events, strongly suggesting automated password attacks against exposed SSH services. Their geographic distribution and known history in malicious infrastructure usage point to an external staging environment, possibly used for pivoting into Korean and Taiwanese systems.



Beyond brute force, the logs also contain evidence of authentication infrastructure reconnaissance. Multiple PAM and OCSP (Online Certificate Status Protocol) errors referenced South Koreaâ€™s national PKI authority, including domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. These errors appear during scripted or automated access attempts, indicating a potential strategy of credential replay or certificate misuse against GPKI endpoints, an approach that aligns with Kimâ€™s broader PKI-targeting operations.



Perhaps the most revealing detail was the presence of successful superuser logins labeled with the Korean term ìµœê³  ê´€ë¦¬ìž (â€œSuper Administratorâ€). This suggests the actor was not just harvesting credentials but successfully leveraging them for privileged access, possibly through cracked accounts, reused credentials, or insider-sourced passwords. The presence of such accounts in conjunction with password rotation entries marked as ë³€ê²½ì™„ë£Œ (â€œchange completeâ€) further implies active control over PAM-protected systems during the operational window captured in the dump.



Together, these logs demonstrate a methodical campaign combining external brute-force access, PKI service probing, and administrative credential takeover, a sequence tailored for persistent infiltration and lateral movement within sensitive government and enterprise networks.



Brute force mapping




Brute-force IPs: 23.95.213[.]210, 218.92.0[.]210, 122.114.233[.]77




IP AddressOriginRole / Threat Context218.92.0[.]210China Telecom (Jiangsu)Part of Chinanet backbone, likely proxy or scanning node23.95.213[.]210Colocrossing (US)Frequently used in brute-force and anonymized hosting for malware ops122.114.233[.]77Presumed PRC local ISPPossibly mobile/ISP-based proxy used to obfuscate lateral movement




PAM/OCSP errors targeting gva.gpki.go[.]kr, ivs.gpki.go[.]kr



Superuser login events under ìµœê³  ê´€ë¦¬ìž (Super Administrator)




Part II: Goals Analysis



Targeting South Korea: Identity, Infrastructure, and Credential Theft



The â€œKimâ€ operatorâ€™s campaign against South Korea was deliberate and strategic, aiming to infiltrate the nationâ€™s digital trust infrastructure at multiple levels. A central focus was the Government Public Key Infrastructure (GPKI), where the attacker exfiltrated certificate files, including .key and .crt formats, some with plaintext passwords, and attempted repeated authentication against domains like gva.gpki.go[.]kr and ivs.gpki.go[.]kr. OCR tools were used to parse Korean technical documents detailing PKI and VPN architectures, demonstrating a sophisticated effort to understand and potentially subvert national identity frameworks. These efforts were not limited to reconnaissance; administrative password changes were logged, and phishing kits targeted military and diplomatic webmail, including clones of mofa.go[.]kr and credential harvesting through adversary-in-the-middle (AiTM) proxy setups.



Attempts at user account authentication



Servlet requests for KR domains



Beyond authentication systems, Kim targeted privileged accounts (oracle, unwadm, svradmin) and rotated credentials to maintain persistent administrative access, as evidenced by PAM and SSH logs showing elevated user activity under the title ìµœê³  ê´€ë¦¬ìž (â€œSuper Administratorâ€). The actor also showed interest in bypassing VPN controls, parsing SecuwaySSL configurations for exploitation potential, and deployed custom Linux rootkits using syscall hooking to establish covert persistence on compromised machines. Taken together, the dump reveals a threat actor deeply invested in credential dominance, policy reconnaissance, and system-level infiltration, placing South Koreaâ€™s public sector identity systems, administrative infrastructure, and secure communications at the core of its long-term espionage objectives.



Taiwan Reconnaissance



Among the most notable aspects of the â€œKimâ€ leak is the operatorâ€™s deliberate focus on Taiwanese infrastructure. The attacker accessed a number of domains with clear affiliations to the islandâ€™s public and private sectors, including tw.systexcloud[.]com (linked to enterprise cloud solutions), mlogin.mdfapps[.]com (a mobile authentication or enterprise login portal), and the .git/ directory of caa.org[.]tw, which belongs to the Chinese Institute of Aeronautics, a government-adjacent research entity.



This last domain is especially telling. Accessing .git/ paths directly implies an attempt to enumerate internal source code repositories, a tactic often used to discover hardcoded secrets, API keys, deployment scripts, or developer credentials inadvertently exposed via misconfigured web servers. This behavior points toÂ  more technical depth than simple phishing; it indicates supply chain reconnaissance and long-term infiltration planning.



Taiwanese target map



The associated IP addresses further reinforce this conclusion. All three, 163.29.3[.]119, 118.163.30[.]45, and 59.125.159[.]81, are registered to academic, government, or research backbone providers in Taiwan. These are not random scans; they reflect targeted probing of strategic digital assets.



Summary of Whois & Ownership Insights




118.163.30[.]45

Appears as part of the IP range used for the domain dtc-tpe.com[.]tw, linked to Taiwanâ€™s HINET provider (118.163.30[.]46 )Site Indices page of HINET provider.





163.29.3[.]119

Falls within the 163.29.3[.]0/24 subnet identified with Taiwanese government or institutional use, notably in Taipei. This corresponds to Bâ€‘class subnets assigned to public/government entities IPåœ°å€ (ç¹é«”ä¸­æ–‡).





59.125.159[.]81

Belongs to the broader 59.125.159[.]0â€“59.125.159[.]254 block, commonly used by Taiwanese ISP operators such as Chunghwa Telecom in Taipei






Taken together, this Taiwan-focused activity reveals an expanded operational mandate. Whether the attacker is purely DPRK-aligned or operating within a DPRKâ€“PRC fusion cell, the intent is clear: compromise administrative and developer infrastructure in Taiwan, likely in preparation for broader credential theft, espionage, or disruption campaigns.




Targeted domains: tw.systexcloud[.]com, caa.org[.]tw/.git/, mlogin.mdfapps[.]com



IPs linked to Taiwanese academic/government assets: 163.29.3[.]119, 118.163.30[.]45, 59.125.159[.]81



Git crawling suggests interest in developer secrets or exposed tokens




Hybrid Attribution Model



The â€œKimâ€ operator embodies the growing complexity of modern nation-state attribution, where cyber activities often blur traditional boundaries and merge capabilities across geopolitical spheres. This case reveals strong indicators of both North Korean origin and Chinese operational entanglement, presenting a textbook example of a hybrid APT model.







On one hand, the technical and linguistic evidence strongly supports a DPRK-native operator. Terminal environments, OCR parsing routines, and system artifacts consistently leverage Korean language and character sets. The operatorâ€™s activities reflect a deep understanding of Korean PKI systems, with targeted extraction of GPKI .key files and automation to parse sensitive Korean government PDF documentation. These are hallmarks of Kimsuky/APT43 operations, known for credential-focused espionage against South Korean institutions and diplomatic targets. The intent to infiltrate identity infrastructure is consistent with North Koreaâ€™s historical targeting priorities. Notably, the system time zone on Kimâ€™s host machine was set to UTC+9 (Pyongyang Standard Time), reinforcing the theory that the actor maintains direct ties to the DPRKâ€™s internal environment, even if operating remotely.



However, this actorâ€™s digital footprint extends well into Chinese infrastructure. Browser and download logs reveal frequent interaction with platforms like gitee[.]com, baidu[.]com, and zhihu[.]com, highly popular within the PRC but unusual for DPRK operators who typically minimize exposure to foreign services. Moreover, session logs include simplified Chinese content and PRC browsing behaviors, suggesting that the actor may be physically operating within China or through Chinese-language systems. This aligns with longstanding intelligence on North Korean cyber operators stationed in Chinese border cities such as Shenyang and Dandong, where DPRK nationals often conduct cyber operations with tacit approval or logistical consent from Chinese authorities. These locations provide higher-speed internet, relaxed oversight, and convenient geopolitical proximity.



Browser History viewing Taiwanese and Chinese sites



The targeting of Taiwanese infrastructure further complicates attribution. Kimsuky has not historically prioritized Taiwan, yet in this case, the actor demonstrated direct reconnaissance of Taiwanese government and developer networks. While this overlaps with Chinese APT priorities, recent evidence from the â€œKimâ€ dump, including analysis of phishing kits and credential theft workflows, suggests this activity was likely performed by a DPRK actor exploring broader regional interests, possibly in alignment with Chinese strategic goals. Researchers have noted that Kimsuky operators have recently asked questions in phishing lures related to potential Chinese-Taiwanese conflicts, implying interest beyond the Korean peninsula.



Some tooling overlaps with PRC-linked APTs, particularly GitHub-based stagers and proxy-resolving modules, but these are not uncommon in the open-source malware ecosystem and may reflect opportunistic reuse rather than deliberate mimicry.



IMINT Analysis: Visual Tradecraft and Cultural Camouflage



A review of image artifacts linked to the â€œKimâ€ actor reveals a deliberate and calculated use of Chinese social and technological visual content as part of their operational persona. These images, extracted from browser history and uploads attributed to the actor, demonstrate both strategic alignment with DPRK priorities and active cultural camouflage within the PRC digital ecosystem.



Uploads of images by Kim found in browser history



Images downloaded from aixfan[.]com



The visual set includes promotional graphics for Honor smartphones, SoC chipset evolution charts, Weibo posts featuring vehicle registration certificates, meme-based sarcasm, and lifestyle imagery typical of Chinese internet users. Notably, the content is exclusively rendered in simplified Chinese, reinforcing prior assessments that the operator either resides within mainland China or maintains a working digital identity embedded in Chinese platforms. Devices and services referenced, such as Xiaomi phones, Zhihu, Weibo, and Baidu, suggest intimate familiarity with PRC user environments.



Operationally, this behavior achieves two goals. First, it enables the actor to blend in seamlessly with native PRC user activity, which complicates attribution and helps bypass platform moderation or behavioral anomaly detection. Second, the content itself may serve as bait or credibility scaffolding (e.g. A framework to give the illusion of trust to allow for easier compromise ) in phishing and social engineering campaigns, especially those targeting developers or technical users on Chinese-language platforms.



Some images, such as the detailed chipset timelines and VPN or device certification posts, suggest a continued interest in supply chain reconnaissance and endpoint profilingâ€”both tradecraft hallmarks of Kimsuky and similar APT units. Simultaneously, meme humor, sarcastic overlays, and visual metaphors (e.g., the â€œKaijuâ€™s tail is showingâ€ idiom) indicate the actorâ€™s fluency in PRC netizen culture and possible mockery of operational security breachesâ€”whether their own or othersâ€™.



Taken together, this IMINT corpus supports the broader attribution model: a DPRK-origin operator embedded, physically or virtually, within the PRC, leveraging local infrastructure and social platforms to facilitate long-term campaigns against South Korea, Taiwan, and other regional targets while maintaining cultural and technical deniability.



Attribution Scenarios:




Option A: DPRK Operator Embedded in PRC

Use of Korean language, OCR targeting of Korean documents, and focus on GPKI systems strongly suggest North Korean origin.



Use of PRC infrastructure (e.g., Baidu, Gitee) and simplified Chinese content implies the operator is physically located in China or benefits from access to Chinese internet infrastructure.





Option B: PRC Operator Emulating DPRK

Taiwan-focused reconnaissance aligns with PRC cyber priorities.



Use of open-source tooling and phishing methods shared with PRC APTs could indicate tactical emulation.






The preponderance of evidence supports the hypothesis that â€œKimâ€ is a North Korean cyber operator embedded in China or collaborating with PRC infrastructure providers. This operational model allows the DPRK to amplify its reach, mask attribution, and adopt regional targeting strategies beyond South Korea, particularly toward Taiwan. As this hybrid model matures, it reflects the strategic adaptation of DPRK-aligned threat actors who exploit the permissive digital environment of Chinese networks to evade detection and expand their operational playbook.



Targeting Profiles



The â€œKimâ€ leak provides one of the clearest windows to date into the role-specific targeting preferences of the operator, revealing a deliberate focus on system administrators, credential issuers, and backend developers, particularly in South Korea and Taiwan.



In South Korea, the operatorâ€™s interest centers around PKI administrators and infrastructure engineers. The recovered OCR commands were used to extract technical details from PDF documents outlining Koreaâ€™s digital signature protocols, such as identity verification, certificate validation, and encrypted communications, components that form the backbone of Koreaâ€™s secure authentication systems. The goal appears to be not only credential theft but full understanding and potential replication of government-trusted PKI procedures. This level of targeting suggests a strategic intent to penetrate deeply trusted systems, potentially for use in later spoofing or identity masquerading operations.



PKI attack targets



In Taiwan, the operator shifted focus to developer infrastructure and cloud access portals. Specific domains accessed, like caa.org[.]tw/.git/, indicate attempts to enumerate internal repositories, most likely to discover hardcoded secrets, authentication tokens, or deployment keys. This is a classic supply chain targeting method, aiming to access downstream systems via compromised developer credentials or misconfigured services.



Additional activity pointed to interaction with cloud service login panels such as tw.systexcloud[.]com and mlogin.mdfapps[.]com. These suggest an attempt to breach centralized authentication systems or identity providers, granting the actor broader access into enterprise or government networks with a single credential set.



Taken together, these targeting profiles reflect a clear emphasis on identity providers, backend engineers, and those with access to system-level secrets. This reinforces the broader theme of the dump: persistent, credential-first intrusion strategies, augmented by reconnaissance of authentication standards, key management policies, and endpoint development infrastructure.



South Korean:




PKI admins, infrastructure engineers



OCR focus on Korean identity standards




Taiwanese:




Developer endpoints and internal .git/ repos



Access to cloud panels and login gateways




Final Assessment



The â€œKimâ€ leak represents one of the most comprehensive and technically intimate disclosures ever associated with Kimsuky (APT43) or its adjacent operators. It not only reaffirms known tactics, credential theft, phishing, and PKI compromise, but exposes the inner workings of the operatorâ€™s environment, tradecraft, and operational intent in ways rarely observed outside of active forensic investigations.



At the core of the leak is a technically competent actor, well-versed in low-level shellcode development, Linux-based persistence mechanisms, and certificate infrastructure abuse. Their use of NASM, API hashing, and rootkit deployment points to custom malware authorship. Furthermore, the presence of parsed government-issued Korean PDFs, combined with OCR automation, shows not just opportunistic data collection but a concerted effort to model, mimic, or break state-level identity systems, particularly South Koreaâ€™s GPKI.



The operatorâ€™s cultural and linguistic fluency in Korean, and their targeting of administrative and privileged systems across South Korean institutions, support a high-confidence attribution to a DPRK-native threat actor. However, the extensive use of Chinese platforms like gitee[.]com, Baidu, and Zhihu, and Chinese infrastructure for both malware hosting and browsing activity reveals a geographical pivot or collaboration: a hybrid APT footprint rooted in DPRK tradecraft but operating from or with Chinese support.



Most notably, this leak uncovers a geographical expansion of operational interest; the actor is no longer solely focused on the Korean peninsula. The targeting of Taiwanese developer portals, government research IPs, and .git/ repositories shows a broadened agenda that likely maps to both espionage and supply chain infiltration priorities. This places Taiwan, like South Korea, at the forefront of North Korean cyber interest, whether for intelligence gathering, credential hijacking, or as staging points for more complex campaigns.



The threat uncovered here is not merely malware or phishing; it is an infrastructure-centric, credential-first APT campaign that blends highly manual operations (e.g., hand-compiled shellcode, direct OCR of sensitive PDFs) with modern deception tactics such as AiTM phishing and TLS proxy abuse.



Organizations in Taiwan and South Korea, particularly those managing identity, certificate, and cloud access infrastructure, should consider themselves under persistent, credential-focused surveillance. Defensive strategies must prioritize detection of behavioral anomalies (e.g., use of OCR tools, GPKI access attempts), outbound communications with spoofed Korean domains, and the appearance of low-level toolchains like NASM or proxyres-based scanning utilities within developer or admin environments.



In short: the â€œKimâ€ actor embodies the evolution of nation-state cyber threatsâ€”a fusion of old-school persistence, credential abuse, and modern multi-jurisdictional staging. The threat is long-term, embedded, and adaptive.



Part III: Threat Intelligence Report



TLP WHITE:



Targeting Summary



The analysis of the â€œKimâ€ operator dump reveals a highly focused credential-theft and infrastructure-access campaign targeting high-value assets in both South Korea and Taiwan. Victims were selected based on their proximity to trusted authentication systems, administrative control panels, and development environments.



CategoryDetailsRegionsSouth Korea, TaiwanTargetsGovernment, Telecom, Enterprise ITAccountssvradmin, oracle, app_adm01, unwadm, shkim88, jaejung91Domainstw.systexcloud[.]com, nid-security[.]com, spo.go[.]kr, caa.org[.]tw/.git/



Indicators of Compromise (IOCs)



Domains




Phishing: nid-security[.]com, html-load[.]com, wuzak[.]com, koala-app[.]com, webcloud-notice[.]com



Spoofed portals: dcc.mil[.]kr, spo.go[.]kr, mofa.go[.]kr



Pastebin raw links: Used for payload staging and malware delivery




IP Addresses




External Targets (Taiwan):

163.29.3[.]119 Â  Â  National Center for High-performance Computing



118.163.30[.]45 Â  Taiwanese government subnet



59.125.159[.]81 Â  Chunghwa Telecom





Brute Forcing / Infrastructure Origins:

23.95.213[.]210 Â  VPS provider with malicious history



218.92.0[.]210 Â  Â  China Unicom



122.114.233[.]77Â  Henan Mobile, PRC






Internal Host IPs (Operator Environment)




192.168.130[.]117



192.168.150[.]117



192.168.0[.]39




Operator Environment: Internal Host IP Narrative



The presence of internal IP addresses such as 192.168.130[.]117, 192.168.150[.]117, and 192.168.0[.]39 within the dump offers valuable insight into the attackerâ€™s local infrastructure, an often-overlooked element in threat intelligence analysis. These addresses fall within private, non-routable RFC1918 address space, commonly assigned by consumer off-the-shelf (COTS) routers and small office/home office (SOHO) network gear.



The use of the 192.168.0[.]0/16 subnet, particularly 192.168.0.x and 192.168.150.x, strongly suggests that the actor was operating from a residential or low-profile environment, not a formal nation-state facility or hardened infrastructure. This supports existing assessments that North Korean operators, particularly those affiliated with Kimsuky, often work remotely from locations in third countries such as China or Southeast Asia, where they can maintain inconspicuous, low-cost setups while accessing global infrastructure.



Moreover, the distinction between multiple internal subnets (130.x, 150.x, and 0.x) may indicate segmentation of test environments or multiple virtual machines running within a single NATed network. This aligns with the forensic evidence of iterative development and testing workflows seen in the .bash_history files, where malware stagers, rootkits, and API obfuscation utilities were compiled, cleaned, and rerun repeatedly.



Together, these IPs reveal an operator likely working from a clandestine, residential base of operations, with modest hardware and commercial-grade routers. This operational setup is consistent with known DPRK remote IT workers and cyber operators who avoid attribution by blending into civilian infrastructure. It also suggests the attacker may be physically located outside of North Korea, possibly embedded in a friendly or complicit environment, strengthening the case for China-based activity by DPRK nationals.



MITRE ATT&CK Mapping



PhaseTechnique(s)Initial AccessT1566.002 ,Â  Adversary-in-the-Middle (AiTM) PhishingExecutionT1059.005 ,Â  Native API ShellcodeT1059.003 ,Â  Bash/Shell ScriptsCredential AccessT1555 ,Â  Credential Store DumpingT1557.003 ,Â  Session HijackingPersistenceT1176 ,Â  Rootkit (via khook syscall manipulation)Defense EvasionT1562.001 ,Â  Disable Security ToolsT1552 ,Â  Unsecured Credential FilesDiscoveryT1592 ,Â  Technical Information DiscoveryT1590 ,Â  Network InformationExfiltrationT1041 ,Â  Exfiltration over C2 ChannelT1567.002 ,Â  Exfil via Cloud Services



Tooling and Capabilities



The actorâ€™s toolkit spans multiple disciplines, blending malware development, system reconnaissance, phishing, and proxy evasion:




NASM-based shellcode loaders: Compiled manually for Windows execution.



Win32 API hashing: Obfuscated imports via hashstring.py to evade detection.



GitHub/Gitee abuse: Tooling hosted or cloned from public developer platforms.



OCR exploitation: Used ocrmypdf to parse Korean PDF specs related to digital certificates and VPN appliances.



Rootkit deployment: Hidden persistence paths including /usr/lib64/tracker-fs and /proc/acpi/pcicard.



Proxy config extraction: Investigated PAC URLs using proxyres-based recon.




Attribution Confidence Assessment



Attribution CandidateConfidence LevelDPRK-aligned (Kimsuky)High, Native Korean targeting, GPKI focus, OCR behaviorChina-blended infrastructureModerate, PRC hosting, Gitee usage, Taiwan focusSolely PRC ActorLow-to-Moderate, Tooling overlap but weak linguistic match



Assessment: The actor appears to be a DPRK-based APT operator working from within or in partnership with Chinese infrastructure, representing a hybrid attribution model.



Defensive Recommendations



AreaRecommendationPKI SecurityMonitor usage of .key, .sig, .crt artifacts; enforce HSM or 2FA for key usePhishing DefenseBlock domains identified in IoCs; validate TLS fingerprints and referrer headersEndpoint HardeningDetect use of nasm, make, and OCR tools; monitor /usr/lib*/tracker-* pathsNetwork TelemetryAlert on .git/ directory access from external IPs; monitor outbound to Pastebin/GitHubTaiwan FocusEstablish watchlists for .tw domains targeted by PRC-originating IPsAdmin AccountsReview usage logs for svradmin, oracle, app_adm01, and ensure rotation policies



APPENDIX A



Overlap or Confusion with Chinese Threat Actors



There is notable evidence of operational blur between Kimsuky and Chinese APTs in the context of Taiwan. The 2025 â€œKimâ€ data breach revealed an attacker targeting Taiwan whose tools and phishing kits matched Kimsukyâ€™s, yet whose personal indicators (language, browsing habits) suggested a Chinese national. Researchers concluded this actor was likely a Chinese hacker either mimicking Kimsuky tactics or collaborating with them.. In fact, the leaked files on DDoS Secrets hint that Kimsuky has â€œopenly cooperated with other Chinese APTs and shared their tools and techniquesâ€. This overlap can cause attribution confusion â€“ a Taiwan-focused operation might initially be blamed on China but could involve Kimsuky elements, or vice versa. So far, consensus is that North Korean and Chinese cyber operations remain separate, but cases like â€œKimâ€ show how a DPRK-aligned actor can operate against Taiwan using TTPs common to Chinese groups, muddying the waters of attribution.



File List from dump:















Master Evidence Inventory:



File NameLanguageContent SummaryCategoryRelevance.bash_historyMixed (EN/KR)Operator shell history commandsSystem/LogShows rootkit compilation, file ops, network testsuser-bash_historyMixed (EN/KR)User-level shell commandsSystem/LogDevelopment and test activityroot-bash_historyMixed (EN/KR)Root-level shell commandsSystem/LogPrivilege-level activity, implant deploymentauth.log.2EN/KRAuthentication logs (PAM/SSH)System/LogCredential changes marked ë³€ê²½ì™„ë£Œ, brute force IPs20190315.logENSystem log fileSystem/LogAuth and system access eventschrome-timeline.txtENBrowser activity timelineBrowserVisited domains extractionchromehistory.txtENBrowser history exportBrowserURLs visitedhistory.sqliteENEmpty DB fileBrowserNo useful dataMedia HistoryENEmpty SQLite DBBrowserNo playback activityHistoryENEmpty Brave/Chromium DBBrowserNo visited URLsWeb DataENAutofill/search DBBrowserSearch engines used (Google, DuckDuckGo, Qwant, Startpage, Ecosia)Visited LinksBinaryLevelDB/binary structureBrowserCould not extract URLsCookiesENSQLite DB with cookiesBrowserGoogle cookies foundrequest_log.txt.20250220ENCaptured phishing sessionPhishingSpoofed spo.go.kr, base64 credential loggingæŠ€æœ¯è¯´æ˜Žä¹¦ â€“ 22.docxZHChinese rootkit stealth manualRootkitKernel hiding, binary embedding1.ko å›¾æ–‡ç¼–è¯‘ .docZHChinese compilation guideRootkitRootkit build process1. build ko .txtZHBuild notesRootkitImplant compilation instructions0. ä½¿ç”¨.txtZHUsage notesRootkitImplant usage and commandsre æ­£å‘å·¥å…·ä¿®æ”¹å»ºè®® 1.0.txtZHModification notesRootkitReverse tool modification suggestions1111.txtZHRootkit/tool snippetRootkitPart of implant notesclientBinaryRootkit client binaryRootkitController for implant communicationSSA_AO_AD_WT_002_ì›¹ë³´ì•ˆ í”„ë¡œí† ì½œì„¤ê³„ì„œ_Ver1.0_.docKRGPKI protocol design docPKIKorean web PKI standardsí–‰ìžë¶€ ì›¹ë³´ì•ˆAPI ì¸ìˆ˜ì¸ê³„.docKRGPKI API deployment manualPKIDeployment and cert API internalsHIRA-IR-T02_ì˜ì•½í’ˆì²˜ë°©ì¡°ì œ_ComLibrary_í†µì‹ ì „ë¬¸.docKRMedical ComLibrary XML specHealthcarePrescription system communication(ë³„ì§€2)í–‰ì •ì „ìžì„œëª…_ê¸°ìˆ ìš”ê±´_141125.pdfKRPKI requirements PDFPKIOCR targetSecuwaySSL U_ì¹´ë‹¬ë¡œê·¸.pdfKRVPN catalogPKI/VPNOCR targetphrack-apt-down-the-north-korea-files.pdfENPhrack articleReferenceBackground on Kimsuky dumpMuddled Libra Threat Assessment.pdfENThreat intel reportReferenceComparative threat actor studyLeaked North Korean Linux Stealth Rootkit Analysis.pdfENRootkit analysisReferenceDetailed implant studyInside the Kimsuky Leak.docx (various)ENThreat report draftsReportWorking versionsaccount (2).txtENDB export (DBsafer, TrustedOrange)InfraAccounts and DB changesresult.txtKRCert-related parsed dataInfraIncluded GPKI .key/.sigenglish_wikipedia.txtENWikipedia dumpReferenceUnrelated baselinebookmarks-2021-01-04.jsonlz4ENFirefox bookmarks (compressed)BrowserNeeds decompressionScreenshot translationsZHChinese text (rootkit marketing blurb)RootkitKernel hiding tool description




                            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Stop writing CLI validation. Parse it right the first time]]></title>
            <link>https://hackers.pub/@hongminhee/2025/stop-writing-cli-validation-parse-it-right-the-first-time</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151622</guid>
            <description><![CDATA[This post introduces Optique, a new library created to address the pervasive problem of repetitive and often messy validation code in CLI tools. The author was motivated by the observation that nearly every CLI tool reinvents the wheel with similar validation patterns for dependent options, mutually exclusive options, and environment-specific requirements. Optique leverages parser combinators and TypeScript's type inference to ensure that CLI arguments are parsed directly into valid configurations, eliminating the need for manual validation. By describing the desired CLI configuration with Optique, TypeScript automatically infers the types and constraints, catching potential bugs at compile time. The author shares their experience of deleting large chunks of validation code and simplifying refactoring tasks. Optique aims to provide a more robust and maintainable approach to CLI argument parsing, potentially saving developers from writing the same validation logic repeatedly.]]></description>
            <content:encoded><![CDATA[I have this bad habit. When something annoys me enough times,
I end up building a library for it. This time, it was CLI validation code.
See, I spend a lot of time reading other people's code. Open source projects,
work stuff, random GitHub repos I stumble upon at 2 AM. And I kept noticing this
thing: every CLI tool has the same ugly validation code tucked away somewhere.
You know the kind:
if (!opts.server && opts.port) {
  throw new Error("--port requires --server flag");
}

if (opts.server && !opts.port) {
  opts.port = 3000; // default port
}

// wait, what if they pass --port without a value?
// what if the port is out of range?
// what if...
It's not even that this code is hard to write. It's that it's everywhere.
Every project. Every CLI tool. The same patterns, slightly different flavors.
Options that depend on other options. Flags that can't be used together.
Arguments that only make sense in certain modes.
And here's what really got me: we solved this problem years ago for other types
of data. Justâ€¦ not for CLIs.
The problem with validation 
There's this blog post that completely changed how I think about parsing.
It's called Parse, don't validate by Alexis King. The gist? Don't parse data
into a loose type and then check if it's valid. Parse it directly into a type
that can only be valid.
Think about it. When you get JSON from an API, you don't just parse it as any
and then write a bunch of if-statements. You use something like Zod to parse
it directly into the shape you want. Invalid data? The parser rejects it. Done.
But with CLIs? We parse arguments into some bag of properties and then spend
the next 100 lines checking if that bag makes sense. It's backwards.
So yeah, I built Optique. Not because the world desperately needed another CLI
parser (it didn't), but because I was tired of seeingâ€”and writingâ€”the same
validation code everywhere.
Three patterns I was sick of validating 
Dependent options 
This one's everywhere. You have an option that only makes sense when another
option is enabled.
The old way? Parse everything, then check:
const opts = parseArgs(process.argv);
if (!opts.server && opts.port) {
  throw new Error("--port requires --server");
}
if (opts.server && !opts.port) {
  opts.port = 3000;
}
// More validation probably lurking elsewhere...
With Optique, you just describe what you want:
const config = withDefault(
  object({
    server: flag("--server"),
    port: option("--port", integer()),
    workers: option("--workers", integer())
  }),
  { server: false }
);
Here's what TypeScript infers for config's type:
type Config = 
  | { readonly server: false }
  | { readonly server: true; readonly port: number; readonly workers: number }
The type system now understands that when server is false, port literally
doesn't exist. Not undefined, not nullâ€”it's not there. Try to access it and
TypeScript yells at you. No runtime validation needed.
Mutually exclusive options 
Another classic. Pick one output format: JSON, YAML, or XML. But definitely not
two.
I used to write this mess:
if ((opts.json ? 1 : 0) + (opts.yaml ? 1 : 0) + (opts.xml ? 1 : 0) > 1) {
  throw new Error('Choose only one output format');
}
(Don't judge me, you've written something similar.)
Now?
const format = or(
  map(option("--json"), () => "json" as const),
  map(option("--yaml"), () => "yaml" as const),
  map(option("--xml"), () => "xml" as const)
);
The or() combinator means exactly one succeeds. The result is just
"json" | "yaml" | "xml". A single string. Not three booleans to juggle.
Environment-specific requirements 
Production needs auth. Development needs debug flags. Docker needs different
options than local. You know the drill.
Instead of a validation maze, you just describe each environment:
const envConfig = or(
  object({
    env: constant("prod"),
    auth: option("--auth", string()),      // Required in prod
    ssl: option("--ssl"),
    monitoring: option("--monitoring", url())
  }),
  object({
    env: constant("dev"),
    debug: optional(option("--debug")),    // Optional in dev
    verbose: option("--verbose")
  })
);
No auth in production? Parser fails immediately. Trying to access --auth in
dev mode? TypeScript won't let youâ€”the field doesn't exist on that type.
â€œBut parser combinators thoughâ€¦â€ 
I know, I know. â€œParser combinatorsâ€ sounds like something you'd need
a CS degree to understand.
Here's the thing: I don't have a CS degree. Actually, I don't have any degree.
But I've been using parser combinators for years because they're actuallyâ€¦ not
that hard? It's just that the name makes them sound way scarier than they are.
I'd been using them for other stuffâ€”parsing config files, DSLs, whatever.
But somehow it never clicked that you could use them for CLI parsing until
I saw Haskell's optparse-applicative. That was a real â€œwait, of courseâ€
moment. Like, why are we doing this any other way?
Turns out it's stupidly simple. A parser is just a function. Combinators are
just functions that take parsers and return new parsers. That's it.
// This is a parser
const port = option("--port", integer());

// This is also a parser (made from smaller parsers)
const server = object({
  port: port,
  host: option("--host", string())
});

// Still a parser (parsers all the way down)
const config = or(server, client);
No monads. No category theory. Just functions. Boring, beautiful functions.
TypeScript does the heavy lifting 
Here's the thing that still feels like cheating: I don't write types for my CLI
configs anymore. TypeScript justâ€¦ figures it out.
const cli = or(
  command("deploy", object({
    action: constant("deploy"),
    environment: argument(string()),
    replicas: option("--replicas", integer())
  })),
  command("rollback", object({
    action: constant("rollback"),
    version: argument(string()),
    force: option("--force")
  }))
);

// TypeScript infers this type automatically:
type Cli = 
  | { 
      readonly action: "deploy"
      readonly environment: string
      readonly replicas: number
    }
  | { 
      readonly action: "rollback"
      readonly version: string
      readonly force: boolean
    }
TypeScript knows that if action is "deploy", then environment exists but
version doesn't. It knows replicas is a number. It knows force is
a boolean. I didn't tell it any of this.
This isn't just about nice autocomplete (though yeah, the autocomplete is great).
It's about catching bugs before they happen. Forget to handle a new option
somewhere? Code won't compile.
What actually changed for me 
I've been dogfooding this for a few weeks. Some real talk:
I delete code now. Not refactor. Delete. That validation logic that used to
be 30% of my CLI code? Gone. It feels weird every time.
Refactoring isn't scary. Want to know something that usually terrifies me?
Changing how a CLI takes its arguments. Like going from --input file.txt to
just file.txt as a positional argument. With traditional parsers,
you're hunting down validation logic everywhere. With this?
You change the parser definition, TypeScript immediately shows you every place
that breaks, you fix them, done. What used to be an hour of â€œdid I catch
everything?â€ is now â€œfix the red squiggles and move on.â€
My CLIs got fancier. When adding complex option relationships doesn't mean
writing complex validation, you justâ€¦ add them. Mutually exclusive groups?
Sure. Context-dependent options? Why not. The parser handles it.
The reusability is real too:
const networkOptions = object({
  host: option("--host", string()),
  port: option("--port", integer())
});

// Reuse everywhere, compose differently
const devServer = merge(networkOptions, debugOptions);
const prodServer = merge(networkOptions, authOptions);
const testServer = merge(networkOptions, mockOptions);
But honestly? The biggest change is trust. If it compiles, the CLI logic works.
Not â€œprobably worksâ€ or â€œworks unless someone passes weird arguments.â€
It just works.
Should you care? 
If you're writing a 10-line script that takes one argument, you don't need this.
process.argv[2] and call it a day.
But if you've ever:

Had validation logic get out of sync with your actual options
Discovered in production that certain option combinations explode
Spent an afternoon tracking down why --verbose breaks when used with
--json
Written the same â€œoption A requires option Bâ€ check for the fifth time

Then yeah, maybe you're tired of this stuff too.
Fair warning: Optique is young. I'm still figuring things out, the API might
shift a bit. But the core ideaâ€”parse, don't validateâ€”that's solid.
And I haven't written validation code in months.
Still feels weird. Good weird.
Try it or don't 
If this resonates:

Tutorial: Build something real, see if you hate it
Concepts: Primitives, constructs, modifiers, value parsers,
the whole thing
GitHub: The code, issues, angry rants

I'm not saying Optique is the answer to all CLI problems. I'm just saying
I was tired of writing the same validation code everywhere, so I built something
that makes it unnecessary.
Take it or leave it. But that validation code you're about to write?
You probably don't need it.
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GigaByte CXL memory expansion card with up to 512GB DRAM]]></title>
            <link>https://www.gigabyte.com/PC-Accessory/AI-TOP-CXL-R5X4</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45151598</guid>
            <description><![CDATA[Expand Memory Pool 16-Layers HDI PCB Equipped with an AIO Fan DDR5 RDIMM Support Support PCle 5.0 Lower Total Cost of Ownership (TCO)]]></description>
            <content:encoded><![CDATA[


            
            
    
    
    




    








    
			
                
                            
                                
                                     FEATURES 
                                    
                                         Expand Memory Pool 
                                         16-Layers HDI PCB 
                                         Equipped with an AIO Fan 
                                         DDR5 RDIMM Support 
                                         Support PCle 5.0 
                                         Lower Total Cost of Ownership (TCO) 
                                    
                                
                                
                                    
                                    
                                
                            
                            
                            
                        
                
                                    
                                        
                                            
                                            
                                        
                                        
                                                 GIGABYTE AI TOP 
                                                 Train your own AI on your desk. 
                                            
                                    
                                    
                                        
                                            
                                            
                                        
                                        
                                                 Ultimate scalability and Connectivityâ€‹ 
                                                 Brings you the future-proofing capability. 
                                            
                                    
                                
                
                    
                    
                        
                            
                                     GIGABYTE AI TOP 
                                     Train your own AI on your desk 
                                     In the age of local AI, GIGABYTE AI TOP is the all-round solution to win advantages ahead of traditional AI training methods. It features a variety of groundbreaking technologies that can be easily adapted by beginners or experts, for most common open-source LLMs, in anyplace even on your desk. 
                                
                            
                                
                                    Supports 236B LLM Local Training
                                
                                
                                    Intuitive Set-up
                                
                                
                                    Flexbility & Upgradability
                                
                                
                                    Privacy & Security
                                
                                
                                    Suitable for Home Use
                                
                            
                        
                        
                            
                                    
                                        
                                    
                                    
                                         AI TOP Utility 2.0 
                                         Reinventing AI Training 
                                    
                                     The all-new AI TOP Utility 2.0 brings enhanced features and broader model support, empowering users from novices to experts to unlock AI's potential right at their desks. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                
                            
                                    
                                         AI TOP Hardware 
                                         Enhanced Performance for AI Training 
                                    
                                     The AI TOP Hardware features a series of GIGABYTE AI TOP products that are optimized in power efficiency and durability for AI training workloads. It includes upgradeable components and is easy to build at home. 
                                    
                                        
                                             Learn more 
                                        
                                    
                                     * Images for reference only, system configuration will vary by model. 
                                
                        
                    
                
                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultimate scalability 
                                                                 Exclusive Memory Offloading Solution 
                                                                 Expand training capacity by offloading data to system memory and SSDs, enabling efficient training of Big models through optimized memory management. 
                                                            
                                                            
                                                            
                                                                                         Offers diverse memory expandability with support for DDR5 ECC Registered memory modules (RDIMM): 
                                                                                        
                                                                                             - 4 x DDR5 DIMM sockets, supporting up to 512 GB of memory (up to 128 GB per DIMM) 
                                                                                        
                                                                                         Provides users with High flexibility and options suitable for various requirement scenarios. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Future Connectivity 
                                                                 Express to PCIE5 Performance 
                                                                 The new AI TOP CXL PCI Express card provides the most efficient solution to enjoy boosted performance of PCIe Gen5 on the existing platform. PCIe 5.0 doubles the data transfer rate of PCIe 4.0 from 16 GT/s to 32 GT/s per lane. This substantial increase in bandwidth particularly benefits high-performance AIO cards, storage devices, and AI accelerators that demand faster data movement. 
                                                            
                                                            
                                                                                         Benefits of CXL Memory: 
                                                                                        
                                                                                            
                                                                                                 *Overall lower total cost of ownership (TCO)* 
                                                                                                 -Improve computational and memory resource utilization for specific applications, thereby reducing capital expenditure and operating costs. 
                                                                                            
                                                                                            
                                                                                                 *Breaking Memory Bottlenecks:* 
                                                                                                 - Traditional server memory capacity limits the development of complex applications like AI. CXL memory provides a way to break through this barrier, allowing servers to expand memory capacity and meet the demands of increasingly complex AI applications. 
                                                                                            
                                                                                            
                                                                                                 *Improving Memory Utilization Efficiency:* 
                                                                                                 - CXL memory enables efficient use of memory resources through cross-device memory sharing and memory pooling technologies, thereby increasing memory utilization rates. 
                                                                                            
                                                                                            
                                                                                                 *Accelerating Computation Speed:* 
                                                                                                 - CXL memory allows the CPU to access memory on accelerators, achieving cross-device memory sharing and heterogeneous computing, which accelerates computation speed. 
                                                                                            
                                                                                            
                                                                                                 *Supporting Memory Consistency:* 
                                                                                                 - The CXL protocol (such as CXL.io, CXL.cache, CXL.mem) ensures consistency in memory operations across devices, avoiding data inconsistency issues. 
                                                                                            
                                                                                            
                                                                                                 *Flexible Scaling and Sharing:* 
                                                                                                 - CXL memory supports many-to-many flexible connections, allowing for many-to-many connections between accelerators and CPUs, achieving flexible scaling and memory sharing. 
                                                                                            
                                                                                        
                                                                                        
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Superior thermal Design 
                                                                 Comprehensive Thermal 
                                                                 Advanced full-metal thermal design and durable heatsinks to keep your system cool and efficient. 
                                                            
                                                            
                                                                                         AIO Fan Thermal Design 
                                                                                         High-efficiency fan design has achieved a simultaneous increase in both airflow and pressure, it helps to quickly dissipate heat and stabilize output performance. 
                                                                                    
                                                        
                                                
                                            
                                                    
                                                    
                                                    
                                                            
                                                                 Ultra Durable 
                                                                 Born from Ultra Durableâ„¢ 
                                                                 Technology embodies our commitment to excellence, providing gamers with a platform that's not only powerful but also built for longevity and reliability. AI TOP series Product are engineered to endure and excel. 
                                                            
                                                            
                                                                                    
                                                                                         Long Lifespan Durableâ„¢ Solid Caps 
                                                                                         GIGABYTE motherboards integrate the absolute best quality solid state capacitors that are rated to perform at maximum efficiency for extended periods, even in extreme performance configurations. With ultra-low ESR no matter how high the load, this provides peace of mind for end users who want to push their system hard, yet demand absolute reliability and stability. These exclusive capacitors also come in customized jet, exclusively on GIGABYTE product. 
                                                                                        
                                                                                    
                                                                                    
                                                                                         Humidity Protection with New Glass Fabric PCB 
                                                                                         There is nothing more harmful to the longevity of your PC than moisture, and most parts of the world experience moisture in the air as humidity at some point during the year. GIGABYTE motherboards have been designed to make sure that humidity is never an issue, incorporating a new Glass Fabric PCB technology that repels moisture caused by humid and damp conditions.  Glass Fabric PCB technology uses a new PCB material which reduces the amount of space between the fiber weave, making it much more difficult for moisture to penetrate compared to traditional motherboard PCBs. This offers much better protection from short circuit and system malfunction caused by humid and damp conditions. 
                                                                                        
                                                                                    
                                                                                
                                                        
                                                
                                        
                
                    
                        
                    
                
                
                         * This product requires compatible AI TOP hardware to be supported. A single CXL card cannot properly activate the AI TOP Utility. We recommend consulting with our Sales representative or authorized dealer contact before purchasing this product. 
                    
            
	

		* Product specifications and product appearance may differ from country to country. We recommend that you check with your local dealers for the specifications and appearance of the products available in your country. Colors of products may not be perfectly accurate due to variations caused by photographic variables and monitor settings so it may vary from images shown on this site. Although we endeavor to present the most accurate and comprehensive information at the time of publication, we reserve the right to make changes without prior notice.
	







                



        ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Oldest recorded transaction]]></title>
            <link>https://avi.im/blag/2025/oldest-txn/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45149626</guid>
            <description><![CDATA[The oldest recorded transaction was in 3100 BC]]></description>
            <content:encoded><![CDATA[The other day I posted a tweet with this image which I thought was funny:This is the oldest transaction database from 3100 BC - recording accounts of malt and barley groats. Considering this thing survived 5000 years (holy shit!) with zero downtime and has stronger durability guarantees than most databases today.I call it rock solid durability.This got me thinking, can I insert this date in todayâ€™s database? What is the oldest timestamp a database can support?So I checked the top three databases: MySQL, Postgres, and SQLite:MySQL1000 ADPostgres4713 BCSQLite4713 BCToo bad you cannot use MySQL for this. Postgres and SQLite support the Julian calendar and the lowest date is Jan 01, 4713 BC:sales=# INSERT INTO orders VALUES ('4713-01-01 BC'::date);
INSERT 0 1
sales=# SELECT * FROM orders;
   timestamp
---------------
 4713-01-01 BC
(1 row)
sales=# INSERT INTO orders VALUES ('4714-01-01 BC'::date);
ERROR:  date out of range: "4714-01-01 BC"
I wonder how people store dates older than this. Maybe if Iâ€™m a British Museum manager, and I want to keep theft inventory details. How do I do it? As an epoch? Store it as text? Use some custom system? How do I get it to support all the custom operations that a typical TIMESTAMP supports?Thanks to aku, happy_shady, Mr. Bhat, and General Bruh for reading an early draft of this post.1. Source of the image: Sumer civilization2. I found this from the talk 1000x: The Power of an Interface for Performance by
Joran Dirk Greef, CEO of TigerBeetle, timestamped @ 38:10.3. The talk has other bangers too, like this or this.]]></content:encoded>
        </item>
    </channel>
</rss>