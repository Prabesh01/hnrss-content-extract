<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Sun, 31 Aug 2025 03:46:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[ETFs now hold more than $3.1T worth of just top US companies]]></title>
            <link>https://www.signalbloom.ai/etf/stats</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45079532</guid>
        </item>
        <item>
            <title><![CDATA[Scottish brothers finish mammoth row across Pacific Ocean after 139 days]]></title>
            <link>https://www.abc.net.au/news/2025-08-30/scottish-maclean-brothers-finish-pacific-ocean-row/105711488</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45079365</guid>
            <description><![CDATA[After 139 days at sea, three Scottish brothers have set a new world record by completing the first and fastest unsupported row across the Pacific Ocean.]]></description>
            <content:encoded><![CDATA[After 139 days at sea, three Scottish brothers have set a new world record by completing the first and fastest unsupported row across the Pacific Ocean.Ewan, Jamie and Lachlan McClean rowed more than 14,000 kilometres from Lima, Peru, to Cairns, becoming the first team ever to complete the full crossing from South America to Australia. The previous record of 159 days had been set in 2014 by Russian rower Fedor Konyukhov.Ewan told the ABC the trip was "pretty overwhelming" upon arrival on Australian shores. "We're still got our sea legs, so yeah, wobbling all over the place to be honest, but very happy to be ashore," he said.The brothers broke multiple records.  (Supplied)The brothers took on the challenge to raise funds for clean water projects in Madagascar, raising more than 795,000 pounds ($1.64m) so far to help 40,000 people. This caught the attention of many celebrities, including Mark Wahlberg, who spoke with the trio while they were at sea.Bad weather changes plansThe trip was not all smooth sailing.Bad weather meant the brothers could not finish at their planned destination, Sydney."The last couple of weeks have been really hard," Jamie said."Our expectations being crushed when we thought we were going to arrive and then getting hit by storm after storm, just getting knocked back, getting thrown north."To have to break the news to them [family] that, unfortunately, the weather's turned against us and we're going to be out here for a lot longer than we anticipated. That was really, really hard." The brothers battled seasickness, injuries, extreme weather, a broken water maker, a faulty auto-helm, and dwindling food supplies.Lachlan experienced a terrifying man-overboard incident during a night shift, when his brother Ewan quickly managed to rescue him."A side-on wave came in out of nowhere. I had like a couple of seconds to react and it just hit me," Lachlan said. "Took me right off my feet. I kind of hit the life-lines on the starboard side, basically did like a backwards somersault into the water." The brothers survived on freshly caught fish and freeze-dried meals prepared by Jamie, which they ran out of on Friday. The brothers lived on freshly caught fish.  (Supplied)Celebrations in CairnsThe brothers arrived to celebrations.  (Supplied)Celebrations were underway as the brothers arrived in Cairns, with dozens of friends and family from the UK — including their mother, Sheila — joining in. "It's honestly so hard to describe … it's just extremely admirable what they are doing, and they make it look like it's just another day in the office," their friend Isla Dolling said. The brothers celebrate arriving in Cairns. (Supplied)The brothers' boat, Rose Emily, named after the sister they never knew, is believed to be the fastest and lightest ocean rowing boat ever built, created with ocean rowing legend Mark Slats.The trio had previously broken three world records on an Atlantic row in 2019."Ocean rowing has given us new-found appreciation for things we used to take for granted — like going for a shower, lying in bed, or simply leaning on something stationary," Jamie said. "While I might miss the routine, the solitude, the sunsets, and sunrises, right now I'm just very glad to be back on land with my friends and family, who I've missed so much."After nearly five months at sea, the brothers will have a well-deserved rest before attending an event at Sydney's Maritime Museum next week.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hurricane category 6 could be introduced under new storm severity scale]]></title>
            <link>https://www.livescience.com/planet-earth/hurricanes/now-is-the-time-hurricane-category-6-could-be-introduced-under-new-storm-severity-scale</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45079065</guid>
            <description><![CDATA[The current hurricane classification does not consider storm surge and rainfall risks, which cause almost 80% of hurricane-related deaths. A new scale could help people better prepare for storms.]]></description>
            <content:encoded><![CDATA[









Hurricane Florence made landfall in South Carolina in September 2018. It was a Category 1 hurricane, but the devastating flooding that followed killed 55 people.
(Image credit: Stocktrek Images/Getty Images)




A new hurricane categorization system could help people better prepare for storms by incorporating risks from storm surges and rainfall into the categories, a study published this month reveals.Storm surges — elevated seawater levels near coasts — and rainfall cause almost 80% of hurricane deaths, yet they are not accounted for in the Saffir-Simpson Hurricane Wind Scale (SSHWS), which forecasters currently use to categorize a hurricane's severity and plays a key role in communicating hurricane risk to the public. Some experts have previously argued that the threat of storms is not always properly reflected in the SSHWS's 1 to 5 category ratings, which are based solely on wind speed."There have been too many instances of incredible loss of life and destruction because a low category number on the SSHWS [...] did not match the danger of the storm," Jennifer Collins, a professor in the School of Geosciences at the University of South Florida and co-author of the new study, said in a statement.The SSHWS estimates potential property damage from sustained wind, ranging from "some damage" in a Category 1 hurricane to "catastrophic damage" in a Category 4 or 5 storm. But property damage isn't the only potentially deadly effect of a hurricane. A low-category hurricane may still cause a tremendous tidal surge and unleash torrential rain, triggering devastating floods and other hazards.One example is 2005's Hurricane Katrina, which was listed as a Category 3 based on wind speeds. But storm surge and rainfall were responsible for most of the 1,800 deaths caused by Katrina and contributed hugely to the $125 billion in damage, according to the new study.Another example is Hurricane Florence, which made landfall in South Carolina in 2018 as a Category 1. The low danger rating did not alert communities to the catastrophic flooding that killed 55 people across the southeastern U.S., the researchers said.Related: Birth of a hurricane: What meteorologists look for as they hunt for early signs of a tropical cyclone formingGet the world’s most fascinating discoveries delivered straight to your inbox."Frequently, people use the storm's category to decide whether to evacuate," Collins said. "That's incredibly dangerous because if they hear it's only a tropical storm or Category 1, too often no alarm bells go off, and they see no cause for concern."To address the SSHWS's shortcomings, Collins and colleagues developed an alternative hurricane warning system in 2021. Dubbed the Tropical Cyclone Severity Scale (TCSS), this system has six categories and takes into account wind speed, storm surge and rainfall — the three biggest hazards from hurricanes. 


Hurricane Katrina in 2005 struck Mobile, Alabama, with a 15-foot storm surge and winds of over 110 mph. It was classified as a Category 3 hurricane. (Image credit: Warren Faidley/Getty Images)The TCSS assigns scores between 1 and 5 to each of the three hazards depending on their predicted severity for a given hurricane. These scores are then combined into a final score, which is established using three rules in different scenarios.First, the final score is never lower than the highest of the three individual hazard scores. Second, if two individual hazards have the same scores of 3 or higher, then the final score increases by one — so, if storm surge has a score of 2 but wind and rainfall are both 3, then the hurricane is classed as a Category 4. The third rule is that a final score of 6 is given if either two hazards have scores of 5, or if two hazards have scores of 4 and the third is a 5."The higher category is important," Collins said. "Many people base their decision to evacuate on that number, not just the details of the hazard."A "more realistic" systemResearchers have been working on the TCSS for several years, but the new study looked to confirm its effectiveness at warning the public of a hurricane's dangers. To test their warning system, Collins and her colleagues sent 4,000 participants living along the Gulf and East coasts forecasts for 10 fictitious hurricanes affecting their communities.Half of the participants received warnings in the SSHWS format, while the other half received warnings using the TCSS system. They then completed an online quiz about how they would react in the different scenarios. The team’s findings were published Aug. 19 in the journal Scientific Reports.Participants who were sent TCSS forecasts were more likely to identify the main hazard from a hurricane correctly, and significantly more likely to evacuate for non-wind hazards than those who were sent SSHWS forecasts, according to the study.RELATED STORIESCorrect identification of the main hazard boosted participants' intent to take relevant precautions, such as shielding their houses against flooding with sandbags and erecting window protections against the wind. On the other hand, participants who had incomplete information about a storm were more likely to miscalculate risks or take no measures at all.The results suggest that shifting away from the SSHWS would improve the public's understanding of hurricane risks and lead to more informed decision-making ahead of storms, Collins said."I'm fairly optimistic that now is the time," she said. "We now know many people make decisions based on the category messaging, so we need to ensure that we are communicating with a scale which is more realistic of the severity of the hurricane."


Sascha is a U.K.-based staff writer at Live Science. She holds a bachelor’s degree in biology from the University of Southampton in England and a master’s degree in science communication from Imperial College London. Her work has appeared in The Guardian and the health website Zoe. Besides writing, she enjoys playing tennis, bread-making and browsing second-hand shops for hidden gems.







]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Enrollment at trade schools is expected to grow 6.6% a year]]></title>
            <link>https://finance.yahoo.com/news/ai-cant-install-an-hvac-system-why-gen-z-is-flocking-to-jobs-in-the-trades-171735856.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45078651</guid>
        </item>
        <item>
            <title><![CDATA[Affiliates flock to scam gambling machine]]></title>
            <link>https://krebsonsecurity.com/2025/08/affiliates-flock-to-soulless-scam-gambling-machine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45078530</guid>
            <description><![CDATA[Last month, KrebsOnSecurity tracked the sudden emergence of hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. We've since learned that these scam gambling sites…]]></description>
            <content:encoded><![CDATA[
												Last month, KrebsOnSecurity tracked the sudden emergence of hundreds of polished online gaming and wagering websites that lure people with free credits and eventually abscond with any cryptocurrency funds deposited by players. We’ve since learned that these scam gambling sites have proliferated thanks to a new Russian affiliate program called “Gambler Panel” that bills itself as a “soulless project that is made for profit.”
A machine-translated version of Gambler Panel’s affiliate website.
The scam begins with deceptive ads posted on social media that claim the wagering sites are working in partnership with popular athletes or social media personalities. The ads invariably state that by using a supplied “promo code,” interested players can claim a $2,500 credit on the advertised gaming website.
The gaming sites ask visitors to create a free account to claim their $2,500 credit, which they can use to play any number of extremely polished video games that ask users to bet on each action. However, when users try to cash out any “winnings” the gaming site will reject the request and prompt the user to make a “verification deposit” of cryptocurrency — typically around $100 — before any money can be distributed.
Those who deposit cryptocurrency funds are soon pressed into more wagering and making additional deposits. And — shocker alert — all players eventually lose everything they’ve invested in the platform.
The number of scam gambling or “scambling” sites has skyrocketed in the past month, and now we know why: The sites all pull their gaming content and detailed strategies for fleecing players straight from the playbook created by Gambler Panel, a Russian-language affiliate program that promises affiliates up to 70 percent of the profits.

Gambler Panel’s website gambler-panel[.]com links to a helpful wiki that explains the scam from cradle to grave, offering affiliates advice on how best to entice visitors, keep them gambling, and extract maximum profits from each victim.
“We have a completely self-written from scratch FAKE CASINO engine that has no competitors,” Gambler Panel’s wiki enthuses. “Carefully thought-out casino design in every pixel, a lot of audits, surveys of real people and test traffic floods were conducted, which allowed us to create something that has no doubts about the legitimacy and trustworthiness even for an inveterate gambling addict with many years of experience.”
Gambler Panel explains that the one and only goal of affiliates is to drive traffic to these scambling sites by any and all means possible.
A machine-translated portion of Gambler Panel’s singular instruction for affiliates: Drive traffic to these scambling sites by any means available.
“Unlike white gambling affiliates, we accept absolutely any type of traffic, regardless of origin, the only limitation is the CIS countries,” the wiki continued, referring to a common prohibition against scamming people in Russia and former Soviet republics in the Commonwealth of Independent States.
The program’s website claims it has more than 20,000 affiliates, who earn a minimum of $10 for each verification deposit. Interested new affiliates must first get approval from the group’s Telegram channel, which currently has around 2,500 active users.
The Gambler Panel channel is replete with images of affiliate panels showing the daily revenue of top affiliates, scantily-clad young women promoting the Gambler logo, and fast cars that top affiliates claimed they bought with their earnings.
A machine-translated version of the wiki for the affiliate program Gambler Panel.
The apparent popularity of this scambling niche is a consequence of the program’s ease of use and detailed instructions for successfully reproducing virtually every facet of the scam. Indeed, much of the tutorial focuses on advice and ready-made templates to help even novice affiliates drive traffic via social media websites, particularly on Instagram and TikTok.
Gambler Panel also walks affiliates through a range of possible responses to questions from users who are trying to withdraw funds from the platform. This section, titled “Rules for working in Live chat,” urges scammers to respond quickly to user requests (1-7 minutes), and includes numerous strategies for keeping the conversation professional and the user on the platform as long as possible.
A machine-translated version of the Gambler Panel’s instructions on managing chat support conversations with users.
The connection between Gambler Panel and the explosion in the number of scambling websites was made by a 17-year-old developer who operates multiple Discord servers that have been flooded lately with misleading ads for these sites.
The researcher, who asked to be identified only by the nickname “Thereallo,” said Gambler Panel has built a scalable business product for other criminals.
“The wiki is kinda like a ‘how to scam 101’ for criminals written with the clarity you would expect from a legitimate company,” Thereallo said. “It’s clean, has step by step guides, and treats their scam platform like a real product. You could swap out the content, and it could be any documentation for startups.”
“They’ve minimized their own risk — spreading the links on Discord / Facebook / YT Shorts, etc. — and outsourced it to a hungry affiliate network, just like a franchise,” Thereallo wrote in response to questions.
“A centralized platform that can serve over 1,200 domains with a shared user base, IP tracking, and a custom API is not at all a trivial thing to build,” Thereallo said. “It’s a scalable system designed to be a resilient foundation for thousands of disposable scam sites.”
The security firm Silent Push has compiled a list of the latest domains associated with the Gambler Panel, available here (.csv).
											]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Six months into tariffs, businesses have no idea how to price anything]]></title>
            <link>https://www.wsj.com/business/retail/trump-tariff-business-price-impact-37b630c8</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077937</guid>
        </item>
        <item>
            <title><![CDATA[Why did books start being divided into chapters? A new history]]></title>
            <link>https://sydneyreviewofbooks.com/reviews/just-a-little-longer</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077735</guid>
            <description><![CDATA[Why did books start being divided into chapters? Joshua Barnes reviews Nicholas Dames’ history of literary segmentation, a study that slices through and pauses over what chapters have always told us about the times we live in.]]></description>
            <content:encoded><![CDATA[Why did books start being divided into chapters? Joshua Barnes reviews Nicholas Dames’ history of literary segmentation, a study that slices through and pauses over what chapters have always told us about the times we live in.I often return to an essay by Lydia Davis about an unusual experiment in translation. Better known for her work on French writers like Gustave Flaubert, Marcel Proust, and Maurice Blanchot, Davis had in this case tried to translate a literary text, not from French but rather from English into English. The text in question was Laurence Sterne’s unfinished 1768 novel, A Sentimental Journey through France and Italy by Mr. Yorick. Even a glance at its first page suggests why the book might require translation. Here is how it begins:  ——They order, said I, this matter better in France—          —You have been in France? said my gentleman, turning quick upon me with the most civil triumph in the world.—Strange! quoth I, debating the matter with myself, That one and twenty miles sailing, for ’tis absolutely no further from Dover to Calais, should give a man these rights—I’ll look into them: so giving up the argument—I went straight to my lodgings, put up half a dozen shirts and a black pair of silk breeches—‘the coat I have on, said I, looking at the sleeve, will do’—took a place in the Dover stage[.] Unusual English, to say the least. Davis sought to do two things: to modernise the novel by translating its sui generis language into contemporary English; and to figure out, in the process, what exactly makes it so unusual in the first place. Guiding her translation, however, is a deeper question: Why is it that visual art from ‘the eighteenth century and further back, to the beginning of discovered painting, is readily available, at least in reproduction, and enjoyed by the general public, not just scholars or specialists’ – but not literature ‘before, say, Jane Austen’? Literature from before 1800, Davis notes, is ‘mostly unread, even by writers’, and while many English speakers will learn foreign languages, they do not also try to ‘cross the barrier to James Boswell’s English, or John Donne’s, or further back to Chaucer’s or Beowulf’s’. The results of this experiment are perhaps less significant than the theoretical speculation it occasions. Translating across the gulf of historical difference – what we could call ‘temporal translation’ – might actually be difficult because ‘the barrier is something other than the language’: ‘maybe it is the sensibility or the worldview that changes too much, as we travel back in time, for us to understand it, or, if we understand it, to feel any sympathy for it’. Sterne’s novel occupies the unusual and contradictory position of being at once proto-modern (or proto-modernist) and somehow, by this very stylistic prolepsis, archaic or antiquated. It seems to shatter our so-called modern conventions before they were even created.A Sentimental Journey appears to begin in the middle of a conversation, but it is not exactly clear who is speaking or to whom they speak: dialogue is not clearly set out in quotation marks; dashes of different lengths are used expressively; and, finally, it is not organised according to legible chapters. Flipping through the first fifteen or twenty pages one sees instead repeated chapter titles: ‘Calais’; ‘The Monk | Calais’; ‘The Monk | Calais’ (again); ‘The Monk | Calais’ (once more); ‘The Desobligeant | Calais’; finally and somewhat belatedly, a ‘Preface | In the Desobligeant’; then, three pages later, ‘Calais’ (again). Such irregular chaptering produces an irregular experience of time, hence the comedy of the moment when Sterne’s hero, Yorick, is found rocking his horse-drawn carriage by the ‘agitation of writing a preface’ – a few chapters in.Sterne’s indifference to normal chaptering only throws into relief the ordinary and invisible work that chapters do as literary infrastructure. As is so often the case, one only notices a convention when it’s violated. But it is equally true of conventions that they are made; they come from somewhere. Why is it that novels have chapters at all? This is the inquiry of The Chapter: A Segmented History from Antiquity to the Twenty-First Century by Nicholas Dames, a professor of English at Columbia University who specialises in Victorian literature and culture. A book on chapters! I know. Stay with me. For this apparently technical question transforms into a historical phenomenology of literary time. In this sense The Chapter continues the inquiry Dames has been carrying out for the past quarter century in his scholarly work – the exploration of what he called, in Amnesiac Selves (2000), the ‘linguistic organization of temporal experience’, borrowing the phrase from the great German historian Reinhart Koselleck. In The Chapter, however, the scope has been radically widened, in part – one suspects – to make sense of the novel’s present fortunes. Dames declared in his previous book, The Physiology of the Novel (2007), the necessity of developing ‘nuanced and even-handed accounts of what I might call the social norms of cognition of given historical moments’, norms that are reproduced in large measure by the norms of writing. Enter the chapter. One of the basic structures of the book, the chapter is a ‘box of time’ that shapes the reader’s experience of temporality. As such, changes in chaptering present one way of exploring changes in the experience of time in literary history. How did time feel in late antiquity, or in fifteenth-century Burgundy, or to a former slave at the end of the eighteenth century? Studying the chapter might also tell us something about our experience of time now, in ‘the present’ – whatever that is – and the historical distance between our time and that of times past. Sterne comes up a lot in The Chapter, partly because his experiments in self-consciousness draw attention to the chapter’s conventionality, if only in the breach. In Sterne’s better-known novel The Life and Opinions of Tristram Shandy, Gentleman (1759-67), for instance, you might catch yourself in volume four thinking you had missed something as chapter twenty-three gives way to chapter twenty-five:  —No doubt, Sir,—there is a whole chapter wanting here—and a chasm of ten pages made in the book by it—but the book-binder is neither a fool, or a knave, or a puppy—nor is the book a jot more imperfect, (at least upon that score)—but, on the contrary, the book is more perfect and complete by wanting the chapter, than having it. If the book truly is ‘more perfect and complete by wanting the chapter’, that is because Tristram Shandy is a book about failure and errancy, where experiments with form and time are manifold. Notoriously it begins with Tristram’s attempt to narrate his life, but he prevaricates so long that he fails to get to any of the key points of his personal history. The preface again arrives late, in volume two; his birth only occurs in volume four; Tristram’s very name is an error, the intended birth name being Trismegistus. And what better expression of errancy than a gap in the novelistic edifice itself? Sterne also breaks off chapter nine of volume four, as Tristram’s father walks down a flight of stairs, and asks: ‘Is it not a shame to make two chapters out of what passed in going down one pair of stairs?’ There begins a ‘chapter upon chapters’, which Sterne calls ‘the best chapter in my whole work; and take my word, whoever reads it, is as well employed as in picking straws’. Picking straws: the very image of contingency. But with Dames’ theory of the chapter in mind, all this meddling with chapterisation, or capitulation (from the Latin capitulum, meaning ‘little head’), is not simply literary estrangement or satire. Rather it reflects, arguably even theorises, the function of the chapter – the presentation of time as an experience of unified discontinuity.Dames illuminates Sterne’s eighteenth-century moment as one in which the novel chapter has lost ‘much of its original function without as yet having acquired a new one’, which makes it properly experimental. But the place of Sterne’s experiments in the history of the chapter belies the fact that Dames’ history is really an attempt to describe the genesis and function of a convention in its very conventionality – not the exceptions, but rather the rule. His attention is directed instead towards the ‘usual chapter and its almost unthinking repetitions of technique’. This is a more ambitious task than it might seem. A chapter is a ubiquitous part of novelistic architecture – so easily overlooked, as Dames notes, that it is difficult even to conceptualise as an object of inquiry. And although the chapter finds its most distinctive uses in the novel, which has the ‘unique ability to […] articulate how the experience of time is the experience of time’s segmentations’, it does not originate there.If the enormous scope of this book invites comparisons with Erich Auerbach, then so too does its method, which similarly offers densely suggestive examples rather than an exhaustive historical inventory. Unlike Auerbach, however, Dames’ organisation of his material tends towards the taxonomic and schematising. He offers eight views of the chapter performing different functions at different historical moments: there is the ‘threshold’ of the classical heading, the ‘abstract syncopation’ of the Gospels, the ‘cut’ and ‘fade’ of medieval prose narratives – and so on, down to the ‘post chapter’ present. This is a longstanding ‘taxonomical urge’, as Dames termed it at the beginning of Amnesiac Selves, a habit that he picked up from his objects of study. Victorian theories of mind such as phrenology, for all their notorious problems, nonetheless ‘provided […] a useful interpretive model’ – in permitting its division ‘into distinct parts’, they rendered a newly ‘spatialized’ and ‘diagrammatic’ mind that was more susceptible of analysis. But in The Chapter there is perhaps a tension between the comprehensive ambitions of this ‘taxonomic urge’, and the suggestive but partial moments of Auerbachian literary history. You could say that the book makes a methodological wager that the nearly scientific goal of taxonomy – to encompass everything – can effectively be grafted onto an historicist hermeneutics constantly shifting its focus from part to whole and back again.We begin in the second century BCE, with a tablet, known today as the Tabula Bembina, upon which are inscribed some Roman anti-corruption laws from the time of the Gracchi. This was a ‘public, technical matter, by no means literary’, but for Dames it captures the chapter’s characteristic early function – as a technology not of narrative, but of reference – that would eventually be imported into the codex. On the tablet, ablative Latin phrases designate the topics covered in the relevant sections (de nomine deferundo iduibusque legundeis or ‘concerning prosecution and the choosing of juries,’ for instance). At once ‘visual and analytic’, these create a sort of resting place for the eye, and they organise the information presented into a logical and navigable form.But we are still very much in the realm of the heading; the tablet is a source of information. Jumping forward three centuries to the second century CE and to the work of the grammarian Aulus Gellius, Dames observes a new breadth in the headings of Gellius’s miscellany, Attic Nights, ranging from brief summary to something more authorial than a legal finding-aid: ‘How Publius Nigidius with great cleverness showed that words are not arbitrary, but natural.’ Yet a text like Attic Nights is still only something to be consulted partially, and on occasion, rather than read and absorbed line-by-line: ‘The text is not an experience’, but rather a ‘storage place from which information is extracted; the condensed summary is not only possible, but desirable’. However, one can begin to see the line of transmission; those ablative phrases of the Tabula Bembina are a precursor to the summative chapter headings of a novel like Charlotte Lennox’s Female Quixote (1752): ‘In which will be found one of the former Mistakes pursued, and another cleared up, to the great Satisfaction of Two Persons; among whom, the Reader, we expect, will make a Third.’ And so, from the tablet, the chapter begins the migration it will be Dames’ project to track: out of its originary informational context and, slowly but surely, into the temporality of the novel.Before the migration is complete, though, we have a centuries-long period of terminological and conceptual confusion as various terms, referring to both the textual unit and its title, are used: capitulum, kephalaia (‘head’), titlos (‘title’), argumentum and breviculus (summaries used to aid the inspection of a text) – these terms are all tangled together. One of the most inspired interpretations of this conceptual history is Dames’ rereading of the Confessions by the fourth-century theologian Augustine. Amid a spiritual crisis, Augustine overhears some nearby children crying ‘take it and read’, and, turning to a random section of the Bible, resolves to read ‘the first chapter [capitulum] I might find’. This is pivotal in converting Augustine to Christianity, for his eyes fall upon what a modern reader of the Bible might know as Romans 13:13-14, a caution against revelry that urges one away from ‘rioting and drunkenness’, exhorting instead that its reader ‘put ye on the Lord Jesus Christ, and make not provision for the flesh, to fulfil the lusts thereof’. Or at least, that is how it reads in the Oxford edition of the King James Bible I have just taken down from my shelf (slightly hungover, I confess, and thus moved and gently interpellated by its message), navigating with relative ease to Romans 13. But this was not Augustine’s experience, for his Bible had no chapters; his capitulum refers to the general ‘head’ or topic of the passage. The meaningful unit discloses itself ‘out of an unmarked stream’. The organisation of the Bible into ‘chapter and verse’ dates from well after Augustine’s time – chapters in the thirteenth century, and verses in the sixteenth – and though this format has to a large degree been naturalised by convention, it was not for this reason free of controversy. Early modern intellectuals like Robert Boyle and John Locke would even rail against Biblical chaptering: Boyle complained of its ‘inconvenient Distinction’, which ‘hath sometimes Sever’d Matters that should have been left United’; Locke for his part despaired that the system of chapter-and-verse left scripture ‘so chop’d and minc’d […] so broken and divided’ that not only do the ‘Common People take the Verses usually for distinct aphorisms’, but even the educated have their powers of memory enfeebled. Yet not even the complaints of Boyle and Locke could overturn the chapter’s ‘embeddedness in biblical textual tradition specifically and literate culture generally’. (To this ‘antichapter’ tradition we might add Donald Trump, who, when asked during an interview in 2015 to name a favourite bit of scripture, replied: ‘The Bible means a lot to me, but I don’t want to get into specifics.’)Christian scripture is a key site of this transformation of the classical heading, indexing discrete topics in a text, into something whose purpose is story-driven and temporal. The Gospels are, after all, narratives, demanding ‘a new method’ for their organisation. Surveying six competing divisions of the Gospels across ten centuries, Dames describes a project of ‘containerization’ in which the chapter becomes capable of holding a wider variety of topics without being ‘tailored’ to the shape of its content. But it is in the late twelfth century that the ‘modern’ system of Biblical chaptering is inaugurated. Usually associated with the medieval Paris Bible, the origins of this chapter system have long been tied to the work of the English theologian Stephen Langton (c. 1150-1228), who sought more accurate methods of citation for the university classroom. Despite the fact that not all that much evidence binds Langton to the creation of this chaptering technique, this historical account has long been the dominant one: ‘a creative and practical-minded English churchman, steeped in the chaotic environment of a cosmopolitan academy, takes on the chaptering of the Bible’ to improve his pedagogy. This story, however, has been challenged by the discovery of the earlier Saint Albans Bible (1180), named for the Hertfordshire abbey where it was produced, which contains Hebrew calligraphy and thus suggests the possibility not only of a Jewish scribe, but also perhaps an immersion in medieval rabbinical practices. In this view, the objective of the chapter was not scholarly and citational but monastic and oriented towards ‘communal reading tied to a ritualized calendar’. Whether first conceived for the ‘classroom or the chapel’, what Dames calls – in the spirit of historiographic compromise – the ‘Langton-Saint Albans model’ of chaptering affords a new experience: the ‘private continuous reading of narrative texts,’ the glorious fact of silent reading.Of course, a whole host of other transformations were needed to make such reading possible: the scroll is first divided into the codex; ancient continuous script is split into discrete words, which are themselves separated uniformly into paragraphs only in the early modern period. Transformations like these are usually treated by book historians as a ‘Babel allegory’, as Dames put it in The Physiology of the Novel, where the historical development of the book as a technology is told as the story of its fragmentation into smaller and smaller parts (which is often a narrative of progress, too: smaller units make reading more accessible and democratic). The Chapter takes this story of fragmentation one step further: part of Dames’ interest is motivated by the chapter’s final dematerialisation and its lingering power as metaphor. The chapter has ‘become a metalanguage’ that describes the different rhythms of social life, from clock time to the lived cadences of the body. One speaks of a new chapter in one’s life – not a new paragraph or a new sentence or, indeed, a new clause. But: ‘If it still works for us this way,’ Dames asks, ‘for how much longer?’ Here, we might be prompted to ask: who in fact is left in this us? Viewed in the less generous glare of media history, and from the perspective of a present less and less oriented towards reading of any kind, the answer is doubtful. If the members of an increasingly postliterate society still measure out their lives in chapters, this may only be a matter of mere habit or convention – in the way that a car’s engine capacity continues to be measured in horsepower.Perhaps it is the inevitable fate of any convention, but literary history does not, it turns out, have many examples of people appreciating great chaptering. In The History of English Prose Rhythm (1912) – one of the sources for James Joyce’s virtuosic-or-unreadable parodies of the evolution of English prose in Ulysses – George Saintsbury remarks on Thomas Malory’s decision to insert a chapter break at a decisive moment in his fifteenth-century Morte d’Arthur. At the end of chapter ten of the Morte, Lancelot rides into a castle, having slayed its gatekeeper, only to hear from the castle’s residents ‘in doors and windows that said “Fair Knight: thou art unhappy.”’ Saintsbury praises Malory’s sense of timing here. The chapter break introduces a pause, leaving those words, as Dames puts it, ‘hovering in the air’. The next chapter begins with Lancelot successfully freeing captives from the prison; as such, the chapter has served to elongate the narrative incident and heighten the tension.The only problem is that this was not Malory’s division, but rather one added by the printer William Caxton (c.1422-92). This fact was only discovered in 1934 when an edition of the Morte predating Caxton was discovered at Winchester College. As it turns out, the Winchester version had no chapters. The modulations of time are the work of Caxton’s specific ‘remediation’. He creates an ‘artful segmentation, a resonant silence, in the printed volume’s visual patterning’. Caxton is paired in this chapter of The Chapter with the anonymous fifteenth-century remediators who transformed Chrétien de Troyes’s great twelfth-century Arthurian verse into prose. Unlike Caxton’s their results are not acclaimed; like the authors of movie novelisations today, they are vulgarisers, profaning the sacred bonds between form and content. In their hands, Chrétien’s flowing verse – praised in Mimesis by Auerbach as ‘light and almost easy’ – is not only segmented with red ink, but also crowded with insistent explanations in the register of narrative history (‘How the king kissed Enide’). Again, in the manner of movie novelisers, moments of introspection are reduced while battle sequences are dilated with a vigour that may equally be judged ‘clumsy technique’ or ‘daring maneuver’. More charitably, we might say these remediators practise what Dames calls, after Roman Jakobson, ‘intralingual translation’ – a phrase that calls back to mind (there it is again!) Davis’ experiment with Sterne. Like Davis, the remediators are working across an historical gap between time-feelings, transforming the internal temporality of Chrétien’s verse to fit their own prosaic times. Dames speculates on the reasons for this transformation. Could it be that the new and uncertain ruling clique in Burgundy – ‘freshly arrived at what would be its historical apex’ – preferred these ‘modes of intense now-time’ to the subtle continuities of Chrétien’s verse? Admitting the possibility of such an ‘ideological effect’, Dames also notes that it is equally likely that these ‘new temporalities’ were simply an ‘accident’. Here one notices a difference between Dames’ previous books and The Chapter, whose broader subject matter perhaps helped it to become a finalist for the National Book Critics Circle Award. As brilliant works of literary history, Amnesiac Selves and The Physiology of the Novel both have the density of specialist knowledge and the sensitivity of immersive textual studies. Each book reconstructs a forgotten discourse: the first book reassembles the understanding of memory in the Victorian period, as explored through close readings of key Victorian novelists and scientific writers; the second builds on this interest by turning to the forgotten paradigm of ‘physiological’ novel theory and its exemplars, the philosopher-scientist-critics GH Lewes, ES Dallas, and Alexander Bain, who explored the embodied rhythms of reading. The physiological basis for a literary theory of form was ultimately swept away by more abstract formalisms espoused, on the one hand, by Henry James and his acolyte Percy Lubbock, and, on the other, by the practico-critical poetics of IA Richards (who effectively banished the novel from the classroom). Though Dames’ close readings in The Chapter are no less attentive and sinuous than in these earlier books, they are perforce more limited by the widened scope. I don’t intend to downplay the brilliance of Dames as a reader of individual texts or as a literary historian. However, as the study twists and turns, the density of historical detail together with the vast scope can at times induce a kind of mental torsion, with the dual impulses to historicise and taxonomise pulling in different directions.In any case, the taxonomic conclusion Dames draws from the Burgundian remediators of Chrétien is that while their clumsy cuts are just that – cuts in a continuous weave – Caxton’s interventions are more like the ‘fade’, offering ‘aeration’ to the narrative text. In this respect Caxton’s edits are oriented not towards reference, but ‘narrative progression and rhythm’. The paradoxical outcome of this intervention is to unify Malory’s text precisely by dividing it; the Morte now comprises ‘semi-discrete moments in a single process, rather than entirely different moments’. Unification-through-division of this sort highlights two logics of narrative time: discontinuous and immersive reading. Chaptering itself comes to generate a ‘feeling of presentness’ by adding white space, a species of visual fermata between narrative actions – ‘emptiness [with] a temporal intensity.’In their evocation of ‘presentness’, blank intensities of this kind recall a much longer-running theological dispute – between Augustine and the great English theologian Bede – on the divisibility of time. Where, after all, is the present? For Augustine, it is impossible to isolate something like ‘presentness’, for it is composed – as he put it in the Confessions – of ‘fugitive moments’, suspended in the future or always being sucked away into the past. The present is thus not measurable by a distinctive unit. Bede, in his eighth-century work The Reckoning of Time, argued to the contrary that there is a ‘minimal’ or ‘atomic’ unit of time. He made his case through a thought experiment. Say you are just about to be punched in the face. As a reflex, you flinch and close your eyes. Between these two moments – that ‘tiniest interval of time in which the lids of our eyes move when a blow is launched’ – is where ‘Bede’s present’ may be found: the atomic unit of presentness. Dames’ point is not that this theological argument directly influenced Caxton and the Burgundian prosateurs, but rather that the disagreement between the two great theologians reflects different investments in literary forms and their relationships to subjectivity in time. For Augustine a poem ‘held entire’ in the mind of a reciter approximates divine omniscience; for Bede, meanwhile, the atomic present is best accessed via a ‘punctuated continuity and directionality’ that might just be the hallmark of well-divided prose – consequently it is ‘seriality, not the transcendence of seriality, [that] is our access to the divine’. It is only in interrupting the present that we are able to perceive it.But it is left to the early novel (as an historian of the form, Dames is candid about this bias) to develop fully the space between Augustine’s durationless void and Bede’s serial present. Leaping forward another two hundred-odd years, then, Dames shows this binary of discontinuous and immersive reading exploding into an array of conceptual possibilities. ‘The eighteenth-century synthesis’, as Dames calls it, spans the period from the picaresque to the first flourishing of the English novel in the middle of the eighteenth century, with the antics of Sterne and Henry Fielding. Functions inherited from older reference-based chapters are here experimentally set in tension with the narrative innovations first explored in the fifteenth-century remediations: the eighteenth-century chapter struggles with the relationship between the strange and the commonplace, the ‘striking and singular’ and the ‘categorizable’. Hence the initial distinction between discontinuous and immersive reading turns out to contain other oppositions that structure it in turn: between space and time; and between the time narrated and the time it takes to narrate or read.Figuring all this is that moment on the staircase from the middle of Tristram Shandy, a kind of novelistic freezeframe, in which Sterne fixes Walter Shandy in place to reflect upon chaptering. In Dames’ account, this metachapter makes explicit the chapter’s full conceptual field: it has a direct address; it narrates both an incident and an interruption. What stands out as the real ‘heart’ of the metachapter is the staircase itself, which serves as a kind of symbolic definition of the chapter’s function. The staircase ‘captures the chapter’s double chronometry, that tension expressed by the simultaneous binaries of space versus time and narrated versus narrating times’. Fielding famously compared his chapters to inns along the road of a long journey, where the reader may ‘stop and take a glass’, but Dames thinks the staircase a better figure. Fielding’s coach trip is merely ‘linear, starting and stopping’; Sterne’s staircase, on the other hand, ‘unpacks two complementary but opposed dimensions’. Walter and Toby head down the stairs, troping narrative progress, while at the same time the sequence of steps and landings displays the segmentation of linearity ‘into discrete stages’. Sterne’s novel is a kind of ‘funhouse mirror’ of temporality: instead of proceeding steadily along a horizontal axis, our temporal schema is thrown down the stairs.Later, in what JGA Pocock once called the ‘second eighteenth century’, the so-called Age of Revolutions, the chapter mutates again. Now ‘elongated’, the chapter is studied in two works that each seem in different ways to dissolve its earlier functions. In The Interesting Narrative of Olaudah Equiano (1789), the famous autobiography of a Nigerian slave who eventually regained his freedom and lived in Britain, Dames observes a mismatch between the protocols of chaptering and the life that these protocols divide up. Equiano’s chapters offer extensive summaries in the manner of a picaresque novel, but seem at the same time to show the inefficacy of that paratextual structure for capturing the experience of domination and eventual manumission. ‘How then to describe the chapter in Equiano, or more bluntly, why bother to do so?’ It is perhaps relevant precisely because the apparent orderliness of chaptering – its ability meaningfully to sculpt time – is shown, against the absolute alienation of slavery, to be unfit for its usual purpose of segmentation. Thus, the intensively expository chapter summaries of the Narrative not only fail to coordinate with the abbreviated summaries in the table of contents, but they also introduce chapters of far greater length (on average, Dames tells us, these are 6,500 words: up to four times longer than is typical for this period). So, then, what is the meaning of this technical decision? ‘To say,’ Dames writes, coming perilously close to ventriloquising Equiano, ‘a life cannot be measured this way, not this kind of life.’As the self-testimony of a former slave, published in the same year as the storming of the Bastille, Equiano’s Narrative is certainly a sign of the times. It is perhaps as iconic a testament to the ‘new epoch’ of the nineteenth century as Girodet’s portrait of Jean-Baptiste Belley, a former slave from Saint-Domingue who would eventually be elected to the French National Convention. ‘New epoch’: this is the legendary, and perhaps apocryphal, phrase of Goethe, uttered in response to the defeat of the Prussians at Valmy in 1792. ‘From this place and from this day a new epoch in world history begins and you can say you were there to see it.’ We might observe that he, for one, did not reach here for the metaphor of the chapter – too ‘partial, fleeting, unhistorical’, according to Dames, to register this period’s epochal shifts. In Goethe’s Wilhem Meister’s Apprenticeship (1795-96), for instance, the chapter becomes even more elongated (one of them is 20,000 words!), doubling in size in the novel’s second half, which was composed after Valmy. Wilhem Meister’s Apprenticeship is a ‘triple turning point’, tying together ‘a world-historical transition, a maturational transition’, and a ‘career transition’ as Goethe, now older and on the other side of the revolution, has to produce fresh material rather than merely revising old writing. It is the very incongruity and ‘dilation’ of the chapter that ‘itself is historical’. Jane Austen’s career is also adduced as an example of the eighteenth century’s passing into the nineteenth, with the three youthful novels drafted in the 1790s averaging chapter lengths of around 2,000 words, while the ‘mature’ novels of the 1810s are nearer 3,500.However sceptical we might like to be about periodisation, and nasty but inevitable grand narratives, it’s observable that history has, well, happened; historical experience makes ‘norms’ normal, and it is potentially why – to return to Davis’ question with which I began – more people still read Austen for pleasure than Smollett, Fielding, Defoe, or, um, John Bunyan. Not unrelatedly, I recently invited some students to read paragraphs from the fourteenth, fifteenth, sixteenth, seventeenth, and eighteenth centuries (respectively, Margery Kempe, Edmund Spenser, Margaret Cavendish, Eliza Haywood, and Sterne: I welcome criticisms of my selections) and one of them said, in so many words, ‘Perhaps some things are forgotten for a reason.’ Perhaps. But we might also wonder: to what extent do novels instruct their readers in how to think, feel, and act? This has been one of the questions that Dames has posed most insistently across his career, with a special emphasis on the contributions of the Victorian novel to readerly subjectivity. At the end of Amnesiac Selves, he speculates on the way that Victorian fiction inculcates a special kind of nostalgia – its warm selective memory is the flipside of the alienating nausea of the historical difference that makes you want to throw a book out the window (or, in homage to Sterne, down the stairs). Yet, as Dames noted then, the cultural prestige of Victorian fiction is ‘increasingly seen in an elegiac manner, as a strange fact that, as the twenty-first century begins, will not last much longer’. As the nineteenth century disappears further and further from view, ‘the Victorians will eventually, if belatedly, make Victorian fiction stranger and less attractive’. Since the publication of Amnesiac Selves in 2000, the Victorians have only receded further away from us in time.It was the Victorian novel that made the chapter seem natural. Key to the reality effects of nineteenth-century British fiction is its synchronisation of novel time with the natural rhythms of life. As a result, novelistic chapters lose their theatrics, their posturing and posing, even those unstable amalgamations surveyed in Equiano and Goethe, and instead become regular and ‘tacit’, receding into the background. It is this very tacitness that secures the permanence of the chapter as a blank, unmarked, and ordinary vehicle for reflection. Surveying Tolstoy’s War and Peace (1867), Dames distils the repertoire of the chapter into another taxonomy of five key functions: the signal or incantation; the crossing of a threshold; the ‘suspended revelation’; the ‘tense use’, which adjusts the temporal frame of narration; and the modulation of point of view. Together with Elizabeth Gaskell – whose Wives and Daughters (1864-66) is shown virtuosically to assemble all five of these ‘tacit’ operations, in a careful and naturalistic counterpart to the brazen theatrics of Sterne – the chaptering of Tolstoy presents a study in indistinctness.Perhaps the most ‘natural’ scheme for novelistic time is that of the day itself, which is what Dames shows to be at work in Charles Dickens and George Eliot, calling this the ‘suturing of story world and reader […] an alignment of times, a synchronization of light’. Epic heroes lived in a time supercharged with meaning – the time of kairos, or propitious instant of action, not the dun-coloured chronos, the everyday time of housework, care work, and all the other kinds of work. In contrast with epic, then, the diurnal frame of novelistic realism appears definitively chronological and quotidian – but it is, of course, a complex literary artefact, one that Dames explores using some old-fashioned counting. There are 146 narrated days in Middlemarch, though the novel covers some 1,000 days. That means around 15% of the total ‘days of our lives’ are narrated; of these, only 18 ‘peak days’ are extended over two chapters. If, in the time of Goethe and Equiano, the coordinates of day, chapter, and epoch fell helplessly out of joint – the chapter form desynchronised from life by historical forms of dislocation – it is the innovation of Eliot’s realism to realign life with text: ‘Neither wholly impersonal and public like the “day” nor intimately personal like the epoch, chapter time is, perhaps, something like an image of weak collective time’. Weakness is an important term, capturing the chapter’s ignorable yet undeniable presence – just like time itself – which is nonetheless experienced collectively. It also calls to my mind Walter Benjamin’s famous evocation of the ‘weak messianic power’, the spark of redemption glowing however faintly in the present. There is something of that melancholia in this history too. As Dames wonders, ‘When you share time, what is it you share?’ A book? A memory? A moment? Or perhaps one shares nothing, for the whole point of fiction is that it is invented, nonactual, negative. The reader of a novel, as Benjamin put it, ‘is isolated, more so than any other reader’. Reading of fictional lives becomes a way of experiencing death before it happens to you: the characters in a novel make its reader ‘understand that death is already waiting for them’. In novelistic time, therefore, one feels in the fate of fictional beings the ‘warmth which we never draw from our own fate’. If chapters become, by the twentieth century, simply ‘embarrassing’, subject to two equal but opposing modernist processes – autonomisation (à la Joyce’s almost freestanding stylistic excursions) or decimation (as in Samuel Beckett where it is obviated entirely) – these new formal strategies do not get around the fundamental matter of finitude that is immanent to the chapter as a vehicle of time. A key transitional figure here is the Brazilian novelist Joachim Maria Machado de Assis, whose experimental fiction of the late nineteenth century seems to repudiate the tacit chaptering of realism in favour of something more akin to Sterne. Yet in Machado’s hands, the ‘Shandean chapter’ is no longer free-wheeling and free-associative, but decisively bound: if Tristram struggled to bring forth the story of his birth, the eponymous narrator of The Posthumous Memoirs of Brás Cubas (1881) is already dead. Speaking from beyond the grave in radically attenuated chapters, Brás Cubas adds a new note of disillusionment and pessimism that Dames calls ‘antique-diminutive’.The diminution – decline? – of the chapter continues in twentieth-century avant-garde fiction and film: The Unfortunates (1969), by the British novelist BS Johnson, is not bound in a codex but rather packaged up as so many loose sheets in a box, becoming as a result a literal ‘box of time’. Consider, too, the ‘antique’ and self-conscious quality of the onscreen chapters in Agnès Varda’s film Cléo from 5 to 7 (1962), tracking its protagonist minute by minute as she awaits the results of a cancer test. For all their apparently lively experimentalism, all three of these cases finally return to the negativity that attends the ‘linguistic organization of temporality’. Machado’s novel is narrated by a dead man; Johnson’s book in a box is about a dead man; Varda’s Cléo receives omens of death: these are texts ‘by, for and about the dead or dying’. Dames refers to these as expressing the ‘poignancy of sequence’, a term that names ‘the sensation of an end indefinitely, but only temporarily, held off’. This finally is ‘a melancholy purpose: to keep something going – a life, a form, a moment – just a little longer’. The chapter, then, not as inns on a journey, but halting steps towards the end.Right at the beginning of the book, Dames recalls the remark of a ‘gifted analyst’ some years ago: ‘You’re starting a new chapter.’ Dames writes that this comment made him feel – quoting the psychoanalyst Donald Winnicott – ‘held’. It also spurred the research and writing of this book. Although his analyst’s offhand ‘novelization’ of his life seemed perfectly to capture the subjective experience of temporal passage, Dames could not explain why it had this comforting effect on him. Despite being a ‘novel reader,’ he ‘had no idea why chapters existed – a historical question – nor what exactly they did to our sense of time, a theoretical question’. Guided by these questions, his journey backwards in time terminates in the continuous present with the novels of Uwe Johnson, Jennifer Egan and László Krasznahorkai. There is a chapter in Egan’s novel A Visit From the Goon Squad (2010) that is presented in the form of a PowerPoint presentation on ‘Great Rock and Roll Pauses’. This is a ‘chapter on chapters,’ Dames notes, in the manner of Sterne but relocated into a wholly changed technological environment. These slides – presented in the novel by a twelve-year-old girl named Alison about her family – represent for Dames an effort to ‘understand the feeling of time passing, a feeling that is shaped by media’. If the project of The Chapter has been to coordinate the feeling of time passing with the changing mediations of that feeling, then it is perhaps unsurprising that one of the prevailing feelings in this study in turn is its melancholia, its very nearly depressive turns towards the experience of temporal passage.It is also significant, I think, that a history concerned with the objective features of literary history has an important but just-visible subjective dimension – significant, that is, that the book began on the couch in analysis. I was struck, reading The Chapter, by its minimal but insistent evocations of finitude. This is a history of the novel that is partly a history of its death. In this respect it complements Dames’ other books that have told this story from a different angle, as when, in The Physiology of the Novel, he writes of George Gissing’s ‘depressive’ and ‘ambivalent’ relation to the novel form in an era of speed reading. ‘In many ways,’ Dames adds, ‘that depressive position has lingered for readers, writers, and critics of novels, to our own day.’ It has – and it perhaps accounts for the alternatingly depressive and wistful tenor of The Chapter, which reconstructs its object from the position of its catastrophic obsolescence. But Dames is not moralising about the decline of the novel or of the reading public. The conclusion of the study refuses any of what he has termed ‘the morality of attention’, remarking that even if the chapter is dispersed across ‘different media that weave in and out of the format of the book, [it] can express the disjuncture of time itself’ – the ‘disjuncture’, that is, between ‘our’ lives and any of the ‘rhythms – biological, cultural-economic, political, planetary – we live among but cannot manipulate’.Yet I think this argument must be evaluated in light of the earlier claim in The Physiology of the Novel that one of the vulnerabilities of Victorian physiological theory was its transformation of readers and texts into technologies: ‘The more its findings turned both novel and reader into machines, the less necessary (or, for that matter, interesting) its procedures seemed, and the more ancillary to other technologies the novel became – a melancholy conclusion that cut short some of the theoretical innovations that the theory had promised.’ If in The Physiology of the Novel Dames was ‘implicitly arguing for the viability of an updated, historically aware version’ of nineteenth-century physiological theories of reading, then The Chapter strikes me as a now-explicit attempt to realise such a theory. In it, Victorian novels stand as the apex of a kind of felt and intuitive ‘chronocommunity’ in which picking up a triple-decker was a reliable way to plug into the interface of temporality that everyone shared. That now is perhaps lost, and the historian’s effort at understanding the genesis of a technical object like the novelistic chapter could be seen as some small recompense – for the lost ‘weak collectivity’ of an earlier period, but maybe, too, for the fearful lack at the centre of all reading. It is difficult not to think of this in Dames’ closing evocation of the chapter’s dispersal across the mediascape. That is, the ‘technical’ question of The Chapter serves to absorb a more basic anxiety: not only about the demise of the novel, but rather about the emptiness at the heart of reading itself.I think here of Maurice Blanchot’s essay ‘Literature and the Right to Death’ (1949) – translated, as it happens, by Davis, albeit in a more traditional manner – in which Blanchot writes of finitude as the inescapable meaning of literature. Language kills, and literary language most of all: ‘Language can only begin with the void,’ he writes, ‘no fullness, no certainty can ever speak; something essential is lacking in anyone who expresses himself.’ Questions like ‘What is literature?’ have received ‘only meaningless answers’ insofar as they fail directly to confront this negativity. ‘People can and do ask, “What is poetry?” “What is art? And even “What is the novel?”’ – but for Blanchot it is in the ineradicable ‘emptiness present in all these serious things’ that the impossible centre of literature consists, that empty heart ‘to which reflection, with its own gravity, cannot direct itself without losing its seriousness’. Another way of saying this is that The Chapter seems to me most fully to grasp its subject not when it considers the historical or technical question of chaptering, but when it turns from this to the lived experience of time, in which the collapse of literature as such figures the collapse of everything else. Perhaps the function of ‘the chapter’ in The Chapter is not so much a ‘point of departure’ as Auerbach once imagined it – in his phrase, a ‘handle […] by which the subject can be seized’ – but rather a point of return: an obsession, an idée fixe, or, in a more Freudian vocabulary, a reaction formation to the anxiety attending time’s ceaseless passage. Viewed in this gloomy half-light, it is possible to see how the most moving parts of The Chapter are those rarer moments in the subjective register that look right into the void at the centre of literary experience. It is, after all, to escape this emptiness that we write in the first place, even as the act of doing so can only return us to it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Are we decentralized yet?]]></title>
            <link>https://arewedecentralizedyet.online/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077291</guid>
            <description><![CDATA[A site with statistics regarding the decentralization status of various web services]]></description>
            <content:encoded><![CDATA[
      How Concentrated Is User Data On The:
      
        Fediverse
        Mastodon, Pixelfed, etc.
        
        
            Servers
            Biggest(%)
            Rest(%)
        
      
      
        Atmosphere
        Bluesky, WhiteWind, etc.
        
        
            Servers
            Biggest(%)
            Rest(%)
        
      
      Data last updated: 
    
      
        This page measures the concentration of user data on the Fediverse and the Atmosphere according to the
        Herfindahl–Hirschman
          Index (HHI),
        an indicator from economics used to measure competition between firms in
        an industry.
        Mathematically, HHI is the sum of the squares of market shares of all servers.
      

      
        Values close to zero indicate perfectly competitive markets (eg. many servers, with users
        spread evenly), while values close to 10000 indicate highly concentrated monopolies (eg.
        most users on a single server). In economics, values below 100 are considered
        "Highly Competitive", below 1500 is "Unconcentrated", and above 2500 is
        considered "Highly Concentrated".
      

      
        This site currently measures the concentration of user data for active users: in the
        Fediverse, this data is on servers (also known as instances);
        in the Atmosphere, it is on the
        PDSes
        that host users' data repos.
        All PDSes run by the company Bluesky Social PBC are aggregated in this
        dataset, since they are under the control of a single entity. Similarly,
        mastodon.social and mastodon.online are combined as they are run by the
        same company.
      

      
        The location of user data is not the only interesting measure of 
        centralization. On a technical level, there is the network
        structure (peer to peer, relays, etc.), identity management, the
        infrastructure on which it is hosted, etc. On a legal level, there are
        issues regarding the jurisdictions where servers are located, companies
        are located, etc. On a social level, there are issues around where
        human power is concentrated in and on the platform, and whether that
        power is disproportionately held by certain groups. If you would like
        to help contribute other measures of decentralization, get in touch.
      

      
        Code and data are available on
          GitHub.
        Comments and pull requests, including other metrics for measuring
        distribution and resiliency, are welcome!
      

      
        By Rob Ricci: @ricci@discuss.systems / 
            @ricci.io 
      

          
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Rise of Hybrid PHP: Blending PHP with Go and Rust]]></title>
            <link>https://yekdeveloper.com/p/4-the-rise-of-hybrid-php</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45077143</guid>
            <description><![CDATA[We used to develop our application as a single DDD monolith (let’s call it the mother) with several smaller microservices around it (the children) to gain some specific advantages. Most of these microservices were built in Go, while the core monolithic service was developed in PHP 8.3.]]></description>
            <content:encoded><![CDATA[We used to develop our application as a single DDD monolith (let’s call it the mother) with several smaller microservices around it (the children) to gain some specific advantages. Most of these microservices were built in Go, while the core monolithic service was developed in PHP 8.3.This stack served us well for a long time. The Go microservices efficiently handled our high-throughput requests, and the carefully designed monolith allowed our relatively small backend team to deliver features quickly and with confidence. It was a good balance: speed where we needed it most, and stability and productivity everywhere else.As many of you may have experienced, 80% of your traffic often targets only 20% of your APIs—the well-known Pareto principle. And unsurprisingly, those hot 20% endpoints are usually the ones where performance matters the most. In the past, our strategies included writing highly optimized code, adding extreme caching layers, or extracting certain parts into Go-based microservices. While effective, these approaches added complexity and operational overhead.But now, thanks to new capabilities in the PHP ecosystem and the rise of powerful libraries and runtimes, it’s becoming much easier to keep more logic inside the monolith while still achieving excellent performance. Let’s look at a few exciting options:1. FFI (Foreign Function Interface)PHP’s FFI feature allows you to call C code directly from PHP. This opens the door to system-level operations or performance-critical logic without leaving your PHP project. Of course, you need to be mindful of context switching costs, but for the right use cases, it’s a game-changer.2. Rust-Based ExtensionsIf writing raw C isn’t your cup of tea, you can now write PHP extensions in Rust (or even Zig). This lets you offload heavy, performance-sensitive parts of your application to safe, memory-efficient, compiled code. Rust, in particular, offers memory safety guarantees without sacrificing speed, which makes it a great fit for extensions that need to be both reliable and fast.3. Go-Based Extensions with FrankenPHPWe’ve recently switched to FrankenPHP (after seeing it become officially supported by the PHP Foundation). Running PHP in FrankenPHP’s worker mode is impressively fast—sometimes over 4x faster in our benchmarks compared to traditional setups.Even more exciting, a recent release introduced the ability to write PHP extensions in Go. This feature is something we are actively exploring because it would let us build high-performance APIs in Go and expose them seamlessly inside our PHP monolith. That way, we can combine the productivity of PHP with the raw speed of Go, without needing to split everything into separate services.But Why Not Just Rewrite Everything in Go or Rust?It’s a fair question—and one we’ve asked ourselves too. There are two main reasons why we don’t simply migrate the entire backend:1. Rewriting is costly. Many applications are already large and stable. Rewrites are risky, time-consuming, and often introduce more problems than they solve. In most scenarios, a rewrite should be the very last option.2. PHP is still a great fit. For the majority of the application, PHP does the job well. It’s fast enough, developer-friendly, and supported by a large ecosystem. For those few cases where you truly need maximum performance, you can now selectively write parts in Go or Rust as extensions—rather than rewriting the entire system.In short, the modern PHP ecosystem gives us the best of both worlds: the ability to build quickly and confidently in PHP, while still having powerful options (C, Rust, Go) for performance-critical parts. This hybrid approach lets us stay productive without sacrificing speed where it matters most. ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The Default Trap: Why Anthropic's Data Policy Change Matters]]></title>
            <link>https://natesnewsletter.substack.com/p/the-default-trap-why-anthropics-data</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45076274</guid>
        </item>
        <item>
            <title><![CDATA[New research reveals longevity gains slowing, life expectancy of 100 unlikely]]></title>
            <link>https://lafollette.wisc.edu/news/new-research-reveals-longevity-gains-slowing-life-expectancy-of-100-unlikely/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075813</guid>
            <description><![CDATA[A new study co-authored by a University of Wisconsin-Madison professor finds that life expectancy gains made by high-income countries in the first half of the 20th century have slowed significantly, and that none of the …]]></description>
            <content:encoded><![CDATA[
	
		
			
			
			

	
		A new study co-authored by a University of Wisconsin-Madison professor finds that life expectancy gains made by high-income countries in the first half of the 20th century have slowed significantly, and that none of the generations born after 1939 will reach 100 years of age on average.
Published in the journal Proceedings of the National Academy of Sciences, the study by Héctor Pifarré i Arolas of the La Follette School of Public Affairs, José Andrade of the Max Planck Institute for Demographic Research, and Carlo Giovanni Camarda of the Institut national d’études démographiques analyzed life expectancy for 23 high-income and low-mortality countries using data from the Human Mortality Database and six different mortality forecasting methods.
Assistant Professor Héctor Pifarré i Arolas
“The unprecedented increase in life expectancy we achieved in the first half of the 20th century appears to be a phenomenon we are unlikely to achieve again in the foreseeable future,” according to Pifarré i Arolas. “In the absence of any major breakthroughs that significantly extend human life, life expectancy would still not match the rapid increases seen in the early 20th century even if adult survival improved twice as fast as we predict.”
From 1900 to 1938, life expectancy rose by about five and a half months with each new generation. The life expectancy for an individual born in a high-income country in 1900 was an average of 62 years. For someone born just 38 years later in similar conditions, life expectancy had jumped to 80 years on average.
For those born between 1939 and 2000, the increase slowed to roughly two and a half to three and a half months per generation, depending on the forecasting method. Mortality forecasting methods are statistical techniques that make informed predictions about future lifespans based on past and current mortality information. These models enabled the research team to estimate how life expectancy will develop under a variety of plausible future scenarios.
Doctoral student José Andrade of the Max Planck Institute for Demographic Research
“We forecast that those born in 1980 will not live to be 100 on average, and none of the cohorts in our study will reach this milestone. This decline is largely due to the fact that past surges in longevity were driven by remarkable improvements in survival at very young ages,” according to corresponding author Andrade.
At the beginning of the 20th century, infant mortality fell rapidly due to medical advances and other improvements in quality of life for high-income countries. This contributed significantly to the rapid increase in life expectancy. However, infant and child mortality are now so low that the forecasted improvements in mortality in older age groups will not be enough to sustain the previous pace of longevity gains.
While mortality forecasts can never be certain as the future may unfold in unexpected ways – by way of pandemics, new medical treatments or other unforeseen societal changes – this study provides critical insight for governments looking to anticipate the needs of their healthcare systems, pension planning and social policies.
Although a population-level analysis, this research also has implications for individuals, as life expectancy influences personal decisions about saving, retirement and long-term planning. If life expectancy increases more slowly as this study shows is likely, both governments and individuals may need to recalibrate their expectations for the future.
Share on: 	

	


	
		Post navigation
		
	
	

	


]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LandChad, a site dedicated to turning internet peasants into Internet Landlords]]></title>
            <link>https://landchad.net</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075384</guid>
            <description><![CDATA[This is LandChad.net, a site dedicated to turning internet peasants into Internet Landlords by showing them how to setup websites, email servers, chat servers and everything in between.
Starting a website is something that can be done in a lazy afternoon and costs pocket change.
Most of the internet’s problems could be solved if more people had their own personal platforms, so the objective of this site is to guide any normal person through the process of installing a website.]]></description>
            <content:encoded><![CDATA[
This is LandChad.net, a site dedicated to turning internet peasants into Internet Landlords by showing them how to setup websites, email servers, chat servers and everything in between.
Starting a website is something that can be done in a lazy afternoon and costs pocket change.
Most of the internet’s problems could be solved if more people had their own personal platforms, so the objective of this site is to guide any normal person through the process of installing a website.
Start a website

“Build your own platform!”


Host your own services, social media and more.
Setup an Email Server

Maintaining a Server
Tips and articles on mastering your server and learning about GNU/Linux systems administration.

Certbot on Standalone Domains and Subdomains[server]
Cronjobs[server]
GeminiA minimalist alternative to HTTP with a modern twist.[server]
Log on with SSH Keys[server]
Maintaining a Server[server]
OpenAlias[server]
Page Quality[server]
Requiring Passwords for Webpages (HTTP Authentication)[server]
Rsync: Upload and Sync Files and Websites[server]
Self hosting[server]
Server-Side Scripting with CGI[server]
SSH - Advanced Usage[server]
Using UFW as a Firewall[server]



Support LandChad.net

BTC: bc1q9f3tmkhnxj8gduytdktlcw8yrnx3g028nzzsc5
XMR: 84RXmrsE7ffCe1ADprxLMHRpmyhZuWYScDR4YghE8pFRFSyLtiZFYwD6EPijVzD3aZiEpg57MfHEr1pGJNPXyJgENMnWrSh

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[You Have to Feel It]]></title>
            <link>https://mitchellh.com/writing/feel-it</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45075048</guid>
            <description><![CDATA[You see a series of checkboxes checked. Schedules met.
Requirements satisfied. Demos delivered.
It's a good day. Good job, you, good job! A promotion is in sight.]]></description>
            <content:encoded><![CDATA[You see a series of checkboxes checked. Schedules met.
Requirements satisfied. Demos delivered.
It's a good day. Good job, you, good job! A promotion is in sight.
But you didn't feel it. You didn't feel it.
We, as people, feel something with every interaction. Frustration, joy, relief,
confidence. A feeling. A person interacts with our work. Our work evokes
a feeling. The feeling matters. The feeling is part of the work. The
desired feeling is part of the requirements.
When you feel it, you know. The feature makes you smile when you use it.
It fits right in, like it was always meant to be there. You want to
use it again. You want to tell people about it.
This is the difference. This is what metrics, specifications, and demos
miss. They don't capture the feeling. For the people who will use and live
in the work, the feeling is part of their daily experience. Which means
you can't stop at checking the boxes on paper. You have to sit with it,
use it, live with it.
You have to feel it.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Condor's Cuzco RISC-V Core at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074895</guid>
            <description><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive.]]></description>
            <content:encoded><![CDATA[Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache missesIf you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI models need a virtual machine]]></title>
            <link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074467</guid>
        </item>
        <item>
            <title><![CDATA[Bcachefs Goes to "Externally Maintained"]]></title>
            <link>https://lwn.net/Articles/1035736/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074312</guid>
            <description><![CDATA[Linus Torvalds has quietly changed the maintainer status of bcachefs to 'externally maintained' [...]]]></description>
            <content:encoded><![CDATA[
[Posted August 29, 2025 by corbet]
               

Linus Torvalds has quietly changed
the maintainer status of bcachefs to "externally maintained",
indicating that further changes are unlikely to enter the mainline anytime
soon.  This change also suggests, though, that the immediate removal of
bcachefs from the mainline kernel is not in the cards.
             

      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 17:30 UTC (Fri)
                               by intelfx (subscriber, #130118)
                              [Link] (38 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 17:42 UTC (Fri)
                               by zdzichu (subscriber, #17118)
                              [Link] (37 responses)
      
      
      
I guess another maintainer, easier to work with, can continue the development of bcachefs. Subsystems with bus factor of one are frowned upon.


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 19:21 UTC (Fri)
                               by NYKevin (subscriber, #129325)
                              [Link] (36 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 29, 2025 22:11 UTC (Fri)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (35 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 4:57 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [Link] (3 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 16:30 UTC (Sat)
                               by ttuttle (subscriber, #51118)
                              [Link] 
      
      
      
Thank you for posting such a compassionate response. This thread could easily turn into an unkind discussion or an all-out flame war, but you went out of your way to be kind instead.


      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 20:48 UTC (Sat)
                               by linuxrocks123 (subscriber, #34648)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 23:41 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] 
      
      
      

      
          
        
     



    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 7:57 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (27 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 11:44 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (12 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 14:25 UTC (Sat)
                               by ma4ris8 (subscriber, #170509)
                              [Link] (11 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:21 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (10 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:00 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (9 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:48 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (8 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:09 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (5 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:43 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (4 responses)
      
      
      
Debian has processes for obtaining carvouts/exceptions for critical system packages, naturally with more review. E2fsprogs used them and that's what should have been done here; there was no need to rush packing bcachefs-tools for Debian.


      
          
        
     
      Debian
       Posted Aug 31, 2025 1:29 UTC (Sun)
                               by comex (subscriber, #71521)
                              [Link] (3 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 1:49 UTC (Sun)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 2:11 UTC (Sun)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      Debian
       Posted Aug 31, 2025 3:40 UTC (Sun)
                               by jmalcolm (subscriber, #8876)
                              [Link] 
      
      
      

      
          
        
     






    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:10 UTC (Sat)
                               by lordsutch (guest, #53)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 23:42 UTC (Sat)
                               by pizza (subscriber, #46)
                              [Link] 
      
      
      

      
          
        
     







    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 12:24 UTC (Sat)
                               by muase (subscriber, #178466)
                              [Link] (11 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:27 UTC (Sat)
                               by paravoid (subscriber, #32869)
                              [Link] (10 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 19:20 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (9 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 21:17 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (8 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:29 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (7 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:35 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (4 responses)
      
      
      
You'll be waiting a long time if you have no desire to engage in a fashion other than "are you ready to agree I was right yet".


      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:41 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] 
      
      
      
And to be clear, 1) I am not "you Debian folks" here, and 2) I am not commenting on whether I agree or disagree with Debian's policies on bundling *because that's not the point here*.


      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:48 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 0:40 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:37 UTC (Sun)
                               by ben0x539 (guest, #119600)
                              [Link] 
      
      
      
Did just removing the package ever come up? Surely not shipping a piece of code is preferable for all parties over shipping a known broken configuration.


      
          
        
     




    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:39 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] 
      
      
      

      
          
        
     

    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 22:51 UTC (Sat)
                               by sheepdestroyer (guest, #54968)
                              [Link] 
      
      
      
Just to understand what is discussed here, was there ever any technical reason mentioned in public from Debian for why Kent's recommendation to not unbundle rust dependencies could and should not be followed?


      
          
        
     






    
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:34 UTC (Sun)
                               by jmalcolm (subscriber, #8876)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      So what exactly *is* in the cards, then?
       Posted Aug 31, 2025 3:41 UTC (Sun)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] 
      
      
      

      
          
        
     



    
      A few suggestions (which you don’t have to follow)
       Posted Aug 30, 2025 17:12 UTC (Sat)
                               by DemiMarie (subscriber, #164188)
                              [Link] (1 responses)
      
      
      

      
          
        
     
      A few suggestions (which you don’t have to follow)
       Posted Aug 30, 2025 22:24 UTC (Sat)
                               by koverstreet (✭ supporter ✭, #4296)
                              [Link] 
      
      
      

      
          
        
     


    
      So what exactly *is* in the cards, then?
       Posted Aug 30, 2025 18:53 UTC (Sat)
                               by ATLief (subscriber, #166135)
                              [Link] 
      
      
      

      
          
        
     





      A broken link?
       Posted Aug 29, 2025 17:38 UTC (Fri)
                               by ahippo (subscriber, #154692)
                              [Link] (5 responses)
      
      
      

      
          
        
     
      A broken link?
       Posted Aug 29, 2025 17:50 UTC (Fri)
                               by Poliorcetics (subscriber, #165001)
                              [Link] (3 responses)
      
      
      

      
          
        
     
      A broken link?
       Posted Aug 29, 2025 18:39 UTC (Fri)
                               by ewen (subscriber, #4772)
                              [Link] 
      
      
      

      
          
        
     

    
      A broken link?
       Posted Aug 29, 2025 19:03 UTC (Fri)
                               by ahippo (subscriber, #154692)
                              [Link] (1 responses)
      
      
      
Ah, yeah, that must be it!
Thank you for pointing me to that blog post!
My phone indeed has an odd number of cores.


      
          
        
     
      A broken link?
       Posted Aug 29, 2025 21:03 UTC (Fri)
                               by alspnost (guest, #2763)
                              [Link] 
      
      
      Fascinating - this is a whole new thing to me, but I guess I'm also "vulnerable", since I have a Pixel 8 Pro with 9 cores!


      
          
        
     



    
      A broken link?
       Posted Aug 30, 2025 13:17 UTC (Sat)
                               by Baughn (subscriber, #124425)
                              [Link] 
      
      
      

      
          
        
     


      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 29, 2025 17:39 UTC (Fri)
                               by JMB (guest, #74439)
                              [Link] (4 responses)
      
      
      

      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 29, 2025 17:47 UTC (Fri)
                               by zdzichu (subscriber, #17118)
                              [Link] (3 responses)
      
      
      
Typo? Looks like one bit flip: 'v' → 01110110; 'b' → 01100010. Linus is not using a filesystem with checksums‽


      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 30, 2025 0:28 UTC (Sat)
                               by josh (subscriber, #17465)
                              [Link] (2 responses)
      
      
      
Much more likely to have been a typo; b and v are next to each other on a qwerty keyboard.


      
          
        
     
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 30, 2025 5:17 UTC (Sat)
                               by awilfox (guest, #124923)
                              [Link] 
      
      
      
Why was this typo made in the first place, though?  Are you telling me Linus types his email all the time instead of using `git commit -s`?  That seems.. really odd to me.


      
          
        
     

    
      FS code quality of Linux seems not to be as one would wish for ...
       Posted Aug 31, 2025 0:58 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] 
      
      
      
Could be either. I'd rate the relative probabilities of Linus writing using a keyboard and just outputting raw bits as roughly even.


      
          
        
     




      Why not removed?
       Posted Aug 29, 2025 18:13 UTC (Fri)
                               by mb (subscriber, #50428)
                              [Link] (2 responses)
      
      
      

      
          
        
     
      Why not removed?
       Posted Aug 29, 2025 18:49 UTC (Fri)
                               by tux3 (subscriber, #101245)
                              [Link] 
      
      
      

      
          
        
     

    
      Why not removed?
       Posted Aug 29, 2025 22:47 UTC (Fri)
                               by hailfinger (subscriber, #76962)
                              [Link] 
      
      
      
There are quite a few advantages of keeping the code in the tree:
1. Minimal result: Users can continue to use bcachefs with newer kernels without having to patch the kernel, they just won't get bug fixes, but there will be no functional regression
2. Improvement with some effort by users: Users willing to patch the kernel can still apply any patches provided by Kent
3. Optimal result: A unicorn with the ability to work with Linus and Kent at the same time may appear, resulting in fixes from Kent being merged with the timing and criteria wanted by Linus


      
          
        
     


      Not so bad
       Posted Aug 29, 2025 21:36 UTC (Fri)
                               by birdie (guest, #114905)
                              [Link] 
      
      
      
I thought Linus would eject it from the kernel, but this change leaves the door open for continued development — exactly what I asked of Linus in my private exchange with him and Kent. Maybe he listened in the end.


      
          
        
     

      Winning the battles, losing the war
       Posted Aug 30, 2025 22:01 UTC (Sat)
                               by julian67 (guest, #99845)
                              [Link] (1 responses)
      
      
      
View from a mildly interested end user:  if you alienate the people you depend on it doesn't matter if you're right or wrong on any particular technical issue. You lose. The people you claim to care for or represent? Congratulations, you lost it for them too.  There is a 30+ year old kernel project with well established protocols, doctrine, structure, methods - all in the public space, totally discoverable, explicitly described, well understood.  But you know better, so much better that you repeatedly ignore every precedent, explicitly oppose and argue every admonition, refuse all advice, bad mouth the people who have been there before and who you need......guess what?  You needed them but they don't need you. Your talent is not unique.  You are not Moses, Jesus or Buddha or Mohammed.  Other people can make file systems too. And they did and they do.  And they work.  And they get a lot of development.  That's why they work, they are in.  And you are out.  Because you are not a voice of truth in the wilderness.


      
          
        
     
      Winning the battles, losing the war
       Posted Aug 30, 2025 23:44 UTC (Sat)
                               by pizza (subscriber, #46)
                              [Link] 
      
      
      

      
          
        
     


      Mediation?
       Posted Aug 30, 2025 22:33 UTC (Sat)
                               by sheepdestroyer (guest, #54968)
                              [Link] (1 responses)
      
      
      
Couldn't some big weights interested in bcachefs (at Valve & others maybe?) just nicely ask Linus to let this one go, or find a positive solution to keep bcachefs upstream?


      
          
        
     
      Mediation?
       Posted Aug 31, 2025 0:41 UTC (Sun)
                               by willy (subscriber, #9762)
                              [Link] 
      
      
      
I love it that you think this hasn't already been attempted and failed.


      
          
        
     


      Are Linus' patches posted to the mailing lists?
       Posted Aug 31, 2025 1:01 UTC (Sun)
                               by SLi (subscriber, #53131)
                              [Link] 
      
      
      
What's the process nowadays with all these git pulls etc. Do all patches, including those by Linus, still end up on some mailing list? I didn't find this one in any, but maybe just not indexed yet.


      
          
        
     
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cognitive load is what matters]]></title>
            <link>https://github.com/zakirullin/cognitive-load</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074248</guid>
            <description><![CDATA[🧠 Cognitive Load is what matters. Contribute to zakirullin/cognitive-load development by creating an account on GitHub.]]></description>
            <content:encoded><![CDATA[Cognitive Load is what matters
Readable version | Chinese translation | Korean translation | Turkish translation
It is a living document, last update: August 2025. Your contributions are welcome!
Introduction
There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.
Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.
Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.
Cognitive load

Cognitive load is how much a developer needs to think in order to complete a task.

When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.
Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.


We should reduce the cognitive load in our projects as much as possible.

  Cognitive load and interruptions
  

Types of cognitive load
Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.
Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.


Let's jump straight to the concrete practical examples of extraneous cognitive load.

We will refer to the level of cognitive load as follows:
🧠: fresh working memory, zero cognitive load
🧠++: two facts in our working memory, cognitive load increased
🤯: cognitive overload, more than 4 facts

Our brain is much more complex and unexplored, but we can go with this simplistic model.

Complex conditionals
if val > someConstant // 🧠+
    && (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    && (condition4 && !condition5) { // 🤯, we are messed up by this point
    ...
}
Introduce intermediate variables with meaningful names:
isValid = val > someConstant
isAllowed = condition2 || condition3
isSecure = condition4 && !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid && isAllowed && isSecure {
    ...
}
Nested ifs
if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} 
Compare it with the early returns:
if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+
We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.
Inheritance nightmare
We are asked to change a few things for our admin users: 🧠
AdminController extends UserController extends GuestController extends BaseController
Ohh, part of the functionality is in BaseController, let's have a look: 🧠+
Basic role mechanics got introduced in GuestController: 🧠++
Things got partially altered in UserController: 🧠+++
Finally we are here, AdminController, let's code stuff! 🧠++++
Oh, wait, there's SuperuserController which extends AdminController. By modifying AdminController we can break things in the inherited class, so let's dive in SuperuserController first: 🤯
Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.
Too many small methods, classes or modules

Method, class and module are interchangeable in this context

Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.
Deep module - simple interface, complex functionality
Shallow module - interface is relatively complex to the small functionality it provides


Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.

Information hiding is paramount, and we don't hide as much complexity in shallow modules.

I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.
Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.

The best components are those that provide powerful functionality yet have a simple interface.
John K. Ousterhout

The interface of the UNIX I/O is very simple. It has only five basic calls:
open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)
A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.

This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.

P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.
Responsible for one thing
All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.
We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.

A module should be responsible to one, and only one, user or stakeholder.

This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.
But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.
Too many shallow microservices
This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.
I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. It took an enormous amount of time to reproduce and debug an issue in such a distributed system. Both time to market and cognitive load were unacceptably high. 🤯
Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.
The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.
A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.
Feature-rich languages
We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.
If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!
You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. 🤯
These statements are made by none other than Rob Pike.

Reduce cognitive load by limiting the number of choices.

Language features are OK, as long as they are orthogonal to each other.

  Thoughts from an engineer with 20 years of C++ experience ⭐️
  
  I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!
  I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.
  Like, can you imagine, the token || has a different meaning in requires ((!P<T> || !Q<T>)) and in requires (!(P<T> || Q<T>)). The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.
  You can't allocate space for a trivial type and just memcpy a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.
  Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. 🤯
  There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then... 🤯
  This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).
  I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.
  By no means I am trying to blame C++. I love the language. It's just that I am tired now.Thanks to 0xd34df00d for writing.

Business logic and HTTP status codes
On the backend we return:
401 for expired jwt token
403 for not enough access
418 for banned users
The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:
401 is for expired jwt token // 🧠+, ok just temporary remember it
403 is for not enough access // 🧠++
418 is for banned users // 🧠+++
Frontend developers would (hopefully) introduce some kind numeric status -> meaning dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.
Then QA engineers come into play:
"Hey, I got 403 status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.
Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:
{
    "code": "jwt_has_expired"
}
Cognitive load on the frontend side: 🧠 (fresh, no facts are held in mind)
Cognitive load on the QA side: 🧠
The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.

People spend time arguing between 401 and 403, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.

P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.
Abusing DRY principle
Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.
Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. 🤯
In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.
Rob Pike once said:

A little copying is better than a little dependency.

We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.
All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.
Tight coupling with a framework
There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.
Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. 🤯
By no means do we advocate to invent everything from scratch!
We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.

Why I Hate Frameworks

Layered architecture
There is a certain engineering excitement about all this stuff.
I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. 🤯
Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. 🤯
This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.

  Coding principles and experience
  
  @flaviocopes

If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.

With a sufficient number of users of an API,
it does not matter what you promise in the contract:
all observable behaviors of your system
will be depended on by somebody.

We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.
So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.
These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.
Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.
Layers of abstraction aren't free of charge, they are to be held in our limited working memory.


Domain-driven design
Domain-driven design has some great points, although it is often misinterpreted. People say, "We write code in DDD", which is a bit strange, because DDD is more about the problem space rather than the solution space.
Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.
Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. 🤯
Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.
Cognitive load in familiar projects

The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!
It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.
In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”
There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.
Thanks to Dan North for his comment.

If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.


The more mental models there are to learn, the longer it takes for a new developer to deliver value.
Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.
If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.
Examples

Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres
How Instagram scaled to 14 million users with only 3 engineers
The companies where we were like ”woah, these folks are smart as hell” for the most part failed
One function that wires up the entire system. If you want to know how the system works - go read it

These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.
Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.
Maintaining software is hard, we would need every bit of mental effort we can save.
Conclusion
Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. 🤯
Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.


We should reduce any cognitive load above and beyond what is intrinsic to the work we do.

LinkedIn, X, GitHub
Readable version

    Comments
    
    Rob PikeNice article.
    Andrej Karpathy (ChatGPT, Tesla)Nice post on software engineering. Probably the most true, least practiced viewpoint.
    Elon MuskTrue.
    Addy Osmani (Chrome, the most complex software system in the world)I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.
    The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."
    What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.
    And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.
    Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.
    One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.
    Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. 
    Sometimes the simplest solution is the best one, even in a complex system.
    antirez (Redis)Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.
    A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.
    Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).
    These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D
    A developer from the internetYou would not hire me... I sell myself on my track record of released enterprise projects.
    I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.
    I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.
    I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.
    Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.
    Comments on Hacker News

]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Client Protocol (ACP)]]></title>
            <link>https://agentclientprotocol.com/overview/introduction</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45074147</guid>
            <description><![CDATA[Get started with the Agent Client Protocol (ACP)]]></description>
            <content:encoded><![CDATA[The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).
The protocol is still under development, but it should be complete enough to build interesting user experiences using it.Why ACP?
AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.
This creates several problems:
Integration overhead: Every new agent-editor combination requires custom work
Limited compatibility: Agents work with only a subset of available editors
Developer lock-in: Choosing an agent often means accepting their available interfaces

ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the Language Server Protocol (LSP) standardized language server integration.
Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.
This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.Overview
ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.
Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.
The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.Supported Editors

Zed
neovim through the CodeCompanion plugin

Supported Agents

Gemini
… more coming soon ;)
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hardening Firefox – a checklist for improved browser privacy]]></title>
            <link>https://andrewmarder.net/firefox/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45073746</guid>
            <description><![CDATA[A checklist for configuring Mozilla Firefox for a more private browsing experience.]]></description>
            <content:encoded><![CDATA[  July 3, 2025 /  3 min read  
Updated:
August 29, 2025    This checklist will walk you (and me) through the settings and extensions I use to improve my privacy when using Firefox.
If you’re looking for a web browser that offers a high degree of privacy out of the box with minimal setup, Brave is a common choice. However, I prefer Firefox for several reasons:

Firefox is developed by the nonprofit organization Mozilla.
I value Mozilla’s commitment to open source software.
Firefox is not based on Chromium. Brave, like most browsers, is based on Chromium, which is developed primarily by Google.

While there are many web browsers to choose from, I’ve decided Firefox is best for me. This post is a checklist of how I’ve configured it to better protect my privacy while browsing the web.
1. Basic Privacy Settings
Access Firefox’s settings by clicking the menu button (three horizontal lines) in the top-right corner and selecting “Settings.”

 Change Default Search Engine: In the Search tab, change the “Default Search Engine” to a privacy-respecting option like DuckDuckGo.
 Enable HTTPS-Only Mode: In the Privacy & Security tab, scroll down to “HTTPS-Only Mode” and select “Enable HTTPS-Only Mode in all windows.”
 Disable Telemetry: Still in Privacy & Security, scroll to “Firefox Data Collection and Use” and uncheck all the boxes to stop Firefox from sending data back to Mozilla.
 Set Enhanced Tracking Protection to Strict: Under Privacy & Security, set “Enhanced Tracking Protection” to Strict. This offers stronger protection against trackers. If a site breaks, you can easily disable it for that specific site by clicking the shield icon in the address bar.

2. Recommended Extensions

 Install uBlock Origin: A comprehensive content blocker that stops ads and tracking scripts, which speeds up page loading and enhances privacy.
 Install ClearURLs: This extension automatically removes tracking elements from URLs, helping prevent another form of web tracking.
 Install Privacy Badger: From the Electronic Frontier Foundation, this extension automatically learns to block invisible trackers. Instead of relying on blocklists, it discovers trackers based on behavior.

3. Advanced Configuration (about:config)
To access this, type about:config into the address bar and accept the warning.
Warning: Changing advanced configuration preferences can impact Firefox performance or security. Proceed with caution.

 Isolate Cookies to the First-Party Domain:

Search for privacy.firstparty.isolate and set its value to true.
This prevents cookies from tracking you from one site to another, but it can break single sign-on on some websites.


Resist Fingerprinting:

I previously set privacy.resistFingerprinting to true to make my browser fingerprint less unique.
It caused minor display issues on some sites and broke image uploads to Bluesky, so I set it back to false.



By following this checklist, you can significantly improve your privacy while using Firefox. Please let me know if I’m missing anything in the comments.
     ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[From multi-head to latent attention: The evolution of attention mechanisms]]></title>
            <link>https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45072160</guid>
            <description><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms
What is attention?
In any autoregressive model, the prediction of the future tokens is based on some preceding context …]]></description>
            <content:encoded><![CDATA[7 min read1 day ago--Press enter or click to view image in full sizeWhat is attention?In any autoregressive model, the prediction of the future tokens is based on some preceding context. However, not all the tokens within this context equally contribute to the prediction, because some tokens might be more relevant than others. The attention mechanism addresses this by allowing the model to concentrate on the important context words selectively, while generating each output word or token. Consider the popular example that explains the attention mechanism.“The animal didn’t cross the street because it was too tired”.In this sentence, the pronoun “it” could refer to either “animal” or “street”. Attention helps the model to associate “it” with “animal” rather than “street” by weighing the relative importance of each word. This helps the model to understand the relationships between words and capture the contextual meaning in various NLP tasks.How is attention calculated?There are various types of attention mechanisms today, beginning with the Multi-Head Attention (MHA), which introduced the attention concept in the seminal paper. More recently, advanced variants like Multi-Latent Head Attention (MHLA) have been employed in popular models like Deepseek. This blog aims to cover the fundamentals of each attention mechanism, including the core ideas, advantages, limitations, etc.Key Concepts in Attention MechanismsBefore diving into specific types of attention, we need to understand some fundamental concepts that underpin all the various attention mechanisms.The main idea behind the attention mechanism is to dynamically weigh, and focus on relevant parts of inputs. Attention is required in both the encoding and decoding stages. But in this blog, we will be discussing this from a decoder's point of view.During each generation step, we need to understand the attention weights, which help us to get a better contextual representation for the next word prediction. At its core, attention operates through three fundamental components — queries, keys, and values — that work together with attention scores to create a flexible, context-aware vector representation.Query (Q): The query is a vector that represents the current token for which the model wants to compute attention.Key (K): Keys are vectors that represent the elements in the context against which the query is compared, to determine the relevance.Attention Scores: These are computed using Query and Key vectors to determine the amount of attention to be paid to each context token.Value (V): Values are the vectors that represent the actual contextual information. After calculating the attention scores using Query and Key vectors, these scores are applied against Value vectors to get the final context vectorKV Caching: Since the key and value vectors are for previous tokens, we can skip this computation for those tokens that are already calculated. KV caching stores the precomputed keys and values from the previous computations, which helps in faster decoding in autoregressive models by reusing the cached vectors. However, the Query vectors cannot be cached, since they are calculated for the current token.To understand how each of these vectors are scores are calculated you can refer to this blog.The high-level concepts remain consistent across all types of attention mechanisms. However, the key difference lies in how efficiently each of them executes the attention process without compromising on performance. Innovations focus on computational speed, reducing memory usage, improving scalability across longer sequences, etc.Now, let's dive into each of these techniquesMulti-Head Attention (MHA)In multi-head attention, for computing the attention weights for the ith token, first, a query vector is calculated for that token. To calculate the attention weights for the token, this query vector is compared with all the preceding tokens. For that, key vectors are calculated for all the preceding tokens. These comparisons will generate an attention score, which is then used to produce a weighted score for each token using the corresponding value vectors.Press enter or click to view image in full sizeImage credits: Illustrated TransformersIn multi-head attention, this process is repeated in parallel across multiple attention “heads”. Each head has its own query, value, and key vectors, using which it calculates the relationship between the words. The final output context vector will be the concatenated output from all the attention heads.Now, this seems straightforward. However, as the context grows, the number of Key and Value vectors will increase dramatically, because these vectors need to be calculated and stored for all the context tokens. For a sequence length of n, each query vector must be compared against all n key vectors and then perform the weighted combination using n value vectors. This results in a quadratic complexity in both computation and memory.KV cache can help in reducing the computation and memory overhead during inference. But as the context grows, the size of the cache grows linearly with sequence length to store all the keys and values for all the preceding tokens. KV cache reduces the redundant computations, but will not reduce the fundamental cost of attending to all the previous tokens.Models using MHA – Bert, RoBerta, T5, etc.Multi-Query Attention (MQA)A significant challenge with MHA was the high computational and memory overhead associated with storing and processing separate Key and Value vectors for each attention head.MQA addresses this problem by using multiple query heads but sharing a common set of Key and Value vectors across all the heads. In other words, there are still “h” distinct Query projections using which the model attends the current token from multiple perspectives. But the same Key and Value vectors are used for every head.This approach will greatly reduce the memory bandwidth requirements without significantly sacrificing the model performance. By sharing the Key and Value vectors, MQA enables an efficient inference, especially for Large language models with long context lengths.Here, the Key and Value vectors need to be calculated only once for a token instead of “h” times, which reduces the computation cost of Key/Value projection. But note that for calculating the attention score, each query head is still multiplied by the Key vectors and then weighed using the Value vectors. So this remains the same.Also, with MQA only one set of Key-Value pairs needs to be cached, regardless of the number of Query heads. This lets the KV cache size grow gradually as the sequence length grows, leading to much lower memory requirements when compared to MHAModels using MQA – PaLM, FalconGrouped Query Attention (GQA)Grouped Query attention offers a balance between the MHA and MQA. As we saw earlier, traditional MHA requires significant memory and computation overhead due to separate Key-Value vectors for each Query head, and the computation overhead even increases as the number of heads increases. MQA addresses this by having a shared Key-Value, which reduces the computation cost and memory, but it may impact the model performance.GQA offers a compromise between these two extremes. Instead of having a common Key-Value for all the heads, GQA divides the Query heads into “g” groups and lets each group share a common Key and Value head. We can say, MHA and MQA come as two extreme cases of GQA, with g=1 leading to MQA and g=h leading to MHA. This approach reduces the memory and computational requirements compared to MHA while retaining a better performance than MQA.Models using GQA – Llama2, Llama3, MistralMulti-Head Latent Attention (MHLA)While GQA performs better than MQA, but still may not match MHA’s performance in some complex tasks.MHLA is a recent innovation in transformer architecture introduced in models like DeepSeek. Its main goal is to dramatically reduce memory usage and accelerate inference, especially for large language models (LLMs), without loss in model performance.The idea is to attain a performance near MHA. So we need to consider separate Key value heads for each attention head, like in MHA, but also improve the inference speed by reducing the memory overhead for storing the large amounts of Key value vectors.MHLA addresses the challenge of high memory usage and slow inference by compressing the Key and Value representations into a much smaller latent space using low-rank projections. Specifically, instead of storing the full Key and Value vectors for every token and head, MHLA applies a linear transformation that projects these vectors into a lower-dimensional space.So during the inference:A down-projection weight matrix W(DKV) is introduced and is multiplied with the input sequence to obtain a compressed latent vector C(KV) for keys and Values. This latent vector is stored in cache, which is significantly smaller in size when compared to the full key and Value vectorsThis is then multiplied by an up-projection matrix W(UK) and W(UV) to get the Key and Value vectorsAdditionally, the matrix W(KR) is used to produce a decoupled Key that carries the Rotary Positional embeddingAdditionally, the same process is done for attention Queries as well, which will reduce the activation memory during trainingPress enter or click to view image in full sizeMHLA supports switching between two computation paradigms for different stages. During the training stage, which is computationally intensive, it operates similarly to MHA, where the computational overhead is slightly lower than conventional MHA. During inference, it can seamlessly switch to a paradigm similar to MQA. Here, the cached KV head interacts with all query heads to produce the final output.Models using MHLA– Deepseek- V2, Deep seek V2ConclusionIn addition to the topics discussed, there are various innovative methods that are designed to optimise the challenges of the traditional attention technique. Some of these include sparse attention, efficient attention, memory augmented attention, etc. These approaches reflect the focus on ongoing research for making the attention more scalable, faster, and adaptable across various tasks and requirements.Thank you for reading this post! Let me know if you liked it, have questions, or spotted an error. Please feel free to contact or follow me through LinkedIn, Twitter, or Medium.]]></content:encoded>
        </item>
    </channel>
</rss>