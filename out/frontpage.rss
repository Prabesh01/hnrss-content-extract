<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Hacker News: Front Page</title>
        <link>https://news.ycombinator.com/</link>
        <description>Hacker News RSS</description>
        <lastBuildDate>Thu, 11 Sep 2025 13:34:59 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>github.com/Prabesh01/hnrss-content-extract</generator>
        <language>en</language>
        <atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/frontpage.rss" rel="self" type="application/rss+xml"/>
        <item>
            <title><![CDATA[GrapheneOS and Forensic Extraction of Data]]></title>
            <link>https://discuss.grapheneos.org/d/13107-grapheneos-and-forensic-extraction-of-data</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210910</guid>
            <description><![CDATA[GrapheneOS discussion forum]]></description>
            <content:encoded><![CDATA[

        
            
            
                    
                                                    GrapheneOS Discussion Forum
                                            
                
        

    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Behind the Scenes of Bun Install]]></title>
            <link>https://bun.com/blog/behind-the-scenes-of-bun-install</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210850</guid>
            <description><![CDATA[Learn how Bun is able to cut install times by up to 25×. Bun skips Node.js's overhead with direct system calls, cache-friendly data layouts, OS-level copy-on-write, and full-core parallelism.]]></description>
            <content:encoded><![CDATA[Running bun install is fast, very fast. On average, it runs ~7× faster than npm, ~4× faster than pnpm, and ~17× faster than yarn. The difference is especially noticeable in large codebases. What used to take minutes now takes (milli)seconds.These aren't just cherry-picked benchmarks. Bun is fast because it treats package installation as a systems programming problem, not a JavaScript problem.In this post we’ll explore what that means: from minimizing syscalls and caching manifests as binary, to optimizing tarball extraction, leveraging OS-native file copying, and scaling across CPU cores.But to understand why this matters, we first have to take a small step back in time.It's the year 2009. You're installing jQuery from a .zip file, your iPhone 3GS has 256MB of RAM. GitHub was just a year old, SSDs cost $700 for 256GB. Your laptop's 5400RPM hard drive maxes out at 100MB/s, and "broadband" means 10 Mbps (if you're lucky).But more importantly: Node.js just launched! Ryan Dahl is on stage explaining why servers spend most of their time waiting.In 2009, a typical disk seek takes 10ms, a database query 50–200ms, and an HTTP request to an external API 300ms+. During each of these transactions, traditional servers would just... wait. Your server would start reading a file, and then just freeze for 10ms.Now multiply that by thousands of concurrent connections each doing multiple I/O operations. Servers spent ~95% of their time waiting for I/O operations.Node.js figured that JavaScript's event loop (originally designed for browser events) was perfect for server I/O. When code makes an async request, the I/O happens in the background while the main thread immediately moves to the next task. Once complete, a callback gets queued for execution.  Simplified illustration of how Node.js handles fs.readFile with the event loop and thread pool. Other async sources and implementation details are omitted for clarity. JavaScript's event loop was a great solution for a world where waiting for data was the primary bottleneck.For the next 15 years, Node's architecture shaped how we built tools. Package managers inherited Node's thread pool, event loop, async patterns; optimizations that made sense when disk seeks took 10ms.But hardware evolved. It's not 2009 anymore, we're 16 years into the future, as hard as that is to believe. The M4 Max MacBook I'm using to write this would've ranked among the 50 fastest supercomputers on Earth in 2009. Today's NVMe drives push 7,000 MB/s, 70× faster than what Node.js was designed for! The slow mechanical drives are gone, internet speeds stream 4K video, and even low-end smartphones have more RAM than high-end servers had in 2009.Yet today's package managers still optimize for the last decade's problems. In 2025, the real bottleneck isn't I/O anymore. It's system calls.The Problem with System CallsEvery time your program wants the operating system to do something (read a file, open a network connection, allocate memory), it makes a system call. Each time you make a system call, the CPU has to perform a mode switch.Your CPU can run programs in two modes:user mode, where your application code runs. Programs in user mode cannot directly access your device's hardware, physical memory addresses, etc. This isolation prevents programs from interfering with each other or crashing the system.kernel mode, where the operating system's kernel runs. The kernel is the core component of the OS that manages resources like scheduling processes to use the CPU, handling memory, and hardware like disks or network devices. Only the kernel and device drivers operate in kernel mode!When you want to open a file, (e.g. fs.readFile()) in your program, the CPU running in user mode cannot directly read from disk. It first has to switch to kernel mode.During this mode switch, the CPU stops executing your program → saves all its state → switches into kernel mode → performs the operation → then switches back to user mode.However, this mode switching is expensive! Just this switch alone costs 1000-1500 CPU cycles in pure overhead, before any actual work happens.Your CPU operates on a clock that ticks billions of times per second. A 3GHz processor completes 3 billion cycles per second. During each cycle the CPU can execute instructions: add numbers, move data, make comparisons, etc. Each cycle takes 0.33ns.On a 3GHz processor, 1000-1500 cycles is about 500 nanoseconds. This might sound negligibly fast, but modern SSDs can handle over 1 million operations per second. If each operation requires a system call, you're burning 1.5 billion cycles per second just on mode switching.Package installation makes thousands of these system calls. Installing React and its dependencies might trigger 50,000+ system calls: that's seconds of CPU time lost to mode switching alone! Not even reading files or installing packages, just switching between user and kernel mode.This is why Bun treats package installation as a systems programming problem. Fast install speeds come from minimizing system calls and leveraging every OS-specific optimization available.You can see the difference when we trace the actual system calls made by each package manager:Benchmark 1: strace -c -f npm install
    Time (mean ± σ):  37.245 s ±  2.134 s [User: 8.432 s, System: 4.821 s]
    Range (min … max):   34.891 s … 41.203 s    10 runs

    System calls: 996,978 total (108,775 errors)
    Top syscalls: futex (663,158),  write (109,412), epoll_pwait (54,496)

  Benchmark 2: strace -c -f bun install
    Time (mean ± σ):      5.612 s ±  0.287 s [User: 2.134 s, System: 1.892 s]
    Range (min … max):    5.238 s …  6.102 s    10 runs

    System calls: 165,743 total (3,131 errors)
    Top syscalls: openat(45,348), futex (762), epoll_pwait2 (298)

  Benchmark 3: strace -c -f yarn install
    Time (mean ± σ):     94.156 s ±  3.821 s    [User: 12.734 s, System: 7.234 s]
    Range (min … max):   89.432 s … 98.912 s    10 runs

    System calls: 4,046,507 total (420,131 errors)
    Top syscalls: futex (2,499,660), epoll_pwait (326,351), write (287,543)

  Benchmark 4: strace -c -f pnpm install
    Time (mean ± σ):     24.521 s ±  1.287 s    [User: 5.821 s, System: 3.912 s]
    Range (min … max):   22.834 s … 26.743 s    10 runs

    System calls: 456,930 total (32,351 errors)
    Top syscalls: futex (116,577), openat(89,234), epoll_pwait (12,705)

  Summary
    'strace -c -f bun install' ran
      4.37 ± 0.28 times faster than 'strace -c -f pnpm install'
      6.64 ± 0.51 times faster than 'strace -c -f npm install'
     16.78 ± 1.12 times faster than 'strace -c -f yarn install'

  System Call Efficiency:
    - bun:  165,743 syscalls (29.5k syscalls/s)
    - pnpm: 456,930 syscalls (18.6k syscalls/s)
    - npm:  996,978 syscalls (26.8k syscalls/s)
    - yarn: 4,046,507 syscalls (43.0k syscalls/s)
We can see that Bun installs much faster, but it also makes far fewer system calls. For a simple install, yarn makes over 4 million system calls, npm almost 1 million, pnpm close to 500k, and bun 165k.At 1000-1500 cycles per call, yarn's 4 million system calls means it's spending billions of CPU cycles just on mode switching. On a 3GHz processor, that's seconds of pure overhead!And it's not just the amount of system calls. Look at those futex calls! Bun made 762 futex calls (only 0.46% of total system calls), whereas npm made 663,158 (66.51%), yarn made 2,499,660 (61.76%), and pnpm made 116,577 (25.51%).futex (fast userspace mutex) is a Linux system call used for thread synchronization. Threads are smaller units of a program that run simultaneously that often share access to memory or resources, so they must coordinate to avoid conflicts.Most of the time, threads coordinate using fast atomic CPU instructions in user mode. There's no need to switch to kernel mode, so it's very efficient!But if a thread tries to acquire a lock that's already taken, it makes a futex syscall to ask the kernel to put it to sleep until the lock becomes available. A high number of futex calls is an indicator that many threads are waiting on one another, causing delays.So what's Bun doing differently here?Eliminating JavaScript overheadnpm, pnpm and yarn are all written in Node.js. In Node.js, system calls aren’t made directly: when you call fs.readFile(), you’re actually going through several layers before reaching the OS.Node.js uses libuv, a C library that abstracts platform differences and manages async I/O through a thread pool.The result is that when Node.js has to read a single file, it triggers a pretty complex pipeline. For a simple fs.readFile('package.json', ...):JavaScript validates arguments and converts strings from UTF-16 to UTF-8 for libuv's C APIs. This briefly blocks the main thread before any I/O even starts.libuv queues the request for one of 4 worker threads. If all threads are busy, your request waits.A worker thread picks up the request, opens the file descriptor, and makes the actual read() system call.The kernel switches to kernel mode, fetches the data from disk, and returns it to the worker thread.The worker pushes the file data back to the main thread through the event loop, which eventually schedules and runs your callback.Every single fs.readFile() call goes through this pipeline. Package installation involves reading thousands of package.json files: scanning directories, processing dependency metadata, and so on. Each time threads coordinate (e.g., when accessing the task queue or signaling back to the event loop), a futex system call can be used to manage locks or waits.The overhead of making thousands of these system calls can take longer than the actual data movement itself!Bun does it differently. Bun is written in Zig, a programming language that compiles to native code with direct system call access:// Direct system call, no JavaScript overhead
var file = bun.sys.File.from(try bun.sys.openatA(
    bun.FD.cwd(),
    abs,
    bun.O.RDONLY,
    0,
).unwrap());
When Bun reads a file:Zig code directly invokes the system call (e.g., openat() )The kernel immediately executes the system call and returns dataThat's it. There's no JavaScript engine, thread pools, event loops or marshaling between different runtime layers. Just native code making direct system calls to the kernel.The performance difference speaks for itself:RuntimeVersionFiles/SecondPerformanceBunv1.2.20146,057Node.jsv24.5.066,5762.2x slowerNode.jsv22.18.064,6312.3x slowerIn this benchmark, Bun processes 146,057 package.json files per second, while Node.js v24.5.0 manages 66,576 and v22.18.0 handles 64,631. That's over 2x faster!Bun's 0.019ms per file represents the actual I/O cost, so how long it takes to read data when you make direct system calls without any runtime overhead. Node.js takes 0.065ms for the same operation. Package managers written in Node.js are "stuck" with Node's abstractions; they use the thread pool whether they need it or not. But they pay this cost on every file operation.Bun's package manager is more like a native application that happens to understand JavaScript packages, not a JavaScript application trying to do systems programming.Even though Bun isn't written in Node.js, you can use bun install in any Node.js project without switching runtimes. Bun's package manager respects your existing Node.js setup and tooling, you just get faster installs!But at this point we haven't even started installing packages yet. Let's see the optimizations Bun applies to the actual installation.When you type bun install, Bun first figures out what you're asking it to do. It reads any flags you've passed, and finds your package.json to read your dependencies.Async DNS Resolution⚠️ Note: This optimization is specific to macOSWorking with dependencies means working with network requests, and network requests require DNS resolution to convert domain names like registry.npmjs.org into IP addresses.As Bun is parsing the package.json, it already starts to prefetch the DNS lookups. This means network resolution begins even before dependency analysis is even complete.For a Node.js-based package managers, one way to do it is by using dns.lookup(). While this looks async from JavaScript's perspective, it's actually implemented as a blocking getaddrinfo() call under the hood, running on libuv's thread pool. It still blocks a thread, just not the main thread.As a nice optimization, Bun takes a different approach on macOS by making it truly asynchronous at the system level. Bun uses Apple's "hidden" async DNS API (getaddrinfo_async_start()), which isn't part of the POSIX standard, but it allows bun to make DNS requests that run completely asynchronously using mach ports, Apple's inter-process communication system.While DNS resolution happens in the background, Bun can continue processing other operations like file I/O, network requests, or dependency resolution without any thread blocking. By the time it needs to download React, the DNS lookup is already done.It's a small optimization (and not benchmarked), but it shows Bun's attention to detail: optimize at every layer!Binary Manifest CachingNow that Bun has established a connection to the npm registry, it needs the package manifests.A manifest is a JSON file containing all versions, dependencies, and metadata for each package. For popular packages like React with 100+ versions, these manifests can be several megabytes!A typical manifest can look something like this:{
  "name": "lodash",
  "versions": {
    "4.17.20": {
      "name": "lodash",
      "version": "4.17.20",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    },
    "4.17.21": {
      "name": "lodash",
      "version": "4.17.21",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    }
    // ... 100+ more versions, nearly identical
  }
}
Most package managers cache these manifests as JSON files in their cache directories. When you run npm install again, instead of downloading the manifest, they read it from the cache.That all makes sense, but the issue is that on every install (even if it's cached), they still need to parse the JSON file. This includes validating the syntax, building the object tree, managing garbage collection, and so on. A lot of parsing overhead.And it's not just the JSON parsing overhead. Looking at lodash: the string "Lodash modular utilities." appears in every single version—that's 100+ times. "MIT" appears 100+ times. "git+https://github.com/lodash/lodash.git" is duplicated for every version, the URL "https://lodash.com/" appears in every version. Overall, lots of repeated strings.In memory, JavaScript creates a separate string object for each string. This wastes memory and makes comparisons slower. Every time the package manager checks if two packages use the same version of postcss, it's comparing separate string objects rather than pointing to the same interned string.Bun stores package manifests in a binary format. When Bun downloads package information, it parses the JSON once and stores it as binary files (.npm files in ~/.bun/install/cache/). These binary files contain all the package information (versions, dependencies, checksums, etc.) stored at specific byte offsets.When Bun accesses the name lodash, it's just pointer arithmetic: string_buffer + offset. No allocations, no parsing, no object traversal, just reading bytes at a known location.// Pseudocode

// String buffer (all strings stored once)
string_buffer = "lodash\0MIT\0Lodash modular utilities.\0git+https://github.com/lodash/lodash.git\0https://lodash.com/\04.17.20\04.17.21\0..."
                 ^0     ^7   ^11                        ^37                                      ^79                   ^99      ^107

// Version entries (fixed-size structs)
versions = [
  { name_offset: 0, name_len: 6, version_offset: 99, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... },  // 4.17.20
  { name_offset: 0, name_len: 6, version_offset: 107, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... }, // 4.17.21
  // ... 100+ more version structs
]
To check if packages need updating, Bun stores the responses's ETag , and sends If-None-Match headers. When npm responds with "304 Not Modified", Bun knows the cached data is fresh without parsing a single byte.Looking at the benchmarks:Benchmark 1: bun install # fresh install
  Time (mean ± σ):      35.7 ms ±  86.6 ms    [User: 8.4 ms, System: 13.4 ms]
  Range (min … max):     4.1 ms … 280.5 ms    10 runs

Benchmark 2: bun install # cached
  Time (mean ± σ):       4.8 ms ±   0.7 ms    [User: 4.7 ms, System: 3.9 ms]
  Range (min … max):     3.7 ms …   6.2 ms    482 runs

Benchmark 3: npm install # fresh install
  Time (mean ± σ):     815.1 ms ± 976.9 ms    [User: 730.1 ms, System: 130.4 ms]
  Range (min … max):   478.8 ms … 3595.1 ms    10 runs

Summary
  bun install # cached ran
    1.02 ± 0.02 times faster than bun install # fresh, no cache
    3.73 ± 0.12 times faster than npm install # cached
Here you can see that a cached(!!) npm install is slower than a fresh Bun install. That's how much overhead JSON parsing the cached files can add (among other factors).Now that Bun has fetched the package manifests, it needs to download and extract compressed tarballs from the npm registry.Tarballs are compressed archive files (like .zip files) that contain all the actual source code and files for each package.Most package managers stream the tarball data as it arrives, and decompress as it streams in. When you extract a tarball that's streaming in, the typical pattern assumes the size is unknown, and looks something like this:let buffer = Buffer.alloc(64 * 1024); // Start with 64KB
let offset = 0;

function onData(chunk) {
  while (moreDataToCome) {
    if (offset + chunk.length > buffer.length) {
      // buffer full → allocate bigger one
      const newBuffer = Buffer.alloc(buffer.length * 2);

      // copy everything we’ve already written
      buffer.copy(newBuffer, 0, 0, offset);

      buffer = newBuffer;
    }

    // copy new chunk into buffer
    chunk.copy(buffer, offset);
    offset += chunk.length;
  }

  // ... decompress from buffer ...
}
Start with a small buffer, and let it grow as more decompressed data arrives. When the buffer fills up, you allocate a larger buffer, copy all the existing data over, and continue.This seems reasonable, but it creates a performance bottleneck: you end up copying the same data multiple times as the buffer repeatedly outgrows its current size.When we have a 1MB package:Start with 64KB bufferFill up → Allocate 128KB → Copy 64KB overFill up → Allocate 256KB → Copy 128KB overFill up → Allocate 512KB → Copy 256KB overFill up → Allocate 1MB → Copy 512KB overYou just copied 960KB of data unnecessarily! And this happens for every single package. The memory allocator has to find contiguous space for each new buffer, while the old buffer stays allocated during the copy operation. For large packages, you might copy the same bytes 5-6 times.Bun takes a different approach by buffering the entire tarball before decompressing. Instead of processing data as it arrives, Bun waits until the entire compressed file is downloaded into memory.Now you might think "Wait, aren't they just wasting RAM keeping everything in memory?" And for large packages like TypeScript (which can be 50MB compressed), you'd have a point.But the vast majority of npm packages are tiny, most are under 1MB. For these common cases, buffering the whole thing eliminates all the repeated copying. Even for those larger packages, the temporary memory spike is usually fine on modern systems, and avoiding 5-6 buffer copies more than makes up for it.Once Bun has the complete tarball in memory, it can read the last 4 bytes of the gzip format. These bytes are special since store the uncompressed size of the file! Instead of having to guess how large the uncompressed file will be, Bun can pre-allocate memory to eliminate buffer resizing entirely:{
  // Last 4 bytes of a gzip-compressed file are the uncompressed size.
  if (tgz_bytes.len > 16) {
    // If the file claims to be larger than 16 bytes and smaller than 64 MB, we'll preallocate the buffer.
    // If it's larger than that, we'll do it incrementally. We want to avoid OOMing.
    const last_4_bytes: u32 = @bitCast(tgz_bytes[tgz_bytes.len - 4 ..][0..4].*);
    if (last_4_bytes > 16 and last_4_bytes < 64 * 1024 * 1024) {
      // It's okay if this fails. We will just allocate as we go and that will error if we run out of memory.
      esimated_output_size = last_4_bytes;
      if (zlib_pool.data.list.capacity == 0) {
          zlib_pool.data.list.ensureTotalCapacityPrecise(zlib_pool.data.allocator, last_4_bytes) catch {};
      } else {
          zlib_pool.data.ensureUnusedCapacity(last_4_bytes) catch {};
      }
    }
  }
}
Those 4 bytes tell Bun "this gzip will decompress to exactly 1,048,576 bytes", so it can pre-allocate exactly this amount of memory upfront. There's no repeated resizing or copying of data; just one memory allocation.To do the actual decompression, Bun uses libdeflate. This is a high-performance lib that decompresses tarballs faster than the standard zlib used by most package managers. It's optimized specifically for modern CPUs with SIMD instructions.Optimized tarball extraction would've been difficult to for package managers written in Node.js. You'd need to create a separate read stream, seek to the end, read 4 bytes, parse them, close the stream, then start over with your decompression. Node's APIs aren't designed for this pattern.In Zig it's pretty straight-forward: you just seek to the end and read the last four bytes, that's it!Now that Bun has all the package data, it faces another challenge: how do you efficiently store and access thousands of (interdependent) packages?Cache-Friendly Data LayoutDealing with thousands of packages can be tricky. Each package has dependencies, which have their own dependencies, creating a pretty complex graph.During installation, package managers have to traverse this graph to check the package versions, resolve any conflicts, and determine which version to install. They also need to "hoist" dependencies by moving them to higher levels so multiple packages can share them.But the way that this dependency graph is stored has a big impact on performance. Traditional package managers store dependencies like this:const packages = {
  next: {
    name: "next",
    version: "15.5.0",
    dependencies: {
      "@swc/helpers": "0.5.15",
      "postcss": "8.4.31",
      "styled-jsx": "5.1.6",
    },
  },
  postcss: {
    name: "postcss",
    version: "8.4.31",
    dependencies: {
      nanoid: "^3.3.6",
      picocolors: "^1.0.0",
    },
  },
};
This looks clean as JavaScript code, but it's not ideal for modern CPU architectures.In JavaScript, each object is stored on the heap. When accessing packages["next"], the CPU accesses a pointer that tells it where Next's data is located in memory. This data then contains yet another pointer to where its dependencies live, which in turn contains more pointers to the actual dependency strings.The key issue is how JavaScript allocates objects in memory. When you create objects at different times, the JavaScript engine uses whatever memory is available at that moment:// These objects are created at different moments during parsing
packages["react"] = { name: "react", ... }  	  // Allocated at address 0x1000
packages["next"] = { name: "next", ... }     		// Allocated at address 0x2000
packages["postcss"] = { name: "postcss", ... }  // Allocated at address 0x8000
// ... hundreds more packages
These addresses are basically just random. There is no locality guarantee - objects can just be scattered across RAM, even objects that are related to each other!This random scattering matters because of how modern CPUs actually fetch data.Modern CPUs are incredibly fast at processing data (billions of operations per second), but fetching data from RAM is slow. To bridge this gap, CPUs have multiple cache levels:L1 cache, small storage, but extremely fast (~4 CPU cycles)L2 cache, medium storage, a bit slower (~12 CPU cycles)L3 cache: 8-32MB storage, requires ~40 CPU cyclesRAM: Lots of GB, requires ~300 cycles (slow!)The "issue" is that caches work with cache lines. When you access memory, the CPU doesn't just load that one byte: it loads the entire 64-byte chunk in which that byte appears. It figures that if you need one byte, you'll probably need nearby bytes soon (this is called spatial locality).This optimization works great for data that's stored sequentially, but it backfires when your data is scattered randomly across memory.When the CPU loads packages["next"] at address 0x2000, it actually loads all the bytes within that cache line. But the next package, packages["postcss"], is at address 0x8000 . This is a completely different cache line! The other 56 bytes the CPU loaded in the cache line are just completely wasted, they're just random memory from whatever happened to be allocated nearby; maybe garbage, maybe parts of unrelated objects.But you paid the cost of loading 64 bytes but only used 8...By the time it's accessed 512 different packages (32KB / 64 bytes), you've filled your entire L1 cache already. Now every new package access evicts a previously loaded cache line to make space. The package you just accessed will be evicted soon, and that dependency it needs to check in 10 microseconds is already gone. Cache hit rate drops, and every access becomes a ~300 cycle trip to RAM instead of a 4 cycle L1 hit, far from optimal.The nested structure of objects creates whats called "pointer chasing", a common anti-pattern in system programming. The CPU can't predict where to load next because each pointer could point anywhere. It simply cannot know where next.dependencies lives until it finishes loading the next object.When traversing Next's dependencies, the CPU has to perform multiple dependent memory loads:Load packages["next"] pointer → Cache miss → RAM fetch (~300 cycles)Follow that pointer to load next.dependencies pointer → Another cache miss → RAM fetch (~300 cycles)Follow that to find "postcss" in the hash table → Cache miss → RAM fetch (~300 cycles)Follow that pointer to load the actual string data → Cache miss → RAM fetch (~300 cycles)We can end up with many cache misses since we're working with hundreds of dependencies, all scattered across memory. Each cache line we load (64 bytes) might contain data for just one object. With all those objects spread across GBs of RAM, the working set easily exceeds the L1 cache (32KB), L2 (256KB) and even the L3 cache (8-32MB). By the time we need an object again, it's likely that it's been evicted from all cache levels.That's ~1200 cycles (400ns on a 3GHz CPU) just to read one dependency name! For a project with 1000 packages averaging 5 dependencies each, that's 2ms of pure memory latency.Bun uses Structure of Arrays. Instead of each package storing its own dependency array, Bun keeps all dependencies in one big shared array, all package names in another shared array, and so on:// ❌ Traditional Array of Structures (AoS) - lots of pointers
packages = {
  next: { dependencies: { "@swc/helpers": "0.5.15", "postcss": "8.4.31" } },
};

// ✅ Bun's Structure of Arrays (SoA) - cache friendly
packages = [
  {
    name: { off: 0, len: 4 },
    version: { off: 5, len: 6 },
    deps: { off: 0, len: 2 },
  }, // next
];

dependencies = [
  { name: { off: 12, len: 13 }, version: { off: 26, len: 7 } }, // @swc/helpers@0.5.15
  { name: { off: 34, len: 7 }, version: { off: 42, len: 6 } }, // postcss@8.4.31
];

string_buffer = "next\015.5.0\0@swc/helpers\00.5.15\0postcss\08.4.31\0";
Instead of each package storing pointers to its own data scattered across memory, Bun just uses large contiguous buffers, including:packages stores lightweight structs that specify where to find this package's data using offsetsdependencies stores the actual dependency relationships for all packages in one placestring_buffer stores all text (names, versions, etc.) sequentially in one massive stringversions stores all parsed semantic versions as compact structsNow, accessing Next's dependencies just becomes arithmetic:packages[0] tells us that Next's dependencies start at position 0 in the dependencies array, and there's 2 dependencies: { name_offset: 0, deps_offset: 0, deps_count: 2 }Go to dependencies[1] which tells us that postcss's name starts at position 34 in the string string_buffer, and version at position 42: { name_offset: 34, version_offset: 42 }Go to position 34 in string_buffer and read postcssGo to position 42 in string_buffer and read "8.4.31"… and so onNow when you access packages[0], the CPU doesn't just load those 8 bytes: it loads an entire 64-byte cache line. Since each package is 8 bytes, and 64 ÷ 8 = 8, you get packages[0] through packages[7] in a single memory fetch.So when your code processes the react dependency (packages[0], packages[1] through packages[7] are already sitting in your L1 cache, ready to be accessed with zero additional memory fetches. That's why sequential access is so fast: you're getting 8 packages just by accessing memory once.Instead of the many small, scattered allocations throughout memory that we saw in the previous example, we now have just ~6 large allocations in total, regardless of how many packages you have. This is completely different from the pointer-based approach, which required a separate memory fetch for each object.Optimized Lockfile FormatBun also applies the Structure of Arrays approach to its bun.lock lockfile.When you run bun install, Bun has to parse the existing lockfile to determine what's already installed and what needs updating. Most package managers store lockfiles as nested JSON (npm) or YAML (pnpm, yarn). When npm parses package-lock.json, it's processing deeply nested objects:{
  "dependencies": {
    "next": {
      "version": "15.5.0",
      "requires": {
        "@swc/helpers": "0.5.15",
        "postcss": "8.4.31"
      }
    },
    "postcss": {
      "version": "8.4.31",
      "requires": {
        "nanoid": "^3.3.6",
        "picocolors": "^1.0.0"
      }
    }
  }
}
Each package becomes its own object with nested dependency objects. JSON parsers must allocate memory for every object, validate syntax, and build complex nested trees. For projects with thousands of dependencies, this creates the same pointer-chasing problem we saw earlier!Bun applies the Structure of Arrays approach to its lockfile, in a human-readable format:{
  "lockfileVersion": 0,
  "packages": {
    "next": [
      "next@npm:15.5.0",
      { "@swc/helpers": "0.5.15", "postcss": "8.4.31" },
      "hash123"
    ],
    "postcss": [
      "postcss@npm:8.4.31",
      { "nanoid": "^3.3.6", "picocolors": "^1.0.0" },
      "hash456"
    ]
  }
}
This again deduplicates strings, and stores dependencies in a cache-friendly layout. They're stored following dependency order rather than alphabetically or in a nested hierarchy. This means that a parser can read memory more efficiently (sequentially), avoiding random jumps between objects.And not only that, Bun also pre-allocates memory based on the lockfile size. Just like with tarball extraction, this avoids the repeated resize-and-copy cycles that create performance bottlenecks during parsing.As a sidenote: Bun originally used a binary lockfile format (bun.lockb) to avoid JSON parsing overhead entirely, but binary files are impossible to review in pull requests and can't be merged when conflicts happen.File copyingAfter the packages are installed and cached in ~/.bun/install/cache/, Bun must copy the files into node_modules. This is where we see most of Bun's performance impact!Traditional file copying traverses each directory and copies files individually. This requires multiple system calls per file:opening the source file (open())creating and opening the destination file (open())repeatedly reading chunks from the source and writing them to the destination until complete (read()/ write())finally, closing both files close().Each of these steps requires that expensive mode switch between user mode and the kernel.For a typical React app with thousands of package files, this generates hundreds of thousands to millions of system calls! This is exactly the systems programming problem we described earlier: the overhead of making all these system calls becomes more expensive than actually moving the data.Bun uses different strategies depending on your operating system and filesystem, leveraging every OS-specific optimization available. Bun supports several file copying backends, each with different performance characteristics:macOSOn macOS, Bun uses Apple's native clonefile() copy-on-write system call.clonefile can clone entire directory trees in a single system call. This system call creates new directory and file metadata entries that reference the same physical disk blocks as the original files. Instead of writing new data to disk, the filesystem just creates new "pointers" to existing data.// Traditional approach: millions of syscalls
for (each file) {
  copy_file_traditionally(src, dst);  // 50+ syscalls per file
}

// Bun's approach: ONE syscall
clonefile("/cache/react", "/node_modules/react", 0);
SSD stores data in fixed-size blocks. When you normally copy a file (copy()), the filesystem allocates new blocks and writes duplicate data. With clonefile, both the original and "copied" file have metadata that points to the exact same physical blocks on your SSD.Copy-on-write means data is only duplicated when modified. This results in an O(1) operation vs. the O(n) of traditional copying.The metadata of both files point to the same data blocks until you modify one of them.When you modify the contents of one of the files, the filesystem automatically allocates new blocks for the edited parts, and updates the file metadata to point to the new blocks.However, this rarely happens since node_modules files are typically read-only after installation; we don't actively modify modules from within our code.This makes copy-on-write extremely efficient: multiple packages can share identical dependency files without using additional disk space.Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):      2.955 s ±  0.101 s    [User: 0.190 s, System: 1.991 s]
  Range (min … max):    2.825 s …  3.107 s    10 runs

Benchmark 2: bun install --backend=clonefile
  Time (mean ± σ):      1.274 s ±  0.052 s    [User: 0.140 s, System: 0.257 s]
  Range (min … max):    1.184 s …  1.362 s    10 runs

Summary
  bun install --backend=clonefile ran
    2.32 ± 0.12 times faster than bun install --backend=copyfile
When clonefile fails (due to lack of filesystem support), Bun falls back to clonefile_each_dir for per-directory cloning. If that also fails, Bun uses traditional copyfile as the final fallback.LinuxLinux doesn't have clonefile(), but it has something even older and more powerful: hardlinks. Bun implements a fallback chain that tries increasingly less optimal approaches until one works:1. HardlinksOn Linux, Bun's default strategy is hardlinks. A hardlink doesn't create a new file at all, it only creates a new name for an existing file, and references this existing file.link("/cache/react/index.js", "/node_modules/react/index.js");
To understand hardlinks, you need to understand inodes. Every file on Linux has an inode, which is a data structure that contains all the file's metadata (permissions, timestamps, etc.). The filename is just a pointer to an inode:Both paths point to the same inode. If you delete one path, the other remains. However, if you modify one, both see changes (because they're the same file!).This results in great performance gains because there's zero data movement. Creating a hard link requires a single system call that completes in microseconds, regardless of whether you're linking a 1KB file or a 100MB bundle. Much more efficient than traditional copying, which has to read and write every single byte.They're also extremely efficient for disk space, since there's only ever one copy of the actual data on disk, no matter how many packages reference the same dependency filesHowever, hardlinks have limitations. They can't cross filesystem boundaries (e.g. your cache is in a different location than your node_modules), some filesystems don't support them, and certain file types or permission configurations can cause hardlink creation to fail.When hardlinks aren't possible, Bun has some fallbacks:2. ioctl_ficloneIt starts with ioctl_ficlone, which enables copy-on-write on filesystems like Btrfs and XFS. This is very similar to clonefile's copy-on-write system in the way that it also creates a new file references that share the same disk data. Unlike hardlinks, these are separate files; they just happen to share storage until modified.3. copy_file_rangeIf copy-on-write isn't available, Bun tries to at least keep the copying in kernel space and falls back to copy_file_range.In a traditional copy, the kernel reads from disk into a kernel buffer, then copies that data to your program's buffer in user space. Later when you call write(), it copies it back to a kernel buffer before writing to disk. That's four memory operations and multiple context switches!With copy_file_range, the kernel reads from disk into a kernel buffer and writes directly to disk. Just two operations and zero context switches for the data movement.4. sendfileIf that's unavailable, Bun uses sendfile. This is a system call that was originally designed for network transfers, but it's also effective for copying data directly between two files on disk.This command also keeps data in kernel space: the kernel reads data from one destination (a reference to an open file on disk, e.g. a source file in ~/.bun/install/cache/) and writes it to another destination (like a destination file in node_modules), all within the kernel's memory space.This process is called disk-to-disk copying, as it moves data between files stored on the same or different disks without touching your program's memory. It's an older API but more widely supported, making it a reliable fallback when newer system calls aren't available while still reducing the number of memory calls.5. copyfileAs a last resort, Bun uses traditional file copying; the same approach most package managers use. This creates entirely separate copies of each file by reading data from the cache and writing it to the destination using a read()/write() loop. This uses multiple system calls, which is exactly what Bun is trying to minimize. It's the least efficient option, but it's universally compatible.Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):     325.0 ms ±   7.7 ms    [User: 38.4 ms, System: 295.0 ms]
  Range (min … max):   314.2 ms … 340.0 ms    10 runs

Benchmark 2: bun install --backend=hardlink
  Time (mean ± σ):     109.4 ms ±   5.1 ms    [User: 32.0 ms, System: 86.8 ms]
  Range (min … max):   102.8 ms … 119.0 ms    19 runs

Summary
  bun install --backend=hardlink ran
    2.97 ± 0.16 times faster than bun install --backend=copyfile
These file copying optimizations address the primary bottleneck: system call overhead. Instead of using a one-size-fits-all approach, Bun chooses the most efficient file copying specifically tailored to you.Multi-Core ParallelismAll the above-mentioned optimizations are great, but they aim to reduce the workload for a single CPU core. However, modern laptops have 8, 16, even 24 CPU cores!Node.js has a thread pool, but all the actual work (e.g. figuring out which version of React works with which version of webpack, building the dependency graph, deciding what to install) happens on one thread and one CPU core. When npm runs on your M3 Max, one core works really hard while the other 15 are idle.A CPU core can independently execute instructions. Early computers had one core, they could only do one thing at a time, but modern CPUs pack multiple cores onto a single chip. A 16-core CPU can execute 16 different instruction streams simultaneously, not just switching between them really fast.This is yet another fundamental bottleneck for traditional package managers: no matter how many cores you have, the package manager can only use one CPU core.Bun takes a different approach with a lock-free, work-stealing thread pool architecture.Work-stealing means that idle threads can "steal" pending tasks from busy threads' queues. When a thread finishes its work, it checks its local queue, then the global queue, then steals from other threads. No thread sits idle when there's still work to do.Instead of being limited to JavaScript's event loop, Bun spawns native threads that can fully utilize every CPU core. The thread pool automatically scales to match your device's CPU's core count, allowing Bun to maximize parallelizing the I/O-heavy parts of the installation process. One thread can be extracting next's tarball, another is resolving postcss dependencies, a third applying patches to webpack, and so on.But multi-threading often comes with synchronization overhead. Those hundreds of thousands of futex calls npm made were just threads constantly waiting for each other. Each time a thread wants to add a task to a shared queue, it has to lock it first, blocking all other threads.// Traditional approach: Locks
mutex.lock();                   // Thread 1 gets exclusive access
queue.push(task);               // Only Thread 1 can work
mutex.unlock();                 // Finally releases lock
// Problem: Threads 2-8 blocked, waiting in line
Bun uses lock-free data structures instead. These use special CPU instructions called atomic operations that allow threads to safely modify shared data without locks:pub fn push(self: *Queue, batch: Batch) void {
  // Atomic compare-and-swap, happens instantly
  _ = @cmpxchgStrong(usize, &self.state, state, new_state, .seq_cst, .seq_cst);
}
In an earlier benchmark we saw that Bun was able to process 51,685 package.json files/second versus Node.js's 15,471. That's the impact of using all cores instead of one.Bun also runs network operations differently. Traditional package managers often block. When downloading a package, the CPU sits idle waiting for the network.Bun maintains a pool of 64(!) concurrent HTTP connections (configurable via BUN_CONFIG_MAX_HTTP_REQUESTS) on dedicated network threads. The network thread runs independently with its own event loop, handling all downloads while CPU threads handle the extraction and processing. Neither waits for the other.Bun also gives each thread its own memory pool. An issue with "traditional" multi-threading is that all threads compete for the same memory allocator. This creates contention: if 16 threads all need memory at once, they have to wait for each other.// Traditional: all threads share one allocator
Thread 1: "I need 1KB for package data"    // Lock allocator
Thread 2: "I need 2KB for JSON parsing"    // Wait...
Thread 3: "I need 512B for file paths"     // Wait...
Thread 4: "I need 4KB for extraction"      // Wait...
Bun instead gives each thread its own large chunk of pre-allocated memory that the thread manages independently. There's no sharing or waiting, each thread works with its own data whenever possible.// Bun: each thread has its own allocator
Thread 1: Allocates from pool 1    // Instant
Thread 2: Allocates from pool 2    // Instant
Thread 3: Allocates from pool 3    // Instant
Thread 4: Allocates from pool 4    // Instant
ConclusionThe package managers we benchmarked weren't built wrong, they were solutions designed for the constraints of their time.npm gave us a foundation to build on, yarn made managing workspaces less painful, and pnpm came up with a clever way to save space and speed things up with hardlinks. Each worked hard to solve the problems developers were actually hitting at the time.But that world no longer exists. SSDs are 70× faster, CPUs have dozens of cores, and memory is cheap. The real bottleneck shifted from hardware speed to software abstractions.Buns approach wasn't revolutionary, it was just willing to look at what actually slows things down today. When SSDs can handle a million operations per second, why accept thread pool overhead? When you're reading the same package manifest for the hundredth time, why parse JSON again? When the filesystem supports copy-on-write, why duplicate gigabytes of data?The tools that will define the next decade of developer productivity are being written right now, by teams who understand that performance bottlenecks shifted when storage got fast and memory got cheap. They're not just incrementally improving what exists; they're rethinking what's possible.Installing packages 25x faster isn't "magic": it's what happens when tools are built for the hardware we actually have.→ Experience software built for 2025 at bun.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CPI for all items rises 0.4% in August, 2.9% YoY; shelter and food up]]></title>
            <link>https://www.bls.gov/news.release/archives/cpi_09112025.htm</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210803</guid>
            <description><![CDATA[All items]]></description>
            <content:encoded><![CDATA[
Transmission of material in this release is embargoed until                                        
8:30 a.m. (ET) Thursday, September 11, 2025    USDL-25-1356
	
Technical information: (202) 691-7000  *  cpi_info@bls.gov  *  www.bls.gov/cpi
Media contact:         (202) 691-5902  *  PressOffice@bls.gov 

CONSUMER PRICE INDEX - AUGUST 2025

The Consumer Price Index for All Urban Consumers (CPI-U) increased 0.4 percent on a seasonally adjusted basis in
August, after rising 0.2 percent in July, the U.S. Bureau of Labor Statistics reported today. Over the last 12 months,
the all items index increased 2.9 percent before seasonal adjustment.

The index for shelter rose 0.4 percent in August and was the largest factor in the all items monthly increase. The food
index increased 0.5 percent over the month as the food at home index rose 0.6 percent and the food away from home index
increased 0.3 percent. The index for energy rose 0.7 percent in August as the index for gasoline increased 1.9 percent
over the month.

The index for all items less food and energy rose 0.3 percent in August, as it did in July. Indexes that increased over
the month include airline fares, used cars and trucks, apparel, and new vehicles. The indexes for medical care,
recreation, and communication were among the few major indexes that decreased in August.

The all items index rose 2.9 percent for the 12 months ending August, after rising 2.7 percent over the 12 months
ending July. The all items less food and energy index rose 3.1 percent over the last 12 months. The energy index
increased 0.2 percent for the 12 months ending August. The food index increased 3.2 percent over the last year. 






Table A. Percent changes in CPI for All Urban Consumers (CPI-U): U.S. city average




	
	Seasonally adjusted changes from preceding month
	Un-adjusted12-mos.endedAug. 2025


	Feb.2025
	Mar.2025
	Apr.2025
	May2025
	Jun.2025
	Jul.2025
	Aug.2025






	All items
	0.2
	-0.1
	0.2
	0.1
	0.3
	0.2
	0.4
	2.9



	Food
	0.2
	0.4
	-0.1
	0.3
	0.3
	0.0
	0.5
	3.2



	Food at home
	0.0
	0.5
	-0.4
	0.3
	0.3
	-0.1
	0.6
	2.7



	Food away from home(1)
	0.4
	0.4
	0.4
	0.3
	0.4
	0.3
	0.3
	3.9



	Energy
	0.2
	-2.4
	0.7
	-1.0
	0.9
	-1.1
	0.7
	0.2



	Energy commodities
	-0.9
	-6.1
	-0.2
	-2.4
	1.0
	-1.9
	1.7
	-6.2



	Gasoline (all types)
	-1.0
	-6.3
	-0.1
	-2.6
	1.0
	-2.2
	1.9
	-6.6



	Fuel oil
	0.8
	-4.2
	-1.3
	0.9
	1.3
	1.8
	-0.3
	-0.5



	Energy services
	1.4
	1.6
	1.5
	0.4
	0.9
	-0.3
	-0.2
	7.7



	Electricity
	1.0
	0.9
	0.8
	0.9
	1.0
	-0.1
	0.2
	6.2



	Utility (piped) gas service
	2.5
	3.6
	3.7
	-1.0
	0.5
	-0.9
	-1.6
	13.8



	All items less food and energy
	0.2
	0.1
	0.2
	0.1
	0.2
	0.3
	0.3
	3.1



	Commodities less food and energy commodities
	0.2
	-0.1
	0.1
	0.0
	0.2
	0.2
	0.3
	1.5



	New vehicles
	-0.1
	0.1
	0.0
	-0.3
	-0.3
	0.0
	0.3
	0.7



	Used cars and trucks
	0.9
	-0.7
	-0.5
	-0.5
	-0.7
	0.5
	1.0
	6.0



	Apparel
	0.6
	0.4
	-0.2
	-0.4
	0.4
	0.1
	0.5
	0.2



	Medical care commodities(1)
	0.1
	-1.1
	0.4
	0.6
	0.1
	0.1
	-0.3
	0.0



	Services less energy services
	0.3
	0.1
	0.3
	0.2
	0.3
	0.4
	0.3
	3.6



	Shelter
	0.3
	0.2
	0.3
	0.3
	0.2
	0.2
	0.4
	3.6



	Transportation services
	-0.8
	-1.4
	0.1
	-0.2
	0.2
	0.8
	1.0
	3.5



	Medical care services
	0.3
	0.5
	0.5
	0.2
	0.6
	0.8
	-0.1
	4.2







	
		Footnotes
		(1) Not seasonally adjusted.
	






 
Food

The index for food rose 0.5 percent in August, after being unchanged in July. The food at home index increased 0.6
percent over the month. All six major grocery store food group indexes increased in August. The index for fruits and
vegetables rose 1.6 percent over the month as the index for tomatoes increased 4.5 percent and the index for apples
rose 3.5 percent. The meats, poultry, fish and eggs index increased 1.0 percent in August with the beef index rising
2.7 percent. The index for nonalcoholic beverages increased 0.6 percent and the index for other food at home increased
0.1 percent. Both the dairy and related products index and the cereals and bakery products index also rose 0.1 percent
in August.

The food away from home index rose 0.3 percent in August. The index for full service meals rose 0.4 percent over the
month and the index for limited service meals increased 0.1 percent.

The index for food at home rose 2.7 percent over the 12 months ending in August. The meats, poultry, fish, and eggs
index rose 5.6 percent over the last 12 months. The index for nonalcoholic beverages increased 4.6 percent over the
same period and the index for other food at home rose 1.5 percent. The fruits and vegetables index increased 1.9
percent over the 12 months ending in August. The index for cereals and bakery products rose 1.1 percent and the index
for dairy and related products increased 1.3 percent over the same period.

The food away from home index rose 3.9 percent over the last year. The index for full service meals rose 4.6 percent
and the index for limited service meals rose 3.2 percent over the same period. 

Energy

The index for energy increased 0.7 percent in August, after falling 1.1 percent in July. The gasoline index increased
1.9 percent over the month. (Before seasonal adjustment, gasoline prices increased 0.3 percent in August.) The index
for electricity increased 0.2 percent over the month while the index for natural gas decreased 1.6 percent over the
same period.

The index for energy increased 0.2 percent over the past 12 months. The gasoline index fell 6.6 percent over this
12-month span and the fuel oil index fell 0.5 percent over the same period. In contrast, the index for electricity
increased 6.2 percent over the last 12 months and the index for natural gas rose 13.8 percent. 

All items less food and energy

The index for all items less food and energy rose 0.3 percent in August, as it did in July. The shelter index
increased 0.4 percent over the month. The index for owners' equivalent rent rose 0.4 percent in August and the index
for rent increased 0.3 percent. The lodging away from home index rose 2.3 percent over the month.

The index for airline fares increased 5.9 percent over the month, after rising 4.0 percent in July. The used cars and
trucks index rose 1.0 percent in August and the apparel index rose 0.5 percent. The index for new vehicles rose 0.3
percent over the month and the index for household furnishings and operations increased 0.2 percent. The recreation
index and the communication index both declined 0.1 percent in August. 

The medical care index decreased 0.2 percent over the month, following a 0.7-percent increase in July. The index for
dental services decreased 0.7 percent in August and the index for prescription drugs declined 0.2 percent. The
physicians' services index increased 0.3 percent over the month, while the hospital services index was unchanged.

The index for all items less food and energy rose 3.1 percent over the past 12 months. The shelter index increased 3.6
percent over the last year. Other indexes with notable increases over the last year include medical care (+3.4 percent),
household furnishings and operations (+3.9 percent), used cars and trucks (+6.0 percent), and motor vehicle insurance
(+4.7 percent).

Not seasonally adjusted CPI measures

The Consumer Price Index for All Urban Consumers (CPI-U) increased 2.9 percent over the last 12 months to an index
level of 323.976 (1982-84=100). For the month, the index increased 0.3 percent prior to seasonal adjustment.  

The Consumer Price Index for Urban Wage Earners and Clerical Workers (CPI-W) increased 2.8 percent over the last 12
months to an index level of 317.306 (1982-84=100). For the month, the index increased 0.3 percent prior to seasonal
adjustment.  

The Chained Consumer Price Index for All Urban Consumers (C-CPI-U) increased 2.7 percent over the last 12 months. For
the month, the index increased 0.3 percent on a not seasonally adjusted basis. Please note that the indexes for the
past 10 to 12 months are subject to revision. 
_______________
The Consumer Price Index for September 2025 is scheduled to be released on Wednesday, October 15, 2025,
at 8:30 a.m. (ET).

------------------------------------------------------------------------------------------------------------
		  		Changes to the health insurance index
			
With the release of October 2025 data on November 13, 2025, the Bureau of Labor Statistics (BLS) will remove
long-term care (LTC) insurance from the health insurance index. Changes in the market for LTC insurance have
made it out of scope and ineligible for pricing in the CPI market basket.
------------------------------------------------------------------------------------------------------------



Technical Note

Brief Explanation of the CPI
The Consumer Price Index (CPI) measures the change in prices paid by consumers for goods and services.
The CPI reflects spending patterns for each of two population groups: all urban consumers and urban
wage earners and clerical workers. The all urban consumer group represents over 90 percent of the total
U.S. population. It is based on the expenditures of almost all residents of urban or metropolitan areas,
including professionals, the self-employed, the poor, the unemployed, and retired people, as well as
urban wage earners and clerical workers. Not included in the CPI are the spending patterns of people
living in rural nonmetropolitan areas, farming families, people in the Armed Forces, and those in
institutions, such as prisons and mental hospitals. Consumer inflation for all urban consumers is
measured by two indexes, namely, the Consumer Price Index for All Urban Consumers (CPI-U) and the
Chained Consumer Price Index for All Urban Consumers (C-CPI-U). The Consumer Price Index for Urban Wage
Earners and Clerical Workers (CPI-W) is based on the expenditures of households included in the CPI-U
definition that meet two requirements: more than one-half of the household's income must come from
clerical or wage occupations, and at least one of the household's earners must have been employed for
at least 37 weeks during the previous 12 months. The CPI-W population represents approximately 30
percent of the total U.S. population and is a subset of the CPI-U population.

The CPIs are based on prices of food, clothing, shelter, fuels, transportation, doctors' and dentists'
services, drugs, and other goods and services that people buy for day-to-day living. Prices are
collected each month in 75 urban areas across the country from about 6,000 housing units and
approximately 22,000 retail establishments (department stores, supermarkets, hospitals, filling
stations, and other types of stores and service establishments). All taxes directly associated with the
purchase and use of items are included in the index. Prices of fuels and a few other items are obtained
every month in all 75 locations. Prices of most other commodities and services are collected every
month in the three largest geographic areas and every other month in other areas. Prices of most goods
and services are obtained by personal visit, telephone call, web, or app collection by the Bureau's
trained representatives.

In calculating the index, price changes for the various items in each location are aggregated using
weights, which represent their importance in the spending of the appropriate population group. Local
data are then combined to obtain a U.S. city average. For the CPI-U and CPI-W, separate indexes are
also published by size of city, by region of the country, for cross-classifications of regions and
population-size classes, and for 23 selected local areas. Area indexes do not measure differences in
the level of prices among cities; they only measure the average change in prices for each area since
the base period. For the C-CPI-U, data are issued only at the national level. The CPI-U and CPI-W are
considered final when released, but the C-CPI-U is issued in preliminary form and subject to three
subsequent quarterly revisions.

The index measures price change from a designed reference date. For most of the CPI-U and the CPI-W,
the reference base is 1982-84 equals 100. The reference base for the C-CPI-U is December 1999 equals
100.  An increase of 7 percent from the reference base, for example, is shown as 107.000. Alternatively,
that relationship can also be expressed as the price of a base period market basket of goods and
services rising from $100 to $107. 

Sampling Error in the CPI

The CPI is a statistical estimate that is subject to sampling error because it is based upon a sample
of retail prices and not the complete universe of all prices. BLS calculates and publishes estimates
of the 1-month, 2-month, 6-month, and 12-month percent change standard errors annually for the CPI-U.
These standard error estimates can be used to construct confidence intervals for hypothesis testing.
For example, the estimated standard error of the 1-month percent change is 0.03 percent for the U.S.
all items CPI. This means that if we repeatedly sample from the universe of all retail prices using
the same methodology, and estimate a percentage change for each sample, then 95 percent of these
estimates will be within 0.06 percent of the 1-month percentage change based on all retail prices. For
example, for a 1-month change of 0.2 percent in the all items CPI-U, we are 95 percent confident that
the actual percent change based on all retail prices would fall between 0.14 and 0.26 percent. For the
latest data, including information on how to use the estimates of standard error, 
see www.bls.gov/cpi/tables/variance-estimates/home.htm. 

Calculating Index Changes

Movements of the indexes from 1 month to another are usually expressed as percent changes rather than
changes in index points, because index point changes are affected by the level of the index in relation
to its base period, while percent changes are not. The following table shows an example of using index
values to calculate percent changes:

                            Item A                  Item B                      Item C
Year I                      112.500                 225.000                     110.000
Year II                     121.500                 243.000                     128.000
Change in index points      9.000                   18.000                      18.000
Percent change              9.0/112.500 x 100 = 8.0  18.0/225.000 x 100 = 8.0   18.0/110.000 x 100 = 16.4

Use of Seasonally Adjusted and Unadjusted Data

The Consumer Price Index (CPI) program produces both unadjusted and seasonally adjusted data. Seasonally adjusted
data are computed using seasonal factors derived by the X-13ARIMA-SEATS seasonal adjustment method. These factors
are updated each February, and the new factors are used to revise the previous 5 years of seasonally adjusted data.
The factors are available at www.bls.gov/cpi/tables/seasonal-adjustment/seasonal-factors-2025.xlsx. For more
information on data revision scheduling, please see the Factsheet on Seasonal Adjustment at
www.bls.gov/cpi/seasonal-adjustment/questions-and-answers.htm and the Timeline of Seasonal Adjustment
Methodological Changes at www.bls.gov/cpi/seasonal-adjustment/timeline-seasonal-adjustment-methodology-changes.htm.

How to Use Seasonally Adjusted and Unadjusted Data

For analyzing short-term price trends in the economy, seasonally adjusted changes are usually preferred since they
eliminate the effect of changes that normally occur at the same time and in about the same magnitude every 
year-such as price movements resulting from weather events, production cycles, model changeovers, holidays, and
sales. This allows data users to focus on changes that are not typical for the time of year. 

The unadjusted data are of primary interest to consumers concerned about the prices they actually pay. Unadjusted
data are also used extensively for escalation purposes. Many collective bargaining contract agreements and pension
plans, for example, tie compensation changes to the Consumer Price Index before adjustment for seasonal variation.
BLS advises against the use of seasonally adjusted data in escalation agreements because seasonally adjusted series
are revised annually for five years.

Intervention Analysis

The Bureau of Labor Statistics uses intervention analysis seasonal adjustment (IASA) for some CPI series. Sometimes
extreme values or sharp movements can distort the underlying seasonal pattern of price change. Intervention
analysis seasonal adjustment is a process by which the distortions caused by such unusual events are estimated and
removed from the data prior to calculation of seasonal factors. The resulting seasonal factors, which more
accurately represent the seasonal pattern, are then applied to the unadjusted data.

For example, this procedure was used for the motor fuel series to offset the effects of the 2009 return to normal
pricing after the worldwide economic downturn in 2008. Retaining this outlier data during seasonal factor
calculation would distort the computation of the seasonal portion of the time series data for motor fuel, so it was
estimated and removed from the data prior to seasonal adjustment. Following that, seasonal factors were calculated
based on this "prior adjusted" data. These seasonal factors represent a clearer picture of the seasonal pattern in
the data. The last step is for motor fuel seasonal factors to be applied to the unadjusted data.

For the seasonal factors introduced for January 2025, BLS adjusted 63 series using intervention analysis seasonal
adjustment, including selected food and beverage items, motor fuels and vehicles.

Revision of Seasonally Adjusted Indexes

Seasonally adjusted data, including the U.S. city average all items index levels, are subject to revision for up to
5 years after their original release. Every year, economists in the CPI calculate new seasonal factors for
seasonally adjusted series and apply them to the last 5 years of data. Seasonally adjusted indexes beyond the last
5 years of data are considered to be final and not subject to revision. For January 2025, revised seasonal factors
and seasonally adjusted indexes for 2020 to 2024 were calculated and published. For series which are directly
adjusted using the Census X-13ARIMA-SEATS seasonal adjustment software, the seasonal factors for 2024 will be
applied to data for 2025 to produce the seasonally adjusted 2025 indexes. Series which are indirectly seasonally
adjusted by summing seasonally adjusted component series have seasonal factors which are derived and are therefore
not available in advance.

Determining Seasonal Status

Each year the seasonal status of every series is reevaluated based upon certain statistical criteria. Using these
criteria, BLS economists determine whether a series should change its status from "not seasonally adjusted" to
"seasonally adjusted", or vice versa. If any of the 81 components of the U.S. city average all items index change
their seasonal adjustment status from seasonally adjusted to not seasonally adjusted, not seasonally adjusted data
will be used in the aggregation of the dependent series for the last 5 years, but the seasonally adjusted indexes
before that period will not be changed. For 2025, 34 of the 81 components of the U.S. city average all items index
are not seasonally adjusted.

Contact Information

For additional information about the CPI visit www.bls.gov/cpi or contact the CPI Information and Analysis Section
at 202-691-7000 or cpi_info@bls.gov. 

For additional information on seasonal adjustment in the CPI visit www.bls.gov/cpi/seasonal-adjustment/home.htm

If you are deaf, hard of hearing, or have a speech disability, please dial 7-1-1 to access telecommunications relay
services.





Table 1. Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, by expenditure category, August 2025
[1982-84=100, unless otherwise noted]




	Expenditure category
	RelativeimportanceJul.2025
	Unadjusted indexes
	Unadjusted percent change
	Seasonally adjusted percent change


	Aug.2024
	Jul.2025
	Aug.2025
	Aug.2024-Aug.2025
	Jul.2025-Aug.2025
	May2025-Jun.2025
	Jun.2025-Jul.2025
	Jul.2025-Aug.2025






	All items
	100.000
	314.796
	323.048
	323.976
	2.9
	0.3
	0.3
	0.2
	0.4



	Food
	13.635
	330.750
	340.036
	341.295
	3.2
	0.4
	0.3
	0.0
	0.5



	Food at home
	7.982
	306.402
	313.263
	314.608
	2.7
	0.4
	0.3
	-0.1
	0.6



	Cereals and bakery products
	1.095
	355.652
	360.048
	359.740
	1.1
	-0.1
	-0.2
	-0.2
	0.1



	Meats, poultry, fish, and eggs
	1.633
	329.108
	344.155
	347.509
	5.6
	1.0
	-0.1
	0.2
	1.0



	Dairy and related products(1)
	0.727
	269.468
	272.586
	272.840
	1.3
	0.1
	-0.3
	0.7
	0.1



	Fruits and vegetables
	1.299
	349.599
	351.764
	356.104
	1.9
	1.2
	0.9
	0.0
	1.6



	Nonalcoholic beverages and beverage materials
	0.905
	219.381
	228.011
	229.406
	4.6
	0.6
	1.4
	-0.5
	0.6



	Other food at home
	2.323
	272.612
	277.002
	276.649
	1.5
	-0.1
	0.2
	-0.5
	0.1



	Food away from home(1)
	5.653
	370.348
	383.808
	384.909
	3.9
	0.3
	0.4
	0.3
	0.3



	


	Energy
	6.422
	282.614
	283.395
	283.247
	0.2
	-0.1
	0.9
	-1.1
	0.7



	Energy commodities
	3.167
	308.297
	288.425
	289.117
	-6.2
	0.2
	1.0
	-1.9
	1.7



	Fuel oil
	0.075
	349.383
	352.435
	347.565
	-0.5
	-1.4
	1.3
	1.8
	-0.3



	Motor fuel
	3.033
	303.089
	282.696
	283.504
	-6.5
	0.3
	1.0
	-2.0
	1.8



	Gasoline (all types)
	2.949
	302.419
	281.490
	282.358
	-6.6
	0.3
	1.0
	-2.2
	1.9



	Energy services
	3.255
	268.419
	290.131
	289.154
	7.7
	-0.3
	0.9
	-0.3
	-0.2



	Electricity
	2.475
	281.333
	299.107
	298.738
	6.2
	-0.1
	1.0
	-0.1
	0.2



	Utility (piped) gas service
	0.780
	224.259
	257.867
	255.253
	13.8
	-1.0
	0.5
	-0.9
	-1.6



	


	All items less food and energy
	79.943
	320.017
	328.980
	329.970
	3.1
	0.3
	0.2
	0.3
	0.3



	Commodities less food and energy commodities
	19.295
	164.912
	166.766
	167.448
	1.5
	0.4
	0.2
	0.2
	0.3



	Apparel
	2.458
	131.683
	129.190
	131.989
	0.2
	2.2
	0.4
	0.1
	0.5



	New vehicles
	4.316
	177.534
	178.569
	178.698
	0.7
	0.1
	-0.3
	0.0
	0.3



	Used cars and trucks
	2.437
	178.192
	188.183
	188.960
	6.0
	0.4
	-0.7
	0.5
	1.0



	Medical care commodities(1)
	1.511
	416.538
	417.800
	416.721
	0.0
	-0.3
	0.1
	0.1
	-0.3



	Alcoholic beverages(1)
	0.825
	291.432
	295.176
	296.908
	1.9
	0.6
	0.1
	0.1
	0.6



	Tobacco and smoking products(1)
	0.488
	1,562.042
	1,643.672
	1,660.548
	6.3
	1.0
	0.5
	0.3
	1.0



	Services less energy services
	60.648
	418.903
	432.778
	433.930
	3.6
	0.3
	0.3
	0.4
	0.3



	Shelter
	35.434
	403.257
	416.271
	417.902
	3.6
	0.4
	0.2
	0.2
	0.4



	Rent of primary residence
	7.451
	422.223
	435.489
	436.981
	3.5
	0.3
	0.2
	0.3
	0.3



	Owners' equivalent rent of residences(2)
	26.199
	413.924
	428.640
	430.387
	4.0
	0.4
	0.3
	0.3
	0.4



	Medical care services
	6.780
	611.935
	637.425
	637.817
	4.2
	0.1
	0.6
	0.8
	-0.1



	Physicians' services(1)
	1.804
	416.370
	429.279
	430.753
	3.5
	0.3
	0.2
	0.2
	0.3



	Hospital services(1)(3)
	1.971
	 
	437.053
	436.917
	 
	0.0
	0.7
	0.5
	0.0



	Transportation services
	6.257
	431.564
	446.417
	446.688
	3.5
	0.1
	0.2
	0.8
	1.0



	Motor vehicle maintenance and repair(1)
	1.030
	407.374
	431.604
	441.987
	8.5
	2.4
	0.2
	1.0
	2.4



	Motor vehicle insurance
	2.818
	854.307
	896.018
	894.075
	4.7
	-0.2
	0.1
	0.1
	0.0



	Airline fares
	0.847
	243.011
	247.859
	250.982
	3.3
	1.3
	-0.1
	4.0
	5.9







	
		Footnotes
		(1) Not seasonally adjusted.
		(2) Indexes on a December 1982=100 base.
		(3) Indexes on a December 1996=100 base.
	










Table 2. Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, by detailed expenditure category, August 2025
[1982-84=100, unless otherwise noted]




	Expenditure category
	RelativeimportanceJul.2025
	Unadjusted percent change
	Seasonally adjusted percent change


	Aug.2024-Aug.2025
	Jul.2025-Aug.2025
	May2025-Jun.2025
	Jun.2025-Jul.2025
	Jul.2025-Aug.2025






	All items
	100.000
	2.9
	0.3
	0.3
	0.2
	0.4



	Food
	13.635
	3.2
	0.4
	0.3
	0.0
	0.5



	Food at home
	7.982
	2.7
	0.4
	0.3
	-0.1
	0.6



	Cereals and bakery products
	1.095
	1.1
	-0.1
	-0.2
	-0.2
	0.1



	Cereals and cereal products
	0.328
	-1.2
	-0.3
	-1.1
	-0.9
	0.3



	Flour and prepared flour mixes
	0.030
	0.3
	0.1
	-1.2
	-1.9
	0.4



	Breakfast cereal(1)
	0.143
	-1.0
	-0.6
	-0.3
	-0.7
	-0.6



	Rice, pasta, cornmeal
	0.156
	-0.8
	-0.2
	-1.2
	-0.6
	0.3



	Rice(1)(2)(3)
	 
	-0.2
	1.5
	-0.5
	-1.1
	1.5



	Bakery products(1)
	0.767
	2.2
	0.0
	0.2
	0.2
	0.0



	Bread(1)(2)
	0.140
	1.2
	-0.2
	0.1
	0.4
	-0.2



	White bread(1)(3)
	 
	-0.2
	0.4
	0.5
	-0.1
	0.4



	Bread other than white(1)(3)
	 
	2.9
	-0.9
	-0.7
	1.2
	-0.9



	Fresh biscuits, rolls, muffins(2)
	0.134
	3.0
	0.3
	-0.1
	-0.2
	-0.5



	Cakes, cupcakes, and cookies(1)
	0.214
	2.9
	-0.4
	-0.1
	1.7
	-0.4



	Cookies(1)(3)
	 
	3.5
	-1.0
	-1.7
	4.2
	-1.0



	Fresh cakes and cupcakes(1)(3)
	 
	2.1
	-0.1
	1.6
	-0.4
	-0.1



	Other bakery products
	0.279
	1.5
	0.3
	0.7
	-1.0
	0.4



	Fresh sweetrolls, coffeecakes, doughnuts(1)(3)
	 
	0.3
	-2.3
	0.8
	-1.8
	-2.3



	Crackers, bread, and cracker products(3)
	 
	0.8
	0.4
	1.1
	-1.2
	1.0



	Frozen and refrigerated bakery products, pies, tarts, turnovers(3)
	 
	-1.0
	-0.1
	1.8
	-1.8
	0.4



	Meats, poultry, fish, and eggs
	1.633
	5.6
	1.0
	-0.1
	0.2
	1.0



	Meats, poultry, and fish
	1.478
	5.4
	1.1
	0.8
	0.7
	1.1



	Meats
	0.948
	7.3
	1.7
	1.0
	1.0
	1.8



	Beef and veal
	0.479
	13.9
	2.7
	2.0
	1.5
	2.7



	Uncooked ground beef(1)
	0.216
	12.8
	2.3
	1.5
	2.4
	2.3



	Uncooked beef roasts(2)
	0.060
	13.6
	3.1
	2.4
	1.4
	4.0



	Uncooked beef steaks(2)
	0.140
	16.6
	3.3
	3.2
	2.3
	4.1



	Uncooked other beef and veal(1)(2)
	0.064
	11.4
	2.3
	1.5
	2.2
	2.3



	Pork
	0.288
	1.2
	0.2
	-0.3
	0.5
	0.2



	Bacon, breakfast sausage, and related products(2)
	0.104
	5.4
	1.1
	1.3
	-0.5
	1.2



	Bacon and related products(3)
	 
	7.2
	0.8
	0.6
	0.6
	1.1



	Breakfast sausage and related products(2)(3)
	 
	3.7
	1.6
	1.3
	-1.2
	2.5



	Ham
	0.044
	-1.9
	-4.0
	-2.5
	3.7
	-4.9



	Ham, excluding canned(3)
	 
	-2.1
	-4.0
	-2.7
	3.9
	-5.2



	Pork chops(1)
	0.050
	-2.4
	-1.3
	1.3
	2.0
	-1.3



	Other pork including roasts, steaks, and ribs(2)
	0.090
	-0.5
	2.1
	-0.7
	0.3
	1.5



	Other meats
	0.181
	1.3
	1.3
	0.5
	0.2
	2.2



	Frankfurters(3)
	 
	0.0
	1.7
	9.0
	-1.1
	-0.5



	Lunchmeats(1)(2)(3)
	 
	-0.6
	0.0
	-2.1
	1.9
	0.0



	Poultry
	0.289
	1.7
	-0.4
	0.6
	-0.1
	-0.7



	Chicken(2)
	0.216
	2.8
	0.2
	1.1
	-0.4
	0.1



	Fresh whole chicken(3)
	 
	-0.2
	-0.6
	0.5
	0.0
	-0.5



	Fresh and frozen chicken parts(3)
	 
	4.2
	0.5
	1.3
	-0.4
	0.3



	Other uncooked poultry including turkey(2)
	0.073
	-2.7
	-2.0
	-0.4
	0.4
	-2.8



	Fish and seafood
	0.241
	2.3
	0.6
	0.1
	0.4
	0.6



	Fresh fish and seafood(1)(2)
	0.140
	2.7
	0.3
	0.7
	-0.5
	0.3



	Processed fish and seafood(2)
	0.101
	2.0
	1.0
	0.0
	0.1
	0.8



	Shelf stable fish and seafood(3)
	 
	-1.0
	0.0
	1.3
	-0.6
	0.1



	Frozen fish and seafood(3)
	 
	6.7
	2.0
	-1.4
	1.1
	1.7



	Eggs
	0.155
	10.9
	-0.2
	-7.4
	-3.9
	0.0



	Dairy and related products(1)
	0.727
	1.3
	0.1
	-0.3
	0.7
	0.1



	Milk(1)(2)
	0.196
	1.7
	-0.2
	-0.7
	1.9
	-0.2



	Fresh whole milk(1)(3)
	 
	0.5
	0.1
	-1.3
	2.5
	0.1



	Fresh milk other than whole(1)(2)(3)
	 
	2.3
	-0.3
	-0.4
	1.5
	-0.3



	Cheese and related products(1)
	0.251
	2.7
	0.4
	0.2
	-0.4
	0.4



	Ice cream and related products
	0.127
	-0.5
	1.0
	0.2
	-0.3
	0.3



	Other dairy and related products(2)
	0.153
	0.6
	-0.7
	-1.1
	1.3
	-0.1



	Fruits and vegetables
	1.299
	1.9
	1.2
	0.9
	0.0
	1.6



	Fresh fruits and vegetables
	1.075
	2.3
	1.7
	1.0
	-0.1
	2.0



	Fresh fruits
	0.541
	1.7
	0.6
	1.3
	-1.4
	1.0



	Apples
	0.082
	9.6
	4.4
	0.0
	-1.7
	3.5



	Bananas(1)
	0.111
	6.6
	2.1
	-0.9
	0.4
	2.1



	Citrus fruits(2)
	0.064
	3.1
	1.4
	2.3
	2.0
	1.2



	Oranges, including tangerines(3)
	 
	5.2
	1.8
	3.5
	2.1
	0.9



	Other fresh fruits(2)
	0.284
	-0.2
	-1.3
	3.2
	-2.6
	-0.2



	Fresh vegetables
	0.534
	2.9
	2.8
	0.6
	1.2
	3.0



	Potatoes
	0.085
	2.2
	2.4
	0.2
	-2.3
	1.8



	Lettuce
	0.068
	3.8
	1.8
	1.1
	4.0
	3.5



	Tomatoes
	0.084
	-1.2
	5.7
	-1.5
	3.3
	4.5



	Other fresh vegetables
	0.296
	4.0
	2.3
	0.5
	1.4
	2.9



	Processed fruits and vegetables(2)
	0.224
	1.4
	-0.9
	0.6
	0.6
	-0.5



	Canned fruits and vegetables(2)
	0.091
	4.0
	0.1
	0.7
	0.0
	0.5



	Canned fruits(2)(3)
	 
	4.3
	0.3
	0.9
	0.6
	0.5



	Canned vegetables(2)(3)
	 
	3.9
	0.1
	0.6
	-0.3
	0.2



	Frozen fruits and vegetables(2)
	0.067
	-0.6
	-0.9
	1.2
	1.4
	-0.6



	Frozen vegetables(3)
	 
	-2.5
	-1.0
	1.2
	1.2
	-0.5



	Other processed fruits and vegetables including dried(2)
	0.065
	0.4
	-2.4
	0.0
	0.8
	-1.8



	Dried beans, peas, and lentils(1)(2)(3)
	 
	1.0
	-3.0
	-0.2
	0.7
	-3.0



	Nonalcoholic beverages and beverage materials
	0.905
	4.6
	0.6
	1.4
	-0.5
	0.6



	Juices and nonalcoholic drinks(2)
	0.626
	1.5
	-0.2
	1.7
	-1.3
	-0.3



	Carbonated drinks
	0.345
	2.0
	0.5
	1.7
	-1.2
	-0.3



	Frozen noncarbonated juices and drinks(1)(2)
	0.004
	7.1
	-0.2
	1.3
	5.3
	-0.2



	Nonfrozen noncarbonated juices and drinks(2)
	0.277
	-0.1
	-1.0
	2.0
	-1.7
	-0.7



	Beverage materials including coffee and tea(2)
	0.279
	12.1
	2.3
	0.7
	1.2
	2.8



	Coffee
	0.148
	20.9
	3.1
	2.2
	2.3
	3.6



	Roasted coffee(3)
	 
	21.7
	3.1
	1.3
	2.1
	4.1



	Instant coffee(1)(3)
	 
	20.1
	4.9
	5.1
	1.6
	4.9



	Other beverage materials including tea(1)(2)
	0.131
	2.4
	1.5
	-0.5
	0.0
	1.5



	Other food at home
	2.323
	1.5
	-0.1
	0.2
	-0.5
	0.1



	Sugar and sweets
	0.335
	5.3
	0.9
	0.9
	-0.2
	0.7



	Sugar and sugar substitutes
	0.028
	2.6
	1.0
	-0.5
	-0.9
	0.6



	Candy and chewing gum(2)
	0.235
	8.1
	1.0
	1.0
	-0.2
	0.7



	Other sweets(2)
	0.072
	-1.1
	0.8
	0.7
	-0.9
	0.6



	Fats and oils
	0.243
	-1.1
	0.1
	0.4
	-1.5
	0.2



	Butter and margarine(2)
	0.054
	-0.4
	0.3
	0.0
	-0.7
	0.2



	Butter(3)
	 
	0.1
	0.9
	-0.4
	-1.0
	1.3



	Margarine(3)
	 
	2.9
	-0.9
	-0.7
	0.0
	-1.6



	Salad dressing(1)(2)
	0.065
	2.5
	1.3
	-0.2
	1.4
	1.3



	Other fats and oils including peanut butter(2)
	0.124
	-3.1
	-0.6
	0.0
	-2.3
	0.3



	Peanut butter(1)(2)(3)
	 
	-1.8
	-1.7
	2.2
	-2.6
	-1.7



	Other foods
	1.745
	1.2
	-0.4
	0.0
	-0.4
	-0.1



	Soups
	0.110
	3.4
	1.1
	1.1
	-1.1
	1.8



	Frozen and freeze dried prepared foods
	0.274
	0.4
	-1.9
	0.4
	-0.8
	-1.2



	Snacks
	0.359
	1.3
	0.9
	-0.6
	-0.7
	1.0



	Spices, seasonings, condiments, sauces
	0.384
	1.7
	1.2
	1.4
	-0.5
	0.4



	Salt and other seasonings and spices(2)(3)
	 
	0.2
	0.8
	1.8
	-1.5
	0.9



	Olives, pickles, relishes(2)(3)
	 
	1.7
	-0.1
	4.9
	-1.3
	-1.2



	Sauces and gravies(2)(3)
	 
	1.1
	0.9
	1.0
	0.4
	-0.1



	Other condiments(3)
	 
	9.3
	1.7
	0.0
	1.6
	0.3



	Baby food and formula(1)(2)
	0.059
	0.5
	-0.9
	0.9
	-1.1
	-0.9



	Other miscellaneous foods(2)
	0.558
	0.5
	-1.7
	-1.0
	0.4
	-1.0



	Prepared salads(3)(4)
	 
	3.8
	0.4
	-2.7
	0.1
	1.7



	Food away from home(1)
	5.653
	3.9
	0.3
	0.4
	0.3
	0.3



	Full service meals and snacks(1)(2)
	2.450
	4.6
	0.4
	0.5
	0.5
	0.4



	Limited service meals and snacks(1)(2)
	2.832
	3.2
	0.1
	0.2
	0.1
	0.1



	Food at employee sites and schools(1)(2)
	0.074
	5.8
	-0.8
	2.7
	0.8
	-0.8



	Food at elementary and secondary schools(1)(3)(5)
	 
	 
	 
	 
	 
	 



	Food from vending machines and mobile vendors(1)(2)
	0.057
	4.8
	0.1
	0.0
	0.0
	0.1



	Other food away from home(1)(2)
	0.241
	5.4
	1.7
	0.5
	0.0
	1.7



	


	Energy
	6.422
	0.2
	-0.1
	0.9
	-1.1
	0.7



	Energy commodities
	3.167
	-6.2
	0.2
	1.0
	-1.9
	1.7



	Fuel oil and other fuels
	0.134
	-0.8
	-0.8
	1.0
	1.0
	-1.1



	Fuel oil
	0.075
	-0.5
	-1.4
	1.3
	1.8
	-0.3



	Propane, kerosene, and firewood(6)
	0.059
	-2.2
	-0.1
	1.1
	-1.1
	-0.2



	Motor fuel
	3.033
	-6.5
	0.3
	1.0
	-2.0
	1.8



	Gasoline (all types)
	2.949
	-6.6
	0.3
	1.0
	-2.2
	1.9



	Gasoline, unleaded regular(3)
	 
	-7.1
	0.3
	1.0
	-2.2
	2.0



	Gasoline, unleaded midgrade(3)(7)
	 
	-5.4
	0.2
	0.9
	-2.0
	1.8



	Gasoline, unleaded premium(3)
	 
	-4.2
	0.3
	0.0
	-1.1
	1.5



	Other motor fuels(1)(2)
	0.084
	-0.4
	-0.5
	0.7
	4.5
	-0.5



	Energy services
	3.255
	7.7
	-0.3
	0.9
	-0.3
	-0.2



	Electricity
	2.475
	6.2
	-0.1
	1.0
	-0.1
	0.2



	Utility (piped) gas service
	0.780
	13.8
	-1.0
	0.5
	-0.9
	-1.6



	


	All items less food and energy
	79.943
	3.1
	0.3
	0.2
	0.3
	0.3



	Commodities less food and energy commodities
	19.295
	1.5
	0.4
	0.2
	0.2
	0.3



	Household furnishings and supplies(8)
	3.392
	2.8
	0.2
	1.0
	0.7
	0.1



	Window and floor coverings and other linens(2)
	0.247
	1.3
	-3.7
	4.2
	1.2
	-5.6



	Floor coverings(1)(2)
	0.059
	0.1
	-1.2
	2.2
	0.7
	-1.2



	Window coverings(1)(2)
	0.055
	0.9
	-0.5
	2.2
	-0.8
	-0.5



	Other linens(2)
	0.133
	2.3
	-6.1
	5.5
	1.4
	-6.7



	Furniture and bedding(1)
	0.791
	4.7
	0.3
	0.4
	0.9
	0.3



	Bedroom furniture(1)
	0.255
	0.1
	-0.4
	0.0
	1.5
	-0.4



	Living room, kitchen, and dining room furniture(1)(2)
	0.387
	9.5
	0.7
	0.4
	1.0
	0.7



	Other furniture(2)
	0.140
	-0.2
	0.6
	1.6
	1.5
	1.8



	Appliances(2)
	0.221
	0.3
	0.3
	1.9
	-0.9
	0.4



	Major appliances(2)
	0.070
	-1.1
	-1.1
	1.9
	-2.2
	-1.1



	Laundry equipment(1)(3)
	 
	0.5
	-2.9
	1.8
	-1.8
	-2.9



	Other appliances(2)
	0.147
	0.9
	1.0
	2.0
	-0.4
	1.4



	Other household equipment and furnishings(2)
	0.494
	2.2
	0.2
	1.7
	-0.2
	-0.2



	Clocks, lamps, and decorator items(1)
	0.290
	1.3
	-0.3
	1.6
	-1.8
	-0.3



	Indoor plants and flowers(9)
	0.112
	5.9
	1.9
	0.3
	1.5
	2.8



	Dishes and flatware(1)(2)
	0.036
	-6.6
	-0.4
	0.2
	2.0
	-0.4



	Nonelectric cookware and tableware(2)
	0.055
	5.6
	-0.1
	3.7
	2.0
	-0.2



	Tools, hardware, outdoor equipment and supplies(1)(2)
	0.848
	3.9
	0.8
	0.2
	1.6
	0.8



	Tools, hardware and supplies(2)
	0.245
	5.8
	0.9
	1.2
	1.2
	0.4



	Outdoor equipment and supplies(1)(2)
	0.366
	2.6
	0.7
	-0.1
	2.2
	0.7



	Housekeeping supplies(1)
	0.792
	1.0
	0.4
	0.8
	0.0
	0.4



	Household cleaning products(1)(2)
	0.293
	-0.5
	0.4
	0.2
	0.1
	0.4



	Household paper products(1)(2)
	0.177
	4.5
	1.2
	1.4
	-0.4
	1.2



	Miscellaneous household products(1)(2)
	0.322
	0.6
	0.0
	1.1
	0.2
	0.0



	Apparel
	2.458
	0.2
	2.2
	0.4
	0.1
	0.5



	Men's and boys' apparel
	0.646
	0.6
	1.6
	0.2
	-1.3
	0.4



	Men's apparel
	0.521
	1.6
	1.8
	0.9
	-1.6
	1.0



	Men's suits, sport coats, and outerwear
	0.087
	3.3
	0.8
	-2.7
	-2.6
	-0.3



	Men's underwear, nightwear, swimwear, and accessories
	0.141
	-1.7
	-1.7
	-0.5
	0.3
	-2.2



	Men's shirts and sweaters(2)
	0.144
	2.0
	5.2
	4.3
	-2.6
	1.3



	Men's pants and shorts
	0.134
	4.2
	2.7
	0.8
	-2.0
	4.2



	Boys' apparel
	0.125
	-2.9
	0.3
	-0.8
	-0.6
	-1.9



	Women's and girls' apparel
	0.954
	-1.5
	3.0
	0.5
	-0.2
	0.2



	Women's apparel
	0.847
	-1.4
	2.9
	0.7
	-0.3
	0.1



	Women's outerwear
	0.074
	5.4
	7.3
	-3.3
	-0.3
	4.4



	Women's dresses
	0.130
	6.2
	5.4
	3.9
	2.7
	1.7



	Women's suits and separates(2)
	0.336
	-3.3
	4.5
	-0.4
	-1.7
	1.1



	Women's underwear, nightwear, swimwear, and accessories(2)
	0.297
	-3.5
	-1.1
	1.6
	0.3
	-2.8



	Girls' apparel
	0.107
	-2.1
	3.5
	-0.8
	0.3
	0.8



	Footwear
	0.573
	1.4
	0.8
	0.7
	1.4
	-0.4



	Men's footwear
	0.198
	-0.2
	-0.6
	2.6
	1.4
	-1.2



	Boys' and girls' footwear(1)
	0.110
	0.9
	1.5
	-1.7
	0.7
	1.5



	Women's footwear
	0.266
	2.8
	1.5
	0.8
	0.4
	0.2



	Infants' and toddlers' apparel
	0.101
	-0.2
	2.5
	0.4
	3.3
	1.0



	Jewelry and watches(6)
	0.184
	6.0
	4.4
	-0.1
	0.8
	5.5



	Watches(1)(6)
	0.041
	5.6
	1.9
	1.8
	-0.8
	1.9



	Jewelry(6)
	0.143
	6.9
	5.1
	-0.4
	1.1
	6.8



	Transportation commodities less motor fuel(8)
	7.253
	2.6
	0.2
	-0.4
	0.2
	0.5



	New vehicles
	4.316
	0.7
	0.1
	-0.3
	0.0
	0.3



	New cars(3)
	 
	1.0
	0.1
	-0.4
	0.0
	0.2



	New trucks(3)(10)
	 
	0.6
	0.1
	-0.3
	0.0
	0.3



	Used cars and trucks
	2.437
	6.0
	0.4
	-0.7
	0.5
	1.0



	Motor vehicle parts and equipment(1)
	0.363
	3.4
	0.6
	0.6
	0.9
	0.6



	Tires(1)
	0.299
	3.9
	0.3
	0.9
	1.0
	0.3



	Vehicle accessories other than tires(1)(2)
	0.064
	2.1
	1.7
	-0.8
	0.1
	1.7



	Vehicle parts and equipment other than tires(1)(3)
	 
	2.1
	2.0
	-0.6
	0.3
	2.0



	Motor oil, coolant, and fluids(1)(3)
	 
	-0.3
	0.8
	-2.2
	0.1
	0.8



	Medical care commodities(1)
	1.511
	0.0
	-0.3
	0.1
	0.1
	-0.3



	Medicinal drugs(1)(8)
	1.332
	-0.2
	-0.4
	0.1
	-0.1
	-0.4



	Prescription drugs(1)
	0.918
	0.9
	-0.2
	0.4
	-0.2
	-0.2



	Nonprescription drugs(8)
	0.413
	-2.4
	-0.9
	-1.0
	-0.5
	-0.9



	Medical equipment and supplies(1)(8)
	0.180
	1.5
	0.6
	0.5
	1.1
	0.6



	Recreation commodities(8)
	1.834
	0.1
	0.0
	0.8
	0.4
	0.0



	Video and audio products(8)
	0.254
	1.5
	0.4
	1.1
	0.8
	0.5



	Televisions
	0.085
	-5.6
	1.4
	-0.1
	0.5
	2.5



	Other video equipment(2)
	0.029
	-0.3
	-0.2
	4.5
	-0.2
	-2.5



	Audio equipment(1)
	0.061
	12.2
	-0.8
	2.9
	2.2
	-0.8



	Recorded music and music subscriptions(1)(2)
	0.074
	3.8
	0.4
	-0.2
	0.6
	0.4



	Pets and pet products(1)
	0.626
	0.1
	-0.5
	0.2
	0.5
	-0.5



	Pet food and treats(1)(2)(3)
	 
	0.2
	-0.3
	0.8
	0.5
	-0.3



	Purchase of pets, pet supplies, accessories(1)(2)(3)
	 
	0.0
	-0.6
	-0.8
	0.0
	-0.6



	Sporting goods(1)
	0.469
	-1.3
	0.1
	1.4
	0.4
	0.1



	Sports vehicles including bicycles(1)
	0.233
	-1.7
	0.6
	1.0
	1.1
	0.6



	Sports equipment(1)
	0.221
	-0.8
	-0.6
	1.8
	-0.4
	-0.6



	Photographic equipment and supplies
	0.020
	4.7
	1.8
	0.7
	1.2
	1.1



	Photographic equipment(1)(2)(3)
	 
	4.7
	1.8
	0.5
	2.1
	1.8



	Recreational reading materials(1)
	0.098
	1.8
	0.9
	-1.0
	-0.3
	0.9



	Newspapers and magazines(1)(2)
	0.055
	2.1
	-1.1
	-4.7
	1.0
	-1.1



	Recreational books(1)(2)
	0.044
	1.4
	3.4
	3.8
	-1.9
	3.4



	Other recreational goods(2)
	0.367
	0.0
	0.3
	1.3
	0.3
	0.0



	Toys
	0.290
	0.1
	-0.4
	1.8
	0.2
	-0.8



	Toys, games, hobbies and playground equipment(2)(3)
	 
	0.6
	-0.4
	1.2
	0.6
	-1.1



	Sewing machines, fabric and supplies(1)(2)
	0.019
	-7.5
	9.1
	-3.7
	2.6
	9.1



	Music instruments and accessories(1)(2)
	0.043
	4.9
	1.1
	0.2
	0.0
	1.1



	Education and communication commodities(8)
	0.734
	-3.8
	0.0
	0.0
	-1.3
	-0.3



	Educational books and supplies(1)
	0.042
	 
	-0.6
	-0.4
	0.1
	-0.6



	College textbooks(1)(3)(11)
	 
	12.2
	-0.2
	-0.8
	0.4
	-0.2



	Information technology commodities(8)
	0.692
	-5.3
	0.1
	0.0
	-1.4
	-0.3



	Computers, peripherals, and smart home assistants(1)(4)
	0.266
	-2.0
	-0.6
	1.4
	-1.2
	-0.6



	Computer software and accessories(1)(2)
	0.027
	-4.6
	-5.5
	-0.2
	-2.6
	-5.5



	Telephone hardware, calculators, and other consumer information items(2)
	0.398
	-7.4
	0.9
	-0.9
	-1.4
	0.3



	Smartphones(1)(3)(12)
	 
	-13.7
	-0.2
	0.0
	0.0
	-0.2



	Alcoholic beverages(1)
	0.825
	1.9
	0.6
	0.1
	0.1
	0.6



	Alcoholic beverages at home
	0.440
	0.3
	0.6
	-0.2
	0.1
	0.6



	Beer, ale, and other malt beverages at home(1)
	0.151
	1.2
	0.6
	-0.2
	0.2
	0.6



	Distilled spirits at home(1)
	0.102
	1.1
	0.6
	-0.2
	0.5
	0.6



	Whiskey at home(1)(3)
	 
	-1.8
	0.5
	-0.3
	0.3
	0.5



	Distilled spirits, excluding whiskey, at home(1)(3)
	 
	2.3
	0.7
	-0.2
	0.6
	0.7



	Wine at home
	0.188
	-0.9
	0.4
	0.0
	-0.3
	0.2



	Alcoholic beverages away from home(1)
	0.385
	3.8
	0.6
	0.4
	0.2
	0.6



	Beer, ale, and other malt beverages away from home(1)(2)(3)
	 
	3.2
	0.5
	0.2
	0.3
	0.5



	Wine away from home(1)(2)(3)
	 
	3.1
	-0.1
	0.9
	0.1
	-0.1



	Distilled spirits away from home(1)(2)(3)
	 
	4.0
	0.1
	0.7
	0.1
	0.1



	Other goods(8)
	1.288
	3.1
	0.2
	0.3
	0.2
	0.4



	Tobacco and smoking products(1)
	0.488
	6.3
	1.0
	0.5
	0.3
	1.0



	Cigarettes(1)(2)
	0.368
	7.7
	1.1
	0.3
	0.8
	1.1



	Tobacco products other than cigarettes(1)(2)
	0.114
	1.5
	0.7
	1.3
	-1.4
	0.7



	Personal care products(1)
	0.642
	1.1
	0.4
	0.1
	0.0
	0.4



	Hair, dental, shaving, and miscellaneous personal care products(1)(2)
	0.286
	1.8
	0.5
	-0.9
	0.2
	0.5



	Cosmetics, perfume, bath, nail preparations and implements(1)
	0.347
	0.3
	0.3
	0.9
	-0.2
	0.3



	Miscellaneous personal goods(2)
	0.158
	1.4
	-2.9
	0.7
	0.8
	-1.8



	Stationery, stationery supplies, gift wrap(3)
	 
	1.1
	-2.9
	-0.3
	0.2
	-0.8



	Services less energy services
	60.648
	3.6
	0.3
	0.3
	0.4
	0.3



	Shelter
	35.434
	3.6
	0.4
	0.2
	0.2
	0.4



	Rent of shelter(13)
	35.014
	3.6
	0.4
	0.2
	0.2
	0.4



	Rent of primary residence
	7.451
	3.5
	0.3
	0.2
	0.3
	0.3



	Lodging away from home(2)
	1.363
	-2.6
	0.3
	-2.9
	-1.0
	2.3



	Housing at school, excluding board(13)
	0.240
	3.5
	1.6
	0.2
	0.1
	0.8



	Other lodging away from home including hotels and motels
	1.123
	-3.7
	0.0
	-3.6
	-1.3
	2.6



	Owners' equivalent rent of residences(13)
	26.199
	4.0
	0.4
	0.3
	0.3
	0.4



	Owners' equivalent rent of primary residence(13)
	25.004
	4.0
	0.4
	0.3
	0.3
	0.4



	Tenants' and household insurance(1)(2)
	0.420
	5.7
	0.6
	1.1
	1.0
	0.6



	Water and sewer and trash collection services(2)
	1.089
	5.3
	0.4
	0.4
	0.4
	0.4



	Water and sewerage maintenance(1)
	0.742
	4.8
	0.3
	0.4
	0.3
	0.3



	Garbage and trash collection(1)(10)
	0.347
	6.5
	0.5
	0.3
	0.6
	0.5



	Household operations(1)(2)
	 
	 
	 
	 
	 
	 



	Domestic services(1)(2)
	 
	 
	 
	 
	 
	 



	Gardening and lawncare services(1)(2)
	 
	 
	 
	 
	 
	 



	Moving, storage, freight expense(2)
	0.129
	1.0
	-2.4
	0.3
	-0.9
	-1.7



	Repair of household items(1)(2)
	 
	 
	 
	 
	 
	 



	Medical care services
	6.780
	4.2
	0.1
	0.6
	0.8
	-0.1



	Professional services
	3.687
	3.5
	0.1
	0.5
	0.7
	0.1



	Physicians' services(1)
	1.804
	3.5
	0.3
	0.2
	0.2
	0.3



	Dental services
	0.957
	4.2
	-0.5
	1.3
	2.6
	-0.7



	Eyeglasses and eye care(1)(6)
	0.331
	3.7
	0.7
	0.5
	0.2
	0.7



	Services by other medical professionals(1)(6)
	 
	 
	 
	0.8
	 
	 



	Hospital and related services(1)
	2.294
	5.3
	0.0
	0.4
	0.4
	0.0



	Hospital services(1)(14)
	1.971
	 
	0.0
	0.7
	0.5
	0.0



	Inpatient hospital services(1)(3)(14)
	 
	 
	 
	 
	 
	 



	Outpatient hospital services(1)(3)(6)
	 
	 
	0.4
	0.6
	0.6
	0.4



	Nursing homes and adult day services(14)
	0.167
	4.5
	0.3
	0.3
	-0.1
	0.3



	Care of invalids and elderly at home(1)(5)
	0.156
	5.6
	-0.2
	-2.6
	0.4
	-0.2



	Health insurance(1)(5)
	0.799
	4.3
	0.1
	0.6
	0.4
	0.1



	Transportation services
	6.257
	3.5
	0.1
	0.2
	0.8
	1.0



	Leased cars and trucks(1)(11)
	0.384
	 
	-0.3
	-0.3
	-0.4
	-0.3



	Car and truck rental(2)
	0.153
	-4.8
	-11.6
	3.2
	-2.9
	-6.9



	Motor vehicle maintenance and repair(1)
	1.030
	8.5
	2.4
	0.2
	1.0
	2.4



	Motor vehicle body work(1)
	 
	 
	 
	0.7
	0.9
	 



	Motor vehicle maintenance and servicing(1)
	0.508
	3.6
	0.4
	0.0
	1.2
	0.4



	Motor vehicle repair(1)(2)
	0.407
	15.0
	5.0
	0.4
	0.8
	5.0



	Motor vehicle insurance
	2.818
	4.7
	-0.2
	0.1
	0.1
	0.0



	Motor vehicle fees(1)(2)
	0.493
	0.8
	-0.1
	-0.6
	0.1
	-0.1



	State motor vehicle registration and license fees(1)(2)
	0.280
	0.7
	-0.5
	0.0
	0.3
	-0.5



	Parking and other fees(1)(2)
	0.200
	0.9
	0.4
	-1.5
	-0.2
	0.4



	Parking fees and tolls(2)(3)
	 
	3.1
	0.0
	-0.1
	0.4
	-0.1



	Public transportation
	1.378
	2.2
	0.3
	0.4
	3.0
	3.6



	Airline fares
	0.847
	3.3
	1.3
	-0.1
	4.0
	5.9



	Other intercity transportation
	0.213
	-2.0
	-2.0
	-1.0
	-1.6
	-1.5



	Ship fare(1)(2)(3)
	 
	-5.4
	0.6
	-3.3
	0.5
	0.6



	Intracity transportation(1)
	0.314
	0.0
	-0.6
	-0.4
	0.9
	-0.6



	Intracity mass transit(1)(3)(8)
	 
	2.0
	0.4
	0.0
	0.0
	0.4



	Recreation services(8)
	3.457
	3.6
	-0.2
	0.2
	0.4
	-0.2



	Video and audio services(8)
	0.811
	1.6
	-0.6
	0.2
	-0.3
	-0.6



	Cable, satellite, and live streaming television service(10)
	0.654
	1.7
	-0.3
	0.4
	-0.5
	-0.3



	Purchase, subscription, and rental of video(1)(2)
	0.157
	0.6
	-1.8
	-0.3
	0.4
	-1.8



	Video discs and other media(1)(2)(3)
	 
	-0.1
	-0.4
	-2.3
	1.0
	-0.4



	Subscription and rental of video and video games(1)(2)(3)
	 
	3.8
	-2.7
	-0.3
	0.4
	-2.7



	Pet services including veterinary(2)
	0.539
	5.5
	0.1
	0.7
	0.4
	0.6



	Pet services(2)(3)
	 
	5.8
	0.1
	1.6
	1.1
	0.2



	Veterinarian services(1)(2)(3)
	 
	6.4
	0.1
	0.0
	0.2
	0.1



	Photographers and photo processing(1)(2)
	0.053
	-3.0
	-0.8
	-2.6
	0.9
	-0.8



	Other recreation services(2)
	2.054
	4.3
	-0.2
	0.2
	0.7
	-0.1



	Club membership for shopping clubs, fraternal, or other organizations, or participant sports fees(2)
	0.808
	4.0
	0.1
	0.4
	-0.3
	0.2



	Admissions(1)
	0.747
	4.6
	-0.6
	-0.2
	1.5
	-0.6



	Admission to movies, theaters, and concerts(1)(2)(3)
	 
	3.4
	-0.9
	-0.3
	0.8
	-0.9



	Admission to sporting events(1)(2)(3)
	 
	-0.5
	-0.7
	-1.5
	1.9
	-0.7



	Fees for lessons or instructions(1)(6)
	0.169
	3.2
	0.1
	1.3
	1.4
	0.1



	Education and communication services(8)
	4.904
	1.0
	0.4
	0.1
	0.2
	0.1



	Tuition, other school fees, and childcare
	2.513
	3.3
	0.8
	0.2
	0.4
	0.2



	College tuition and fees
	1.297
	2.2
	0.7
	0.2
	0.3
	-0.1



	Elementary and high school tuition and fees
	0.388
	3.7
	1.3
	0.2
	0.0
	0.6



	Day care and preschool(1)(9)
	0.724
	5.0
	0.7
	0.0
	0.5
	0.7



	Technical and business school tuition and fees(2)
	0.039
	2.2
	0.9
	-0.1
	0.5
	0.9



	Postage and delivery services(2)
	0.053
	4.7
	0.5
	0.4
	2.0
	1.4



	Postage
	0.051
	4.9
	0.5
	0.3
	2.0
	1.4



	Delivery services(2)
	0.002
	8.2
	0.4
	0.6
	0.9
	0.8



	Telephone services(1)(2)
	1.412
	-1.7
	-1.0
	-0.3
	-0.1
	-1.0



	Wireless telephone services(1)(2)
	1.242
	-2.1
	-1.1
	-0.4
	0.0
	-1.1



	Residential telephone services(1)(8)
	0.170
	1.3
	-0.3
	0.7
	-0.8
	-0.3



	Internet services and electronic information providers(1)(2)
	0.917
	-0.8
	1.2
	0.5
	0.0
	1.2



	Other personal services(1)(8)
	1.660
	4.4
	0.1
	0.6
	0.5
	0.1



	Personal care services(1)
	0.658
	3.7
	0.5
	0.6
	0.1
	0.5



	Haircuts and other personal care services(1)(2)
	0.658
	3.7
	0.5
	0.6
	0.1
	0.5



	Miscellaneous personal services(1)
	1.001
	4.9
	-0.1
	0.6
	0.8
	-0.1



	Legal services(1)(6)
	 
	 
	 
	 
	 
	 



	Funeral expenses(1)(6)
	0.139
	2.6
	0.2
	0.0
	0.1
	0.2



	Laundry and dry cleaning services(1)(2)
	0.166
	4.8
	0.4
	1.6
	0.3
	0.4



	Apparel services other than laundry and dry cleaning(1)(2)
	0.027
	5.2
	0.9
	1.9
	0.9
	0.9



	Financial services(1)(6)
	0.254
	4.7
	-0.7
	-0.1
	2.7
	-0.7



	Checking account and other bank services(1)(2)(3)
	 
	0.4
	0.1
	0.0
	0.4
	0.1



	Tax return preparation and other accounting fees(1)(2)(3)
	 
	6.4
	-1.4
	-0.1
	3.5
	-1.4







	
		Footnotes
		(1) Not seasonally adjusted.
		(2) Indexes on a December 1997=100 base.
		(3) Special index based on a substantially smaller sample.
		(4) Indexes on a December 2007=100 base.
		(5) Indexes on a December 2005=100 base.
		(6) Indexes on a December 1986=100 base.
		(7) Indexes on a December 1993=100 base.
		(8) Indexes on a December 2009=100 base.
		(9) Indexes on a December 1990=100 base.
		(10) Indexes on a December 1983=100 base.
		(11) Indexes on a December 2001=100 base.
		(12) Indexes on a December 2019=100 base.
		(13) Indexes on a December 1982=100 base.
		(14) Indexes on a December 1996=100 base.
	










Table 3. Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, special aggregate indexes, August 2025
[1982-84=100, unless otherwise noted]




	Special aggregate indexes
	RelativeimportanceJul.2025
	Unadjusted indexes
	Unadjusted percent change
	Seasonally adjusted percent change


	Aug.2024
	Jul.2025
	Aug.2025
	Aug.2024-Aug.2025
	Jul.2025-Aug.2025
	May2025-Jun.2025
	Jun.2025-Jul.2025
	Jul.2025-Aug.2025






	


	All items less food
	86.365
	312.308
	320.408
	321.287
	2.9
	0.3
	0.3
	0.2
	0.4



	All items less shelter
	64.566
	283.681
	290.116
	290.784
	2.5
	0.2
	0.3
	0.2
	0.4



	All items less food and shelter
	50.931
	272.145
	277.922
	278.458
	2.3
	0.2
	0.4
	0.2
	0.3



	All items less food, shelter, and energy
	44.509
	273.907
	280.601
	281.241
	2.7
	0.2
	0.3
	0.4
	0.3



	All items less food, shelter, energy, and used cars and trucks
	42.072
	279.345
	285.703
	286.324
	2.5
	0.2
	0.3
	0.4
	0.2



	All items less medical care
	91.709
	302.733
	310.451
	311.423
	2.9
	0.3
	0.3
	0.2
	0.4



	All items less energy
	93.578
	320.728
	329.713
	330.737
	3.1
	0.3
	0.2
	0.3
	0.4



	Commodities
	36.097
	223.363
	225.508
	226.364
	1.3
	0.4
	0.3
	0.0
	0.5



	Commodities less food, energy, and used cars and trucks
	16.858
	163.472
	164.380
	165.051
	1.0
	0.4
	0.3
	0.2
	0.2



	Commodities less food
	22.462
	178.434
	178.144
	178.830
	0.2
	0.4
	0.3
	-0.1
	0.5



	Commodities less food and beverages
	21.637
	174.621
	174.234
	174.891
	0.2
	0.4
	0.3
	-0.1
	0.5



	Services
	63.903
	405.074
	419.436
	420.424
	3.8
	0.2
	0.3
	0.3
	0.3



	Services less rent of shelter(1)
	28.890
	417.066
	433.583
	433.800
	4.0
	0.1
	0.4
	0.3
	0.1



	Services less medical care services
	57.123
	389.178
	402.699
	403.730
	3.7
	0.3
	0.3
	0.2
	0.4



	Durables
	10.942
	122.201
	124.252
	124.570
	1.9
	0.3
	0.1
	0.4
	0.4



	Nondurables
	25.155
	278.025
	279.927
	281.139
	1.1
	0.4
	0.4
	-0.2
	0.6



	Nondurables less food
	11.520
	234.098
	230.255
	231.424
	-1.1
	0.5
	0.7
	-0.5
	0.5



	Nondurables less food and beverages
	10.695
	230.575
	226.293
	227.428
	-1.4
	0.5
	0.8
	-0.6
	0.4



	Nondurables less food, beverages, and apparel
	8.237
	297.113
	291.647
	291.660
	-1.8
	0.0
	0.8
	-0.7
	0.3



	Nondurables less food and apparel
	9.062
	295.045
	290.413
	290.581
	-1.5
	0.1
	0.8
	-0.6
	0.3



	Housing
	44.371
	335.931
	348.210
	349.277
	4.0
	0.3
	0.3
	0.2
	0.4



	Education and communication(2)
	5.638
	146.643
	146.696
	147.152
	0.3
	0.3
	0.1
	0.0
	0.0



	Education(2)
	2.555
	301.065
	309.439
	311.752
	3.5
	0.7
	0.2
	0.4
	0.2



	Communication(2)
	3.083
	74.541
	73.029
	72.991
	-2.1
	-0.1
	0.0
	-0.3
	-0.1



	Information and information processing(2)
	3.030
	70.131
	68.619
	68.577
	-2.2
	-0.1
	0.0
	-0.4
	-0.1



	Information technology, hardware and services(3)
	1.618
	7.014
	6.771
	6.821
	-2.8
	0.7
	0.3
	-0.6
	0.6



	Recreation(2)
	5.292
	138.214
	141.587
	141.394
	2.3
	-0.1
	0.4
	0.4
	-0.1



	Video and audio(2)
	1.064
	117.849
	120.103
	119.661
	1.5
	-0.4
	0.4
	0.0
	-0.3



	Pets, pet products and services(2)
	1.165
	223.816
	229.728
	229.313
	2.5
	-0.2
	0.4
	0.4
	0.0



	Photography(2)
	0.074
	86.378
	85.459
	85.366
	-1.2
	-0.1
	-1.7
	1.0
	-0.3



	Food and beverages
	14.460
	328.156
	337.077
	338.366
	3.1
	0.4
	0.3
	0.0
	0.5



	Domestically produced farm food
	6.724
	317.240
	323.572
	324.773
	2.4
	0.4
	0.0
	0.2
	0.4



	Other services
	10.021
	417.480
	427.263
	427.731
	2.5
	0.1
	0.2
	0.3
	0.0



	Apparel less footwear
	1.884
	123.808
	120.649
	123.770
	0.0
	2.6
	0.4
	-0.3
	0.8



	Fuels and utilities
	4.478
	314.948
	337.149
	336.541
	6.9
	-0.2
	0.8
	-0.1
	-0.1



	Household energy
	3.389
	263.440
	283.847
	282.839
	7.4
	-0.4
	0.9
	-0.2
	-0.3



	Medical care
	8.291
	564.407
	583.856
	583.875
	3.4
	0.0
	0.5
	0.7
	-0.2



	Transportation
	16.542
	271.391
	273.452
	273.910
	0.9
	0.2
	0.1
	0.0
	0.9



	Private transportation
	15.164
	272.087
	274.289
	274.710
	1.0
	0.2
	0.0
	-0.2
	0.7



	New and used motor vehicles(2)
	7.427
	124.224
	126.914
	126.820
	2.1
	-0.1
	-0.4
	0.1
	0.4



	Utilities and public transportation
	7.788
	258.017
	268.500
	267.873
	3.8
	-0.2
	0.4
	0.0
	0.3



	Household furnishings and operations
	4.460
	147.348
	152.865
	153.039
	3.9
	0.1
	1.0
	0.4
	0.2



	Other goods and services
	2.948
	561.561
	582.371
	583.313
	3.9
	0.2
	0.4
	0.4
	0.2



	Personal care
	2.460
	283.080
	292.613
	292.584
	3.4
	0.0
	0.3
	0.4
	0.1







	
		Footnotes
		(1) Indexes on a December 1982=100 base.
		(2) Indexes on a December 1997=100 base.
		(3) Indexes on a December 1988=100 base.
	










Table 4. Consumer Price Index for All Urban Consumers (CPI-U): Selected areas, all items index, August 2025
[1982-84=100, unless otherwise noted]




	Area
	PricingSchedule(1)
	Percent change to Aug. 2025 from:
	Percent change to Jul. 2025 from:


	Aug.2024
	Jun.2025
	Jul.2025
	Jul.2024
	May2025
	Jun.2025






	U.S. city average
	M
	2.9
	0.4
	0.3
	2.7
	0.5
	0.2



	


	Region and area size(2)
				



	


	Northeast
	M
	3.3
	0.6
	0.4
	3.2
	0.6
	0.2



	Northeast - Size Class A
	M
	3.2
	0.2
	0.2
	3.2
	0.5
	-0.1



	Northeast - Size Class B/C(3)
	M
	3.4
	1.0
	0.5
	3.1
	0.8
	0.4



	New England(4)
	M
	3.7
	0.8
	0.6
	3.3
	0.4
	0.2



	Middle Atlantic(4)
	M
	3.1
	0.4
	0.3
	3.1
	0.7
	0.2



	


	Midwest
	M
	2.8
	0.4
	0.3
	2.6
	0.8
	0.1



	Midwest - Size Class A
	M
	2.3
	0.4
	0.4
	1.9
	0.6
	0.0



	Midwest - Size Class B/C(3)
	M
	3.1
	0.4
	0.3
	3.0
	1.0
	0.1



	East North Central(4)
	M
	3.0
	0.4
	0.3
	2.8
	0.9
	0.1



	West North Central(4)
	M
	2.3
	0.5
	0.4
	2.2
	0.7
	0.1



	


	South
	M
	2.6
	0.4
	0.3
	2.3
	0.4
	0.1



	South - Size Class A
	M
	1.8
	0.1
	0.0
	1.9
	0.4
	0.1



	South - Size Class B/C(3)
	M
	3.0
	0.6
	0.4
	2.5
	0.4
	0.2



	South Atlantic(4)
	M
	2.6
	0.4
	0.3
	2.4
	0.4
	0.2



	East South Central(4)
	M
	3.5
	0.6
	0.3
	3.3
	1.0
	0.3



	West South Central(4)
	M
	2.0
	0.3
	0.2
	1.6
	0.1
	0.1



	


	West
	M
	3.2
	0.4
	0.3
	3.0
	0.3
	0.2



	West - Size Class A
	M
	2.9
	0.4
	0.2
	2.8
	0.4
	0.2



	West - Size Class B/C(3)
	M
	3.5
	0.5
	0.3
	3.3
	0.1
	0.2



	Mountain(4)
	M
	2.5
	0.6
	0.4
	2.4
	0.3
	0.3



	Pacific(4)
	M
	3.5
	0.3
	0.2
	3.3
	0.2
	0.1



	


	Size classes
				



	


	Size Class A(5)
	M
	2.6
	0.3
	0.2
	2.5
	0.5
	0.1



	Size Class B/C(3)
	M
	3.2
	0.6
	0.4
	2.9
	0.5
	0.2



	


	Selected local areas
				



	


	Chicago-Naperville-Elgin, IL-IN-WI
	M
	3.1
	0.4
	0.6
	2.7
	0.1
	-0.2



	Los Angeles-Long Beach-Anaheim, CA
	M
	3.3
	0.4
	0.3
	3.2
	0.2
	0.2



	New York-Newark-Jersey City, NY-NJ-PA
	M
	3.2
	0.3
	0.3
	3.2
	0.7
	0.0



	


	Atlanta-Sandy Springs-Roswell, GA
	2
	1.7
	0.1
	 
	 
	 
	 



	Baltimore-Columbia-Towson, MD(6)
	2
	2.8
	0.3
	 
	 
	 
	 



	Detroit-Warren-Dearborn, MI
	2
	0.7
	0.2
	 
	 
	 
	 



	Houston-The Woodlands-Sugar Land, TX
	2
	1.1
	-0.1
	 
	 
	 
	 



	Miami-Fort Lauderdale-West Palm Beach, FL
	2
	2.5
	-0.2
	 
	 
	 
	 



	Philadelphia-Camden-Wilmington, PA-NJ-DE-MD
	2
	3.3
	0.5
	 
	 
	 
	 



	Phoenix-Mesa-Scottsdale, AZ(7)
	2
	1.4
	0.9
	 
	 
	 
	 



	San Francisco-Oakland-Hayward, CA
	2
	2.5
	0.4
	 
	 
	 
	 



	Seattle-Tacoma-Bellevue, WA
	2
	2.8
	0.2
	 
	 
	 
	 



	St. Louis, MO-IL
	2
	2.6
	0.2
	 
	 
	 
	 



	Urban Alaska
	2
	2.4
	0.8
	 
	 
	 
	 



	


	Boston-Cambridge-Newton, MA-NH
	1
	 
	 
	 
	3.2
	-0.3
	 



	Dallas-Fort Worth-Arlington, TX
	1
	 
	 
	 
	0.9
	-0.3
	 



	Denver-Aurora-Lakewood, CO
	1
	 
	 
	 
	2.1
	0.4
	 



	Minneapolis-St.Paul-Bloomington, MN-WI
	1
	 
	 
	 
	1.8
	0.7
	 



	Riverside-San Bernardino-Ontario, CA(4)
	1
	 
	 
	 
	3.5
	0.2
	 



	San Diego-Carlsbad, CA
	1
	 
	 
	 
	4.0
	0.8
	 



	Tampa-St. Petersburg-Clearwater, FL(8)
	1
	 
	 
	 
	3.3
	1.1
	 



	Urban Hawaii
	1
	 
	 
	 
	2.3
	-0.3
	 



	Washington-Arlington-Alexandria, DC-VA-MD-WV(6)
	1
	 
	 
	 
	2.0
	0.4
	 







	
		Footnotes
		(1) Foods, fuels, and several other items are priced every month in all areas.  Most other goods and services are priced as indicated: M - Every month. 1 - January, March, May, July, September, and November. 2 - February, April, June, August, October, and December.
		(2) Regions defined as the four Census regions.
		(3) Indexes on a December 1996=100 base.
		(4) Indexes on a December 2017=100 base.
		(5) Indexes on a December 1986=100 base.
		(6) 1998 - 2017 indexes based on substantially smaller sample.
		(7) Indexes on a December 2001=100 base.
		(8) Indexes on a 1987=100 base.
	





	
		NOTE: Local area indexes are byproducts of the national CPI program.  Each local index has a smaller sample size than the national index and is, therefore, subject to substantially more sampling and other measurement error.  As a result, local area indexes show greater volatility than the national index, although their long-term trends are similar.  Therefore, the Bureau of Labor Statistics strongly urges users to consider adopting the national average CPI for use in their escalator clauses.
	










Table 5. Chained Consumer Price Index for All Urban Consumers (C-CPI-U) and the Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, all items index, August 2025
[Percent changes]




	Month Year
	Unadjusted 1-month percent change
	Unadjusted 12-month percent change


	C-CPI-U(1)
	CPI-U
	C-CPI-U(1)
	CPI-U






	December 2012
	 
	 
	1.5
	1.7



	December 2013
	 
	 
	1.3
	1.5



	December 2014
	 
	 
	0.5
	0.8



	December 2015
	 
	 
	0.4
	0.7



	December 2016
	 
	 
	1.8
	2.1



	December 2017
	 
	 
	1.7
	2.1



	December 2018
	 
	 
	1.5
	1.9



	December 2019
	 
	 
	1.8
	2.3



	December 2020
	 
	 
	1.5
	1.4



	December 2021
	 
	 
	6.5
	7.0



	December 2022
	 
	 
	6.4
	6.5



	


	January 2023
	0.8
	0.8
	6.4
	6.4



	February 2023
	0.5
	0.6
	6.0
	6.0



	March 2023
	0.3
	0.3
	4.8
	5.0



	April 2023
	0.5
	0.5
	4.7
	4.9



	May 2023
	0.2
	0.3
	3.8
	4.0



	June 2023
	0.3
	0.3
	2.9
	3.0



	July 2023
	0.1
	0.2
	3.0
	3.2



	August 2023
	0.4
	0.4
	3.5
	3.7



	September 2023
	0.2
	0.2
	3.4
	3.7



	October 2023
	0.0
	0.0
	2.9
	3.2



	November 2023
	-0.3
	-0.2
	2.7
	3.1



	December 2023
	-0.1
	-0.1
	2.9
	3.4



	January 2024
	0.5
	0.5
	2.6
	3.1



	February 2024
	0.6
	0.6
	2.8
	3.2



	March 2024
	0.6
	0.6
	3.1
	3.5



	April 2024
	0.4
	0.4
	3.0
	3.4



	May 2024
	0.1
	0.2
	2.9
	3.3



	June 2024
	0.0
	0.0
	2.6
	3.0



	July 2024
	0.0
	0.1
	2.5
	2.9



	August 2024
	0.0
	0.1
	2.2
	2.5



	September 2024
	0.1
	0.2
	2.1
	2.4



	October 2024
	0.1
	0.1
	2.3
	2.6



	November 2024
	-0.1
	-0.1
	2.4
	2.7



	December 2024
	0.0
	0.0
	2.6
	2.9



	January 2025
	0.6
	0.7
	2.7
	3.0



	February 2025
	0.4
	0.4
	2.5
	2.8



	March 2025
	0.2
	0.2
	2.1
	2.4



	April 2025
	0.3
	0.3
	2.0
	2.3



	May 2025
	0.2
	0.2
	2.1
	2.4



	June 2025
	0.3
	0.3
	2.4
	2.7



	July 2025
	0.1
	0.2
	2.5
	2.7



	August 2025
	0.3
	0.3
	2.7
	2.9







	
		Footnotes
		(1) The C-CPI-U is designed to be a closer approximation to a cost-of-living index in that it, in its final form, accounts for any substitution that consumers make across item categories in response to changes in relative prices. Since the expenditure data required for the calculation of the C-CPI-U are available only with a time lag, the C-CPI-U is issued first in preliminary form using the latest available expenditure data at that time and is subject to four revisions.
	





	
		Indexes are issued as initial estimates.  Indexes are revised each quarter with the publication of January, April, July, and October data as updated expenditure estimates become available. The C-CPI-U indexes are updated quarterly until they become final.  January-March indexes are final in January of the following year; April-June indexes are final in April of the following year; July-September indexes are final in July of the following year; October-December indexes are final in October of the following year.
	










Table 6. Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, by expenditure category, August 2025, 1-month analysis table
[1982-84=100, unless otherwise noted]




	Expenditure category
	RelativeimportanceJul.2025
	One Month


	Seasonally adjusted percent changeJul. 2025-Aug. 2025
	Seasonally adjusted effect on All ItemsJul. 2025-Aug. 2025(1)
	Standard error, median price change(2)
	Largest (L) or Smallest (S) seasonally adjusted change since:(3)


	Date
	Percent change






	All items
	100.000
	0.4
	 
	0.05
	L-Jan. 2025
	0.5



	Food
	13.635
	0.5
	0.063
	0.07
	L-Jan. 2023
	0.5



	Food at home
	7.982
	0.6
	0.046
	0.12
	L-Oct. 2022
	0.6



	Cereals and bakery products
	1.095
	0.1
	0.001
	0.30
	L-May 2025
	1.1



	Cereals and cereal products
	0.328
	0.3
	0.001
	0.50
	L-May 2025
	1.4



	Flour and prepared flour mixes
	0.030
	0.4
	0.000
	0.75
	L-May 2025
	1.1



	Breakfast cereal(4)
	0.143
	-0.6
	-0.001
	0.93
	L-Jun. 2025
	-0.3



	Rice, pasta, cornmeal
	0.156
	0.3
	0.000
	0.63
	L-May 2025
	1.1



	Rice(4)(5)(6)
	 
	1.5
	 
	0.73
	L-May 2022
	2.6



	Bakery products(4)
	0.767
	0.0
	0.000
	0.38
	S-Apr. 2025
	-0.3



	Bread(4)(5)
	0.140
	-0.2
	0.000
	0.61
	S-May 2025
	-0.4



	White bread(4)(6)
	 
	0.4
	 
	0.64
	L-Jun. 2025
	0.5



	Bread other than white(4)(6)
	 
	-0.9
	 
	1.08
	S-Mar. 2025
	-1.6



	Fresh biscuits, rolls, muffins(5)
	0.134
	-0.5
	-0.001
	1.02
	S-Nov. 2024
	-2.1



	Cakes, cupcakes, and cookies(4)
	0.214
	-0.4
	-0.001
	0.59
	S-Feb. 2025
	-0.5



	Cookies(4)(6)
	 
	-1.0
	 
	0.80
	S-Jun. 2025
	-1.7



	Fresh cakes and cupcakes(4)(6)
	 
	-0.1
	 
	0.69
	L-Jun. 2025
	1.6



	Other bakery products
	0.279
	0.4
	0.001
	0.64
	L-Jun. 2025
	0.7



	Fresh sweetrolls, coffeecakes, doughnuts(4)(6)
	 
	-2.3
	 
	0.84
	S-Mar. 2020
	-2.3



	Crackers, bread, and cracker products(6)
	 
	1.0
	 
	1.02
	L-Jun. 2025
	1.1



	Frozen and refrigerated bakery products, pies, tarts, turnovers(6)
	 
	0.4
	 
	1.14
	L-Jun. 2025
	1.8



	Meats, poultry, fish, and eggs
	1.633
	1.0
	0.017
	0.24
	L-Mar. 2025
	1.3



	Meats, poultry, and fish
	1.478
	1.1
	0.017
	0.25
	L-Feb. 2022
	1.2



	Meats
	0.948
	1.8
	0.017
	0.31
	L-Oct. 2021
	2.1



	Beef and veal
	0.479
	2.7
	0.013
	0.46
	L-Sep. 2021
	3.8



	Uncooked ground beef(4)
	0.216
	2.3
	0.005
	0.69
	S-Jun. 2025
	1.5



	Uncooked beef roasts(5)
	0.060
	4.0
	0.002
	1.10
	L-Sep. 2021
	7.1



	Uncooked beef steaks(5)
	0.140
	4.1
	0.006
	0.81
	L-Jun. 2021
	6.0



	Uncooked other beef and veal(4)(5)
	0.064
	2.3
	0.001
	0.88
	L-Feb. 2025
	3.1



	Pork
	0.288
	0.2
	0.001
	0.55
	S-Jun. 2025
	-0.3



	Bacon, breakfast sausage, and related products(5)
	0.104
	1.2
	0.001
	0.89
	L-Jun. 2025
	1.3



	Bacon and related products(6)
	 
	1.1
	 
	1.07
	L-Mar. 2025
	2.4



	Breakfast sausage and related products(5)(6)
	 
	2.5
	 
	1.18
	L-Apr. 2024
	3.1



	Ham
	0.044
	-4.9
	-0.002
	1.06
	S-Mar. 2021
	-6.4



	Ham, excluding canned(6)
	 
	-5.2
	 
	1.11
	S-Mar. 2021
	-6.7



	Pork chops(4)
	0.050
	-1.3
	-0.001
	1.29
	S-Apr. 2025
	-1.6



	Other pork including roasts, steaks, and ribs(5)
	0.090
	1.5
	0.001
	1.20
	L-Jan. 2025
	2.0



	Other meats
	0.181
	2.2
	0.004
	0.58
	L-Apr. 2020
	2.4



	Frankfurters(6)
	 
	-0.5
	 
	1.36
	L-Jun. 2025
	9.0



	Lunchmeats(4)(5)(6)
	 
	0.0
	 
	0.76
	S-Jun. 2025
	-2.1



	Poultry
	0.289
	-0.7
	-0.002
	0.50
	S-Apr. 2024
	-1.3



	Chicken(5)
	0.216
	0.1
	0.000
	0.53
	L-Jun. 2025
	1.1



	Fresh whole chicken(6)
	 
	-0.5
	 
	0.99
	S-May 2025
	-1.1



	Fresh and frozen chicken parts(6)
	 
	0.3
	 
	0.69
	L-Jun. 2025
	1.3



	Other uncooked poultry including turkey(5)
	0.073
	-2.8
	-0.002
	1.20
	S-Sep. 2021
	-3.4



	Fish and seafood
	0.241
	0.6
	0.002
	0.60
	L-Apr. 2025
	0.7



	Fresh fish and seafood(4)(5)
	0.140
	0.3
	0.000
	0.83
	L-Jun. 2025
	0.7



	Processed fish and seafood(5)
	0.101
	0.8
	0.001
	0.96
	L-May 2025
	0.9



	Shelf stable fish and seafood(6)
	 
	0.1
	 
	1.43
	L-Jun. 2025
	1.3



	Frozen fish and seafood(6)
	 
	1.7
	 
	1.19
	L-May 2025
	2.9



	Eggs
	0.155
	0.0
	0.000
	1.00
	L-Mar. 2025
	5.9



	Dairy and related products(4)
	0.727
	0.1
	0.001
	0.29
	S-Jun. 2025
	-0.3



	Milk(4)(5)
	0.196
	-0.2
	0.000
	0.39
	S-Jun. 2025
	-0.7



	Fresh whole milk(4)(6)
	 
	0.1
	 
	0.49
	S-Jun. 2025
	-1.3



	Fresh milk other than whole(4)(5)(6)
	 
	-0.3
	 
	0.55
	S-Jun. 2025
	-0.4



	Cheese and related products(4)
	0.251
	0.4
	0.001
	0.69
	L-Mar. 2025
	1.9



	Ice cream and related products
	0.127
	0.3
	0.000
	1.04
	L-May 2025
	2.4



	Other dairy and related products(5)
	0.153
	-0.1
	0.000
	0.54
	S-Jun. 2025
	-1.1



	Fruits and vegetables
	1.299
	1.6
	0.020
	0.32
	L-Feb. 2022
	1.9



	Fresh fruits and vegetables
	1.075
	2.0
	0.021
	0.41
	L-Feb. 2022
	2.1



	Fresh fruits
	0.541
	1.0
	0.005
	0.58
	L-Jun. 2025
	1.3



	Apples
	0.082
	3.5
	0.003
	0.95
	L-Oct. 2024
	4.1



	Bananas(4)
	0.111
	2.1
	0.002
	0.65
	L-May 2025
	3.3



	Citrus fruits(5)
	0.064
	1.2
	0.001
	0.88
	S-May 2025
	0.1



	Oranges, including tangerines(6)
	 
	0.9
	 
	1.44
	S-May 2025
	0.8



	Other fresh fruits(5)
	0.284
	-0.2
	-0.001
	1.11
	L-Jun. 2025
	3.2



	Fresh vegetables
	0.534
	3.0
	0.016
	0.58
	L-Jan. 2020
	3.0



	Potatoes
	0.085
	1.8
	0.001
	0.97
	L-Nov. 2024
	2.2



	Lettuce
	0.068
	3.5
	0.002
	1.17
	S-Jun. 2025
	1.1



	Tomatoes
	0.084
	4.5
	0.004
	1.22
	L-Jan. 2020
	5.5



	Other fresh vegetables
	0.296
	2.9
	0.009
	0.79
	L-Apr. 2017
	3.4



	Processed fruits and vegetables(5)
	0.224
	-0.5
	-0.001
	0.47
	S-Apr. 2025
	-1.6



	Canned fruits and vegetables(5)
	0.091
	0.5
	0.000
	0.60
	L-Jun. 2025
	0.7



	Canned fruits(5)(6)
	 
	0.5
	 
	0.94
	S-Apr. 2025
	-1.0



	Canned vegetables(5)(6)
	 
	0.2
	 
	0.90
	L-Jun. 2025
	0.6



	Frozen fruits and vegetables(5)
	0.067
	-0.6
	0.000
	0.89
	S-May 2025
	-0.8



	Frozen vegetables(6)
	 
	-0.5
	 
	1.08
	S-May 2025
	-1.3



	Other processed fruits and vegetables including dried(5)
	0.065
	-1.8
	-0.001
	0.66
	S-Apr. 2017
	-2.3



	Dried beans, peas, and lentils(4)(5)(6)
	 
	-3.0
	 
	0.76
	S-Feb. 2023
	-3.2



	Nonalcoholic beverages and beverage materials
	0.905
	0.6
	0.006
	0.40
	L-Jun. 2025
	1.4



	Juices and nonalcoholic drinks(5)
	0.626
	-0.3
	-0.002
	0.48
	L-Jun. 2025
	1.7



	Carbonated drinks
	0.345
	-0.3
	-0.001
	0.93
	L-Jun. 2025
	1.7



	Frozen noncarbonated juices and drinks(4)(5)
	0.004
	-0.2
	0.000
	0.81
	S-May 2025
	-4.6



	Nonfrozen noncarbonated juices and drinks(5)
	0.277
	-0.7
	-0.002
	0.49
	L-Jun. 2025
	2.0



	Beverage materials including coffee and tea(5)
	0.279
	2.8
	0.008
	0.68
	L-Jul. 2022
	2.8



	Coffee
	0.148
	3.6
	0.005
	0.97
	L-Apr. 2011
	3.9



	Roasted coffee(6)
	 
	4.1
	 
	1.25
	L-Apr. 2008
	4.4



	Instant coffee(4)(6)
	 
	4.9
	 
	1.07
	L-Jun. 2025
	5.1



	Other beverage materials including tea(4)(5)
	0.131
	1.5
	0.002
	1.06
	L-Apr. 2025
	1.9



	Other food at home
	2.323
	0.1
	0.002
	0.24
	L-Jun. 2025
	0.2



	Sugar and sweets
	0.335
	0.7
	0.002
	0.44
	L-Jun. 2025
	0.9



	Sugar and sugar substitutes
	0.028
	0.6
	0.000
	0.62
	L-May 2025
	1.9



	Candy and chewing gum(5)
	0.235
	0.7
	0.002
	0.60
	L-Jun. 2025
	1.0



	Other sweets(5)
	0.072
	0.6
	0.000
	0.82
	L-Jun. 2025
	0.7



	Fats and oils
	0.243
	0.2
	0.000
	0.53
	L-Jun. 2025
	0.4



	Butter and margarine(5)
	0.054
	0.2
	0.000
	0.99
	L-Feb. 2025
	0.8



	Butter(6)
	 
	1.3
	 
	1.11
	L-Sep. 2024
	1.7



	Margarine(6)
	 
	-1.6
	 
	1.56
	S-Dec. 2024
	-2.6



	Salad dressing(4)(5)
	0.065
	1.3
	0.001
	1.52
	S-Jun. 2025
	-0.2



	Other fats and oils including peanut butter(5)
	0.124
	0.3
	0.000
	0.68
	L-May 2025
	2.2



	Peanut butter(4)(5)(6)
	 
	-1.7
	 
	0.76
	L-Jun. 2025
	2.2



	Other foods
	1.745
	-0.1
	-0.001
	0.30
	L-Jun. 2025
	0.0



	Soups
	0.110
	1.8
	0.002
	0.97
	L-Sep. 2022
	2.9



	Frozen and freeze dried prepared foods
	0.274
	-1.2
	-0.003
	0.69
	S-Feb. 2025
	-1.7



	Snacks
	0.359
	1.0
	0.003
	0.78
	L-Jan. 2025
	1.4



	Spices, seasonings, condiments, sauces
	0.384
	0.4
	0.002
	0.50
	L-Jun. 2025
	1.4



	Salt and other seasonings and spices(5)(6)
	 
	0.9
	 
	0.91
	L-Jun. 2025
	1.8



	Olives, pickles, relishes(5)(6)
	 
	-1.2
	 
	0.90
	L-Jun. 2025
	4.9



	Sauces and gravies(5)(6)
	 
	-0.1
	 
	0.70
	S-May 2025
	-1.0



	Other condiments(6)
	 
	0.3
	 
	1.10
	S-Jun. 2025
	0.0



	Baby food and formula(4)(5)
	0.059
	-0.9
	-0.001
	0.57
	L-Jun. 2025
	0.9



	Other miscellaneous foods(5)
	0.558
	-1.0
	-0.006
	0.54
	S-Jun. 2025
	-1.0



	Prepared salads(6)(7)
	 
	1.7
	 
	0.98
	L-Mar. 2025
	2.0



	Food away from home(4)
	5.653
	0.3
	0.016
	0.06
	-
	-



	Full service meals and snacks(4)(5)
	2.450
	0.4
	0.009
	0.07
	S-May 2025
	0.3



	Limited service meals and snacks(4)(5)
	2.832
	0.1
	0.004
	0.09
	-
	-



	Food at employee sites and schools(4)(5)
	0.074
	-0.8
	-0.001
	0.29
	S-Nov. 2024
	-0.9



	Food at elementary and secondary schools(4)(6)(8)
	 
	 
	 
	 
	 
	 



	Food from vending machines and mobile vendors(4)(5)
	0.057
	0.1
	0.000
	0.25
	L-Apr. 2025
	1.2



	Other food away from home(4)(5)
	0.241
	1.7
	0.004
	0.10
	L-Aug. 2023
	1.7



	


	Energy
	6.422
	0.7
	0.043
	0.15
	L-Jun. 2025
	0.9



	Energy commodities
	3.167
	1.7
	0.051
	0.18
	L-Jan. 2025
	1.9



	Fuel oil and other fuels
	0.134
	-1.1
	-0.001
	0.34
	S-Apr. 2025
	-2.6



	Fuel oil
	0.075
	-0.3
	0.000
	0.47
	S-Apr. 2025
	-1.3



	Propane, kerosene, and firewood(9)
	0.059
	-0.2
	0.000
	0.42
	L-Jun. 2025
	1.1



	Motor fuel
	3.033
	1.8
	0.052
	0.19
	L-Jan. 2025
	1.8



	Gasoline (all types)
	2.949
	1.9
	0.053
	0.19
	L-Dec. 2024
	4.0



	Gasoline, unleaded regular(6)
	 
	2.0
	 
	0.46
	L-Dec. 2024
	4.1



	Gasoline, unleaded midgrade(6)(10)
	 
	1.8
	 
	0.43
	L-Jan. 2025
	1.9



	Gasoline, unleaded premium(6)
	 
	1.5
	 
	0.50
	L-Dec. 2024
	3.4



	Other motor fuels(4)(5)
	0.084
	-0.5
	0.000
	0.28
	S-May 2025
	-1.1



	Energy services
	3.255
	-0.2
	-0.008
	0.26
	L-Jun. 2025
	0.9



	Electricity
	2.475
	0.2
	0.005
	0.30
	L-Jun. 2025
	1.0



	Utility (piped) gas service
	0.780
	-1.6
	-0.013
	0.44
	S-Aug. 2024
	-1.6



	


	All items less food and energy
	79.943
	0.3
	0.277
	0.05
	-
	-



	Commodities less food and energy commodities
	19.295
	0.3
	0.053
	0.09
	L-Jan. 2025
	0.3



	Household furnishings and supplies(11)
	3.392
	0.1
	0.003
	0.21
	S-Mar. 2025
	0.0



	Window and floor coverings and other linens(5)
	0.247
	-5.6
	-0.014
	0.90
	S-EVER
	-



	Floor coverings(4)(5)
	0.059
	-1.2
	-0.001
	0.74
	S-Oct. 2024
	-1.2



	Window coverings(4)(5)
	0.055
	-0.5
	0.000
	1.68
	L-Jun. 2025
	2.2



	Other linens(5)
	0.133
	-6.7
	-0.009
	1.18
	S-EVER
	-



	Furniture and bedding(4)
	0.791
	0.3
	0.003
	0.41
	S-May 2025
	-0.8



	Bedroom furniture(4)
	0.255
	-0.4
	-0.001
	0.62
	S-May 2025
	-2.0



	Living room, kitchen, and dining room furniture(4)(5)
	0.387
	0.7
	0.003
	0.60
	S-Jun. 2025
	0.4



	Other furniture(5)
	0.140
	1.8
	0.003
	0.94
	L-Oct. 2023
	3.7



	Appliances(5)
	0.221
	0.4
	0.001
	0.68
	L-Jun. 2025
	1.9



	Major appliances(5)
	0.070
	-1.1
	-0.001
	1.04
	L-Jun. 2025
	1.9



	Laundry equipment(4)(6)
	 
	-2.9
	 
	0.76
	S-Nov. 2023
	-3.8



	Other appliances(5)
	0.147
	1.4
	0.002
	0.87
	L-Jun. 2025
	2.0



	Other household equipment and furnishings(5)
	0.494
	-0.2
	-0.001
	0.53
	-
	-



	Clocks, lamps, and decorator items(4)
	0.290
	-0.3
	-0.001
	0.60
	L-Jun. 2025
	1.6



	Indoor plants and flowers(12)
	0.112
	2.8
	0.003
	0.89
	L-Aug. 2023
	3.0



	Dishes and flatware(4)(5)
	0.036
	-0.4
	0.000
	1.59
	S-Apr. 2025
	-2.6



	Nonelectric cookware and tableware(5)
	0.055
	-0.2
	0.000
	1.38
	S-May 2025
	-0.9



	Tools, hardware, outdoor equipment and supplies(4)(5)
	0.848
	0.8
	0.007
	0.38
	S-Jun. 2025
	0.2



	Tools, hardware and supplies(5)
	0.245
	0.4
	0.001
	0.59
	S-Mar. 2025
	0.2



	Outdoor equipment and supplies(4)(5)
	0.366
	0.7
	0.002
	0.53
	S-Jun. 2025
	-0.1



	Housekeeping supplies(4)
	0.792
	0.4
	0.003
	0.26
	L-Jun. 2025
	0.8



	Household cleaning products(4)(5)
	0.293
	0.4
	0.001
	0.42
	L-Apr. 2025
	0.6



	Household paper products(4)(5)
	0.177
	1.2
	0.002
	0.34
	L-Jun. 2025
	1.4



	Miscellaneous household products(4)(5)
	0.322
	0.0
	0.000
	0.39
	S-May 2025
	-0.7



	Apparel
	2.458
	0.5
	0.013
	0.34
	L-Feb. 2025
	0.6



	Men's and boys' apparel
	0.646
	0.4
	0.002
	0.50
	L-Mar. 2025
	0.4



	Men's apparel
	0.521
	1.0
	0.005
	0.61
	L-Mar. 2025
	1.1



	Men's suits, sport coats, and outerwear
	0.087
	-0.3
	0.000
	1.98
	L-Apr. 2025
	0.3



	Men's underwear, nightwear, swimwear, and accessories
	0.141
	-2.2
	-0.003
	0.78
	S-May 2025
	-2.4



	Men's shirts and sweaters(5)
	0.144
	1.3
	0.002
	1.04
	L-Jun. 2025
	4.3



	Men's pants and shorts
	0.134
	4.2
	0.006
	1.12
	L-Mar. 2023
	4.6



	Boys' apparel
	0.125
	-1.9
	-0.002
	0.96
	S-Mar. 2025
	-2.6



	Women's and girls' apparel
	0.954
	0.2
	0.002
	0.61
	L-Jun. 2025
	0.5



	Women's apparel
	0.847
	0.1
	0.001
	0.66
	L-Jun. 2025
	0.7



	Women's outerwear
	0.074
	4.4
	0.003
	2.56
	L-Mar. 2025
	4.4



	Women's dresses
	0.130
	1.7
	0.002
	1.89
	S-Apr. 2025
	-0.6



	Women's suits and separates(5)
	0.336
	1.1
	0.004
	0.86
	L-Apr. 2025
	1.2



	Women's underwear, nightwear, swimwear, and accessories(5)
	0.297
	-2.8
	-0.008
	0.89
	S-May 2020
	-4.7



	Girls' apparel
	0.107
	0.8
	0.001
	1.29
	L-May 2025
	0.8



	Footwear
	0.573
	-0.4
	-0.002
	0.48
	S-May 2025
	-0.4



	Men's footwear
	0.198
	-1.2
	-0.002
	0.82
	S-May 2025
	-1.6



	Boys' and girls' footwear(4)
	0.110
	1.5
	0.002
	0.86
	L-Sep. 2024
	3.0



	Women's footwear
	0.266
	0.2
	0.000
	0.74
	S-May 2025
	-0.7



	Infants' and toddlers' apparel
	0.101
	1.0
	0.001
	0.97
	S-Jun. 2025
	0.4



	Jewelry and watches(9)
	0.184
	5.5
	0.010
	1.56
	L-EVER
	-



	Watches(4)(9)
	0.041
	1.9
	0.001
	1.47
	L-Oct. 2024
	3.3



	Jewelry(9)
	0.143
	6.8
	0.010
	2.00
	L-EVER
	-



	Transportation commodities less motor fuel(11)
	7.253
	0.5
	0.039
	0.03
	L-Jan. 2025
	0.8



	New vehicles
	4.316
	0.3
	0.012
	0.01
	L-Dec. 2024
	0.4



	New cars(6)
	 
	0.2
	 
	0.06
	L-Mar. 2025
	0.3



	New trucks(6)(13)
	 
	0.3
	 
	0.03
	L-Dec. 2024
	0.4



	Used cars and trucks
	2.437
	1.0
	0.025
	0.03
	L-Jan. 2025
	2.2



	Motor vehicle parts and equipment(4)
	0.363
	0.6
	0.002
	0.36
	S-Jun. 2025
	0.6



	Tires(4)
	0.299
	0.3
	0.001
	0.40
	S-Apr. 2025
	-0.4



	Vehicle accessories other than tires(4)(5)
	0.064
	1.7
	0.001
	0.62
	L-Nov. 2023
	2.4



	Vehicle parts and equipment other than tires(4)(6)
	 
	2.0
	 
	0.78
	L-Apr. 2025
	2.2



	Motor oil, coolant, and fluids(4)(6)
	 
	0.8
	 
	0.63
	L-May 2025
	1.7



	Medical care commodities(4)
	1.511
	-0.3
	-0.004
	0.28
	S-Mar. 2025
	-1.1



	Medicinal drugs(4)(11)
	1.332
	-0.4
	-0.005
	0.30
	S-Mar. 2025
	-1.3



	Prescription drugs(4)
	0.918
	-0.2
	-0.001
	0.26
	-
	-



	Nonprescription drugs(11)
	0.413
	-0.9
	-0.004
	0.63
	S-Jun. 2025
	-1.0



	Medical equipment and supplies(4)(11)
	0.180
	0.6
	0.001
	0.53
	S-Jun. 2025
	0.5



	Recreation commodities(11)
	1.834
	0.0
	0.000
	0.24
	S-Mar. 2025
	-0.3



	Video and audio products(11)
	0.254
	0.5
	0.001
	0.36
	S-May 2025
	0.3



	Televisions
	0.085
	2.5
	0.002
	0.61
	L-Aug. 2021
	3.4



	Other video equipment(5)
	0.029
	-2.5
	-0.001
	1.17
	S-Aug. 2023
	-3.9



	Audio equipment(4)
	0.061
	-0.8
	0.000
	0.87
	S-Mar. 2025
	-1.2



	Recorded music and music subscriptions(4)(5)
	0.074
	0.4
	0.000
	0.66
	S-Jun. 2025
	-0.2



	Pets and pet products(4)
	0.626
	-0.5
	-0.003
	0.34
	S-Apr. 2024
	-0.7



	Pet food and treats(4)(5)(6)
	 
	-0.3
	 
	0.37
	S-Mar. 2025
	-0.5



	Purchase of pets, pet supplies, accessories(4)(5)(6)
	 
	-0.6
	 
	0.75
	S-Jun. 2025
	-0.8



	Sporting goods(4)
	0.469
	0.1
	0.000
	0.50
	S-Mar. 2025
	-0.6



	Sports vehicles including bicycles(4)
	0.233
	0.6
	0.002
	0.74
	S-May 2025
	0.5



	Sports equipment(4)
	0.221
	-0.6
	-0.001
	0.52
	S-Apr. 2025
	-0.6



	Photographic equipment and supplies
	0.020
	1.1
	0.000
	0.68
	S-Jun. 2025
	0.7



	Photographic equipment(4)(5)(6)
	 
	1.8
	 
	1.03
	S-Jun. 2025
	0.5



	Recreational reading materials(4)
	0.098
	0.9
	0.001
	1.41
	L-May 2025
	1.0



	Newspapers and magazines(4)(5)
	0.055
	-1.1
	-0.001
	1.46
	S-Jun. 2025
	-4.7



	Recreational books(4)(5)
	0.044
	3.4
	0.001
	2.14
	L-Jun. 2025
	3.8



	Other recreational goods(5)
	0.367
	0.0
	0.000
	0.53
	S-Mar. 2025
	-0.7



	Toys
	0.290
	-0.8
	-0.002
	0.65
	S-Mar. 2025
	-0.9



	Toys, games, hobbies and playground equipment(5)(6)
	 
	-1.1
	 
	0.90
	S-Mar. 2024
	-1.5



	Sewing machines, fabric and supplies(4)(5)
	0.019
	9.1
	0.002
	1.12
	L-Aug. 2020
	15.0



	Music instruments and accessories(4)(5)
	0.043
	1.1
	0.000
	0.71
	L-Mar. 2025
	1.4



	Education and communication commodities(11)
	0.734
	-0.3
	-0.002
	0.64
	L-Jun. 2025
	0.0



	Educational books and supplies(4)
	0.042
	-0.6
	0.000
	0.88
	S-Dec. 2024
	-0.8



	College textbooks(4)(6)(14)
	 
	-0.2
	 
	1.17
	S-Jun. 2025
	-0.8



	Information technology commodities(11)
	0.692
	-0.3
	-0.002
	0.70
	L-Jun. 2025
	0.0



	Computers, peripherals, and smart home assistants(4)(7)
	0.266
	-0.6
	-0.002
	0.83
	L-Jun. 2025
	1.4



	Computer software and accessories(4)(5)
	0.027
	-5.5
	-0.002
	1.62
	S-Sep. 2020
	-5.6



	Telephone hardware, calculators, and other consumer information items(5)
	0.398
	0.3
	0.001
	0.88
	L-Mar. 2025
	0.5



	Smartphones(4)(6)(15)
	 
	-0.2
	 
	0.93
	S-May 2025
	-1.6



	Alcoholic beverages(4)
	0.825
	0.6
	0.005
	0.14
	L-Sep. 2023
	0.6



	Alcoholic beverages at home
	0.440
	0.6
	0.002
	0.18
	L-Dec. 2022
	0.7



	Beer, ale, and other malt beverages at home(4)
	0.151
	0.6
	0.001
	0.24
	L-Feb. 2025
	0.6



	Distilled spirits at home(4)
	0.102
	0.6
	0.001
	0.36
	L-Apr. 2025
	0.6



	Whiskey at home(4)(6)
	 
	0.5
	 
	0.66
	L-Apr. 2025
	0.5



	Distilled spirits, excluding whiskey, at home(4)(6)
	 
	0.7
	 
	0.50
	L-Apr. 2025
	0.7



	Wine at home
	0.188
	0.2
	0.000
	0.29
	L-Mar. 2025
	0.2



	Alcoholic beverages away from home(4)
	0.385
	0.6
	0.002
	0.20
	L-Oct. 2024
	1.2



	Beer, ale, and other malt beverages away from home(4)(5)(6)
	 
	0.5
	 
	0.21
	L-Oct. 2024
	2.1



	Wine away from home(4)(5)(6)
	 
	-0.1
	 
	0.20
	S-Jul. 2024
	-0.3



	Distilled spirits away from home(4)(5)(6)
	 
	0.1
	 
	0.25
	-
	-



	Other goods(11)
	1.288
	0.4
	0.005
	0.24
	L-Feb. 2025
	0.8



	Tobacco and smoking products(4)
	0.488
	1.0
	0.005
	0.22
	L-Nov. 2024
	1.0



	Cigarettes(4)(5)
	0.368
	1.1
	0.004
	0.25
	L-Nov. 2024
	1.2



	Tobacco products other than cigarettes(4)(5)
	0.114
	0.7
	0.001
	0.70
	L-Jun. 2025
	1.3



	Personal care products(4)
	0.642
	0.4
	0.003
	0.37
	L-Feb. 2025
	0.9



	Hair, dental, shaving, and miscellaneous personal care products(4)(5)
	0.286
	0.5
	0.001
	0.43
	L-Feb. 2025
	1.6



	Cosmetics, perfume, bath, nail preparations and implements(4)
	0.347
	0.3
	0.001
	0.58
	L-Jun. 2025
	0.9



	Miscellaneous personal goods(5)
	0.158
	-1.8
	-0.003
	0.76
	S-Jan. 2021
	-2.3



	Stationery, stationery supplies, gift wrap(6)
	 
	-0.8
	 
	0.74
	S-Apr. 2025
	-0.8



	Services less energy services
	60.648
	0.3
	0.212
	0.07
	S-Jun. 2025
	0.3



	Shelter
	35.434
	0.4
	0.155
	0.09
	L-Jan. 2025
	0.4



	Rent of shelter(16)
	35.014
	0.4
	0.144
	0.09
	L-Apr. 2025
	0.4



	Rent of primary residence
	7.451
	0.3
	0.022
	0.04
	-
	-



	Lodging away from home(5)
	1.363
	2.3
	0.030
	1.69
	L-Nov. 2024
	2.6



	Housing at school, excluding board(16)
	0.240
	0.8
	0.002
	0.07
	L-Aug. 2023
	0.8



	Other lodging away from home including hotels and motels
	1.123
	2.6
	0.028
	2.00
	L-Nov. 2024
	3.1



	Owners' equivalent rent of residences(16)
	26.199
	0.4
	0.100
	0.05
	L-Apr. 2025
	0.4



	Owners' equivalent rent of primary residence(16)
	25.004
	0.4
	0.096
	0.05
	L-Apr. 2025
	0.4



	Tenants' and household insurance(4)(5)
	0.420
	0.6
	0.003
	0.23
	S-Apr. 2025
	0.3



	Water and sewer and trash collection services(5)
	1.089
	0.4
	0.004
	0.11
	-
	-



	Water and sewerage maintenance(4)
	0.742
	0.3
	0.002
	0.13
	-
	-



	Garbage and trash collection(4)(13)
	0.347
	0.5
	0.002
	0.15
	S-Jun. 2025
	0.3



	Household operations(4)(5)
	 
	 
	 
	 
	 
	 



	Domestic services(4)(5)
	 
	 
	 
	 
	 
	 



	Gardening and lawncare services(4)(5)
	 
	 
	 
	 
	 
	 



	Moving, storage, freight expense(5)
	0.129
	-1.7
	-0.002
	0.61
	S-Oct. 2023
	-2.2



	Repair of household items(4)(5)
	 
	 
	 
	 
	 
	 



	Medical care services
	6.780
	-0.1
	-0.009
	0.17
	S-Aug. 2024
	-0.1



	Professional services
	3.687
	0.1
	0.003
	0.21
	S-May 2025
	0.0



	Physicians' services(4)
	1.804
	0.3
	0.006
	0.36
	L-Apr. 2025
	0.3



	Dental services
	0.957
	-0.7
	-0.007
	0.24
	S-Aug. 2018
	-0.8



	Eyeglasses and eye care(4)(9)
	0.331
	0.7
	0.002
	0.28
	L-Oct. 2024
	1.3



	Services by other medical professionals(4)(9)
	 
	 
	 
	 
	 
	 



	Hospital and related services(4)
	2.294
	0.0
	0.000
	0.25
	S-Jul. 2024
	-1.0



	Hospital services(4)(17)
	1.971
	0.0
	-0.001
	0.26
	S-Nov. 2024
	0.0



	Inpatient hospital services(4)(6)(17)
	 
	 
	 
	 
	 
	 



	Outpatient hospital services(4)(6)(9)
	 
	0.4
	 
	0.26
	S-Feb. 2025
	0.1



	Nursing homes and adult day services(17)
	0.167
	0.3
	0.000
	0.19
	L-Jun. 2025
	0.3



	Care of invalids and elderly at home(4)(8)
	0.156
	-0.2
	0.000
	0.53
	S-Jun. 2025
	-2.6



	Health insurance(4)(8)
	0.799
	0.1
	0.001
	0.19
	S-Dec. 2024
	0.0



	Transportation services
	6.257
	1.0
	0.063
	0.21
	L-Jan. 2025
	1.8



	Leased cars and trucks(4)(14)
	0.384
	-0.3
	-0.001
	0.63
	L-Jun. 2025
	-0.3



	Car and truck rental(5)
	0.153
	-6.9
	-0.009
	1.18
	S-Apr. 2020
	-18.3



	Motor vehicle maintenance and repair(4)
	1.030
	2.4
	0.025
	0.26
	L-EVER
	-



	Motor vehicle body work(4)
	 
	 
	 
	 
	 
	 



	Motor vehicle maintenance and servicing(4)
	0.508
	0.4
	0.002
	0.30
	S-Jun. 2025
	0.0



	Motor vehicle repair(4)(5)
	0.407
	5.0
	0.020
	0.48
	L-EVER
	-



	Motor vehicle insurance
	2.818
	0.0
	-0.001
	0.31
	S-Mar. 2025
	-0.8



	Motor vehicle fees(4)(5)
	0.493
	-0.1
	-0.001
	0.24
	S-Jun. 2025
	-0.6



	State motor vehicle registration and license fees(4)(5)
	0.280
	-0.5
	-0.001
	0.09
	S-Sep. 2014
	-1.6



	Parking and other fees(4)(5)
	0.200
	0.4
	0.001
	0.53
	L-May 2025
	0.5



	Parking fees and tolls(5)(6)
	 
	-0.1
	 
	0.34
	S-Jun. 2025
	-0.1



	Public transportation
	1.378
	3.6
	0.050
	0.55
	L-May 2022
	7.4



	Airline fares
	0.847
	5.9
	0.050
	0.77
	L-May 2022
	11.0



	Other intercity transportation
	0.213
	-1.5
	-0.003
	1.03
	L-Jun. 2025
	-1.0



	Ship fare(4)(5)(6)
	 
	0.6
	 
	1.04
	L-May 2025
	0.9



	Intracity transportation(4)
	0.314
	-0.6
	-0.002
	0.62
	S-May 2025
	-1.4



	Intracity mass transit(4)(6)(11)
	 
	0.4
	 
	0.09
	L-Feb. 2025
	1.2



	Recreation services(11)
	3.457
	-0.2
	-0.005
	0.22
	S-Apr. 2025
	-0.3



	Video and audio services(11)
	0.811
	-0.6
	-0.005
	0.24
	S-Apr. 2025
	-0.6



	Cable, satellite, and live streaming television service(13)
	0.654
	-0.3
	-0.002
	0.20
	L-Jun. 2025
	0.4



	Purchase, subscription, and rental of video(4)(5)
	0.157
	-1.8
	-0.003
	0.99
	S-Nov. 2024
	-2.1



	Video discs and other media(4)(5)(6)
	 
	-0.4
	 
	1.92
	S-Jun. 2025
	-2.3



	Subscription and rental of video and video games(4)(5)(6)
	 
	-2.7
	 
	0.47
	S-May 2024
	-3.9



	Pet services including veterinary(5)
	0.539
	0.6
	0.003
	0.32
	L-Jun. 2025
	0.7



	Pet services(5)(6)
	 
	0.2
	 
	0.50
	S-Mar. 2025
	-0.5



	Veterinarian services(4)(5)(6)
	 
	0.1
	 
	0.63
	S-Jun. 2025
	0.0



	Photographers and photo processing(4)(5)
	0.053
	-0.8
	0.000
	0.41
	S-Jun. 2025
	-2.6



	Other recreation services(5)
	2.054
	-0.1
	-0.003
	0.38
	S-May 2025
	-0.6



	Club membership for shopping clubs, fraternal, or other organizations, or participant sports fees(5)
	0.808
	0.2
	0.002
	0.17
	L-Jun. 2025
	0.4



	Admissions(4)
	0.747
	-0.6
	-0.004
	0.71
	S-May 2025
	-1.6



	Admission to movies, theaters, and concerts(4)(5)(6)
	 
	-0.9
	 
	0.55
	S-Sep. 2024
	-1.3



	Admission to sporting events(4)(5)(6)
	 
	-0.7
	 
	1.75
	S-Jun. 2025
	-1.5



	Fees for lessons or instructions(4)(9)
	0.169
	0.1
	0.000
	0.34
	S-Apr. 2025
	-1.2



	Education and communication services(11)
	4.904
	0.1
	0.003
	0.08
	S-Jun. 2025
	0.1



	Tuition, other school fees, and childcare
	2.513
	0.2
	0.005
	0.06
	S-Jun. 2025
	0.2



	College tuition and fees
	1.297
	-0.1
	-0.001
	0.08
	S-Jan. 2025
	-0.1



	Elementary and high school tuition and fees
	0.388
	0.6
	0.002
	0.09
	L-Mar. 2025
	0.9



	Day care and preschool(4)(12)
	0.724
	0.7
	0.005
	0.14
	L-Jan. 2025
	0.8



	Technical and business school tuition and fees(5)
	0.039
	0.9
	0.000
	0.07
	L-Aug. 2022
	0.9



	Postage and delivery services(5)
	0.053
	1.4
	0.001
	0.07
	S-Jun. 2025
	0.4



	Postage
	0.051
	1.4
	0.001
	0.00
	S-Jun. 2025
	0.3



	Delivery services(5)
	0.002
	0.8
	0.000
	0.33
	S-Jun. 2025
	0.6



	Telephone services(4)(5)
	1.412
	-1.0
	-0.014
	0.06
	S-Jun. 2023
	-1.2



	Wireless telephone services(4)(5)
	1.242
	-1.1
	-0.013
	0.06
	S-Jun. 2023
	-1.5



	Residential telephone services(4)(11)
	0.170
	-0.3
	0.000
	0.30
	L-Jun. 2025
	0.7



	Internet services and electronic information providers(4)(5)
	0.917
	1.2
	0.011
	0.23
	L-Feb. 2024
	1.3



	Other personal services(4)(11)
	1.660
	0.1
	0.002
	0.16
	S-Jan. 2025
	-0.5



	Personal care services(4)
	0.658
	0.5
	0.003
	0.24
	L-Jun. 2025
	0.6



	Haircuts and other personal care services(4)(5)
	0.658
	0.5
	0.003
	0.24
	L-Jun. 2025
	0.6



	Miscellaneous personal services(4)
	1.001
	-0.1
	-0.001
	0.15
	S-Jan. 2025
	-1.2



	Legal services(4)(9)
	 
	 
	 
	 
	 
	 



	Funeral expenses(4)(9)
	0.139
	0.2
	0.000
	0.24
	L-Apr. 2025
	0.2



	Laundry and dry cleaning services(4)(5)
	0.166
	0.4
	0.001
	0.27
	L-Jun. 2025
	1.6



	Apparel services other than laundry and dry cleaning(4)(5)
	0.027
	0.9
	0.000
	0.68
	-
	-



	Financial services(4)(9)
	0.254
	-0.7
	-0.002
	0.42
	S-May 2025
	-1.4



	Checking account and other bank services(4)(5)(6)
	 
	0.1
	 
	0.00
	S-Jun. 2025
	0.0



	Tax return preparation and other accounting fees(4)(5)(6)
	 
	-1.4
	 
	0.96
	S-May 2025
	-2.3



	


	Special aggregate indexes
				



	


	All items less food
	86.365
	0.4
	0.320
	0.05
	L-Jan. 2025
	0.5



	All items less shelter
	64.566
	0.4
	0.228
	0.05
	L-Jan. 2025
	0.5



	All items less food and shelter
	50.931
	0.3
	0.165
	0.05
	L-Jun. 2025
	0.4



	All items less food, shelter, and energy
	44.509
	0.3
	0.122
	0.06
	S-Jun. 2025
	0.3



	All items less food, shelter, energy, and used cars and trucks
	42.072
	0.2
	0.097
	0.06
	S-May 2025
	0.1



	All items less medical care
	91.709
	0.4
	0.396
	0.05
	L-Jan. 2025
	0.5



	All items less energy
	93.578
	0.4
	0.340
	0.05
	L-Jan. 2025
	0.4



	Commodities
	36.097
	0.5
	0.167
	0.05
	L-Dec. 2024
	0.5



	Commodities less food, energy, and used cars and trucks
	16.858
	0.2
	0.029
	0.10
	-
	-



	Commodities less food
	22.462
	0.5
	0.104
	0.07
	L-Dec. 2024
	0.6



	Commodities less food and beverages
	21.637
	0.5
	0.099
	0.07
	L-Dec. 2024
	0.6



	Services
	63.903
	0.3
	0.204
	0.07
	-
	-



	Services less rent of shelter(16)
	28.890
	0.1
	0.042
	0.07
	S-Aug. 2024
	0.1



	Services less medical care services
	57.123
	0.4
	0.211
	0.07
	L-Jan. 2025
	0.5



	Durables
	10.942
	0.4
	0.046
	0.09
	-
	-



	Nondurables
	25.155
	0.6
	0.140
	0.07
	L-Dec. 2024
	0.9



	Nondurables less food
	11.520
	0.5
	0.052
	0.11
	L-Jun. 2025
	0.7



	Nondurables less food and beverages
	10.695
	0.4
	0.048
	0.12
	L-Jun. 2025
	0.8



	Nondurables less food, beverages, and apparel
	8.237
	0.3
	0.021
	0.11
	L-Jun. 2025
	0.8



	Nondurables less food and apparel
	9.062
	0.3
	0.025
	0.10
	L-Jun. 2025
	0.8



	Housing
	44.371
	0.4
	0.160
	0.08
	L-Apr. 2025
	0.5



	Education and communication(5)
	5.638
	0.0
	0.001
	0.12
	-
	-



	Education(5)
	2.555
	0.2
	0.005
	0.06
	S-Jun. 2025
	0.2



	Communication(5)
	3.083
	-0.1
	-0.004
	0.17
	L-Jun. 2025
	0.0



	Information and information processing(5)
	3.030
	-0.1
	-0.004
	0.18
	L-Jun. 2025
	0.0



	Information technology, hardware and services(18)
	1.618
	0.6
	0.009
	0.33
	L-Feb. 2025
	0.6



	Recreation(5)
	5.292
	-0.1
	-0.006
	0.19
	S-Mar. 2025
	-0.1



	Video and audio(5)
	1.064
	-0.3
	-0.004
	0.21
	S-Apr. 2025
	-0.3



	Pets, pet products and services(5)
	1.165
	0.0
	0.000
	0.25
	S-Mar. 2025
	-0.3



	Photography(5)
	0.074
	-0.3
	0.000
	0.53
	S-Jun. 2025
	-1.7



	Food and beverages
	14.460
	0.5
	0.067
	0.07
	L-Jan. 2023
	0.5



	Domestically produced farm food(4)
	6.724
	0.4
	0.025
	0.14
	L-Mar. 2025
	0.5



	Other services
	10.021
	0.0
	0.000
	0.10
	S-Apr. 2025
	-0.2



	Apparel less footwear
	1.884
	0.8
	0.015
	0.41
	L-Feb. 2025
	0.8



	Fuels and utilities
	4.478
	-0.1
	-0.005
	0.19
	-
	-



	Household energy
	3.389
	-0.3
	-0.009
	0.25
	S-Aug. 2024
	-0.7



	Medical care
	8.291
	-0.2
	-0.013
	0.14
	S-Mar. 2023
	-0.3



	Transportation
	16.542
	0.9
	0.154
	0.10
	L-Jan. 2025
	1.2



	Private transportation
	15.164
	0.7
	0.104
	0.10
	L-Jan. 2025
	1.2



	New and used motor vehicles(5)
	7.427
	0.4
	0.026
	0.06
	L-Jan. 2025
	1.0



	Utilities and public transportation
	7.788
	0.3
	0.024
	0.15
	L-Jun. 2025
	0.4



	Household furnishings and operations
	4.460
	0.2
	0.010
	0.17
	S-Mar. 2025
	0.0



	Other goods and services
	2.948
	0.2
	0.006
	0.15
	S-Apr. 2025
	0.1



	Personal care
	2.460
	0.1
	0.001
	0.15
	S-Apr. 2025
	0.1







	
		Footnotes
		(1) The 'effect' of an item category is a measure of that item's contribution to the All items price change. For example, if the Food index had an effect of 0.40, and the All items index rose 1.2 percent, then the increase in food prices contributed 0.40 / 1.2, or 33.3 percent, to that All items increase. Said another way, had food prices been unchanged for that month the change in the All items index would have been 1.2 percent minus 0.40, or 0.8 percent.  Effects can be negative as well. For example, if the effect of food was a negative 0.1, and the All items index rose 0.5 percent, the All items index actually would have been 0.1 percent higher (or 0.6 percent) had food prices been unchanged. Since food prices fell while prices overall were rising, the contribution of food to the All items price change was negative (in this case, -0.1 / 0.5, or minus 20 percent).
		(2) A statistic's margin of error is often expressed as its point estimate plus or minus two standard errors. For example, if a CPI category rose 0.6 percent, and its standard error was 0.15 percent, the margin of error on this item's 1-month percent change would be 0.6 percent, plus or minus 0.3 percent.
		(3) If the current seasonally adjusted 1-month percent change is greater than the previous published 1-month percent change, then this column identifies the closest prior month with a 1-month percent change as (L)arge as or (L)arger than the current 1-month change.  If the current 1-month percent change is smaller than the previous published 1-month percent change, the most recent month with a change as (S)mall or (S)maller than the current month change is identified.  If the current and previous published 1-month percent changes are equal, a dash will appear.  Standard numerical comparisons are used.  For example, 0.8% is greater than 0.6%, -0.4% is less than -0.2%, and -0.2% is less than 0.0%.  Note that a (L)arger change can be a smaller decline, for example, a -0.2% change is larger than a -0.4% change, but still represents a decline in the price index.  Likewise, (S)maller changes can be increases, for example, a 0.6% change is smaller than 0.8%, but still represents an increase in the price index.  In this context, a -0.2% change is considered to be smaller than a 0.0% change.
		(4) Not seasonally adjusted.
		(5) Indexes on a December 1997=100 base.
		(6) Special indexes based on a substantially smaller sample.  These series do not contribute to the all items index aggregation and therefore do not have a relative importance or effect.
		(7) Indexes on a December 2007=100 base.
		(8) Indexes on a December 2005=100 base.
		(9) Indexes on a December 1986=100 base.
		(10) Indexes on a December 1993=100 base.
		(11) Indexes on a December 2009=100 base.
		(12) Indexes on a December 1990=100 base.
		(13) Indexes on a December 1983=100 base.
		(14) Indexes on a December 2001=100 base.
		(15) Indexes on a December 2019=100 base.
		(16) Indexes on a December 1982=100 base.
		(17) Indexes on a December 1996=100 base.
		(18) Indexes on a December 1988=100 base.
	










Table 7. Consumer Price Index for All Urban Consumers (CPI-U): U.S. city average, by expenditure category, August 2025, 12-month analysis table
[1982-84=100, unless otherwise noted]




	Expenditure category
	RelativeimportanceJul.2025
	Twelve Month


	Unadjusted percent changeAug. 2024-Aug. 2025
	Unadjusted effect on All ItemsAug. 2024-Aug. 2025(1)
	Standard error, median price change(2)
	Largest (L) or Smallest (S) unadjusted change since:(3)


	Date
	Percent change






	All items
	100.000
	2.9
	 
	0.11
	L-Jan. 2025
	3.0



	Food
	13.635
	3.2
	0.432
	0.13
	L-Oct. 2023
	3.3



	Food at home
	7.982
	2.7
	0.215
	0.18
	L-Aug. 2023
	3.0



	Cereals and bakery products
	1.095
	1.1
	0.013
	0.43
	L-Mar. 2025
	1.1



	Cereals and cereal products
	0.328
	-1.2
	-0.004
	0.77
	-
	-



	Flour and prepared flour mixes
	0.030
	0.3
	-0.001
	1.21
	L-Jun. 2025
	1.7



	Breakfast cereal
	0.143
	-1.0
	-0.002
	1.66
	S-May 2025
	-1.1



	Rice, pasta, cornmeal
	0.156
	-0.8
	-0.002
	0.82
	L-May 2025
	0.0



	Rice(4)(5)
	 
	-0.2
	 
	1.28
	L-Mar. 2025
	0.0



	Bakery products
	0.767
	2.2
	0.017
	0.48
	L-Jan. 2024
	2.5



	Bread(4)
	0.140
	1.2
	0.002
	0.75
	S-Jun. 2025
	-0.2



	White bread(5)
	 
	-0.2
	 
	1.19
	S-Jun. 2025
	-0.4



	Bread other than white(5)
	 
	2.9
	 
	1.00
	L-Dec. 2023
	3.6



	Fresh biscuits, rolls, muffins(4)
	0.134
	3.0
	0.004
	1.27
	L-Jun. 2025
	3.3



	Cakes, cupcakes, and cookies
	0.214
	2.9
	0.006
	0.86
	S-Jun. 2025
	2.1



	Cookies(5)
	 
	3.5
	 
	1.10
	S-Jun. 2025
	1.4



	Fresh cakes and cupcakes(5)
	 
	2.1
	 
	1.16
	S-Apr. 2025
	2.1



	Other bakery products
	0.279
	1.5
	0.005
	0.94
	L-Jul. 2024
	1.5



	Fresh sweetrolls, coffeecakes, doughnuts(5)
	 
	0.3
	 
	1.74
	S-Mar. 2024
	0.2



	Crackers, bread, and cracker products(5)
	 
	0.8
	 
	1.63
	L-Mar. 2025
	0.8



	Frozen and refrigerated bakery products, pies, tarts, turnovers(5)
	 
	-1.0
	 
	1.11
	L-Jun. 2025
	0.1



	Meats, poultry, fish, and eggs
	1.633
	5.6
	0.091
	0.38
	L-Jun. 2025
	5.6



	Meats, poultry, and fish
	1.478
	5.4
	0.078
	0.37
	L-Oct. 2022
	5.9



	Meats
	0.948
	7.3
	0.068
	0.47
	L-Jun. 2022
	8.2



	Beef and veal
	0.479
	13.9
	0.062
	0.69
	L-Apr. 2022
	14.3



	Uncooked ground beef
	0.216
	12.8
	0.026
	1.04
	L-May 2022
	13.6



	Uncooked beef roasts(4)
	0.060
	13.6
	0.008
	1.53
	L-Apr. 2022
	16.9



	Uncooked beef steaks(4)
	0.140
	16.6
	0.022
	1.11
	L-Feb. 2022
	16.9



	Uncooked other beef and veal(4)
	0.064
	11.4
	0.007
	1.34
	L-May 2022
	12.1



	Pork
	0.288
	1.2
	0.003
	0.95
	L-Apr. 2025
	1.3



	Bacon, breakfast sausage, and related products(4)
	0.104
	5.4
	0.006
	1.41
	L-Sep. 2022
	7.9



	Bacon and related products(5)
	 
	7.2
	 
	1.89
	L-Jul. 2024
	8.5



	Breakfast sausage and related products(4)(5)
	 
	3.7
	 
	1.76
	L-Mar. 2025
	4.1



	Ham
	0.044
	-1.9
	-0.001
	1.90
	S-Oct. 2024
	-2.0



	Ham, excluding canned(5)
	 
	-2.1
	 
	1.97
	S-Oct. 2024
	-2.7



	Pork chops
	0.050
	-2.4
	-0.001
	2.08
	S-Jul. 2023
	-2.4



	Other pork including roasts, steaks, and ribs(4)
	0.090
	-0.5
	0.000
	2.05
	S-Jun. 2025
	-0.8



	Other meats
	0.181
	1.3
	0.003
	0.88
	L-Aug. 2024
	1.3



	Frankfurters(5)
	 
	0.0
	 
	2.78
	L-Jun. 2025
	2.3



	Lunchmeats(4)(5)
	 
	-0.6
	 
	1.11
	L-Apr. 2025
	-0.2



	Poultry
	0.289
	1.7
	0.005
	0.80
	S-Mar. 2025
	0.9



	Chicken(4)
	0.216
	2.8
	0.006
	0.85
	S-May 2025
	2.3



	Fresh whole chicken(5)
	 
	-0.2
	 
	1.48
	S-Jun. 2021
	-0.8



	Fresh and frozen chicken parts(5)
	 
	4.2
	 
	1.08
	L-Jun. 2025
	4.7



	Other uncooked poultry including turkey(4)
	0.073
	-2.7
	-0.001
	1.96
	S-Oct. 2024
	-3.9



	Fish and seafood
	0.241
	2.3
	0.006
	0.81
	L-Mar. 2023
	2.6



	Fresh fish and seafood(4)
	0.140
	2.7
	0.004
	1.17
	L-Feb. 2023
	4.0



	Processed fish and seafood(4)
	0.101
	2.0
	0.002
	1.10
	L-Feb. 2025
	2.2



	Shelf stable fish and seafood(5)
	 
	-1.0
	 
	1.78
	L-Jun. 2025
	-0.6



	Frozen fish and seafood(5)
	 
	6.7
	 
	1.93
	L-Nov. 2022
	8.8



	Eggs
	0.155
	10.9
	0.013
	1.83
	S-Jun. 2024
	10.2



	Dairy and related products
	0.727
	1.3
	0.009
	0.45
	S-Jun. 2025
	0.9



	Milk(4)
	0.196
	1.7
	0.003
	0.70
	S-Feb. 2025
	1.5



	Fresh whole milk(5)
	 
	0.5
	 
	0.75
	S-Jun. 2025
	-0.4



	Fresh milk other than whole(4)(5)
	 
	2.3
	 
	0.86
	S-Feb. 2025
	1.5



	Cheese and related products
	0.251
	2.7
	0.006
	0.86
	L-Jun. 2025
	2.9



	Ice cream and related products
	0.127
	-0.5
	-0.001
	1.19
	S-Jun. 2025
	-0.6



	Other dairy and related products(4)
	0.153
	0.6
	0.000
	0.80
	-
	-



	Fruits and vegetables
	1.299
	1.9
	0.025
	0.48
	L-Mar. 2024
	2.0



	Fresh fruits and vegetables
	1.075
	2.3
	0.024
	0.60
	L-Feb. 2023
	2.6



	Fresh fruits
	0.541
	1.7
	0.009
	0.84
	L-Jun. 2025
	3.4



	Apples
	0.082
	9.6
	0.008
	1.37
	L-Aug. 2016
	10.3



	Bananas
	0.111
	6.6
	0.007
	1.14
	L-Sep. 2022
	7.3



	Citrus fruits(4)
	0.064
	3.1
	-0.001
	1.56
	L-Oct. 2024
	3.4



	Oranges, including tangerines(5)
	 
	5.2
	 
	2.38
	L-Oct. 2024
	7.2



	Other fresh fruits(4)
	0.284
	-0.2
	-0.005
	1.75
	S-Jun. 2024
	-0.8



	Fresh vegetables
	0.534
	2.9
	0.015
	0.83
	L-Dec. 2024
	3.1



	Potatoes
	0.085
	2.2
	0.003
	1.54
	L-Aug. 2023
	3.1



	Lettuce
	0.068
	3.8
	0.002
	2.36
	L-Dec. 2024
	4.4



	Tomatoes
	0.084
	-1.2
	-0.001
	1.81
	L-Dec. 2024
	1.6



	Other fresh vegetables
	0.296
	4.0
	0.011
	1.17
	L-Dec. 2024
	4.0



	Processed fruits and vegetables(4)
	0.224
	1.4
	0.001
	0.63
	S-Jun. 2025
	1.1



	Canned fruits and vegetables(4)
	0.091
	4.0
	0.002
	0.81
	L-Aug. 2023
	4.5



	Canned fruits(4)(5)
	 
	4.3
	 
	1.45
	L-Sep. 2023
	4.6



	Canned vegetables(4)(5)
	 
	3.9
	 
	0.87
	L-Apr. 2024
	4.8



	Frozen fruits and vegetables(4)
	0.067
	-0.6
	-0.001
	1.54
	S-Jun. 2025
	-1.2



	Frozen vegetables(5)
	 
	-2.5
	 
	1.67
	S-Jun. 2025
	-2.9



	Other processed fruits and vegetables including dried(4)
	0.065
	0.4
	0.000
	1.26
	S-Feb. 2024
	0.4



	Dried beans, peas, and lentils(4)(5)
	 
	1.0
	 
	2.29
	S-Jan. 2024
	-2.3



	Nonalcoholic beverages and beverage materials
	0.905
	4.6
	0.042
	0.62
	L-Aug. 2023
	4.8



	Juices and nonalcoholic drinks(4)
	0.626
	1.5
	0.009
	0.83
	-
	-



	Carbonated drinks
	0.345
	2.0
	0.007
	1.28
	L-Jun. 2025
	3.1



	Frozen noncarbonated juices and drinks(4)
	0.004
	7.1
	0.000
	3.05
	L-Jan. 2025
	7.7



	Nonfrozen noncarbonated juices and drinks(4)
	0.277
	-0.1
	0.002
	0.98
	S-Jun. 2021
	-0.1



	Beverage materials including coffee and tea(4)
	0.279
	12.1
	0.032
	0.99
	L-Jan. 2023
	12.6



	Coffee
	0.148
	20.9
	0.029
	1.16
	L-Oct. 1997
	21.6



	Roasted coffee(5)
	 
	21.7
	 
	1.42
	L-Aug. 2011
	22.3



	Instant coffee(5)
	 
	20.1
	 
	1.82
	L-Jun. 1995
	28.9



	Other beverage materials including tea(4)
	0.131
	2.4
	0.003
	1.58
	S-Jun. 2025
	1.5



	Other food at home
	2.323
	1.5
	0.035
	0.34
	L-Jun. 2024
	1.6



	Sugar and sweets
	0.335
	5.3
	0.017
	0.77
	L-Jun. 2025
	5.5



	Sugar and sugar substitutes
	0.028
	2.6
	0.000
	1.19
	L-Jun. 2025
	3.1



	Candy and chewing gum(4)
	0.235
	8.1
	0.017
	1.11
	L-Jun. 2025
	8.1



	Other sweets(4)
	0.072
	-1.1
	-0.001
	1.18
	S-May 2025
	-1.4



	Fats and oils
	0.243
	-1.1
	-0.003
	0.86
	L-May 2025
	-0.5



	Butter and margarine(4)
	0.054
	-0.4
	-0.001
	1.15
	L-May 2025
	0.7



	Butter(5)
	 
	0.1
	 
	1.57
	L-May 2025
	1.9



	Margarine(5)
	 
	2.9
	 
	1.72
	L-May 2025
	3.7



	Salad dressing(4)
	0.065
	2.5
	0.002
	1.73
	L-Sep. 2024
	2.8



	Other fats and oils including peanut butter(4)
	0.124
	-3.1
	-0.004
	1.46
	L-Jun. 2025
	-1.5



	Peanut butter(4)(5)
	 
	-1.8
	 
	1.63
	S-Apr. 2025
	-2.6



	Other foods
	1.745
	1.2
	0.021
	0.40
	L-May 2025
	1.2



	Soups
	0.110
	3.4
	0.004
	1.56
	L-Aug. 2023
	4.0



	Frozen and freeze dried prepared foods
	0.274
	0.4
	0.001
	0.92
	S-Mar. 2025
	-1.0



	Snacks
	0.359
	1.3
	0.005
	1.04
	L-Feb. 2024
	1.6



	Spices, seasonings, condiments, sauces
	0.384
	1.7
	0.008
	0.76
	-
	-



	Salt and other seasonings and spices(4)(5)
	 
	0.2
	 
	1.23
	L-Jun. 2025
	1.7



	Olives, pickles, relishes(4)(5)
	 
	1.7
	 
	1.79
	S-May 2025
	0.6



	Sauces and gravies(4)(5)
	 
	1.1
	 
	1.26
	S-Jun. 2025
	0.8



	Other condiments(5)
	 
	9.3
	 
	2.48
	S-Mar. 2025
	5.3



	Baby food and formula(4)
	0.059
	0.5
	0.000
	1.47
	S-Mar. 2025
	0.0



	Other miscellaneous foods(4)
	0.558
	0.5
	0.003
	0.80
	S-Jun. 2025
	-0.1



	Prepared salads(5)(6)
	 
	3.8
	 
	1.20
	L-May 2025
	5.8



	Food away from home
	5.653
	3.9
	0.218
	0.17
	-
	-



	Full service meals and snacks(4)
	2.450
	4.6
	0.111
	0.27
	L-Sep. 2023
	5.1



	Limited service meals and snacks(4)
	2.832
	3.2
	0.087
	0.24
	S-Apr. 2020
	3.2



	Food at employee sites and schools(4)
	0.074
	5.8
	0.004
	1.01
	S-May 2025
	3.9



	Food at elementary and secondary schools(5)(7)
	 
	 
	 
	 
	 
	 



	Food from vending machines and mobile vendors(4)
	0.057
	4.8
	0.003
	1.42
	S-Feb. 2025
	3.9



	Other food away from home(4)
	0.241
	5.4
	0.013
	0.55
	L-May 2024
	5.7



	


	Energy
	6.422
	0.2
	0.003
	0.46
	L-Jan. 2025
	1.0



	Energy commodities
	3.167
	-6.2
	-0.236
	0.39
	L-Feb. 2025
	-3.2



	Fuel oil and other fuels
	0.134
	-0.8
	-0.001
	1.05
	L-Jul. 2024
	1.0



	Fuel oil
	0.075
	-0.5
	0.000
	1.00
	L-Jul. 2024
	-0.3



	Propane, kerosene, and firewood(8)
	0.059
	-2.2
	-0.001
	1.38
	L-Jun. 2025
	0.0



	Motor fuel
	3.033
	-6.5
	-0.235
	0.40
	L-Feb. 2025
	-3.2



	Gasoline (all types)
	2.949
	-6.6
	-0.234
	0.41
	L-Feb. 2025
	-3.1



	Gasoline, unleaded regular(5)
	 
	-7.1
	 
	0.86
	L-Feb. 2025
	-3.4



	Gasoline, unleaded midgrade(5)(9)
	 
	-5.4
	 
	0.91
	L-Feb. 2025
	-2.1



	Gasoline, unleaded premium(5)
	 
	-4.2
	 
	0.97
	L-Feb. 2025
	-1.3



	Other motor fuels(4)
	0.084
	-0.4
	-0.001
	0.56
	L-Feb. 2023
	16.5



	Energy services
	3.255
	7.7
	0.239
	0.88
	L-Mar. 2023
	9.2



	Electricity
	2.475
	6.2
	0.146
	1.10
	L-Apr. 2023
	8.4



	Utility (piped) gas service
	0.780
	13.8
	0.093
	0.75
	-
	-



	


	All items less food and energy
	79.943
	3.1
	2.482
	0.14
	-
	-



	Commodities less food and energy commodities
	19.295
	1.5
	0.309
	0.18
	L-May 2023
	2.0



	Household furnishings and supplies(10)
	3.392
	2.8
	0.095
	0.54
	L-Jun. 2023
	3.2



	Window and floor coverings and other linens(4)
	0.247
	1.3
	0.003
	1.92
	S-May 2025
	0.7



	Floor coverings(4)
	0.059
	0.1
	0.000
	3.63
	S-May 2025
	-1.0



	Window coverings(4)
	0.055
	0.9
	0.000
	3.66
	S-Apr. 2025
	0.0



	Other linens(4)
	0.133
	2.3
	0.003
	2.65
	S-May 2025
	-0.3



	Furniture and bedding
	0.791
	4.7
	0.038
	1.14
	L-Dec. 2022
	4.7



	Bedroom furniture
	0.255
	0.1
	-0.001
	1.50
	L-May 2023
	1.0



	Living room, kitchen, and dining room furniture(4)
	0.387
	9.5
	0.039
	1.50
	L-Nov. 2022
	10.3



	Other furniture(4)
	0.140
	-0.2
	0.000
	2.57
	-
	-



	Appliances(4)
	0.221
	0.3
	0.001
	1.45
	L-Jun. 2025
	0.8



	Major appliances(4)
	0.070
	-1.1
	-0.001
	1.76
	S-Apr. 2025
	-3.6



	Laundry equipment(5)
	 
	0.5
	 
	2.51
	S-Sep. 2024
	-0.9



	Other appliances(4)
	0.147
	0.9
	0.001
	1.95
	L-Nov. 2023
	1.0



	Other household equipment and furnishings(4)
	0.494
	2.2
	0.011
	1.14
	L-Jun. 2025
	2.5



	Clocks, lamps, and decorator items
	0.290
	1.3
	0.004
	1.73
	L-Jun. 2025
	4.8



	Indoor plants and flowers(11)
	0.112
	5.9
	0.007
	2.36
	L-Jul. 2024
	10.6



	Dishes and flatware(4)
	0.036
	-6.6
	-0.003
	3.61
	S-Apr. 2025
	-8.7



	Nonelectric cookware and tableware(4)
	0.055
	5.6
	0.003
	2.83
	S-Jun. 2025
	2.1



	Tools, hardware, outdoor equipment and supplies(4)
	0.848
	3.9
	0.034
	1.50
	L-Oct. 2023
	4.2



	Tools, hardware and supplies(4)
	0.245
	5.8
	0.014
	1.24
	L-Jul. 2023
	6.0



	Outdoor equipment and supplies(4)
	0.366
	2.6
	0.011
	2.29
	L-Nov. 2023
	3.7



	Housekeeping supplies
	0.792
	1.0
	0.008
	0.58
	S-May 2025
	0.7



	Household cleaning products(4)
	0.293
	-0.5
	-0.001
	0.97
	S-Apr. 2018
	-1.2



	Household paper products(4)
	0.177
	4.5
	0.008
	0.92
	L-Oct. 2023
	6.3



	Miscellaneous household products(4)
	0.322
	0.6
	0.002
	0.95
	S-Jun. 2025
	0.4



	Apparel
	2.458
	0.2
	0.006
	0.72
	L-Mar. 2025
	0.3



	Men's and boys' apparel
	0.646
	0.6
	0.005
	1.19
	L-Jun. 2025
	1.5



	Men's apparel
	0.521
	1.6
	0.009
	1.35
	L-Mar. 2025
	2.0



	Men's suits, sport coats, and outerwear
	0.087
	3.3
	0.003
	4.19
	L-Apr. 2025
	5.3



	Men's underwear, nightwear, swimwear, and accessories
	0.141
	-1.7
	-0.003
	1.47
	S-Feb. 2021
	-1.9



	Men's shirts and sweaters(4)
	0.144
	2.0
	0.003
	2.59
	L-Nov. 2024
	2.3



	Men's pants and shorts
	0.134
	4.2
	0.006
	2.45
	L-Dec. 2024
	6.1



	Boys' apparel
	0.125
	-2.9
	-0.005
	2.44
	S-Dec. 2020
	-5.5



	Women's and girls' apparel
	0.954
	-1.5
	-0.017
	1.27
	S-Apr. 2025
	-1.5



	Women's apparel
	0.847
	-1.4
	-0.013
	1.16
	S-Mar. 2021
	-5.4



	Women's outerwear
	0.074
	5.4
	0.004
	4.43
	L-May 2025
	6.0



	Women's dresses
	0.130
	6.2
	0.009
	3.60
	L-Aug. 2023
	6.5



	Women's suits and separates(4)
	0.336
	-3.3
	-0.016
	1.71
	L-Jun. 2025
	-1.7



	Women's underwear, nightwear, swimwear, and accessories(4)
	0.297
	-3.5
	-0.011
	1.78
	S-Mar. 2021
	-5.1



	Girls' apparel
	0.107
	-2.1
	-0.004
	3.41
	S-Jun. 2025
	-2.9



	Footwear
	0.573
	1.4
	0.009
	1.01
	L-Apr. 2024
	1.4



	Men's footwear
	0.198
	-0.2
	0.000
	1.51
	S-May 2025
	-2.4



	Boys' and girls' footwear
	0.110
	0.9
	0.001
	2.10
	L-Dec. 2024
	2.5



	Women's footwear
	0.266
	2.8
	0.008
	1.58
	L-Feb. 2023
	2.9



	Infants' and toddlers' apparel
	0.101
	-0.2
	0.000
	2.49
	S-Jun. 2025
	-2.9



	Jewelry and watches(8)
	0.184
	6.0
	0.009
	3.57
	L-Jul. 2023
	6.5



	Watches(8)
	0.041
	5.6
	0.002
	2.95
	L-Dec. 2024
	5.6



	Jewelry(8)
	0.143
	6.9
	0.007
	4.04
	L-Jul. 2023
	7.5



	Transportation commodities less motor fuel(10)
	7.253
	2.6
	0.182
	0.13
	L-Oct. 2022
	5.5



	New vehicles
	4.316
	0.7
	0.029
	0.19
	L-Jan. 2024
	0.7



	New cars(5)
	 
	1.0
	 
	0.20
	L-Nov. 2023
	1.0



	New trucks(5)(12)
	 
	0.6
	 
	0.18
	L-Jan. 2024
	0.8



	Used cars and trucks
	2.437
	6.0
	0.137
	0.11
	L-Sep. 2022
	7.2



	Motor vehicle parts and equipment
	0.363
	3.4
	0.013
	0.73
	L-Jun. 2023
	3.6



	Tires
	0.299
	3.9
	0.012
	0.86
	L-Mar. 2023
	4.3



	Vehicle accessories other than tires(4)
	0.064
	2.1
	0.001
	1.48
	L-May 2025
	2.4



	Vehicle parts and equipment other than tires(5)
	 
	2.1
	 
	1.81
	L-Oct. 2024
	5.9



	Motor oil, coolant, and fluids(5)
	 
	-0.3
	 
	1.00
	L-May 2025
	2.7



	Medical care commodities
	1.511
	0.0
	0.001
	1.10
	S-Oct. 2021
	-0.4



	Medicinal drugs(10)
	1.332
	-0.2
	-0.002
	1.18
	S-Oct. 2021
	-0.4



	Prescription drugs
	0.918
	0.9
	0.008
	1.56
	-
	-



	Nonprescription drugs(10)
	0.413
	-2.4
	-0.011
	1.54
	S-EVER
	-



	Medical equipment and supplies(10)
	0.180
	1.5
	0.004
	1.09
	L-Oct. 2024
	2.1



	Recreation commodities(10)
	1.834
	0.1
	0.000
	0.45
	L-Sep. 2023
	0.2



	Video and audio products(10)
	0.254
	1.5
	0.003
	1.01
	L-Sep. 2021
	1.7



	Televisions
	0.085
	-5.6
	-0.006
	1.17
	L-Dec. 2024
	-4.2



	Other video equipment(4)
	0.029
	-0.3
	0.000
	2.42
	S-May 2025
	-1.9



	Audio equipment
	0.061
	12.2
	0.007
	2.33
	S-Jun. 2025
	11.1



	Recorded music and music subscriptions(4)
	0.074
	3.8
	0.003
	2.30
	S-Oct. 2024
	2.8



	Pets and pet products
	0.626
	0.1
	0.001
	1.10
	S-Jun. 2025
	-0.5



	Pet food and treats(4)(5)
	 
	0.2
	 
	1.31
	L-Feb. 2025
	0.4



	Purchase of pets, pet supplies, accessories(4)(5)
	 
	0.0
	 
	1.90
	S-Jun. 2025
	-0.1



	Sporting goods
	0.469
	-1.3
	-0.008
	1.02
	-
	-



	Sports vehicles including bicycles
	0.233
	-1.7
	-0.007
	1.55
	L-Jun. 2024
	-0.8



	Sports equipment
	0.221
	-0.8
	-0.001
	1.17
	-
	-



	Photographic equipment and supplies
	0.020
	4.7
	0.001
	3.36
	L-Oct. 2024
	4.9



	Photographic equipment(4)(5)
	 
	4.7
	 
	4.99
	L-Sep. 2024
	9.9



	Recreational reading materials
	0.098
	1.8
	0.002
	2.37
	L-May 2025
	4.1



	Newspapers and magazines(4)
	0.055
	2.1
	0.001
	2.97
	L-Jun. 2025
	2.4



	Recreational books(4)
	0.044
	1.4
	0.001
	2.63
	L-Feb. 2025
	3.7



	Other recreational goods(4)
	0.367
	0.0
	0.001
	1.02
	S-Jun. 2025
	-0.2



	Toys
	0.290
	0.1
	0.001
	1.32
	S-Apr. 2025
	-1.4



	Toys, games, hobbies and playground equipment(4)(5)
	 
	0.6
	 
	1.90
	S-Apr. 2025
	-1.3



	Sewing machines, fabric and supplies(4)
	0.019
	-7.5
	-0.002
	3.34
	L-Apr. 2025
	-2.6



	Music instruments and accessories(4)
	0.043
	4.9
	0.002
	2.10
	L-Oct. 2023
	7.2



	Education and communication commodities(10)
	0.734
	-3.8
	-0.032
	1.43
	L-Jun. 2025
	-3.3



	Educational books and supplies
	0.042
	 
	0.008
	2.44
	-
	-



	College textbooks(5)(13)
	 
	12.2
	 
	3.00
	S-Jun. 2025
	10.2



	Information technology commodities(10)
	0.692
	-5.3
	-0.040
	1.58
	L-Jun. 2025
	-4.7



	Computers, peripherals, and smart home assistants(6)
	0.266
	-2.0
	-0.007
	1.68
	S-May 2025
	-3.5



	Computer software and accessories(4)
	0.027
	-4.6
	-0.001
	3.81
	S-May 2024
	-6.6



	Telephone hardware, calculators, and other consumer information items(4)
	0.398
	-7.4
	-0.032
	2.32
	L-May 2025
	-6.7



	Smartphones(5)(14)
	 
	-13.7
	 
	2.26
	L-Feb. 2025
	-13.7



	Alcoholic beverages
	0.825
	1.9
	0.016
	0.45
	L-Mar. 2025
	1.9



	Alcoholic beverages at home
	0.440
	0.3
	0.001
	0.50
	L-Apr. 2025
	0.8



	Beer, ale, and other malt beverages at home
	0.151
	1.2
	0.002
	0.61
	L-Apr. 2025
	1.4



	Distilled spirits at home
	0.102
	1.1
	0.001
	0.81
	L-May 2025
	1.5



	Whiskey at home(5)
	 
	-1.8
	 
	1.40
	S-Jun. 2025
	-2.8



	Distilled spirits, excluding whiskey, at home(5)
	 
	2.3
	 
	1.10
	L-Apr. 2025
	2.4



	Wine at home
	0.188
	-0.9
	-0.002
	0.81
	L-Jun. 2025
	-0.9



	Alcoholic beverages away from home
	0.385
	3.8
	0.014
	0.84
	L-Jan. 2024
	4.0



	Beer, ale, and other malt beverages away from home(4)(5)
	 
	3.2
	 
	1.12
	L-Feb. 2025
	3.2



	Wine away from home(4)(5)
	 
	3.1
	 
	0.62
	S-Jun. 2025
	2.9



	Distilled spirits away from home(4)(5)
	 
	4.0
	 
	1.37
	L-Apr. 2025
	4.0



	Other goods(10)
	1.288
	3.1
	0.041
	0.44
	L-Apr. 2025
	3.2



	Tobacco and smoking products
	0.488
	6.3
	0.031
	0.64
	S-Jun. 2025
	6.3



	Cigarettes(4)
	0.368
	7.7
	0.029
	0.64
	S-Jun. 2025
	7.5



	Tobacco products other than cigarettes(4)
	0.114
	1.5
	0.002
	1.69
	L-Apr. 2025
	1.5



	Personal care products
	0.642
	1.1
	0.008
	0.70
	L-May 2024
	1.3



	Hair, dental, shaving, and miscellaneous personal care products(4)
	0.286
	1.8
	0.005
	0.94
	L-Jul. 2024
	1.9



	Cosmetics, perfume, bath, nail preparations and implements
	0.347
	0.3
	0.002
	1.19
	S-Jun. 2025
	0.3



	Miscellaneous personal goods(4)
	0.158
	1.4
	0.002
	1.47
	S-Mar. 2025
	0.7



	Stationery, stationery supplies, gift wrap(5)
	 
	1.1
	 
	1.65
	S-Oct. 2024
	0.6



	Services less energy services
	60.648
	3.6
	2.172
	0.17
	-
	-



	Shelter
	35.434
	3.6
	1.292
	0.19
	S-Oct. 2021
	3.5



	Rent of shelter(15)
	35.014
	3.6
	1.269
	0.19
	-
	-



	Rent of primary residence
	7.451
	3.5
	0.263
	0.19
	-
	-



	Lodging away from home(4)
	1.363
	-2.6
	-0.041
	2.02
	L-Jun. 2025
	-2.5



	Housing at school, excluding board(15)
	0.240
	3.5
	0.008
	0.32
	L-Jun. 2025
	3.5



	Other lodging away from home including hotels and motels
	1.123
	-3.7
	-0.050
	2.40
	L-Jun. 2025
	-3.7



	Owners' equivalent rent of residences(15)
	26.199
	4.0
	1.047
	0.18
	S-Dec. 2021
	3.8



	Owners' equivalent rent of primary residence(15)
	25.004
	4.0
	0.996
	0.18
	S-Dec. 2021
	3.8



	Tenants' and household insurance(4)
	0.420
	5.7
	0.023
	0.78
	S-Jun. 2025
	4.8



	Water and sewer and trash collection services(4)
	1.089
	5.3
	0.057
	0.35
	-
	-



	Water and sewerage maintenance
	0.742
	4.8
	0.035
	0.37
	S-May 2025
	4.8



	Garbage and trash collection(12)
	0.347
	6.5
	0.022
	0.69
	L-Dec. 2023
	6.5



	Household operations(4)
	 
	 
	 
	 
	 
	 



	Domestic services(4)
	 
	 
	 
	 
	 
	 



	Gardening and lawncare services(4)
	 
	 
	 
	 
	 
	 



	Moving, storage, freight expense(4)
	0.129
	1.0
	0.001
	4.88
	S-Apr. 2025
	1.0



	Repair of household items(4)
	 
	 
	 
	 
	 
	 



	Medical care services
	6.780
	4.2
	0.280
	0.63
	S-Jun. 2025
	3.4



	Professional services
	3.687
	3.5
	0.128
	0.85
	L-Apr. 2021
	3.5



	Physicians' services
	1.804
	3.5
	0.062
	1.42
	L-Dec. 2021
	4.3



	Dental services
	0.957
	4.2
	0.039
	1.58
	S-Jun. 2025
	2.4



	Eyeglasses and eye care(8)
	0.331
	3.7
	0.012
	0.96
	L-Mar. 2025
	4.2



	Services by other medical professionals(8)
	 
	 
	 
	 
	 
	 



	Hospital and related services
	2.294
	5.3
	0.120
	0.73
	S-Jun. 2025
	4.2



	Hospital services(16)
	1.971
	 
	0.104
	0.85
	-
	-



	Inpatient hospital services(5)(16)
	 
	 
	 
	 
	 
	 



	Outpatient hospital services(5)(8)
	 
	 
	 
	1.87
	-
	-



	Nursing homes and adult day services(16)
	0.167
	4.5
	0.007
	0.75
	S-Feb. 2025
	4.1



	Care of invalids and elderly at home(7)
	0.156
	5.6
	0.009
	2.37
	-
	-



	Health insurance(7)
	0.799
	4.3
	0.032
	0.62
	S-Jun. 2025
	3.4



	Transportation services
	6.257
	3.5
	0.222
	0.67
	-
	-



	Leased cars and trucks(13)
	0.384
	 
	-0.008
	1.84
	-
	-



	Car and truck rental(4)
	0.153
	-4.8
	-0.007
	1.58
	S-Mar. 2025
	-8.7



	Motor vehicle maintenance and repair
	1.030
	8.5
	0.091
	1.82
	L-Nov. 2023
	8.5



	Motor vehicle body work
	 
	 
	 
	 
	 
	 



	Motor vehicle maintenance and servicing
	0.508
	3.6
	0.019
	0.76
	L-May 2025
	3.6



	Motor vehicle repair(4)
	0.407
	15.0
	0.063
	3.85
	L-Oct. 2023
	15.1



	Motor vehicle insurance
	2.818
	4.7
	0.132
	1.06
	S-May 2022
	4.5



	Motor vehicle fees(4)
	0.493
	0.8
	0.004
	0.55
	S-Sep. 2021
	0.8



	State motor vehicle registration and license fees(4)
	0.280
	0.7
	0.002
	0.28
	S-Jun. 2022
	0.7



	Parking and other fees(4)
	0.200
	0.9
	0.001
	1.22
	L-May 2025
	1.2



	Parking fees and tolls(4)(5)
	 
	3.1
	 
	1.12
	L-Mar. 2025
	3.6



	Public transportation
	1.378
	2.2
	0.010
	0.92
	L-Jan. 2025
	4.9



	Airline fares
	0.847
	3.3
	0.018
	1.21
	L-Jan. 2025
	7.1



	Other intercity transportation
	0.213
	-2.0
	-0.007
	2.10
	-
	-



	Ship fare(4)(5)
	 
	-5.4
	 
	2.52
	S-Aug. 2022
	-7.4



	Intracity transportation
	0.314
	0.0
	-0.001
	1.02
	S-Jun. 2025
	-0.6



	Intracity mass transit(5)(10)
	 
	2.0
	 
	2.29
	L-Jun. 2025
	2.3



	Recreation services(10)
	3.457
	3.6
	0.122
	0.52
	S-Apr. 2025
	3.6



	Video and audio services(10)
	0.811
	1.6
	0.012
	0.72
	S-Apr. 2025
	0.6



	Cable, satellite, and live streaming television service(12)
	0.654
	1.7
	0.011
	0.50
	S-Apr. 2025
	0.3



	Purchase, subscription, and rental of video(4)
	0.157
	0.6
	0.001
	3.19
	S-Mar. 2022
	-2.2



	Video discs and other media(4)(5)
	 
	-0.1
	 
	3.73
	S-Jun. 2025
	-1.0



	Subscription and rental of video and video games(4)(5)
	 
	3.8
	 
	2.16
	S-Dec. 2024
	1.6



	Pet services including veterinary(4)
	0.539
	5.5
	0.028
	1.07
	S-May 2025
	4.9



	Pet services(4)(5)
	 
	5.8
	 
	2.58
	S-May 2025
	4.9



	Veterinarian services(4)(5)
	 
	6.4
	 
	1.88
	-
	-



	Photographers and photo processing(4)
	0.053
	-3.0
	-0.002
	2.15
	S-Jun. 2025
	-3.1



	Other recreation services(4)
	2.054
	4.3
	0.084
	0.62
	S-Jun. 2025
	4.0



	Club membership for shopping clubs, fraternal, or other organizations, or participant sports fees(4)
	0.808
	4.0
	0.030
	0.50
	S-Oct. 2024
	3.8



	Admissions
	0.747
	4.6
	0.032
	1.52
	L-Apr. 2025
	5.9



	Admission to movies, theaters, and concerts(4)(5)
	 
	3.4
	 
	1.66
	S-Apr. 2025
	3.4



	Admission to sporting events(4)(5)
	 
	-0.5
	 
	6.37
	L-Apr. 2025
	9.3



	Fees for lessons or instructions(8)
	0.169
	3.2
	0.006
	1.18
	S-May 2025
	2.7



	Education and communication services(10)
	4.904
	1.0
	0.051
	0.24
	S-May 2025
	1.0



	Tuition, other school fees, and childcare
	2.513
	3.3
	0.082
	0.43
	S-Aug. 2024
	3.2



	College tuition and fees
	1.297
	2.2
	0.029
	0.68
	S-Jun. 2025
	2.2



	Elementary and high school tuition and fees
	0.388
	3.7
	0.014
	0.37
	L-Jun. 2025
	3.8



	Day care and preschool(11)
	0.724
	5.0
	0.036
	0.51
	S-Jun. 2024
	4.9



	Technical and business school tuition and fees(4)
	0.039
	2.2
	0.001
	0.92
	L-Nov. 2023
	2.2



	Postage and delivery services(4)
	0.053
	4.7
	0.003
	0.47
	L-Jan. 2025
	7.6



	Postage
	0.051
	4.9
	0.003
	0.54
	L-Jan. 2025
	8.4



	Delivery services(4)
	0.002
	8.2
	0.000
	0.88
	L-Mar. 2023
	10.5



	Telephone services(4)
	1.412
	-1.7
	-0.025
	0.16
	S-Apr. 2024
	-1.7



	Wireless telephone services(4)
	1.242
	-2.1
	-0.027
	0.20
	S-May 2024
	-2.1



	Residential telephone services(10)
	0.170
	1.3
	0.002
	0.63
	S-May 2025
	1.2



	Internet services and electronic information providers(4)
	0.917
	-0.8
	-0.008
	0.54
	L-Feb. 2025
	-0.7



	Other personal services(10)
	1.660
	4.4
	0.072
	0.51
	S-Jun. 2025
	4.3



	Personal care services
	0.658
	3.7
	0.024
	0.73
	S-Apr. 2025
	3.6



	Haircuts and other personal care services(4)
	0.658
	3.7
	0.024
	0.73
	S-Apr. 2025
	3.6



	Miscellaneous personal services
	1.001
	4.9
	0.048
	0.57
	S-Jun. 2025
	4.6



	Legal services(8)
	 
	 
	 
	 
	 
	 



	Funeral expenses(8)
	0.139
	2.6
	0.004
	0.81
	S-Jun. 2025
	2.1



	Laundry and dry cleaning services(4)
	0.166
	4.8
	0.008
	0.82
	S-May 2025
	3.6



	Apparel services other than laundry and dry cleaning(4)
	0.027
	5.2
	0.001
	2.34
	S-Jun. 2025
	2.2



	Financial services(8)
	0.254
	4.7
	0.012
	1.41
	S-Jun. 2025
	2.5



	Checking account and other bank services(4)(5)
	 
	0.4
	 
	3.33
	S-Aug. 2023
	0.3



	Tax return preparation and other accounting fees(4)(5)
	 
	6.4
	 
	2.69
	S-Jun. 2025
	3.2



	


	Special aggregate indexes
				



	


	All items less food
	86.365
	2.9
	2.484
	0.13
	L-Feb. 2025
	2.9



	All items less shelter
	64.566
	2.5
	1.624
	0.14
	L-Apr. 2023
	3.4



	All items less food and shelter
	50.931
	2.3
	1.192
	0.17
	L-Mar. 2024
	2.4



	All items less food, shelter, and energy
	44.509
	2.7
	1.190
	0.19
	L-Jun. 2023
	2.7



	All items less food, shelter, energy, and used cars and trucks
	42.072
	2.5
	1.052
	0.19
	L-Jun. 2024
	2.5



	All items less medical care
	91.709
	2.9
	2.635
	0.10
	L-Jan. 2025
	3.0



	All items less energy
	93.578
	3.1
	2.914
	0.12
	L-Jan. 2025
	3.1



	Commodities
	36.097
	1.3
	0.505
	0.12
	L-Sep. 2023
	1.4



	Commodities less food, energy, and used cars and trucks
	16.858
	1.0
	0.172
	0.20
	L-Oct. 2023
	1.4



	Commodities less food
	22.462
	0.2
	0.073
	0.16
	L-Sep. 2023
	0.2



	Commodities less food and beverages
	21.637
	0.2
	0.057
	0.16
	L-Feb. 2023
	0.5



	Services
	63.903
	3.8
	2.411
	0.17
	-
	-



	Services less rent of shelter(15)
	28.890
	4.0
	1.142
	0.26
	-
	-



	Services less medical care services
	57.123
	3.7
	2.131
	0.16
	S-Sep. 2021
	3.5



	Durables
	10.942
	1.9
	0.218
	0.21
	L-Nov. 2022
	2.4



	Nondurables
	25.155
	1.1
	0.287
	0.14
	L-Feb. 2025
	1.3



	Nondurables less food
	11.520
	-1.1
	-0.146
	0.24
	L-Feb. 2025
	0.0



	Nondurables less food and beverages
	10.695
	-1.4
	-0.161
	0.25
	L-Feb. 2025
	-0.2



	Nondurables less food, beverages, and apparel
	8.237
	-1.8
	-0.167
	0.26
	L-Feb. 2025
	-0.4



	Nondurables less food and apparel
	9.062
	-1.5
	-0.152
	0.25
	L-Feb. 2025
	-0.2



	Housing
	44.371
	4.0
	1.757
	0.18
	L-Jun. 2025
	4.0



	Education and communication(4)
	5.638
	0.3
	0.020
	0.26
	-
	-



	Education(4)
	2.555
	3.5
	0.090
	0.41
	S-Aug. 2024
	3.1



	Communication(4)
	3.083
	-2.1
	-0.070
	0.43
	L-Jun. 2025
	-1.9



	Information and information processing(4)
	3.030
	-2.2
	-0.073
	0.44
	L-Jun. 2025
	-2.0



	Information technology, hardware and services(17)
	1.618
	-2.8
	-0.048
	0.81
	L-Sep. 2024
	-2.5



	Recreation(4)
	5.292
	2.3
	0.122
	0.36
	S-Jun. 2025
	2.1



	Video and audio(4)
	1.064
	1.5
	0.016
	0.64
	S-May 2025
	1.5



	Pets, pet products and services(4)
	1.165
	2.5
	0.029
	0.90
	S-Jun. 2025
	2.1



	Photography(4)
	0.074
	-1.2
	-0.001
	1.86
	S-Jun. 2025
	-2.0



	Food and beverages
	14.460
	3.1
	0.448
	0.13
	L-Oct. 2023
	3.3



	Domestically produced farm food
	6.724
	2.4
	0.160
	0.19
	L-Mar. 2025
	2.6



	Other services
	10.021
	2.5
	0.246
	0.24
	S-Jun. 2025
	2.5



	Apparel less footwear
	1.884
	0.0
	-0.003
	0.85
	L-Mar. 2025
	0.7



	Fuels and utilities
	4.478
	6.9
	0.295
	0.66
	L-Mar. 2023
	7.3



	Household energy
	3.389
	7.4
	0.238
	0.84
	L-Mar. 2023
	7.8



	Medical care
	8.291
	3.4
	0.281
	0.60
	S-Jun. 2025
	2.8



	Transportation
	16.542
	0.9
	0.169
	0.31
	L-Feb. 2025
	1.7



	Private transportation
	15.164
	1.0
	0.159
	0.32
	L-Feb. 2025
	1.9



	New and used motor vehicles(4)
	7.427
	2.1
	0.155
	0.19
	L-Nov. 2022
	3.6



	Utilities and public transportation
	7.788
	3.8
	0.292
	0.42
	L-Apr. 2023
	4.2



	Household furnishings and operations
	4.460
	3.9
	0.170
	0.50
	L-May 2023
	4.2



	Other goods and services
	2.948
	3.9
	0.113
	0.39
	-
	-



	Personal care
	2.460
	3.4
	0.082
	0.44
	L-Jul. 2024
	3.4







	
		Footnotes
		(1) The 'effect' of an item category is a measure of that item's contribution to the All items price change. For example, if the Food index had an effect of 0.40, and the All items index rose 1.2 percent, then the increase in food prices contributed 0.40 / 1.2, or 33.3 percent, to that All items increase. Said another way, had food prices been unchanged for that year the change in the All items index would have been 1.2 percent minus 0.40, or 0.8 percent.  Effects can be negative as well. For example, if the effect of food was a negative 0.1, and the All items index rose 0.5 percent, the All items index actually would have been 0.1 percent higher (or 0.6 percent) had food prices been unchanged. Since food prices fell while prices overall were rising, the contribution of food to the All items price change was negative (in this case, -0.1 / 0.5, or minus 20 percent).
		(2) A statistic's margin of error is often expressed as its point estimate plus or minus two standard errors. For example, if a CPI category rose 2.6 percent, and its standard error was 0.25 percent, the margin of error on this item's 12-month percent change would be 2.6 percent, plus or minus 0.5 percent.
		(3) If the current 12-month percent change is greater than the previous published 12-month percent change, then this column identifies the closest prior month with a 12-month percent change as (L)arge as or (L)arger than the current 12-month change.  If the current 12-month percent change is smaller than the previous published 12-month percent change, the most recent month with a change as (S)mall or (S)maller than the current month change is identified.  If the current and previous published 12-month percent changes are equal, a dash will appear.  Standard numerical comparison is used.  For example, 2.0% is greater than 0.6%, -4.4% is less than -2.0%, and -2.0% is less than 0.0%.  Note that a (L)arger change can be a smaller decline, for example, a -0.2% change is larger than a -0.4% change, but still represents a decline in the price index.  Likewise, (S)maller changes can be increases, for example, a 0.6% change is smaller than 0.8%, but still represents an increase in the price index.  In this context, a -0.2% change is considered to be smaller than a 0.0% change.
		(4) Indexes on a December 1997=100 base.
		(5) Special indexes based on a substantially smaller sample.  These series do not contribute to the all items index aggregation and therefore do not have a relative importance or effect.
		(6) Indexes on a December 2007=100 base.
		(7) Indexes on a December 2005=100 base.
		(8) Indexes on a December 1986=100 base.
		(9) Indexes on a December 1993=100 base.
		(10) Indexes on a December 2009=100 base.
		(11) Indexes on a December 1990=100 base.
		(12) Indexes on a December 1983=100 base.
		(13) Indexes on a December 2001=100 base.
		(14) Indexes on a December 2019=100 base.
		(15) Indexes on a December 1982=100 base.
		(16) Indexes on a December 1996=100 base.
		(17) Indexes on a December 1988=100 base.
	





]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gregg Kellogg has passed away]]></title>
            <link>https://lists.w3.org/Archives/Public/public-json-ld-wg/2025Sep/0012.html</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210564</guid>
            <description><![CDATA[Received on Thursday, 11 September 2025 11:58:56 UTC]]></description>
            <content:encoded><![CDATA[

Dear Members of the group,

We were informed yesterday of the passing last Saturday of Gregg Kellogg. It is terrible news but Gregg had been open about his health <https://greggkellogg.net/health-faq.html>.

Gregg had been a prolific and appreciated W3C contributor for many years as a W3C Invited Expert, most recently as co-chair of the JSON-LD Working Group; he was also Chair of several data-related Community Groups. He was said to truly value the energy, brilliance, and camaraderie he found in his groups, and to feel rewarded for being part of such an inventive community.

Over the past 13 years [1], Gregg has been co-editor of 9 published recommendations, and a dozen other W3C specifications (CSV2RDF suite, a large portion of the RDF 1.2 suite, 1.0 and 1.1 versions of the JSON-LD suite, RCH, etc.) Remarkably, Gregg also provided open-source implementations of all these specifications (and more), as well a numerous test suites that the relevant Working Groups are still using. His implication in the JSON-LD Working Group was instrumental in the huge success of this W3C technology.

For all this, but also and foremost for his friendliness and good nature, he will be missed.

If any of you would like to pay tribute to honor Gregg Kellogg, please get in touch with the JSON-LD Working Group Staff Contact, Pierre-Antoine Champin <pierre-antoine@w3.org>, as this group is currently making plans.

With kind regards,
Coralie Mercier, Director of W3C Marketing & Communications

[1] https://api.w3.org/users/8nomt3nm1n8cw4s44koggo4wcoo8w4s/specifications


[This message was distributed to:
* the Members mailing list;
* the W3C Groups chairs mailing list;
* the public mailing lists of the WG and CGs that Gregg was chairing: public-json-ld-wg public-json-ld public-rdf-tests public-schemaorg]


--
Coralie Mercier (she/her) - Director of W3C Marketing & Communications 
mailto:coralie@w3.org - https://www.w3.org/People/Coralie/

Received on Thursday, 11 September 2025 11:58:56 UTC
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Piramidal (YC W24) Is Hiring Back End Engineer]]></title>
            <link>https://www.ycombinator.com/companies/piramidal/jobs/1HvdaXs-full-stack-engineer-platform</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210539</guid>
            <description><![CDATA[We are looking for a software engineer to help us enable interactions and automations with Piramidal’s newest technologies. We value proactive, customer-centric engineers who prioritize foundational details (data models, architecture, security) to enable excellent products.
In this role you will:
Build and maintain the infrastructure and backend systems for our flagship platform focused on neural data.
Collaborate closely with ML engineers to iterate on applying our latest models. and
Work with the product team and our internal customers to understand their problems and implement effective solutions.
Your background looks something like:
3+ years of engineering at product-driven companies.
Proficiency with Python and other backend languages.
Proficiency with containerisation and orchestration technologies (e.g., Kubernetes).
Proficiency with relational databases (e.g. Postgres/MySQL).
Proficiency with web technologies (e.g. JavaScript, React).
Ability to move fast and independently.
About Us
We are building a first-of-its-kind foundation model for electrophysiological brain data. Our goal is to create scaled neural decoders that enable humans to understand and control neural syntax. 
We are dedicated to redirecting technology to maximize human potential. At the heart of our mission is support for cognitive liberty - the fundamental right to freedom of thought, mental privacy, and self-determination.]]></description>
            <content:encoded><![CDATA[Foundation Model for the BrainFull Stack Engineer - Platform$120K - $210K•0.10% - 0.50%•New York, NY, US / San Francisco, CA, US / Remote (San Francisco, CA, US; Chicago, IL, US; Austin, TX, US)Job typeFull-timeRoleEngineering, Full stackExperience3+ yearsVisaUS citizen/visa onlySkillsPython, Software ArchitectureConnect directly with founders of the best YC-funded startups.Apply to role ›About the roleWe are looking for a software engineer to help us enable interactions and automations with Piramidal’s newest technologies. We value proactive, customer-centric engineers who prioritize foundational details (data models, architecture, security) to enable excellent products.
In this role you will:

Build and maintain the infrastructure and backend systems for our flagship platform focused on neural data.
Collaborate closely with ML engineers to iterate on applying our latest models. and
Work with the product team and our internal customers to understand their problems and implement effective solutions.

Your background looks something like:

3+ years of engineering at product-driven companies.
Proficiency with Python and other backend languages.
Proficiency with containerisation and orchestration technologies (e.g., Kubernetes).
Proficiency with relational databases (e.g. Postgres/MySQL).
Proficiency with web technologies (e.g. JavaScript, React).
Ability to move fast and independently.

About Us
We are building a first-of-its-kind foundation model for electrophysiological brain data. Our goal is to create scaled neural decoders that enable humans to understand and control neural syntax.
We are dedicated to redirecting technology to maximize human potential. At the heart of our mission is support for cognitive liberty - the fundamental right to freedom of thought, mental privacy, and self-determination.
About the interview
Initial Screen: 30 minute discussion to ensure timelines, experience and expectations align.
Technical Screen: 1:15h live coding and system design via Google Meet.
Product screen: Product strategy, prioritisation, and user needs.

About PiramidalFounded:2024Batch:W24Team Size:10Status:ActiveLocation:New YorkFoundersSimilar Jobs]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI's $344B 'Language Model' Bet Looks Fragile]]></title>
            <link>https://www.bloomberg.com/opinion/articles/2025-09-11/ai-s-344-billion-language-model-bet-looks-fragile</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210451</guid>
        </item>
        <item>
            <title><![CDATA[Brussels faces privacy crossroads over encryption backdoors]]></title>
            <link>https://www.theregister.com/2025/09/11/eu_chat_control/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45210442</guid>
            <description><![CDATA[: Over 600 security boffins say planned surveillance crosses the line]]></description>
            <content:encoded><![CDATA[
Europe, long seen as a bastion of privacy and digital rights, will debate this week whether to enforce surveillance on citizens' devices.
Representatives from member states will meet on Friday to consider legislation critics call Chat Control, aka "laying down rules to prevent and combat child sexual abuse," which seeks to require ISPs or messaging app providers to scan user content or backdoor encryption so that intelligence agencies can do it themselves. It's the latest attempt in a three-year campaign by some in the community to allow government agencies unprecedented access to private communications.
The proposed legislation has been in the works since 2022 but immediately drew fire from security professionals. After being rejected by EU member states repeatedly, this latest attempt has come at the request of the Danish delegation, which currently holds the EU presidency, and should go to a full vote next month.

    

An open letter signed by more than 600 security academics, practitioners, and stakeholders has called on the proposals to be dropped and claimed they are unworkable and highly intrusive. It also points out that the false positive detection rate for such a serious crime is unacceptable and could lead to many people being unfairly smeared.

        


        

One signatory, Matthew Green, associate professor of computer science at the Johns Hopkins Information Security Institute, told The Register that the plans, if implemented, would be a "national security disaster."
He pointed out that if encryption backdoors were implemented, adversarial nations would see it as a "Manhattan Project" which could be used to expose all data, and if client-side scanning was used then it would create a privacy nightmare.

        

The revised legislative proposals call for systems to be set up to find all current "and new" forms of CSAM, but decline to give any guidance as to how this seemingly impossible task would be achieved. Government and military communications would be exempt from the plan.
"It is science fiction," fellow signatory Bart Preneel, the Belgian cryptographer and former president of the International Association for Cryptologic Research, told us. "The latest draft extends the detection order to new CSAM – it is assumed that AI can do this in a reliable way 'quod non.'" This is a Latin term loosely translated as "which it does not."
While there are plenty of companies that would love to provide this service, they lack the technical expertise to do so, he pointed out. Also, the best estimates show around a 10 percent false positive rate for client-side scanning – which could see a huge number of people accused of crimes they didn't commit.


EU attempt to sneak through new encryption-eroding law slammed by Signal, politicians

European Court of Human Rights declares backdoored encryption is illegal

German Digital Affairs Committee hearing heaps scorn on Chat Control

Scanning phones to detect child abuse evidence is harmful, 'magical' thinking

If passed, the legislation would require encrypted app makers like WhatsApp, iMessage, Signal, Telegram, and Tuta to find ways to enforce such scanning – something they have neither the ability nor the desire to do.
Similar legislation has passed in the UK, but with an admission that the plans for message scanning are unworkable at the moment. Attempts to enforce them have failed, and drawn the ire of the US government, which has warned it would not look on such proposals favorably.

        

Signal, possibly the gold standard of end-to-end encrypted services, has said it will fight any moves to enforce such rules. Tuta spokesperson Hanna Bozakov told us that the company would not comply and would consider moving outside the EU if the legislation passed, but only after fighting it in the courts.
"First of all, we will sue, because we are pretty certain that this will not stand up in court," she said. "You can't do this because we have privacy rights in the EU Constitution, and you can't just overwrite this."
However, sources told The Register that some EU members might be getting cold feet about the plans. Two people told us that the German delegation, which has previously been highly skeptical of the proposals, could ask for a delay for further consideration. We'll see on Friday what happens. ®                                
                    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reshaped is now open source]]></title>
            <link>https://reshaped.so/blog/reshaped-oss</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45209558</guid>
            <description><![CDATA[After five years of closed-source, Reshaped is finally open for all – both in React and Figma.]]></description>
            <content:encoded><![CDATA[About five years ago, I started Reshaped.
I built it for myself, since I had a few projects in urgent need of a component library for both React and Figma.
Having worked in the design systems space for over a decade, I had developed a clear sense of what a good design system should be — and what tends to go wrong in others.
I noticed that no matter how trends evolve, around 80% of the web is still built on the same core design practices.
So I set out to build a system that covers that 80%, while giving developers the flexibility to handle the last 20% with low-level utilities.
From the start, I didn’t want to focus only on accessibility or only on design.
Instead, I prioritized alignment between design and engineering, while also solving common UI challenges like theming, dark mode, and micro-animations.
To keep the project sustainable, I made it a paid product: one-time licenses for individuals, and source code licenses for larger teams.
This allowed me to focus on supporting a smaller community and dive deeply into every bug report and feature request.
While this model kept me motivated and financially supported, I always hoped to remove the paywall one day.
Two years ago, I took the first step by making the React package free.
That unlocked new possibilities—not only did indie developers gain free access, but teams using Reshaped with source code licenses could now install it directly from npm.


Today, I’m taking the next step: making all of Reshaped fully open source. The React library source code is now on GitHub and the Figma library is available in the Figma Community.
I’m especially excited because Reshaped bridges both design and engineering, and I hope it helps both communities learn best practices for building design systems that scale while staying minimal.
Making both libraries public also opens the door for me to share more behind-the-scenes work as new features roll out.
I think this is particularly valuable when it comes to integration with other tools.
Imagine Figma or React releasing new features – you’ll be able to see how they’re implemented in Reshaped before you even need to migrate your company’s design system.


Reshaped component libraries will continue to grow.
Everyone who purchased licenses in the past will still get full access to future updates, and we’ll keep chatting through the same channels.
Nothing changes there.
I’m also interested in taking Reshaped further by introducing more complex, opinionated premium components on top of the core library.
Not “50 landing page layouts,” but advanced components that require sophisticated CSS and React logic.

This is a leap of faith for me after five years of working closed-source.
It feels like the right time to give everything back to the community—and to have some fun along the way ❤️
]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DeepCodeBench: Real-World Codebase Understanding by Q&A Benchmarking]]></title>
            <link>https://www.qodo.ai/blog/deepcodebench-real-world-codebase-understanding-by-qa-benchmarking/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45209532</guid>
            <description><![CDATA[Read about Qodo's new benchmark, which evaluates how well AI agents navigate and understand large, complex enterprise codebases using real-world questions.]]></description>
            <content:encoded><![CDATA[
									
											
					At Qodo, we’ve created a new benchmark dataset of real-world questions derived from large, complex code repositories. We are excited to release the dataset, methodology, and prompts used in its creation to support further research and development.
Motivation
Enterprises often maintain massive codebases that are difficult for any individual developer to navigate and fully understand. Whether onboarding, doing routine development, or using AI-assisted workflows, teams often have questions about their codebase. To effectively address this, we’ve developed specialized retrieval capabilities within our research agents. However, to benchmark and validate these systems effectively, we require a robust set of real-world questions and answers.
Prior Work
Existing benchmarks, such as CodeQA, primarily contain artificially generated code with questions limited to provided code snippets, requiring no retrieval from broader contexts. Another recent work (arXiv:2407.02883) involves real-world scenarios but focuses on retrieval from databases rather than code repositories, which does not adequately represent common real-world use-cases.
To address this gap, we propose a new approach. We introduce a benchmark based on realistic questions derived from pull requests that require retrieval across multiple files in a codebase.
Dataset Generation
To effectively challenge retrieval systems, questions in our benchmark must:

Require deep retrieval, often spanning multiple interconnected files.
Reflect realistic questions developers encounter when solving actual issues.

We identified that pull requests (PRs) are good sources for complex code changes with proper context that can be used for question and answer generation. PRs naturally link related code, not always through explicit imports or function calls, but through functional changes made together. We leveraged this insight to generate context:

For each code change within a PR, we retrieved its containing method, class or file from the current default branch.
We bundled these retrieved code snippets along with the PR’s title and description to form a meaningful context..

Using the above mentioned PR data as context, we prompt large language models (LLMs) to generate questions relevant to real developer workflows, ensuring authenticity and practical value (see prompt in Appendix). The same context is also used to generate the ground-true answer.

Example
Consider PR 39363 of Hugging Face’s Transformers repository. This PR touches 4 files in different places, including methods such as:

BaseImageProcessorFast.__init__ (src/transformers/image_processing_utils_fast.py)
BaseVideoProcessor.__init__  (src/transformers/video_processing_utils.py)

We are not concerned with the exact code changes in the PR. Instead, we use the PR as a signal to locate blocks of code that are functionally related and should be considered together.
We try to find these methods in the updated code to avoid asking questions about irrelevant code. We don’t always find them, due to renames and refactors, but in this PR we found both BaseImageProcessorFast and BaseVideoProcessor.
In the next step, we pass the complete methods, along with the PR title, description, and the prompt in Appendix A, to an LLM, asking it to generate a question.
For this PR, the generated question is:
How do the fast image and video processor base classes prevent shared mutable state when instantiating multiple instances?
And the answer is:
Both the fast image and video processor bases deep-copy their mutable defaults when you instantiate them. In BaseImageProcessorFast.__init__ (src/transformers/image_processing_utils_fast.py) and BaseVideoProcessor.__init__ (src/transformers/video_processing_utils.py) they loop over all valid kwargs and do:

 If you passed a value, use it;
Otherwise, setattr(self, key, deepcopy(getattr(self, key))), and they rebuild size/crop_size via get_size_dict.

 By deep-copying every default dict/list on init, no two instances share the same mutable object.
Dataset Statistics
We generated 1,144 questions derived from eight open-source repositories. Below are detailed insights into the characteristics of the dataset:
Context Distribution
The histograms illustrate the distribution of context used for each question:

Number of Context Blocks: Indicates how many individual code blocks were involved in generating each question.
Number of Context Files: Reflects the number of distinct files utilized per question.

In the example above, there are two blocks across two files. However, PRs often touch multiple methods within the same file, resulting in more blocks than files.

Categorical Breakdown
Scope:

Deep: Questions focusing on specific, detailed aspects of a single block of code.
Broad: Questions involving interactions or relationships across multiple code blocks or files.

Core Questions: Questions targeting fundamental, core functionality versus those focusing on peripheral technical details.
Searchable Questions: Questions containing specific keywords or identifiers that facilitate direct searches within the codebase.

Evaluation Mechanism: LLM as a Judge

Evaluating model predictions requires an objective and scalable approach.Rather than relying solely on subjective LLM judgment, we:

Extracted discrete, verifiable facts from each ground-truth (GT) answer.
Checked whether each fact appeared in the predicted answer using a simple LLM call.

This method, that we call “fact recall,” was introduced in the 2003 TREC (Text REtrieval Conference) QA Track (paper, overview) and is widely used today – for example in Google/DeepMind’s SAFE and in the TREC 2024 RAG Track (e.g., MSR/Waterloo’s AutoNuggetizer). It ensures robust, objective, and scalable assessment of model performance.
Baselines
To better understand our dataset, we established several baseline evaluations:

Ground Truth (GT) answers: Verifies both the accuracy of fact extraction and the reliability of the automated fact verification method
LLM with full context: Provides an LLM with all context used to generate the questions, setting an upper-bound performance baseline.
LLM with no context: Evaluates how well an LLM could answer questions using only the repository name, capturing inherent model knowledge and setting a lower-bound baseline.

These baselines help evaluate the quality of the dataset, validate our evaluation methods, and measure the inherent knowledge of different LLMs.
Results
We evaluated Codex CLI, Claude Code, and our Deep Research agent in Qodo Aware.

Overall: Qodo’s deep-research agent achieves the best fact recall (~76%), just ahead of OpenAI’s Codex (~74%), while being about twice as fast. Also, with the high reasoning feature, we reached (~80%) with a tradeoff on runtimes, where we see a 10-second optimization for our agent. Both outperform Claude (~64%) and Gemini (~45%).
Searchable: All agents improved with searchable keywords in the question, but our DeepResearch’s gain was smallest thanks to strong semantic search.
Scope: Codex and Claude preferred deep over broad questions, while DeepResearch performed equally well on both due to wide search capabilities.

Overall results

Results by data segment




scope

codex-cli
claude-code
gemini-cli

deep-research (Qodo)




broad

0.72
0.6
0.41
0.76


deep
0.76
0.67
0.48

0.77









Searchable 

codex-cli
claude-code
gemini-cli
deep-research(Qodo)


False
0.73
0.59
0.43
0.76


True
0.76
0.68
0.47
0.77



What We’re Releasing

Dataset: 1,144 carefully curated question-answer pairs – deep_code_bench.
Metadata and context: Each question is linked to the pull request (PR) it was generated from and tagged with category labels (e.g., broad/deep, is searchable).
Prompts: The exact prompts used to guide question and answer generation.

Appendix A – prompt for question generation
System Prompt
You are helping build a high-quality dataset of real-world codebase questions to test our search AI agents. Each question should require the agent to search through the codebase to find the relevant code.
Guidelines
Your task is to generate exactly ONE onboarding question adhering to these guidelines:

The question must be clearly grounded in the provided code context.
Do not include exact file paths, line numbers, or raw code snippets in the question text.
Prefer questions involving relationships across multiple functions, components, or files.
Keep the wording concise, clear, and readable.
Avoid vague reference to code elements like ‘the function’ or ‘that class’.
Don’t make identifier references (function names, class names, variables, etc.) too obvious, so that the search will be as challenging as possible.
Despite the above, the question should still be answerable, and the context should be unambiguous.
The question should be answerable with a short, concise response—ideally, a single short sentence.

Scopes
There are 2 kinds of scopes. When provided with only 1–2 short code blocks, generate a DEEP question: a highly specific question that explores internal logic, error handling, edge cases, or detailed behaviors. When provided with multiple code blocks or a larger context, generate a BROAD question: a higher-level question about architecture, overall flow, interactions between modules, or general system design.
Core questions
Core questions targeting fundamental, core functionality versus non-core questions which are focusing on peripheral technical aspects.
PR details
If a PR title and description are provided, use them only to infer the high-level subject of the question. Think of questions that the developer needs to know in order to address the PR. The question must still be answerable using the code context. If the PR text lacks details, base the question solely on the code.
Examples
Here are examples to illustrate the desired style and scope:
Broad question examples:

What is the general workflow for training and deploying a transformer-based language model?
Can you describe the internal steps involved in performing hyperparameter tuning with a grid search?
What’s the end-to-end flow involved in generating images using diffusion-based models?

Deep question examples:

How are gradient updates managed when training gradient-boosted decision trees on sparse data?
Which parameter directly controls the number of leaves permitted in each decision tree of a gradient boosting algorithm?
How does a functional deep learning API internally handle merging layers with multiple input tensors?

Core question examples:

How are token and positional embeddings combined and fed into the BERT model?
How does the Keras Layer base class manage weight creation and the build/call lifecycle?
What happens in one XGBoost boosting iteration—how are new trees grown and combined?

Output format
Return the question, its type, whether it is a core question, and the relevant NODE IDENTIFIER headers from the context as a JSON object with keys ‘question’, ‘scope’, ‘is_core_question’, and ‘nodes’ (a list of strings). Wrap the JSON in triple backticks.
User message prompt
PR info:
{pr_context}
Code context:
{context}
Based on the PR information and code above, write ONE question and return only the requested JSON.

				
					
						
					
				
			]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Germany is not supporting ChatControl – blocking minority secured]]></title>
            <link>https://digitalcourage.social/@echo_pbreyer/115184350819592476</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45209366</guid>
        </item>
        <item>
            <title><![CDATA[PgEdge Goes Open Source]]></title>
            <link>https://www.pgedge.com/blog/pgedge-goes-open-source</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45209065</guid>
            <description><![CDATA[All the core components of pgEdge Distributed Postgres, along with any other pgEdge repositories that previously used the pgEdge Community License have now been re-licenced under the permissive PostgreSQL License, as approved by the Open Source Initiative!]]></description>
            <content:encoded><![CDATA[In November last year after nearly two decades at my previous gig, I came to the conclusion that I didn’t want to work at what seemed to be rapidly becoming an AI-focused company and moved to pgEdge where the focus is well and truly on distributed PostgreSQL and Postgres generally. Distributed databases (and particularly Postgres of course) have always been a passion of mine – even being a key topic of my master’s dissertation many years ago.Moving to pgEdge was a breath of fresh air. Not only did I get to work with some outstanding engineers and other folks on Postgres, but a good number of them were friends and colleagues that I’d worked with in the past. I’ve since had the privilege of hiring even more colleagues from the Postgres world, and look forward to expanding the team even further with more fantastic engineers from the PostgreSQL and wider database communities.There was a wrinkle in my ideal view of how things should be though - the key components of pgEdge were “source available” and not Open Source. That means the source code to our replication engine known as Spock and key extensions such as Snowflake which provides cluster-wide unique sequence values and Lolor which enables logical replication of large objects, had a proprietary licence – known as the pgEdge Community License – which allowed you to view and modify the source code, but limited how you could actually use it. Well, I’m pleased to be able to say that that is no longer the case. All the core components of pgEdge Distributed Postgres, along with any other pgEdge repositories that previously used the pgEdge Community License have now been re-licenced under the permissive PostgreSQL License, as approved by the Open Source Initiative!We’re proud to be able to make this change to support Open Source software and contribute to the PostgreSQL ecosystem, and I’m looking forward to seeing us continue to expand our contributions as much as we can.So, if you want to try out multimaster distributed Postgres, and get involved with the development of the technology, head on over to GitHub and in particular check out the spock, snowflake, and lolor repositories.If you just want to use the tech without having to build it yourself or are looking for supported builds for production use, then we have cloud, container, and VM options you can try out on our website.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Court rejects Verizon claim that selling location data without consent is legal]]></title>
            <link>https://arstechnica.com/tech-policy/2025/09/court-rejects-verizon-claim-that-selling-location-data-without-consent-is-legal/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45206567</guid>
            <description><![CDATA[Verizon and T-Mobile lost, but AT&T beat the FCC. SCOTUS may have to step in.]]></description>
            <content:encoded><![CDATA[
          
          
Instead of providing notice to customers and obtaining or verifying customer consent itself, Verizon "largely delegated those functions via contract," the court said. This system and its shortcomings were revealed in 2018 when "the New York Times published an article reporting security breaches involving Verizon's (and other major carriers') location-based services program," the court said.
Securus Technologies, a provider of communications services to correctional facilities, "was misusing the program to enable law enforcement officers to access location data without customers' knowledge or consent, so long as the officers uploaded a warrant or some other legal authorization," the ruling said. A Missouri sheriff "was able to access customer data with no legal process at all" because Securus did not review the documents that law enforcement uploaded.
Verizon claimed that Section 222 of the Communications Act covers only call-location data, as opposed to device location data. The court disagreed, pointing to the law's text stating that customer proprietary network information includes data that is related to the location of a telecommunications service, and which is made available to the carrier "solely by virtue of the carrier-customer relationship."
"Device-location data comfortably satisfies both conditions," the court said.
Verizon chose to pay fine, giving up right to jury trial
As for Verizon's claim that the FCC violated its right to a jury trial, the court said that "Verizon could have gotten such a trial" if it had "declined to pay the forfeiture and preserved its opportunity for a de novo jury trial if the government sought to collect." Instead, Verizon chose to pay the fine "and seek immediate review in our Court."
By contrast, the 5th Circuit decision in AT&T's favor said the FCC "acted as prosecutor, jury, and judge," violating the right to a jury trial. The 5th Circuit said it was guided by the Supreme Court's June 2024 ruling in Securities and Exchange Commission v. Jarkesy, which held that "when the SEC seeks civil penalties against a defendant for securities fraud, the Seventh Amendment entitles the defendant to a jury trial."
The 2nd Circuit ruling said there are key differences between US telecom law and the securities laws considered in Jarkesy. It's because of those differences that Verizon had the option of declining to pay the penalty and preserving its right to a jury trial, the court said.
In the Jarkesy case, the problem "was that the SEC could 'siphon' its securities fraud claims away from Article III courts and compel payment without a jury trial," the 2nd Circuit panel said. "The FCC's forfeiture order, however, does not, by itself, compel payment. The government needs to initiate a collection action to do that. Against this backdrop, the agency's proceedings before a § 504(a) trial create no Seventh Amendment injury."


          
                  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Where did the Smurfs get their hats (2018)]]></title>
            <link>https://www.pipelinecomics.com/beginning-bd-smurfs-hats-origin/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45206311</guid>
            <description><![CDATA[What's that hat the Smurfs wear? And what's it covering up? Do Smurfs have luxurious locks of hair? Are they bald? And, most crucial of all, is it the wrong hat?!?]]></description>
            <content:encoded><![CDATA[
	
	
That white floppy hat on top of every Smurf’s head.



What is it?  Where did it come from?



Do Smurfs have bald spots?



We’ve got a history lesson coming up here, folks.  Sit back.



But, first, let’s answer a side question:



What’s Under a Smurf’s Hat?



There’s only one true source for information on this question, and that’s Peyo’s comics.  Accept no substitutes. Yes, there are probably moments in various movies and cartoons where a Smurf can been seen without their hat on.  That’s not Peyo.



Peyo showed us one example of a Smurf without a hat.  Go to the classic story, “The Purple Smurf.”  At one point, Papa Smurf’s lab blows up, along with his hat.







Papa Smurf is bald. This doesn’t necessarily mean that Brainy Smurf isn’t sporting a pompadour under his hat or that Hefty Smurf doesn’t have a buzz cut, but…







We know Smurfs are capable of having hair.  Smurfette has luxurious blonde locks, but she’s also not really a natural born Smurf.  Technically, she’s a concoction of Gargamel’s.  But let’s give his spell-making the benefit of the doubt.  Maybe Smurfs can have hair.



I mean, where do Smurfs come from?  What’s the genetics at work that would determine–



–wait, no, I’m not going down that rabbit hole.



Papa Smurf is definitely bald.  I bet you already assumed that, though. Let’s move on.



Where Do Smurfs Get Such Wonderful Hats?



It’s called a Phrygian cap, which is about as much fun to type as Vercingetorix, who is an amazing story for another day.



The headgear is over 2000 years old.  Here’s proof:



By Jastrow (2006), Public Domain, https://commons.wikimedia.org/w/index.php?curid=647031



Handsome fella, isn’t he?  But check out that hat!  He’s totally a Smurf, right?



His name is Attis, and this sculpture comes from somewhere in the 100s AD, though Attis lived even further back, in the 4th century BC.



In that time period, you know who else worse a Smurfs hat?  King Midas.  Yes the guy with the thing for gold. (Croesus is also involved in this story, but let’s not get completely off topic.)



Phrygis, in case your curious, is the name of an ancient group of people who lived in the Balkans region of eastern Europe — Greece, Turkey, Romania, etc.  Their language and culture went extinct by the 5th century AD. Near the end, the Romans thought of them as being lazy and dull.



Those Phrygian caps do kind of resemble a dunce cap, don’t they?  It’s completely unrelated, though.  That’s a dead end.



Liberty and Freedom



The hat is often associated with liberty and freedom. Why?



It was adopted during the French Revolution (at the end of the 18th century AD) as “the red cap of liberty” by the revolutionaries.



You can see one such cap on the French personification of “Liberté”.  Here she is:







Looks just like Papa Smurf’s hat.  And while Peyo was Belgian, he was working with French publishers, so drawing inspiration from a French symbol isn’t too crazy.



The thing that Peyo maybe didn’t realize and the French revolutionaries definitely didn’t realize, though, is that it’s the wrong hat.



The Right Hat







This is the pileus hat.  It’s a brimless hat, often made of felt or maybe wool.  It started in Ancient Greece as a sailor’s hat, and eventually found its way to Ancient Rome, too.



In Rome, a freed slave had his head shaved.  Then, they would wear a pileus, in part to keep their head warm.  The hat was a sign of the slave’s freedom/liberty.



Somewhere along the line in the French Revolution, they adopted the freed slaves’ head gear as their own symbol of freedom, but picked the wrong one.



They didn’t have Google Images back then, so don’t be too hard on them.



I’m sure Peyo picked that hat because it looked good on the Smurfs, helped further set them apart from all the other characters, and would someday make for an awesome free giveaway at Angouleme.



Side By Side



Wikipedia has an image of the Phrygian and Pileus next to each other on the bottom shelf here:



By MisterPlus65 – Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=46040581



On the bottom shelf, the Smurfs’ Phrygian cap on the left is next to the pileus in the center there.



Yes, there were metal versions of both that were used as head gear during war times.  Imagine an army of Smurfs coming over the hill to attack!



Glad You Asked?



To sum it all up: Some Greeks had a hat that the Romans borrowed and that their slaves used to represent their freedom.  2000 years later, some French revolutionaries confused that hat for a different one from the Phrygis folks and made it their own sign of freedom.



150 years after that, Peyo created the Smurfs and gave them that hat, but in white.



Here endeth the lesson.





				
		Augie De Blieck Jr.Comic fan since 1989.  Letterhack from 1991.  Pipeline writer since 1997.
		
	]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DOOMscrolling: The Game]]></title>
            <link>https://ironicsans.ghost.io/doomscrolling-the-game/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45205232</guid>
            <description><![CDATA[Can a game work where all you do is scroll?]]></description>
            <content:encoded><![CDATA[
        We’re all familiar with doomscrolling, spending too much time scrolling endless feeds of content that make you feel bad about everything.But sometimes when I hear the word “doomscrolling” it makes me think of two other things: the classic video game Doom and, well, scrolling.That got me wondering if I could make a Doom-inspired game in a web browser where the only thing you do to play is scroll. No moving your character laterally, no jumping. Just scrolling.So I made it. And it’s fun! Here’s a small clip:That’s what the game looks like. But here’s what playing it feels like:You can go play it right now on desktop or mobile. The rest of this newsletter is the non-technical story of how I made the game.The first time was a failureAs readers know, I’m not a coder, but I enjoy how vibe coding lets me turn an idea into something real. So naturally, I turned to vibe coding for this.It didn’t work.This was around nine months ago. I tried and tried, but none of the LLMs were able to help me go from idea to a playable game at all. Like, not even close. GPT-4 absolutely refused to understand that scrolling down a page means that the background should move up the page. I ended up with something kinda pathetic that didn’t even resemble a game. So I gave up, figuring this was either beyond an LLM’s skills, beyond my skills, or both.But then GPT-5 came out a few weeks ago, and I tried again to see how much better it might be at coding. In just two hours I had a very good prototype. I even made a little title screen for my prototype so it felt more like a real game:I described the game design to ChatGPT very broadly at first. I said it should work kinda like Galaga turned upside-down. But I explained that unlike Galaga, the player moves forward and backward rather than side to side, and that the monster’s position should remain relative to the floor. That and a few more details got me surprisingly far as a first step.For prototyping purposes, I asked ChatGPT to just come up with five different monsters, each one with a two-frame animation, like Space Invaders aliens. They were little more than basic shapes, and I figured at some point I’d replace them with actual pre-rendered monster sprites. But this worked for now.The original 5 monstersThen I went on vacation. I spent an hour or two each morning over coffee working on this game until the kids woke up, gradually adding and refining features one at a time.Improving the gameI focused on making the game more fun to play, with incentives to keep moving but also things to stop you from racing through it too fast. I added things like weapon upgrades for every 100 monsters you kill, a wall of fire that chases you if you stay in one place too long, and obstacles to slow you down, like brick walls and spider webs.Don’t let the wall of fire get youSome other little quality-of-life features I came up with include:Five different background textures so you can have a different visual experience each time you play.Health potions. The first one appears when you’re down to three health points. After that, they are more rare and may require weapon upgrades to reach.A visual marker when you pass your record distanceA pause screen with some statsMaking it really DoomscrollingI was pretty happy with the game and ready to share it. But then at the last minute I got another nagging idea in the back of my mind: What if it was somehow more like actual doomscrolling?It would be easy to get an RSS Feed of headlines from a news site. Could I make them appear in the game as you scroll, in a way that felt integrated with the game?First I tried having headlines just appear on the floor as you play, but it felt very tacked-on. So I came up with some lore for the game that helped.I decided that the game takes place in the future, in a lair that was sealed up today, whatever day you happen to be playing the game. And the civilization that sealed up the cave left behind plaques featuring today’s headlines for some unexplained reason. So as you play, you encounter these plaques that each has a bit of news from when the cave was sealed up. Today’s actual news.It’s not really doomscrolling if there isn’t awful news to readThe plaques have no effect on the game, except to the extent that they tempt you to read them and get distracted from the gameplay. But they’re just decorative elements. Feel free to ignore them. If you can.The headlines all come from the New York Times front page RSS feed. So in a sense, this game is actually an extremely complex single-feed RSS reader.Working with AI is still a pain. This was my solution.If you’ve ever tried to work with AI, you’ve likely run into a roadblock where you’re describing something over and over and it’s simply not getting it. “No, don’t do it that way, for the umpteenth time, do it this way.”I still planned on making the monsters as pre-rendered sprites, but the background textures, plaques, and decorative items like torches could still be rendered in-game if I could just get GPT-5 to understand what I want them to look like. An LLM isn’t really good at making artwork like this.So I simplified things. I had the AI set up simple “labs,” standalone test pages where we could work on different designs, using the style from the game. For example, here’s one “lab” I made for testing how some in-world elements would look on different backgrounds:Everything above is rendered on the fly. One big advantage of that approach is that I could randomize some visual elements in the game. For example, look at the spider webs above. They all follow the same rules but they’re all slightly different. The game’s background textures are also slightly different each time you play.Next, I set about making pre-rendered monsters. But wow, small-scale pixel art is hard. I went through a lot of versions of different monsters. Eventually, I had a few I felt were good enough. The game looked like this:It had its own charm, but in the end, I didn’t love it. Somehow, my pre-rendered monsters made the game feel worse. Maybe it’s because I just am not a good pixel artist.So I decided to see what I could do with ChatGPT in a “lab” like I did for other in-game items, but focused on monster designs. After a lot of experimentation, I settled on the simple monsters that ended up in the game:I assume doing all this computationally is more processor-intensive than using pre-rendered monsters, but it’s very smooth for me on both desktop and phone, so it must not be too intensive. I guess I’ll hear from people if it’s choppy on their device.Sometimes I still needed a little more control over how something looked. So in those cases I had ChatGPT build labs with sliders that I could adjust to decide how I want things to appear, instead of getting frustrated with the chatbot. This way I could quickly settle on a look and feel.Here for example is the lab page for the plaques. I wanted them to look integrated into the game world, so I described the parameters I wanted to play with for the text styling and plaque itself. We put a “copy settings” button that I could use to report back to the LLM once I liked it, so I could say “Okay, let’s go with these settings in the game.”I’ve made this lab page live for you to play with if you’re curious.Ship itThere are still features and adjustments I’d like to add, but I’m not on vacation anymore, so think I just need to stop here. I may still tweak it, but for now I’m calling version 1.0 ready to ship. It has successfully scratched my itch to see if I could make a fun scrolling-only game, which was really all I wanted.It should play equally well on mobile and desktop. The only difference is that with a taller device you can see more of the world at a time, which makes it a little easier.Oh, and you can save it to your home screen so it acts like a standalone app rather than a browser game. That’s how I play.Happy Doomscrolling!And that’s it for another newsletter!Here’s where I remind you that you can support my endeavors by becoming a paid member, or making a one-time donation. Every little bit matters.And if not, that’s okay, too. But maybe you can share the newsletter? Moving from Beehiiv to Ghost resulted in a subscriber hit, which I anticipated because the same thing happened when I first left Substack. It took a while to begin growing again after the first move, and you can help me get back to positive growth by spreading the word now that I’ve landed at Ghost. It would mean the world to me.Until next time, thanks as always for reading!David
    ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Intel's E2200 "Mount Morgan" IPU at Hot Chips 2025]]></title>
            <link>https://chipsandcheese.com/p/intels-e2200-mount-morgan-ipu-at</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204838</guid>
            <description><![CDATA[Intel’s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities.]]></description>
            <content:encoded><![CDATA[Intel’s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities. IPUs take things a step further, aiming to take on a wide variety of infrastructure services in a cloud environment in addition to traditional software defined networking functions. Infrastructure services are run by the cloud operator and orchestrate tasks like provisioning VMs or collecting metrics. They won’t stress a modern server CPU, but every CPU core set aside for those tasks is one that can’t be rented out to customers. Offloading infrastructure workloads also provides an extra layer of isolation between a cloud provider’s code and customer workloads. If a cloud provider rents out bare metal servers, running infrastructure services within the server may not even be an option.Intel’s incoming “Mount Morgan” IPU packs a variety of highly configurable accelerators alongside general purpose CPU cores, and aims to capture as many infrastructure tasks as possible. It shares those characteristics with its predecessor, “Mount Evans”. Flexibility is the name of the game with these IPUs, which can appear as a particularly capable network card to up to four host servers, or run standalone to act as a small server. Compared to Mount Evans, Mount Morgan packs more general purpose compute power, improved accelerators, and more off-chip bandwidth to support the whole package.Intel includes a set of Arm cores in their IPU, because CPUs are the ultimate word in programmability. They run Linux and let the IPU handle a wide range of infrastructure services, and ensure the IPU stays relevant as infrastructure requirements change. Mount Morgan’s compute complex gets an upgrade to 24 Arm Neoverse N2 cores, up from 16 Neoverse N1 cores in Mount Evans. Intel didn’t disclose the exact core configuration, but Mount Evans set its Neoverse N1 cores up with 512 KB L2 caches and ran them at 2.5 GHz. It’s not the fastest Neoverse N1 configuration around, but it’s still nothing to sneeze at. Mount Morgan of course takes things further. Neoverse N1 is a 5-wide out-of-order core with a 160 entry ROB, ample execution resources, and a very capable branch predictor. Each core is already a substantial upgrade over Neoverse N1. 24 Neoverse N2 cores would be enough to handle some production server workloads, let alone a collection of infrastructure services.Mount Morgan gets a memory subsystem upgrade to quad channel LPDDR5-6400 to feed the more powerful compute complex. Mount Evans had a triple channel LPDDR4X-4267 setup, connected to 48 GB of onboard memory capacity. If Intel keeps the same memory capacity per channel, Mount Morgan would have 64 GB of onboard memory. Assuming Intel’s presentation refers to 16-bit LPDDR4/5(X) channels, Mount Morgan would have 51.2 GB/s of DRAM bandwidth compared to 25.6 GB/s in Mount Evans. Those figures would be doubled if Intel refers to 32-bit data buses to LPDDR chips, rather than channels. A 32 MB System Level Cache helps reduce pressure on the memory controllers. Intel didn’t increase the cache’s capacity compared to the last generation, so 32 MB likely strikes a good balance between hitrate and die area requirements. The System Level Cache is truly system level, meaning it services the IPU’s various hardware acceleration blocks in addition to the CPU cores.A Lookaside Crypto and Compression Engine (LCE) sits within the compute complex, and shares lineage with Intel’s Quickassist (QAT) accelerator line. Intel says the LCE features a number of upgrades over QAT targeted towards IPU use cases. But perhaps the most notable upgrade is getting asymmetric crypto support, which was conspicuously missing from Mount Evans’s LCE block. Asymmetric cryptography algorithms like RSA and ECDHE are used in TLS handshakes, and aren’t accelerated by special instructions on many server CPUs. Therefore, asymmetric crypto can consume significant CPU power when a server handles many connections per second. It was a compelling use case for QAT, and it’s great to see Mount Morgan get that as well. The LCE block also supports symmetric crypto and compression algorithms, capabilities inherited from QAT.A programmable DMA engine in the LCE lets cloud providers move data as part of hardware accelerated workflows. Intel gives an example workflow for accessing remote storage, where the LCE helps move, compress, and encrypt data. Other accelerator blocks located in the IPU’s network subsystem help complete the process.Networking bandwidth and offloads are a core function of the IPU, and its importance can’t be understated. Cloud servers need high network and storage bandwidth. The two are often two sides of the same coin, because cloud providers might use separate storage servers accessed over datacenter networking. Mount Morgan has 400 Gbps of Ethernet throughput, double Mount Evans’s 200 Gbps.True to its smart NIC lineage, Mount Morgan uses a large number of inline accelerators to handle cloud networking tasks. A programmable P4-based packet processing pipeline, called the FXP, sits at the heart of the network subsystem. P4 is a packet processing language that lets developers express how they want packets handled. Hardware blocks within the FXP pipeline closely match P4 demands. A parser decodes packet headers and translates the packet into a representation understood by downstream stages. Downstream stages can check for exact or wildcard matches. Longest prefix matches can be carried out in hardware too, which is useful for routing.The FXP can handle a packet every cycle, and can be configured to perform multiple passes per packet. Intel gives an example where one pass processes outer packet layers to perform decapsulation and checks against access control lists. A second pass can look at the inner packet, and carry out connection tracking or implement firewall rules.An inline crypto block sits within the network subsystem as well. Unlike the LCE in the compute complex, this crypto block is dedicated to packet processing and focuses on symmetric cryptography. It includes its own packet parsers, letting it terminate IPSec and PSP connections and carry out IPSec/PSP functions like anti-replay window protection, sequence number generation, and error checking in hardware. IPSec is used for VPN connections, which are vital for letting customers connect to cloud services. PSP is Google’s protocol for encrypting data transfers internal to Google’s cloud. Compared to Mount Evans, the crypto block’s throughput has been doubled to support 400 Gbps, and supports 64 million flows.Cloud providers have to handle customer network traffic while ensuring fairness. Customers only pay for a provisioned amount of network bandwidth. Furthermore, customer traffic can’t be allowed to monopolize the network and cause problems with infrastructure services. The IPU has a traffic shaper block, letting it carry out quality of service measures completely in hardware. One mode uses a mutli-level hierarchical scheduler to arbitrate between packets based on source port, destination port, and traffic class. Another “timing wheel” mode does per-flow packet pacing, which can be controlled by classification rules set up at the FXP. Intel says the timing wheel mode gives a pacing resolution of 512 nanoseconds per slot.RDMA traffic accounts for a significant portion of datacenter traffic. For example, Azure says RDMA accounts for 70% of intra-cloud network traffic, and is used for disk IO. Mount Morgan has a RDMA transport option to provide hardware offload for that traffic. It can support two million queue pairs across multiple hosts, and can expose 1K virtual functions per host. The latter should let a cloud provider directly expose RDMA acceleration capabilities to VMs. To ensure reliable transport, the RDMA transport engine supports the Falcon and Swift transport protocols. Both protocols offer improvements over TCP, and Intel implements congestion control for those protocols completely in hardware. To reduce latency, the RDMA block can bypass the packet processing pipeline and handle RDMA connections on its own.All of the accelerator blocks above are clients of the system level cache. Some hardware acceleration use cases, like connection tracking with millions of flows, can have significant memory footprints. The system level cache should let the IPU keep frequently accessed portions of accelerator memory structures on-chip, reducing DRAM bandwidth needs.Mount Morgan’s PCIe capabilities have grown far beyond what a normal network card may offer. It has 32 PCIe Gen 5 lanes, providing more IO bandwidth than some recent desktop CPUs. It’s also a huge upgrade over the 16 PCIe Gen 4 lanes in Mount Evans.Traditionally, a network card sits downstream of a host, and thus appears as a device attached to a server. The host fabric and PCIe subsystem is flexible to let the IPU wear many hats. It can appear as a downstream device to up to four server hosts, each of which sees the IPU as a separate, independent device. Mount Evans supported this “multi-host” mode as well, but Mount Morgan’s higher PCIe bandwidth is necessary to utilize its 400 Gigabit networking.Mount Morgan can run in a “headless” mode, where it acts as a standalone server and a lightweight alternative to dedicating a traditional server to infrastructure tasks. In this mode, Mount Morgan’s 32 PCIe lanes can let it connect to many SSDs and other devices. The IPU’s accelerators as well as the PCIe lanes appear downstream of the IPU’s CPU cores, which act as a host CPU.A “converged” mode can use some PCIe lanes to connect to upstream server hosts, while other lanes connect to downstream devices. In this mode, the IPU shows up as a PCIe switch to connected hosts, with downstream devices visible behind it. A server could connect to SSDs and GPUs through the IPU. The IPU’s CPU cores can sit on top of the PCIe switch and access downstream devices, or can be exposed as a downstream device behind the PCIe switch.The IPU’s multiple modes are a showcase of IO flexibility. It’s a bit like how AMD uses the same die as an IO die within the CPU and a part of the motherboard chipset on AM4 platforms. The IO die’s PCIe lanes can connect to downstream devices when it’s serving within the CPU, or be split between an upstream host and downstream devices when used in the chipset. Intel is also no stranger to PCIe configurability. Their early QAT PCIe cards reused their Lewisburg chipset, exposing it as a downstream device with three QAT devices appearing behind a PCIe switch.Cloud computing plays a huge role in the tech world today. It originally started with commodity hardware, with similar server configurations to what customers might deploy in on-premise environments. But as cloud computing expanded, cloud providers started to see use cases for cloud-specific hardware accelerators. Examples include "Nitro" cards in Amazon Web Services, or smart NICs with FPGAs in Microsoft Azure. Intel has no doubt seen this trend, and IPUs are the company's answer.Mount Morgan tries to service all kinds of cloud acceleration needs by packing an incredible number of highly configurable accelerators, in recognition of cloud providers’ diverse and changing needs. Hardware acceleration always runs the danger of becoming obsolete as protocols change. Intel tries to avoid this by having very generalized accelerators, like the FXP, as well as packing in CPU cores that can run just about anything under the sun. The latter feels like overkill for infrastructure tasks, and could let the IPU remain relevant even if some acceleration capabilities become obsolete.At a higher level, IPUs like Mount Morgan show that Intel still has ambitions to stretch beyond its core CPU market. Developing Mount Morgan must have been a complex endeavor. It’s a showcase of Intel’s engineering capability even when their CPU side goes through a bit of a rough spot. It’ll be interesting to see whether Intel’s IPUs can gain ground in the cloud market, especially with providers that have already developed in-house hardware offload capabilities tailored to their requirements.If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KDE launches its own distribution]]></title>
            <link>https://lwn.net/SubscriberLink/1037166/caa6979c16a99c9e/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45204393</guid>
            <description><![CDATA[At Akademy 2025, the KDE Project released an alpha version of KDE Linux, a distribution built b [...]]]></description>
            <content:encoded><![CDATA[



Welcome to LWN.net

The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider accepting the discount offer on the right.  Thank you
for visiting LWN.net!


Special discount offer
           
           Subscribe to LWN now at the
           "professional hacker" level for at least six months,
           and you will
           receive a special discount of 25%.
           







At Akademy 2025, the
KDE Project released an
alpha version of KDE Linux, a
distribution built by the project to "include the best
implementation of everything KDE has to offer, using the most advanced
technologies". It is aimed at providing an operating system
suitable for home use, business use, OEM installations, and more
"eventually". For now there are many rough edges and missing
features that users should be aware of before taking the plunge; but
it is an interesting look at the kind of complete Linux system that
KDE developers would like to see.

Development and goals

KDE contributor Nate Graham wrote an announcement
blog post on September 6 to accompany the release of KDE
Linux. Harald Sitter had introduced the project as "Project Banana"
during a talk (video, slides)
at Akademy in 2024, and has
been leading its development along with major contributions from Hadi Chokr, Lasath Fernando,
Justin Zobel, Graham, and others.

KDE Linux is an immutable distribution that uses Arch Linux
packages as its base, but Graham notes that it is "definitely not
an 'Arch-based distro!'" Pacman is not included, and Arch is used
only for the base operating system. Everything else, he said, is
either compiled from source using KDE Builder or installed
using Flatpak.

Some may wonder why another Linux distribution is needed; Graham
said that he has expressed that sentiment himself in the past
regarding other distributions, but he thinks that KDE Linux is justified:


KDE is a huge producer of software. It's awkward for us to not have
our own method of distributing it. Yes, KDE produces source code that
others distribute, but we self-distribute our apps on app stores like
Flathub and the Snap and Microsoft stores, so I think it's a natural
thing for us to have our own platform for doing that distribution too,
and that's an operating system. I think all the major producers of
free software desktop environments should have their own OS, and many
already do: Linux Mint and ElementaryOS spring to mind, and GNOME is working on one too.

Besides, this matter was settled 10 years ago with the creation of
KDE neon, our first bite at the "in-house OS" apple. The sky did not
fall; everything was beautiful and nothing hurt.


Speaking of neon, Graham points
out that it is "being held together by a heroic volunteer"
(singular) and that no decision has been made as of yet about its
future. Neon has "served admirably for a decade", he said, but
it "has somewhat reached its limit in terms of what we can do with
it" because of its Ubuntu base. According to the wiki
page, neon's Ubuntu LTS base is built on old technology and
requires "a lot of packaging busywork". It also becomes less
stable as time goes on, "because it needs to be tinkered with to
get Plasma to build on it, breaking the LTS promise".

Architecture and plans

KDE Linux, on the other hand, is designed to be a greenfield
project that allows KDE to make use of newer technologies and more
modern approaches to a Linux distribution unhampered by the needs of a
general-purpose distribution. If KDE Linux's technology choices are
not appealing, Graham says, "feel free to ignore KDE Linux and
continue using the operating system of your choice. There are plenty
of them!"

KDE Linux is Wayland-only; there is no X.org session and no plan
to add one. Users with some of the older NVIDIA cards will need to
manually
configure the system to work properly with KDE Linux. The
distribution also only supports UEFI systems, and there are no plans
to add support for BIOS-only systems.

The root filesystem (/) is a read/write Btrfs
volume, while /usr is a read-only Enhanced
Read-Only File System (EROFS) volume backed by a single file. The
system is updated atomically by swapping out the EROFS volume;
currently KDE Linux caches up to five of the files to allow users to
roll back to previous versions if the most recent updates are
broken.

The files have names like kde-linux_202509082242.erofs and
are stored in /system. The most recent releases are about
4.8GB in size. The project uses systemd-sysupdate
under the hood, which does not have
support for delta updates yet. Users should expect to set aside at least 30GB
just to cache the EROFS files for now.

Unlike Fedora's image-based Atomic Desktops,
KDE Linux does not supply a way for users to add packages to the base
system. So, for example, users have no way to add packages with
additional kernel modules. Users can add applications packaged as 
Flatpaks using KDE's Discover graphical software manager; the 
Snap format is also supported, but it is not integrated with
Discover—the snap command-line
utility can be used to do install Snaps for now. KDE Linux also includes Distrobox, which allows users to set
up a container with the distribution of their choice and install
software in the container that is integrated with the system. LWN touched on Distrobox in
our coverage of the Bluefin image-based operating system in December
2023.

Unfortunately, it looks
like users are not set up correctly for Podman, which Distrobox
needs, on KDE Linux; trying to set up a new container gives a
"potentially insufficient UIDs or GIDs available in user namespace"
error when trying to test Distrobox on the latest KDE Linux build. This
comment in the Podman repository on GitHub set me on the right
path to fix the problem. This kind of bug is to be expected in an
alpha release; no doubt it will be ironed out in the coming weeks or
months.

System updates are also performed using Discover: when a new system
image is available, it will show up in the Updates tab and can be
installed from there. (Or using "sudo updatectl update" from
the command line, for those who prefer doing it that way.) Likewise,
installed Flatpaks with updates will show up in the Updates tab. For
now, at least, users will have to manually manage any applications
installed in a Distrobox container.

The default software selection is a good start for a desktop
distribution; it includes the Gwenview image viewer, Okular document
viewer, Haruna media player, Kate text editor, and Konsole for
terminal emulation. Firefox is the only prominent non-KDE application
included with the default install. The base system currently includes
GNU Bash 5.3.3, curl 8.15, Linux 6.16.5, GCC 15.2.1, Perl 5.42, Python 3.13.7, Vim
9.1, and wget 1.25. It does not include some utilities users might
want or expect, such as GNU Screen, Emacs, tmux, pip, or alternate shells like Fish.







KDE Linux's base packages are not meant to be user-customizable,
but it should be possible to create custom images using systemd's mkosi tool, which is what is used
by the project itself. The mkosi.conf.d
directory in the KDE Linux repository contains the various
configuration files for managing the packages included in the system image.



Development and what's next

The plan, longer term, is to have three editions of KDE Linux: the
testing edition, which is what is available now, an enthusiast
edition, and a stable edition. The testing edition is meant for
developers and quality assurance folks; it is to be built daily from
Git and to be similar in quality to KDE neon's unstable release. The
enthusiast edition will include beta or released software, depending
on the status of a given application at the time; this edition is
aimed at "KDE enthusiasts, power users, and influencers". The
stable edition, as the name suggests, will include only released
software that meets quality metrics (which are not yet defined),
indicating it's ready for users not in the other categories.

KDE Linux can be installed
on bare metal or in a virtual
machine using virt-manager. Support
for UEFI Secure Boot is currently missing. Since KDE Linux uses a lot
of space for cached images, users should provision more disk space for
a virtual machine than they might ordinarily; I allocated 50GB, but
probably should have gone with 75GB or more.

Those wishing to follow along with KDE Linux development can check
out the milestone trackers for the enthusiast
and stable
editions. All of the milestones
have been reached for the testing edition. There are quite a few items
to complete before KDE Linux reaches beta status; for example, the
project is currently using packages from the Arch User Repository (AUR) but
the plan is to move
away from using AUR soon. The project also needs to move
production to official KDE infrastructure rather than depending on
Sitter's personal systems.

At the moment, the project does not have a security announcement mailing
list or other notification mechanism; those using KDE Linux for more
than testing should keep an eye on Arch's security tracker and
KDE security advisories.
Since KDE Linux is an immutable derivative of Arch Linux, with no
way to immediately pull updated Arch packages, users should remember
that they will be at a disadvantage when there are security
vulnerabilities in the base operating system. Any security update would
need to be created by Arch Linux, pushed out as an Arch package, and
then incorporated into a build for KDE Linux. Conservatively, that
will add at least a day for any security updates to reach KDE Linux
users.

One of the downsides of having no package manager is that there is
no easy way to take stock of what is installed on the system. Normally,
one might do an inventory of software using a package manager's query
tools; a quick "rpm -qa" shows all of the system software on
my desktop's Fedora 42 install. There is no such mechanism for
KDE Linux, and it's not clear that there are any plans for that type
of feature long term. To be suitable for some of the target audiences,
KDE Linux will need (for example) ways to manage the base operating
system and easily query what is installed.

The project's governance is described
as a "'Council of elders' model with major contributors being
the elders". Sitter has final decision-making authority in cases
of disagreement.

Obviously the team working on KDE Linux wants the project to
succeed, but it has put some thought into what will happen if the
distribution is put out to pasture at some point. There is an end-of-life
contingency plan to "push a final update shipping an OS image
that transforms the system into a completely different
distro". The successor distribution has not been chosen yet; it 
would be picked based on the KDE Linux team's relationship with the other
distribution and its ability to take on all of the new users.

Part of the rationale for KDE Linux is to satisfy an impulse that
is common to many open-source developers: the desire to ship software
directly to users without an intermediary tampering with it.
The process of creating and refining KDE Linux will satisfy
that for KDE developers, but it may also serve another purpose: to
demonstrate just how difficult it is to create and maintain a
desktop distribution for widespread use. Whether KDE Linux succeeds as a
standalone distribution or not, it may be a useful exercise to
illustrate why projects like Debian, Fedora, openSUSE, Ubuntu,
and others make choices that ultimately frustrate application
developers.


               
               
            ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Defeating Nondeterminism in LLM Inference]]></title>
            <link>https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200925</guid>
            <description><![CDATA[Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).]]></description>
            <content:encoded><![CDATA[
    
    
    
    Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.
For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.
What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).
But why aren’t LLM inference engines deterministic? One common hypothesis is that some combination of floating-point non-associativity and concurrent execution leads to nondeterminism based on which concurrent core finishes first. We will call this the “concurrency + floating point” hypothesis for LLM inference nondeterminism. For example, a recent arXiv preprint writes:

Floating-point arithmetic in GPUs exhibits non-associativity, meaning $(a + b) + c \neq a + (b + c)$ due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.


You can also find the “concurrency + floating point” hypothesis repeated by others, like here (“There are speed tradeoffs, and in order to make the endpoints fast GPUs are used, which do parallel [nondeterministic] calculations. Any modern GPU neural net calculations will be subject to these."), or here (“Because GPUs are highly parallelized, the ordering of additions or multiplications might be different on each execution, which can cascade into small differences in output.").
While this hypothesis is not entirely wrong, it doesn’t reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. We’re definitely using floating-point numbers. And our GPU definitely has a lot of concurrency. Why don’t we see nondeterminism in this test?
A = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
B = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
ref = torch.mm(A, B)
for _ in range(1000):
    assert (torch.mm(A, B) - ref).abs().max().item() == 0
To understand the true cause of LLM inference nondeterminism, we must look deeper.
Unfortunately, even defining what it means for LLM inference to be deterministic is difficult. Perhaps confusingly, the following statements are all simultaneously true:

Some kernels on GPUs are nondeterministic.
However, all the kernels used in a language model’s forward pass are deterministic.
Moreover, the forward pass of an LLM inference server (like vLLM) can also be claimed to be deterministic.
Nevertheless, from the perspective of anybody using the inference server, the results are nondeterministic.

In this post, we will explain why the “concurrency + floating point” hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.
The original sin: floating-point non-associativity
Before talking about nondeterminism, it’s useful to explain why there are numerical differences at all. After all, we typically think of machine learning models as mathematical functions following structural rules such as commutativity or associativity. Shouldn’t there be a “mathematically correct” result that our machine learning libraries should provide us?
The culprit is floating-point non-associativity. That is, with floating-point numbers:
$$ (a + b) + c \neq a + (b + c) $$(0.1 + 1e20) - 1e20
>>> 0
0.1 + (1e20 - 1e20)
>>> 0.1
Ironically, breaking associativity is what makes floating-point numbers useful.
Floating-point numbers are useful because they allow for a “dynamic” level of precision. For the purposes of explanation, we will use base 10 (instead of binary), where floating-point numbers are in the format $\text{mantissa} * 10^\text{exponent}$. We will also use 3 digits for the mantissa and 1 digit for the exponent.
For example, for the value 3450, we can represent it exactly as $3.45 * 10^3$. We can also represent much smaller values like 0.486 as $4.86 * 10^{-1}$. In this way, floating point allows us to represent both very small as well as very large values. In the sciences, we might say that floating point allows us to maintain a constant number of “significant figures”.
If you add together two floating-point numbers with the same exponent, it looks similar to integer addition. For example, 123 ($1.23 * 10^2$) + 456 ($4.56 * 10^2$) results in 579 ($5.79 * 10^2$).
But what happens when we add two floating-point numbers with different exponents, such as 1230 and 23.4?  In this case, the exact result is 1253.4. However, we can only maintain 3 digits of precision at a time. Floating-point addition will thus drop the last 2 digits and obtain the value $1.25 * 10^3$ (or 1250).


        
            1.23 × 10²
        
        +
        
            3.45 × 10¹
        
        =
        
            1.575 × 10²
            Exact: 1575
        
    



We require 3 digits of precision to represent 1230 and 3 digits of precision to represent 23.4. However, adding these 2 numbers together results in a number that requires 5 digits of precision to represent (1253.4). Our floating-point format must then drop the 34 off the end. In some sense, we have effectively rounded our original 23.4 to 20.0 before adding it. 

At this point, however, we’ve destroyed information. Note that this can happen every time we add two floating-point numbers with different “scales” (i.e. different exponents). And adding together floating-point numbers with different exponents happens all of the time. In fact, if we could guarantee that we never needed different exponents, we could just use integers!
In other words, every time we add together floating-point numbers in a different order, we can get a completely different result. To take an extreme example, there are 102 possible different results for summing this array depending on the order.
import random

vals = [1e-10, 1e-5, 1e-2, 1]
vals = vals + [-v for v in vals]

results = []
random.seed(42)
for _ in range(10000):
    random.shuffle(vals)
    results.append(sum(vals))

results = sorted(set(results))
print(f"There are {len(results)} unique results: {results}")

# Output:
# There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]
Although this is the underlying cause for non-identical outputs, it does not directly answer where the nondeterminism comes from. It doesn’t help us understand why floating-point values get added in different orders, when that happens, nor how it can be avoided.
The answers lie in how kernels are implemented.
Why don’t kernels always add numbers in the same order?
As mentioned above, one common explanation for why kernels add numbers in different orders is the “concurrency + floating point” hypothesis. The hypothesis states that if the order in which concurrent threads finish is nondeterministic and the accumulation order depends on the order in which concurrent threads finish (such as with an atomic add), our accumulation order will be nondeterministic as well.
Confusingly, although this can lead to nondeterministic kernels, concurrency (and atomic adds) end up being completely uninvolved in LLM inference nondeterminism! To explain what the real culprit is, let’s first understand why modern GPU kernels rarely need atomic adds.
When are atomic adds needed?
Typically a GPU launches a program concurrently across many “cores” (i.e. SMs). As the cores have no inherent synchronization among them, this poses a challenge if the cores need to communicate among each other. For example, if all cores must accumulate to the same element, you can use an “atomic add” (sometimes known as a “fetch-and-add”). The atomic add is “nondeterministic” — the order in which the results accumulate is purely dependent on which core finishes first.
Concretely, imagine that you are reducing a 100-element vector with 100 cores (e.g. torch.sum()). Although you can load all 100 elements in parallel, we must eventually reduce down to a single element. One way to accomplish this is with some kind of “atomic add” primitive, where the hardware guarantees that all additions will be processed but does not guarantee the order.




 The atomic add ensures that every core's contributions will be reflected in the final sum. However, it makes no guarantee about what order the contributions will be added. The order depends entirely on which core finishes first, a nondeterministic property. Thus, executing the same parallel program multiple times can result in nondeterministic outputs. 

This is usually what folks mean by “nondeterminism” — you execute the same kernel twice with exactly the same inputs and you get a different result out. This is known as run-to-run nondeterminism, where you run the same python script twice with the exact same dependencies but get a different result.
Although concurrent atomic adds do make a kernel nondeterministic, atomic adds are not necessary for the vast majority of kernels. In fact, in the typical forward pass of an LLM, there is usually not a single atomic add present.
This may be surprising, given that parallelizing a reduction can benefit from atomic adds. There are two main reasons why atomic adds do not end up being needed.

There is often sufficient parallelism along the “batch” dimension that we don’t need to parallelize along the reduction dimension. For example, let’s say that instead of reducing a single 100-dim vector we were reducing 500 vectors in parallel. In this case, we can reduce an entire vector in each core and allow every core to operate on a different vector.
Over time, most neural network libraries have adopted a variety of strategies for achieving determinism without sacrificing performance. For example, we can perform a “split” (or tree) reduction, where we split the 100-element reduction into five 20-element reductions (thus achieving five-way parallelism). Then, to combine the remaining five elements, we can either perform a separate “clean-up” reduction (which isn’t parallelized, but operates over few enough elements to be cheap) or utilize a semaphore (which ensures that each concurrent thread-block will accumulate in a deterministic order).The semaphore strategy can be found described here.

Due to these two factors, avoiding atomics adds is a negligible performance penalty for the vast majority of neural network operations.
There are still a couple of common operations that have significant performance penalties for avoiding atomics. For example, scatter_add in PyTorch (a[b] += c). The only one commonly used in LLMs, however, is FlashAttention backward.Fun fact: did you know that the widely used Triton implementations of FlashAttention backward actually differ algorithmically from Tri Dao’s FlashAttention-2 paper? The standard Triton implementation does additional recomputation in the backward pass, avoiding atomics but costing 40% more FLOPs!
However, the forward pass of an LLM involves no operations that require atomic adds. Thus, the forward pass in an LLM is in fact “run-to-run deterministic.”











Model

Deterministic



User requests




Other user requests




Output




















































From the perspective of the inference server, it is deterministic. Given the exact same user requests, it will always provide the same deterministic output. 

Wikipedia writes that “a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.” And in this case, given the exact same inputs (i.e. the exact requests the inference server is processing), the forward pass always produces the exact same outputs.
However, the forward pass itself being “deterministic” is not sufficient to ensure that a system that includes it is deterministic. For example, what if our request’s output depended on the parallel user requests (e.g. batch-norm)? Since each individual request has no way of knowing what the parallel requests will be, from their perspective our overall LLM inference is also nondeterministic!
As it turns out, our request’s output does depend on the parallel user requests. Not because we’re somehow leaking information across batches — instead, it’s because our forward pass lacks “batch invariance”, causing our request’s output to depend on the batch size of our forward pass.
Batch invariance and “determinism”
To explain batch invariance, let’s simplify the system and look solely at matmuls. You can assume that all matmul implementations are “run-to-run deterministic."This is not totally true, but most common matmul implementations do have this property.  However, they are not “batch-invariant.” In other words, when the batch size changes, each element in the batch can get different results.
This is a fairly unusual property from a mathematical perspective. Matrix multiplication should be “independent” along every element in the batch — neither the other elements in the batch nor how large the batch is should affect the computation results of a specific element in the batch.
However, as we can observe empirically, this isn’t true.
import torch
torch.set_default_device('cuda') 

B = 2048
D = 4096
a = torch.linspace(-1000, 1000, B*D).reshape(B, D)
b = torch.linspace(-1000, 1000, D*D).reshape(D, D)
# Doing a matrix vector multiplication by taking
# the first element of the batch
out1 = torch.mm(a[:1], b)
# Doing a matrix matrix multiplication and then taking
# the first element of the batch
out2 = torch.mm(a, b)[:1]
print((out1 - out2).abs().max()) # tensor(1669.2500, device='cuda:0')
Note that this is “run-to-run deterministic.” If you run the script multiple times, it will deterministically return the same result.It is not “hardware/software version invariant” — your GPU/PyTorch version may return a different value, but it should deterministically return the same value.
However, when a non-batch-invariant kernel is used as part of a larger inference system, the system can become nondeterministic. When you make a query to an inference endpoint, the amount of load the server is under is effectively “nondeterministic” from the user’s perspective. The load determines the batch size that the kernels are run under, and thus changes the eventual result of each individual request!


















Model

Deterministic
Nondeterministic



User requests




Other user requests




Output


































































Although the inference server itself can be claimed to be "deterministic", the story is different for an individual user. From the perspective of an individual user, the other concurrent users are not an "input" to the system but rather a nondeterministic property of the system. This makes LLM inference "nondeterministic" from the perspective of each user.

If you compose some property under which the kernel is not invariant (i.e. batch-size) with nondeterminism of that property (i.e. the load the server is under), you get a nondeterministic system.
In other words, the primary reason nearly all LLM inference endpoints are nondeterministic is that the load (and thus batch-size) nondeterministically varies! This nondeterminism is not unique to GPUs — LLM inference endpoints served from CPUs or TPUs will also have this source of nondeterminism.
So, if we’d like to avoid nondeterminism in our inference servers, we must achieve batch invariance in our kernels. In order to understand how that can be achieved, let’s first take a look at why kernels don’t have batch invariance in the first place.
How do we make kernels batch-invariant?
In order to make a transformer implementation batch-invariant, we must make every kernel batch-invariant. Luckily, we can assume that every pointwise operation is batch-invariant.Although this is true for all kernels in say, PyTorch, it’s not inherently true. For example, there are some kernel implementations on CPU that will use vectorized intrinsics on some parts of the array and non-vectorized intrinsics on other parts, and these intrinsics don’t necessarily always have bitwise identical numerics. Thus, we only need to worry about the 3 operations that involve reductions — RMSNorm, matrix multiplication, and attention.Reductions related to parallelism are out of the scope of this discussion, but the same principles apply. One factoid that may be useful is that NVLink-Sharp in-switch reductions are deterministic on Blackwell as well as Hopper with CUDA 12.8+. As is the case with many things, this information can be found on NCCL’s github issues
Conveniently, these are also ordered in ascending levels of difficulty. Each one requires some additional considerations to achieve batch invariance with reasonable performance. Let’s talk about RMSNorm first.
Batch-invariant RMSNorm











































Data Parallel RMSNorm Ideally, we'd like to avoid communication between cores in our parallelization strategy. One way to achieve that is by assigning one batch-element to each core, thus guaranteeing that each reduction is done entirely within a single core. This is what's known as a "data-parallel" strategy, since we're simply parallelizing along a dimension that doesn't require communication. In this example, we have four rows and four cores, saturating our cores. 

RMSNorm can be implemented as:
# x: [batch_size, hidden_dim]
# weight: [hidden_dim]
def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
The requirement for batch invariance is that the reduction order for each element must be fixed regardless of the batch-size of the kernel. Note that this doesn’t mean we must always use the same reduction strategy. For example, if we change the number of elements we’re reducing over, we can still be batch-invariant even if our reduction strategy changes.The Quack blog post has some nice examples showing the hierarchy of various reduction strategies you can do (e.g. thread reduction, warp reduction, block reduction, cluster reduction).
Thus, we only break batch invariance when our batch-size affects the reduction strategy.
Let’s look at the standard parallelism strategy for RMSNorm. Generally, parallel algorithms benefit from minimizing communication across cores. For the purpose of this discussion you can assume that when we refer to “cores” we mean SMs. More specifically, the property here that’s important is that the # of threadblocks our kernel launches is greater than the # of SMs. So, one strategy we can start with is to assign each batch element to one core, as seen in the above figure.
Increasing our batch size doesn’t affect our reduction strategy; if a batch size of 200 provides sufficient parallelism to our kernel then a batch size of 2000 will definitely provide sufficient parallelism.



































































Data Parallel RMSNorm for larger batches Extending the data-parallel strategy to larger batches is fairly straightforward --- instead of having each core handle one row you allow each core to handle different rows sequentially. This preserves batch invariance as the reduction strategy for each batch element remains identical. 

On the other hand, decreasing the batch size can pose challenges. Because we assign each batch element to one core, decreasing our batch size will eventually lead to having more cores than batch elements, leaving some cores idle.
Upon encountering this situation, a good kernel engineer would reach for one of the solutions mentioned in the prior section (atomic adds or split reductions), maintaining good parallelism and thus, good performance. Unfortunately, this changes the reduction strategy, preventing this kernel from being batch-invariant.































Split-Reduction RMSNorm If we have a small batch size, our data-parallel strategy may no longer have sufficient parallelism to saturate our cores. In this case, it may be more efficient to "split" a reduction among multiple cores, allowing us to fully utilize our GPU. However, this loses batch invariance, as we are no longer reducing each element in the same order.

The easiest solution is to simply ignore these cases altogether. This is not completely unreasonable — a small batch size means that the kernel is likely to execute quickly anyways, and so a slowdown may not be catastrophic.
If we were compelled to optimize this use case, one approach would be to consistently use a reduction strategy that has enough parallelism even for very small batch sizes. Such a reduction strategy would lead to an excess amount of parallelism for larger batch sizes but would allow us to achieve decent (but not peak) performance across the entire range of sizes.
Batch-invariant matrix multiplication














































































Data Parallel Matmul Similar to RMSNorm, the standard parallelism strategy for matmuls is a "data-parallel" strategy, keeping the entire reduction in one core. It is most straightforward to think about splitting the output tensor into 2D tiles and assigning each tile to a different core. Each core then computes the dot products that belong to that tile, once again performing the entire reduction within one core.
Unlike for RMSNorm, additional constraints around arithmetic intensity and utilizing tensorcores force us to split 2D tiles instead of individual output elements for efficient matmul kernels.


At its core, you can view matrix multiplication as simply a pointwise operation followed by a reduction. Then, if we parallelize our matrix multiplication by chunking the output into tiles, we have an analogous “data-parallel” kernel strategy that keeps each reduction within one core.
Also similar to RMSNorm, it is possible for our “batch” dimensions (M and N) to become too small, forcing us to split along the reduction dimension (K). Despite having two “batch” dimensions, matmuls also require us to have much more “work” per core in order to leverage tensorcores effectively. For example, if you have a [1024, K] x [K, 1024] matmul and a standard 2D tile size of [128, 128], a data-parallel strategy would only be able to split this matmul into 64 cores, insufficient to saturate the GPU.
Splitting along the reduction dimension in a matmul is known as a Split-K Matmul. And just like RMSNorm, using this strategy breaks batch invariance.

Another interesting parallelism strategy for matmuls is stream-k. Stream-k is interesting because it has even less invariance than typical matmuls. As discussed, most matmul libraries are not batch-invariant, but they’re at least what you could call batch-position-invariant (i.e. changing the position of the element within the batch does not affect numerics). However, stream-k is not batch-position-invariant either! Its core insight is that you can get cleaner load-balancing by splitting along k in different ways for different output tiles, but taking advantage of this makes our kernel not batch-position-invariant either. 






















































Split-K Matmul If our batch dimension is fairly small we may not have enough parallelism and require a split-k matmul. In this example, we split each reduction across two cores, which would accumulate separately and then combine their results at the end. However, splitting each reduction across two cores allows us to still leverage eight cores.



There’s an additional complexity with matmuls — tensor core instructions. Whereas with reductions we could simply operate on one row at a time, efficient matrix multiplication kernels must operate on an entire “tile” at a time.
Each tensor-core instruction (like say, wgmma.mma_async.sync.aligned.m64n128k16) may have a different reduction order internally. One reason to use a different tensor-core instruction might be that the batch size is very small. For example, if we use a tensor-core PTX instruction that operates on a tile of length 256 but the batch size is only 32, we’re wasting almost all of that compute! At a batch-size of 1, the fastest kernels usually don’t use tensor cores at all.







































































Padded Tensor-Core Instructions If the batch size is too small, we may be in our situation where we can't fit even one of our 2D tiles in the output. In this case, it is most efficient to switch to a smaller tensor-core instruction or eschew tensor-cores altogether! However, both of these options prevent our kernel from being batch-invariant.

So, the easiest way to ensure batch invariance for matmuls is to compile one kernel configuration and use that for all shapes. Although we will lose some performance, this isn’t typically disastrous in LLM inference. In particular, split-k is most needed when both M and N are small, and luckily in our case, N (i.e. the model dim) is usually pretty large!

  Despite obtaining batch invariance, we only lose about 20% performance compared to cuBLAS. Note that this is not an optimized Triton kernel either (e.g. no TMA). However, some of the patterns in performance are illustrative of where our batch-invariant requirement loses performance. First, note that we lose a significant amount of performance at very small batch sizes due to an overly large instruction and insufficient parallelism. Second, there is a "jigsaw" pattern as we increase the batch-size that is caused by quantization effects (both tile and wave) that are typically ameliorated through changing tile sizes. You can find more on these quantization effects here.
  
  

Batch-invariant attention





















































FlashAttention2 Strategy We parallelize along Q, and reduce along K/V simultaneously. This means that our entire reduction can be kept within a single core, making it another data-parallel strategy.

After obtaining batch invariance for matmuls, attention introduces two additional wrinkles — fittingly,  because it contains two matmuls.

As opposed to only reducing over the feature dimension like both RMSNorm and matmuls, we now reduce over the feature dimension and sequence dimension.
Due to the above, attention must deal with a variety of inference optimizations that affect how sequences get processed (chunked prefill, prefix caching, etc.).

Thus, to achieve determinism in LLM inference our numerics must be invariant to both how many requests are processed at once and how each request gets sliced up in the inference engine.
Let’s first walk through the standard parallelism strategy for attention, first introduced in FlashAttention2. Similar to RMSNorm and Matmul, the default strategy is a “data-parallel” strategy. Since we reduce along the key/value tensors, a data-parallel strategy can only parallelize along the query tensor.
For example, depending on the inference engine’s choices, it’s possible that a sequence might get processed in several parts (such as in chunked prefill) or perhaps all at once (if the prefill isn’t split up). In order to achieve “batch invariance”, it’s necessary that the reduction order for a given token does not depend on how many other tokens from its sequence are being simultaneously processed. If you reduce over the K/V values in the KV cache separately from the K/V values in the current tokens being processed (like in vLLM’s Triton attention kernel), this can’t be achieved. For example, when processing the 1000th query token in a sequence, the reduction order must be identical regardless of whether 0 tokens are in the KV cache (prefill) or 999 tokens are in the KV cache (decoding).






























































FlashAttention with a KV Cache The reason why explicitly handling the KV cache separately from the current KV values breaks batch invariance is a bit subtle and is related to "boundary conditions". In particular, imagine your block size is 32 but we currently have 80 elements in our KV cache. We then compute an additional 48 elements that aren't cached. In this case, we need three blocks (two full and one masked) to compute "P cache" and another two blocks (one full and one masked) to compute "P". This is therefore five total blocks to compute our reduction when we only have four total blocks (i.e. 128) of elements to compute, which will definitely change our reduction order. 
For example, if we instead had no elements in our KV Cache and were processing 128 elements altogether, we need to have identical numerics in both of these situations to ensure “batch invariance” for attention.
 

To resolve this, we can just update the KV cache and page table before the attention kernel itself, ensuring that our keys and values are always consistently laid out regardless of how many tokens are being processed.
With this additional detail (as well as all the things mentioned in the previous section, like consistent tile sizes), we are able to achieve a batch-invariant attention implementation!
However, there is a significant problem here. Unlike with matrix multiplication, the attention shapes we see in LLM inference often do require a split-reduction kernel, often known as Split-KV or FlashDecoding. This is because if we don’t parallelize along the reduction, we can only parallelize along the batch dimension, head dimension, and “query length” dimension. In the decode stage of attention, query length is very small, and so unless we have a very large batch size we are often unable to saturate the GPU.
Unfortunately, it’s not as easy to ignore this case as it was for RMSNorm and Matmuls. For example, if you have a very long KV cache, the attention kernel may take a very long time despite only processing one request.



































Fixed # Split-KV Strategy (i.e. FlashDecode) If our query length becomes very small (like it does during decoding), we may end up in a situation where there is very little parallelism in our kernel at all. In these cases, we'll need to once again split along the reduction dimension --- the KV dimension this time. The typical strategy for how to split along the KV dimension is to figure out how much parallelism we need and then divide the KV dimension evenly. For example, if our KV length was 1000 and we needed 4 splits, each core would handle 250 elements.
This unfortunately also breaks batch invariance, as our precise reduction strategy depends on how many query tokens from the sequence we’re processing in any given request.


Furthermore, the split-reduction strategies commonly used for attention also pose challenges for batch invariance. For example, FlashInfer’s “balanced scheduling algorithm” chooses the largest split-size that can still saturate all the GPU’s cores, thus making the reduction strategy not “batch-invariant”. However, unlike with RMSNorm/Matmuls, it’s not sufficient to choose a fixed number of splits regardless of the batch size.
Instead, to achieve batch invariance, we must adopt a “fixed split-size” strategy. In other words, instead of fixing the # of splits, we fix the size of each split and then end up with a varying number of splits. In this manner, we can guarantee that regardless of how many tokens we’re processing, we always perform the identical reduction order. This requires some internal FlexAttention changes that are not included in our code release. We will upstream them in the near future!




































Fixed Size Split-KV Strategy 
The only difference between this strategy and the previous strategy is that our splits are now "fixed size". For example, if our KV length was 1000, instead of splitting it into four even length 250 splits, we would split it into three fixed-size length 256 splits and one length 232 split.
This allows us to preserve batch invariance as our reduction strategy is no longer dependent on how many query tokens we’re processing at once!
 

Implementation
We provide a demonstration of deterministic inference on top of vLLM by leveraging its FlexAttention backend as well as torch.Library.
Through torch.Library, we’re able to substitute out most of the relevant PyTorch operators in an unintrusive way. You can find the library of “batch-invariant” kernels at thinking-machines-lab/batch-invariant-ops, as well as the vLLM example of running in “deterministic” mode.
Experiments
How nondeterministic are completions?
We use Qwen/Qwen3-235B-A22B-Instruct-2507 and sample 1000 completions at temperature 0 with the prompt “Tell me about Richard Feynman” (non-thinking mode), generating 1000 tokens each. Surprisingly, we generate 80 unique completions, with the most common of these occuring 78 times.
Looking at where the completions differ, we see that the completions are actually identical for the first 102 tokens! The first instance of diverging completions occurs at the 103rd token. All completions generate the sequence “Feynman was born on May 11, 1918, in” However, 992 of the completions go on to generate “Queens, New York” whereas 8 of the completions generate “New York City”.

On the other hand, when we enable our batch-invariant kernels, all of our 1000 completions are identical. This is what we would mathematically expect from our sampler, but we aren’t able to achieve deterministic results without our batch-invariant kernels.

Performance
We have not put a significant effort into optimizing the performance of the batch-invariant kernels here. However, let’s run some experiments to verify that our performance remains usable.
We will set up an API server with one GPU running Qwen-3-8B, and request 1000 sequences with an output length of between 90 and 110.

  
      
          Configuration
          Time (seconds)
      
  
  
      
          vLLM default
          26
      
      
          Unoptimized Deterministic vLLM
          55
      
      
          + Improved Attention Kernel
          42
      
  

Much of the slowdown comes from the fact that the FlexAttention integration in vLLM has not been heavily optimized yet. Nevertheless, we see that performance is not disastrous.
True on-policy RL
As researchers have noted, the different numerics between training and inference implicitly turns our on-policy RL into off-policy RL.
Of course, it is impossible to get bitwise identical results between training and inference if we can’t even get bitwise identical results from two identical inference requests. Then, deterministic inference enables us to also modify our training stack to obtain bitwise identical results between sampling and training, thus resulting in true on-policy RL.
We run experiments in a RLVR setup on Bigmath with the RL policy initialized from the Qwen 2.5-VL instruct 8B with a max rollout length of 4096.
If we train without off-policy correction (i.e. importance weighting), our reward collapses partway through training, whereas adding an off-policy correction term  allows training to proceed smoothly. But, if we achieve bitwise identical results between our sampler and trainer, we are fully on policy (i.e. 0 KL divergence) and can also train smoothly.
We can also plot the KL-divergence in logprobs between our sampler and trainer, where all 3 runs have notably different behavior. When running with importance weighting, it stays around 0.001 with occasional spikes. However, running without importance weighting eventually leads to a spike in KL-divergence around the same time that reward crashes. And, of course, when running “True On-Policy RL”, our KL-divergence stays flat at 0, indicating that there is no divergence between the training policy and sampling policy.



  Note that the run without importance weighting has a significant loss spike around Step 318, and this comes with a correspond ing spike in KL-divergence of logprobs. Meanwhile, either using an off-policy correction or running with "True On-Policy" allows RL to continue smoothly. The blue line showing "True On-Policy" is not a bug - it's just a flat line at 0. 
  
  

Conclusion
Modern software systems contain many layers of abstractions. In machine learning, when we run into nondeterminism and subtle numerical differences it can often be tempting to paper over them. After all, our systems are already “probabilistic”, so what’s wrong with a little more nondeterminism? What’s wrong with bumping up the atol/rtol on the failing unit test? The difference in logprobs between the trainer and the sampler probably isn’t a real bug, right?
We reject this defeatism. With a little bit of work, we can understand the root causes of our nondeterminism and even solve them! We hope that this blog post provides the community with a solid understanding of how to resolve nondeterminism in our inference systems and inspires others to obtain a full understanding of their systems.
Citation
Please cite this work as:
He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference", 
Thinking Machines Lab: Connectionism, Sep 2025.
Or use the BibTeX citation:
@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}

    
  ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[“No Tax on Tips” Includes Digital Creators, Too]]></title>
            <link>https://www.hollywoodreporter.com/business/business-news/no-tax-on-tips-guidance-creators-trump-treasury-1236366513/</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45200024</guid>
            <description><![CDATA[A U.S. Treasury Department document includes podcasters, social media influencers and streamers among occupations eligible for tax-free tips.]]></description>
            <content:encoded><![CDATA[
	President Trump’s One Big Beautiful Bill Act may have quietly changed the economics of the creator economy.




	The U.S. Treasury Department this past week released a list of occupations “that customarily and regularly received tips” and thus will be eligible for the administration’s flagship “no tax on tips” policy, which will let eligible taxpayers deduct their tipped income, within certain limits.






	And while the list includes the obvious (bartenders, food servers, casino dealers and housekeepers are all there) it also includes some surprising jobs that could alter the economics of the creator economy.

	







	That’s because the Treasury Department has determined that “digital content creators” are eligible, including podcasters, social media influencers and streamers.




	Comedians, singers, musicians, DJs and magicians are also included, though that is more relevant to the wedding performer crowd than Grammy-winners.




	The change could cause digital creators to rethink how they seek income. Platforms like TikTok, YouTube, Twitch and Snapchat all offer a variety of ways for creators to generate income, be it a share of advertising revenue or creator funding programs, or options to launch subscription tiers for their channels or profiles. But they also give creators the option to turn on tips or gifts. If revenue from user tips or gifts is eligible, while recurring subscription revenue is not, it could shift how streamers, podcasters or influencers ask their followers to support them.




	To be sure, there are limitations: The tax deduction is capped at $25,000 per year, and it begins to phase out at $150,000 in income for single filers and $300,000 for married joint filers. The act also provides that tips do not qualify for the deduction if they are received “in the course of certain specified trades or businesses — including the fields of health, performing arts, and athletics,” Treasury says, further limiting the deduction opportunity for some in entertainment-adjacent lines of work.

	





	But by making influencers, Twitch streamers and podcasters eligible, the administration has nonetheless changed the incentive structure for digital creators, and the ramifications could be felt across the creator economy in the name of tax efficiency (Don’t be surprised if users are asked to like, subscribe, and tip).




	Platforms may also develop more ways to more prominently feature tips and gifts, pushing creators to add more opportunities for that income.




	But the inclusion of digital creators is also a recognition of how the power dynamics have shifted in media.




	Podcasters and creators, as everyone knows by now, have emerged as a driving force in today’s political climate, and the classification by Treasury could push more people to consider joining the fray or ramping up their content, as long as it is tipped, of course.














]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ChatGPT Developer Mode: Full MCP client access]]></title>
            <link>https://platform.openai.com/docs/guides/developer-mode</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199713</guid>
        </item>
        <item>
            <title><![CDATA[Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts]]></title>
            <link>https://news.ycombinator.com/item?id=45199648</link>
            <guid isPermaLink="false">https://news.ycombinator.com/item?id=45199648</guid>
            <description><![CDATA[Hey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today we’re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. It’s our biggest release in quite a while so we thought we’d finally do our Launch HN :)]]></description>
            <content:encoded><![CDATA[Launch HN: Recall.ai (YC W20) – API for meeting recordings and transcripts89 points by davidgu 19 hours ago  | hide | past | favorite | 45 commentsHey HN, we're David and Amanda from Recall.ai (https://www.recall.ai). Today we’re launching our Desktop Recording SDK, a way to get meeting data without a bot in the meeting: https://www.recall.ai/product/desktop-recording-sdk. It’s our biggest release in quite a while so we thought we’d finally do our Launch HN :)Here’s a demo that shows it producing a transcript from a meeting, followed by examples in code: https://www.youtube.com/watch?v=4croAGGiKTA . API docs are at https://docs.recall.ai/.Back in W20, our first product was an API that lets you send a bot participant into a meeting. This gives developers access to audio/video streams and other data in the meeting. Today, this API powers most of the meeting recording products on the market.Recently, meeting recording through a desktop form factor instead of a bot has become popular. Many products like Notion and ChatGPT have added desktop recording functionality, and LLMs have made it easier to work with unstructured transcripts. But it’s actually hard to reliably record meetings at scale with a desktop app, and most developers who want to add recording functionality don’t want to build all this infrastructure.Doing a basic recording with just the microphone and system audio is fairly straightforward since you can just use the system APIs. But it gets a lot harder when you want to capture speaker names, produce a video recording, get real-time data, or run this in production at large scale:- Capturing speaker names involves using accessibility APIs to screen-scrape the video conference window to monitor who is speaking at what time. When video conferencing platforms change their UI, we must ship a change immediately, so this keeps working.- Producing a video recording that is clean, and doesn’t capture the video conferencing platform UI involves detecting the participant tiles, cropping them out, and compositing them together into a clean video recording.- Because the desktop recording code runs on end-user machines, we need to make it as efficient as possible. This means writing highly platform-optimized code, taking advantage of hardware encoders when available, and spending a lot of time doing profiling and performance testing.Meeting recording has zero margin for failure because if anything breaks, you lose the data forever. Reliability is especially important, which dramatically increases the amount of engineering effort required.Our Desktop Recording SDK takes care of all this and lets developers build meeting recording features into their desktop apps, so they can record both video conferences and in-person meetings without a bot.We built Recall.ai because we experienced this problem ourselves. At our first startup, we built a tool for product managers that included a meeting recording feature. 70% of our engineering time was taken up by just this feature! We ended up starting Recall.ai to solve this instead. Since then, over 2000 companies use us to power their recording features, e.g. Hubspot for sales call recording, Clickup for their AI note taker. Our users are engineering teams building commercial products for financial services, telehealth, incident management, sales, interviewing, and more. We also power internal tooling for large enterprises.Running this sort of infrastructure has led to unexpected technical challenges! For example, we had to debug a 1 in 36 million segfault in our audio encoder (https://www.recall.ai/blog/debugging-a-1-in-36-000-000-segfa...), we encountered a Postgres lock-up that only occurs when you have tens of thousands of concurrent writers (https://news.ycombinator.com/item?id=44490510), and we saved over $1M a year on AWS by optimizing the way we shuffle data around between our processes (https://news.ycombinator.com/item?id=42067275).You can try it here: https://www.recall.ai. It's self-serve with $5 of free credits. Pricing starts at $0.70 for every hour of recording, prorated to the second. We offer volume discounts with scale.All data recorded through Recall.ai is the property of our customers, we support 0-day retention, and we don’t train models on customer data.We would love your feedback!
]]></content:encoded>
        </item>
    </channel>
</rss>