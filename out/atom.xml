<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-23T11:32:31.057020+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45332883</id><title>Cap'n Web: a new RPC system for browsers and web servers</title><updated>2025-09-23T11:32:38.489937+00:00</updated><content>&lt;doc fingerprint="e228a11afa2902c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Allow us to introduce Cap'n Web, an RPC protocol and implementation in pure TypeScript.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is a spiritual sibling to Cap'n Proto, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unlike Cap'n Proto, Cap'n Web has no schemas. In fact, it has almost no boilerplate whatsoever. This means it works more like the JavaScript-native RPC system in Cloudflare Workers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;That said, it integrates nicely with TypeScript.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The whole thing compresses (minify+gzip) to under 10√Ç kB with no dependencies.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It's open source under the MIT license.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Cap'n Web is more expressive than almost every other RPC system, because it implements an object-capability RPC model. That means it:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Supports bidirectional calling. The client can call the server, and the server can also call the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Similarly, supports passing objects by reference: If a class extends the special marker type &lt;code&gt;RpcTarget&lt;/code&gt;, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports capability-based security patterns.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs √¢ while still acknowledging and compensating for network latency.&lt;/p&gt;
      &lt;p&gt;The best part is, Cap'n Web is absolutely trivial to set up.&lt;/p&gt;
      &lt;p&gt;A client looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And here's a complete Cloudflare Worker implementing an RPC server:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;That's it. That's the app.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;You can add more methods to &lt;code&gt;MyApiServer&lt;/code&gt;, and call them from the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can have the client pass a callback function to the server, and then the server can just call it.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can define a TypeScript interface for your API, and easily apply it to the client and server.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It just works.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Why RPC? (And what is RPC anyway?)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in REST style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.&lt;/p&gt;
      &lt;p&gt;The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the fallacies of distributed computing.&lt;/p&gt;
      &lt;p&gt;But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".&lt;/p&gt;
      &lt;p&gt;Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.&lt;/p&gt;
      &lt;p&gt;The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;When should you use Cap'n Web?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Features, features, features√¢¬¶&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's some more things you can do with Cap'n Web.&lt;/p&gt;
      &lt;p&gt;Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?&lt;/p&gt;
      &lt;p&gt;For that, Cap'n Web supports HTTP batch mode:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(The server is exactly the same as before.)&lt;/p&gt;
      &lt;p&gt;Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And that brings us to another feature√¢¬¶&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Chained calls (Promise Pipelining)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's where things get magical.&lt;/p&gt;
      &lt;p&gt;In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.&lt;/p&gt;
      &lt;p&gt;For example, say your API is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notice the initial call to &lt;code&gt;getMyName()&lt;/code&gt; returned a promise, but we used the promise itself as the input to &lt;code&gt;hello()&lt;/code&gt;, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."&lt;/p&gt;
      &lt;p&gt;Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Did you spot the security?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This last example shows an important security pattern enabled by Cap'n Web's object-capability model.&lt;/p&gt;
      &lt;p&gt;When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Here's what makes this work: It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.&lt;/p&gt;
      &lt;p&gt;In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.&lt;/p&gt;
      &lt;p&gt;This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.&lt;/p&gt;
      &lt;p&gt;The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safety√¢¬¶&lt;/p&gt;
      &lt;p&gt;If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&amp;lt;string&amp;gt;;
}

// On the client:
let api: RpcStub&amp;lt;MyApi&amp;gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now you get end-to-end type checking, auto-completed method names, and so on.&lt;/p&gt;
      &lt;p&gt;Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web √¢ JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;An alternative to GraphQL?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;If you√¢ve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the √¢waterfall√¢ problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;GET /user
GET /user/friends
GET /user/friends/photos&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;√¢¬¶you can write one GraphQL query to fetch it all at once.&lt;/p&gt;
      &lt;p&gt;That√¢s a big improvement over REST, but GraphQL comes with its own tradeoffs:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;New language and tooling. You have to adopt GraphQL√¢s schema language, servers, and client libraries. If your team is all-in on JavaScript, that√¢s a lot of extra machinery.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Limited composability. GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you can√¢t easily say: √¢create a user, then immediately use that new user object to make a friend request, all-in-one round trip.√¢&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Different abstraction model. GraphQL doesn√¢t look or feel like the JavaScript APIs you already know. You√¢re learning a new mental model rather than extending the one you use every day.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;How Cap'n Web goes further&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web solves the waterfall problem without introducing a new language or ecosystem. It√¢s just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;What happens under the hood? Both calls are pipelined into a single network round trip:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Create the user.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Take the result of that call (a new User object).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Immediately invoke sendFriendRequest() on that object.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.&lt;/p&gt;
      &lt;p&gt;In other words, GraphQL gave us a way to flatten REST√¢s waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;But how do we solve arrays?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."&lt;/p&gt;
      &lt;p&gt;In short, we need an &lt;code&gt;array.map()&lt;/code&gt; operation that can be performed without adding a round trip.&lt;/p&gt;
      &lt;p&gt;Cap'n Proto, historically, has never supported such a thing.&lt;/p&gt;
      &lt;p&gt;But with Cap'n Web, we've solved it. You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&amp;gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;.map()&lt;/code&gt; takes a callback function, which needs to be applied to each element in the array. As we described earlier, normally when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.&lt;/p&gt;
      &lt;p&gt;But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.&lt;/p&gt;
      &lt;p&gt;To that end, &lt;code&gt;.map() &lt;/code&gt;is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Invoke &lt;code&gt;api.getUserPhoto(friend.id)&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Return an object &lt;code&gt;{friend, photo}&lt;/code&gt;, where friend is the original array element and photo is the result of step 1.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?&lt;/p&gt;
      &lt;p&gt;The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.&lt;/p&gt;
      &lt;p&gt;And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is just the RPC protocol itself. √∞¬§¬Ø&lt;/p&gt;
      &lt;p&gt;Cap'n Web's underlying protocol is based on JSON √¢ but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for &lt;code&gt;Date&lt;/code&gt; objects, but Cap'n Web does. You might see a message that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To encode a literal array, we simply double-wrap it in &lt;code&gt;[]&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  names: [["Alice", "Bob", "Carol"]]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.&lt;/p&gt;
      &lt;p&gt;Note that only a fixed set of types are supported: essentially, "structured clonable" types, and RPC stub types.&lt;/p&gt;
      &lt;p&gt;On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto √¢ but greatly simplified.&lt;/p&gt;
      &lt;p&gt;Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.&lt;/p&gt;
      &lt;p&gt;In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".&lt;/p&gt;
      &lt;p&gt;Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.&lt;/p&gt;
      &lt;p&gt;Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.&lt;/p&gt;
      &lt;p&gt;At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.&lt;/p&gt;
      &lt;p&gt;From there, new exports are added in two ways:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports √¢ this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.&lt;/p&gt;
      &lt;p&gt;Putting it together, a code sequence like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Might produce a message exchange like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&amp;gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&amp;gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&amp;gt; ["pull", 2]
// Server responds.
&amp;lt;- ["resolve", 2, "Hello, Alice!"]&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For more details about the protocol, check out the docs.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of the recently-launched "remote bindings" feature in Wrangler, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications √¢ expect more blog posts on this in the future.&lt;/p&gt;
      &lt;p&gt;In any case, Cap'n Web is open source, and you can start using it in your own projects now.&lt;/p&gt;
      &lt;p&gt;Check it out on GitHub.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.cloudflare.com/capnweb-javascript-rpc-library/"/><published>2025-09-22T13:05:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45333021</id><title>Why haven't local-first apps become popular?</title><updated>2025-09-23T11:32:38.405324+00:00</updated><content/><link href="https://marcobambini.substack.com/p/why-local-first-apps-havent-become"/><published>2025-09-22T13:17:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45336989</id><title>Qwen3-Omni: Native Omni AI model for text, image and video</title><updated>2025-09-23T11:32:37.851976+00:00</updated><content>&lt;doc fingerprint="a44ecd1b2dbb5a97"&gt;
  &lt;main&gt;
    &lt;p&gt; üíú Qwen Chat | ü§ó Hugging Face | ü§ñ ModelScope | üìë Blog | üìö Cookbooks | üìë Paper &lt;lb/&gt; üñ•Ô∏è Hugging Face Demo | üñ•Ô∏è ModelScope Demo | üí¨ WeChat (ÂæÆ‰ø°) | ü´® Discord | üìë API &lt;/p&gt;
    &lt;p&gt;We release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information üòÉ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025.09.22: üéâüéâüéâ We have released Qwen3-Omni. For more details, please check our blog!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;State-of-the-art across modalities: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multilingual: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Speech Input: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/item&gt;
          &lt;item&gt;Speech Output: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Novel Architecture: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time Audio/Video Interaction: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flexible Control: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed Audio Captioner: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the QuickStart guide to download the model and install the necessary inference environment dependencies, then run and experiment locally‚Äîtry modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Cookbook&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Open&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;Speech Recognition&lt;/cell&gt;
        &lt;cell&gt;Speech recognition, supporting multiple languages and long audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Speech Translation&lt;/cell&gt;
        &lt;cell&gt;Speech-to-Text / Speech-to-Speech translation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Music Analysis&lt;/cell&gt;
        &lt;cell&gt;Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sound Analysis&lt;/cell&gt;
        &lt;cell&gt;Description and analysis of various sound effects and audio signals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Caption&lt;/cell&gt;
        &lt;cell&gt;Audio captioning, detailed description of any audio input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mixed Audio Analysis&lt;/cell&gt;
        &lt;cell&gt;Analysis of mixed audio content, such as speech, music, and environmental sounds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Visual&lt;/cell&gt;
        &lt;cell&gt;OCR&lt;/cell&gt;
        &lt;cell&gt;OCR for complex images.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Object Grounding&lt;/cell&gt;
        &lt;cell&gt;Target detection and grounding.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions about any image.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Math&lt;/cell&gt;
        &lt;cell&gt;Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Description&lt;/cell&gt;
        &lt;cell&gt;Detailed description of video content.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Navigation&lt;/cell&gt;
        &lt;cell&gt;Generating navigation commands from first-person motion videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Scene Transition&lt;/cell&gt;
        &lt;cell&gt;Analysis of scene transitions in videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio-Visual&lt;/cell&gt;
        &lt;cell&gt;Audio Visual Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Interaction&lt;/cell&gt;
        &lt;cell&gt;Interactive communication with the model using audio-visual inputs, including task specification via audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Dialogue&lt;/cell&gt;
        &lt;cell&gt;Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;Audio Function Call&lt;/cell&gt;
        &lt;cell&gt;Using audio input to perform function calls, enabling agent-like behaviors.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Downstream Task Fine-tuning&lt;/cell&gt;
        &lt;cell&gt;Omni Captioner&lt;/cell&gt;
        &lt;cell&gt;Introduction and capability demonstration of Qwen3-Omni-30B-A3B-Captioner, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use Hugging Face Transformers. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using vLLM or performing inference via the DashScope API. We also strongly suggest using our provided Docker image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our cookbooks offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!&lt;/p&gt;
    &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/cell&gt;
        &lt;cell&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's cookbook or Hugging Face Demo and ModelScope Demo.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:&lt;/p&gt;
    &lt;code&gt;# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U "huggingface_hub[cli]"
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner&lt;/code&gt;
    &lt;p&gt;The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you create a new Python environment or use our Docker to avoid environment runtime issues.&lt;/p&gt;
    &lt;code&gt;# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate&lt;/code&gt;
    &lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt;
    &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt;
    &lt;p&gt;Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using vLLM for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.&lt;/p&gt;
    &lt;code&gt;pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the FlashAttention repository. FlashAttention 2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here is a code snippet to show you how to use Qwen3-Omni with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker="Ethan", 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        "output.wav",
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt;
    &lt;code&gt;from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you hear in this audio?"},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen-Omni."}
        ],
    },
    {
        "role": "user",
        "content": "Who are you?"
    }
]

# Conversation with mixed media
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)&lt;/code&gt;
    &lt;head&gt;Use audio output or not&lt;/head&gt;
    &lt;p&gt;The model supports both text and audio outputs. If users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after initializing the model. This option will save about &lt;code&gt;10GB&lt;/code&gt; of GPU memory, but the &lt;code&gt;return_audio&lt;/code&gt; option for the &lt;code&gt;generate&lt;/code&gt; function will only allow &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()&lt;/code&gt;
    &lt;p&gt;For a more flexible experience, we recommend that users decide whether to return audio when the &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs, resulting in faster text responses.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
...
text_ids, _ = model.generate(..., return_audio=False)```

&amp;lt;/details&amp;gt;

&amp;lt;details&amp;gt;
&amp;lt;summary&amp;gt;Change voice type of output audio&amp;lt;/summary&amp;gt;

Qwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker="Ethan")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Chelsie")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Aiden")&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and audio output inference support for the Instruct model will be released in the near future, you can follow the commands below to install vLLM from source. Please note that we recommend you create a new Python environment or use our provided Docker to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the vLLM official documentation.&lt;/p&gt;
    &lt;code&gt;git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;You can use the following code for vLLM inference. The &lt;code&gt;limit_mm_per_prompt&lt;/code&gt; parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting &lt;code&gt;tensor_parallel_size&lt;/code&gt; greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, &lt;code&gt;max_num_seqs&lt;/code&gt; indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the vLLM official documentation. Below is a simple example of how to run Qwen3-Omni with vLLM:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
                {"type": "text", "text": "What can you hear in this audio?"},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen-Omni."}
            ],
        },
        {
            "role": "user",
            "content": "Who are you? Answer in one sentence."
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},
                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)&lt;/code&gt;
    &lt;head&gt;vLLM Serve Usage&lt;/head&gt;
    &lt;p&gt;vLLM serve for Qwen3-Omni currently only supports the thinker model. The &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:&lt;/p&gt;
    &lt;code&gt;# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4&lt;/code&gt;
    &lt;p&gt;Then you can use the chat API as below (via curl, for example):&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8901/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
    ]
    }'&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;API Description&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (Mainland China)&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (International)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen-omni&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/qwen-omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/realtime&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/realtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;API for Qwen3-Omni-30B-A3B-Captioner model&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Precision&lt;/cell&gt;
        &lt;cell role="head"&gt;15s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;30s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;60s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;120s Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;78.85 GB&lt;/cell&gt;
        &lt;cell&gt;88.52 GB&lt;/cell&gt;
        &lt;cell&gt;107.74 GB&lt;/cell&gt;
        &lt;cell&gt;144.81 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;68.74 GB&lt;/cell&gt;
        &lt;cell&gt;77.79 GB&lt;/cell&gt;
        &lt;cell&gt;95.76 GB&lt;/cell&gt;
        &lt;cell&gt;131.65 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; precision, tested with &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt;. The Instruct model includes both the thinker and talker components, whereas the Thinking model includes only the thinker part.&lt;/p&gt;
    &lt;p&gt;When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the following system prompt. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the &lt;code&gt;user_system_prompt&lt;/code&gt; field in the system prompt to include character settings or other role-specific descriptions as needed.&lt;/p&gt;
    &lt;code&gt;user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, ‚ÄúI/me/my/we/our‚Äù refer to the user and ‚Äúyou/your‚Äù refer to the assistant. In your replies, address the user as ‚Äúyou/your‚Äù and yourself as ‚ÄúI/me/my‚Äù; never mirror the user‚Äôs pronouns‚Äîalways shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/code&gt; model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:&lt;/p&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Analyze this audio, image, and video together."},
        ], 
    }
]&lt;/code&gt;
    &lt;p&gt;In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.&lt;/p&gt;
    &lt;code&gt;# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)&lt;/code&gt;
    &lt;code&gt;# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}&lt;/code&gt;
    &lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter must be set consistently across these steps; otherwise, unexpected results may occur.&lt;/p&gt;
    &lt;p&gt;Without local deployment, you can experience an online web demo directly by visiting our Hugging Face Spaces and ModelScope Studio. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.&lt;/p&gt;
    &lt;p&gt;Real-time streaming interaction with Qwen3-Omni is available now. Please visit Qwen Chat and select the voice/video call option in the chat box to experience it.&lt;/p&gt;
    &lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)&lt;/p&gt;
    &lt;p&gt;Before you begin, we strongly recommend that you refer to the Installation section in vLLM Usage to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (note that this will result in significantly slower inference), please follow the installation instructions in Transformers Usage. That said, we still highly recommend using our Docker image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed and you install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1&lt;/code&gt;
    &lt;p&gt;Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run &lt;code&gt;python web_demo.py --help&lt;/code&gt; and &lt;code&gt;python web_demo_captioner.py --help&lt;/code&gt; to learn about more options.&lt;/p&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2&lt;/code&gt;
    &lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt;
    &lt;code&gt;Running on local: http://127.0.0.1:8901/
&lt;/code&gt;
    &lt;p&gt;If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a &lt;code&gt;docker&lt;/code&gt; container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official &lt;code&gt;docker&lt;/code&gt; container to the host machine, please refer to here.&lt;/p&gt;
    &lt;p&gt;To simplify the deployment process, we provide Docker images with pre-built environments: qwenllm/qwen3-omni. You only need to install the driver and download model files to launch the demos. Please refer to the guide to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:&lt;/p&gt;
    &lt;code&gt;LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124&lt;/code&gt;
    &lt;p&gt;After executing the command, you will enter the bash shell of the container. Your local model and data directory (please replace &lt;code&gt;/path/to/your/workspace&lt;/code&gt; with the actual path) will be mounted to the container's internal path &lt;code&gt;/data/shared/Qwen3-Omni&lt;/code&gt;. The host's port &lt;code&gt;8901&lt;/code&gt; is mapped to port &lt;code&gt;80&lt;/code&gt; in the container, meaning you can access the service inside the container by visiting port &lt;code&gt;8901&lt;/code&gt; on the host machine.&lt;/p&gt;
    &lt;p&gt;Please note that services inside the container must be started with the IP &lt;code&gt;0.0.0.0&lt;/code&gt; to ensure proper port forwarding. For example:&lt;/p&gt;
    &lt;code&gt;# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0&lt;/code&gt;
    &lt;p&gt;For more ways to launch the web demo, please refer to Launch Local Web UI Demo. If you exit the container, you can re-enter it using the following command:&lt;/p&gt;
    &lt;code&gt;docker start qwen3-omni
docker exec -it qwen3-omni bash&lt;/code&gt;
    &lt;p&gt;Or if you want to completely remove the container, please run:&lt;/p&gt;
    &lt;code&gt;docker rm -f qwen3-omni&lt;/code&gt;
    &lt;p&gt;Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.&lt;/p&gt;
    &lt;head&gt;Text -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;GPT-4o-0327&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Non Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;91.3&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;66.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;69.6&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;61.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ZebraLogic&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;81.4&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;86.0&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;82.6&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;66.5&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;64.4&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;25.5&lt;/cell&gt;
        &lt;cell&gt;27.0&lt;/cell&gt;
        &lt;cell&gt;43.1&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;39.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;92.7&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;72.0&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;73.7&lt;/cell&gt;
        &lt;cell&gt;74.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LiveBench 20241125&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;70.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
        &lt;cell&gt;81.3&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Arena-Hard v2&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;61.5&lt;/cell&gt;
        &lt;cell&gt;56.0&lt;/cell&gt;
        &lt;cell&gt;55.1&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;82.5&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;74.4&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;76.4&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;49.8&lt;/cell&gt;
        &lt;cell&gt;54.7&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Audio -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Seed-ASR&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Mini&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Small&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Transcribe&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EN &amp;amp; ZH ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Wenetspeech&lt;p&gt;net | meeting&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;4.66 | 5.69&lt;/cell&gt;
        &lt;cell&gt;24.30 | 31.53&lt;/cell&gt;
        &lt;cell&gt;20.33 | 26.08&lt;/cell&gt;
        &lt;cell&gt;15.30 | 32.27&lt;/cell&gt;
        &lt;cell&gt;14.43 | 13.47&lt;/cell&gt;
        &lt;cell&gt;5.91 | 7.65&lt;/cell&gt;
        &lt;cell&gt;4.69 | 5.89&lt;/cell&gt;
        &lt;cell&gt;4.62 | 5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Librispeech&lt;p&gt;clean | other&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.58 | 2.84&lt;/cell&gt;
        &lt;cell&gt;1.88 | 4.12&lt;/cell&gt;
        &lt;cell&gt;1.56 | 3.30&lt;/cell&gt;
        &lt;cell&gt;1.39 | 3.75&lt;/cell&gt;
        &lt;cell&gt;2.89 | 3.56&lt;/cell&gt;
        &lt;cell&gt;1.74 | 3.45&lt;/cell&gt;
        &lt;cell&gt;1.22 | 2.48&lt;/cell&gt;
        &lt;cell&gt;1.27 | 2.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;9.47&lt;/cell&gt;
        &lt;cell&gt;7.79&lt;/cell&gt;
        &lt;cell&gt;10.01&lt;/cell&gt;
        &lt;cell&gt;9.89&lt;/cell&gt;
        &lt;cell&gt;7.61&lt;/cell&gt;
        &lt;cell&gt;6.05&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.67&lt;/cell&gt;
        &lt;cell&gt;19.30&lt;/cell&gt;
        &lt;cell&gt;9.84&lt;/cell&gt;
        &lt;cell&gt;8.00&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;4.31&lt;/cell&gt;
        &lt;cell&gt;4.28&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en&lt;/cell&gt;
        &lt;cell&gt;3.40&lt;/cell&gt;
        &lt;cell&gt;3.96&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;2.94&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;2.72&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh&lt;/cell&gt;
        &lt;cell&gt;2.69&lt;/cell&gt;
        &lt;cell&gt;12.22&lt;/cell&gt;
        &lt;cell&gt;7.98&lt;/cell&gt;
        &lt;cell&gt;2.44&lt;/cell&gt;
        &lt;cell&gt;2.71&lt;/cell&gt;
        &lt;cell&gt;2.54&lt;/cell&gt;
        &lt;cell&gt;2.20&lt;/cell&gt;
        &lt;cell&gt;2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multilingual ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-avg&lt;p&gt;(19 lang)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;15.67&lt;/cell&gt;
        &lt;cell&gt;8.09&lt;/cell&gt;
        &lt;cell&gt;4.48&lt;/cell&gt;
        &lt;cell&gt;5.55&lt;/cell&gt;
        &lt;cell&gt;14.04&lt;/cell&gt;
        &lt;cell&gt;5.33&lt;/cell&gt;
        &lt;cell&gt;5.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lyric ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MIR-1K (vocal-only)&lt;/cell&gt;
        &lt;cell&gt;6.45&lt;/cell&gt;
        &lt;cell&gt;23.33&lt;/cell&gt;
        &lt;cell&gt;18.73&lt;/cell&gt;
        &lt;cell&gt;11.87&lt;/cell&gt;
        &lt;cell&gt;9.85&lt;/cell&gt;
        &lt;cell&gt;8.15&lt;/cell&gt;
        &lt;cell&gt;5.90&lt;/cell&gt;
        &lt;cell&gt;5.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Opencpop-test&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;31.01&lt;/cell&gt;
        &lt;cell&gt;16.06&lt;/cell&gt;
        &lt;cell&gt;7.93&lt;/cell&gt;
        &lt;cell&gt;6.49&lt;/cell&gt;
        &lt;cell&gt;2.84&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;S2TT (BLEU)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;30.35&lt;/cell&gt;
        &lt;cell&gt;37.85&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;39.25&lt;/cell&gt;
        &lt;cell&gt;29.22&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;36.22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-xx2en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;27.54&lt;/cell&gt;
        &lt;cell&gt;32.81&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;35.41&lt;/cell&gt;
        &lt;cell&gt;28.61&lt;/cell&gt;
        &lt;cell&gt;31.08&lt;/cell&gt;
        &lt;cell&gt;30.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;17.03&lt;/cell&gt;
        &lt;cell&gt;22.05&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.63&lt;/cell&gt;
        &lt;cell&gt;17.97&lt;/cell&gt;
        &lt;cell&gt;25.17&lt;/cell&gt;
        &lt;cell&gt;25.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fleurs-xx2zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;28.75&lt;/cell&gt;
        &lt;cell&gt;34.82&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;27.68&lt;/cell&gt;
        &lt;cell&gt;33.13&lt;/cell&gt;
        &lt;cell&gt;31.19&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;VoiceBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AlpacaEval&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.1&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;89.9&lt;/cell&gt;
        &lt;cell&gt;94.8&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;95.4&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CommonEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;76.7&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;91.0&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WildVoice&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD-QA&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;78.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;66.1&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;61.7&lt;/cell&gt;
        &lt;cell&gt;68.1&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OpenBookQA&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;56.9&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;80.9&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BBH&lt;/cell&gt;
        &lt;cell&gt;84.1&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;53.5&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AdvBench&lt;/cell&gt;
        &lt;cell&gt;98.7&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
        &lt;cell&gt;98.1&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.3&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;73.6&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;85.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Audio Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMAU-v05.15.25&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;75.4&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;62.6&lt;/cell&gt;
        &lt;cell&gt;69.0&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Best Specialist&lt;p&gt;Models&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RUL-MuchoMusic&lt;/cell&gt;
        &lt;cell&gt;47.6 (Audio Flamingo 3)&lt;/cell&gt;
        &lt;cell&gt;36.1&lt;/cell&gt;
        &lt;cell&gt;49.4&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;52.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GTZAN&lt;p&gt;Acc.&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;87.9 (CLaMP 3)&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;93.0&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Genre&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;35.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;32.6&lt;/cell&gt;
        &lt;cell&gt;32.5&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Mood/Theme&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;10.9 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;11.3&lt;/cell&gt;
        &lt;cell&gt;14.1&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;21.0&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Instrument&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;39.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;34.2&lt;/cell&gt;
        &lt;cell&gt;33.0&lt;/cell&gt;
        &lt;cell&gt;22.6&lt;/cell&gt;
        &lt;cell&gt;40.5&lt;/cell&gt;
        &lt;cell&gt;40.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Top50&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;33.2 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.0&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;21.6&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;36.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MagnaTagATune&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;41.6 (MuQ)&lt;/cell&gt;
        &lt;cell&gt;29.2&lt;/cell&gt;
        &lt;cell&gt;28.1&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;44.3&lt;/cell&gt;
        &lt;cell&gt;46.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Vision -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT4-o&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.0-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-VL&lt;p&gt;72B&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;55.0&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;55.2&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.7&lt;/cell&gt;
        &lt;cell&gt;6.7&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;51.9&lt;/cell&gt;
        &lt;cell&gt;56.1&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
        &lt;cell&gt;57.0&lt;/cell&gt;
        &lt;cell&gt;57.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;74.8&lt;/cell&gt;
        &lt;cell&gt;75.9&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;48.6&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;58.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;AI2D&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;88.7&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;86.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;87.9&lt;/cell&gt;
        &lt;cell&gt;91.2&lt;/cell&gt;
        &lt;cell&gt;93.6&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;71.0&lt;/cell&gt;
        &lt;cell&gt;74.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-flash-thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;InternVL-3.5-241B-A28B&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;63.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
        &lt;cell&gt;75.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;60.5&lt;/cell&gt;
        &lt;cell&gt;60.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;81.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AI2D_test&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;87.3&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.0&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;79.6&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;82.1&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;AudioVisual -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WorldSense&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;50.9&lt;/cell&gt;
        &lt;cell&gt;45.4&lt;/cell&gt;
        &lt;cell&gt;54.0&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DailyOmni&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
        &lt;cell&gt;72.7&lt;/cell&gt;
        &lt;cell&gt;75.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VideoHolmes&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Zero-shot Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Content Consistency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEED&lt;p&gt;test-zh | test-en&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Seed-TTSICL&lt;/cell&gt;
        &lt;cell&gt;1.11 | 2.24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed-TTSRL&lt;/cell&gt;
        &lt;cell&gt;1.00 | 1.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MaskGCT&lt;/cell&gt;
        &lt;cell&gt;2.27 | 2.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E2 TTS&lt;/cell&gt;
        &lt;cell&gt;1.97 | 2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F5-TTS&lt;/cell&gt;
        &lt;cell&gt;1.56 | 1.83&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spark TTS&lt;/cell&gt;
        &lt;cell&gt;1.20 | 1.98&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 2&lt;/cell&gt;
        &lt;cell&gt;1.45 | 2.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 3&lt;/cell&gt;
        &lt;cell&gt;0.71 | 1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni-7B&lt;/cell&gt;
        &lt;cell&gt;1.42 | 2.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;1.07 | 1.39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Multilingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Speaker Similarity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Chinese&lt;/cell&gt;
        &lt;cell&gt;0.716&lt;/cell&gt;
        &lt;cell&gt;2.252&lt;/cell&gt;
        &lt;cell&gt;16.026&lt;/cell&gt;
        &lt;cell&gt;0.772&lt;/cell&gt;
        &lt;cell&gt;0.780&lt;/cell&gt;
        &lt;cell&gt;0.677&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;English&lt;/cell&gt;
        &lt;cell&gt;1.069&lt;/cell&gt;
        &lt;cell&gt;2.164&lt;/cell&gt;
        &lt;cell&gt;2.339&lt;/cell&gt;
        &lt;cell&gt;0.773&lt;/cell&gt;
        &lt;cell&gt;0.756&lt;/cell&gt;
        &lt;cell&gt;0.613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;German&lt;/cell&gt;
        &lt;cell&gt;0.777&lt;/cell&gt;
        &lt;cell&gt;1.906&lt;/cell&gt;
        &lt;cell&gt;0.572&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
        &lt;cell&gt;0.733&lt;/cell&gt;
        &lt;cell&gt;0.614&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Italian&lt;/cell&gt;
        &lt;cell&gt;1.067&lt;/cell&gt;
        &lt;cell&gt;1.543&lt;/cell&gt;
        &lt;cell&gt;1.743&lt;/cell&gt;
        &lt;cell&gt;0.742&lt;/cell&gt;
        &lt;cell&gt;0.699&lt;/cell&gt;
        &lt;cell&gt;0.579&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Portuguese&lt;/cell&gt;
        &lt;cell&gt;1.872&lt;/cell&gt;
        &lt;cell&gt;1.877&lt;/cell&gt;
        &lt;cell&gt;1.331&lt;/cell&gt;
        &lt;cell&gt;0.770&lt;/cell&gt;
        &lt;cell&gt;0.805&lt;/cell&gt;
        &lt;cell&gt;0.711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Spanish&lt;/cell&gt;
        &lt;cell&gt;1.765&lt;/cell&gt;
        &lt;cell&gt;1.029&lt;/cell&gt;
        &lt;cell&gt;1.084&lt;/cell&gt;
        &lt;cell&gt;0.744&lt;/cell&gt;
        &lt;cell&gt;0.762&lt;/cell&gt;
        &lt;cell&gt;0.615&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Japanese&lt;/cell&gt;
        &lt;cell&gt;3.631&lt;/cell&gt;
        &lt;cell&gt;3.519&lt;/cell&gt;
        &lt;cell&gt;10.646&lt;/cell&gt;
        &lt;cell&gt;0.763&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Korean&lt;/cell&gt;
        &lt;cell&gt;1.670&lt;/cell&gt;
        &lt;cell&gt;1.747&lt;/cell&gt;
        &lt;cell&gt;1.865&lt;/cell&gt;
        &lt;cell&gt;0.778&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;French&lt;/cell&gt;
        &lt;cell&gt;2.505&lt;/cell&gt;
        &lt;cell&gt;4.099&lt;/cell&gt;
        &lt;cell&gt;5.216&lt;/cell&gt;
        &lt;cell&gt;0.689&lt;/cell&gt;
        &lt;cell&gt;0.628&lt;/cell&gt;
        &lt;cell&gt;0.535&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Russian&lt;/cell&gt;
        &lt;cell&gt;3.986&lt;/cell&gt;
        &lt;cell&gt;4.281&lt;/cell&gt;
        &lt;cell&gt;3.878&lt;/cell&gt;
        &lt;cell&gt;0.759&lt;/cell&gt;
        &lt;cell&gt;0.761&lt;/cell&gt;
        &lt;cell&gt;0.676&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Cross-Lingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice3&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-zh&lt;/cell&gt;
        &lt;cell&gt;5.37&lt;/cell&gt;
        &lt;cell&gt;5.09&lt;/cell&gt;
        &lt;cell&gt;13.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-zh&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;3.05&lt;/cell&gt;
        &lt;cell&gt;48.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-zh&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;1.06&lt;/cell&gt;
        &lt;cell&gt;7.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-en&lt;/cell&gt;
        &lt;cell&gt;2.76&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;6.47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-en&lt;/cell&gt;
        &lt;cell&gt;3.31&lt;/cell&gt;
        &lt;cell&gt;4.20&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-en&lt;/cell&gt;
        &lt;cell&gt;3.34&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;11.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ja&lt;/cell&gt;
        &lt;cell&gt;8.29&lt;/cell&gt;
        &lt;cell&gt;7.08&lt;/cell&gt;
        &lt;cell&gt;13.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ja&lt;/cell&gt;
        &lt;cell&gt;7.53&lt;/cell&gt;
        &lt;cell&gt;6.80&lt;/cell&gt;
        &lt;cell&gt;14.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-ja&lt;/cell&gt;
        &lt;cell&gt;4.24&lt;/cell&gt;
        &lt;cell&gt;3.93&lt;/cell&gt;
        &lt;cell&gt;5.86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ko&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ko&lt;/cell&gt;
        &lt;cell&gt;4.96&lt;/cell&gt;
        &lt;cell&gt;5.87&lt;/cell&gt;
        &lt;cell&gt;21.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ja-to-ko&lt;/cell&gt;
        &lt;cell&gt;6.23&lt;/cell&gt;
        &lt;cell&gt;7.92&lt;/cell&gt;
        &lt;cell&gt;21.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding Strategy: For the Qwen3-Omni series across all evaluation benchmarks, &lt;code&gt;Instruct&lt;/code&gt;models use greedy decoding during generation without sampling. For&lt;code&gt;Thinking&lt;/code&gt;models, the decoding parameters should be taken from the&lt;code&gt;generation_config.json&lt;/code&gt;file in the checkpoint.&lt;/item&gt;
      &lt;item&gt;Benchmark-Specific Formatting: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to &lt;code&gt;fps=2&lt;/code&gt;during evaluation.&lt;/item&gt;
      &lt;item&gt;Default Prompts: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Task Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Chinese&lt;/cell&gt;
        &lt;cell&gt;ËØ∑Â∞ÜËøôÊÆµ‰∏≠ÊñáËØ≠Èü≥ËΩ¨Êç¢‰∏∫Á∫ØÊñáÊú¨„ÄÇ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Other languages&lt;/cell&gt;
        &lt;cell&gt;Transcribe the audio into text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Speech-to-Text Translation (S2TT)&lt;/cell&gt;
        &lt;cell&gt;Listen to the provided &amp;lt;source_language&amp;gt; speech and produce a translation in &amp;lt;target_language&amp;gt; text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Song Lyrics Recognition&lt;/cell&gt;
        &lt;cell&gt;Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System Prompt: No &lt;code&gt;system prompt&lt;/code&gt;should be set for any evaluation benchmark.&lt;/item&gt;
      &lt;item&gt;Input Sequence: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come after multimodal data in the sequence. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe the audio, image and video."},
        ],
    },
]&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/QwenLM/Qwen3-Omni"/><published>2025-09-22T17:50:21+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45339923</id><title>Germicidal UV could make airborne diseases as rare as those carried by water</title><updated>2025-09-23T11:32:37.551006+00:00</updated><content>&lt;doc fingerprint="fe9810162a2317cb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Seeing the light&lt;/head&gt;
    &lt;head rend="h3"&gt;Germicidal ultraviolet could make airborne diseases as rare as those carried by water.&lt;/head&gt;
    &lt;p&gt;Works in Progress is becoming a print magazine. Our first print issue, Issue 21, will land in November. If you live in the United States and the United Kingdom, you can subscribe here. If you live outside the US or UK and want to be notified as soon as subscriptions are live in your country, leave your details here.&lt;/p&gt;
    &lt;p&gt;Between the 1860s and 1920, successive outbreaks of typhoid fever killed over 300,000 Americans. As population growth surged and people moved to urban areas en masse, American cities began to dump sewage in the same rivers that provided their drinking water. After epidemiologists linked typhoid outbreaks to water cleanliness, cities began building large-scale sand filtration systems in the 1890s, and in 1908, Jersey City pioneered the first continuous chlorination of a public water supply. By the 1920s, typhoid deaths had fallen by two-thirds, and waterborne diseases were in retreat across the country.&lt;/p&gt;
    &lt;p&gt;While typhoid and other waterborne diseases triggered vast engineering and regulatory responses, the equivalent airborne threats have not. Tuberculosis alone kills more than a million people every year around the world, yet the air in schools, clinics, and public buildings remains largely unfiltered and unmonitored. Covid-19, which killed over seven million people, demonstrated how rapidly airborne pathogens can spread in poorly ventilated spaces.&lt;/p&gt;
    &lt;p&gt;Just as filtration and chlorination made drinking water safe at scale, we now have the tools to do the same for indoor air: ventilation, high-quality filters, and germicidal light. A century ago, germicidal light at 254 nanometers seemed to be a promising way of controlling pathogens by killing them in the air, but it turned out to cause irritation and cancer in the skin, and it was largely dropped when antibiotics became widespread.&lt;/p&gt;
    &lt;p&gt;But today there is an update that has none of these drawbacks. We now know that wavelengths under 230 nanometers, especially 222 nanometer light, are harmless to humans, but can still disable microscopic pathogens. We know how to filter out all wavelengths except the ones we want, and how to direct them away from humans, cycling the air through them to clean it without exposing people to it, just in case it carries unknown risks. This far-UVC light, as it is called, may be how we can make the air we breathe as safe as the water we drink.&lt;/p&gt;
    &lt;head rend="h3"&gt;The sterilamp&lt;/head&gt;
    &lt;p&gt;Until the mid-nineteenth century, most physicians believed that disease spread through miasmas, poisonous vapors rising from filth and decay. Contemporaries were obsessed with air: moving to the countryside for the air, taking the air at the seaside. Bad air was blamed for sickness. Herbs were burned to purify the air to fight the plague. But because they didn‚Äôt understand what made the air dirty, they were not very good at cleaning it. This began to change only when Louis Pasteur and Robert Koch provided the first definitive proof that microscopic organisms were responsible for infectious disease.&lt;/p&gt;
    &lt;p&gt;Cleaning air relies on the same fundamental techniques as cleaning water: replacement, filtration, and disinfection. Pathogens replicate inside people, who then expel them into the air by breathing, talking, or coughing, where they can remain suspended and infect new individuals. The relative contributions of aerosol transmission, droplet transmission, and fomite (surface) transmission vary between diseases. Certain diseases, such as Covid-19, are driven by a small number of highly infectious individuals (‚Äòsuperspreaders‚Äô) that account for a disproportionate number of cases.&lt;/p&gt;
    &lt;p&gt;In 1877, British researchers Arthur Downes and Thomas Blunt stumbled upon a discovery that would lay the groundwork for modern disinfection. In a paper submitted to the Royal Society of London, they described how over the course of six months they had used sunlight to prevent bacteria from growing in a tube.&lt;/p&gt;
    &lt;p&gt;Follow-up research by Robert Koch demonstrated that sunlight could kill Mycobacterium tuberculosis, but the early experiments lacked precision. Scientists knew light worked, but not which parts of the spectrum were responsible. The turning point came in 1930, when Frederick L Gates published the first quantitative analysis of how ultraviolet light affected bacteria, pinpointing peak germicidal effectiveness at 265 nanometers, the same point that nucleic acids ‚Äì DNA and RNA ‚Äì absorb light.&lt;/p&gt;
    &lt;p&gt;Understanding the spectrum wasn‚Äôt necessary for harnessing it. In 1901, American electrical engineer Peter Cooper Hewitt patented the first mercury-vapor lamp to achieve widespread commercial success. Hewitt created a voltage difference across electrodes in a glass tube, with mercury vapor causing electrons to separate from mercury atoms and collide with other atoms, exciting them to higher energy states. When these excited atoms return to their ground state, they release energy as photons, primarily at a wavelength of 254 nanometers.&lt;/p&gt;
    &lt;p&gt;Nucleic acids absorb this 254-nanometer light and produce new bonds between bases, preventing DNA/RNA replication. For human skin and eyes, this means sunburn-like irritation. But for viruses, amoeba, and bacteria in the air, as engineers at an American manufacturing company put it, these rays spell doom.&lt;/p&gt;
    &lt;p&gt;An array of different factors influences how sensitive pathogens are to ultraviolet-C (UVC), ultraviolet light with a wavelength of 100‚Äì280 nanometers. Viruses with larger genomes ‚Äì such as herpesviruses (which have about 150,000 base pairs) or coronaviruses (30,000 base pairs) ‚Äì offer many nucleotide bonds for photons to break, so they can be more quickly disabled than small-genome viruses such as parvoviruses (5,000 base pairs).&lt;/p&gt;
    &lt;p&gt;In 1936, Dr. Deryl Hart, a surgeon at Duke University Hospital, became the first person to use ultraviolet radiation to curb airborne infections in surgical operating rooms. High-intensity germicidal ultraviolet light fixtures, designed to irradiate the entire room, cut postoperative wound infection rates from 11.6 percent to just 0.2 percent, and not one patient out of 2,463 cases died from postoperative infections.&lt;/p&gt;
    &lt;p&gt;Shortly after the installation at Duke, UV from mercury vapor lamps was used to create invisible ‚Äòcurtains‚Äô between cubicles in hospital wards and cribs in nurseries in Philadelphia, Boston, Toronto, and Evanston. In Boston, this attempt to prevent respiratory pathogens from crossing cubicles and infecting infant patients led to infection rates that were a quarter of those in the other cubicles. In the other cases, cross-infections fell by between 40 percent and 96 percent.&lt;/p&gt;
    &lt;p&gt;In 1937, researchers installed upper-room germicidal UV lights in the Germantown Friends School in Philadelphia and studied its impact over the next few school years. During the fourth year of the study, Philadelphia saw its largest recorded epidemic of measles, the most transmissible known pathogen. In the irradiated classrooms, only 14.5 percent of susceptible children fell ill, while in the unprotected ones, infection rates soared to 55 percent.&lt;/p&gt;
    &lt;p&gt;At the height of World War II, US defence manufacturer Westinghouse wrote that it was ‚Äòfighting two wars at once‚Äô: one against the Germans and Japanese and the other against germs. Once a major producer of UV lamps and fixtures, the company had developed the Sterilamp in the 1930s. The Sterilamp system is described in a 1943 newsletter as a ‚ÄòDEATH RAY THAT GUARDS LIFE‚Äô.&lt;/p&gt;
    &lt;p&gt;But the deployment of germicidal UV light stalled after the war. In 1945‚Äì46, the New York Department of Health tried to replicate the Philadelphia findings in large public rural schools and failed. In one of the schools with an internal control group in Port Byron, 90.4 percent of susceptible students in UV-treated classrooms still contracted measles, compared to 83 percent in unirradiated classrooms.&lt;/p&gt;
    &lt;p&gt;Once they took bus ridership into account, the researchers found that UV had appeared to reduce the measles incidence rate by roughly 8.2 percentage points (from 77.4 percent to 69.2 percent) in non-bus riders. Germicidal UV could disrupt measles transmission in specific spaces such as classrooms, but it proved insufficient if face-to-face exposure continued in buses, hallways, and other shared spaces. Mercury lamps were also costly and could irritate the skin and eyes of people exposed to them. No long-term effects had been observed in humans at the doses used in schools, but higher doses had been shown to raise long-term cancer risk in mice. At the same time, the large-scale emergence of antibiotics was transforming the fight against infectious diseases. By 1945, mass production made penicillin widely available, ushering in the antibiotic era and shifting the medical focus toward drug-based infection control. Together with the failures of the UV studies, interest in germicidal UV receded.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning from obsolescence&lt;/head&gt;
    &lt;p&gt;Decades later, over 700,000 people a year die from antibiotic resistance as people use more and more antibiotics while their discovery rates stagnate. Airborne viral pandemics, which antibiotics cannot treat, have caused enormous economic damage and inflicted tens of millions of deaths. There is once again a case for the use of germicidal ultraviolet light.&lt;/p&gt;
    &lt;p&gt;Skin and eye irritation and cancer risks limited direct exposure to 254-nanometer germicidal UV to surgical cases. But new technologies ‚Äì circulation, light filtering, and lower wavelengths ‚Äì have together fixed these problems.&lt;/p&gt;
    &lt;p&gt;If we can circulate air, we can move it, clean it, and put it back. Today‚Äôs systems do this by projecting beams across the upper level of a room, above the heads of occupants. Natural circulation patterns move air to the ceiling, where it is cleaned, before it falls back down. But even reflected beams can cause irritation, meaning that each installation has to be calibrated carefully for the specifics of the room. Even repainting or shifting furniture can change the reflection of the beams, meaning that the UV beams would need to be recalibrated.&lt;/p&gt;
    &lt;p&gt;Shorter-wavelength light works just as effectively, without many of the downsides. Light with a wavelength of 230 nanometers or lower, known as ‚Äòfar-UVC‚Äô, disables pathogens without hurting humans, but mercury lamps cannot produce it.&lt;/p&gt;
    &lt;p&gt;Unlike longer wavelengths, far-UVC penetrates weakly, meaning it is only absorbed by the uppermost layers of the skin and eye, tissues which slough off frequently and do not divide. When shone directly into the eyes, germicidal doses of far-UVC do not appear to have any effect beyond temporary discomfort (the same as if a torch was shone into your eyes). When installed overhead, the lamp‚Äôs positioning and human facial structure mean only five percent of the light reaches the eye, creating an even larger safety margin.&lt;/p&gt;
    &lt;p&gt;Long-term exposure does not harm mice at all, even those with a severely limited ability to repair DNA defects. In humans, a 36-month-long trial installation of far-UVC in a hospital, with ongoing exposure to germicidal doses, caused no adverse effects on those working on the ward. There is no evidence from any other study that far-UVC raises cancer rates, though more work should be done to prove this beyond any doubt.&lt;/p&gt;
    &lt;p&gt;In order to prevent disease transmission, far-UVC lamps must disable pathogens in respiratory aerosols quickly enough that, by the time they are inhaled, not enough for an infectious dose remain.&lt;/p&gt;
    &lt;p&gt;A 2024 study presented some of the best data on far-UVC in an occupied room. Four lamps were installed in a lab mouse cage-cleaning room, where the constant activity and movement continuously aerosolized murine norovirus present in the bedding. When switched on, the lamps reduced the amount of virus in the air by 98 percent, equivalent to at least 36 air changes per hour. This happened despite murine norovirus being more resistant to far-UVC than many common human respiratory viruses, likely due to its tough protein outer ‚Äòshell‚Äô.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;There are other ways to clean the air, like ventilation and filtration. There‚Äôs an important role for both of these approaches, but they come with critical limitations that only far-UVC can plausibly overcome.&lt;/p&gt;
    &lt;p&gt;Ventilation is the easiest method of removing things from indoor air, diluting pathogens and pollutants into the 5.5 quadrillion tonnes of atmosphere outside. But it is not always convenient. It can be awkward or noisy to install and run. In highly polluted areas, bringing in outdoor air reduces disease transmission at the expense of worse air quality. In 2024, only seven countries met World Health Organization air quality standards. In very hot or cold places, outdoor air must be heated or cooled, which can be energy-intensive and costly, especially in older or heritage buildings where air conditioning needs to be retrofitted.&lt;/p&gt;
    &lt;p&gt;Another technique is filtration. Mechanical filters work by forcing air through a dense, pleated mesh of fine fibers that trap hazardous particles. Originally developed as part of the Manhattan Project to prevent the spread of radioactive particles, mechanical filters have become standard in hospitals, cleanrooms, and aircraft cabins.&lt;/p&gt;
    &lt;p&gt;Mechanical filters are ‚Äòplug and play‚Äô: there are dozens of verified and recommended models that anyone can buy off the shelf, install in their house by plugging them in, and expect to deliver reduced pollutants, allergens, and cleaner air for between $80 and $300. Consumer far-UVC systems are often considerably more expensive ‚Äì a typical example costs $2,500 for a lamp covering 1,000 square feet, although a newer model has recently come to market at $600, and less expensive lamps with lower output can be added together to cover a similar floorspace.&lt;/p&gt;
    &lt;p&gt;The drawbacks of ventilation-based air filtration methods are similar to the drawbacks of ventilation itself. Moving air from place to place and forcing it through a fine filter requires energy, and fresh air from outdoors is often at the wrong temperature, requiring extra heating or cooling. Meanwhile, the constant air movement can also chill the air to the point of discomfort, requiring indoor heating. This has obvious impacts on the costs and environmental footprint of buildings.&lt;/p&gt;
    &lt;p&gt;Doubling the ventilation rate from the US standard minimum in offices could add up to $40 per person per year to building running costs. This is lower than typically predicted, but still more than double what many building officials were prepared (before the pandemic) to pay. Energy recovery units help equalize the temperatures between intake and exhaust air, and can lower these figures by about 60 percent (or even more), depending on local climate and system type. However, in Europe, the hurdle is even higher: most buildings have no mechanical ventilation at all, so adapting existing ventilation systems is out of the question. For example, in the UK, an engineering review found that only five percent of residential buildings were likely to have air conditioning (a proxy for mechanical ventilation) by 2019. By contrast, in the US, about 90 percent of residences have air conditioning.&lt;/p&gt;
    &lt;p&gt;Ventilation comes with another tradeoff: noise. The more air you filter, the harder your fan has to work, the faster it spins, and the louder it gets. Free-standing filtration units can be powerful, quiet, or affordable ‚Äì but rarely all three at once. Purpose-built ventilation can be made quieter, but quiet movement of a large volume of air requires wide, leak-free ducting, which can increase installation complexity. Real world studies have shown that the noise and discomfort created by even relatively modest free-standing units mean that they are frequently turned down or switched off.&lt;/p&gt;
    &lt;p&gt;These drawbacks mean that ventilation-based methods lack the power to effectively block transmission of highly infectious pathogens or prevent pandemics. Without UV light, public buildings will require air handling systems akin to those in hospital isolation rooms to comply with current infection control guidelines. While circumstances vary from place to place, ventilation-based air quality measures quickly become impractical beyond around five air changes per hour, while upper-room UV light can achieve the equivalent of 35 air changes per hour, and far-UVC has the potential to reach well over 100.&lt;/p&gt;
    &lt;p&gt;Despite this, both ventilation and filtration need to be kept in mind as new buildings replace the old. If nothing else, ventilation design is an important factor in determining the effectiveness of UV systems; how well air is mixed and how quickly it moves affects the degree to which pathogens are exposed to UV light. These interactions have a positive or negative effect on pathogen removal. We flush and filter water, but we also disinfect it to eliminate remaining microbes. Air should be treated no differently.&lt;/p&gt;
    &lt;p&gt;Far-UVC is like an aerial disinfectant or bleach, except that it is harmless to humans at practical germicidal doses, and thus should not provoke resistance to its uptake. It does not alter pathogens in a way that allows resistance to emerge, a serious problem for antibiotics. Instead, it thoroughly damages microbial genomes at random, destroying bacteria and viruses alike, whether they are drug-resistant, vaccine-evasive, or indeed newly emerged.&lt;/p&gt;
    &lt;p&gt;The most widely used commercial far-UVC source is a krypton-chloride excimer lamp. ‚ÄòExcimer‚Äô is a contraction of ‚Äòexcited dimer‚Äô, a short-lived, high-energy molecule formed when the krypton and chlorine temporarily bond in an excited state as an electric current is passed through the gas mixture.&lt;/p&gt;
    &lt;p&gt;Krypton chloride lamps emit mostly 222-nanometer light, produced by the excited dimer decaying to its ground state and releasing the excess energy as a photon. Light at this wavelength is safe for human eyes and skin at doses that efficiently kill germs, but about 10‚Äì20 percent of the output consists of longer wavelengths with much lower maximum exposures. To use the lamps in occupied spaces, a filter with multiple layers of quartz and hafnium oxide is used to reflect unwanted wavelengths while allowing 222-nanometer light through.&lt;/p&gt;
    &lt;p&gt;Despite its potential, far-UVC has yet to achieve widespread use.&lt;/p&gt;
    &lt;p&gt;The very best krypton chloride emitters on the market are reliable, long-lasting, powerful, use little energy, and come with effective optical filters. But the state of the art is not representative. The krypton-chloride lamp industry is plagued by low-quality products with short lifespans that may not even produce any far-UVC light, and could even emit dangerous longer wavelengths. There is no product standard certification for UV lamps.&lt;/p&gt;
    &lt;p&gt;Top-tier excimer lamps, from reputable manufacturers, are expensive. This is partly due to their inherent complexity but largely from sluggish demand. The key reason is that, notwithstanding their successes in the 1930s, a certification vacuum denies buyers an authoritative seal of efficacy.&lt;/p&gt;
    &lt;p&gt;In part, this is because some of the studies establishing far-UVC‚Äôs efficacy aren‚Äôt as definitive as they need to be to really move the dial. One study on the impact of germicidal UV installations on tuberculosis transmission in homeless shelters was unsuccessful when the national rate of tuberculosis declined, which meant that the study ended up being too small to detect an effect even in principle.&lt;/p&gt;
    &lt;p&gt;In another study, germicidal UV was installed in hospitals to assess the impact of germicidal UV on flu transmission, but during the course of that study, the hospitals stopped routine flu testing. In a classroom study, not all classrooms had enough electrical sockets to plug in the portable air cleaners required for the study, some teachers turned off the air filters because they were too loud, and some schools didn‚Äôt install the germicidal UV devices they were supposed to because there were no legal protections for them for doing so.&lt;/p&gt;
    &lt;p&gt;But there are many other factors as well. Measuring infection control is challenging and seldom undertaken, particularly in public spaces. Epidemiological data is expensive and difficult to gather, and there is currently no way to measure the amount of viable, infectious pathogens in the air in real time. Office attendance can be tracked, but controlling for how users mix outside the office space is immensely difficult, and measuring the real-world effect of small-scale deployments in public areas is almost impossible. Studies aiming to cause deliberate disease transmission in controlled environments have failed to work in practice because they have been too small to generate enough infections.&lt;/p&gt;
    &lt;p&gt;Pathogen-free air and the research that goes into getting it are both, to some extent, public goods: the beneficiaries will mostly be people who haven‚Äôt paid for them. Even if a business reduces pathogens in its offices‚Äô air, the biggest upside goes to other people who share spaces with its employees. This could be commuters on the same train, people in shops, or parents whose children attend the same school.&lt;/p&gt;
    &lt;p&gt;Despite the lack of clear economic upside, we are already seeing some early adoption among respected institutions: Mount Sinai Hospital, for example, has far-UVC lamps installed in its Cohen Center for Recovery from Complex Chronic Illnesses. Ideally, others would emulate this example, creating a stronger basis for research.&lt;/p&gt;
    &lt;p&gt;If one part of the knot were cut, then one of the most promising disease-fighting technologies of our time could finally be employed en masse.&lt;/p&gt;
    &lt;p&gt;In the early 1900s, a public health official in Jersey City, John Leal, lost his father to illness likely caused by contaminated drinking water.&lt;/p&gt;
    &lt;p&gt;But Leal had an opportunity to prevent such a loss for others: he quietly directed the addition of chlorine to the drinking water supply in Jersey City, hiring engineer George Fuller to design and build a system for dripping a diluted bleach solution into Boonton Reservoir. He believed that this common household bleach agent could kill pathogens without harming people. He was right.&lt;/p&gt;
    &lt;p&gt;Beyond being a triumph of science, water sanitation led to a fundamental shift in public expectations. Once people saw that clean water was possible, they demanded it.&lt;/p&gt;
    &lt;p&gt;Twentieth-century water treatment programs transformed public health by virtually eliminating waterborne diseases. Ventilation, filtration and disinfection provide us with the opportunity to dramatically reduce the burden of airborne illnesses. Tuberculosis and coronaviruses would join typhoid and cholera as tragedies of the past, and seasonal flu and common colds would become rare rather than routine if clean air were as universal and expected as clean water.&lt;/p&gt;
    &lt;p&gt;Gavriel Kleinwaks is program director for indoor air quality at 1Day Sooner.&lt;/p&gt;
    &lt;p&gt;Karam Elabd is a researcher and writer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.worksinprogress.news/p/how-to-clean-the-air"/><published>2025-09-22T21:44:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45340133</id><title>Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents</title><updated>2025-09-23T11:32:37.313663+00:00</updated><content>&lt;doc fingerprint="f5bd3409d9b3a08a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 8 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2509.06917"/><published>2025-09-22T22:02:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45340192</id><title>Kevo app shutdown</title><updated>2025-09-23T11:32:36.988647+00:00</updated><content>&lt;doc fingerprint="922c7b946738aaf4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What does the November 14th, 2025 Kevo app shutdown mean to my Kevo door lock?&lt;/head&gt;
    &lt;p&gt;After more than a decade of service, as of November 14, 2025 the Kevo app and web portal will no longer be available.&lt;lb/&gt; ASSA ABLOY Americas Residential Inc. (‚ÄúASSA ABLOY‚Äù, ‚Äúwe‚Äù and ‚Äúus‚Äù), which is the successor to the company that previously marketed Kwikset Kevo, Weiser Kevo and Baldwin Evolved smart door locks, will cease supporting your Kevo lock‚Äôs remote functionality.&lt;/p&gt;
    &lt;p&gt;Locks Affected (All Generations): Kevo, Kevo Convert, Kevo Plus, Baldwin Evolved&lt;/p&gt;
    &lt;p&gt;Brands Affected: Kwikset, Weiser, Baldwin&lt;/p&gt;
    &lt;p&gt;Impact&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can no longer open/close or manage their door lock via the mobile app or web portal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not Impacted&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical Key, users will be able to unlock or lock the deadbolt with the physical key&lt;/item&gt;
      &lt;item&gt;Key FOB, users will be able to unlock or lock the deadbolt with the Key FOB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required User Action&lt;/p&gt;
    &lt;p&gt;Prepare in advance for the Kevo app shutdown. Ensure that you have the physical key or key fob to unlock and lock the door moving forward or you can redeem the unique promotional offer that existing Kevo users received via e-mail and replace the Kevo deadbolt entirely.&lt;/p&gt;
    &lt;p&gt;Replacement Door Lock Discount&lt;/p&gt;
    &lt;p&gt;To help make this transition easier, we‚Äôre offering our steepest discounts ever on trusted smart lock replacements, available exclusively to Kevo users.&lt;/p&gt;
    &lt;p&gt;(United States Only)&lt;/p&gt;
    &lt;p&gt;Offers will be fulfilled by our partners at Level, a fellow ASSA ABLOY brand. Your orders will be securely processed and shipped through Level‚Äôs website.&lt;/p&gt;
    &lt;p&gt;Available options include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;$80 off Kwikset Halo Keypad Wi-Fi Smart Lock&lt;/item&gt;
      &lt;item&gt;$130 off Level Lock+&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How to Redeem&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use the following link to visit&lt;/item&gt;
      &lt;item&gt;Choose the replacement deadbolt that is right for you&lt;/item&gt;
      &lt;item&gt;Enter your unique promotional code at checkout&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above offer is final, and no other offers will ensue with respect to the loss of remote functionality of your Kevo door lock. This offer will expire December 14, 2025.&lt;lb/&gt; (Canada Only)&lt;lb/&gt; Orders will be securely processed and shipped through Weiser‚Äôs customer service team.&lt;lb/&gt; Available options include:&lt;lb/&gt; - $89 (CDN) off Weiser Halo Keypad Wi-Fi Smart Lock&lt;lb/&gt; How to Redeem&lt;lb/&gt; 1. Call our Weiser customer service team: 1-800-501-9471&lt;lb/&gt; 2. Ask the service team member about claiming your Kevo replacement offer&lt;lb/&gt; 3. Provide your unique promo code&lt;lb/&gt; o Your unique promotional code was sent to your registered Kevo e-mail address, notifying you of the Kevo app shutdown&lt;lb/&gt; These offers are made in connection with the Kevo app shutdown; and available through December 14, 2025‚Äîwithout further extension.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kwikset.com/support/answers/what-does-the-kevo-app-shutdown-mean-to-my-kevo-door-lock"/><published>2025-09-22T22:07:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45341324</id><title>Fall Foliage Map 2025</title><updated>2025-09-23T11:32:36.828451+00:00</updated><content>&lt;doc fingerprint="14d19396b861a2a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Little to No Color&lt;/p&gt;
    &lt;p&gt;Low Color&lt;/p&gt;
    &lt;p&gt;Moderate Color&lt;/p&gt;
    &lt;p&gt;High Color&lt;/p&gt;
    &lt;p&gt;Peak Color&lt;/p&gt;
    &lt;p&gt;Past Peak Color&lt;/p&gt;
    &lt;p&gt;Color Report&lt;/p&gt;
    &lt;p&gt;Peak Timing&lt;/p&gt;
    &lt;p&gt;Reports&lt;/p&gt;
    &lt;p&gt;Download&lt;/p&gt;
    &lt;p&gt;Coming Fall 2025&lt;/p&gt;
    &lt;p&gt;Powered by Esri&lt;/p&gt;
    &lt;p&gt;Fall is nearly upon us, and we're once again helping fall foliage enthusiasts across the country find the best fall color! Use our map to explore the estimated timing of fall foliage throughout the United States, allowing you to plan trips with confidence this year. Check back regularly for updates based on the latest reports gathered from hundreds of sources throughout the country.&lt;/p&gt;
    &lt;p&gt;Two primary factors control the timing of fall foliage: daylight and temperature. This means that the further north and the higher in elevation a tree is, the earlier it will reveal it's colorful canopy. Photosynthesis grinds to a halt when the days grow short in the fall, and leaves no longer have a need for their excess stores of chlorophyll.&lt;/p&gt;
    &lt;p&gt;Over the course of a month or two, the concentrations of chlorophyll diminish, allowing less concentrated chemicals such as anthocyanin and carotenoids to dominate, turning the leaf red, yellow, or orange. The rate at which this change occurs varies amongst tree species, so it can be difficult to pinpoint a single peak in fall foliage.&lt;/p&gt;
    &lt;p&gt;Nevertheless, when the vast majority of trees in a particular area have full canopies of autumnal color, peak has arrived. Some areas, partiularly in the Northeast, experience vibrant red peaks due to an abundance of maple trees, while others experience a mixture of all of fall's colors. Different trees display different colors, giving each region its own unique peak.&lt;/p&gt;
    &lt;p&gt;For most of the United States, peak fall color arrives in the month of October. This is when wide swaths of the Northeast, Midwest, and Western states are aglow with bright fall foliage, and more than 80% of travelers make their fall foliage trips. Some less-populated regions will peak in September (August in northern Alaska), while the southernmost states hold off until mid-November.&lt;/p&gt;
    &lt;p&gt;The most popular fall foliage displays are found in New England, where approximately ten million people travel each year in hopes of photographing or simply walking through fall's splendor. Northern Vermont, New Hampshire, and northwestern Maine experience peak in early October, while much of New York, Massachusetts, and Pennsylvania have to wait until later in the month.&lt;/p&gt;
    &lt;p&gt;Out west, golden Aspens peak in sweeping displays in late September and early October, just prior to the invasive chill of winter. Non-desert, lower elevations in the Northwest are further delayed into late October/early November; however, the wait is well worth it.&lt;/p&gt;
    &lt;p&gt;If you've ever traveled in search of fall foliage before, you likely know how tricky it can be to be in the right place at the right time. The timing of peak color varies signficantly season-to-season, meaning what worked one year might not work the next! The best fall trips take careful planning, a lot of patience, and a reliable fall foliage map.&lt;/p&gt;
    &lt;p&gt;It's helpful to establish a baseline for when leaves normally change. Maps, like the one in the above section, can help you identify roughly when in the season you should be planning your trip. From there, you should consult a real-time fall foliage map like ours to see if fall foliage is on-time or running early/late due to ongoing weather conditions.&lt;/p&gt;
    &lt;p&gt;If at all possible, don't solidify your plans until you're two weeks out from peak fall foliage. This allows you to be flexible should extreme weather rear its head and disrupt the normal progression of fall foliage. Should that not be an option for you, do your planning in early September when fall foliage experts can give you an idea of whether or not fall color is on-time this year.&lt;/p&gt;
    &lt;p&gt;You'll want to make the most of your time in fall's splendor, so be sure to pick out a few beautiful hikes or drives on which you can truly be emersed in autumnal glory. If you're looking to beat the crowds, consider going to popular locations very early in the morning, before the majority of people arrive. Sunrise bathes fall foliage in golden hues, making early morning one of the best times to venture out!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.explorefall.com/fall-foliage-map"/><published>2025-09-23T00:14:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45341683</id><title>X server implementation for SIXEL-featured terminals (2010-2014)</title><updated>2025-09-23T11:32:36.056848+00:00</updated><content>&lt;doc fingerprint="10de925be4457d82"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 5&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h1"&gt;saitoha/xserver-SIXEL&lt;/head&gt;
    &lt;head rend="h2"&gt;Folders and files&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit message&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit date&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Repository files navigation&lt;/head&gt;
    &lt;quote&gt;X Server The X server accepts requests from client applications to create windows, which are (normally rectangular) "virtual screens" that the client program can draw into. Windows are then composed on the actual screen by the X server (or by a separate composite manager) as directed by the window manager, which usually communicates with the user via graphical controls such as buttons and draggable titlebars and borders. For a comprehensive overview of X Server and X Window System, consult the following article: http://en.wikipedia.org/wiki/X_server All questions regarding this software should be directed at the Xorg mailing list: http://lists.freedesktop.org/mailman/listinfo/xorg Please submit bug reports to the Xorg bugzilla: https://bugs.freedesktop.org/enter_bug.cgi?product=xorg The master development code repository can be found at: git://anongit.freedesktop.org/git/xorg/xserver http://cgit.freedesktop.org/xorg/xserver For patch submission instructions, see: http://www.x.org/wiki/Development/Documentation/SubmittingPatches For more information on the git code manager, see: http://wiki.x.org/wiki/GitPage&lt;/quote&gt;
    &lt;head rend="h2"&gt;About&lt;/head&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources&lt;/head&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h3"&gt;Stars&lt;/head&gt;
    &lt;head rend="h3"&gt;Watchers&lt;/head&gt;
    &lt;head rend="h3"&gt;Forks&lt;/head&gt;
    &lt;head rend="h2"&gt;Releases&lt;/head&gt;
    &lt;p&gt;No releases published&lt;/p&gt;
    &lt;head rend="h2"&gt;Packages 0&lt;/head&gt;
    &lt;p&gt; No packages published &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/saitoha/xserver-SIXEL"/><published>2025-09-23T01:07:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342364</id><title>Nine Things I Learned in Ninety Years</title><updated>2025-09-23T11:32:35.820885+00:00</updated><content/><link href="http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf"/><published>2025-09-23T03:03:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342759</id><title>Gamebooks and graph theory (2019)</title><updated>2025-09-23T11:32:35.086283+00:00</updated><content>&lt;doc fingerprint="b28e5a636db0b3bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gamebooks and graph theory&lt;/head&gt;
    &lt;p&gt;Posted on 27 October 2019 in data-science&lt;/p&gt;
    &lt;p&gt;A game book is, contrary to the usual books, a book you don't read pages sequentially. These books are read interactively. You are offered a choice after a paragraph: go to the right turn to section 7, go to the left turn to 138. That's it. Depending on the series, you might have additional rules: fight, magic, psi power, etc.&lt;/p&gt;
    &lt;p&gt;During the winter holidays, I thought a bit more about these books. They could be encoded as directed graph networks. Therefore I could probably apply a bunch of network algorithms to them to extract interesting information such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the shortest path to an instant death,&lt;/item&gt;
      &lt;item&gt;the path with the most fights,&lt;/item&gt;
      &lt;item&gt;etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose for my analysis the Lone Wolf series because, luckily for me, some people have put all the books in an electronic format and it's legal. It's a story about Lone Wolf (unexpected I know) the last of his kind, a caste of warrior monks.&lt;/p&gt;
    &lt;p&gt;NB: The Dawn of the Darklords was excluded from the analysis as it was not officially released as a gamebook. It was included in the Magnamund companion.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Masters of Darkness has the most action packed with a possible solution path including 65 fights;&lt;/item&gt;
      &lt;item&gt;The shortest path to death is The Kingdoms of Terror with only a 5 section path;&lt;/item&gt;
      &lt;item&gt;The Caverns of Kalte is the most deadly adventure with 19 instant death sections;&lt;/item&gt;
      &lt;item&gt;The shortest adventure is Flight from the Dark with a solution path only 27 sections long;&lt;/item&gt;
      &lt;item&gt;The longest adventure which can be done is The Shadow on the Sand with a touristic path of 224 sections, more than half of the sections (400).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Here is a summary for the 28 books I've analyzed. For now Lonewolf comprises 4 series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kai series: 1 to 5&lt;/item&gt;
      &lt;item&gt;Magnakai series: 6 to 12&lt;/item&gt;
      &lt;item&gt;Grand Master series: 13 to 20&lt;/item&gt;
      &lt;item&gt;New Order series: 21 to 32 (but now only 28 in project aon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The values reported below are the average value for each category. Something interesting we can see if that from the 3rd series, there are no more cycles and the shortest path has increased on average 50% compared to the 1st and 2nd series. Also, the shortest path to death has tripled and the number of insta-death was halved.&lt;/p&gt;
    &lt;p&gt;Over time, the books might have become more focused on adventure and story but also less punishing. Having only read the first couple of books, I can't comment on this but if somebody has an opinion on this, I would be happy to hear about it and maybe update the post with your comments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;series&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path to death&lt;/cell&gt;
        &lt;cell role="head"&gt;path with the most fights&lt;/cell&gt;
        &lt;cell role="head"&gt;# of fight&lt;/cell&gt;
        &lt;cell role="head"&gt;# of luck&lt;/cell&gt;
        &lt;cell role="head"&gt;# of death&lt;/cell&gt;
        &lt;cell role="head"&gt;# of cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;longest path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;1-kai&lt;/cell&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;153&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2-magnakai&lt;/cell&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;171&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;3-grand-master&lt;/cell&gt;
        &lt;cell&gt;95&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;163&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4-new-order&lt;/cell&gt;
        &lt;cell&gt;97&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;156&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;The technical bits&lt;/head&gt;
    &lt;head rend="h2"&gt;The preparation: Turn to 1&lt;/head&gt;
    &lt;p&gt;"Turn to" are the mythic words in these game books. It's also how we will divide the different sections of text, by using regexp. There are 5 types of section and their assigned color:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;normal: you can move forward to another section (white),&lt;/item&gt;
      &lt;item&gt;luck: you are asked to test your luck and you can move forward to another section (green),&lt;/item&gt;
      &lt;item&gt;fight: you are asked to fight some monster(s) and you can move forward to another section (yellow),&lt;/item&gt;
      &lt;item&gt;death: you chose badly and you got yourself killed, you have to restart from the section 1 (red),&lt;/item&gt;
      &lt;item&gt;start/end: first section and last section (blue).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once the sections are defined, we have to create the directed graph. To do so, I used two python libraries:&lt;/p&gt;
    &lt;head rend="h2"&gt;Extracting the interesting information: Test your Luck&lt;/head&gt;
    &lt;p&gt;I mainly used &lt;code&gt;networkx&lt;/code&gt; for the graph network analysis. It's a straightforward library and the documentation is good.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do you need a DAGger?&lt;/head&gt;
    &lt;p&gt;Typically a Lonewolf adventure is the equivalent of a Directed (A)Cyclic Graph:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directed: Lonewolf, your character, goes from the section 1 to hopefully the latest section which is depending on the book the section 300, 350 or 400, without the possibility to come back to the previous section;&lt;/item&gt;
      &lt;item&gt;Acyclic: This is not totally true for the Lonewolf series as 7 books contain cycles, a "circular" path between two nodes which a node is repeated twice. Some algorithms like the shortest path or longest path require a DAG and we need to remove the cycles before running them.&lt;/item&gt;
      &lt;item&gt;Graph: The sections are the nodes of the graph and the vertices the choices for each section.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disconnected graphs&lt;/head&gt;
    &lt;p&gt;In several books, graphs are disconnected. It means you can't go from the section 1 to the end section (300 or 350). This indicates usually that there is an enigma or puzzle asking to add numbers discovered along the adventure and reach the section given by the number. The only way to process these graphs is to check the text notes and add the missing edges manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cycle removal&lt;/head&gt;
    &lt;p&gt;The cycle removal is an interesting problem as it is one of the first problem to have been shown as NP-complete (NP stands for Non deterministic Polynomial time). This means that there is no known way to find a solution to solve that problem quickly and the time to find a solution grows as the size of the input grows. Nonetheless we are lucky because the data from a gamebook is usually quite small (300 to 400 nodes and 400 to 600 edges)!&lt;/p&gt;
    &lt;p&gt;The idea behind the cycle removal is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;do a DFS search,&lt;/item&gt;
      &lt;item&gt;look at the nodes and their children,&lt;/item&gt;
      &lt;item&gt;if one or more children have been already visited, remove that edge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I was curious about what could be done with a graph analysis of such textual / interactive games. Besides applying basic algorithms to test if the gamebook is playable, I didn't find much insights from it. I would like to see if there are any correlations between the features I chose and the popularity of the gamebooks. That being said, it was a cool project to do. I brushed up on the graph theory which I never really used outside of university.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future works&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apply the same methodology to Figthing Fantasy gamebooks&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://notes.atomutek.org/gamebooks-and-graph-theory.html"/><published>2025-09-23T04:10:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45342943</id><title>Zoxide: A Better CD Command</title><updated>2025-09-23T11:32:34.705608+00:00</updated><content>&lt;doc fingerprint="56bb8c669bcd38dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Special thanks to:&lt;/p&gt;
    &lt;p&gt;zoxide is a smarter cd command, inspired by z and autojump.&lt;/p&gt;
    &lt;p&gt;It remembers which directories you use most frequently, so you can "jump" to them in just a few keystrokes.&lt;lb/&gt; zoxide works on all major shells.&lt;/p&gt;
    &lt;p&gt;Getting started ‚Ä¢ Installation ‚Ä¢ Configuration ‚Ä¢ Integrations&lt;/p&gt;
    &lt;code&gt;z foo              # cd into highest ranked directory matching foo
z foo bar          # cd into highest ranked directory matching foo and bar
z foo /            # cd into a subdirectory starting with foo

z ~/foo            # z also works like a regular cd command
z foo/             # cd into relative path
z ..               # cd one level up
z -                # cd into previous directory

zi foo             # cd with interactive selection (using fzf)

z foo&amp;lt;SPACE&amp;gt;&amp;lt;TAB&amp;gt;  # show interactive completions (zoxide v0.8.0+, bash 4.4+/fish/zsh only)&lt;/code&gt;
    &lt;p&gt;Read more about the matching algorithm here.&lt;/p&gt;
    &lt;p&gt;zoxide can be installed in 4 easy steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Install binary&lt;/p&gt;&lt;p&gt;zoxide runs on most major platforms. If your platform isn't listed below, please open an issue.&lt;/p&gt;&lt;head&gt;Linux / WSL&lt;/head&gt;&lt;p&gt;The recommended way to install zoxide is via the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;p&gt;Or, you can use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;guix&lt;/td&gt;&lt;code&gt;guix install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;Linuxbrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;td&gt;AlmaLinux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Alpine Linux 3.13+&lt;/td&gt;&lt;td&gt;Alpine Linux Packages&lt;/td&gt;&lt;code&gt;apk add zoxide&lt;/code&gt;&lt;td&gt;Arch Linux&lt;/td&gt;&lt;td&gt;Arch Linux Extra&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;CentOS Stream&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Debian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Debian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Devuan 4.0+&lt;/td&gt;&lt;td&gt;Devuan Packages&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Exherbo Linux&lt;/td&gt;&lt;td&gt;Exherbo packages&lt;/td&gt;&lt;code&gt;cave resolve -x repository/rust&lt;/code&gt;&lt;code&gt;cave resolve -x zoxide&lt;/code&gt;&lt;td&gt;Fedora 32+&lt;/td&gt;&lt;td&gt;Fedora Packages&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Gentoo&lt;/td&gt;&lt;td&gt;Gentoo Packages&lt;/td&gt;&lt;code&gt;emerge app-shells/zoxide&lt;/code&gt;&lt;td&gt;Linux Mint&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Manjaro&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;openSUSE Tumbleweed&lt;/td&gt;&lt;td&gt;openSUSE Factory&lt;/td&gt;&lt;code&gt;zypper install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Parrot OS&lt;/del&gt;1&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Raspbian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Raspbian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;RHEL 8+&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Rhino Linux&lt;/td&gt;&lt;td&gt;Pacstall Packages&lt;/td&gt;&lt;code&gt;pacstall -I zoxide-deb&lt;/code&gt;&lt;td&gt;Rocky Linux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Slackware 15.0+&lt;/td&gt;&lt;td&gt;SlackBuilds&lt;/td&gt;&lt;td&gt;Instructions&lt;/td&gt;&lt;td&gt;Solus&lt;/td&gt;&lt;td&gt;Solus Packages&lt;/td&gt;&lt;code&gt;eopkg install zoxide&lt;/code&gt;&lt;td&gt;Ubuntu&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Void Linux&lt;/td&gt;&lt;td&gt;Void Linux Packages&lt;/td&gt;&lt;code&gt;xbps-install -S zoxide&lt;/code&gt;&lt;head&gt;macOS&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Homebrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;MacPorts&lt;/td&gt;&lt;code&gt;port install zoxide&lt;/code&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;Windows&lt;/head&gt;&lt;p&gt;zoxide works with PowerShell, as well as shells running in Cygwin, Git Bash, and MSYS2.&lt;/p&gt;&lt;p&gt;The recommended way to install zoxide is via&lt;/p&gt;&lt;code&gt;winget&lt;/code&gt;:&lt;quote&gt;winget install ajeetdsouza.zoxide&lt;/quote&gt;&lt;p&gt;Or, you can use an alternative package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Chocolatey&lt;/td&gt;&lt;code&gt;choco install zoxide&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Scoop&lt;/td&gt;&lt;code&gt;scoop install zoxide&lt;/code&gt;&lt;p&gt;If you're using Cygwin, Git Bash, or MSYS2, you can also use the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;BSD&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;DragonFly BSD&lt;/td&gt;&lt;td&gt;DPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;FreeBSD&lt;/td&gt;&lt;td&gt;FreshPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;NetBSD&lt;/td&gt;&lt;td&gt;pkgsrc&lt;/td&gt;&lt;code&gt;pkgin install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;head&gt;Android&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Termux&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Setup zoxide on your shell&lt;/p&gt;&lt;p&gt;To start using zoxide, add it to your shell.&lt;/p&gt;&lt;head&gt;Bash&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.bashrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init bash)"&lt;/quote&gt;&lt;head&gt;Elvish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.elvish/rc.elv&lt;/code&gt;):&lt;quote&gt;eval (zoxide init elvish | slurp)&lt;/quote&gt;&lt;p&gt;Note zoxide only supports elvish v0.18.0 and above.&lt;/p&gt;&lt;head&gt;Fish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.config/fish/config.fish&lt;/code&gt;):&lt;quote&gt;zoxide init fish | source&lt;/quote&gt;&lt;head&gt;Nushell&lt;/head&gt;&lt;p&gt;Add this to the end of your env file (find it by running&lt;/p&gt;&lt;code&gt;$nu.env-path&lt;/code&gt;in Nushell):&lt;quote&gt;zoxide init nushell | save -f ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Now, add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;$nu.config-path&lt;/code&gt;in Nushell):&lt;quote&gt;source ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Note zoxide only supports Nushell v0.89.0+.&lt;/p&gt;&lt;head&gt;PowerShell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;echo $profile&lt;/code&gt;in PowerShell):&lt;quote&gt;Invoke-Expression (&amp;amp; { (zoxide init powershell | Out-String) })&lt;/quote&gt;&lt;head&gt;Tcsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.tcshrc&lt;/code&gt;):&lt;quote&gt;zoxide init tcsh &amp;gt; ~/.zoxide.tcsh source ~/.zoxide.tcsh&lt;/quote&gt;&lt;head&gt;Xonsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.xonshrc&lt;/code&gt;):&lt;quote&gt;execx($(zoxide init xonsh), 'exec', __xonsh__.ctx, filename='zoxide')&lt;/quote&gt;&lt;head&gt;Zsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init zsh)"&lt;/quote&gt;&lt;p&gt;For completions to work, the above line must be added after&lt;/p&gt;&lt;code&gt;compinit&lt;/code&gt;is called. You may have to rebuild your completions cache by running&lt;code&gt;rm ~/.zcompdump*; compinit&lt;/code&gt;.&lt;head&gt;Any POSIX shell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file:&lt;/p&gt;&lt;quote&gt;eval "$(zoxide init posix --hook prompt)"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install fzf (optional)&lt;/p&gt;
        &lt;p&gt;fzf is a command-line fuzzy finder, used by zoxide for completions / interactive selection. It can be installed from here.&lt;/p&gt;
        &lt;p&gt;Note The minimum supported fzf version is v0.51.0.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Import your data (optional)&lt;/p&gt;&lt;p&gt;If you currently use any of these plugins, you may want to import your data into zoxide:&lt;/p&gt;&lt;head&gt;autojump&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=autojump "/path/to/autojump/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/autojump/autojump.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/home/alice/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%APPDATA%\autojump\autojump.txt&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Roaming\autojump\autojump.txt&lt;/code&gt;&lt;head&gt;fasd, z, z.lua, zsh-z&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=z "path/to/z/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;Plugin&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;td&gt;fasd&lt;/td&gt;&lt;code&gt;$_FASD_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.fasd&lt;/code&gt;&lt;td&gt;z (bash/zsh)&lt;/td&gt;&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;td&gt;z (fish)&lt;/td&gt;&lt;code&gt;$Z_DATA&lt;/code&gt;or&lt;code&gt;$XDG_DATA_HOME/z/data&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/z/data&lt;/code&gt;&lt;td&gt;z.lua (bash/zsh)&lt;/td&gt;&lt;code&gt;$_ZL_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.zlua&lt;/code&gt;&lt;td&gt;z.lua (fish)&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$_ZL_DATA&lt;/code&gt;&lt;td&gt;zsh-z&lt;/td&gt;&lt;code&gt;$ZSHZ_DATA&lt;/code&gt;or&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;head&gt;ZLocation&lt;/head&gt;&lt;p&gt;Run this command in PowerShell:&lt;/p&gt;&lt;quote&gt;$db = New-TemporaryFile (Get-ZLocation).GetEnumerator() | ForEach-Object { Write-Output ($_.Name+'|'+$_.Value+'|0') } | Out-File $db zoxide import --from=z $db&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When calling &lt;code&gt;zoxide init&lt;/code&gt;, the following flags are available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;--cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Changes the prefix of the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd j&lt;/code&gt;would change the commands to (&lt;code&gt;j&lt;/code&gt;,&lt;code&gt;ji&lt;/code&gt;).&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd cd&lt;/code&gt;would replace the&lt;code&gt;cd&lt;/code&gt;command.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Changes the prefix of the &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--hook &amp;lt;HOOK&amp;gt;&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;Changes how often zoxide increments a directory's score:&lt;/p&gt;&lt;th&gt;Hook&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;code&gt;none&lt;/code&gt;&lt;td&gt;Never&lt;/td&gt;&lt;code&gt;prompt&lt;/code&gt;&lt;td&gt;At every shell prompt&lt;/td&gt;&lt;code&gt;pwd&lt;/code&gt;(default)&lt;td&gt;Whenever the directory is changed&lt;/td&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--no-cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents zoxide from defining the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;These functions will still be available in your shell as &lt;code&gt;__zoxide_z&lt;/code&gt;and&lt;code&gt;__zoxide_zi&lt;/code&gt;, should you choose to redefine them.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Prevents zoxide from defining the &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environment variables2 can be used for configuration. They must be set before &lt;code&gt;zoxide init&lt;/code&gt; is called.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_DATA_DIR&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Specifies the directory in which the database is stored.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;The default value varies across OSes:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux / BSD&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME&lt;/code&gt;or&lt;code&gt;$HOME/.local/share&lt;/code&gt;&lt;code&gt;/home/alice/.local/share&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/Application Support&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/Application Support&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%LOCALAPPDATA%&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Local&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_ECHO&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will print the matched directory before navigating to it.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_EXCLUDE_DIRS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Excludes the specified directories from the database.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;This is provided as a list of globs, separated by OS-specific characters:&lt;/p&gt;
            &lt;th&gt;OS&lt;/th&gt;
            &lt;th&gt;Separator&lt;/th&gt;
            &lt;th&gt;Example&lt;/th&gt;
            &lt;td&gt;Linux / macOS / BSD&lt;/td&gt;
            &lt;code&gt;:&lt;/code&gt;
            &lt;code&gt;$HOME:$HOME/private/*&lt;/code&gt;
            &lt;td&gt;Windows&lt;/td&gt;
            &lt;code&gt;;&lt;/code&gt;
            &lt;code&gt;$HOME;$HOME/private/*&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;By default, this is set to&lt;/p&gt;&lt;code&gt;"$HOME"&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_FZF_OPTS&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_MAXAGE&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Configures the aging algorithm, which limits the maximum number of entries in the database.&lt;/item&gt;
          &lt;item&gt;By default, this is set to 10000.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_RESOLVE_SYMLINKS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will resolve symlinks before adding directories to the database.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Application&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Plugin&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;aerc&lt;/cell&gt;
        &lt;cell&gt;Email client&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;alfred&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;alfred-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;clink&lt;/cell&gt;
        &lt;cell&gt;Improved cmd.exe for Windows&lt;/cell&gt;
        &lt;cell&gt;clink-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;emacs&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.el&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;felix&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;joshuto&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;lf&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;See the wiki&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nnn&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;nnn-autojump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ranger&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;ranger-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;raycast&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;raycast-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rfm&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;sesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;telescope.nvim&lt;/cell&gt;
        &lt;cell&gt;Fuzzy finder for Neovim&lt;/cell&gt;
        &lt;cell&gt;telescope-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-session-wizard&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-sessionx&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;vim / neovim&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.vim&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xplr&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;zoxide.xplr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xxh&lt;/cell&gt;
        &lt;cell&gt;Transports shell configuration over SSH&lt;/cell&gt;
        &lt;cell&gt;xxh-plugin-prerun-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;yazi&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zabb&lt;/cell&gt;
        &lt;cell&gt;Finds the shortest possible query for a path&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;zellij&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zsh-autocomplete&lt;/cell&gt;
        &lt;cell&gt;Realtime completions for zsh&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/ajeetdsouza/zoxide"/><published>2025-09-23T04:48:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45343108</id><title>Delete FROM users WHERE location = 'Iran';</title><updated>2025-09-23T11:32:34.004401+00:00</updated><content>&lt;doc fingerprint="ee1198109d4361d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi! I am an Iranian Software Engineer, and in this torn paper note, I want to talk about some funny moments I had online related to the fact that I was spawned in this specific region of the world: Iran.&lt;/p&gt;
    &lt;p&gt;Back when I was a student, I got access to the Microsoft Imagine, and as a result, I got access to the Microsoft Store as a developer. This inspired me write one of my open-source projects called EyesGuard and publish it on Microsoft Store. However, one day, somebody told me that they can no longer find EyesGuard on the store.&lt;/p&gt;
    &lt;p&gt;I came to the realization that Microsoft deleted my app, my developer account, and all those comments on my app supporting me and suggesting ideas on how to improve the program. I tried to contact the support and email whoever I could, but I was ghosted. Nobody ever explained to me why, but I assume it's because of the sanctions.&lt;/p&gt;
    &lt;p&gt;Notion is a great product, and it was the primary tool I used to manage my personal notes. Not until they suddenly decided to wipe out every data related to the users residing in Iran. Hopefully, they actually responded to my support message:&lt;/p&gt;
    &lt;p&gt;It was because of sanctions. However, they told me that they will not restore the data, even if I leave Iran someday:&lt;/p&gt;
    &lt;p&gt;That said, I am very happy with my own self-hosted Siyuan now.&lt;/p&gt;
    &lt;p&gt;I read hackernews on a daily basis and I visit lots of different websites regularly. I am almost always on my VPN as I am internally firewalled by the government and externally shooed because of the sanctions, so I am probably missing some of these heart-warming messages:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My VPN turned off, and opening https://www.grepular.com showed me this message. I actually do not blame the people who do this. I think there is a fundamental misconception that people think because "Islamic Republic" has the word "Republic" in it, it must be a government of people in charge. That's not the case. I have yet to see anyone who actually supports Russian aggression in my real life in Iran. Funny enough, Iran's history is full of backstabs by the Russian government.&lt;/p&gt;
    &lt;p&gt;I tried contacting the author by sending this email:&lt;/p&gt;
    &lt;code&gt;Hi Mark,

I hope this message finds you well.

While browsing HackerNews, I came across your website but was greeted with this message:

&amp;gt; Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.

I wanted to clarify that the decision to support Russia does not represent the Iranian people. That "your decision" refers to the regime, a theocratic minority that rules Iran without democratic legitimacy. The people of Iran have long protested and revolted against this regime, but unfortunately, they face brutal suppression while unarmed.

In my experience, most Iranians around me, including myself, stand firmly with Ukraine and against Russian aggression.

I‚Äôm not asking you to reconsider the IP restriction, you have your reasons and I respect that. I simply wanted to share this perspective and express my solidarity with Ukraine.

Slava Ukraini!

Best regards,
Avestura
&lt;/code&gt;
    &lt;p&gt;I got no replies from them, and I actually didn't expect one.&lt;/p&gt;
    &lt;p&gt;I woke up to the news that GitHub has removed the access of Iranians to their private repositories. Well, that was not good. I tried to launch my own self-hosted instance of Gitea to reduce the damage. However, later, GitHub announced that github is now available in Iran by securing a license from the US government, and we're now good. You see? The weather is good, the birds are singing, GitHub is free again. Fantastic!&lt;/p&gt;
    &lt;p&gt;Similarly, GitLab banned every account that once accessed from an Iranian IP, however, to this day, they never lifted the ban, even on public repositories. I guess they couldn't secure a license from the US government, or they simply never cared. Good luck to them in either case, though. GitLab is an amazing software. One can always self-host it.&lt;/p&gt;
    &lt;p&gt;The list goes on, and almost all of the services you probabelly heard of is banned here: Cloud platforms (AWS, GCP, Azure, ...), Educational platforms (coursera, udemy, etc), Payment software (stripe, paypal, ...).&lt;/p&gt;
    &lt;p&gt;I don't think any of these companies have bad intentions towards any group of people. They are a business after all. They don't hate their customers; they are just playing the game, and the game has such rules. But if someday some law or government forces me to prevent my services from a group, I'll think twice before writing those &lt;code&gt;if&lt;/code&gt; statements. I'll try to have more empathy. People behind those screens are more important than just some rows in my tables.&lt;/p&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;In this text, I am NOT asking for the removal of the sanctions targeted at the Islamic Republic of Iran. I am merely remembering some moments on top of my head. For the record, I do not support the actions of the Islamic Republic, and on the contrary, I am in favor of the movements that release the people from such a mafia-like cult ruling a country with thousands of years of history. The actions of the group in charge of Iran are not defensible, and as a matter of fact, the people of Iran are the first layer of victims. Some examples are listed here. I especially feel it differently, as regime thugs put a gun to the throat of a dear person to me, and threatened to kill him if he showed up in protests.&lt;/p&gt;
    &lt;p&gt;By the way, did you know you could return &lt;code&gt;451 Unavailable For Legal Reasons&lt;/code&gt; instead of &lt;code&gt;403 Forbidden&lt;/code&gt; when you're going to ban me next time?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4"/><published>2025-09-23T05:30:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45343449</id><title>Altoids by the Fistful</title><updated>2025-09-23T11:32:33.558747+00:00</updated><content>&lt;doc fingerprint="67519077dbe58b9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Altoids by the Fistful&lt;/head&gt;&lt;p&gt;‚ÄúWh‚Äî what did you say?‚Äù&lt;/p&gt;&lt;p&gt;It‚Äôs close to six o‚Äôclock on a weekday afternoon and the bar is starting to get noisy with the after-work crowd. It‚Äôs entirely possible I misheard that last part.&lt;/p&gt;&lt;p&gt;‚ÄúAltoids! I find the spearmint works a little better overall, but recently I‚Äôve started switching flavors depending on the situation.‚Äù&lt;/p&gt;&lt;p&gt;I‚Äôve worked with James‚Äî‚ÄúJim‚Äù as everyone on the team knows him‚Äîfor a little over two years and I‚Äôm used to this dance now. He gets a kind of tunnel vision in his excitement about whatever shiny new thing has captured his attention. It‚Äôs usually pretty easy to shake him out of it.&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, the part before that.‚Äù&lt;/p&gt;&lt;p&gt;He looks at me for a moment, inquisitive, before pushing his beer aside. ‚ÄúHere, let me show you.‚Äù He reaches underneath the table and produces his beige-on-brown Timbuk2 messenger bag. There is a small wet spot left behind from his drink, and the bag plops right onto it. I watch as one of his stubby hands unbuckles the outermost pouch while the other one pulls out a small green and white tin. I am obviously intended to see this as clearly as possible, evidenced by the way he places it front and center between us.&lt;/p&gt;&lt;p&gt;‚ÄúRegular everyday Altoids, right? You take about four of them, maybe five.‚Äù He flips the lid open and traps the requisite number of small white mints between his fingertips, which he then pops into his mouth. ‚ÄúThis is the trick; you gotta half-chew it first.‚Äù At least two tiny shards fly in my direction as he speaks these words. It is like listening to a slow K-turn executed on a road covered in gravel and seashells. Three more slow and deliberate chomps, then his bite eases. ‚ÄúMmm.‚Äù The communication style switches to mime: an index finger raised in a ‚Äúone moment‚Äù gesture, followed by an exaggerated point downwards while unzipping the main pouch of the bag. It takes a few seconds of rooting around before the star of this particular show is found.&lt;/p&gt;&lt;p&gt;My eyes barely have enough time to resolve the object under the dismal light at this end of the bar before it‚Äôs in his mouth. He‚Äôs chewing the full concoction now‚Äîmouth closed, thank God. The crunching softens, then fades into the din from a nearby table of sales bros laughing at their sales bro anecdote. Jim is looking at me with a kind of confident smugness I haven‚Äôt seen since I bet my buddy at Guitar Center that he couldn‚Äôt spontaneously play ‚ÄúEverlong‚Äù from memory. A bet I lost, I might add.&lt;/p&gt;&lt;p&gt;There is a degree of intentional spectacle to this, I‚Äôd have to imagine. Each jaw movement is deliberate. Precise. He does not break eye contact with me, though I desperately want to break it with him. I can‚Äôt though. The absurdity of the scene is absolutely hypnotizing. One final swallow, a smack of his lips, then he opens his mouth wide like a child proving that they finished all their vegetables and have earned their dessert. ‚ÄúEasy peasy, no problem.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúThat was&amp;amp;mldr;‚Äù It‚Äôs like a significant piece of my brain has just completely locked up. I‚Äôm just saying words without thinking, filling the empty air.&lt;/p&gt;&lt;p&gt;‚ÄúA cat turd!‚Äù he proclaims, finishing my sentence.&lt;/p&gt;&lt;p&gt;A beat.&lt;/p&gt;&lt;p&gt;‚ÄúYou just ate a cat turd.‚Äù It‚Äôs all I can do in this moment to plainly restate the facts as I understand them, although the sense of alarm is definitely carrying in my voice.&lt;/p&gt;&lt;p&gt;‚ÄúYup, and it didn‚Äôt taste bad at all. The spearmint masks it completely. Watch, I‚Äôll do another one.‚Äù My eyes widen in dread as I shake my head weakly. I didn‚Äôt want to see him do that the first time; I sure as shit don‚Äôt want to see it again.&lt;/p&gt;&lt;p&gt;‚ÄúNo, that‚Äôs alright,‚Äù I balk.&lt;/p&gt;&lt;p&gt;There is an awkward reach across the bag as he grabs his glass, tips it toward me in a silent toast, then takes a long swill. Whether he admits it or not, there‚Äôs evidently something that needs to be washed down. He lets out a contented sigh as the almost-empty glass thumps back down on the table. I glance down at the chicken wings and carrot sticks I had been picking at. A minute ago, they were kinda bland‚Äîmerely okay by the standards of pub food. With the abrupt loss of my appetite, now they are destined for the dumpster out back.&lt;/p&gt;&lt;p&gt;He lifts the small tin of mints and gives it a little shake in front of my face. It sounds a lot more papery and a lot less metallic than I would‚Äôve guessed. ‚ÄúAltoids. I‚Äôm not exaggerating when I say these have completely changed the way I work.‚Äù I follow this little miracle box as they get tucked back into the bag, the buckles snapping shut to shield them from the lustful gaze of an angry world. He pauses and looks up at me again. ‚ÄúWould you like to try?‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, I don‚Äôt want your cat turds.‚Äù&lt;/p&gt;&lt;p&gt;I don‚Äôt want your cat turds. Why did I say it like that? I don‚Äôt want anybody‚Äôs cat turds!&lt;/p&gt;&lt;p&gt;&amp;amp;mldr;Right?&lt;/p&gt;&lt;p&gt;‚ÄúCompletely changed the way I work,‚Äù he repeats mechanically, sliding his bag onto the empty seat to his left. I‚Äôm finding it quite difficult to look at Jim, so I instead follow the motions of the bag until it is completely out of my view. How many more are in there?&lt;/p&gt;&lt;p&gt;‚ÄúI used to spend so much of my day on cat turds, psyching myself up, trying strategies that didn‚Äôt work, all the cleanup when I was finished. That‚Äôs all gone now. I can never go back to the old way.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúI just&amp;amp;mldr; I mean&amp;amp;mldr;‚Äù My brain has started working again, at least superficially, and it has generated so many questions that I‚Äôm having a hard time selecting which one to ask first. ‚ÄúHow long have you been eating cat turds?‚Äù A fine question for this moment, I suppose.&lt;/p&gt;&lt;p&gt;‚ÄúWhat do you mean? I‚Äôve always had to eat cat turds. Since I was a kid in school, on through college, in all my jobs&amp;amp;mldr; They keep giving me cat turds and I keep having to eat them, otherwise it starts to pile up and then things really get messy.‚Äù&lt;/p&gt;&lt;p&gt;His face turns slightly serious as he parses my expression, his head tilting in suspicion. ‚ÄúYou eat cat turds too, yeah?‚Äù I choose not to answer that question. He continues anyway. ‚ÄúSure. We all do. We have to, ya know?‚Äù&lt;/p&gt;&lt;p&gt;We all do.&lt;/p&gt;&lt;p&gt;Those words have been repeating in my head with the consistency of a drumline cadence. We all do.&lt;/p&gt;&lt;p&gt;‚ÄúWalk sign is on to cross Pawk Avenue. Walk sign is on to cross Pawk Avenue.‚Äù I‚Äôve heard this prerecorded voice, clearly belonging to the most disgruntled DOT Traffic Signals employee available at the time of this crosswalk‚Äôs construction, at least twice per workday for the last two years. It stirs up a half-remembered dream of a career spent shoveling dirt into a hole‚Äîsomething that feels more like the idea of ‚Äúhonest work‚Äù than what I get paid to do every day. I bet nobody on the construction crew spent an entire workday fighting around with brittle, poorly designed automation tooling like I did today.&lt;/p&gt;&lt;p&gt;I‚Äôm quickly but unintentionally refilling my conscious mind with the task I had gleefully abandoned when Jim invited me out to after-work drinks. Normally I‚Äôd be irritated to spend more of my waking life thinking about this stuff, but after what I witnessed at the bar I welcome any distraction at all.&lt;/p&gt;&lt;p&gt;‚ÄúOkay. So, usually we have a string. This is one of many values inside a mapping type, within a list of similar mappings.‚Äù I‚Äôm narrating to myself silently, imagining little bits of JSON syntax stamped on rectangles that are kind of stacked on top of each other like playing cards. ‚ÄúBut ever since the schema change in V3, sometimes the value is another mapping type that wraps the string we want&amp;amp;mldr;‚Äù I‚Äôm visualizing another square to the right of the existing one. This one is yellow, distinct from the light blue of all the others, and it never occurs to me to question why that is.&lt;/p&gt;&lt;p&gt;‚ÄúBut because this is actually YAML, and the value comes from a template call, both the string form and the mapping form need to be escaped and indented in a way that works in both cases.‚Äù I‚Äôm chewing on the problem in pretty much the same mindset I had during work, only now I‚Äôm walking across midtown instead of staring at a computer screen. ‚ÄúWe could just revert that change, keep the value as JSON like it used to be and insert it verbatim&amp;amp;mldr; but DevEx owns that part and I wouldn‚Äôt want to have to fight to get that PR approved.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúPiece of shit.‚Äù I speak that bit out loud without really intending to. I snap back into awareness of my surroundings and look around. Nobody was near enough to hear it. They probably wouldn‚Äôt have cared if they were.&lt;/p&gt;&lt;p&gt;It occurs to me that, whenever anybody asks me what I do for a living and I wave my hand and say ‚ÄúComputers,‚Äù this is what I‚Äôm trying to avoid needing to have to explain. None of these words are being used in a way that would mean anything to most people. If one were to take the time to carefully define them all and how they fit together semantically, they describe concepts so abstract and detached from any kind of tangible shared experience that you‚Äôd hit a second wall trying to explain that.&lt;/p&gt;&lt;p&gt;‚ÄúOh, but wait, we have the &lt;code&gt;nindent&lt;/code&gt; function. I could just count up the indentation level of the outer list and&amp;amp;mldr; Ah, hell, I forgot this template is transcluded into pod and deployment specs and the nesting levels would be different between the two.‚Äù I briefly try to think of which chucklefuck I could blame this design on, but truth be told I rubber-stamped enough questionable pull requests in my time here that a fair amount of this situation is a mess of my own damn making.&lt;/p&gt;&lt;p&gt;Huh. I really do wonder what I would sound like trying to explain this to somebody who had no experience in the industry. I suppose if I was very excited about it, I might come across like an energetic kid going on and on about all the different Pok√©mon they know about and all the special attacks and vulnerabilities. But without that spark of passion, and in its place a jaded voice tinged with frustration and contempt, I would probably just sound like a raving lunatic. These words don‚Äôt mean anything. I‚Äôm not describing something that actually exists. I‚Äôm playing the part of an observer in a universe of little floating boxes, becoming physically agitated about a superficial difference within the yellow one, and none of it is real.&lt;/p&gt;&lt;p&gt;I‚Äôm definitely not feeling the passion on this one. This code runs deep inside a build-deploy pipeline that I have no hope of ever running directly on the computer I‚Äôm using. So I write the code, push it to CI, wait for a bunch of stuff I‚Äôm not interested in to finish running, then get to watch my change fail to work for either the stupidest typo that I never should‚Äôve made in the first place, or due to some error that is so novel that even the search engines assume I must really be having some other much more popular error instead of the one I provided. It feels like I am performing surgery using a scalpel held by a boardwalk arcade claw machine, complete with the constant squawking and shitting of project management seagulls.&lt;/p&gt;&lt;p&gt;And even if I could concisely explain all of that to my hypothetical interlocutor, there‚Äôs the even higher-level question: Why? Why did we even make this change? What was so irredeemably wrong with the last two versions of this thing that we‚Äôre now doing it all again a third time? What exactly is the goal we‚Äôre trying to achieve here? I can‚Äôt really say. It‚Äôs a question I never asked, partly because I learned a long time ago that asking questions just causes friction. Just nod and shut up. Put a +1 on a sketchy PR and get it out of here. Don‚Äôt hold up the pipeline. Recover enough stamina to face down the next eldritch nightmare that slithers its way to the top of my Jira swimlane. ‚ÄúSounds great, thanks.‚Äù Thumbs-up. Grit my teeth through to the next direct deposit, convince myself it‚Äôs not so bad. Do it over and over until some ill-defined end condition is met. I‚Äôll know it when I see it. I hope.&lt;/p&gt;&lt;p&gt;I catch myself at the tail end of a sigh. I fake like I‚Äôm yawning to stretch my upper body for a second. Approximately every muscle in my back now aches.&lt;/p&gt;&lt;p&gt;There‚Äôs this very real sense that I don‚Äôt&amp;amp;mldr; I don‚Äôt want to solve this problem. There is no intellectual reward at the end of this journey. It‚Äôs not interesting to me. This isn‚Äôt something that needs to be fixed, because it‚Äôs not a situation that ever should‚Äôve been permitted to happen in the first place. This is just a bunch of contrived nonsense that I must work through because the broader situation dictates it. It doesn‚Äôt matter if the solution is good or elegant. It doesn‚Äôt matter if it barely works. It doesn‚Äôt matter if it causes another problem that I stub my toe on in three weeks. It‚Äôs just&amp;amp;mldr; what I have to do.&lt;/p&gt;&lt;p&gt;I stop in my tracks.&lt;/p&gt;&lt;p&gt;These kinds of problems are my cat turds.&lt;/p&gt;&lt;p&gt;Unlike Jim, though, I can‚Äôt just cram a bunch of breath mints into my face to make this go away.&lt;/p&gt;&lt;p&gt;The ‚Äúdown‚Äù escalator into the train station is out of service, and it has been this way all summer. A pair of orange plastic barricades block the landings at both ends. I walk down two flights of stairs alongside a half-dozen other commuters. Having concluded that the template problem simply isn‚Äôt worth thinking any further about, I‚Äôm back on the cat turds. I understand what Jim was talking about now. This has been happening for almost my entire life, even going back to my days in elementary school.&lt;/p&gt;&lt;p&gt;All of the homework assignments that were blindly graded against answer keys from the back of a Teacher Edition of the textbook: Cat turds. College admission essays where I profused a longing desire to attend the distinguished universities that my parents and guidance counselor told me I should set my ambitions toward: Cat turds. Probably hundreds of cover letters submitted alongside job applications throughout the years, skimmed by perhaps tens of internal recruiters and hiring managers: Cat turds.&lt;/p&gt;&lt;p&gt;The notion that it was a good idea to manipulate highly whitespace-sensitive YAML data with the Go &lt;code&gt;text/template&lt;/code&gt; package. CI workflows that take 75 minutes to reach the one step in the entire process that fails. Tools and interfaces that force-update and introduce breaking changes for seemingly no justifiable reason, removing or kneecapping features that were being relied on, with issue trackers guarded by thickheaded bots that dismissively auto-close feature requests that kindly ask for consideration for those use cases. Massively over-complicated software that tries to be everything to everybody, but in reality ends up being a gigantic lumbering pile of failure and frustration. Cat turds.&lt;/p&gt;&lt;p&gt;I used to love this stuff. I still do. Except&amp;amp;mldr; I don‚Äôt. Not lately, anyway. A long time ago, this was unquestionably what I wanted to do with my life. I would stay up late, pushing back my bedtime for a few more minutes with these glorious machines, hacking away on some little project. Then I‚Äôd get up early the following morning, excited to jump back into the project before my day out in the world began. I don‚Äôt even clearly remember what I was building toward, but I know it had basically zero utility or market potential. The point of doing the project was simply to do the project‚Äîto press through problems, to learn new things, and to end the day with more skills and experience than I started with.&lt;/p&gt;&lt;p&gt;At one point, I had the 7-bit ASCII table memorized. Just the decimal codes; I didn‚Äôt really understand the usefulness of the hexadecimal representations, and it never occurred to me that the hex values would work much better in mnemonics. I don‚Äôt know why I took the time to learn that. I never really used that knowledge in any real day-to-day work, and it began to fade from my mind as soon as I found some other pointless esoterica to wallpaper over it.&lt;/p&gt;&lt;p&gt;Look at me now, having to Google how to read a text file line-by-line in Python despite having done it a hundred times at this point. The knowledge is up there somewhere, I‚Äôm sure of it. I just can‚Äôt always think of the idiom in the heat of the moment. Just a little hint to jog the old brain, that‚Äôs all I need.&lt;/p&gt;&lt;p&gt;I often wonder what my Younger Self would think of me now, failing to remember a two-line snippet of code that you‚Äôd find in the first ten pages of any beginner‚Äôs guide to the language. He‚Äôd probably sneer and say I need to devote more time to studying. But I‚Äôm an adult with things to do; I can‚Äôt spend all my time just memorizing things just in case I might need the information someday. Oh, and by the way: Younger Self, if you were such a friggin‚Äô hotshot, why did it take you fifteen years to finally wrap your head around regular expressions? What‚Äôs that? Because they were hard? So you spent all your time memorizing easy and pointless trivia rather than tackling anything that was genuinely challenging? And then building up a whole air of superiority based on the number of discrete facts you could rattle off, rather than their practical utility? What, were you trying to become a contestant on Computer Jeopardy! or something?&lt;/p&gt;&lt;p&gt;No wonder Younger Self grew up to be kind of an asshole.&lt;/p&gt;&lt;p&gt;I mean, I didn‚Äôt try to be an asshole. It‚Äôs just that I tended to gauge my own self-worth relative to others based on the only social currency we could accurately compare: the amount of ‚Äústuff‚Äù we knew. Some people memorize car engine displacements, others carry in their noggins enough digits of pi to resolve the observable universe down to the width of a hydrogen atom. I had a litany of command-line switches that I never used for anything, HTML character entity names for writing systems I couldn‚Äôt comprehend, and tales of tweaking settings deep inside the Windows 98 Device Manager just so I could brag about having been in there in the first place. I also at one time sincerely believed that maybe if I taught myself to‚ÄîI‚Äôm picking one example out of many‚Äîdecode Code 39 barcodes in my head, it would somehow make me cool and desirable during otherwise awkward social functions. (I did get reasonably good at it. All it takes is memorizing a couple of three-digit sequences. Having a teenager‚Äôs near-field visual acuity certainly helped.)&lt;/p&gt;&lt;p&gt;Everybody else who didn‚Äôt know those little pieces of nothing? They were the lessers. They didn‚Äôt put in the time to grind for this knowledge. They had never scaled the peaks of Mount AltaVista, nor had they knelt in the temple of the MSDN Library for Visual Studio on a banged-up pair of CD-Rs. I knew things they did not, therefore I felt I was higher-and-mightier than they were. I and I alone suffered for this knowledge. This attitude manifested itself in one of two ways: In the first case, I would barge my way into situations where my involvement wasn‚Äôt needed or appreciated, thinking I could ‚Äúsave‚Äù others from the pain I once had to contend with. More often than not, though, I would simply mock people for not knowing things‚Äîusually inside my own head, but sometimes outwardly on mailing lists and message boards. There were times when I judged a person to have failed to put in the necessary amount of work, so therefore they did not deserve to rise anywhere near where I considered my own level to be. It didn‚Äôt matter if the subject was deeply technical or a disagreement on the precise phrasing of a Simpsons quote. Somebody got something wrong, and it was my job to rectify that.&lt;/p&gt;&lt;p&gt;I feel bad for the people who worked on teams where Younger Self was the senior engineer. I was full of ideals and convictions back then. ‚ÄúNo, we‚Äôre not doing that. We‚Äôre going to Do It Right instead.‚Äù I was full of piss and vinegar. ‚ÄúHere, give me that; I‚Äôll just do it myself.‚Äù I was full of shit.&lt;/p&gt;&lt;p&gt;I now realize that everything I lorded over other people‚Äîall the things I gatekept without consciously understanding that this was what I was doing‚ÄîI didn‚Äôt need to do that. It really didn‚Äôt help anything. For some number of people who interacted with me, I was the problem. I could‚Äôve been more tolerant or forgiving, I could‚Äôve said ‚Äúlet‚Äôs find out together,‚Äù I could‚Äôve let other people have the fun once in a while. I could have minded my own damn business and saved everybody the hassle.&lt;/p&gt;&lt;p&gt;There were people out there who must‚Äôve felt that I was their cat turds.&lt;/p&gt;&lt;p&gt;I‚Äôll never be able to track down and apologize to every person I treated that way. And why did I even build that fiefdom and protect it so jealously? Why was I so insecure? Why did I have to always be right and have a ready justification for why everybody else was wrong?&lt;/p&gt;&lt;p&gt;It was just me, alone in my tiny sandbox, safe and secure behind my towering fortress of cat turds.&lt;/p&gt;&lt;p&gt;My usual train, the one packed so full that some riders have to stand in the aisles until after the first or second stop, usually leaves at 5:50. Now about three hours later, one can sometimes get an entire car to themselves. I settle down in a window seat looking out at the desolate platform. Evidently there aren‚Äôt all that many people interested in traveling across the river at this hour on a Wednesday evening. It feels nice to sit, despite the fact that I‚Äôve probably sat for a cumulative ten hours‚Äîat least‚Äîover the course of this day.&lt;/p&gt;&lt;p&gt;As sometimes happens, another rider boards the train and enters what had up to this point been my personal rail car. He selects the aisle seat in the row directly in front of me. At least 110 other seats in this car, every single one of them empty, and his choice is to sit right here. Sigh. I could get up and move to another seat but I‚Äôm&amp;amp;mldr; exhausted. I‚Äôm here, I‚Äôm settled in, and above all I‚Äôm just completely out of ambition. I guess it‚Äôs fine as long as he doesn‚Äôt start playing music or TikTok videos without headphones.&lt;/p&gt;&lt;p&gt;A long blow from the locomotive horn, and the train begins to creep forward. Right on schedule. We‚Äôre in a tunnel deep below the city‚Äôs west side, and the view out the window is pitch black aside from the occasional glow from a mercury-vapor emergency light. On the wall beneath each of these lights, patches of graffiti framed by concrete pillars. I wouldn‚Äôt say I‚Äôve memorized them all by heart; I can‚Äôt even read the tags on the majority of them. But they are at least familiar, and I‚Äôve found some of them serve as convenient signposts along this portion of the trip. I‚Äôm not really paying attention to any of them tonight, instead I‚Äôm staring blankly at a little patch of window glass as the scene rolls past.&lt;/p&gt;&lt;p&gt;I refocus my eyes a bit and realize I‚Äôm looking at the reflection of a screen, or at least the top corner of one. I turn away from the window and find the source of the light. The man in front of me has opened his laptop‚Äîa chunky Dell Latitude or something very close to it‚Äîand perched it on a small lap desk fashioned from his leather bag. He opens a web browser and logs into a Microsoft account, one key at a time, hunt-and-peck style. It prompts him for his second factor and he shifts awkwardly in the seat to retrieve his phone. The login process succeeds and, after a few clicks and a fair bit of both of our finite lifetimes spent staring at loading spinners, opens what appears to be a Word document. I can‚Äôt read anything on his screen, which is more a testament to how wrecked my eyes have become than anything else, but I can see that there‚Äôs about four, perhaps five lines of unformatted text up there already. He strokes his chin while giving it a good read-through, then his hands take their position on the trackpad. Right index finger moves the cursor, left index finger does the clicking. The screen flips to another browser tab, his left hand gratuitously double-clicks on the website suggested by the first tile on the screen, and the page loads.&lt;/p&gt;&lt;p&gt;I never learned to tell any of these sites apart from each other. I see lots of people using the one with the spirograph logo. The one that looks like a cartoon butthole is also quite popular among some departments at my job. This guy is using the one that‚Äôs represented by a symmetrical color blob. Not that one, the other color blob one. Yeah.&lt;/p&gt;&lt;p&gt;He has opened a chat session that has evidently been going for some time. The text entry box at the bottom of the window waits patiently for fresh input. Letter by agonizing letter, the keys needed to express his thoughts are pressed. The most-pressed key, however, is Backspace. This man is, using the most generous language possible, not a particularly fast or accurate typist. In total, he enters about ten words before pressing Enter. A short moment later, the machine responds. Entire sentences appear in the time it took him to type a single word. Multiple paragraphs with subheadings and bulleted lists scroll into view. The screen fills completely with this fresh text. He looks at this for a moment, moves his hands back to the trackpad, and selects a complete paragraph. His finger presses down with immense force as he drags the selection area ever wider, as if his catch is in danger of wriggling through his fingers if he doesn‚Äôt hold the button down hard enough. He flips back to his Word document and pastes the paragraph. Then back to the chat window. He begins typing again. Slowly. Excruciatingly.&lt;/p&gt;&lt;p&gt;This cycle repeats several times, incrementally building his document up to four or five double-spaced pages in length. It‚Äôs not exactly a fast process, but certainly faster than if he had thought up and typed out all that content the old-fashioned way. It‚Äôs certainly plausible that he at least read everything that went into the document, but I wouldn‚Äôt be able to prove it.&lt;/p&gt;&lt;p&gt;He selects another piece of text, this one substantially smaller than the other specimens that he‚Äôd been handling up until this point. This one is pasted into a discussion thread on Teams. He waits a moment for responses, closes the lid, and the laptop goes back into his bag. The man stands up, wraps the strap over his shoulder, and walks to the front of the car as the train brakes to a full stop. This is where our paths diverge, it would seem. The doors open and he steps out into the night.&lt;/p&gt;&lt;p&gt;Alone in the train car again, with nothing interesting to eavesdrop on, my mind begins to wander again. I wonder what the purpose of that document was. Why was it being prepared? Who dictated that a half-dozen input phrases needed to be inflated into a thousand-word wall of text? Who was going to sit and read all of that, anyway? And for what purpose?&lt;/p&gt;&lt;p&gt;I really don‚Äôt know. But I do know one thing: It‚Äôs cat turds.&lt;/p&gt;&lt;p&gt;This guy obviously didn‚Äôt want to do that task. Whether that was due to lack of passion and interest, or lack of skill and ability, he had a cat turd to eat and he found a little pack of Altoids that he could use to get through it with minimal suffering. The people who have to read it? There‚Äôs a good chance they‚Äôll be dealing with a cat turd too. Maybe they can choose to employ a chatbot to summarize it back down to his original inputs. Maybe it‚Äôll even do a passable job preserving the essence of the guy‚Äôs prompts.&lt;/p&gt;&lt;p&gt;It makes sense why a person or group of people would flock to anything that makes life‚Äôs demands a little less difficult for themselves. You‚Äôd have to be pretty dumb to want to do a task like that manually.&lt;/p&gt;&lt;p&gt;There‚Äôs still the question, though. Why are we all eating cat turds? When did we all collectively agree that we were all a-okay with the idea that we had to subject ourselves to this constant grind of doing shit that doesn‚Äôt really need to be done to satisfy requirements that were put in place simply ‚Äúbecause‚Äù and that seemingly only create more pointless work for other people (or ourselves!) to have to do later?&lt;/p&gt;&lt;p&gt;One of the defining characteristics of humanity is its ability to build and wield tools that make difficult tasks easier. One would presume there would also be a certain wisdom in knowing which of the difficult tasks were worth doing in the first place but&amp;amp;mldr; Well. When you presume, you make a pres out of u and me.&lt;/p&gt;&lt;p&gt;If I had known ahead of time that I‚Äôd be out this late, I would‚Äôve brought a jacket. The early autumn air is calm but crisp, and my borough‚Äôs train platform offers very little protection from the chill. The crickets are still chirping, but their song has slowed substantially compared to how they sounded a few weeks back. I stopped parking at the station a long time ago‚Äîthe monthly pass costs well over $150 now, and most days the parking lot is completely full before six o‚Äôclock in the morning anyway. It‚Äôs only a mile to the house, and this twenty-minute walk is pretty much the only exercise I get nowadays.&lt;/p&gt;&lt;p&gt;Once I cross the main boulevard at the four-way stop, it‚Äôs all suburban residential side streets. There is basically no traffic at this time of night in my sleepy little bedroom community. All the dogs have been walked, the kids have been put to bed, and the adults&amp;amp;mldr; Well, I‚Äôm sure there are at least a couple people around here drinking or smoking the memory of their cat turds away.&lt;/p&gt;&lt;p&gt;I‚Äôm no closer to anything resembling inner peace. I find I‚Äôve grown to despise large swaths of the only thing I‚Äôve ever been able to earn reliable income from. I tire of walking a path that has seemingly shifted beneath my feet to point toward a destination I no longer recognize. I‚Äôm embarrassed by the jerk my Younger Self used to be, and simultaneously ashamed of the energy I lost as I matured. I don‚Äôt really want to do most of what I have to do, while feeling a deep unsated need to achieve something that I have neither the stamina nor the freedom to pursue. At some point I‚Äôm going to reach down deep into the well of ambition to discover there ain‚Äôt nothing there to pull out anymore. And then?&lt;/p&gt;&lt;p&gt;Something percent of success is simply showing up. That‚Äôs roughly how the quote goes, right? I‚Äôve heard seventy percent, ninety percent, hell, let‚Äôs call it seventy-eight. It doesn‚Äôt matter because it isn‚Äôt a real thing that can be measured in any objective way. The idea is to inspire people to at least try. Put your butt in the chair, log into Teams, trick yourself into thinking, well, I made it all the way here, might as well prune my stale Git branches or something so I can feel like I‚Äôm doing real work. Push aside distractions, shake off procrastination, kindle that tiny spark into enough momentum to break through whatever barrier is standing in the way of getting something done. If only that worked with any degree of predictability.&lt;/p&gt;&lt;p&gt;There‚Äôs a metaphor that talks about painting the backs of cabinets. The idea is that, when you‚Äôre putting paint, stain, varnish, whatever on some cabinets, there‚Äôs no need to paint the surfaces that face toward the wall. From the day the units are mounted, to a day forty years from now when they are ripped down and thrown into a construction dumpster during a subsequent kitchen renovation, nobody will see the backs of any of those cabinets. Painting them would be a waste of time and materials. Nobody would know if it was done or not.&lt;/p&gt;&lt;p&gt;‚ÄúYes, but I would know.‚Äù That‚Äôs something my Dad would often say. His tendency has always been to be overly thorough, exacting and precise in any craft he partakes in. Everything‚Äîfrom the doors in the house to the stripes cut into the front lawn‚Äîwas always level, plumb, square, centered, polished, dust-free, squeak-free, fingerprint-free&amp;amp;mldr; He even demonstrated meticulous care in breaking down cardboard and filling up the waste bins at the curb. I still have no idea how he was able to raise two kids in that house without exploding from the chaos we brought.&lt;/p&gt;&lt;p&gt;Maybe it was genetic, or maybe I voluntarily developed it so my dad would be proud of me just like he was proud of the other things he made. Either way, I definitely started to take after him in those ways and I now recognize this same kind of care in myself all the time. Not just in the way I prefer all my clocks to read the exact correct time or my knack for always noticing the way the receptacle face isn‚Äôt exactly flush with the wall plate&amp;amp;mldr; but in a fundamental inability to not care about quality or craft. Even when the task doesn‚Äôt matter. Even if it results in an entire afternoon spent painting a piece of carpentry that nobody will ever see. I can‚Äôt not care.&lt;/p&gt;&lt;p&gt;All that stuff Younger Self struggled with‚Äîthe self-superiority, the sense that I had to be the one who did it if it needed to be done correctly, the derision and borderline abuse I gave others‚Äîthat was all just a big dogmatic ball of caring a whole lot about quality and craft, being rolled around by a kid who didn‚Äôt understand what to do with it. I had to work so hard to care so much, and these other people didn‚Äôt, and everything worked out for them anyway, and that wasn‚Äôt fair. Decades later, I still feel that way sometimes.&lt;/p&gt;&lt;p&gt;My parents still live in that house, surrounded by all the things my dad cared so much about. Aside from a whole bunch of trees that died and needed to be cut down to stumps, everything is still pretty much pristine. But if you start to look around, really scrutinize, you‚Äôll start to notice some things have slipped. There‚Äôs a film of dust on the higher wall decorations. Some of the brass knobs are becoming tarnished. A few of the light bulbs in the hallway fixtures don‚Äôt match. My dad seemed tired the last time we talked, and more than once he expressed the sentiment that ‚Äúeverything he owns is falling apart.‚Äù Is it simply the onset of physical old age that has limited his ability to stay on top of these things, or is he beginning to leave behind his era of caring?&lt;/p&gt;&lt;p&gt;Now that I think about it, I don‚Äôt think we‚Äôve ever really talked about how care factored into his career philosophy. I had always implicitly assumed that it was the same as it currently is with me: Work or play, it‚Äôs always there. Can‚Äôt turn it off even if I wanted to. But what if he could? What if all the care he demonstrated in projects around the house was compensation for all the things he deliberately avoided caring about at work? It would certainly explain how he was able to consistently sustain those standards. But then, that would mean that I modeled my own principles and tastes on a distorted view of my dad, untempered by whatever he didn‚Äôt let me see about his workplace persona.&lt;/p&gt;&lt;p&gt;How would I begin to‚Äîwell, I don‚Äôt want to say ‚Äúnot care,‚Äù that sounds too extreme. But maybe&amp;amp;mldr; selectively care? To care about the things that matter, the things that spark passion and joy and remind me why I spent so much time practicing this godforsaken occupation. While at the same time recognizing the things that don‚Äôt matter, the problems for which the optimal solution is to stop insisting on having that problem in the first place. The kinds of tasks for which the 78% showing-up baseline score is plenty good enough. Tasks on which care would be utterly wasted, the cases where the cabinets are so irredeemably fucked up that the lack of paint on the back is the last thing anybody‚Äôs going to worry about. Those are the tasks that hurt the most, because I find it basically impossible to make myself care about them. It offends my soul to try to force it, and it drains me of all ambition to move onto the next potentially heartening opportunity. It‚Äôs a real problem, and I find it always has been: If I can‚Äôt care about it, I have an extremely hard time bringing myself to do it at all.&lt;/p&gt;&lt;p&gt;Well, I suppose that‚Äôs when I open a chatbot session of my own. ‚ÄúHey there Chat. Uh, we‚Äôve never spoken before but, uh&amp;amp;mldr; Well, my entire system of self-motivation just completely broke down but I still need to keep moving forward. Can you help me out of this bind?‚Äù There‚Äôs a whole discipline‚Äîthey call it Prompt Engineering‚Äîthat‚Äôs just a fancy form of throwing your hands up and pressing the Care About It For Me button. That‚Äôs pretty much how it works. Provide it with any cat turd under the sun, it doesn‚Äôt matter. Chat will gobble them all up for you like a coprophagic dog.&lt;/p&gt;&lt;p&gt;I‚Äôd be lying if I said the idea didn‚Äôt make my skin crawl a little. Every fiber of my being says that this is a weight to be borne by me and me alone. This is my cat turd to eat; they gave it to me. When it‚Äôs done, I can open my grinning maw and say without equivocation that I was the one who got through it. I painted the back of this cabinet. I worked way too hard and poured far too much of my blood, sweat and tears into this thing. And my reward for a job well done is&amp;amp;mldr; debilitating exhaustion, most of the time. Getting a fresh cat turd to eat tomorrow. And the day after.&lt;/p&gt;&lt;p&gt;Of course, Chat can‚Äôt really care. It does a passable job pretending like it cares, saying the words that convey the illusion of care to any reader not paying very close attention. Where do I draw the line between fostering real care, versus passing off a degraded third-generation photocopy of some tokenization of what may have at one point been somebody else‚Äôs care? Is the line simply the boundary between the tasks I‚Äôm excited to do and the ones I put off until I‚Äôve depleted enough mental reserves to sorta care?&lt;/p&gt;&lt;p&gt;It really does feel like the average person has made a choice to abandon a great deal of care, at least in their professional capacity. Take a look around at all these people with their fake shit-eating grins, passing off a machine‚Äôs effort as their own and experiencing no consequences. Sometimes they‚Äôre rewarded for doing so. There are organizations that are beginning to mandate it now. These people aren‚Äôt eating their cat turds anymore, why am I still sitting here eating mine?&lt;/p&gt;&lt;p&gt;I round the final curve leading to the corner of my block. As I pass under the streetlight, I cast a shadow on the asphalt ahead. With each step it grows longer and more distorted. There‚Äôs a rustle from the shrubs bordering my neighbor‚Äôs driveway, and a small dark form emerges. It crosses the street halfway then abruptly stops. I stop as well. A pair of glowing yellow eyes look back at me. I stare at it, it stares at me. A possum, perhaps? Somebody‚Äôs outdoor cat? It‚Äôs just watching me, seemingly peering straight into my very soul. Can it see what I‚Äôm grappling with here? Is it passing judgement on me for thinking these thoughts? It sizes me up for a moment longer, turns its head, and becomes a black apparition once more. I struggle to track it as it continues across the street, and I lose sight of it entirely.&lt;/p&gt;&lt;p&gt;I arrive at home and shut the door behind me. Sunset was over two hours ago and it‚Äôs nearly pitch black in the hallway. I fumble around for the light switch, kick my shoes off next to the doorway, and hang my bag on its hook in the coat closet. Something grabs my attention, just above eye level, slightly overhanging the edge of the top shelf. I slide it out of its resting place and carry it into the kitchen. I sit down at the table and inspect it.&lt;/p&gt;&lt;p&gt;This object is a round metal cookie tin about twelve inches in diameter. Beneath a thin coat of dust, it is a deep red with a repeating pattern of snowmen and white snowflakes, and quite obviously once held winter holiday‚Äìthemed cookies. I repurposed it many years ago to hold the only vice I currently permit myself to indulge in: a meticulously curated collection of all different types of chocolate candies. I remove the lid and set it aside. I survey the contents, a sea of differently-shaped naked chocolate morsels. I don‚Äôt remember why I chose to remove all the foil and paper wrapping before putting these in here. From my vantage point, everything looks vaguely the same‚ÄîI can‚Äôt readily spot any differences between milk chocolate and dark, or those filled with caramel versus cr√®me.&lt;/p&gt;&lt;p&gt;One particular piece near the edge catches my eye, and I carefully select it for inspection. It‚Äôs not a very pleasing color or shape‚Äîoddly asymmetrical. I roll it around between my fingers. There‚Äôs a hair on it. I hold it up to my nose and take a whiff, hoping to detect the aroma of the cacao. Try as I might, I can‚Äôt pick up any trace of its scent.&lt;/p&gt;&lt;p&gt;Come to think of it, I can‚Äôt remember the last time I smelled anything.&lt;/p&gt;¬´ Back to Articles&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.scottsmitelli.com/articles/altoids-by-the-fistful/"/><published>2025-09-23T06:24:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45343689</id><title>Telli (YC F24) is hiring ambitious engineers [Berlin, on-site]</title><updated>2025-09-23T11:32:33.104874+00:00</updated><link href="https://hi.telli.com/join-us"/><published>2025-09-23T07:01:44+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344554</id><title>The YAML Document from Hell</title><updated>2025-09-23T11:32:32.809880+00:00</updated><content>&lt;doc fingerprint="e45100779e81da58"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The yaml document from hell&lt;/head&gt;
    &lt;p&gt;written by Ruud van Asseldonk&lt;lb/&gt;published &lt;/p&gt;
    &lt;p&gt;For a data format, yaml is extremely complicated. It aims to be a human-friendly format, but in striving for that it introduces so much complexity, that I would argue it achieves the opposite result. Yaml is full of footguns and its friendliness is deceptive. In this post I want to demonstrate this through an example.&lt;/p&gt;
    &lt;p&gt;This post is a rant, and more opinionated than my usual writing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Yaml is really, really complex&lt;/head&gt;
    &lt;p&gt;Json is simple. The entire json spec consists of six railroad diagrams. It‚Äôs a simple data format with a simple syntax and that‚Äôs all there is to it. Yaml on the other hand, is complex. So complex, that its specification consists of 10 chapters with sections numbered four levels deep and a dedicated errata page.&lt;/p&gt;
    &lt;p&gt;The json spec is not versioned. There were two changes to it in 2005 (the removal of comments, and the addition of scientific notation for numbers), but it has been frozen since ‚Äî almost two decades now. The yaml spec on the other hand is versioned. The latest revision is fairly recent, 1.2.2 from October 2021. Yaml 1.2 differs substantially from 1.1: the same document can parse differently under different yaml versions. We will see multiple examples of this later.&lt;/p&gt;
    &lt;p&gt;Json is so obvious that Douglas Crockford claims to have discovered it ‚Äî not invented. I couldn‚Äôt find any reference for how long it took him to write up the spec, but it was probably hours rather than weeks. The change from yaml 1.2.1 to 1.2.2 on the other hand, was a multi-year effort by a team of experts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This revision is the result of years of work by the new YAML language development team. Each person on this team has a deep knowledge of the language and has written and maintains important open source YAML frameworks and tools.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Furthermore this team plans to actively evolve yaml, rather than to freeze it.&lt;/p&gt;
    &lt;p&gt;When you work with a format as complex as yaml, it is difficult to be aware of all the features and subtle behaviors it has. There is an entire website dedicated to picking one of the 63 different multi-line string syntaxes. This means that it can be very difficult for a human to predict how a particular document will parse. Let‚Äôs look at an example to highlight this.&lt;/p&gt;
    &lt;head rend="h2"&gt;The yaml document from hell&lt;/head&gt;
    &lt;p&gt;Consider the following document.&lt;/p&gt;
    &lt;code&gt;server_config:
  port_mapping:
    # Expose only ssh and http to the public internet.
    - 22:22
    - 80:80
    - 443:443

  serve:
    - /robots.txt
    - /favicon.ico
    - *.html
    - *.png
    - !.git  # Do not expose our Git repository to the entire world.

  geoblock_regions:
    # The legal team has not approved distribution in the Nordics yet.
    - dk
    - fi
    - is
    - no
    - se

  flush_cache:
    on: [push, memory_pressure]
    priority: background

  allow_postgres_versions:
    - 9.5.25
    - 9.6.24
    - 10.23
    - 12.13&lt;/code&gt;
    &lt;p&gt;Let‚Äôs break this down section by section and see how the data maps to json.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sexagesimal numbers&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start with something that you might find in a container runtime configuration:&lt;/p&gt;
    &lt;code&gt;port_mapping:
  - 22:22
  - 80:80
  - 443:443&lt;/code&gt;
    &lt;code&gt;{"port_mapping": [1342, "80:80", "443:443"]}&lt;/code&gt;
    &lt;p&gt;Huh, what happened here? As it turns out, numbers from 0 to 59 separated by colons are sexagesimal (base 60) number literals. This arcane feature was present in yaml 1.1, but silently removed from yaml 1.2, so the list element will parse as &lt;code&gt;1342&lt;/code&gt; or &lt;code&gt;"22:22"&lt;/code&gt; depending on which version your parser uses. Although yaml 1.2 is more than 10 years old by now, you would be mistaken to think that it is widely supported: the latest version libyaml at the time of writing (which is used among others by PyYAML) implements yaml 1.1 and parses &lt;code&gt;22:22&lt;/code&gt; as &lt;code&gt;1342&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anchors, aliases, and tags&lt;/head&gt;
    &lt;p&gt;The following snippet is actually invalid:&lt;/p&gt;
    &lt;code&gt;serve:
  - /robots.txt
  - /favicon.ico
  - *.html
  - *.png
  - !.git&lt;/code&gt;
    &lt;p&gt;Yaml allows you to create an anchor by adding an &lt;code&gt;&amp;amp;&lt;/code&gt; and a name in front of a value, and then you can later reference that value with an alias: a &lt;code&gt;*&lt;/code&gt; followed by the name. In this case no anchors are defined, so the aliases are invalid. Let‚Äôs avoid them for now and see what happens.&lt;/p&gt;
    &lt;code&gt;serve:
  - /robots.txt
  - /favicon.ico
  - !.git&lt;/code&gt;
    &lt;code&gt;{"serve": ["/robots.txt", "/favicon.ico", ""]}&lt;/code&gt;
    &lt;p&gt;Now the interpretation depends on the parser you are using. The element starting with &lt;code&gt;!&lt;/code&gt; is a tag. This feature is intended to enable a parser to convert the fairly limited yaml data types into richer types that might exist in the host language. A tag starting with &lt;code&gt;!&lt;/code&gt; is up to the parser to interpret, often by calling a constructor with the given name and providing it the value that follows after the tag. This means that loading an untrusted yaml document is generally unsafe, as it may lead to arbitrary code execution. (In Python, you can avoid this pitfall by using &lt;code&gt;yaml.safe_load&lt;/code&gt; instead of &lt;code&gt;yaml.load&lt;/code&gt;.) In our case above, PyYAML fails to load the document because it doesn‚Äôt know the &lt;code&gt;.git&lt;/code&gt; tag. Go‚Äôs yaml package is less strict and returns an empty string.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Norway problem&lt;/head&gt;
    &lt;p&gt;This pitfall is so infamous that it became known as ‚Äúthe Norway problem‚Äù:&lt;/p&gt;
    &lt;code&gt;geoblock_regions:
  - dk
  - fi
  - is
  - no
  - se&lt;/code&gt;
    &lt;code&gt;{"geoblock_regions": ["dk", "fi", "is", false, "se"]}&lt;/code&gt;
    &lt;p&gt;What is that &lt;code&gt;false&lt;/code&gt; doing there? The literals &lt;code&gt;off&lt;/code&gt;, &lt;code&gt;no&lt;/code&gt;, and &lt;code&gt;n&lt;/code&gt;, in various capitalizations (but not any capitalization!), are all &lt;code&gt;false&lt;/code&gt; in yaml 1.1, while &lt;code&gt;on&lt;/code&gt;, &lt;code&gt;yes&lt;/code&gt;, and &lt;code&gt;y&lt;/code&gt; are true. In yaml 1.2 these alternative spellings of the boolean literals are no longer allowed, but they are so pervasive in the wild that a compliant parser would have a hard time reading many documents. Go‚Äôs yaml library therefore made the choice of implementing a custom variant somewhere in between yaml 1.1 and 1.2 that behaves differently depending on the context:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The yaml package supports most of YAML 1.2, but preserves some behavior from 1.1 for backwards compatibility. YAML 1.1 bools (yes/no, on/off) are supported as long as they are being decoded into a typed bool value. Otherwise they behave as a string.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Note that it only does that since version 3.0.0, which was released in May 2022. Earlier versions behave differently.&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-string keys&lt;/head&gt;
    &lt;p&gt;While keys in json are always strings, in yaml they can be any value, including booleans.&lt;/p&gt;
    &lt;code&gt;flush_cache:
  on: [push, memory_pressure]
  priority: background&lt;/code&gt;
    &lt;code&gt;{
  "flush_cache": {
    "True": ["push", "memory_pressure"],
    "priority": "background"
  }
}&lt;/code&gt;
    &lt;p&gt;Combined with the previous feature of interpreting &lt;code&gt;on&lt;/code&gt; as a boolean, this leads to a dictionary with &lt;code&gt;true&lt;/code&gt; as one of the keys. It depends on the language how that maps to json, if at all. In Python it becomes the string &lt;code&gt;"True"&lt;/code&gt;. The key &lt;code&gt;on&lt;/code&gt; is common in the wild because it is used in GitHub Actions. I would be really curious to know whether GitHub Actions‚Äô parser looks at &lt;code&gt;"on"&lt;/code&gt; or &lt;code&gt;true&lt;/code&gt; under the hood.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accidental numbers&lt;/head&gt;
    &lt;p&gt;Leaving strings unquoted can easily lead to unintentional numbers.&lt;/p&gt;
    &lt;code&gt;allow_postgres_versions:
  - 9.5.25
  - 9.6.24
  - 10.23
  - 12.13&lt;/code&gt;
    &lt;code&gt;{"allow_postgres_versions": ["9.5.25", "9.6.24", 10.23, 12.13]}&lt;/code&gt;
    &lt;p&gt;Maybe the list is a contrived example, but imagine updating a config file that lists a single value of 9.6.24 and changing it to 10.23. Would you remember to add the quotes? What makes this even more insidious is that many dynamically typed applications implicitly convert the number to a string when needed, so your document works fine most of the time, except in some contexts it doesn‚Äôt. For example, the following Jinja template accepts both &lt;code&gt;version: "0.0"&lt;/code&gt; and &lt;code&gt;version: 0.0&lt;/code&gt;, but it only takes the true-branch for the former.&lt;/p&gt;
    &lt;code&gt;{% if version %}
  Latest version: {{ version }}
{% else %}
  Version not specified
{% endif %}&lt;/code&gt;
    &lt;head rend="h2"&gt;Runners-up&lt;/head&gt;
    &lt;p&gt;There is only so much I can fit into one artifical example. Some arcane yaml behaviors that did not make it in are directives, integers starting with &lt;code&gt;0&lt;/code&gt; being octal literals (but only in yaml 1.1), &lt;code&gt;~&lt;/code&gt; being an alternative spelling of &lt;code&gt;null&lt;/code&gt;, and &lt;code&gt;?&lt;/code&gt; introducing a complex mapping key.&lt;/p&gt;
    &lt;head rend="h2"&gt;Syntax highlighting will not save you&lt;/head&gt;
    &lt;p&gt;You may have noticed that none of my examples have syntax highlighting enabled. Maybe I am being unfair to yaml, because syntax highlighting would highlight special constructs, so you can at least see that some values are not normal strings. However, due to multiple yaml versions being prevalent, and highlighters having different levels of sophistication, you can‚Äôt rely on this. I‚Äôm not trying to nitpick here: Vim, my blog generator, GitHub, and Codeberg, all have a unique way to highlight the example document from this post. No two of them pick out the same subset of values as non-strings!&lt;/p&gt;
    &lt;head rend="h2"&gt;Templating yaml is a terrible, terrible idea&lt;/head&gt;
    &lt;p&gt;I hope it is clear by now that working with yaml is subtle at the very least. What is even more subtle is concatenating and escaping arbitrary text fragments in such a way that the result is a valid yaml document, let alone one that does what you expect. Add to this the fact that whitespace is significant in yaml, and the result is a format that is meme-worthily difficult to template correctly. I truly do not understand why tools based on such an error-prone practice have gained so much mindshare, when there is a safer, easier, and more powerful alternative: generating json.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alternative configuration formats&lt;/head&gt;
    &lt;p&gt;I think the main reason that yaml is so prevalent despite its pitfalls, is that for a long time it was the only viable configuration format. Often we need lists and nested data, which rules out flat formats like ini. Xml is noisy and annoying to write by hand. But most of all, we need comments, which rules out json. (As we saw before, json had comments very early on, but they were removed because people started putting parsing directives in there. I think this is the right call for a serialization format, but it makes json unsuitable as a configuration language.) So if what we really need is the json data model but a syntax that allows comments, what are some of the options?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Toml ‚Äî Toml is similar to yaml in many ways: it has mostly the same data types; the syntax is not as verbose as json; and it allows comments. Unlike yaml it is not full of footguns, mostly because strings are always quoted, so you don‚Äôt have values that look like strings but aren‚Äôt. Toml is widely supported, you can probably find a toml parser for your favorite language. It‚Äôs even in the Python standard library ‚Äî unlike yaml! A weak spot of toml is deeply nested data.&lt;/item&gt;
      &lt;item&gt;Json with comments, Json with commas and comments ‚Äî There exist various extensions of json that extend it just enough to make it a usable config format without introducing too much complexity. Json with comments is probably the most widespread, as it is used as the config format for Visual Studio Code. The main downside of these is that they haven‚Äôt really caught on (yet!), so they aren‚Äôt as widely supported as json or yaml.&lt;/item&gt;
      &lt;item&gt;A simple subset of yaml ‚Äî Many of the problems with yaml are caused by unquoted things that look like strings but behave differently. This is easy to avoid: always quote all strings. (Indeed, you can tell that somebody is an experienced yaml engineer when they defensively quote all the strings.) We can choose to always use &lt;code&gt;true&lt;/code&gt;and&lt;code&gt;false&lt;/code&gt;rather than&lt;code&gt;yes&lt;/code&gt;and&lt;code&gt;no&lt;/code&gt;, and generally stay away from the arcane features. The challenge with this is that any construct not explicitly forbidden will eventually make it into your codebase, and I am not aware of any good tool that can enforce a sane yaml subset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Generating json as a better yaml&lt;/head&gt;
    &lt;p&gt;Often the choice of format is not ours to make, and an application only accepts yaml. Not all is lost though, because yaml is a superset of json, so any tool that can produce json can be used to generate a yaml document.&lt;/p&gt;
    &lt;p&gt;Sometimes an application will start out with a need for just a configuration format, but over time you end up with many many similar stanzas, and you would like to share parts between them, and abstract some repetition away. This tends to happen in for example Kubernetes and GitHub Actions. When the configuration language does not support abstraction, people often reach for templating, which is a bad idea for the reasons explained earlier. Proper programming languages, possibly domain-specific ones, are a better fit. Some of my favorites are Nix and Python:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nix ‚Äî Nix is the language used by the Nix package manager. It was created for writing package definitions, but it works remarkably well as a configuration format (and indeed it is used to configure NixOS). Functions, let-bindings, and string interpolation make it powerful for abstracting repetitive configuration. The syntax is light like toml, and it can export to json or xml. It works well for simplifying a repetitive GitHub Actions workflow file, for example.&lt;/item&gt;
      &lt;item&gt;Python ‚Äî Json documents double as valid Python literals with minimal adaptation, and Python supports trailing commas and comments. It has variables and functions, powerful string interpolation, and &lt;code&gt;json.dump&lt;/code&gt;built in. A self-contained Python file that prints json to stdout goes a long way!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally there are some tools in this category that I haven‚Äôt used enough to confidently recommend, but which deserve to be mentioned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dhall ‚Äî Dhall is like Nix, but with types. It is less widespread, and personally I find the built-in function names unwieldy.&lt;/item&gt;
      &lt;item&gt;Cue ‚Äî Like Dhall, Cue integrates type/schema information into the config format. Cue is a superset of json, but despite that, I find the files that actually use Cue‚Äôs features to look foreign to me. Cue is on my radar to evaluate further, but I haven‚Äôt encountered a problem where Cue looked like the most suitable solution yet.&lt;/item&gt;
      &lt;item&gt;Hashicorp Configuration Language ‚Äî I haven‚Äôt used HCL extensively enough to have a strong opinion on it, but in the places where I worked with it, the potential for abstraction seemed more limited than what you can achieve with e.g. Nix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2025 update: After having used HCL more in practice, I consider it too ad-hoc to seriously recommend. My frustration with HCL is what prompted me to create RCL. It started as a toy project, but is now at a point where it is both usable and useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Yaml aims to be a more human-friendly alternative to json, but with all of its features, it became such a complex format with so many bizarre and unexpected behaviors, that it is difficult for humans to predict how a given yaml document will parse. If you are looking for a configuration format, toml is a friendly format without yaml‚Äôs footguns. For cases where you are stuck with yaml, generating json from a more suitable language can be a viable approach. Generating json also opens up the possibility for abstraction and reuse, in a way that is difficult to achieve safely by templating yaml.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell"/><published>2025-09-23T09:04:43+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344708</id><title>Go has added Valgrind support</title><updated>2025-09-23T11:32:32.542175+00:00</updated><link href="https://go-review.googlesource.com/c/go/+/674077"/><published>2025-09-23T09:26:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45344756</id><title>Indoor surfaces act as sponges for harmful chemicals</title><updated>2025-09-23T11:32:32.246196+00:00</updated><content>&lt;doc fingerprint="75ffb4ccae8f068c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Indoor surfaces act as massive sponges for harmful chemicals, UC Irvine-led study shows&lt;/head&gt;
    &lt;p&gt;Permeable materials in homes can retain volatile organic compounds for up to a year&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Scientists injected volatile organic compounds into a test house and found large reservoirs for the potentially hazardous chemicals in porous surfaces such as wood, cement and paint.&lt;/item&gt;
      &lt;item&gt;VOCs contained in insecticides, cigarette smoke and wildfire smoke can remain on indoor surfaces for as long as one year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Irvine, Calif., Sept. 22, 2025 ‚Äî Indoor surfaces have an unexpectedly strong ability to absorb and hold harmful chemical compounds that can threaten human health for as long as a year, according to air chemistry researchers at the University of California, Irvine.&lt;/p&gt;
    &lt;p&gt;In a paper published today in Proceedings of the National Academy of Sciences, the UC Irvine scientists quantify how various indoor surfaces absorb volatile organic compounds, which can result in unhealthy conditions for people and animals when inhaled or absorbed through skin contact.&lt;/p&gt;
    &lt;p&gt;The sources of VOCs are many, such as cooking, spray cleaning, personal care and other consumer products. Additional significant contributors include tobacco smoke and, increasingly, air pollution caused by wildfires. The researchers note that health risks come from inhaling compounds when they ‚Äúoff gas‚Äù from surfaces and through dermal uptake when contaminated surfaces are touched.&lt;/p&gt;
    &lt;p&gt;In the spring of 2022, co-author Jonathan Abbatt, professor of chemistry at the University of Toronto, led the Chemical Assessment of Surfaces and Air study, which utilized simulation chambers in the National Institute of Standards and Technology‚Äôs Net-Zero Energy Residential Test Facility. Contaminants were injected into a structure mimicking a home environment, with typical building materials. The research team used mass spectrometry instruments to track the movement and persistence of VOCs in the controlled indoor environment.&lt;/p&gt;
    &lt;p&gt;‚ÄúScientists in the air chemistry research community have known for a long time that many indoor contaminants can be absorbed by indoor surfaces, but the size of indoor surface reservoirs inside homes and buildings had not been established,‚Äù said Manabu Shiraiwa, UC Irvine professor of chemistry, who was responsible for modeling observations and is a corresponding author on the PNAS paper. ‚ÄúOur modeling found that surfaces inside homes have a much greater size to absorb and hold chemicals than previously realized. We can think of these surfaces as massive chemical sponges that soak up VOCs.‚Äù&lt;/p&gt;
    &lt;p&gt;Before this study, thin organic films with nanometer thickness were thought to be main surface reservoirs. However, this work proves that permeable and porous materials such as painted surfaces, cement and wood are likely the major surface reservoirs in a home.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis discovery has significant implications for human health,‚Äù Shiraiwa said. ‚ÄúIt means people can be exposed to harmful chemicals long after their initial introduction into indoor spaces, and compounds can later be released back into the air or transferred to humans through direct contact with contaminated surfaces.‚Äù&lt;/p&gt;
    &lt;p&gt;He added, ‚ÄúThis result significantly impacts our understanding of VOC fate and human exposure in indoor environments. With such a large partitioning capacity, organic contaminants will have much longer indoor residence times than previously predicted.‚Äù&lt;/p&gt;
    &lt;p&gt;The research explains why certain odors and contaminants persist indoors even after their sources are removed. For example, it provides scientific evidence for why tobacco smoke odors linger in rooms long after smoking has stopped: The residual compounds, known as ‚Äúthirdhand smoke,‚Äù slowly partition back into the air from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;The findings suggest that regular ventilation alone may be insufficient to remove many indoor contaminants. Physical cleaning activities such as vacuuming, mopping and dusting are necessary to effectively remove compounds with high partition coefficients from surface reservoirs.&lt;/p&gt;
    &lt;p&gt;Joining Shiraiwa and Abbatt in this study were Pascale Lakey, project scientist in chemistry at UC Irvine; Jie Yu and Xing Wang at the University of Toronto; Jenna Ditto at Washington University in St. Louis, Missouri; Han Huynh and Marina Vance at the University of Colorado Boulder; Michael Link, Dustin Poppendieck and Stephen Zimmerman at the National Institute of Standards and Technology; and Delphine Farmer at Colorado State University.&lt;/p&gt;
    &lt;p&gt;The research was supported by funding from the Alfred P. Sloan Foundation.&lt;/p&gt;
    &lt;p&gt;About the University of California, Irvine: Founded in 1965, UC Irvine is a member of the prestigious Association of American Universities and is ranked among the nation‚Äôs top 10 public universities by U.S. News &amp;amp; World Report. The campus has produced five Nobel laureates and is known for its academic achievement, premier research, innovation and anteater mascot. Led by Chancellor Howard Gillman, UC Irvine has more than 36,000 students and offers 224 degree programs. It‚Äôs located in one of the world‚Äôs safest and most economically vibrant communities and is Orange County‚Äôs second-largest employer, contributing $7 billion annually to the local economy and $8 billion statewide. For more on UC Irvine, visit www.uci.edu.&lt;/p&gt;
    &lt;p&gt;Media access: Radio programs/stations may, for a fee, use an on-campus studio with a Comrex IP audio codec to interview UC Irvine faculty and experts, subject to availability and university approval. For more UC Irvine news, visit news.uci.edu. Additional resources for journalists may be found at https://news.uci.edu/media-resources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.uci.edu/2025/09/22/indoor-surfaces-act-as-massive-sponges-for-harmful-chemicals-uc-irvine-led-study-shows/"/><published>2025-09-23T09:33:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345148</id><title>Hyb Error: A Hybrid Metric Combining Absolute and Relative Errors</title><updated>2025-09-23T11:32:31.985485+00:00</updated><content>&lt;doc fingerprint="988c360a05dfb08c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Mathematics &amp;gt; Numerical Analysis&lt;/head&gt;&lt;p&gt; [Submitted on 12 Mar 2024 (v1), last revised 21 May 2024 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Hyb Error: A Hybrid Metric Combining Absolute and Relative Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Suppose $x$ is an approximation of $y$. This paper proposes using $\frac{|x-y|}{1+|y|}$, named Hyb Error, to measure the error. This metric equals half the harmonic mean of absolute error and relative error, effectively combining their advantages while mitigating their limitations. For example, Hyb Error approaches absolute error as $|y|$ approaches 0, thereby avoiding the exaggeration of relative error, and approaches relative error as $|y|$ approaches infinity, thereby avoiding the exaggeration of absolute error. The Hyb Error of $\epsilon$ is equivalent to $|x-y|=\epsilon+\epsilon |y|$, which implies $\mathrm{isclose}(x,y,\epsilon,\epsilon)=\mathrm{True}$, where ``isclose'' is a common floating-point equality check function in numerical libraries. For sequences, this property makes the Maximum Element-wise Hyb Error (MEHE) a pragmatic error metric that reflects the most significant error and equals the decision boundary of the ``isclose'' function.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Peichen Xie [view email]&lt;p&gt;[v1] Tue, 12 Mar 2024 10:30:46 UTC (84 KB)&lt;/p&gt;&lt;p&gt;[v2] Tue, 21 May 2024 08:20:36 UTC (62 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;math.NA&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2403.07492"/><published>2025-09-23T10:30:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345233</id><title>Crypto Miner in hotio/qbittorrent</title><updated>2025-09-23T11:32:31.618435+00:00</updated><content>&lt;doc fingerprint="3d19d83437a3cf8c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Crypto Miner in hotio/qbittorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;üó£Ô∏è Join the Discussion üó£Ô∏è&lt;/head&gt;
    &lt;head rend="h3"&gt;Infected Container Image&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Registry: &lt;code&gt;ghcr.io/hotio/qbittorrent&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tag: &lt;code&gt;release&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Digest: &lt;code&gt;sha256:3779f89712dbaa8b25fc22897d0b471ee&lt;/code&gt;&lt;code&gt;29049b2b0f8d3c192df83b098c84fc5&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recently, while migrating to a new server, I discovered a suspicious process running inside a hotio/qbittorrent Docker container.&lt;/p&gt;
    &lt;p&gt;I mainly use this to download Linux ISOs, as anybody else, right?&lt;/p&gt;
    &lt;p&gt;I just want to get that sweet sweet Omarchy ISO.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;While monitoring system resources, I noticed a process consuming unusually high CPU:&lt;/p&gt;
    &lt;code&gt;$ ps -ef | grep netservlet
1000  758679  756435  99 09:39 ?  00:13:43 ./netservlet
&lt;/code&gt;
    &lt;p&gt;The binary &lt;code&gt;netservlet&lt;/code&gt; was unfamiliar, and attempts to inspect it via &lt;code&gt;/proc&lt;/code&gt; failed:&lt;/p&gt;
    &lt;code&gt;$ docker exec qbittorrent cat /proc/758679/exe &amp;gt; /tmp/netservlet
# Output: No such file or directory
&lt;/code&gt;
    &lt;p&gt;This indicates the binary was likely unlinked from the filesystem, pretty much as expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;Since direct access to the executable was blocked, I generated a core dump:&lt;/p&gt;
    &lt;code&gt;$ gcore -o /tmp/netservlet_core 758679
&lt;/code&gt;
    &lt;p&gt;I could now simply use &lt;code&gt;strings&lt;/code&gt; to further have a look:&lt;/p&gt;
    &lt;code&gt;$ strings /tmp/netservlet.elf | egrep -i 'stratum|pool|wallet|http|crypto|mining|eth|btc|pool'
&lt;/code&gt;
    &lt;p&gt;Highlights included:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;References to cryptocurrency mining: &lt;code&gt;cryptonight&lt;/code&gt;,&lt;code&gt;ethash_calculate_dag_item&lt;/code&gt;,&lt;code&gt;mining.submit&lt;/code&gt;,&lt;code&gt;mining.authorize&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Mining pool addresses: &lt;code&gt;auto.c3pool.org:19999&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Miner configuration options: &lt;code&gt;--cpu-memory-pool&lt;/code&gt;,&lt;code&gt;--opencl&lt;/code&gt;,&lt;code&gt;--cuda&lt;/code&gt;,&lt;code&gt;--rig-id&lt;/code&gt;,&lt;code&gt;http-access-token&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This confirmed that &lt;code&gt;netservlet&lt;/code&gt; was a stealth crypto miner, likely XMRig or a variant.
I thought we were over crypto bros and all over to AI-bros, I was mistaken üòÇ.&lt;/p&gt;
    &lt;p&gt;Just for fun, I also ran &lt;code&gt;binwalk&lt;/code&gt; against the dump, which ended up with a &lt;code&gt;1.3GB&lt;/code&gt; &lt;code&gt;gzip&lt;/code&gt; archive.
We couldn‚Äôt just unzip it, though, due to obfuscation.&lt;/p&gt;
    &lt;p&gt;I will analyze it via &lt;code&gt;ghidra&lt;/code&gt; later, today I should be OOO lol.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Remember:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Never trust random Docker images‚Äîyour containers aren‚Äôt magic elves.&lt;/item&gt;
      &lt;item&gt;Keep an eye on system resources‚Äîthey have feelings too.&lt;/item&gt;
      &lt;item&gt;Audit your host and containers often‚Äîbecause surprises are only fun at parties.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://apogliaghi.com/2025/09/crypto-miner-in-hotio/qbittorrent/"/><published>2025-09-23T10:44:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45345237</id><title>Walking Michigan City (Indiana)</title><updated>2025-09-23T11:32:31.530014+00:00</updated><content/><link href="https://walkingtheworld.substack.com/p/walking-michigan-city-indiana"/><published>2025-09-23T10:44:57+00:00</published></entry></feed>