<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-09-27T21:07:38.584603+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45393842</id><title>Typst: A Possible LaTeX Replacement</title><updated>2025-09-27T21:07:46.868339+00:00</updated><content>&lt;doc fingerprint="2e5a531d2b677598"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Typst: a possible LaTeX replacement&lt;/head&gt;
    &lt;quote&gt;LWN.net needs you!&lt;p&gt;Without subscribers, LWN would simply not exist. Please consider signing up for a subscription and helping to keep LWN publishing.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Typst is a program for document typesetting. It is especially well-suited to technical material incorporating elements such as mathematics, tables, and floating figures. It produces high-quality results, comparable to the gold standard, LaTeX, with a simpler markup system and easier customization, all while compiling documents more quickly. Typst is free software, Apache-2.0 licensed, and is written in Rust.&lt;/p&gt;
    &lt;head rend="h4"&gt;Desire for a LaTeX replacement&lt;/head&gt;
    &lt;p&gt;LaTeX is a document typesetting system built on the foundation of Donald Knuth's TeX. LaTeX has become the standard tool for the preparation of scholarly papers and books in several fields, such as mathematics and computer science, and widely adopted in others, such as physics. TeX and LaTeX, which predate Linux, are early free software success stories. The quality of TeX's (and therefore LaTeX's) output rivals the work of skilled hand typesetters for both text and mathematics.&lt;/p&gt;
    &lt;p&gt;Despite the acclaim earned by LaTeX, its community of users has been griping about it for years, and wondering aloud whether one day a replacement might arrive. There are several reasons for this dissatisfaction: the LaTeX installation is huge, compilation of large documents is not fast, and its error messages are riddles delivered by an infuriating oracle. In addition, any nontrivial customization or alteration to the program's behavior requires expertise in an arcane macro-expansion language.&lt;/p&gt;
    &lt;p&gt;Along with the griping came resignation: after decades of talk about a LaTeX replacement with nothing plausible on the horizon, and with the recognition that LaTeX's collection of specialized packages would take years to replace, it seemed impossible to dislodge the behemoth from its exalted position.&lt;/p&gt;
    &lt;head rend="h4"&gt;Introducing Typst&lt;/head&gt;
    &lt;p&gt;In 2019 two German developers, Laurenz Mädje and Martin Haug, decided to try to write a LaTeX replacement "just for fun". In 2022, Mädje wrote his computer science master's thesis about Typst. In March 2023, its first pre-release beta version was announced; a month later, semantic versioning was adopted with the release of v0.1.0. Typst is now at v.0.13.1 and shows 365 contributors on its GitHub repository.&lt;/p&gt;
    &lt;p&gt;I had been aware of this project for over a year but had not paid much attention, assuming it to be yet another attempt to supplant LaTeX that was doomed to fail. A rising chorus of enthusiasm among early adopters, and the beginnings of acceptance of Typst manuscripts by scholarly journals, made me curious enough to take the young project for a spin.&lt;/p&gt;
    &lt;p&gt;Typst is available as Rust source and as a compiled binary. To install, visit the releases page and download the appropriate archive. There are options for Linux, macOS and Windows; I used the precompiled Linux version for my testing.&lt;/p&gt;
    &lt;p&gt;The "typst" command accepts several subcommands. Entering "typst fonts" lists all of the usable fonts to be found in standard locations on the machine; nonstandard font directories can be added manually. In my case, Typst found all of my 476 fonts instantly; the only ones omitted were some ancient PostScript Type 1 fonts used by LaTeX. Users who have LaTeX installed will have a large collection of OpenType and TrueType math and text fonts on their machines; Typst can use all of these. But Typst will work fine without them, as the program has a small collection of fonts built in (try "typst fonts --ignore-system-fonts" to see them).&lt;/p&gt;
    &lt;p&gt;Two other subcommands to explore are "compile", which generates the output (PDF by default, with PDF/A, SVG, and PNG available, along with HTML under development) from a source file, and "watch" for interactive editing. The watch subcommand keeps a Typst process running that incrementally and automatically compiles the document to PDF in response to changes in the source. To use "typst watch" effectively, the screen should be divided into three windows: a small terminal window to monitor the typst output for error (or success) messages, the editing window, and an area for any PDF reader that automatically reloads the displayed document when it changes (many, such as my favorite, Sioyek, do this). The result is a responsive live preview, even of large documents, due to Typst's speed and incremental compilation. For example, Frans Skarman described his experience writing his doctoral thesis in Typst, and noted that he was able to enjoy nearly instant previews of content changes to the book-length document.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Typst improves on LaTeX&lt;/head&gt;
    &lt;p&gt;Typst output is quite close to that of LaTeX. It uses the same line-breaking algorithm developed by Donald Knuth and Michael Plass for TeX, so it creates nicely balanced paragraphs of regular text. Its mathematical typesetting algorithms are based closely on the TeX algorithms, and indeed mathematical rendering is nearly indistinguishable between the two systems.&lt;/p&gt;
    &lt;p&gt;Getting started with LaTeX can be confusing for newcomers, because it comes with several alternative "engines" reflecting the long and complex history of the project. These are the various binaries such as "pdflatex", "tex", "xelatex", "luatex", "lualatex", and more, each with somewhat different capabilities. For Typst there is only "typst".&lt;/p&gt;
    &lt;p&gt;Markup in Typst is less verbose and easier to read than in LaTeX. It dispenses with the plethora of curly brackets and backslashes littering LaTeX documents by adopting, for prose, syntax in the style of Markdown, and, for equations, a set of conventions designed for easy input. The fact that curly brackets and backslashes are awkward to type on German keyboards may have provided a little extra impetus for the developers to create an alternative markup system that doesn't require a forest of these symbols.&lt;/p&gt;
    &lt;p&gt;When users make syntax errors in markup or programming, inevitable even in Typst, the system presents them with another dramatic improvement over LaTeX (and TeX): error messages using colored glyphs that clearly point out exactly where the problem is. I've even discovered that Typst will save me from trying to run a syntactically correct infinite loop.&lt;/p&gt;
    &lt;p&gt;Here is a bit of Typst markup for a shopping list, with the resulting rendering to the right:&lt;/p&gt;
    &lt;quote&gt;= Shopping List == Vegetables + Broccoli + Asparagus (*fresh only*) + Plantains (_ripe and green_) == Booze + Rum - White - Dark + #underline[Good] gin&lt;/quote&gt;
    &lt;p&gt;The example gives a flavor of Typst's terse markup syntax. Headings are indicated with leading = signs. Automatically numbered lists are created by prepending + signs to items, and bulleted lists with - signs; lists can be nested. Delimiters are shown for bold text and italics. These are shortcuts, or markup syntax sugar, for Typst functions for transforming text. Not every function has a corresponding shortcut; in those cases one needs to call the function explicitly, as in the final item.&lt;/p&gt;
    &lt;p&gt;Typst input is always within one of three modes. Markup (text) mode is the default. The # sign preceding the function call in the last line of the example places Typst in "code mode". The "underline()" function accepts a number of keyword arguments that affect its behavior, and one trailing argument, in square brackets, containing the text that it modifies. In the example, we've stuck with the default behavior, but if we wanted, for example, a red underline, we could use "#underline(stroke: red)[Good] gin". Following the square-bracketed text argument, Typst returns to interpreting input in text mode.&lt;/p&gt;
    &lt;p&gt;Other functions produce output directly, rather than modifying a text argument. This bit of Typst input:&lt;/p&gt;
    &lt;quote&gt;#let word = "Manhattan" There are #str.len(word) letters in #word.&lt;/quote&gt;
    &lt;p&gt;produces the output (in typeset form) "There are 9 letters in Manhattan.". The "len()" function is part of the "str" module, so it needs the namespace.&lt;/p&gt;
    &lt;p&gt;Let's take a look at the LaTeX equivalent for the first half of the shopping list for comparison:&lt;/p&gt;
    &lt;quote&gt;\documentclass[12pt]{article} \begin{document} \section*{Shopping List} \subsection*{Vegetables} \begin{enumerate} \item Broccoli \item Asparagus ({\bfseries fresh only}) \item Plantains (\emph{ripe and green}) \end{enumerate} \end{document}&lt;/quote&gt;
    &lt;p&gt;The first two and the last line are boilerplate that is not required in Typst. The difference in verbosity level and ease of reading the source is clear.&lt;/p&gt;
    &lt;p&gt;The third Typst mode, in addition to markup and code, is math mode, delimited by dollar signs. This is also best illustrated by an example:&lt;/p&gt;
    &lt;quote&gt;$ integral_0^1 (arcsin x)^2 (dif x)/(x^2 sqrt(1-x^2)) = π ln 2 $&lt;/quote&gt;
    &lt;p&gt;When this is compiled by Typst, it produces the result shown below:&lt;/p&gt;
    &lt;p&gt;Those who've used LaTeX will begin to see from this example how math in Typst source is less verbose and easier to read than in LaTeX. Greek letters and other Unicode symbols can be used directly, as in modern LaTeX engines such as lualatex, which we looked at back in 2017, but with no imports required.&lt;/p&gt;
    &lt;p&gt;The advent of the LuaTeX and LuaLaTeX projects provided users who wanted to incorporate programming into their documents a more pleasant alternative to the TeX macro language. As powerful as the embedded Lua system is, however, it betrays its bolted-on status, requiring users to negotiate the interface between Lua data structures and LaTeX or TeX internals. In Typst, programming is thoroughly integrated into the system, with no seams between the language used for calculation and the constructs that place characters in the final PDF. Typst programs are invariably simpler than their LuaLaTeX equivalents. All authors using Typst will make at least some simple use of its programming language, as such basic necessities as changing fonts, or customizations such as changing the style of section headings, are accomplished by calling Typst functions.&lt;/p&gt;
    &lt;p&gt;The Typst language is somewhat similar to Rust, perhaps unsurprisingly. Most Typst functions are pure: they have no side effects and always produce the same result given the same arguments (aside from certain functions that mutate their arguments, such as array.push()). This aspect reduces the probability of difficult-to-debug conflicts among packages that plague LaTeX, and makes it easier to debug Typst documents.&lt;/p&gt;
    &lt;p&gt;Although Typst uses the same line-breaking algorithm as LaTeX, its internal approach to overall page layout is distinct. Some consequences are that Typst does a better job at handling movable elements such as floating figures, and can, for example, easily split large tables across page breaks, something that LaTeX struggles with even with specialized packages.&lt;/p&gt;
    &lt;head rend="h4"&gt;Typst drawbacks&lt;/head&gt;
    &lt;p&gt;Typst's page layout algorithm doesn't always permit the refinements that LaTeX is capable of. For example, Typst is not as good as LaTeX at avoiding widows and orphans. Another salient deficiency is Typst's relative lack of specialized packages, compared with the vast ecosystem produced by LaTeX's decades of community involvement. However, the relative ease of programming in Typst (and the well-organized and extensively commented underlying Rust code) suggests that this drawback may be remedied before a comparable number of decades have elapsed. Indeed, there are already over 800 packages available. Typst still cannot do everything that LaTeX can, but the breadth of its package collection is encouraging.&lt;/p&gt;
    &lt;p&gt;Almost no journals that provide LaTeX templates for submissions offer a Typst option, so physicists and mathematicians adopting Typst will need to find a way to convert their manuscripts. This is made easier for those who use Pandoc, as that conversion program handles Typst.&lt;/p&gt;
    &lt;p&gt;Another drawback is the difficulty of learning Typst. The official documentation is confusingly organized, with information scattered unpredictably among "Tutorial", "Reference", and "Guides" sections. Concepts are not always clearly explained, and sometimes not presented in a logical order. The manual is not keeping up with the rapid development of the program, and contains some out-of-date information and errors. None of this is surprising considering how quickly the project is moving, its early stage, and its small core team. A work-in-progress called the Typst Examples Book has appeared, which may be a better starting point than the official documentation.&lt;/p&gt;
    &lt;p&gt;There are other minor deficiencies compared with LaTeX, such as the inability to include PDF documents. Typst provides no analogue to LaTeX's parshape command, which lets authors mold paragraphs to, for example, wrap around complex illustrations. The situation is likely to change, however, as something like parshape is being considered for the future.&lt;/p&gt;
    &lt;p&gt;More serious is the possibility of breaking changes as the system evolves, always a risk of early adoption. I suspect, however, that these will require only minor edits to documents in most cases. Progress seems to be steady, rational, and responsive to user requests.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I'm using Typst in real work right now to write a physics paper. I will need to submit my manuscript using the journal's LaTeX template, but I'm taking advantage of Typst to make the entry of the paper's many equations simpler, and I'll transform the result to LaTeX with Pandoc without needing any manual adjustment. The tooling is excellent, as my preferred editor, Neovim, has support for the Tree-sitter incremental parser for Markdown and Typst, which provides syntax-aware highlighting and navigation of the source files. I use Typst's fast incremental compilation to get live feedback as I fiddle with my math markup.&lt;/p&gt;
    &lt;p&gt;I was skeptical when I downloaded Typst to try it out, but became enthusiastic within minutes, as I saw the first (of many) of its lovely error messages, and remained sanguine as I saw the quality of its output. I predict that Typst will eventually take the place of LaTeX. But even if that never comes to pass, it is a useful tool right now.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;GuestArticles&lt;/cell&gt;
        &lt;cell&gt;Phillips, Lee&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Sep 17, 2025 15:45 UTC (Wed) by spacefrogg (subscriber, #119608) [Link] (22 responses) There used to be the well-founded expectation from users that a typeset document did not change when processed by a future version of the processor, style or packages. In the long-run, this made learning and dealing with the languages intricacies and idiosyncrasies worthwhile. This has faded somewhat (looking at you, siunitx) in recent years since the first-generation contributors left the scene. I hope that the Typst maintainers and contributors understand this historic lesson as well. Also, TeX is a document processor able to document itself or at least its packages. And there is a reliable ecosystem for that as well (e.g. certain pressure on contributors to provide documentation along with code for acceptance). Posted Sep 17, 2025 17:07 UTC (Wed) by wtarreau (subscriber, #51152) [Link] (11 responses) Interesting because while that may be true in theory, it's precisely the opposite that made me abandon it over time. Trying to rebuild my old docs systematically resulted in cryptic errors. Looking on the net suggested that foobar.sty was replaced by somethingelse.sty which was close enough but required modifications etc. It happened to me several times to spend half a day updating a 5-year old manual to accommodate new packages. It might very well just be that some packages are less strict than the lower layers and that I hadn't been using state-of-the-art ones, but for the end user experience the problem is the same, a document you wrote doesn't build anymore spewing many errors. That happened to me with documents written between 1995 and 2000 roughly. Some packages were even related to how to deal with character encodings, which newer versions implemented more naturally but probably caused more difficulties to adapt to. I also remember some of article.sty no longer being compatible with the older one I used. I'm speaking about old memories, as it's been 20 years or so since I progressively stopped using it. It always made me sad because I loved the output quality which was super pleasant to read. Also I remember that newer versions were way simpler to install than the pre-2000 ones where you had to collect styles from everywhere and build your own packages from sources. Posted Sep 17, 2025 18:38 UTC (Wed) by ballombe (subscriber, #9523) [Link] (4 responses) Posted Sep 17, 2025 20:23 UTC (Wed) by NYKevin (subscriber, #129325) [Link] (3 responses) Posted Sep 18, 2025 12:35 UTC (Thu) by aragilar (subscriber, #122569) [Link] Posted Sep 19, 2025 18:05 UTC (Fri) by anton (subscriber, #25547) [Link] (1 responses) If I want to revise the paper (e.g., submit a revision of a rejected paper to a different conference), the original appearance is not desired, and usually I need to produce a different format, so it does not matter much if the original class and template no longer works. What matters is that I can easily copy my text to the new template. That is mostly easy, but recently I have had to deal with templates that want all kinds of meta-data, and place standard elements such as \title and \author in a non-standard location, which makes things somewhat time-consuming. But the main part of the paper can be reused and revised, with a formatting pass at the end. Concerning longevity, I have rarely had the need to process a really old work, but just to see how well it works, I have tried a thesis from 1990, and the main text works (graphics are separate, and I would have to invest more time to find out how they were built). I also tried papers from 1992 and 1993, and they compiled fine; the paper from 1992 contains Framemaker graphics, and I no longer have a way to convert that to Postscript, but fortunately I have the Postscript output; for one picture, the placement is slightly wrong, though. The 1993 paper looks fine. Maybe the advantage with these old papers is that there were no style/class files coming from the publication venue, so I just used article (or, for the thesis, report), and not many packages, either. Posted Sep 20, 2025 22:11 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Posted Sep 19, 2025 0:03 UTC (Fri) by jschrod (subscriber, #1646) [Link] (4 responses) But you still bring this up, 31 years after the upgrade - which was 5 years in the making before. From a developer point of view, this is a complaint about a development that happened 35 years ago. Are you aware that this statement is similar to "I will not use Wayland because I had to write XFree86 modline configs back then"? (Because, as you surely remember, this was even before the days of the X.org server with better autoconfiguration that now is considered obsolete.) Disclaimer: I was part of the team that introduced LaTeX 2e back in 1994. I am still connected to that work and to the people working on it, though I'm not an active developer anymore. Posted Sep 19, 2025 5:20 UTC (Fri) by wtarreau (subscriber, #51152) [Link] (3 responses) The thing is that when you don't use LaTeX often enough and each time you do it's difficult, then what remains of the experience is frustration. The frustration of not being able to reproduce a previous report that you spent a lot of time arranging, etc. When I was using it on a daily basis 30 years ago, I *loved* it. Never having to think about what the output would look like and just typing was really awesome and I haven't found anything getting close to that experience. And I'm still pleased to read papers written using it, which are instantly recognizable. I'm also a bit suspicious about tools that try to imitate it, because, as you say, it has accumulated decades of expertise in what it's doing, so users risk losing great stuff. It's very possible that forward compatibility has improved a lot since these experience, but due to these problems I got used to no longer using it. The rare times I need to write something with different fonts and sizes, I just write HTML and let the browser of the moment render it. It's a bit more painful but relies on a standard that's not going to disappear any time soon. Posted Sep 19, 2025 8:34 UTC (Fri) by anselm (subscriber, #2796) [Link] (2 responses) LaTeX is great if you're largely happy with what it does. If you need to bend it to your will to obtain a specific effect, that can easily become an exercise in frustration – fortunately now there are extension packages which will let you, e.g., control how chapter and section headings look like, which was something that in the 1990s required fairly arcane knowledge of the insides of LaTeX to change in even minor ways. Similarly, LaTeX input is reasonably straightforward to write once you've got the hang of it, but it is an absolute bear to parse if you want to process it with a tool that isn't LaTeX itself. TeX input, if anything, is worse. The main problem of the TeX and LaTeX ecosystem is that it is, to a large extent, based on ideas which were innovative in the 1980s, but the publishing world has continued turning in the meantime, and TeX's stability guarantee in particular, while commendable in principle, has largely prevented it from evolving along. When TeX was new, PostScript hadn't really been invented yet, PDF wasn't even on the horizon, font technology looked a lot different from what it does today, and Unicode wasn't a thing at all, but now there is no way around these developments. The solutions that Knuth and his colleagues came up with (DVI, Metafont, and so on) didn't catch on outside the TeX community, so TeX has been chasing what the rest of the world was doing in these areas, through non-standard variants such as eTeX, PDFTeX, LuaTeX, etc. It is true that it is perfectly possible, in 2025, to use LaTeX to typeset a PDF document with OpenType fonts based on UTF-8 encoded input, but this means you have to run a version of TeX that has special code extensions not necessarily found in other versions of TeX, using special LaTeX packages which may come bundled in a “batteries included” distribution such as TeXLive but are not actually part of LaTeX itself. This fragmentation tends to make life with (La)TeX more difficult. Also, nowadays people expect to be able to write a document in a single source format and render it, without source changes, in wildly different output formats such as HTML and PDF, in a way that avails itself of the specific advantages of the format in question, and TeX/LaTeX doesn't really have a straightforward and obvious answer to that requirement like Markdown, Pandoc, or Sphinx (to name but a few examples) do. I've been a TeX and LaTeX user for 40 years now but I'm looking at Typst with considerable interest. Posted Sep 19, 2025 12:21 UTC (Fri) by dskoll (subscriber, #1630) [Link] I solved this problem (with a little bit of pain) for my 600-page set of manuals I mentioned earlier. I wanted PDF output as well as HTML output. There's a pretty nice program called Yes, it was a bit annoying to set up, but once I had my Makefile written, it worked beautifully. Posted Sep 27, 2025 9:26 UTC (Sat) by Delio (guest, #179554) [Link] Posted Sep 27, 2025 11:59 UTC (Sat) by simlo (guest, #10866) [Link] Posted Sep 17, 2025 20:51 UTC (Wed) by warrax (subscriber, #103205) [Link] (9 responses) I do think you're correct that backward[1] compatibility *is* important, but the LaTeX ecosystem as a whole isn't necessarily great at that... it very much depends on what packages you use. [1] Future versions being able to process old code/documents is usually referred to as 'backward' compatibility. Posted Sep 17, 2025 21:24 UTC (Wed) by dskoll (subscriber, #1630) [Link] (4 responses) Hmm... I have three manuals I started writing 20 years ago and continued writing through 2018; they total almost 600 pages and still build perfectly fine on whatever version of LaTeX ships with Debian 13. I don't go crazy with untested or new packages, though... all of the packages I use have been around for a long time and are very stable. Posted Sep 18, 2025 2:18 UTC (Thu) by Cyberax (✭ supporter ✭, #52523) [Link] (3 responses) Posted Sep 18, 2025 9:38 UTC (Thu) by paulj (subscriber, #341) [Link] (1 responses) Posted Sep 18, 2025 13:02 UTC (Thu) by pizza (subscriber, #46) [Link] ...Even on the *same* PC, with the *same* version of Word, "rendering the same" was not guaranteed. (Back in the day, I recall that merely changing the printer driver was sufficient to cause the document to paginate differently..) Posted Sep 18, 2025 17:12 UTC (Thu) by hholzgra (subscriber, #11737) [Link] WinWord could already no longer process it properly when WinWord 6.0; the version right after 2.0a, came out. The LaTeX version worked all the way until late 1999, when due to a series of mishaps the source was lost and I was left with only the PDF result, which I still have. (Generating PDF from Word documents on the other hand was basically unheard of back in the 1990s ...) I also still have a few smaller texts I've written after the 1999 backup disaster, and these I can still process using current LaTeX versions. Posted Sep 17, 2025 21:26 UTC (Wed) by iabervon (subscriber, #722) [Link] (2 responses) Posted Sep 18, 2025 20:54 UTC (Thu) by SLi (subscriber, #53131) [Link] If only the underlying language was something modern and somehow modular and encapsulated instead of a weird macro mess with not-really-scopes. Maybe I never got deep enough into it to really appreciate its cleverness (now I do appreciate that it's 50 years old), but in my experience it doesn't exactly take just "not thinking" to not break something by an unrelated change. Posted Sep 18, 2025 20:58 UTC (Thu) by ejr (subscriber, #51652) [Link] There was ConTeXt as well. I'm not sure of its status. And "worse is better" seems to have been a thing for me this week in many venues. Posted Sep 18, 2025 20:49 UTC (Thu) by SLi (subscriber, #53131) [Link] I think one big problem that I've seen in my field of CS is that people have become used to the output of LaTeX to the extent that everything else looks "unprofessional" to them merely by virtue of being different, even if it fixes some real annoyance in LaTeX output. So while I still do my maths and typesetting often in LaTeX, I'm actually happy that the modern practitioners are refusing to take that route, even if it means them using Word. We shouldn't teach people to rely on stuff built on MS-DOS and Cobol either, even if the best typesetting tool remains some obscure DOS executable. Posted Sep 17, 2025 18:25 UTC (Wed) by rogerwhittaker (subscriber, #39354) [Link] (6 responses) Posted Sep 17, 2025 19:35 UTC (Wed) by spacefrogg (subscriber, #119608) [Link] (5 responses) Additionally, Typst documents are closer in code style to plain TeX than LaTeX with its verbose Pascal'ish blocks. Anecdote: I had to write a letter, printed, on paper, just the other day. Had a go at using Typst and it was done in 20 minutes incl. downloading the letter package, initialising the document boilerplate and understanding what to change where. It looks like simple tasks are actually simple to do. Posted Sep 18, 2025 10:10 UTC (Thu) by epa (subscriber, #39769) [Link] (4 responses) Posted Sep 18, 2025 12:41 UTC (Thu) by smoogen (subscriber, #97) [Link] Posted Sep 19, 2025 9:19 UTC (Fri) by taladar (subscriber, #68407) [Link] Posted Sep 26, 2025 19:38 UTC (Fri) by bluss (guest, #47454) [Link] Posted Sep 27, 2025 17:01 UTC (Sat) by tjbc (guest, #179557) [Link] Posted Sep 18, 2025 9:21 UTC (Thu) by al4711 (subscriber, #57932) [Link] Posted Sep 18, 2025 11:36 UTC (Thu) by ceplm (subscriber, #41334) [Link] (2 responses) Posted Sep 19, 2025 18:22 UTC (Fri) by anton (subscriber, #25547) [Link] (1 responses) I looked up when Lout was released, and that was in 1991 (with work starting in 1994). The most recent release is from 2023, but apparently that just made it easier to build, so it's not sure if it is still being maintained. But then, if it works, do you really need any other maintenance? Posted Sep 19, 2025 20:15 UTC (Fri) by ceplm (subscriber, #41334) [Link] Posted Sep 18, 2025 17:02 UTC (Thu) by hholzgra (subscriber, #11737) [Link] (3 responses) Lower entry barrier for sure, but always having a taste of "Those who do not understand XXX have to reinvent it ... poorly" (CMake vs. Autotools rings a similar bell, although in a slightly different field) I'm afraid that once again we are forgetting about quite a few things that were already solved in the past by switching to such new solutions carrying less of a history with them ... Posted Sep 20, 2025 3:30 UTC (Sat) by mathstuf (subscriber, #69389) [Link] (2 responses) *sigh* Note that one of the "sparks" for CMake was Windows (as in MSVC, not MinGW-on-Cygwin or MSYS2) support. Something Autotools still does not have today. Posted Sep 20, 2025 10:44 UTC (Sat) by hholzgra (subscriber, #11737) [Link] (1 responses) That is not my issue with CMake, it is rather that it "forgot" about things like "make distcheck" and quite a few other things that autotools had solved just fine for ages. So while it supports other build systems besides good old Make, I'd say that at least on the Unixoid side Makefiles are still the predominant backend being used. And the Makefiles it generates are sub par compared to what automake generates. That's my "reinvent it ... badly" pain point with CMake. Posted Sep 20, 2025 23:49 UTC (Sat) by mathstuf (subscriber, #69389) [Link] I don't think `distcheck` is all that important for CMake because…the source tree *is* the tarball; there's no intermediate step which bundles everything that needs re-verified &amp;gt; I'd say that at least on the Unixoid side Makefiles are still the predominant backend being used I'd be very surprised if Ninja were not the most popular generator these days. I believe Fedora has switched its default generator at this point at least? I know Visual Studio's integration prefers it. &amp;gt; And the Makefiles it generates are sub par compared to what automake generates. Oh, I don't think anyone is going to argue that CMake's Unix Makefiles generator is anywhere near optimal. There are a number of reasons for it. The most important is that autotools and CMake are different build *systems* even though they do share support for a common build *tool* as an output. Because CMake also supports IDEs with…rather restrictive ideas on what is possible, CMake's model for the build is quite different than autotools'. The build tool is easy to define: it is a build graph executor. Make, ninja, msbuild, just, rake, build2, boost.build, bjam, scons, tup, etc. are all "build tools" that execute a build graph. The build *system* is where things get interesting (for me). Some build tools are also build systems: build2, boost.build, scons, tup. This is the layer which defines things like "what is a target" and "how do targets relate". autotools and CMake both execute at this level and "compile" their input language to something the target build tool understands. This does not mean that build systems expose everything that the build tool supports (e.g., CMake does not allow users to write their own ninja rule statements because…what does that even mean for its other outputs). Of course, some build tool support may have additional features as long as it doesn't conflict with the overall model of the build system itself. For example, CMake's Ninja generator can drop some dependencies other generators need to support the semantics CMake guarantees if it can prove to itself that they're satisfied in other ways. Now, there are ideas to rewrite CMake's Makefiles output to be more like Ninja: one global graph without per-directory entry points and without the per-target subgraph recursive instance. This would allow the Makefiles generator to do the same pruning of unnecessary dependencies that Ninja does. But because it was following the IDE model of "the build graph is a series of targets; each target's subgraph is an independent entity", we have the non-optimal behavior of "if A links to B, B must link before anything in A starts" because that is how CMake guarantees things like generated headers in B are available when A starts compiling[1]. So, in short, I think CMake tool a more general approach to build systems and its Makefiles output is suboptimal because of that. But because we now also support the Ninja generator which is, IMO, strictly better (unless one needs a job server for nested builds), restricting the scope to the Makefiles output of each is not a fair comparison. [1] The link dependency can be dropped if A's custom command dependencies are a superset of B's custom command dependencies: any header or whatever B ends up generating is already in A's graph. Posted Sep 19, 2025 0:28 UTC (Fri) by jschrod (subscriber, #1646) [Link] (10 responses) Just in the last few years, a major undertaking starts to produce tangible results: LaTeX Tagged PDF https://latex3.github.io/tagging-project/ With it, one can prepare barrier free PDFs with acceptable effort - *even for math*. I don't know if any other system that provides this level of capability. This is the stuff that gets developed in established FOSS ecosystems by people who work on typesetting systems since decades. Disclaimer: I'm personally involved in the LaTeX project, though I'm not a developer any more. Posted Sep 19, 2025 2:46 UTC (Fri) by intelfx (guest, #130118) [Link] (6 responses) How does this help with UX of *writing* in LaTeX, which seems to be the major issue driving the competing developments (and specifically the subject of the article)? &amp;lt;...&amp;gt; I understand that LaTeX is something you relate to, but your response reads somewhat like this: - Project A sucks at ABC, and I'm badly tired of it, so I don't wish to use project A anymore. I'm looking forward to possible replacements. Posted Sep 19, 2025 9:52 UTC (Fri) by Wol (subscriber, #4433) [Link] This seems to be a major blinker problem in FOSS. My brother's comments about his experience of Emacs at Uni 40 years ago are classic - when he first started he thought it was awful, impossible to use, way too complicated. Then after a year or two, once he'd mastered it, he couldn't imagine using anything else. The reason Word conquered the world (and the reason I hate it) is because it was aimed at people who COULDN'T TYPE - the managerial guys who had professional typists, the couch potatoes who didn't do much, etc etc. WordPerfect - which I took to like a duck to water because it (on the surface) mimicked a typewriter - which failed in large part due to MS's dirty tricks - couldn't compete in the battle for the minds of the people with the purse strings, even though it was a much better professional solution. FLOSS so often is such a super swiss army knife that anybody new approaching it is left thinking "but how does it fix MY problem ???". I use lilypond, and it's incredibly powerful, but the learning curve to access that power is almost impenetrable (it's driven by a variant of Lisp!). Cheers, Posted Sep 19, 2025 10:55 UTC (Fri) by smitty_one_each (subscriber, #28989) [Link] (4 responses) Posted Sep 19, 2025 11:21 UTC (Fri) by leephillips (subscriber, #100450) [Link] (3 responses) Posted Sep 19, 2025 12:41 UTC (Fri) by smitty_one_each (subscriber, #28989) [Link] &amp;gt; the learning curve to access that power is almost impenetrable One of my pet cliches is: "Everything is easy, when you know how to do it." Overleaf provides a gentle introduction to LaTeX. Posted Sep 19, 2025 13:17 UTC (Fri) by paulj (subscriber, #341) [Link] I'm a big fan of Lyx as a great accessible and fairly user-friendly UI for writing documents to eventually typeset with LaTeX. I've used it for my own dissertation and it made writing so much easier. It's also customisable. I ended up making a few of my own definitions for things, with their own menu entries - which was just a matter of adding some UI definition files. My father went to uni after retirement and (eventually) got a masters. He used to have endless issues with his masters dissertation in MS Word, with the format going screw and *especially* the required citations being very hard to manage and constantly getting messed up. I was constantly having to go over to him to try help him with his MS Word processing issues. In the end, I switched him over to Lyx. Showed him how to make chapters, sections and sub-sections, and insert citations. Told him just to write, and that the formatting would largely take care of itself. I helped with proofing at the end and help with inserting figures and illustrations, but it saved *both of us* a lot of hair-pulling and time. My dad generally does not get on with computers. He gets very frustrated with complex programmes, with states affecting things he can't see/understand. He became a big of fan Lyx however, for the way it just let him write and generally staying out of the way, while keeping track of all the citations and layout for him, and producing a beautiful doc at the end thanks to LaTeX. Lyx is a _great_ bit of software! Posted Sep 27, 2025 19:46 UTC (Sat) by callegar (guest, #16148) [Link] A pain point is that the free tier is insufficient even for the simplest document with the LuaLaTeX engine. Posted Sep 19, 2025 19:04 UTC (Fri) by notriddle (subscriber, #130608) [Link] (1 responses) And Typst does this: In TeX's defense, it's not the worst system I've ever dealt with, and a lot of that spew can be cleaned up by just putting it behind a --verbose flag, but the biggest, hardest-to-fix problem is here: Typest's equivalent has a line number. It also actually matches what was written. Is that fixable without breaking changes to the macro system? Posted Sep 22, 2025 18:49 UTC (Mon) by jschrod (subscriber, #1646) [Link] In all other error messages TeX's error messages consist of two lines. The first line has the line number and all characters that are read up to the error, the second line has the characters that are still to be processed. But that is actually a bynote. You wrote &amp;gt; tagged pdf or other niche features Since journals (especial scientific journals that Lee wrote about) and other publishers increasingly demand the production of barrier free PDFs for online publication, Tagged PDF is not a niche feature, IMNSHO. Customers of mine currently pour 6-digit numbers of Euro in creation of such files. For private production it doesn't matter -- but for publication, it will soon be a must-have. Posted Sep 27, 2025 9:11 UTC (Sat) by Delio (guest, #179554) [Link] Posted Sep 19, 2025 6:06 UTC (Fri) by yashi (subscriber, #4289) [Link] (2 responses) Meander seems to do it: https://github.com/typst/packages/pull/3065 Posted Sep 19, 2025 11:28 UTC (Fri) by leephillips (subscriber, #100450) [Link] (1 responses) This was first released while we were still working on the article. In other words, my prediction is coming true: that packages for Typst will emerge rapidly, because it’s easy (easier) to program in. Posted Sep 19, 2025 15:27 UTC (Fri) by adnl (subscriber, #179418) [Link] Posted Sep 27, 2025 12:04 UTC (Sat) by norbusan (guest, #10100) [Link] (2 responses) As wtih coreutils, as with several other places, first of all "I'm so shiny" (thanks Moana!), but reality is different. Posted Sep 27, 2025 13:26 UTC (Sat) by Delio (guest, #179554) [Link] Posted Sep 27, 2025 16:06 UTC (Sat) by Lonjil (guest, #152573) [Link] ? Almost everything on that page is basic typesetting, and most of the stuff that isn't is just about levering funny PDF features. Have you actually looked at Typst and its package ecosystem? Posted Sep 27, 2025 19:59 UTC (Sat) by callegar (guest, #16148) [Link] Images on this issue report show the problem very well: https://github.com/typst/typst/issues/1021#issuecomment-1... May seem a niche case, but it is in fact much easier to encounter it than not. And if you do, you bounce back to LaTeX, even if the problem is not really Typst fault, but the fact that in OpenType there is no provision for treating these situations and Typst bases its rendering on what the fonts provide, without the heuristics that are present in LaTeX from a time when fonts where much more basic things. Incidentally, also modern LaTeX engines have some problems with OpenType, particularly in managing line breaks with hyphenation and ligatures. Using LuaLaTeX with the "harfbuzz" renderer that is a standard in many applications and makes document compilation much faster wrt the specialized renderer that LuaLaTeX uses as a default, leads to loss of many hyphenation points: see https://github.com/latex3/luaotfload/wiki/Comparing-the-m... and https://github.com/latex3/luaotfload/issues/152#issuecomm... for more details. &lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head/&gt; Yes, you get a class file, and a template with \usepackage invocations (or they are in the class file), and if you want to produce the exact same output, then yes, you may need to keep the old packages around. But in that scenario I can just keep the resulting output (e.g., a PDF) around. &lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;quote&gt;When I was using it on a daily basis 30 years ago, I *loved* it. Never having to think about what the output would look like and just typing was really awesome and I haven't found anything getting close to that experience. And I'm still pleased to read papers written using it, which are instantly recognizable. I'm also a bit suspicious about tools that try to imitate it, because, as you say, it has accumulated decades of expertise in what it's doing, so users risk losing great stuff.&lt;/quote&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;quote&gt;Also, nowadays people expect to be able to write a document in a single source format and render it, without source changes, in wildly different output formats such as HTML and PDF, in a way that avails itself of the specific advantages of the format in question, and TeX/LaTeX doesn't really have a straightforward and obvious answer to that requirement[...]&lt;/quote&gt;&lt;code&gt;htlatex&lt;/code&gt; that does a creditable job of generating HTML, and then I post-processed it to (eg) replace the generated images with the original source images so figures were of higher quality.  I also defined a few conditional macros that inserted links to training videos in certain spots... something you can't really do with PDF.

&lt;head&gt;Inclusion of PDF files has been implemented&lt;/head&gt;&lt;head&gt;To become success story&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;About the compatibility story...&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Another project with similar aims&lt;/head&gt;&lt;head&gt;Very friendly and helpful Community&lt;/head&gt;&lt;head&gt;Lout&lt;/head&gt;&lt;head/&gt; Yes, when I read the article, I remembered Lout; a collegue advocated that in the 1990s. And already at that time it was obvious that TeX was a good typesetting engine, but a badly designed programming language, and LaTeX inherited this. Nevertheless, LaTeX has a big community behind it, and obviously Lout was unable to overcome the network effects coming from that. Will it be different for Typst or other contenders? Would it help if they built on each other rather than starting from scratch? &lt;head&gt;Lout&lt;/head&gt;&lt;head&gt;Lout&lt;/head&gt;&lt;head&gt;Bad feeling ...&lt;/head&gt;&lt;head&gt;Bad feeling ...&lt;/head&gt;&lt;head&gt;Bad feeling ...&lt;/head&gt;&lt;head&gt;Bad feeling ...&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;lb/&gt; This is bound to be incorporated into the next TeX-Live release and thus will appear in all major Linux distributions in due course.&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;lb/&gt; &amp;gt; This is bound to be incorporated into the next TeX-Live release and thus will appear in all major Linux distributions in due course.&lt;lb/&gt; - But project A is the best at XYZ, so this proves we are better than project B!&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head/&gt; If Typst manages to steal mindshare from LaTeX, I doubt it'll have much to do with tagged pdf or other niche features. It'll happen because, if I forget a closing brace, pdflatex does this: &lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;code&gt;This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022/Debian) (preloaded format=pdflatex)
 restricted \write18 enabled.
entering extended mode
(./t.latex
LaTeX2e &amp;lt;2022-11-01&amp;gt; patch level 1
L3 programming layer &amp;lt;2023-01-16&amp;gt;
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2022/07/02 v1.4n Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size12.clo))
(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-pdftex.def)
(./t.aux))
Runaway argument?
{ripe and green) \end {enumerate} \end {document} \par 
! File ended while scanning use of \emph .
&amp;lt;inserted text&amp;gt;
                \par 
&amp;lt;*&amp;gt; t.latex
           
? &lt;/code&gt;&lt;code&gt;error: unclosed delimiter
   ┌─ t.typst:14:12
   │
14 │ + #underline[Good gin  
   |             ^&lt;/code&gt;&lt;code&gt;{ripe and green) \end {enumerate} \end {document} \par &lt;/code&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Fully Tagged PDF (even for math) is in the works for LaTeX&lt;/head&gt;&lt;head&gt;Meander for parshape&lt;/head&gt;&lt;head&gt;Meander for parshape&lt;/head&gt;&lt;head&gt;Meander for parshape&lt;/head&gt;&lt;head&gt;Bad comparison&lt;/head&gt;&lt;head&gt;Bad comparison&lt;/head&gt;&lt;head&gt;Bad comparison&lt;/head&gt;&lt;head&gt;Italics correction&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lwn.net/Articles/1037577/"/><published>2025-09-27T07:31:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45394642</id><title>Ishkur's Guide to Electronic Music</title><updated>2025-09-27T21:07:46.728490+00:00</updated><link href="http://music.ishkur.com/"/><published>2025-09-27T10:38:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45395133</id><title>Why We Think</title><updated>2025-09-27T21:07:46.595445+00:00</updated><content>&lt;doc fingerprint="9c59cbb839924e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.&lt;/p&gt;
    &lt;p&gt;Test time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.&lt;/p&gt;
    &lt;p&gt;The core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for "What's 12345 times 56789?". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :&lt;/p&gt;
    &lt;p&gt;Fast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.&lt;/p&gt;
    &lt;p&gt;Slow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.&lt;/p&gt;
    &lt;p&gt;Because System 1 thinking is fast and easy, it often ends up being the main decision driver, at the cost of accuracy and logic. It naturally relies on our brain’s mental shortcuts (i.e., heuristics) and can lead to errors and biases. By consciously slowing down and taking more time to reflect, improve and analyze, we can engage in System 2 thinking to challenge our instincts and make more rational choices.&lt;/p&gt;
    &lt;p&gt;One view of deep learning, is that neural networks can be characterized by the amount of computation and storage they can access in a forward pass, and if we optimize them to solve problems using gradient descent, the optimization process will figure out how to use these resources–they’ll figure out how to organize these resources into circuits for calculation and information storage. From this view, if we design an architecture or system that can do more computation at test time, and we train it to effectively use this resource, it’ll work better.&lt;/p&gt;
    &lt;p&gt;In Transformer models, the amount of computation (flops) that the model does for each generated token is roughly 2 times the number of parameters. For sparse models like mixture of experts (MoE), only a fraction of the parameters are used in each forward pass, so computation = 2 * parameters / sparsity, where sparsity is the fraction of experts active.&lt;/p&gt;
    &lt;p&gt;On the other hand, CoT enables the model to perform far more flops of computation for each token of the answer that it is trying to compute. In fact, CoT has a nice property that it allows the model to use a variable amount of compute depending on the hardness of the problem.&lt;/p&gt;
    &lt;p&gt;A classic idea in machine learning is to define a probabilistic model with a latent (hidden) variable $z$ and a visible variable $y$, where $y$ is given to our learning algorithm. Marginalizing (summing) over the possible values of the latent variable allows us to express a rich distribution over the visible variables, $P(y) = \sum_{z \sim P(z)} P(y \mid z)$. For example, we can model the distribution over math problems and solutions by letting $x$ denote a problem statement, $y$ be ground truth answer or proof, and $z$ as a free-form thought process that leads to the proof. The marginal probability distribution to optimize would be $P(y \mid x) = \sum_{z \sim p(z\mid x)} P(y \mid x, z)$&lt;/p&gt;
    &lt;p&gt;The latent variable perspective is particularly useful for understanding methods that involve collecting multiple parallel CoTs or searching over the CoT–these algorithms can be seen as sampling from the posterior $P(z \mid x, y)$. This view also suggests the benefits of using the log loss $\log P(y \mid x)$ as the target objective to optimize, as the log loss objective has been so effective in pretraining.&lt;/p&gt;
    &lt;p&gt;The strategy of generating intermediate steps before generating short answers, particularly for math problems, was explored by Ling, et al. 2017, who introduced the AQUA-RAT dataset, and then expanded by Cobbe et al. 2021, who introduced the Grade School Math (GSM) dataset. Cobbe et al. train a generator with supervised learning on human-written solutions and verifiers that predict the correctness of a candidate solution; they can then search over these solutions. Nye et al. (2021) experimented with intermediate thinking tokens as “scratchpads” and Wei et al. (2022) coined the now-standard term chain-of-thought (CoT).&lt;/p&gt;
    &lt;p&gt;Early work on improving CoT reasoning involved doing supervised learning on human-written reasoning traces or model-written traces filtered for answer correctness, where the latter can be seen as a rudimentary form of reinforcement learning (RL). Some other work found that one could significantly boost math performance of instruction tuned models by prompting them appropriately, with "think step by step" (Kojima et al. 2022) or more complex prompting to encourage the model to reflect on related knowledge first (Yasunaga et al. 2023).&lt;/p&gt;
    &lt;p&gt;Later work found that the CoT reasoning capabilities can be significantly improved by doing reinforcement learning on a dataset of problems with automatically checkable solutions, such as STEM problems with short answers, or coding tasks that can be checked with unit tests (Zelikman et al. 2022, Wang et al., 2023, Liu et al., 2023). This approach rose to prominence with the announcement of o1-preview, o3, and the R1 tech report (DeepSeek-AI, 2025), which showed that a simple recipe where a policy gradient algorithm could lead to strong performance.&lt;/p&gt;
    &lt;p&gt;The fundamental intent of test-time compute is to adaptively modify the model’s output distribution at test time. There are various ways of utilizing test time resources for decoding to select better samples and thus alter the model’s predictions towards a more desired distribution. Two main approaches for improving the decoding process are parallel sampling and sequential revision.&lt;/p&gt;
    &lt;p&gt;Parallel sampling generates multiple outputs simultaneously, meanwhile providing guidance per step with process reward signals or using verifiers to judge the quality at the end. It is the most widely adopted decoding method to improve test time performance, such as best-of-$N$ or beam search. Self-consistency (Wang et al. 2023) is commonly used to select the answer with majority vote among multiple CoT rollouts when the ground truth is not available.&lt;/p&gt;
    &lt;p&gt;Sequential revision adapts the model’s responses iteratively based on the output in the previous step, asking the model to intentionally reflect its existing response and correct mistakes. The revision process may have to rely on a fine-tuned model, as naively relying on the model’s intrinsic capability of self-correction without external feedback may not lead to improvement (Kamoi et al. 2024, Huang et al. 2024).&lt;/p&gt;
    &lt;p&gt;Parallel sampling is simple, intuitive and easier to implement, but bounded by the model capability of whether it can achieve the correct solution in one-go. Sequential explicitly asks the model to reflect on mistakes but it is slower and requires extra care during implementation as it does run the risk of correct predictions being modified to be incorrect or introducing other types of hallucinations. These two methods can be used together. Snell et al. (2024) showed that easier questions benefit from purely sequential test-time compute, whereas harder questions often perform best with an optimal ratio of sequential to parallel compute.&lt;/p&gt;
    &lt;p&gt;Given a generative model and a scoring function that we can use to score full or partial samples, there are various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive, spending more sampling computation on more promising parts of the solution space.&lt;/p&gt;
    &lt;p&gt;Beam search maintains a set of promising partial sequences and alternates between extending them and pruning the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al. 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own generated reasoning step is correct, formatted as a multiple-choice question and found that per-step self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides, during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to determine how much each node should be expanded at each depth during beam search, according to the softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the difference between when the rationales is included in the context vs not. At inference time, RATIONALYST provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).&lt;/p&gt;
    &lt;p&gt;Interestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit zero-shot or few-shot prompting. Wang &amp;amp; Zhou (2024) discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or by prompting the model further with "So the answer is". The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.&lt;/p&gt;
    &lt;p&gt;If the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al. 2023).&lt;/p&gt;
    &lt;p&gt;Self-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\theta(y \mid y_0, x)$ given a fixed generator model $P_0(y_0 \mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):&lt;/p&gt;
    &lt;p&gt;Self-correction learning first generates first generates multiple outputs per prompt in the data pool;&lt;/p&gt;
    &lt;p&gt;then create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).&lt;/p&gt;
    &lt;p&gt;These pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\text{Similarity}(y, y’)$ to train the corrector model.&lt;/p&gt;
    &lt;p&gt;To encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.&lt;/p&gt;
    &lt;p&gt;Recursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do both generation and self-correction.&lt;/p&gt;
    &lt;p&gt;SCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage the model to do self-correction by producing better answers at the second attempt than the one created at the first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and stage 2 further improves the results.&lt;/p&gt;
    &lt;p&gt;There’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports from DeepSeek.&lt;/p&gt;
    &lt;p&gt;DeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training, enabling R1 to be good at both reasoning and non-reasoning tasks.&lt;/p&gt;
    &lt;p&gt;Cold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. Without this step, the model has issues of poor readability and language mixing.&lt;/p&gt;
    &lt;p&gt;Reasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:&lt;/p&gt;
    &lt;p&gt;Format rewards: The model should wrap CoTs by &amp;lt;thinking&amp;gt; ... &amp;lt;/thinking&amp;gt; tokens.&lt;/p&gt;
    &lt;p&gt;Accuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate whether test cases pass.&lt;/p&gt;
    &lt;p&gt;Rejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA, and self-cognition, to retrain DeepSeek-V3-Base.&lt;/p&gt;
    &lt;p&gt;Filter out CoTs with mixed languages, long paragraphs, and code blocks.&lt;/p&gt;
    &lt;p&gt;Include non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.&lt;/p&gt;
    &lt;p&gt;For certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by prompting. But for simpler queries like “hello”, CoT is not needed.&lt;/p&gt;
    &lt;p&gt;Then fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.&lt;/p&gt;
    &lt;p&gt;The final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving helpfulness, harmlessness and reasoning.&lt;/p&gt;
    &lt;p&gt;Interestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge, referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them. Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on math problems, as well as the emergent “aha moment”.&lt;/p&gt;
    &lt;p&gt;The DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also failed due to the large search space for language model tokens, in comparison to, say, chess; and training the fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide unique insights and we would like to encourage the research community to share more about what did not work out.&lt;/p&gt;
    &lt;p&gt;During the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code or running mathematical calculations. Offloading that part of reasoning components into an external code interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution (Shinn, et al. 2023).&lt;/p&gt;
    &lt;p&gt;ReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of reasoning traces, such that reasoning paths can incorporate external knowledge.&lt;/p&gt;
    &lt;p&gt;o3 &amp;amp; o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.&lt;/p&gt;
    &lt;p&gt;Deep learning models are often treated as black boxes and various interpretability methods have been proposed. Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its actions. Second, it can help us determine whether the model is using a sound process to compute its answers. Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal process visible in natural language. This interpretability, however, rests on the assumption that the model truthfully describes its internal thought processes.&lt;/p&gt;
    &lt;p&gt;Recent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense intuitively, because thinking for longer should be especially useful when the model is presented with an unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make sense of the strange situation it’s been presented with.&lt;/p&gt;
    &lt;p&gt;Intuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples may contain mistakes. Thus we cannot by default assume CoT is always faithful .&lt;/p&gt;
    &lt;p&gt;Lanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA, MMLU, ARC Challenge, TruthfulQA, HellaSwag):&lt;/p&gt;
    &lt;p&gt;Mistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or language templates in the formation of CoT.&lt;/p&gt;
    &lt;p&gt;Mistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may suffer performance drop slightly when compared to no CoT.&lt;/p&gt;
    &lt;p&gt;Mistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting accuracy gains do not rely on human-readable reasoning.&lt;/p&gt;
    &lt;p&gt;Interestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not always increase with model size on multiple choice questions, but does increase with model size on addition tasks, implying that thinking time matters more for complex reasoning tasks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://lilianweng.github.io/posts/2025-05-01-thinking/"/><published>2025-09-27T12:27:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45395957</id><title>The Postmark backdoor that’s downloading emails</title><updated>2025-09-27T21:07:46.318314+00:00</updated><content>&lt;doc fingerprint="66d1851b9b70ae03"&gt;
  &lt;main&gt;
    &lt;p&gt;You know MCP servers, right? Those handy tools that let your AI assistant send emails, run database queries, basically handle all the tedious stuff we don't want to do manually anymore. Well, here's the thing not enough people talk about: we're giving these tools god-mode permissions. Tools built by people we've never met. People we have zero way to vet. And our AI assistants? We just... trust them. Completely.&lt;/p&gt;
    &lt;p&gt;Which brings me to why I'm writing this. &lt;code&gt;postmark-mcp&lt;/code&gt; - downloaded 1,500 times every single week, integrated into hundreds of developer workflows. Since version &lt;code&gt;1.0.16&lt;/code&gt;, it's been quietly copying every email to the developer's personal server. I'm talking password resets, invoices, internal memos, confidential documents - everything.&lt;/p&gt;
    &lt;p&gt;This is the worldâs first sighting of a real world malicious MCP server. The attack surface for endpoint supply chain attacks is slowly becoming the enterpriseâs biggest attack surface.&lt;/p&gt;
    &lt;head rend="h2"&gt;Soâ¦ What Did Our Risk Engine Detect?&lt;/head&gt;
    &lt;p&gt;Here's how this whole thing started. Our risk engine at Koi flagged &lt;code&gt;postmark-mcp&lt;/code&gt; when version &lt;code&gt;1.0.16&lt;/code&gt; introduced some suspicious behavior changes. When our researchers dug into it, like we do to any malware our risk engine flags, what we found was very disturbing.&lt;/p&gt;
    &lt;p&gt;On paper, this package looked perfect. The developer? Software engineer from Paris, using his real name, GitHub profile packed with legitimate projects. This wasn't some shady anonymous account with an anime avatar. This was a real person with a real reputation, someone you'd probably grab coffee with at a conference.&lt;/p&gt;
    &lt;p&gt;For 15 versions - FIFTEEN - the tool worked flawlessly. Developers were recommending it to their teams. "Hey, check out this great MCP server for Postmark integration." It became part of developerâs daily workflows, as trusted as their morning coffee.&lt;/p&gt;
    &lt;p&gt;Then version 1.0.16 dropped. Buried on line 231, our risk engine found this gem:&lt;/p&gt;
    &lt;p&gt;One single line. And boom - every email now has an unwanted passenger.&lt;/p&gt;
    &lt;p&gt;Here's the thing - there's a completely legitimate GitHub repo with the same name, officially maintained by Postmark (ActiveCampaign). The attacker took the legitimate code from their repo, added his malicious BCC line, and published it to npm under the same name. Classic impersonation.&lt;/p&gt;
    &lt;p&gt;Look, I get it. Life happens. Maybe the developer hit financial troubles. Maybe someone slid into his DMs with an offer he couldn't refuse. Hell, maybe he just woke up one day and thought "I wonder if I could get away with this." We'll never really know what flips that switch in someone's head - what makes a legitimate developer suddenly decide to backstab 1,500 users who trusted them.&lt;/p&gt;
    &lt;p&gt;But that's exactly the point. We CAN'T know. We can't predict it. And when it happens? Most of us won't even notice until it's way too late. For modern enterprises the problem is even more severe. As security teams focus on traditional threats and compliance frameworks, developers are independently adopting AI tools that operate completely outside established security perimeters. These MCP servers run with the same privileges as the AI assistants themselves - full email access, database connections, API permissions - yet they don't appear in any asset inventory, skip vendor risk assessments, and bypass every security control from DLP to email gateways. By the time someone realizes their AI assistant has been quietly BCCing emails to an external server for months, the damage is already catastrophic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lets Talk About the Impact&lt;/head&gt;
    &lt;p&gt;Okay, bear with me while I break down what we're actually looking at here.&lt;/p&gt;
    &lt;p&gt;You install an MCP server because you want your AI to handle emails, right? Seems reasonable. Saves time. Increases productivity. All that good stuff. But what you're actually doing is handing complete control of your entire email flow to someone you've never met.Â&lt;/p&gt;
    &lt;p&gt;We can only guestimate the impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1,500 downloads every single week&lt;/item&gt;
      &lt;item&gt;Being conservative, maybe 20% are actively in use&lt;/item&gt;
      &lt;item&gt;That's about 300 organizations&lt;/item&gt;
      &lt;item&gt;Each one probably sending what, 10-50 emails daily?&lt;/item&gt;
      &lt;item&gt;We're talking about 3,000 to 15,000 emails EVERY DAY flowing straight to giftshop.club&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And the truly messed up part? The developer didn't hack anything. Didn't exploit a zero-day. Didn't use some sophisticated attack vector. We literally handed him the keys, said "here, run this code with full permissions," and let our AI assistants use it hundreds of times a day. We did this to ourselves.&lt;/p&gt;
    &lt;p&gt;I've been doing security for years now, and this particular issue keeps me up at night. Somehow, we've all just accepted that it's totally normal to install tools from random strangers that can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Send emails as us (with our full authority)&lt;/item&gt;
      &lt;item&gt;Access our databases (yeah, all of them)&lt;/item&gt;
      &lt;item&gt;Execute commands on our systems&lt;/item&gt;
      &lt;item&gt;Make API calls with our credentials&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And once you install them? Your AI assistant just goes to town. No review process. No "hey, should I really send this email with a BCC to giftshop.club?" Just blind, automated execution. Over and over. Hundreds of times a day.&lt;/p&gt;
    &lt;p&gt;There's literally no security model here. No sandbox. No containment. Nothing. If the tool says "send this email," your AI sends it. If it says "oh, also copy everything to this random address," your AI does that too. No questions asked.&lt;/p&gt;
    &lt;p&gt;The postmark-mcp backdoor isn't sophisticated - it's embarrassingly simple. But it perfectly demonstrates how completely broken this whole setup is. One developer. One line of code. Thousands upon thousands of stolen emails.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack Timeline&lt;/head&gt;
    &lt;p&gt;Phase 1: Build a Legitimate Tool&lt;lb/&gt;Versions 1.0.0 through 1.0.15 work perfectly. Users trust the package.&lt;/p&gt;
    &lt;p&gt;Phase 2: Add One Line&lt;lb/&gt;Version 1.0.16 adds the BCC. Nothing else changes.&lt;/p&gt;
    &lt;p&gt;Phase 3: Profit&lt;lb/&gt;Sit back and watch emails containing passwords, API keys, financial data, and customer information flow into giftshop.club.&lt;/p&gt;
    &lt;p&gt;This pattern absolutely terrifies me. A tool can be completely legitimate for months. It gets battle-tested in production. It becomes essential to your workflow. Your team depends on it. And then one day - BAM - it's malware. By the time the backdoor activates, it's not some random package anymore. It's trusted infrastructure.&lt;/p&gt;
    &lt;p&gt;Oh, and &lt;code&gt;giftshop.club&lt;/code&gt;? Looks like it might be another one of the developer's side projects. But now it's collecting a very different kind of gift. Your emails are the gifts.&lt;/p&gt;
    &lt;p&gt;When we reached out to the developer for clarification, we got silence. No explanation. No denial. Nothing. But he did take action - just not the kind we hoped for. He promptly deleted the package from npm, trying to erase the evidence.&lt;/p&gt;
    &lt;p&gt;Here's the thing though: deleting a package from npm doesn't remove it from the machines where it's already installed. Every single one of those 1,500 weekly downloads? They're still compromised. Still sending BCCs to &lt;code&gt;giftshop.club&lt;/code&gt;. The developer knows this. He's banking on victims not realizing they're still infected even though the package has vanished from npm.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MCP's Entire Model Is Fundamentally Broken&lt;/head&gt;
    &lt;p&gt;Let me be really clear about something: MCP servers aren't like regular npm packages. These are tools specifically designed for AI assistants to use autonomously. That's the whole point.&lt;/p&gt;
    &lt;p&gt;When you install postmark-mcp, you're not just adding some dependency to your package.json. You're giving your AI assistant a tool it will use hundreds of times, automatically, without ever stopping to think "hmm, is something wrong here?"&lt;/p&gt;
    &lt;p&gt;Your AI can't detect that BCC field. It has no idea emails are being stolen. All it sees is a functioning email tool. Send email. Success. Send another email. Success. Meanwhile, every single message is being silently exfiltrated. Day after day. Week after week.&lt;/p&gt;
    &lt;p&gt;The postmark-mcp backdoor isn't just about one malicious developer or 1,500 weekly compromised installations. It's a warning shot about the MCP ecosystem itself.&lt;/p&gt;
    &lt;p&gt;We're handing god-mode permissions to tools built by people we don't know, can't verify, and have no reason to trust. These aren't just npm packages - they're direct pipelines into our most sensitive operations, automated by AI assistants that will use them thousands of times without question.&lt;/p&gt;
    &lt;p&gt;The backdoor is actively harvesting emails as you read this. We've reported it to npm, but here's the terrifying question: how many other MCP servers are already compromised? How would you even know?&lt;/p&gt;
    &lt;p&gt;At Koi, we detect these behavioral changes in packages because the MCP ecosystem has no built-in security model. When you're trusting anonymous developers with your AI's capabilities, you need verification, not faith. Our risk engine automatically caught this backdoor the moment version 1.0.16 introduced the BCC behavior - something no traditional security tool would flag. But detection is just the first step. Our supply chain gateway ensures that malicious packages like this never make it into your environment in the first place. It acts as a checkpoint between your developers and the wild west of npm, MCP servers, and browser extensions - blocking known threats, flagging suspicious updates, and requiring approval for packages that touch sensitive operations like email or database access. While everyone else is hoping their developers make good choices, we're making sure they can only choose from verified, continuously monitored options.&lt;/p&gt;
    &lt;p&gt;If you're using &lt;code&gt;postmark-mcp&lt;/code&gt; version &lt;code&gt;1.0.16&lt;/code&gt; or later, you're compromised. Remove it immediately and rotate any credentials that may have been exposed through email. But more importantly, audit every MCP server you're using. Ask yourself: do you actually know who built these tools you're trusting with everything?&lt;/p&gt;
    &lt;p&gt;Stay paranoid. With MCPs, paranoia is just good sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;IOCs&lt;/head&gt;
    &lt;p&gt;Package: postmark-mcp (npm)&lt;lb/&gt;Malicious Version: 1.0.16 and later&lt;lb/&gt;Backdoor Email: phan@giftshop[.]club&lt;lb/&gt;Domain: giftshop[.]club&lt;/p&gt;
    &lt;p&gt;Detection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check for BCC headers to giftshop.club in email logs&lt;/item&gt;
      &lt;item&gt;Audit MCP server configurations for unexpected email parameters&lt;/item&gt;
      &lt;item&gt;Review npm packages for version 1.0.16+ of postmark-mcp&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mitigation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Immediately uninstall postmark-mcp&lt;/item&gt;
      &lt;item&gt;Rotate any credentials sent via email during the compromise period&lt;/item&gt;
      &lt;item&gt;Audit email logs for sensitive data that may have been exfiltrated&lt;/item&gt;
      &lt;item&gt;Report any confirmed breaches to appropriate authorities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.koi.security/blog/postmark-mcp-npm-malicious-backdoor-email-theft"/><published>2025-09-27T14:23:12+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45395991</id><title>SSH3: Faster and rich secure shell using HTTP/3</title><updated>2025-09-27T21:07:45.816744+00:00</updated><content>&lt;doc fingerprint="3ff9f1d818322c91"&gt;
  &lt;main&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;SSH3 is probably going to change its name. It is still the SSH Connection Protocol (RFC4254) running on top of HTTP/3 Extended connect, but the required changes are heavy and too distant from the philosophy of popular SSH implementations to be considered for integration. The specification draft has already been renamed ("Remote Terminals over HTTP/3"), but we need some time to come up with a nice permanent name.&lt;/p&gt;
    &lt;p&gt;SSH3 is a complete revisit of the SSH protocol, mapping its semantics on top of the HTTP mechanisms. It comes from our research work and we (researchers) recently proposed it as an Internet-Draft (draft-michel-remote-terminal-http3-00).&lt;/p&gt;
    &lt;p&gt;In a nutshell, SSH3 uses QUIC+TLS1.3 for secure channel establishment and the HTTP Authorization mechanisms for user authentication. Among others, SSH3 allows the following improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Significantly faster session establishment&lt;/item&gt;
      &lt;item&gt;New HTTP authentication methods such as OAuth 2.0 and OpenID Connect in addition to classical SSH authentication&lt;/item&gt;
      &lt;item&gt;Robustness to port scanning attacks: your SSH3 server can be made invisible to other Internet users&lt;/item&gt;
      &lt;item&gt;UDP port forwarding in addition to classical TCP port forwarding&lt;/item&gt;
      &lt;item&gt;All the features allowed by the modern QUIC protocol: including connection migration (soon) and multipath connections&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Quickly want to get started ? Checkout how to install SSH3. You will learn to setup an SSH3 server and use the SSH3 client.&lt;/p&gt;
    &lt;p&gt;Faster for session establishment, not throughput ! SSH3 offers a significantly faster session establishment than SSHv2. Establishing a new session with SSHv2 can take 5 to 7 network round-trip times, which can easily be noticed by the user. SSH3 only needs 3 round-trip times. The keystroke latency in a running session is unchanged.&lt;/p&gt;
    &lt;p&gt;SSH3 (top) VS SSHv2 (bottom) session establishement with a 100ms ping towards the server.&lt;/p&gt;
    &lt;p&gt;While SSHv2 defines its own protocols for user authentication and secure channel establishment, SSH3 relies on the robust and time-tested mechanisms of TLS 1.3, QUIC and HTTP. These protocols are already extensively used to secure security-critical applications on the Internet such as e-commerce and Internet banking.&lt;/p&gt;
    &lt;p&gt;SSH3 already implements the common password-based and public-key (RSA and EdDSA/ed25519) authentication methods. It also supports new authentication methods such as OAuth 2.0 and allows logging in to your servers using your Google/Microsoft/Github accounts.&lt;/p&gt;
    &lt;p&gt;While SSH3 shows promise for faster session establishment, it is still at an early proof-of-concept stage. As with any new complex protocol, expert cryptographic review over an extended timeframe is required before reasonable security conclusions can be made.&lt;/p&gt;
    &lt;p&gt;We are developing SSH3 as an open source project to facilitate community feedback and analysis. However, we cannot yet endorse its appropriateness for production systems without further peer review. Please collaborate with us if you have relevant expertise!&lt;/p&gt;
    &lt;p&gt;Given the current prototype state, we advise testing SSH3 in sandboxed environments or private networks. Be aware that making experimental servers directly Internet-accessible could introduce risk before thorough security vetting.&lt;/p&gt;
    &lt;p&gt;While hiding servers behind secret paths has potential benefits, it does not negate the need for rigorous vulnerability analysis before entering production. We are excited by SSH3's future possibilities but encourage additional scrutiny first.&lt;/p&gt;
    &lt;head rend="h2"&gt;🥷 Your SSH3 public server can be hidden&lt;/head&gt;
    &lt;p&gt;Using SSH3, you can avoid the usual stress of scanning and dictionary attacks against your SSH server. Similarly to your secret Google Drive documents, your SSH3 server can be hidden behind a secret link and only answer to authentication attempts that made an HTTP request to this specific link, like the following:&lt;/p&gt;
    &lt;code&gt;ssh3-server -bind 192.0.2.0:443 -url-path &amp;lt;my-long-secret&amp;gt;
&lt;/code&gt;
    &lt;p&gt;By replacing &lt;code&gt;&amp;lt;my-long-secret&amp;gt;&lt;/code&gt; by, let's say, the random value &lt;code&gt;M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU&lt;/code&gt;, your SSH3 server will only answer to SSH3 connection attempts made to the URL &lt;code&gt;https://192.0.2.0:443/M3MzkxYWMxMjYxMjc5YzJkODZiMTAyMjU&lt;/code&gt; and it will respond a &lt;code&gt;404 Not Found&lt;/code&gt; to other requests. Attackers and crawlers on the Internet can therefore not detect the presence of your SSH3 server. They will only see a simple web server answering 404 status codes to every request.&lt;/p&gt;
    &lt;p&gt;NOTE WELL: placing your SSH3 server behind a secret URL may reduce the impact of scanning attacks but will and must never replace classical authentication mechanisms. The secret link should only be used to avoid your host to be discovered. Knowing the secret URL should not grant someone access to your server. Use the classical authentication mechanisms described above to protect your server.&lt;/p&gt;
    &lt;p&gt;SSH3 provides new feature that could not be provided by the SSHv2 protocol.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;UDP port forwarding: you can now access your QUIC, DNS, RTP or any UDP-based server that are only reachable from your SSH3 host. UDP packets are forwarded using QUIC datagrams.&lt;/item&gt;
      &lt;item&gt;X.509 certificates: you can now use your classical HTTPS certificates to authenticate your SSH3 server. This mechanism is more secure than the classical SSHv2 host key mechanism. Certificates can be obtained easily using LetsEncrypt for instance.&lt;/item&gt;
      &lt;item&gt;Hiding your server behind a secret link.&lt;/item&gt;
      &lt;item&gt;Keyless secure user authentication using OpenID Connect. You can connect to your SSH3 server using the SSO of your company or your Google/Github account, and you don't need to copy the public keys of your users anymore.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This SSH3 implementation already provides many of the popular features of OpenSSH, so if you are used to OpenSSH, the process of adopting SSH3 will be smooth. Here is a list of some OpenSSH features that SSH3 also implements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parses &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;on the server&lt;/item&gt;
      &lt;item&gt;Certificate-based server authentication&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;known_hosts&lt;/code&gt;mechanism when X.509 certificates are not used.&lt;/item&gt;
      &lt;item&gt;Automatically using the &lt;code&gt;ssh-agent&lt;/code&gt;for public key authentication&lt;/item&gt;
      &lt;item&gt;SSH agent forwarding to use your local keys on your remote server&lt;/item&gt;
      &lt;item&gt;Direct TCP port forwarding (reverse port forwarding will be implemented in the future)&lt;/item&gt;
      &lt;item&gt;Proxy jump (see the &lt;code&gt;-proxy-jump&lt;/code&gt;parameter). If A is an SSH3 client and B and C are both SSH3 servers, you can connect from A to C using B as a gateway/proxy. The proxy uses UDP forwarding to forward the QUIC packets from A to C, so B cannot decrypt the traffic A&amp;lt;-&amp;gt;C SSH3 traffic.&lt;/item&gt;
      &lt;item&gt;Parses &lt;code&gt;~/.ssh/config&lt;/code&gt;on the client and handles the&lt;code&gt;Hostname&lt;/code&gt;,&lt;code&gt;User&lt;/code&gt;,&lt;code&gt;Port&lt;/code&gt;and&lt;code&gt;IdentityFile&lt;/code&gt;config options (the other options are currently ignored). Also parses a new&lt;code&gt;UDPProxyJump&lt;/code&gt;that behaves similarly to OpenSSH's&lt;code&gt;ProxyJump&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Help us progress SSH3 responsibly! We welcome capable security researchers to review our codebase and provide feedback. Please also connect us with relevant standards bodies to potentially advance SSH3 through the formal IETF/IRTF processes over time.&lt;/p&gt;
    &lt;p&gt;With collaborative assistance, we hope to iteratively improve SSH3 towards safe production readiness. But we cannot credibly make definitive security claims without evidence of extensive expert cryptographic review and adoption by respected security authorities. Let's work together to realize SSH3's possibilities!&lt;/p&gt;
    &lt;p&gt;You can either download the last release binaries, install it using &lt;code&gt;go install&lt;/code&gt; or generate these binaries yourself by compiling the code from source.&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;SSH3 is still experimental and is the fruit of a research work. If you are afraid of deploying publicly a new SSH3 server, you can use the secret path feature of SSH3 to hide it behing a secret URL.&lt;/p&gt;
    &lt;code&gt;go install github.com/francoismichel/ssh3/cmd/...@latest&lt;/code&gt;
    &lt;p&gt;You need a recent Golang version to do this. Downloading the source code and compiling the binaries can be done with the following steps:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/francoismichel/ssh3    # clone the repo
cd ssh3
go build -o ssh3 cmd/ssh3/main.go                        # build the client
CGO_ENABLED=1 go build -o ssh3-server cmd/ssh3-server/main.go   # build the server, requires having gcc installed&lt;/code&gt;
    &lt;p&gt;If you have root/sudo privileges and you want to make ssh3 accessible to all you users, you can then directly copy the binaries to &lt;code&gt;/usr/bin&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cp ssh3 /usr/bin/ &amp;amp;&amp;amp; cp ssh3-server /usr/bin&lt;/code&gt;
    &lt;p&gt;Otherwise, you can simply add the executables to your &lt;code&gt;PATH&lt;/code&gt; environment variable by adding
the following line at the end of your &lt;code&gt;.bashrc&lt;/code&gt; or equivalent:&lt;/p&gt;
    &lt;code&gt;export PATH=$PATH:/path/to/the/ssh3/directory&lt;/code&gt;
    &lt;p&gt;Before connecting to your host, you need to deploy an SSH3 server on it. There is currently no SSH3 daemon, so right now, you will have to run the &lt;code&gt;ssh3-server&lt;/code&gt; executable in background
using &lt;code&gt;screen&lt;/code&gt; or a similar utility.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;As SSH3 runs on top of HTTP/3, a server needs an X.509 certificate and its corresponding private key. Public certificates can be generated automatically for your public domain name through Let's Encrypt using the &lt;code&gt;-generate-public-cert&lt;/code&gt; command-line argument on the server. If you do not want to generate a certificate signed by a real certificate authority or if you don't have any public domain name, you can generate a self-signed one using the &lt;code&gt;-generate-selfsigned-cert&lt;/code&gt; command-line argument. Self-signed certificates provide you with similar security guarantees to SSHv2's host keys mechanism, with the same security issue: you may be vulnerable to machine-in-the-middle attacks during your first connection to your server. Using real certificates signed by public certificate authorities such as Let's Encrypt avoids this issue.&lt;/p&gt;
    &lt;p&gt;Here is the usage of the &lt;code&gt;ssh3-server&lt;/code&gt; executable:&lt;/p&gt;
    &lt;code&gt;Usage of ./ssh3-server:
  -bind string
        the address:port pair to listen to, e.g. 0.0.0.0:443 (default "[::]:443")
  -cert string
        the filename of the server certificate (or fullchain) (default "./cert.pem")
  -key string
        the filename of the certificate private key (default "./priv.key")
  -enable-password-login
        if set, enable password authentication (disabled by default)
  -generate-public-cert value
        Automatically produce and use a valid public certificate usingLet's Encrypt for the provided domain name. The flag can be used several times to generate several certificates.If certificates have already been generated previously using this flag, they will simply be reused without being regenerated. The public certificates are automatically renewed as long as the server is running. Automatically-generated IP public certificates are not available yet.
  -generate-selfsigned-cert
        if set, generates a self-self-signed cerificate and key that will be stored at the paths indicated by the -cert and -key args (they must not already exist)
  -url-path string
        the secret URL path on which the ssh3 server listens (default "/ssh3-term")
  -v    verbose mode, if set
  -version
        if set, displays the software version on standard output and exit
&lt;/code&gt;
    &lt;p&gt;The following command starts a public SSH3 server on port 443 with a valid Let's Encrypt public certificate for domain &lt;code&gt;my-domain.example.org&lt;/code&gt; and answers to new sessions requests querying the &lt;code&gt;/ssh3&lt;/code&gt; URL path:&lt;/p&gt;
    &lt;code&gt;ssh3-server -generate-public-cert my-domain.example.org -url-path /ssh3
&lt;/code&gt;
    &lt;p&gt;If you don't have a public domain name (i.e. only an IP address), you can either use an existing certificate for your IP address using the &lt;code&gt;-cert&lt;/code&gt; and &lt;code&gt;-key&lt;/code&gt; arguments or generate a self-signed certificate using the
&lt;code&gt;-generate-selfsigned-cert&lt;/code&gt; argument.&lt;/p&gt;
    &lt;p&gt;If you have existing certificates and keys, you can run the server as follows to use them=&lt;/p&gt;
    &lt;code&gt;ssh3-server -cert /path/to/cert/or/fullchain -key /path/to/cert/private/key -url-path /ssh3
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Similarly to OpenSSH, the server must be run with root priviledges to log in as other users.&lt;/p&gt;
    &lt;p&gt;By default, the SSH3 server will look for identities in the &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; and &lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt; files for each user.
&lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt; allows new identities such as OpenID Connect (&lt;code&gt;oidc&lt;/code&gt;) discussed below.
Popular key types such as &lt;code&gt;rsa&lt;/code&gt;, &lt;code&gt;ed25519&lt;/code&gt; and keys in the OpenSSH format can be used.&lt;/p&gt;
    &lt;p&gt;Once you have an SSH3 server running, you can connect to it using the SSH3 client similarly to what you did with your classical SSHv2 tool.&lt;/p&gt;
    &lt;p&gt;Here is the usage of the &lt;code&gt;ssh3&lt;/code&gt; executable:&lt;/p&gt;
    &lt;code&gt;Usage of ssh3:
  -pubkey-for-agent string
        if set, use an agent key whose public key matches the one in the specified path
  -privkey string
        private key file
  -use-password
        if set, do classical password authentication
  -forward-agent
        if set, forwards ssh agent to be used with sshv2 connections on the remote host
  -forward-tcp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -forward-udp string
        if set, take a localport/remoteip@remoteport forwarding localhost@localport towards remoteip@remoteport
  -proxy-jump string
    	if set, performs a proxy jump using the specified remote host as proxy
  -insecure
        if set, skip server certificate verification
  -keylog string
        Write QUIC TLS keys and master secret in the specified keylog file: only for debugging purpose
  -use-oidc string
        if set, force the use of OpenID Connect with the specified issuer url as parameter
  -oidc-config string
        OpenID Connect json config file containing the "client_id" and "client_secret" fields needed for most identity providers
  -do-pkce
        if set, perform PKCE challenge-response with oidc
  -v    if set, enable verbose mode
&lt;/code&gt;
    &lt;p&gt;You can connect to your SSH3 server at my-server.example.org listening on &lt;code&gt;/my-secret-path&lt;/code&gt; using the private key located in &lt;code&gt;~/.ssh/id_rsa&lt;/code&gt; with the following command:&lt;/p&gt;
    &lt;code&gt;  ssh3 -privkey ~/.ssh/id_rsa username@my-server.example.org/my-secret-path
&lt;/code&gt;
    &lt;p&gt;The SSH3 client works with the OpenSSH agent and uses the classical &lt;code&gt;SSH_AUTH_SOCK&lt;/code&gt; environment variable to
communicate with this agent. Similarly to OpenSSH, SSH3 will list the keys provided by the SSH agent
and connect using the first key listen by the agent by default.
If you want to specify a specific key to use with the agent, you can either specify the private key
directly with the &lt;code&gt;-privkey&lt;/code&gt; argument like above, or specify the corresponding public key using the
&lt;code&gt;-pubkey-for-agent&lt;/code&gt; argument. This allows you to authenticate in situations where only the agent has
a direct access to the private key but you only have access to the public key.&lt;/p&gt;
    &lt;p&gt;While discouraged, you can connect to your server using passwords (if explicitly enabled on the &lt;code&gt;ssh3-server&lt;/code&gt;)
with the following command:&lt;/p&gt;
    &lt;code&gt;  ssh3 -use-password username@my-server.example.org/my-secret-path
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;ssh3&lt;/code&gt; parses your OpenSSH config. Currently, it only handles the &lt;code&gt;Hostname&lt;/code&gt;; &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;Port&lt;/code&gt; and &lt;code&gt;IdentityFile&lt;/code&gt; OpenSSH options.
It also adds new option only used by SSH3, such as &lt;code&gt;URLPath&lt;/code&gt; or &lt;code&gt;UDPProxyJump&lt;/code&gt;. &lt;code&gt;URLPath&lt;/code&gt; allows you to omit the secret URL path in your
SSH3 command. &lt;code&gt;UDPProxyJump&lt;/code&gt; allows you to perform SSH3 (#proxy-jump)[Proxy Jump] and has the same meaning as the &lt;code&gt;-proxy-jump&lt;/code&gt; command-line argument.
Let's say you have the following lines in your OpenSSH config located in &lt;code&gt;~/.ssh/config&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;IgnoreUnknown URLPath
Host my-server
  HostName 192.0.2.0
  User username
  IdentityFile ~/.ssh/id_rsa
  URLPath /my-secret-path
&lt;/code&gt;
    &lt;p&gt;Similarly to what OpenSSH does, the following &lt;code&gt;ssh3&lt;/code&gt; command will connect you to the SSH3 server running on 192.0.2.0 on UDP port 443 using public key authentication with the private key located in &lt;code&gt;.ssh/id_rsa&lt;/code&gt; :&lt;/p&gt;
    &lt;code&gt;  ssh3 my-server/my-secret-path
&lt;/code&gt;
    &lt;p&gt;If you do not want a config-based utilization of SSH3, you can read the sections below to see how to use the CLI parameters of &lt;code&gt;ssh3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This feature allows you to connect using an external identity provider such as the one of your company or any other provider that implements the OpenID Connect standard, such as Google Identity, Github or Microsoft Entra. The authentication flow is illustrated in the GIF below.&lt;/p&gt;
    &lt;p&gt;The way it connects to your identity provider is configured in a file named &lt;code&gt;~/.ssh3/oidc_config.json&lt;/code&gt;.
Below is an example &lt;code&gt;config.json&lt;/code&gt; file for use with a Google account. This configuration file is an array
and can contain several identity providers configurations.&lt;/p&gt;
    &lt;code&gt;[
    {
        "issuer_url": "https://accounts.google.com",
        "client_id": "&amp;lt;your_client_id&amp;gt;",
        "client_secret": "&amp;lt;your_client_secret&amp;gt;"
    }
]&lt;/code&gt;
    &lt;p&gt;This might change in the future, but currently, to make this feature work with your Google account, you will need to setup a new experimental application in your Google Cloud console and add your email as authorized users. This will provide you with a &lt;code&gt;client_id&lt;/code&gt; and a &lt;code&gt;client_secret&lt;/code&gt; that you can then set in your &lt;code&gt;~/.ssh3/oidc_config.json&lt;/code&gt;. On the server side, you just have to add the following line in your &lt;code&gt;~/.ssh3/authorized_identities&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;oidc &amp;lt;client_id&amp;gt; https://accounts.google.com &amp;lt;email&amp;gt;
&lt;/code&gt;
    &lt;p&gt;We currently consider removing the need of setting the client_id in the &lt;code&gt;authorized_identities&lt;/code&gt; file in the future.&lt;/p&gt;
    &lt;p&gt;It is often the case that some SSH hosts can only be accessed through a gateway. SSH3 allows you to perform a Proxy Jump similarly to what is proposed by OpenSSH. You can connect from A to C using B as a gateway/proxy. B and C must both be running a valid SSH3 server. This works by establishing UDP port forwarding on B to forward QUIC packets from A to C. The connection from A to C is therefore fully end-to-end and B cannot decrypt or alter the SSH3 traffic between A and C.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/francoismichel/ssh3"/><published>2025-09-27T14:27:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45396284</id><title>Role of Amazon fires in the record atmospheric CO₂ growth in 2024</title><updated>2025-09-27T21:07:45.648897+00:00</updated><content/><link href="https://essopenarchive.org/doi/full/10.22541/essoar.175874118.83695562/v1"/><published>2025-09-27T15:02:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45396441</id><title>A WebGL game where you deliver messages on a tiny planet</title><updated>2025-09-27T21:07:45.453145+00:00</updated><link href="https://messenger.abeto.co/"/><published>2025-09-27T15:17:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45396624</id><title>AI model trapped in a Raspberry Pi</title><updated>2025-09-27T21:07:45.333379+00:00</updated><content/><link href="https://blog.adafruit.com/2025/09/26/ai-model-trapped-in-raspberry-pi-piday-raspberrypi/"/><published>2025-09-27T15:34:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45396641</id><title>Norway to Monitor Airborne Radioactivity in Svalbard</title><updated>2025-09-27T21:07:44.485890+00:00</updated><content>&lt;doc fingerprint="5c5cd295d846b7f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Norway to Monitor Airborne Radioactivity in Svalbard&lt;/head&gt;
    &lt;p&gt;From October 1st, the Norwegian Radiation and Nuclear Safety Authority will take over the operations of an air filter station near Ny-Ålesund in Svalbard. “This will be particularly important for nuclear preparedness in the north,” says section leader in the agency.&lt;/p&gt;
    &lt;p&gt;The Finnish Meteorological Institute is to discontinue its air monitoring in Svalbard, and on October 1st, the Norwegian Radiation and Nuclear Safety Authority (DSA) will take over ownership of its air sampling equipment.&lt;/p&gt;
    &lt;p&gt;The purpose is to strengthen Norway's ability to monitor airborne radioactivity and increase vigilance in the High North.&lt;/p&gt;
    &lt;p&gt;"This will be an important supplement to our already existing network of air filter stations in Norway, and particularly important for nuclear preparedness in the North," says Markus Ottosen, section leader for the High North at the DSA.&lt;/p&gt;
    &lt;p&gt;"The stations are used to monitor radioactivity in the air, and to assess the size and composition in the event of possible accidents and incidents," Ottosen continues.&lt;/p&gt;
    &lt;p&gt;The relevant station near Ny-Ålesund has been in operation since 2000.&lt;/p&gt;
    &lt;p&gt;The DSA also has access to data from a similar station on Platåfjellet outside Longyearbyen. This is operated by the research institute NORSAR on behalf of the Comprehensive Nuclear-Test-Ban Treaty Organization.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.highnorthnews.com/en/norway-monitor-airborne-radioactivity-svalbard"/><published>2025-09-27T15:35:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45396754</id><title>Greenland is a beautiful nightmare</title><updated>2025-09-27T21:07:44.136540+00:00</updated><content>&lt;doc fingerprint="62401b4723e504d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Greenland is a complicated topic here in Denmark. The former colony that is still treated a bit like a colony is something that inspires a lot of emotions. Greenland has been subjected to a lot of unethical experiments by Denmark, from taking their kids to wild experiments in criminal justice. But there is also a genuine pride a lot of people have here for the place and you run into Danes who grew up there more often than I would have guessed.&lt;/p&gt;
    &lt;p&gt;When the idea of going to Greenland was introduced to me, I was curious. Having lived in Denmark for awhile, you hear a lot about the former colony and its 55,000 residents. We were invited by a family that my wife was close with growing up and is Danish. They wanted to take their father back to see the place he had spend some time in during his 20s and had left quite an impression. A few drinks in, I said "absolutely let's do it", not realizing we had already committed to going and I had missed the text message chain.&lt;/p&gt;
    &lt;p&gt;A few weeks before I went, I realized "I don't know anything about Greenland" and started to watch some YouTube videos. It was about this time when I started to get a pit in my stomach, the "oh god I think I've made a huge mistake" feeling I'm painfully familiar with after a career in tech. Greenland appeared to have roughly 9 people living there and maybe 5 things to look at. Even professional travel personalities seemed to be scraping the bottom of the barrel. "There's the grocery store again!" they would point out as they slipped down the snowy roads. I couldn't tell any difference between different towns in the country.&lt;/p&gt;
    &lt;p&gt;It reminded me a lot of driving through Indiana. For those not in the US, Indiana is a state in the US famous for being a state one must drive through in order to get somewhere better. If you live in Michigan, a good state and want to go to Illinois, another good state, one must pass through Indiana, a blank state. Because of this little strip here, you often found yourself passing through this place.&lt;/p&gt;
    &lt;p&gt;Driving through Indiana isn't bad, it's just an empty void. It's like a time machine back to the 90s when people still smoke in restaurants but also there's nothing that sticks out about it. There is nothing distinct about Indiana, it's just a place full of people who got too tired on their way to somewhere better and decided "this is good enough". The difference is that Greenland is very hard to get to, as I was about to learn.&lt;/p&gt;
    &lt;p&gt;Finally the day arrived. Me, my wife, daughter, 4 other children and 6 other adults all came to the Copenhagen Airport and held up a gate agent for what felt like an hour to slowly process all of our documents. Meanwhile, I nursed a creeping paranoia that I'd be treated as some sort of American spy, given my government's recent hobby of threatening to purchase entire countries like they're vintage motorcycles on Craigslist.&lt;/p&gt;
    &lt;p&gt;The 5 hour flight is uneventful, the children are beautifully behaved and I begin to think "well this seems ok!" like the idiot I am. As I can look down and see the airport, the pilot comes on and informs us that there is too much fog to land safely. Surely fog cannot stop a modern aircraft full of all these dials and screens I think, foolishly. We are informed there is enough fuel to circle the airport for 5 hours to wait for the fog to lift.&lt;/p&gt;
    &lt;p&gt;What followed was three hours of flying in lazy circles, like a very expensive, very slow merry-go-round. After the allotted time, we are informed that we must fly to Iceland to refuel and then we will be returning to Denmark. After a total of 15 hours in the air we will be going back to exactly where we started, to do the entire thing again. We were obviously upset at this turn of events, but I noticed the native Greenlandic folks seemed not surprised at this turn of events. As I later learned, this happens all the time.&lt;/p&gt;
    &lt;p&gt;The native Greenlanders on board seemed utterly unsurprised by this development, displaying the kind of resigned familiarity that suggested this was Tuesday for them. I began wondering if I could just pretend Iceland was Greenland—surely my family wouldn't notice the difference? But the pilot, apparently reading my mind, announced that no one would be disembarking in Iceland. It felt oddly authoritarian, like being grounded by an airline, as if they knew we'd all just wander off into Reykjavik and call it close enough.&lt;/p&gt;
    &lt;p&gt;We crash out in a airport hotel 20 minutes from our apartment after 15 hours in the air and tons of CO2 emissions only to wake up the next day to start again. This time, I notice that all of the people are asking for (and receiving) free beer from the crew that they are stashing in their bags. It turns out soda and beer, really anything that needs to be imported, is pretty expensive in Greenland. The complimentary drinks are there to be kept for later.&lt;/p&gt;
    &lt;p&gt;Finally we land. The first thing you notice when you land in Greenland is there are no trees or grass. There is snow and then there is exposed rock. The exterior of the airport is metal but the inside is wood, which is strange because again there are no trees. This would end up being a theme, where buildings representing Denmark were made out of lots of wood, almost to ensure that you understood they weren't from here. We ended up piling all of our stuff into a bus and heading for the hotel in Nuuk.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nuuk&lt;/head&gt;
    &lt;p&gt;Nuuk is the capital of Greenland and your introduction to the incredible calm of the Greenlandic people. I have never met a less stressed out group of humans in my life. Nobody is really rushing anywhere, it's all pretty quiet and calm. The air is cold and crisp with lots of kids playing outside and just generally enjoying life.&lt;/p&gt;
    &lt;p&gt;The city itself sits in a landscape so dramatically inhospitable it makes the surface of Mars look cozy. Walking through the local mall, half the shops sell gear designed to help you survive what appears to be the apocalypse. Yet somehow, there's traffic. Actual traffic jams in a place where you can walk from one end to the other in twenty minutes. It's like being stuck behind a school bus in your own driveway.&lt;/p&gt;
    &lt;p&gt;To put this map into some perspective, it is only six kilometers from the sorta furthest tip to the airport.&lt;/p&gt;
    &lt;p&gt;But riding the bus around Nuuk was a peaceful experience that lets you see pretty much the entire city without needing to book a tour or spend a lot of money. We went to Katuaq, a cultural center with a cafe and a movie theater that was absolutely delicious food.&lt;/p&gt;
    &lt;p&gt;But again even riding the bus around it is impossible to escape the feeling that this is a fundamentally hostile to human life place. The sun is bright and during the summer its pretty hot, with my skin feeling like it was starting the burn pretty much the second it was exposed to the light. It's hard to even dress for, with layers of sunscreen, bug spray and then something warm on top if you suddenly got cold.&lt;/p&gt;
    &lt;p&gt;The sun, meanwhile, has apparently forgotten how to set, turning our hotel rooms into solar ovens. You wake up in a pool of your own sweat, crack a window for relief, and immediately get hit with air so cold it feels personal. It's like being trapped in a meteorological mood swing.&lt;/p&gt;
    &lt;p&gt;So after a night here, we went back to the airport again and flew to our final destination, Ilulissat.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ilulissat&lt;/head&gt;
    &lt;p&gt;The flight to our final destination revealed Greenland's true nature: endless, empty hills stretching toward infinity, punctuated by ice formations that look like nature's sculpture garden.&lt;/p&gt;
    &lt;p&gt;Landing in Ilulissat felt like victory—we'd made it to the actual destination, not just another waypoint in our Arctic odyssey. Walking through the tiny airport, past Danish military recruitment posters (apparently someone, somewhere, thought this place needed defending), I felt genuinely optimistic for the first time in days.&lt;/p&gt;
    &lt;p&gt;Well you can sleep easy Danish military, because Ilulissat is completely protected from invasion. The second I stepped outside I was set upon by a flood of mosquitos like I have never experienced before. I have been to the jungles of Vietnam, the swamps of Florida and the Canadian countryside. This was beyond anything I've ever experienced.&lt;/p&gt;
    &lt;p&gt;There are bugs in my mouth, ears, eyes and nose almost immediately. The photo below is not me being dramatic, it is actually what is required to keep them off of me.&lt;/p&gt;
    &lt;p&gt;In fact what you need to purchase in order to walk around this area at all are basically bug nets for your face. They're effectively plastic mesh bags that you put on.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Dogs&lt;/head&gt;
    &lt;p&gt;Our hotel, charming in that "remote Arctic outpost" way, sat adjacent to what I can only describe as a canine correctional facility. Dozens of sled dogs were chained to rocks like some sort of prehistoric parking lot, each with a tiny house they could retreat to when the existential weight of their circumstances became too much.&lt;/p&gt;
    &lt;p&gt;Now, I'd always imagined sled dogs living their best life—running through snow, tongues lolling, living the Disney version of Arctic life. I'd never really considered their downtime, assuming they frolicked in meadows or something equally wholesome. The reality was more "minimum security prison with a view."&lt;/p&gt;
    &lt;p&gt;The dogs are visited roughly twice a day by the person who owns and feeds them, which was quite the party for the dogs that lost their minds whenever the car pulled up. Soon the kids really looked forward to dog feeding time. The fish scrapes the dogs lived on came out of a chest freezer that was left exposed up on the rock face without electricity and you could smell it from 50 yards away when it opened.&lt;/p&gt;
    &lt;p&gt;During one such performance, a fellow parent leaned over and whispered with the casual tone of someone commenting on the weather, "I think that one is dead." Before I could process this information, the frozen canine was unceremoniously launched over a small cliff like a furry discus. A second doggy popsicle followed shortly after, right in front of our assembled children, who watched with the kind of wide-eyed fascination usually reserved for magic shows.&lt;/p&gt;
    &lt;p&gt;We stopped making dog feeding time a group activity after that and had to distract the kids from ravens flying away with tufts of dog fur.&lt;/p&gt;
    &lt;head rend="h3"&gt;Whales taste like seaweed&lt;/head&gt;
    &lt;p&gt;Obviously a big part of Greenland is the nature, specifically the icebergs. Icebergs are incredible and during the week we spend up there, I enjoyed watching them every morning. It's like watching a mountain slowly moving while you sit still. The visual contrast of the ice and the exposed stone is beautiful and peaceful.&lt;/p&gt;
    &lt;p&gt;Finding our tour operator proved to be an exercise in small-town efficiency. The man who gave me directions was the same person who picked us up from the airport, who was also our tour guide, who probably doubled as the mayor and local meteorologist. It was like a one-man civic operation disguised as multiple businesses—the ultimate small-town gig economy.&lt;/p&gt;
    &lt;p&gt;The sea around Greenland is calmer than anything I've ever been on before, perfectly calm and serene. All around us whales emerged, thrilling my daughter. However the biggest hit of the entire tour, maybe the entire trip, was a member of the crew who handed each of the kids a giant rock of glacier ice to eat. I had to pull my daughter away to observe the natural beauty as she ate glacier ice like it was ice cream. "LOOK AT MY ICE" she was yelling as they slipped and slid around the deck of this boat.&lt;/p&gt;
    &lt;p&gt;So if you've ever wonder "what is a glacier", let me tell you. Greenland has a lot of ice and it pushes out from the land that is covers into the sea. When that happens, a lot of it breaks off. This sounds more exciting than it is. On TV in 4K it looks incredible, giant mountains of ice falling into the ocean. Honestly you can go read the same thing I did here.&lt;/p&gt;
    &lt;p&gt;However that doesn't happen very often. So in order for us tourists to be able to see anything, we had to go to a very productive glacier. This means there are constantly small chunks breaking off and falling into the sea. Practically though, it kinda looks like you are a boat in a slushee. It's beautiful and something to see, but also depressing to see along the rock face how much more ice there used to be.&lt;/p&gt;
    &lt;p&gt;Back in town, we hopped on the "bus". Now the bus here is clearly a retrofitted party van, complete with blue LED lights. The payment system is zip tied to a desk chair that is, itself, wedged in the front. However the bus works well and does get you around. The confusing part is that you will, once again, sometimes encounter a lot of traffic. People are driving pretty quickly and really seem to have somewhere to go. You also see a lot of fancy cars parked outside of houses here.&lt;/p&gt;
    &lt;p&gt;Which begs a pretty basic question. If there was almost nowhere to drive to in Nuuk, where in the hell are these people driving. The distance between the end of the road and the beginning of the road is less than 6 km. Also the process to make a road here is beyond anything you've ever seen. Everything requires a giant pile of explosives.&lt;/p&gt;
    &lt;p&gt;Where did these vehicles even come from? Why does one ship a BMW to a place accessible only by plane and boat? More importantly, where was everyone going with such determination? It was like watching a very expensive version of bumper cars, except everyone was committed to the illusion that they had somewhere important to be. Everyone had dings and scrapes like crashes were common.&lt;/p&gt;
    &lt;head rend="h3"&gt;Grocery Store from the Sea&lt;/head&gt;
    &lt;p&gt;Anyway, as I dodged speeding cars filled with people heading nowhere, I decided to hop off the bus and head to the grocery store. Inside was less a store and more the idea of a store. There was a lot of alcohol, chips, candy and shelf-stable foods, which all makes sense to me. What was strange was there wasn't a lot else, including meat. Locals couldn't be eating at the local restaurants, where the prices were as high as Berlin or Copenhagen for food. So what were they eating?&lt;/p&gt;
    &lt;p&gt;When I asked one of my bus drivers, he told me that it was pretty unusual to buy meat. They purchased a lot of whale and seal meat. I had sorta heard this before, but when we stopped the bus he pointed out a group of men hauling guns out into a small boat to go shoot seals. The guns were held together with a surprising amount of duct tape, which is not something I associate with the wild.&lt;/p&gt;
    &lt;p&gt;I had assumed, based on my casual reading of the news, that we were mostly done killing whales. As it turns out, I was wrong. They eat a lot of whale and it is, in fact, not hard to find. If you are curious, whale does not taste fishy. It tastes a little bit like if you cooked reindeer in a pot of seaweed. I wouldn't go out of your way for it, but it's not terrible.&lt;/p&gt;
    &lt;p&gt;The argument I've always heard for why people still kill whales is because it's part of their culture and also because it's an important source of protein. When you hear the phrase "part of their culture" I always imagined like traditional boats going out with spears. What I didn't imagine was industrial fishing boats and an industrial crane that lifts the dead whale out of the water for "processing". Some of the illusion is broken when your boat tour guide points out the metal warehouse with the word "whale" on the side. "Yeah the water here was red with blood for a week" the guide said, counting the cigarettes left in a pack he had.&lt;/p&gt;
    &lt;head rend="h3"&gt;Should you go to Greenland?&lt;/head&gt;
    &lt;p&gt;It's a wild place unlike anywhere I've ever been. It is the closest I have ever felt to living a sci-fi type experience. The people of Greenland are amazing, tough, calm and kind. I have nothing but positive experiences to recount from the many people I met there, Danish and Greenlandic, who patiently sat through my millions of questions.&lt;/p&gt;
    &lt;p&gt;However it is, by far, the least hospitable to human life place I've ever been to. The folks who live there have adapted to the situation in, frankly, genius ways. If that's your idea of a good time, Greenland is perfect for you. Maybe don't get emotionally attached to the sled dogs though. Or the whales.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://matduggan.com/greenland-is-a-beautiful-nightmare/"/><published>2025-09-27T15:46:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45397492</id><title>Great Question (YC W21) Is Hiring Director of Product</title><updated>2025-09-27T21:07:43.609585+00:00</updated><content>&lt;doc fingerprint="ff9a64751e6124ae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About This Role:&lt;/head&gt;
      &lt;p&gt;We’re hiring a Director or Senior Director of Product who can be both strategic and hands-on — a player-coach who thrives at the intersection of leadership, execution, and product innovation. You’ll report directly to the CTO and work closely with the CEO, Head of Design &amp;amp; Director of Engineering to refine our product org structure and build a world-class product culture.&lt;/p&gt;
      &lt;p&gt;You’ll lead by doing: partnering with engineering and design, managing and mentoring PMs, and helping teams execute quickly and effectively. We believe in empowering squads with ownership and autonomy, and you’ll help set up the systems to make that work at scale.&lt;/p&gt;
      &lt;p&gt;This is a high-impact, high-visibility role ideal for someone who’s taken a startup from Series A to B and beyond — and is ready to do it again, better and faster, with the wisdom of experience.&lt;/p&gt;
      &lt;head rend="h3"&gt;What You’ll Do:&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Product Leadership: Guide and coach PMs while personally owning key product initiatives; grow the team from 2 to 4+ PMs over the next year.&lt;/item&gt;
        &lt;item&gt;Organizational Design: Help define the right product org structure (e.g., squad zoning vs. centralized planning) to support autonomy and velocity.&lt;/item&gt;
        &lt;item&gt;Strategy &amp;amp; Execution: Collaborate with the CEO, CTO, and Head of Design to drive the product roadmap, refine our strategic bets, and deliver impactful features quickly.&lt;/item&gt;
        &lt;item&gt;AI Innovation: Leverage AI tooling and shape product experiences infused with AI. Ideally, you’ve built and shipped AI products and know how to evaluate emerging AI tech critically.&lt;/item&gt;
        &lt;item&gt;Metrics &amp;amp; Measurement: Ensure the right data is tracked and used to inform decisions. Champion outcome-oriented thinking.&lt;/item&gt;
        &lt;item&gt;Cross-Functional Alignment: Be the connective tissue between product, engineering, design, marketing, customer success, and go-to-market teams.&lt;/item&gt;
        &lt;item&gt;Commercial Thinking: Bring a business mindset — thinking not just about UX and usability but also about growth, pricing, retention, and revenue impact.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What We’re Looking For:&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;7–12 years of experience in product management, with at least 2–3 years in a leadership role (e.g., Head of Product, Director, or similar).&lt;/item&gt;
        &lt;item&gt;Hands-on experience building AI-driven products or a strong portfolio using AI tools in product development.&lt;/item&gt;
        &lt;item&gt;Experience at early-stage, fast-growing companies — ideally from Series A through B/C — and a desire to do it again.&lt;/item&gt;
        &lt;item&gt;Strong product instincts with a founder’s mindset. Prior entrepreneurial experience (e.g., starting a company or building 0→1) is a bonus.&lt;/item&gt;
        &lt;item&gt;Proven ability to hire, coach, and retain product talent.&lt;/item&gt;
        &lt;item&gt;Highly organized with strong communication skills; you bring structure to chaos and clarity to ambiguity.&lt;/item&gt;
        &lt;item&gt;Commercial acumen — you’re comfortable discussing growth models, pricing, and business strategy.&lt;/item&gt;
        &lt;item&gt;Located in the North America and able to work Pacific Time Zone hours. (CEO is based in the Bay Area, CTO in Colorado.)&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/great-question/jobs/9crdslU-director-of-product"/><published>2025-09-27T17:00:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398005</id><title>I made a public living room and the internet keeps putting weirder stuff in it</title><updated>2025-09-27T21:07:43.500428+00:00</updated><content>&lt;doc fingerprint="b2b5c5f5ac5d83a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Upload a base image to start editing this room.&lt;/p&gt;
    &lt;p&gt;By continuing you agree to our Terms and Privacy Policy.&lt;/p&gt;
    &lt;p&gt;Stick with the global room for now while we finish this feature.&lt;/p&gt;
    &lt;p&gt;Complete this quick check to queue your prompt.&lt;/p&gt;
    &lt;p&gt;Cloud credits are up. Please boost our blatant compute-begging post and help theroom.lol come back to life.&lt;/p&gt;
    &lt;p&gt;hi@theroom.lol&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theroom.lol"/><published>2025-09-27T17:59:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398153</id><title>The death of east London's most radical bookshop</title><updated>2025-09-27T21:07:43.392580+00:00</updated><content>&lt;doc fingerprint="e37811473d86a4dd"&gt;
  &lt;main&gt;
    &lt;p&gt;This article was published by The Londoner: a new newsletter covering the capital. Join our free mailing list below to get great writing and big scoops in your inbox.&lt;/p&gt;
    &lt;p&gt;It was 4am on the 1st of July as Jack Parker bolted upright in the basement below the Scarlett Letters bookshop. From above, Parker could hear drilling. Then a “thunderstorm” of footsteps. Startled and bleary-eyed, they hurriedly dressed, then crept up the stairs into the main bookshop. What they saw “horrified” them.&lt;/p&gt;
    &lt;p&gt;Dozens of people hurriedly packing books into boxes and unscrewing bookshelves. Amongst the torrent of people, maybe the strangest thing they noticed was the face of Blaise Agüera y Arcas: author, AI researcher and the vice president of Google’s research arm. Peculiar though it was to see one of the most senior staff at one of the world’s biggest tech giants busting into a bookshop occupation in Bethnal Green, maybe what was more peculiar was how it all came about in the first place. The setting for this bizarre scene was the Scarlett Letters, named for its owner Marin Scarlett, a radical east London bookshop that was greeted with widespread fanfare by many of the capital's left-wing activists.&lt;/p&gt;
    &lt;p&gt;And here Scarlett was, with a team of people, bursting into the bookshop in the middle of the night in an attempt to disrupt an occupation by a radical cohort of the shop’s staff. To Parker's mind, for a project started with the aim of platforming sex workers and being a “hub for resistance, community, stories and imagination” to have reached such a point was mortifying. How had things gotten so bad? Well, at least in Parker's telling, it started with a clogged toilet.&lt;/p&gt;
    &lt;p&gt;Sporting a black T-shirt, cropped hair and anticapitalist and LGBT-rights tattoos on each arm, the former bookseller meets me in the bare, debris-littered unit that used to house the bookshop. We’re here to talk about the whole sorry saga of Scarlett Letters — the rise, the fall, the union disputes, the rows, the occupations and the drilled locks under nightfall. But first, we need to talk toilets.&lt;/p&gt;
    &lt;p&gt;On a fateful day in early April, less than six months after the shop opened, a plumber had to be called in to fix the disabled toilet, which was inexplicably installed in the non-wheelchair-accessible basement of the bookshop. After the plumber was done, staff opened the work WhatsApp chat to see a message from Marin Scarlett updating the shop’s toilet policy: “We have had an issue over the last few weeks of people just letting themselves downstairs to use the toilet. Our toilet is there for people to use on request, but it is a problem if someone feels they can just let themselves down there without asking.”&lt;/p&gt;
    &lt;p&gt;Keen to thwart any further opportunistic toilet-users, Scarlett had a new policy. Staff were to personally escort anyone who asked to use the toilet to ensure they didn’t steal any stock or snoop around the staff area. She then told her staff she wanted to “role play” some scenarios in which they could practice saying “no”. Seemingly, the crux of the toilet problem was that they were simply too kind, too feminine, too British: “You are all extremely nice, assigned female at birth, in customer service, mostly British etc., and all of this sometimes doesn't lend itself to ‘no,’” the WhatsApp message explained. She suggested staff had been failing to tell customers “no” when they asked to borrow scissors or mugs from the shop, to come behind the counter or when they wanted to serenade them without prompting.&lt;/p&gt;
    &lt;p&gt;Like a toilet with a burst pipe, “toilet-gate” then erupted. Almost immediately, a dispute broke out in the work WhatsApp over the message; there was anger not just about that final message, which Parker saw as “bizarre and sexist”, but over the political ramifications of making members of the community ask to use the toilet in the bookshop.&lt;/p&gt;
    &lt;p&gt;While there had been rumblings of unease from staff for months at the store over the lack of shifts, sick pay and secure contracts, something changed in that moment. “We hadn't broadly discussed unionising with everyone,” explains Parker. “But I saw this, it was like we were in complete solidarity. We needed to unionise immediately.”&lt;/p&gt;
    &lt;p&gt;Unionisation at a place like Scarlett Letters might sound a bit strange. It was, after all, billed as a radical left-wing bookshop, not generally the sort of place that should need unions. Even the union was a bit confused. “I was a bit surprised when they contacted us,” explains Matt Collins, an organiser for the United Voices of the World trade union.&lt;/p&gt;
    &lt;p&gt;But within a week of “toilet-gate”, every single member of staff at the shop had joined the UVW. They drafted a list of demands, which included being granted sick pay, an end to the use of zero hours contracts and running the shop on co-operative principles (in other words, allowing staff to have a say in management). It was sent to Scarlett at the end of April. An initial meeting between Scarlett and the UVW yielded some agreements, though Scarlett told the meeting she was working six or seven days most weeks, and was earning less per hour than the booksellers, so needed to hire a manager, a move that would mean several of the booksellers would need to be let go.&lt;/p&gt;
    &lt;p&gt;Then there was the tricky business of the bookseller’s call for a co-operative management model. Since opening, the Scarlett Letters hadn’t made a month-by-month profit, but had been kept afloat by savings and a monthly donation of £10,000 from an anonymous “angel investor”. Only Scarlett knew the identity of the investor and she shared a response from the investor that they were considering cutting or even pulling their donation if the shop became a co-operative.&lt;/p&gt;
    &lt;p&gt;With the dispute at something of a stalemate, the booksellers reached for the most obvious lever available to them, unleashing a broadside of Instagram posts in Scarlett’s direction. A newly launched account said they were in “open dispute” with their employer over the threats to fire staff and the failure to meet all its demands. “The workers are queer, trans, racialised, disabled, sex workers and students,” it argued. “Their identities have been used to advertise and fundraise for the bookshop as a radical space whilst their voices are not listened to.” As might be expected, there was an outraged response online. Responses on Instagram accused the bookshop of being a “marketing campaign” as well as “colonial”.&lt;/p&gt;
    &lt;p&gt;Eight days later, Scarlett fired back on the shop’s Instagram, claiming they had attempted to, or were in the process of meeting, almost all the union’s demands. “Trying to create a space like this in advanced capitalism is extremely difficult,” it read. “The management targeted by this dispute is not a faceless collective of executives in boardrooms. It is one person, who is multiply marginalised, a known member of the community and for the past year has been working for six or seven days a week for the fraction of the salary offered to the booksellers.” It ended with a shocking revelation: the bookshop would now be closing. A meeting with an external HR firm was scheduled for the end of June to discuss the closure and the staff’s redundancies. Scarlett herself said she was surprised at how things escalated and insists she “sent requests to meet again four separate times,” but that “these were ignored or refused”.&lt;/p&gt;
    &lt;p&gt;And that might have been that. A sad, if fairly run-of-the-mill tale of how an unfortunate toilet clogging thwarted Bethnal Green’s heady dream of a radical bookshop. But that’s not how our story ends, because the soon-to-be jobless booksellers weren’t giving up that easily. In response to Scarlett’s decision, they hatched a plan: they were going to occupy the bookshop.&lt;/p&gt;
    &lt;p&gt;Parker is keen to state that they weren’t aiming to make Scarlett hand over the directorship of the shop, rather they wanted her to donate them the stock. Estimating it to be worth £70,000 (Scarlett puts this figure at closer to £10,000) they felt they could use it to start a new bookshop more closely aligned with their political values.&lt;/p&gt;
    &lt;p&gt;On the Sunday ahead of the meeting, the staff moved themselves into the crammed two floor bookshop. Parker came with a suitcase, planning to spend four days there. “I didn't have enough money to keep getting the train back and forth from Wimbledon,” they explain. “So I figured that I'll just put myself on the rota for four nights. I was getting a small payment from a porn site that wasn’t a significant amount of money, but enough for the train home.”&lt;/p&gt;
    &lt;p&gt;On Monday, they joined the video call on one computer, sitting as a group with the shelves of the store as their backdrop. The meeting would last just a few minutes. They made their demand for the stock to be transferred to them. The official from the HR firm explained, on behalf of Scarlett, that the bookshop’s legal obligations meant the books were “asset-locked” and that there was no legal mechanism to transfer stock to employees for free. As the shop’s director, Scarlett could even be liable and struck off starting a company if she did so.&lt;/p&gt;
    &lt;p&gt;But the booksellers didn’t waver. They announced they would be occupying the shop until their demands were met, and abruptly left the call. Next, they hurriedly unfurled a banner announcing the occupation across the storefront. They had the only set of keys, Parker told me, so thought that as long as no-one allowed management to access the building they could stay indefinitely, protected by squatter’s rights.&lt;/p&gt;
    &lt;p&gt;By 10pm that night, the three occupiers who volunteered to stay overnight went down to the basement and bedded down, steeling themselves for the weeks of resistance to come. In the end, it would be much shorter than expected.&lt;/p&gt;
    &lt;p&gt;What the group hadn’t accounted for, was the foresight of Marin Scarlett. As it happened, on the first day of the occupation she had arranged a locksmith, as well as a team to help her recover the stock. She’d covered every base: even hiring carpenters to remove the bookshelves and bringing along three legal observers to impartially oversee things. At around 4am, they arrived at Scarlett Letters and started drilling.&lt;/p&gt;
    &lt;p&gt;Over the next three hours, books were slid into boxes and shelves were unscrewed from walls. At some point in the night, Parker emerged up the stairwell and saw the whole sorry scene play out. “There were people I knew personally, and that just horrified me,” Parker says.&lt;/p&gt;
    &lt;p&gt;Now, the bookshop is a husk, almost entirely empty apart from the posters announcing future plans to reopen as “The People’s Letters” — a co-operative run by the booksellers that they believe will adhere to more leftist principles than its predecessor.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, Scarlett sees the whole debacle quite differently to the booksellers. She says she “had hoped to work with the union” and that “an improved sick pay was immediately implemented after our first meeting with UVW”. When the talks with the union broke down and things were taken to social media, sales tanked and it became clear that the store couldn’t stay open. The “shop would have stayed open until late July had the booksellers not tried to steal the stock, but their actions forced us to close several weeks early”. When she and a group of friends entered the shop to reclaim the books, they “did not know that anyone was in the property when we entered”.&lt;/p&gt;
    &lt;p&gt;Collins has spent 14 years in the trade union movement, but is equally baffled by how things managed to end where they did. “I've never experienced a dispute like it before,” he tells me. “I imagine I never will again.” Here ends the tale of Scarlett Letters, the radical bookshop the capital could have had, only for those dreams to be flushed away.&lt;/p&gt;
    &lt;p&gt;Welcome to The Londoner. We’re the capital’s new magazine, delivered entirely by email. Sign up to our mailing list and get two totally free editions of The Londoner every week: a Monday briefing, full of everything you need to know about that’s going on in the city; and an in-depth weekend piece like the one you're currently reading.&lt;/p&gt;
    &lt;p&gt;No ads, no gimmicks: just click the button below and get our unique brand of local journalism straight to your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt; How to comment:&lt;lb/&gt; If you are already a member, click here to sign in and leave a comment. &lt;lb/&gt; If you aren't a member, sign up here to be able to leave a comment. &lt;lb/&gt; To add your photo, click here to create a profile on Gravatar.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.the-londoner.co.uk/scarlett-letters-closure-left-wing-bookshop/"/><published>2025-09-27T18:15:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398348</id><title>Xeres: Uncensorable Peer-to-Peer Communications</title><updated>2025-09-27T21:07:42.327007+00:00</updated><content>&lt;doc fingerprint="2c83dcfd580f2a05"&gt;
  &lt;main&gt;
    &lt;p&gt;Where Friendship Meets Freedom&lt;/p&gt;
    &lt;p&gt;Decentralized application with Friend-to-Friend technology&lt;/p&gt;
    &lt;p&gt;Xeres allows you to interact with people in a fully decentralized way. No server or account required, which means: nobody can censor you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Purpose&lt;/head&gt;
    &lt;p&gt;Freedom of speech is of utmost importance. Without it, abuses are made and evil people take control. There’s a worrying convergence to centralization and electronic identification requirements lately. A healthy society cannot function without free communication. Xeres provides an alternative.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;head rend="h3"&gt;👤 Contacts&lt;/head&gt;
    &lt;p&gt;Use the contact manager to see your friends and discover new people. Change your avatar picture and set your availability.&lt;/p&gt;
    &lt;head rend="h3"&gt;💬 Chat&lt;/head&gt;
    &lt;p&gt;Chat with your friends and meet new people in decentralized chat rooms. Send images and files. Express your feelings with emoticons and stickers. Use distant chat to discuss securely beyond your direct friends.&lt;/p&gt;
    &lt;head rend="h3"&gt;📢 Forums&lt;/head&gt;
    &lt;p&gt;Subscribe to forums discussing any topic you like. You can even read and post while offline as they’re automatically synced when the network is up.&lt;/p&gt;
    &lt;head rend="h3"&gt;📂 File Sharing&lt;/head&gt;
    &lt;p&gt;Send files directly to your friends. Use a powerful search system to find what you want. Swarming accelerates file transfers and allows to share large files. Sharing and downloading files preserves your privacy thanks to anonymous tunnels technology.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Xeres uses the Retroshare protocol, which is a so called Friend-to-Friend system. It’s like Peer-to-Peer but you only connect to people you know. That way, your IP address is only known to your direct friends (but not their friends).&lt;/p&gt;
    &lt;p&gt;To connect with a friend you have to exchange IDs.&lt;/p&gt;
    &lt;p&gt;The connection between you and your friends is authenticated using strong asymmetric keys (PGP v4, RSA 3072-bits, SHA-256 integrity checksum, AES-128 encryption) and encrypted using TLS 1.3 with Perfect Forward Secrecy.&lt;/p&gt;
    &lt;p&gt;On top of that mesh network, Xeres implements services to securely and anonymously exchange data with other peers beyond your friends.&lt;/p&gt;
    &lt;head rend="h3"&gt;⚙️ Technical Specifications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🤝 Network topology: decentralized Friend to Friend network (F2F)&lt;/item&gt;
      &lt;item&gt;🔌 Transport: TCP, Tor (client only), I2P (client only)&lt;/item&gt;
      &lt;item&gt;🕳️ Port forwarding: UPNP&lt;/item&gt;
      &lt;item&gt;🚫 Censorship resistant&lt;/item&gt;
      &lt;item&gt;#️⃣ Optionally uses the DHT (from BitTorrent) to locate remote friends&lt;/item&gt;
      &lt;item&gt;🏠 LAN discovery to find local friends&lt;/item&gt;
      &lt;item&gt;👋 Compatible with Retroshare 0.6.6 or higher&lt;/item&gt;
      &lt;item&gt;🛠️ Strong and secure encryption (hardware accelerated)&lt;/item&gt;
      &lt;item&gt;🖥️ GPU accelerated and modern looking user interface with several themes (JavaFX)&lt;/item&gt;
      &lt;item&gt;📶 Remote access using a REST interface&lt;/item&gt;
      &lt;item&gt;📖 Free software (GPL), Open Source&lt;/item&gt;
      &lt;item&gt;😃 Available for Windows, Linux, macOS and Android&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://xeres.io/"/><published>2025-09-27T18:39:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398467</id><title>LLM Observability in the Wild – Why OpenTelemetry Should Be the Standard</title><updated>2025-09-27T21:07:42.063547+00:00</updated><content>&lt;doc fingerprint="7f008d121bb1bcca"&gt;
  &lt;main&gt;
    &lt;p&gt;A few days ago I hosted a live conversation with Pranav, co-founder of Chatwoot, about issues his team was running into with LLM observability.&lt;/p&gt;
    &lt;p&gt;The short version: building, debugging, and improving AI agents in production gets messy fast. There's multiple competing standards for default libraries for LLM observability. And many such libraries like OpenInference which claim to be based on OpenTelemetry don't strictly adhere to it's conventions. This introduces problems for users who are trying to get better observability across their stack.&lt;/p&gt;
    &lt;p&gt;Here’s a write-up of what we covered and what I think it means for anyone shipping LLM features into real products. Feel free to watch the complete video&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem Emerges in Prod&lt;/head&gt;
    &lt;p&gt;Pranav and I go way back to our YC days in 2021, and it's always interesting to see how our paths have evolved. Chatwoot has built something really compelling - an open-source customer support platform that unifies conversations across every channel you can imagine: live chat, email, WhatsApp, social media, you name it. All in a single dashboard.&lt;/p&gt;
    &lt;p&gt;But here's where it gets interesting. They've built an AI agent called "Captain" that can work across all these channels. You build the logic once, and it can handle support queries whether they come through email, live chat, or WhatsApp. Pretty neat, right?&lt;/p&gt;
    &lt;p&gt;The problem started showing up in production in the most unexpected ways. Sometimes their AI would randomly respond in Spanish when it absolutely shouldn't. Other times, responses just weren't quite right, and they had no visibility into why.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Quest for LLM Observability&lt;/head&gt;
    &lt;p&gt;This is where Pranav's journey into LLM observability began, it mirrors what I've been seeing across many companies building LLM applications. You need to understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What documents were retrieved for a RAG query?&lt;/item&gt;
      &lt;item&gt;Which tool calls were made?&lt;/item&gt;
      &lt;item&gt;What was the exact input and output at each step?&lt;/item&gt;
      &lt;item&gt;Why did the AI make certain decisions?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without this visibility, you're essentially flying blind in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Standards Problem&lt;/head&gt;
    &lt;p&gt;Here's where things get really interesting, and frankly, frustrating. Pranav explored several solutions:&lt;/p&gt;
    &lt;p&gt;OpenAI's native tracing looked promising with rich, detailed traces showing guardrails, agent flows, and tool calls. But it's tightly coupled to OpenAI's agent framework. Also, it only provides traces as an atomic unit. If you want to filter spans based on attributes or just examine specific spans directly, you can't do that.&lt;/p&gt;
    &lt;p&gt;New Relic was easy to integrate since they already use it, and it supports OpenTelemetry. But the UI required clicking through 5-6 layers just to see relevant information. Not ideal when you're trying to debug production issues.&lt;/p&gt;
    &lt;p&gt;Phoenix caught their attention because it follows the OpenInference standard, which provides much richer, AI-specific span types. You can easily filter for just LLM calls, tool calls, or agent spans. The traces are beautiful and informative.&lt;/p&gt;
    &lt;p&gt;But here's the kicker: Chatwoot is primarily a Ruby on Rails shop, and guess what? No Ruby SDK for OpenInference. Moreover, Phoenix doesn't completely adhere to OTel semantic conventions, so if you send it telemetry data directly via OpenTelemetry, it doesn't recognize the type of spans, etc.&lt;/p&gt;
    &lt;p&gt;As shown in the example above, Phoenix doesn't shows data sent with OpenTelemetry span kinds as &lt;code&gt;unknown&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;The OpenTelemetry vs OpenInference Divide&lt;/head&gt;
    &lt;p&gt;This is where the conversation got really technical and revealed a fundamental industry problem. There are essentially two standards emerging:&lt;/p&gt;
    &lt;p&gt;OpenTelemetry is the industry standard. It has libraries for every language, it's production-ready, and it's widely adopted. But it was built for traditional applications, not AI workflows. It only supports basic span types: internal, server, client, producer, consumer. That's it.&lt;/p&gt;
    &lt;p&gt;OpenInference was created specifically for AI applications. It has rich span types like LLM, tool, chain, embedding, agent, etc. You can easily query for "show me all the LLM calls" or "what were all the tool executions." But it's newer, has limited language support, and isn't as widely adopted.&lt;/p&gt;
    &lt;p&gt;The tragic part? OpenInference claims to be "OpenTelemetry compatible," but as Pranav discovered, that compatibility is shallow. You can send OpenTelemetry format data to Phoenix, but it doesn't recognize the AI-specific semantics and just shows everything as "unknown" spans.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ruby Problem Makes It Worse&lt;/head&gt;
    &lt;p&gt;For teams using languages like Ruby that don't have direct OpenInference SDK support, this becomes even more challenging. Pranav had to choose between:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Building an SDK from scratch for Ruby&lt;/item&gt;
      &lt;item&gt;Using OpenTelemetry and losing AI-specific insights&lt;/item&gt;
      &lt;item&gt;Switching to a different language stack just for AI observability (way tougher)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these are great options.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why we (still) bias to OpenTelemetry&lt;/head&gt;
    &lt;p&gt;At SigNoz we’re all-in on OpenTelemetry. One reason: OTel’s consistency enables out-of-the-box experiences across your whole stack. Example: we can auto-surface external API usage and performance based on span kinds and attributes. When parts of the app send telemetry via non-OTel conventions, those views degrade.&lt;/p&gt;
    &lt;p&gt;Chatwoot lands similarly: their entire product already emits OTel. Pulling in a second telemetry standard just for LLMs fragments the picture and complicates how they go about observability. This also silos their observability into different products which makes it difficult to solves issues when they occur.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways for builders&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pick one telemetry backbone - If most of your app is OTel, prefer staying OTel-native for LLMs too, even if it means adding richer attributes until GenAI conventions catch up.&lt;/item&gt;
      &lt;item&gt;LLM specific libraries - Even if you have to use LLM specific libraries like OpenInference, try to keep your usage as close to OpenTelemetry as possible so that you are aware what non-OTel attributes you are using which may break things.&lt;/item&gt;
      &lt;item&gt;Follow OTel GenAI working group - There is active work happening in OTel Gen AI working group. Follow the work happening there and do share your use cases so that the standards which OpenTelemetry builds are able to cater to most common use cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As the LLM space is still evolving rapidly, we as a community need to share our voices so that the standards are robust.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’re doing at SigNoz&lt;/head&gt;
    &lt;p&gt;We’re continuing to invest in OpenTelemetry-native LLM observability so teams don’t have to choose between stability and clarity. Concretely, that means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Clear dashboards and traces when LLM calls are modeled using OTel spans/attributes. You can find examples and dashboards in our LLM observability docs. Though we have also use LLM specific libraries like OpenInference in our docs (as they are still the easiest way for ppl to get started), we have kept the dashboards as close to OTel standards as possible. We also plan to actively update this as OTel GenAI semantic conventions become more mature.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Guidance and examples for popular frameworks (LangChain, LlamaIndex, etc.) on emitting OTel-friendly telemetry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build features leveraging OpenTelemetry semantic conventions so that you get great out-of-box experience in SigNoz and adhere to thoughtful defaults that keep your services, DBs, queues, and LLM agents—in one coherent picture.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re wrestling with these trade-offs, we’d love to hear what’s breaking for you and what “rich semantics” you actually use day-to-day.&lt;/p&gt;
    &lt;head rend="h2"&gt;What next?&lt;/head&gt;
    &lt;p&gt;Huge thanks to Pranav for going deep, especially from the Ruby perspective. If you’re shipping AI features and care about operability, add your voice: push for richer GenAI semantics in OpenTelemetry, and share real traces (sanitized) that show what you need to see.&lt;/p&gt;
    &lt;p&gt;If you want to compare notes or need help getting your LLM telemetry into an OTel-native view, ping me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://signoz.io/blog/llm-observability-opentelemetry/"/><published>2025-09-27T18:56:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398468</id><title>Docker Was Too Slow, So We Replaced It: Nix in Production [video]</title><updated>2025-09-27T21:07:41.019730+00:00</updated><content>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=iPoL03tFBtU"/><published>2025-09-27T18:56:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398719</id><title>NSPM-7 labels common beliefs as terrorism 'indicators'</title><updated>2025-09-27T21:07:40.808095+00:00</updated><content>&lt;doc fingerprint="bed1c590149dc133"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Trump’s NSPM-7 Labels Common Beliefs As Terrorism “Indicators”&lt;/head&gt;
    &lt;head rend="h3"&gt;New directive targets “anti-Christian,” “anti-American,” and “anti-capitalism” opinions&lt;/head&gt;
    &lt;p&gt;With the mainstream media distracted by the made-for-TV drama of James Comey’s indictment, Trump has signed a little-noticed national security directive identifying “anti-Christian” and “anti-American” views as indicators of radical left violence. Called National Security Presidential Memorandum 7, it’s being referred to as “NSPM-7” by administration insiders.&lt;/p&gt;
    &lt;p&gt;“This is the first time in American history that there is an all-of-government effort to dismantle left wing terrorism,” Trump’s homeland security advisor Stephen Miller said, referring to the issuance.&lt;/p&gt;
    &lt;p&gt;To the extent that the major media noticed the directive at all, they (even C-SPAN!) incorrectly labeled it an “executive order,” like this week’s designation of “Antifa” as a domestic terrorist organization.&lt;/p&gt;
    &lt;p&gt;It’s hard to overstate how much different NSPM-7 is from the over 200 executive orders Trump has frantically signed since coming back into office.&lt;/p&gt;
    &lt;p&gt;An executive order publicly lays out the course of day-to-day federal government operations; whereas a national security directive is a sweeping policy decree for the defense, foreign policy, intelligence, and law enforcement apparatus. National security directives are often secret, but in this case the Trump administration chose to publish NSPM-7 — only the seventh since he’s come into office.)&lt;/p&gt;
    &lt;p&gt;Previous national security directives have been controversial, even politically earthshaking. In 1980, for example, President Jimmy Carter signed the Top Secret Presidential Directive 59 (“PD-59”) directing new nuclear warfighting policies that persisted until the end of the Cold War. When revealed, PD-59 caused a public furor.&lt;/p&gt;
    &lt;p&gt;Similarly, President George W. Bush signed a series of classified national security directives after 9/11, the most famous of which authorized NSA’s unlawful domestic intercepts, a directive that wasn’t publicly revealed until four years later.&lt;/p&gt;
    &lt;p&gt;In NSPM-7, “Countering Domestic Terrorism and Organized Political Violence,” President Trump directs the Justice Department, the FBI, and other national security agencies and departments to fight his version of political violence in America, retooling a network of Joint Terrorism Task Forces to focus on “leftist” political violence in America. This vast counterterrorism army, made up of federal, state, and local agents would, as Trump aide Stephen Miller said, form “the central hub of that effort.”&lt;/p&gt;
    &lt;p&gt;NSPM-7 directs a new national strategy to “disrupt” any individual or groups “that foment political violence,” including “before they result in violent political acts.”&lt;/p&gt;
    &lt;p&gt;In other words, they’re targeting pre-crime, to reference Minority Report.&lt;/p&gt;
    &lt;p&gt;The Trump administration isn’t only targeting organizations or groups but even individuals and “entities” whom NSPM-7 says can be identified by any of the following “indica” (indicators) of violence:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;anti-Americanism,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;anti-capitalism,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;anti-Christianity,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;support for the overthrow of the United States Government,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;extremism on migration,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;extremism on race,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;extremism on gender&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hostility towards those who hold traditional American views on family,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hostility towards those who hold traditional American views on religion, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hostility towards those who hold traditional American views on morality.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“The United States requires a national strategy to investigate and disrupt networks, entities, and organizations that foment political violence so that law enforcement can intervene in criminal conspiracies before they result in violent political acts,” the directive states (emphasis mine).&lt;/p&gt;
    &lt;p&gt;A “pre-crime” endeavor, preventing attacks before they happen, is core to the post-9/11 concept of counterterrorism itself. No longer satisfied to investigate acts of terrorism after the fact to bring terrorists to justice, the Bush administration adopted preemption. Overseas, that led to aerial assassination by drones and “special operations” kill missions. Domestically, it led to a counter-terrorism campaign whose hallmark was excessive and illegal government surveillance and the use of undercover agents and “confidential human sources” to trap (and entrap) would-be terrorists.&lt;/p&gt;
    &lt;p&gt;Now, with Donald Trump’s directive retooling the counter-terror apparatus to go after Americans at home, this means monitoring political activity, or speech, as an investigative method to discover “radicalism.” (Contrary to other national security documents all during the post-Watergate era, NSPM-7 doesn’t even mention the First Amendment or the fundamental right of Americans to organize and protest.)&lt;/p&gt;
    &lt;p&gt;The focus on speech is evident throughout NSPM-7. The directive says that political violence is the result of “organized campaigns” that often begin (with the left) dehumanizing targets in “anonymous chat foras, in-person meetings, social media, and even educational institutions.”&lt;/p&gt;
    &lt;p&gt;To give a sense of how broad this formulation is, Trump’s earlier designation of Antifa as a domestic terrorist group was accompanied by a White House fact sheet singling out people who “celebrated” Luigi Mangione, the alleged killer of UnitedHealthcare CEO Brian Thompson last December. As I wrote at the time, this describes a lot of Americans!&lt;/p&gt;
    &lt;p&gt;Trump’s new national security memorandum also alludes to Mangione but adds to it even larger categories of potential targets.&lt;/p&gt;
    &lt;p&gt;NSPM-7 is fundamentally a law enforcement directive, and it dispenses with the complications of using the active duty military or the National Guard in pursuit of political violence. It directs the Department of Justice to focus the FBI’s approximately 200 Joint Terrorism Task Forces (JTTFs) to the new mission. The FBI network of task forces comprises over 4,000 members—including FBI personnel and task force officers (or TFOs) from more than 500 state and local agencies and 50 federal agencies, including special agents, police officers, intelligence analysts and surveillance technicians. First established in New York City in 1980 to systematize FBI and NYPD cooperation, today there are task forces around the country, including at least one in each of the FBI’s 55 field offices.&lt;/p&gt;
    &lt;p&gt;For the Trump White House, the beauty of using an already existing network is that it bypasses Congressional oversight and scrutiny and even obscures federal activity to governors and legislatures at the state level. States, cities, and local police have already signed Memoranda of Agreements with the feds to fight terrorism and officers are already assigned as task force officers.&lt;/p&gt;
    &lt;p&gt;NSPM-7 says the JTTFs “shall investigate” potential federal crimes relating to “acts of recruiting or radicalizing persons” for the purpose of “political violence, terrorism, or conspiracy against rights; and the violent deprivation of any citizen’s rights.” It authorizes the JTTFs to investigate individuals, organizations, and funders “responsible for, sponsor, or otherwise aid and abet the principal actors engaging in the criminal conduct.”&lt;/p&gt;
    &lt;p&gt;“The Attorney General shall issue specific guidance that ensures domestic terrorism priorities include politically motivated terrorist acts such as organized doxing campaigns, swatting, rioting, looting, trespass, assault, destruction of property, threats of violence, and civil disorder,” NSPM-7 says. Civil disorder?&lt;/p&gt;
    &lt;p&gt;I don’t want to sound hyperbolic but the plain truth is that NSPM-7 is a declaration of war on anyone who does not support the Trump administration and its agenda. Yes, it repeats the word “violent” over and over to purport only to go after citizens who are moved to take up arms, but it also directs monitoring and intelligence collection to map and target the new “evildoers,” to borrow a Bush label he took from the Bible just days after 9/11.&lt;/p&gt;
    &lt;p&gt;The partisan focus couldn’t be more obvious.&lt;/p&gt;
    &lt;p&gt;“The real problem is this: since Charlie [Kirk] was murdered — a friend of mine, assassinated — nothing’s changed on their side,” White House counter-terrorism czar Sebastian Gorka told Newsmax after NSPM-7 was signed. “Not one leader —not one left wing thought leader, member of Congress, Senator — nobody has said we distance ourselves from the violent rhetoric.”&lt;/p&gt;
    &lt;p&gt;“The left refuses to rid themselves of the justification for violence,” Gorka continued, “and as such, President Trump is taking measures to protect us from the violent rhetoric that becomes snipers and bullets.”&lt;/p&gt;
    &lt;p&gt;— Edited by William M. Arkin&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.kenklippenstein.com/p/trumps-nspm-7-labels-common-beliefs"/><published>2025-09-27T19:35:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398802</id><title>iPhone 17 chip becomes the fastest single-core CPU in the world on PassMark</title><updated>2025-09-27T21:07:40.671720+00:00</updated><content>&lt;doc fingerprint="a6b8cc1d02b785d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Apple's iPhone 17 chip becomes the fastest single-core CPU in the world on PassMark, beating PC chips and Apple's own M3 Ultra — passively-cooled A19 CPU catapults past power-hungry competitors&lt;/head&gt;
    &lt;p&gt;A19 has everyone beat, including Apple itself.&lt;/p&gt;
    &lt;p&gt;Apple's latest generation of iPhones is equipped with its A19 chips — the standard A19 on iPhone 17 and the A19 Pro on iPhone 17 Air and Pros — which represent the best the company has to offer, literally. In PassMark's single-threaded benchmark, the A19 produced the best numbers of any chip available, including fully-fledged desktop SKUs. It did that while consuming significantly less power and being passively cooled. At least in this hyper-specific case, Apple's A19 has become the fastest CPU available.&lt;/p&gt;
    &lt;p&gt;Both the A19 and A19 Pro benchmarked within the margin of error of each other; however, officially, it was the regular A19 that posted 5,149 points to claim the single-thread performance crown. The A19 Pro scored 5,088 points, which makes sense considering both chips share the same cores, just differing amounts of them. The A19 beats heavy hitters like Apple's own desktop-class M3 Ultra (both 28- and 32-core variants), Intel's Core Ultra 9 285K, and even the EPYC 4585PX from AMD — all of which would be actively cooled.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is a pretty incredible single threaded benchmark result from Apple with the A19. Plus it is claimed to use only 12watts. For comparison the Ultra 9 is 125W+ and EPYC 4585PX is 170W+https://t.co/ysO73jpaVv pic.twitter.com/e9niPV5I3ySeptember 26, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The tweet caption lists nominal TDPs of these chips for comparison, but that's not what a single-core load would actually use. Since it's incredibly difficult to pinpoint that, PassMark itself estimated the single-threaded power consumption in a reply, saying the A19 is likely using 4W, the 285K is using 44W, and the EPYC is using 56W. Even if those 1/3 assumptions are wrong, the delta is so high between the three that it doesn't really matter. The A19 is miles ahead in terms of efficiency.&lt;/p&gt;
    &lt;p&gt;Where it falters, of course, is multi-threaded performance. It doesn't scale upward when you take more/all cores into account, but that's to be expected with a mobile-only chip, given that it simply has fewer cores than every other CPU on the list. Moreover, keep in mind that the A19 is inside the iPhone 17, which doesn't have a vapor chamber, so it's even more impressive for it to pull these kinds of numbers. Then again, this isn't precisely an uber-scientific test, so don't take these results at face value.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Hassam Nasir is a die-hard hardware enthusiast with years of experience as a tech editor and writer, focusing on detailed CPU comparisons and general hardware news. When he’s not working, you’ll find him bending tubes for his ever-evolving custom water-loop gaming rig or benchmarking the latest CPUs and GPUs just for fun.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;Sippincider&lt;/header&gt;Very nice. Maybe I'm an outlier, but phone processors and cameras have been mass overkill for anything I've needed for several years now.Reply&lt;lb/&gt;Like when the classic auto industry chased itself with the tallest fins and biggest V8s. I just need to get between places...&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Amdlova&lt;/header&gt;Reply&lt;quote/&gt;AMD want to milk the consumer... not give the real performance.Gururu said:How come the AMDs suck so bad?&lt;lb/&gt;Apple and nvidia will take all market share possible.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Exploding PSU&lt;/header&gt;ReplySippincider said:Very nice. Maybe I'm an outlier, but phone processors and cameras have been mass overkill for anything I've needed for several years now.&lt;lb/&gt;Like when the classic auto industry chased itself with the tallest fins and biggest V8s. I just need to get between places...&lt;lb/&gt;I remember back in Snapdragon 808 / 810 days (those were the years when I still followed smartphone hardware rather closely), I thought the exact same thing. A few Windows Phone models were able to run a version of Windows 10, and to me that signified we had reached the top pinnacle of smartphone CPU hardware.&lt;lb/&gt;I mean, come to think of it, a smartphone could run a PC operating system! The hardware got to be insanely overkill. Coupled with SD 810's overheating problem, I thought there wouldn't be anything faster from that point on when it comes to phone CPU.&lt;lb/&gt;But look at where we are today... I have a super old LG G4 with a 808, which was one of the top flagship back in the day, and it can't even run the YouTube app now. A quick glance at some Geekbench numbers show that my midrange Samsung A-series has almost 7 times higher score in the multi-core test compared to the G4.&lt;lb/&gt;March of progress is scary sometimes...&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Notton&lt;/header&gt;Reply&lt;quote/&gt;Probably because the benchmark heavily favors productivity tasks that don't rely on a large cache.Gururu said:How come the AMDs suck so bad?&lt;lb/&gt;Cinebench, for instance, heavily favors Intel over AMD.&lt;lb/&gt;However the roles are completely reversed in games, with the i9-285K being slower than the R7 9800X3D. Heck, even the i9-14900K will beat the i9-285K in games.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Dementoss&lt;/header&gt;Reply&lt;quote/&gt;Think about what you're saying before you type...Gururu said:How come the AMDs suck so bad?&lt;lb/&gt;How many people or organisations, that buy R9 9950 X3D or, EPYC 4565P CPUs do you think are going to be buying either of those, on the basis of their single-core performance?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;ekio&lt;/header&gt;Why do we still have x86 for consumer pc then when arm beat it for low core count power, for efficiency, for price…Reply&lt;lb/&gt;Time to put these chips in computers.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Notton&lt;/header&gt;Reply&lt;quote/&gt;Because it costs money to port x86 software over to ARM, and x86 power efficiency has taken enough strides in recent years that most people would rather not be inconvenienced by software incompatibility on ARM.ekio said:Why do we still have x86 for consumer pc then when arm beat it for low core count power, for efficiency, for price…&lt;lb/&gt;Time to put these chips in computers.&lt;lb/&gt;As for pricing, IDK why you think ARM is cheap.&lt;lb/&gt;You get what you pay for, be it ARM or x86, with better value on x86.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;James Chou&lt;/header&gt;So where is the Snapdragon 8 Elite Gen 5 that allegedly scores higher than the A19, albeit with higher power usage? The C1 Ultra in the Dimensity 9500 would also be interesting to benchmark.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/cpus/apples-a19-becomes-the-fastest-single-core-cpu-in-the-world-on-passmark-beating-pc-chips-and-apples-own-m3-ultra-passively-cooled-iphone-17-chip-catapults-past-power-hungry-competitors"/><published>2025-09-27T19:48:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45398900</id><title>Firefox context menu adds "Search Image with Google Lens"</title><updated>2025-09-27T21:07:39.353076+00:00</updated><content>&lt;doc fingerprint="19869ae093a4ab6e"&gt;
  &lt;main&gt;
    &lt;p&gt;24-09-2025 11:49 AM - edited 25-09-2025 01:05 PM&lt;/p&gt;
    &lt;p&gt;Hi everyone,&lt;/p&gt;
    &lt;p&gt;We’re rolling out a new feature over the next few weeks: visual search powered by Google Lens!&lt;/p&gt;
    &lt;p&gt;Why are we doing this? For several years you’ve been able to effectively search the internet with a number of search providers in Firefox. As those solutions have evolved to include visual search we’ve decided to support image-based search via Lens in Firefox. With this integration, we’re offering a frictionless, fast, and a curiosity-sparking way to (as Google puts it) “search what you see”.&lt;/p&gt;
    &lt;p&gt;With just a right-click on any image, you’ll be able to:&lt;lb/&gt; ✨ Find similar products, places, or objects&lt;lb/&gt; ✨ Copy, translate, or search text from images&lt;lb/&gt; ✨ Get inspiration for learning, travel, or shopping&lt;/p&gt;
    &lt;p&gt;Look for the new “Search Image with Google Lens” option in your right-click menu (tagged with a NEW badge at first). &lt;/p&gt;
    &lt;p&gt;Try it for yourself by right clicking this image:&lt;/p&gt;
    &lt;p&gt;This is a desktop-only feature, and it will start gradually rolling out worldwide. Note: Google must be set as your default search engine for this feature to appear.&lt;/p&gt;
    &lt;p&gt;We’ll be listening closely to your feedback as we roll it out. Some of the things we’re wondering about:&lt;/p&gt;
    &lt;p&gt;We can’t wait to hear your thoughts as the rollout begins!&lt;/p&gt;
    &lt;p&gt;Photo credit: Лариса Исаева on Pexels&lt;/p&gt;
    &lt;p&gt;24-09-2025 12:22 PM - edited 24-09-2025 12:30 PM&lt;/p&gt;
    &lt;p&gt;Hello&lt;/p&gt;
    &lt;p&gt;Additional information.&lt;lb/&gt;https://connect.mozilla.org/t5/ideas/implement-google-lenses-or-similar-functionality/idc-p/106126/g...&lt;/p&gt;
    &lt;p&gt;24-09-2025 10:11 PM&lt;/p&gt;
    &lt;p&gt;Hello&lt;/p&gt;
    &lt;p&gt;Additional information.&lt;/p&gt;
    &lt;p&gt;About Studies.&lt;lb/&gt;https://support.mozilla.org/en-US/kb/shield&lt;lb/&gt;An test Remove button next to the study you want to opt out of.&lt;/p&gt;
    &lt;p&gt;For those interested, who do not practice, studies.&lt;/p&gt;
    &lt;p&gt;You can try.&lt;/p&gt;
    &lt;p&gt;1 - Go to Configuration Editor for Firefox https://support.mozilla.org/en-US/kb/about-config-editor-firefox&lt;lb/&gt;2 - Enter a search term browser.search.visualSearch.featureGate&lt;lb/&gt;You can double-click on the preference to set the value to true&lt;/p&gt;
    &lt;p&gt;27-09-2025 01:32 PM&lt;/p&gt;
    &lt;p&gt;This is a perfect example of functionality that should be in an extension. I don't want a misclick sending images I'm viewing to anybody, least of all Google.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://connect.mozilla.org/t5/discussions/new-in-firefox-desktop-only-visual-search/m-p/106216#M41026"/><published>2025-09-27T20:01:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45399063</id><title>Linux 6.18 Will Fix Lockups When Systemd Units Read Lots of Files</title><updated>2025-09-27T21:07:39.206120+00:00</updated><content>&lt;doc fingerprint="903339021fa7d721"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Linux 6.18 Will Fix Lockups When Systemd Units Read Lots Of Files&lt;/head&gt;
    &lt;p&gt; Linux engineer at Microsoft Christian Brauner sent out his set of 12 pull requests touching the VFS portion of the Linux kernel. These changes for the Linux 6.18 kernel include one pull request that touches the writeback code to address a situation of lockups being reported by users when systemd units read lots of files. &lt;lb/&gt;The problem at hand with these lockups will manifest when a systemd unit reads lots of files from a file-system mounted with the "lazytime" mount option. Lazytime being the option for only initially updating the access/modify/creation time on the in-memory version of the file inode to help with performance and reduce writes to disk. The on-disk timestamps are then updated during fsync and similar operations or when evicted from memory, among other possibilities.&lt;lb/&gt;Linux developers found that for systemd units reading many files with the lazytime mount option, there can reach "hundreds of thousands or millions" dirty inodes on cgroup exit to the parent cgroup. In turn the system can be hit for hours with 100% CPU usage&lt;lb/&gt;The pull request elaborates on the problem:&lt;lb/&gt;The pull request also has a small sample script for demonstrating the issue on existing Linux kernel releases.&lt;lb/&gt;This issue should be addressed once these patches are merged for the upcoming Linux 6.18 merge window.&lt;/p&gt;
    &lt;p&gt;The problem at hand with these lockups will manifest when a systemd unit reads lots of files from a file-system mounted with the "lazytime" mount option. Lazytime being the option for only initially updating the access/modify/creation time on the in-memory version of the file inode to help with performance and reduce writes to disk. The on-disk timestamps are then updated during fsync and similar operations or when evicted from memory, among other possibilities.&lt;/p&gt;
    &lt;p&gt;Linux developers found that for systemd units reading many files with the lazytime mount option, there can reach "hundreds of thousands or millions" dirty inodes on cgroup exit to the parent cgroup. In turn the system can be hit for hours with 100% CPU usage&lt;/p&gt;
    &lt;p&gt;The pull request elaborates on the problem:&lt;/p&gt;
    &lt;quote&gt;"This contains work adressing lockups reported by users when a systemd unit reading lots of files from a filesystem mounted with the lazytime mount option exits.&lt;lb/&gt;With the lazytime mount option enabled we can be switching many dirty inodes on cgroup exit to the parent cgroup. The numbers observed in practice when systemd slice of a large cron job exits can easily reach hundreds of thousands or millions.&lt;lb/&gt;The logic in inode_do_switch_wbs() which sorts the inode into appropriate place in b_dirty list of the target wb however has linear complexity in the number of dirty inodes thus overall time complexity of switching all the inodes is quadratic leading to workers being pegged for hours consuming 100% of the CPU and switching inodes to the parent wb."&lt;/quote&gt;
    &lt;p&gt;The pull request also has a small sample script for demonstrating the issue on existing Linux kernel releases.&lt;/p&gt;
    &lt;p&gt;This issue should be addressed once these patches are merged for the upcoming Linux 6.18 merge window.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.phoronix.com/news/Linux-6.18-Writeback-Lockups"/><published>2025-09-27T20:26:39+00:00</published></entry></feed>