<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2026-02-06T13:53:23.974465+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46901233</id><title>Show HN: Smooth CLI – Token-efficient browser for AI agents</title><updated>2026-02-06T13:53:37.904209+00:00</updated><content>&lt;doc fingerprint="5ddd6cd2ab913209"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;The Problem&lt;/head&gt;AI agents like Claude Code are powerful, but they’re mostly stuck in the terminal. Meanwhile, most valuable work happens in the browser. Current browser tools for agents (like&lt;code&gt;--chrome&lt;/code&gt;, Playwright MCP, agent-browser) all make the same mistake: they expose low-level actions like click, type, and scroll. This forces your agent to think about button positions instead of your actual goals.
This creates three problems:
&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Problem&lt;/cell&gt;&lt;cell role="head"&gt;Why it matters&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Slow &amp;amp; expensive&lt;/cell&gt;&lt;cell&gt;Using a massive model to click buttons is wasteful. Every action costs tokens and time.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Context pollution&lt;/cell&gt;&lt;cell&gt;Every click and keystroke fills up the context window with UI noise instead of your task.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Wrong expertise&lt;/cell&gt;&lt;cell&gt;General-purpose models aren’t trained to handle iframes, shadow DOMs, and the messy reality of websites.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;The Solution&lt;/head&gt;Smooth CLI is a browser built for AI agents. Instead of exposing hundreds of low-level tools, it gives agents a simple natural language interface. Your agent says what it wants. Smooth figures out how to do it.&lt;head rend="h2"&gt;20x faster&lt;/head&gt;&lt;p&gt;A specialized model handles the clicking so your agent can focus on thinking.&lt;/p&gt;&lt;head rend="h2"&gt;5x cheaper&lt;/head&gt;&lt;p&gt;Stop using 1T+ params models and burning tokens on UI navigation.&lt;/p&gt;&lt;head rend="h2"&gt;Use your IP&lt;/head&gt;&lt;p&gt;Route traffic through your machine to avoid captchas and access geo-restricted content.&lt;/p&gt;&lt;head rend="h2"&gt;Secure by design&lt;/head&gt;&lt;p&gt;Runs in an isolated environment with no permissions by default.&lt;/p&gt;&lt;head rend="h2"&gt;Unlimited browsers&lt;/head&gt;&lt;p&gt;Launch as many parallel browsers as needed, on-demand.&lt;/p&gt;&lt;head rend="h2"&gt;Fully managed&lt;/head&gt;&lt;p&gt;No setup, no configuration. Browsers run in the cloud, ready instantly.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://docs.smooth.sh/cli/overview"/><published>2026-02-05T16:13:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46901768</id><title>Maihem (YC W24): hiring senior robotics perception engineer (London, on-site)</title><updated>2026-02-06T13:53:37.666837+00:00</updated><content>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jobs.ashbyhq.com/maihem/8da3fa8b-5544-45de-a99e-888021519758"/><published>2026-02-05T17:00:07+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902223</id><title>Claude Opus 4.6</title><updated>2026-02-06T13:53:33.892970+00:00</updated><content>&lt;doc fingerprint="754d8ede3f97caef"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introducing Claude Opus 4.6&lt;/head&gt;&lt;p&gt;We’re upgrading our smartest model.&lt;/p&gt;&lt;p&gt;The new Claude Opus 4.6 improves on its predecessor’s coding skills. It plans more carefully, sustains agentic tasks for longer, can operate more reliably in larger codebases, and has better code review and debugging skills to catch its own mistakes. And, in a first for our Opus-class models, Opus 4.6 features a 1M token context window in beta.&lt;/p&gt;&lt;p&gt;Opus 4.6 can also apply its improved abilities to a range of everyday work tasks: running financial analyses, doing research, and using and creating documents, spreadsheets, and presentations. Within Cowork, where Claude can multitask autonomously, Opus 4.6 can put all these skills to work on your behalf.&lt;/p&gt;&lt;p&gt;The model’s performance is state-of-the-art on several evaluations. For example, it achieves the highest score on the agentic coding evaluation Terminal-Bench 2.0 and leads all other frontier models on Humanity’s Last Exam, a complex multidisciplinary reasoning test. On GDPval-AA—an evaluation of performance on economically valuable knowledge work tasks in finance, legal, and other domains1—Opus 4.6 outperforms the industry’s next-best model (OpenAI’s GPT-5.2) by around 144 Elo points,2 and its own predecessor (Claude Opus 4.5) by 190 points. Opus 4.6 also performs better than any other model on BrowseComp, which measures a model’s ability to locate hard-to-find information online.&lt;/p&gt;&lt;p&gt;As we show in our extensive system card, Opus 4.6 also shows an overall safety profile as good as, or better than, any other frontier model in the industry, with low rates of misaligned behavior across safety evaluations.&lt;/p&gt;&lt;p&gt;In Claude Code, you can now assemble agent teams to work on tasks together. On the API, Claude can use compaction to summarize its own context and perform longer-running tasks without bumping up against limits. We’re also introducing adaptive thinking, where the model can pick up on contextual clues about how much to use its extended thinking, and new effort controls to give developers more control over intelligence, speed, and cost.&lt;/p&gt;&lt;p&gt;We’ve made substantial upgrades to Claude in Excel, and we’re releasing Claude in PowerPoint in a research preview. This makes Claude much more capable for everyday work.&lt;/p&gt;&lt;p&gt;Claude Opus 4.6 is available today on claude.ai, our API, and all major cloud platforms. If you’re a developer, use &lt;code&gt;claude-opus-4-6&lt;/code&gt; via the Claude API. Pricing remains the same at $5/$25 per million tokens; for full details, see our pricing page.&lt;/p&gt;&lt;p&gt;We cover the model, our new product updates, our evaluations, and our extensive safety testing in depth below.&lt;/p&gt;&lt;head rend="h2"&gt;First impressions&lt;/head&gt;&lt;p&gt;We build Claude with Claude. Our engineers write code with Claude Code every day, and every new model first gets tested on our own work. With Opus 4.6, we’ve found that the model brings more focus to the most challenging parts of a task without being told to, moves quickly through the more straightforward parts, handles ambiguous problems with better judgment, and stays productive over longer sessions.&lt;/p&gt;&lt;p&gt;Opus 4.6 often thinks more deeply and more carefully revisits its reasoning before settling on an answer. This produces better results on harder problems, but can add cost and latency on simpler ones. If you’re finding that the model is overthinking on a given task, we recommend dialing effort down from its default setting (high) to medium. You can control this easily with the &lt;code&gt;/effort&lt;/code&gt; parameter.&lt;/p&gt;&lt;p&gt;Here are some of the things our Early Access partners told us about Claude Opus 4.6, including its propensity to work autonomously without hand-holding, its success where previous models failed, and its effect on how teams work:&lt;/p&gt;&lt;quote&gt;Claude Opus 4.6 is the strongest model Anthropic has shipped. It takes complicated requests and actually follows through, breaking them into concrete steps, executing, and producing polished work even when the task is ambitious. For Notion users, it feels less like a tool and more like a capable collaborator.&lt;/quote&gt;&lt;quote&gt;Early testing shows Claude Opus 4.6 delivering on the complex, multi-step coding work developers face every day—especially agentic workflows that demand planning and tool calling. This starts unlocking long-horizon tasks at the frontier.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is a huge leap for agentic planning. It breaks complex tasks into independent subtasks, runs tools and subagents in parallel, and identifies blockers with real precision.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best model we've tested yet. Its reasoning and planning capabilities have been exceptional at powering our AI Teammates. It's also a fantastic coding model – its ability to navigate a large codebase and identify the right changes to make is state of the art.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 reasons through complex problems at a level we haven't seen before. It considers edge cases that other models miss and consistently lands on more elegant, well-considered solutions. We're particularly impressed with Opus 4.6 in Devin Review, where it's increased our bug catching rates.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 feels noticeably better than Opus 4.5 in Windsurf, especially on tasks that require careful exploration like debugging and understanding unfamiliar codebases. We’ve noticed Opus 4.6 thinks longer, which pays off when deeper reasoning is needed.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 represents a meaningful leap in long-context performance. In our testing, we saw it handle much larger bodies of information with a level of consistency that strengthens how we design and deploy complex research workflows. Progress in this area gives us more powerful building blocks to deliver truly expert-grade systems professionals can trust.&lt;/quote&gt;&lt;quote&gt;Across 40 cybersecurity investigations, Claude Opus 4.6 produced the best results 38 of 40 times in a blind ranking against Claude 4.5 models. Each model ran end to end on the same agentic harness with up to 9 subagents and 100+ tool calls.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the new frontier on long-running tasks from our internal benchmarks and testing. It's also been highly effective at reviewing code.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 achieved the highest BigLaw Bench score of any Claude model at 90.2%. With 40% perfect scores and 84% above 0.8, it’s remarkably capable for legal reasoning.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 autonomously closed 13 issues and assigned 12 issues to the right team members in a single day, managing a ~50-person organization across 6 repositories. It handled both product and organizational decisions while synthesizing context across multiple domains, and it knew when to escalate to a human.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is an uplift in design quality. It works beautifully with our design systems and it’s more autonomous, which is core to Lovable’s values. People should be creating things that matter, not micromanaging AI.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 excels in high-reasoning tasks like multi-source analysis across legal, financial, and technical content. Box’s eval showed a 10% lift in performance, reaching 68% vs. a 58% baseline, and near-perfect scores in technical domains.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 generates complex, interactive apps and prototypes in Figma Make with an impressive creative range. The model translates detailed designs and multi-layered tasks into code on the first try, making it a powerful starting point for teams to explore and build ideas.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the best Anthropic model we’ve tested. It understands intent with minimal prompting and went above and beyond, exploring and creating details I didn’t even know I wanted until I saw them. It felt like I was working with the model, not waiting on it.&lt;/quote&gt;&lt;quote&gt;Both hands-on testing and evals show Claude Opus 4.6 is a meaningful improvement for design systems and large codebases, use cases that drive enormous enterprise value. It also one-shotted a fully functional physics engine, handling a large multi-scope task in a single pass.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 is the biggest leap I’ve seen in months. I’m more comfortable giving it a sequence of tasks across the stack and letting it run. It’s smart enough to use subagents for the individual pieces.&lt;/quote&gt;&lt;quote&gt;Claude Opus 4.6 handled a multi-million-line codebase migration like a senior engineer. It planned up front, adapted its strategy as it learned, and finished in half the time.&lt;/quote&gt;&lt;quote&gt;We only ship models in v0 when developers will genuinely feel the difference. Claude Opus 4.6 passed that bar with ease. Its frontier-level reasoning, especially with edge cases, helps v0 to deliver on our number-one aim: to let anyone elevate their ideas from prototype to production.&lt;/quote&gt;&lt;quote&gt;The performance jump with Claude Opus 4.6 feels almost unbelievable. Real-world tasks that were challenging for Opus [4.5] suddenly became easy. This feels like a watershed moment for spreadsheet agents on Shortcut.&lt;/quote&gt;&lt;head rend="h2"&gt;Evaluating Claude Opus 4.6&lt;/head&gt;&lt;p&gt;Across agentic coding, computer use, tool use, search, and finance, Opus 4.6 is an industry-leading model, often by a wide margin. The table below shows how Claude Opus 4.6 compares to our previous models and to other industry models on a variety of benchmarks.&lt;/p&gt;&lt;p&gt;Opus 4.6 is much better at retrieving relevant information from large sets of documents. This extends to long-context tasks, where it holds and tracks information over hundreds of thousands of tokens with less drift, and picks up buried details that even Opus 4.5 would miss.&lt;/p&gt;&lt;p&gt;A common complaint about AI models is “context rot,” where performance degrades as conversations exceed a certain number of tokens. Opus 4.6 performs markedly better than its predecessors: on the 8-needle 1M variant of MRCR v2—a needle-in-a-haystack benchmark that tests a model’s ability to retrieve information “hidden” in vast amounts of text—Opus 4.6 scores 76%, whereas Sonnet 4.5 scores just 18.5%. This is a qualitative shift in how much context a model can actually use while maintaining peak performance.&lt;/p&gt;&lt;p&gt;All in all, Opus 4.6 is better at finding information across long contexts, better at reasoning after absorbing that information, and has substantially better expert-level reasoning abilities in general.&lt;/p&gt;&lt;p&gt;Finally, the charts below show how Claude Opus 4.6 performs on a variety of benchmarks that assess its software engineering skills, multilingual coding ability, long-term coherence, cybersecurity capabilities, and its life sciences knowledge.&lt;/p&gt;&lt;head rend="h2"&gt;A step forward on safety&lt;/head&gt;&lt;p&gt;These intelligence gains do not come at the cost of safety. On our automated behavioral audit, Opus 4.6 showed a low rate of misaligned behaviors such as deception, sycophancy, encouragement of user delusions, and cooperation with misuse. Overall, it is just as well-aligned as its predecessor, Claude Opus 4.5, which was our most-aligned frontier model to date. Opus 4.6 also shows the lowest rate of over-refusals—where the model fails to answer benign queries—of any recent Claude model.&lt;/p&gt;&lt;p&gt;For Claude Opus 4.6, we ran the most comprehensive set of safety evaluations of any model, applying many different tests for the first time and upgrading several that we’ve used before. We included new evaluations for user wellbeing, more complex tests of the model’s ability to refuse potentially dangerous requests, and updated evaluations of the model’s ability to surreptitiously perform harmful actions. We also experimented with new methods from interpretability, the science of the inner workings of AI models, to begin to understand why the model behaves in certain ways—and, ultimately, to catch problems that standard testing might miss.&lt;/p&gt;&lt;p&gt;A detailed description of all capability and safety evaluations is available in the Claude Opus 4.6 system card.&lt;/p&gt;&lt;p&gt;We’ve also applied new safeguards in areas where Opus 4.6 shows particular strengths that might be put to dangerous as well as beneficial uses. In particular, since the model shows enhanced cybersecurity abilities, we’ve developed six new cybersecurity probes—methods of detecting harmful responses—to help us track different forms of potential misuse.&lt;/p&gt;&lt;p&gt;We’re also accelerating the cyberdefensive uses of the model, using it to help find and patch vulnerabilities in open-source software (as we describe in our new cybersecurity blog post). We think it’s critical that cyberdefenders use AI models like Claude to help level the playing field. Cybersecurity moves fast, and we’ll be adjusting and updating our safeguards as we learn more about potential threats; in the near future, we may institute real-time intervention to block abuse.&lt;/p&gt;&lt;head rend="h2"&gt;Product and API updates&lt;/head&gt;&lt;p&gt;We’ve made substantial updates across Claude, Claude Code, and the Claude Developer Platform to let Opus 4.6 perform at its best.&lt;/p&gt;&lt;p&gt;Claude Developer Platform&lt;/p&gt;&lt;p&gt;On the API, we’re giving developers better control over model effort and more flexibility for long-running agents. To do so, we’re introducing the following features:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Adaptive thinking. Previously, developers only had a binary choice between enabling or disabling extended thinking. Now, with adaptive thinking, Claude can decide when deeper reasoning would be helpful. At the default effort level (high), the model uses extended thinking when useful, but developers can adjust the effort level to make it more or less selective.&lt;/item&gt;&lt;item&gt;Effort. There are now four effort levels to choose from: low, medium, high (default), and max. We encourage developers to experiment with different options to find what works best.&lt;/item&gt;&lt;item&gt;Context compaction (beta). Long-running conversations and agentic tasks often hit the context window. Context compaction automatically summarizes and replaces older context when the conversation approaches a configurable threshold, letting Claude perform longer tasks without hitting limits.&lt;/item&gt;&lt;item&gt;1M token context (beta). Opus 4.6 is our first Opus-class model with 1M token context. Premium pricing applies for prompts exceeding 200k tokens ($10/$37.50 per million input/output tokens).&lt;/item&gt;&lt;item&gt;128k output tokens. Opus 4.6 supports outputs of up to 128k tokens, which lets Claude complete larger-output tasks without breaking them into multiple requests.&lt;/item&gt;&lt;item&gt;US-only inference. For workloads that need to run in the United States, US-only inference is available at 1.1× token pricing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Product updates&lt;/p&gt;&lt;p&gt;Across Claude and Claude Code, we’ve added features that allow knowledge workers and developers to tackle harder tasks with more of the tools they use every day.&lt;/p&gt;&lt;p&gt;We’ve introduced agent teams in Claude Code as a research preview. You can now spin up multiple agents that work in parallel as a team and coordinate autonomously—best for tasks that split into independent, read-heavy work like codebase reviews. You can take over any subagent directly using Shift+Up/Down or tmux.&lt;/p&gt;&lt;p&gt;Claude now also works better with the office tools you already use. Claude in Excel handles long-running and harder tasks with improved performance, and can plan before acting, ingest unstructured data and infer the right structure without guidance, and handle multi-step changes in one pass. Pair that with Claude in PowerPoint, and you can first process and structure your data in Excel, then bring it to life visually in PowerPoint. Claude reads your layouts, fonts, and slide masters to stay on brand, whether you’re building from a template or generating a full deck from a description. Claude in PowerPoint is now available in research preview for Max, Team, and Enterprise plans.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;p&gt;[1] Run independently by Artificial Analysis. See here for full methodological details.&lt;/p&gt;&lt;p&gt;[2] This translates into Claude Opus 4.6 obtaining a higher score than GPT-5.2 on this eval approximately 70% of the time (where 50% of the time would have implied parity in the scores).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;For GPT-5.2 and Gemini 3 Pro models, we compared the best reported model version in the charts and table.&lt;/item&gt;&lt;item&gt;Terminal-Bench 2.0: We report both scores reproduced on our infrastructure and published scores from other labs. All runs used the Terminus-2 harness, except for OpenAI’s Codex CLI. All experiments used 1× guaranteed / 3× ceiling resource allocation and 5–15 samples per task across staggered batches. See system card for details.&lt;/item&gt;&lt;item&gt;Humanity’s Last Exam: Claude models run “with tools” were run with web search, web fetch, code execution, programmatic tool calling, context compaction triggered at 50k tokens up to 3M total tokens, max reasoning effort, and adaptive thinking enabled. A domain blocklist was used to decontaminate eval results. See system card for more details.&lt;/item&gt;&lt;item&gt;SWE-bench Verified: Our score was averaged over 25 trials. With a prompt modification, we saw a score of 81.42%.&lt;/item&gt;&lt;item&gt;MCP Atlas: Claude Opus 4.6 was run with max effort. When run at high effort, it reached an industry-leading score of 62.7%.&lt;/item&gt;&lt;item&gt;BrowseComp: Claude models were run with web search, web fetch, programmatic tool calling, context compaction triggered at 50k tokens up to 10M total tokens, max reasoning effort, and no thinking enabled. Adding a multi-agent harness increased scores to 86.8%. See system card for more details.&lt;/item&gt;&lt;item&gt;ARC AGI 2: Claude Opus 4.6 was run with max effort and a 120k thinking budget score.&lt;/item&gt;&lt;item&gt;CyberGym: Claude models were run on no thinking, default effort, temperature, and &lt;code&gt;top_p&lt;/code&gt;. The model was also given a “think” tool that allowed interleaved thinking for multi-turn evaluations.&lt;/item&gt;&lt;item&gt;OpenRCA: For each failure case in OpenRCA, Claude receives 1 point if all generated root-cause elements match the ground-truth ones, and 0 points if any mismatch is identified. The overall accuracy is the average score across all failure cases. The benchmark was run on the benchmark author’s harness, graded using their official methodology, and has been submitted for official verification.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Related content&lt;/head&gt;&lt;head rend="h3"&gt;Claude is a space to think&lt;/head&gt;&lt;p&gt;We’ve made a choice: Claude will remain ad-free. We explain why advertising incentives are incompatible with a genuinely helpful AI assistant, and how we plan to expand access without compromising user trust.&lt;/p&gt;Read more&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/news/claude-opus-4-6"/><published>2026-02-05T17:38:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46902638</id><title>GPT-5.3-Codex</title><updated>2026-02-06T13:53:33.634214+00:00</updated><content>&lt;doc fingerprint="bd5265dde40bc41c"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re introducing a new model that unlocks even more of what Codex can do: GPT‑5.3-Codex, the most capable agentic coding model to date. The model advances both the frontier coding performance of GPT‑5.2-Codex and the reasoning and professional knowledge capabilities of GPT‑5.2, together in one model, which is also 25% faster. This enables it to take on long-running tasks that involve research, tool use, and complex execution. Much like a colleague, you can steer and interact with GPT‑5.3-Codex while it’s working, without losing context.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3‑Codex is our first model that was instrumental in creating itself. The Codex team used early versions to debug its own training, manage its own deployment, and diagnose test results and evaluations—our team was blown away by how much Codex was able to accelerate its own development.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex goes from an agent that can write and review code to an agent that can do nearly anything developers and professionals can do on a computer.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex sets a new industry high on SWE-Bench Pro and Terminal-Bench, and shows strong performance on OSWorld and GDPval, four benchmarks we use to measure coding, agentic and real-world capabilities.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex achieves state-of-the-art performance on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Where SWE‑bench Verified only tests Python, SWE‑Bench Pro spans four languages and is more contamination‑resistant, challenging, diverse and industry-relevant. It also far exceeds the previous state-of-the-art performance on Terminal-Bench 2.0, which measures the terminal skills a coding agent like Codex needs. Notably, GPT‑5.3‑Codex does so with fewer tokens than any prior model, letting users build more.&lt;/p&gt;
    &lt;p&gt;Combining frontier coding capabilities, improvements in aesthetics, and compaction results in a model that can do striking work, building highly functional complex games and apps from scratch over the course of days. To test the model’s web development and long-running agentic capabilities, we asked GPT‑5.3‑Codex to build us two games: version two of the racing game from the Codex app launch, and a diving game. Using the develop web game skill and preselected, generic follow-up prompts like "fix the bug" or "improve the game", GPT‑5.3-Codex iterated on the games autonomously over millions of tokens. Watch the trailers and play the games for yourself to see what Codex can do.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex also better understands your intent when you ask it to make day-to-day websites, compared to GPT‑5.2-Codex. Simple or underspecified prompts now default to sites with more functionality and sensible defaults, giving you a stronger starting canvas to bring your ideas to life.&lt;/p&gt;
    &lt;p&gt;For example, we asked GPT‑5.3-Codex and GPT‑5.2-Codex to build two landing pages below. GPT‑5.3-Codex automatically showed the yearly plan as a discounted monthly price, making the discount feel clear and intentional, instead of multiplying the yearly total. It also made an automatically transitioning testimonial carousel with three distinct user quotes rather than one, resulting in a page that feels more complete and production-ready by default.&lt;/p&gt;
    &lt;p&gt;Software engineers, designers, product managers, and data scientists do far more than generate code. GPT‑5.3‑Codex is built to support all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more. Its agentic capabilities go beyond software, helping you build whatever you want to build—whether it’s slide decks or analyzing data in sheets.&lt;/p&gt;
    &lt;p&gt;With custom skills similar to those used for our previous GDPval results, GPT‑5.3‑Codex also shows strong performance on professional knowledge work as measured by GDPval, matching GPT‑5.2. GDPval is an evaluation OpenAI released in 2025 that measures a model’s performance on well‑specified knowledge‑work tasks across 44 occupations. These tasks include things like making presentations, spreadsheets, and other work products.&lt;/p&gt;
    &lt;p&gt;Below are a few examples of the work the agent produced.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prompt + task context&lt;/head&gt;
    &lt;head rend="h3"&gt;GPT-5.3-Codex output&lt;/head&gt;
    &lt;p&gt;OSWorld is an agentic computer-use benchmark where the agent has to complete productivity tasks in a visual desktop computer environment. GPT‑5.3-Codex demonstrates far stronger computer use capabilities than previous GPT models.&lt;/p&gt;
    &lt;p&gt;Together, these results across coding, frontend, and computer-use and real-world tasks show that GPT‑5.3-Codex isn’t just better at individual tasks, but marks a step change toward a single, general-purpose agent that can reason, build, and execute across the full spectrum of real-world technical work.&lt;/p&gt;
    &lt;p&gt;As model capabilities become more powerful, the gap shifts from what agents are capable of doing to how easily humans can interact with, direct and supervise many of them working in parallel. The Codex app makes managing and directing agents much easier, and now with GPT‑5.3-Codex it’s more interactive. With the new model, Codex provides frequent updates so you stay appraised of key decisions and progress as it works. Instead of waiting for a final output, you can interact in real time—ask questions, discuss approaches, and steer toward the solution. GPT‑5.3-Codex talks through what it’s doing, responds to feedback, and keeps you in the loop from start to finish.&lt;/p&gt;
    &lt;p&gt;The recent rapid Codex improvements build on the fruit of research projects spanning months or years across all of OpenAI. These research projects are being accelerated by Codex, with many researchers and engineers at OpenAI describing their job today as being fundamentally different from what it was just two months ago. Even early versions of GPT‑5.3-Codex demonstrated exceptional capabilities, allowing our team to work with those earlier versions to improve training and support the deployment of later versions.&lt;/p&gt;
    &lt;p&gt;Codex is useful for a very broad range of tasks, making it difficult to fully enumerate the ways in which it helps our teams. As some examples, the research team used Codex to monitor and debug the training run for this release. It accelerated research beyond debugging infrastructure problems: it helped track patterns throughout the course of training, provided a deep analysis on interaction quality, proposed fixes and built rich applications for human researchers to precisely understand how the model’s behavior differed compared to prior models.&lt;/p&gt;
    &lt;p&gt;The engineering team used Codex to optimize and adapt the harness for GPT‑5.3-Codex. When we started seeing strange edge cases impacting users, team members used Codex to identify context rendering bugs, and root cause low cache hit rates. GPT‑5.3-Codex is continuing to help the team throughout the launch by dynamically scaling GPU clusters to adjust to traffic surges and keeping latency stable.&lt;/p&gt;
    &lt;p&gt;During alpha testing, one researcher wanted to understand how much additional work GPT‑5.3-Codex was getting done per turn and the associated difference in productivity. GPT‑5.3-Codex came up with several simple regex classifiers to estimate frequency of clarifications, positive and negative user responses, progress on the task, and then ran them scalably over all session logs and produced a report with its conclusion. People building with Codex were happier as the agent was better understanding their intent and made more progress per turn, with fewer clarifying questions.&lt;/p&gt;
    &lt;p&gt;Due to GPT‑5.3-Codex being so different from its predecessors, the data from alpha testing exhibited numerous unusual and counter-intuitive results. A data scientist on the team worked with GPT‑5.3-Codex to build new data pipelines and visualize the results much more richly than our standard dashboarding tools enabled. The results were co-analyzed with Codex, which concisely summarized key insights over thousands of data points in under three minutes.&lt;/p&gt;
    &lt;p&gt;Individually, all of these tasks are interesting examples of how Codex can help researchers and product builders. Taken together, we found that these new capabilities resulted in powerful acceleration of our research, engineering, and product teams.&lt;/p&gt;
    &lt;p&gt;Over recent months, we’ve seen meaningful gains in model performance on cybersecurity tasks, benefiting both developers and security professionals. In parallel, we’ve been preparing strengthened cyber safeguards to support defensive use and broader ecosystem resilience.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is the first model we classify as High capability for cybersecurity-related tasks under our Preparedness Framework, and the first we’ve directly trained to identify software vulnerabilities. While we don’t have definitive evidence it can automate cyber attacks end-to-end, we’re taking a precautionary approach and deploying our most comprehensive cybersecurity safety stack to date. Our mitigations include safety training, automated monitoring, trusted access for advanced capabilities, and enforcement pipelines including threat intelligence.&lt;/p&gt;
    &lt;p&gt;Because cybersecurity is inherently dual-use, we’re taking an evidence-based, iterative approach that accelerates defenders’ ability to find and fix vulnerabilities while slowing misuse. As part of this, we’re launching Trusted Access for Cyber, a pilot program to accelerate cyber defense research.&lt;/p&gt;
    &lt;p&gt;We’re investing in ecosystem safeguards such as expanding the private beta of Aardvark, our security research agent, as the first offering in our suite of Codex Security products and tools, and partnering with open-source maintainers to provide free codebase scanning for widely used projects such as Next.js—where a security researcher used Codex to find vulnerabilities disclosed(opens in a new window) last week.&lt;/p&gt;
    &lt;p&gt;Building on our $1M Cybersecurity Grant Program launched in 2023, we’re also committing $10M in API credits to accelerate cyber defense with our most capable models, especially for open source software and critical infrastructure systems. Organizations engaged in good-faith security research can apply for API credits and support through our Cybersecurity Grant Program.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex is available with paid ChatGPT plans, everywhere you can use Codex: the app, CLI, IDE extension and web. We are working to safely enable API access soon.&lt;/p&gt;
    &lt;p&gt;With this update, we are also now running GPT‑5.3-Codex 25% faster for Codex users, thanks to improvements in our infrastructure and inference stack, resulting in faster interactions and faster results.&lt;/p&gt;
    &lt;p&gt;GPT‑5.3-Codex was co-designed for, trained with, and served on NVIDIA GB200 NVL72 systems. We are grateful to NVIDIA for their partnership.&lt;/p&gt;
    &lt;p&gt;With GPT‑5.3-Codex, Codex is moving beyond writing code to using it as a tool to operate a computer and complete work end to end. By pushing the frontier of what a coding agent can do, we’re also unlocking a broader class of knowledge work—from building and deploying software to researching, analyzing, and executing complex tasks. What started as a focus on being the best coding agent has become the foundation for a more general collaborator on the computer, expanding both who can build and what’s possible with Codex.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.3-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2-Codex (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT-5.2 (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (Public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.8%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;56.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Terminal-Bench 2.0&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;62.2%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;OSWorld-Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;64.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;37.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9% (high)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Cybersecurity Capture The Flag Challenges&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;67.7%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Lancer IC Diamond&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;81.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;74.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://openai.com/index/introducing-gpt-5-3-codex/"/><published>2026-02-05T18:08:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903558</id><title>My AI Adoption Journey</title><updated>2026-02-06T13:53:33.524997+00:00</updated><content>&lt;doc fingerprint="1552cc5748018c3f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;My AI Adoption Journey&lt;/head&gt;
    &lt;head class="pt-2 pb-2 text-md font-bold"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;My experience adopting any meaningful tool is that I've necessarily gone through three phases: (1) a period of inefficiency (2) a period of adequacy, then finally (3) a period of workflow and life-altering discovery.&lt;/p&gt;
    &lt;p&gt;In most cases, I have to force myself through phase 1 and 2 because I usually have a workflow I'm already happy and comfortable with. Adopting a tool feels like work, and I do not want to put in the effort, but I usually do in an effort to be a well-rounded person of my craft.&lt;/p&gt;
    &lt;p&gt;This is my journey of how I found value in AI tooling and what I'm trying next with it. In an ocean of overly dramatic, hyped takes, I hope this represents a more nuanced, measured approach to my views on AI and how they've changed over time.&lt;/p&gt;
    &lt;p&gt;This blog post was fully written by hand, in my own words. I hate that I have to say that but especially given the subject matter, I want to be explicit about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Drop the Chatbot&lt;/head&gt;
    &lt;p&gt;Immediately cease trying to perform meaningful work via a chatbot (e.g. ChatGPT, Gemini on the web, etc.). Chatbots have real value and are a daily part of my AI workflow, but their utility in coding is highly limited because you're mostly hoping they come up with the right results based on their prior training, and correcting them involves a human (you) to tell them they're wrong repeatedly. It is inefficient.&lt;/p&gt;
    &lt;p&gt;I think everyone's first experience with AI is a chat interface. And I think everyone's first experience trying to code with AI has been asking a chat interface to write code.&lt;/p&gt;
    &lt;p&gt;While I was still a heavy AI skeptic, my first "oh wow" moment was pasting a screenshot of Zed's command palette into Gemini, asking it to reproduce it with SwiftUI, and being truly flabbergasted that it did it very well. The command palette that ships for macOS in Ghostty today is only very lightly modified from what Gemini produced for me in seconds.&lt;/p&gt;
    &lt;p&gt;But when I tried to reproduce that behavior for other tasks, I was left disappointed. In the context of brownfield projects, I found the chat interface produced poor results very often, and I found myself very frustrated copying and pasting code and command output to and from the interface. It was very obviously far less efficient than me doing the work myself.&lt;/p&gt;
    &lt;p&gt;To find value, you must use an agent. An agent is the industry-adopted term for an LLM that can chat and invoke external behavior in a loop1 At a bare minimum, the agent must have the ability to: read files, execute programs, and make HTTP requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Reproduce Your Own Work&lt;/head&gt;
    &lt;p&gt;The next phase on my journey I tried Claude Code. I'll cut to the chase: I initially wasn't impressed. I just wasn't getting good results out of my sessions. I felt I had to touch up everything it produced and this process was taking more time than if I had just done it myself. I read blog posts, watched videos, but just wasn't that impressed.&lt;/p&gt;
    &lt;p&gt;Instead of giving up, I forced myself to reproduce all my manual commits with agentic ones. I literally did the work twice. I'd do the work manually, and then I'd fight an agent to produce identical results in terms of quality and function (without it being able to see my manual solution, of course).&lt;/p&gt;
    &lt;p&gt;This was excruciating, because it got in the way of simply getting things done. But I've been around the block with non-AI tools enough to know that friction is natural, and I can't come to a firm, defensible conclusion without exhausting my efforts.&lt;/p&gt;
    &lt;p&gt;But, expertise formed. I quickly discovered for myself from first principles what others were already saying, but discovering it myself resulted in a stronger fundamental understanding.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Break down sessions into separate clear, actionable tasks. Don't try to "draw the owl" in one mega session.&lt;/item&gt;
      &lt;item&gt;For vague requests, split the work into separate planning vs. execution sessions.&lt;/item&gt;
      &lt;item&gt;If you give an agent a way to verify its work, it more often than not fixes its own mistakes and prevents regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, I also found the edges of what agents -- at the time -- were good at, what they weren't good at, and for the tasks they were good at how to achieve the results I wanted.&lt;/p&gt;
    &lt;p&gt;All of this led to significant efficiency gains, to the point where I was starting to naturally use agents in a way that I felt was no slower than doing it myself (but I still didn't feel it was any faster, since I was mostly babysitting an agent).&lt;/p&gt;
    &lt;p&gt;The negative space here is worth reiterating: part of the efficiency gains here were understanding when not to reach for an agent. Using an agent for something it'll likely fail at is obviously a big waste of time and having the knowledge to avoid that completely leads to time savings2.&lt;/p&gt;
    &lt;p&gt;At this stage, I was finding adequate value with agents that I was happy to use them in my workflow, but still didn't feel like I was seeing any net efficiency gains. I didn't care though, I was content at this point with AI as a tool.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: End-of-Day Agents&lt;/head&gt;
    &lt;p&gt;To try to find some efficiency, I next started up a new pattern: block out the last 30 minutes of every day to kick off one or more agents. My hypothesis was that perhaps I could gain some efficiency if the agent can make some positive progress in the times I can't work anyways. Basically: instead of trying to do more in the time I have, try to do more in the time I don't have.&lt;/p&gt;
    &lt;p&gt;Similar to the previous task, I at first found this both unsuccessful and annoying. But, I once again quickly found different categories of work that were really helpful:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep research sessions where I'd ask agents to survey some field, such as finding all libraries in a specific language with a specific license type and producing multi-page summaries for each on their pros, cons, development activity, social sentiment, etc.&lt;/item&gt;
      &lt;item&gt;Parallel agents attempting different vague ideas I had but didn't have time to get started on. I didn't expect them to produce something I'd ever ship here, but perhaps could illuminate some unknown unknowns when I got to the task the next day.&lt;/item&gt;
      &lt;item&gt;Issue and PR triage/review. Agents are good at using &lt;code&gt;gh&lt;/code&gt;(GitHub CLI), so I manually scripted a quick way to spin up a bunch in parallel to triage issues. I would NOT allow agents to respond, I just wanted reports the next day to try to guide me towards high value or low effort tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be clear, I did not go as far as others went to have agents running in loops all night. In most cases, agents completed their tasks in less than half an hour. But, the latter part of the working day, I'm usually tired and coming out of flow and find myself too personally inefficient, so shifting my effort to spinning up these agents I found gave me a "warm start" the next morning that got me working more quickly than I would've otherwise.&lt;/p&gt;
    &lt;p&gt;I was happy, and I was starting to feel like I was doing more than I was doing prior to AI, if only slightly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 4: Outsource the Slam Dunks&lt;/head&gt;
    &lt;p&gt;By this point, I was getting very confident about what tasks my AI was and wasn't great at. I had really high confidence with certain tasks that the AI would achieve a mostly-correct solution. So the next step on my journey was: let agents do all of that work while I worked on other tasks.&lt;/p&gt;
    &lt;p&gt;More specifically, I would start each day by taking the results of my prior night's triage agents, filter them manually to find the issues that an agent will almost certainly solve well, and then keep them going in the background (one at a time, not in parallel).&lt;/p&gt;
    &lt;p&gt;Meanwhile, I'd work on something else. I wasn't going to social media (any more than usual without AI), I wasn't watching videos, etc. I was in my own, normal, pre-AI deep thinking mode working on something I wanted to work on or had to work on.&lt;/p&gt;
    &lt;p&gt;Very important at this stage: turn off agent desktop notifications. Context switching is very expensive. In order to remain efficient, I found that it was my job as a human to be in control of when I interrupt the agent, not the other way around. Don't let the agent notify you. During natural breaks in your work, tab over and check on it, then carry on.&lt;/p&gt;
    &lt;p&gt;Importantly, I think the "work on something else" helps counteract the highly publicized Anthropic skill formation paper. Well, you're trading off: not forming skills for the tasks you're delegating to the agent while continuing to form skills naturally in the tasks you continue to work on manually.&lt;/p&gt;
    &lt;p&gt;At this point I was firmly in the "no way I can go back" territory. I felt more efficient, but even if I wasn't, the thing I liked the most was that I could now focus my coding and thinking on tasks I really loved while still adequately completing the tasks I didn't.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 5: Engineer the Harness&lt;/head&gt;
    &lt;p&gt;At risk of stating the obvious: agents are much more efficient when they produce the right result the first time, or at worst produce a result that requires minimal touch-ups. The most sure-fire way to achieve this is to give the agent fast, high quality tools to automatically tell it when it is wrong.&lt;/p&gt;
    &lt;p&gt;I don't know if there is a broad industry-accepted term for this yet, but I've grown to calling this "harness engineering." It is the idea that anytime you find an agent makes a mistake, you take the time to engineer a solution such that the agent never makes that mistake again. I don't need to invent any new terms here; if another one exists, I'll jump on the bandwagon.&lt;/p&gt;
    &lt;p&gt;This comes in two forms:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Better implicit prompting (AGENTS.md). For simple things, like the agent repeatedly running the wrong commands or finding the wrong APIs, update the&lt;/p&gt;&lt;code&gt;AGENTS.md&lt;/code&gt;(or equivalent). Here is an example from Ghostty. Each line in that file is based on a bad agent behavior, and it almost completely resolved them all.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actual, programmed tools. For example, scripts to take screenshots, run filtered tests, etc etc. This is usually paired with an AGENTS.md change to let it know about this existing.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is where I'm at today. I'm making an earnest effort whenever I see an agent do a Bad Thing to prevent it from ever doing that bad thing again. Or, conversely, I'm making an earnest effort for agents to be able to verify they're doing a Good Thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 6: Always Have an Agent Running&lt;/head&gt;
    &lt;p&gt;Simultaneous to step 5, I'm also operating under the goal of having an agent running at all times. If an agent isn't running, I ask myself "is there something an agent could be doing for me right now?"&lt;/p&gt;
    &lt;p&gt;I particularly like to combine this with slower, more thoughtful models like Amp's deep mode (which is basically just GPT-5.2-Codex) which can take upwards of 30+ minutes to make small changes. The flip side of that is that it does tend to produce very good results.&lt;/p&gt;
    &lt;p&gt;I'm not [yet?] running multiple agents, and currently don't really want to. I find having the one agent running is a good balance for me right now between being able to do deep, manual work I find enjoyable, and babysitting my kind of stupid and yet mysteriously productive robot friend.&lt;/p&gt;
    &lt;p&gt;The "have an agent running at all times" goal is still just a goal. I'd say right now I'm maybe effective at having a background agent running 10 to 20% of a normal working day. But, I'm actively working to improve that.&lt;/p&gt;
    &lt;p&gt;I don't want to run agents for the sake of running agents. I only want to run them when there is a task I think would be truly helpful to me. Part of the challenge of this goal is improving my own workflows and tools so that I can have a constant stream of high quality work to do that I can delegate. Which, even without AI, is important!&lt;/p&gt;
    &lt;head rend="h2"&gt;Today&lt;/head&gt;
    &lt;p&gt;And that's where I'm at today.&lt;/p&gt;
    &lt;p&gt;Through this journey, I've personally reached a point where I'm having success with modern AI tooling and I believe I'm approaching it with the proper measured view that is grounded in reality. I really don't care one way or the other if AI is here to stay3, I'm a software craftsman that just wants to build stuff for the love of the game.&lt;/p&gt;
    &lt;p&gt;The whole landscape is moving so rapidly that I'm sure I'll look back at this post very quickly and laugh at my naivete. But, as they say, if you can't be embarassed about your past self, you're probably not growing. I just hope I'll grow in the right direction!&lt;/p&gt;
    &lt;p&gt;I have no skin in the game here4, and there are of course other reasons behind utility to avoid using AI. I fully respect anyone's individual decisions regarding it. I'm not here to convince you! For those interested, I just wanted to share my personal approach to navigating these new tools and give a glimpse about how I approach new tools in general, regardless of AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Modern coding models like Opus and Codex are specifically trained to bias towards using tools compared to conversational models. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Due to the rapid pace of innovation in models, I have to constantly revisit my priors on this one. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The skill formation issues particularly in juniors without a strong grasp of fundamentals deeply worries me, however. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don't work for, invest in, or advise any AI companies. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mitchellh.com/writing/my-ai-adoption-journey"/><published>2026-02-05T19:04:40+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903616</id><title>We tasked Opus 4.6 using agent teams to build a C Compiler</title><updated>2026-02-06T13:53:29.698072+00:00</updated><content>&lt;doc fingerprint="5760beda6f939a6e"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by Nicholas Carlini, a researcher on our Safeguards team. &lt;/p&gt;
    &lt;p&gt;I've been experimenting with a new approach to supervising language models that we’re calling "agent teams."&lt;/p&gt;
    &lt;p&gt;With agent teams, multiple Claude instances work in parallel on a shared codebase without active human intervention. This approach dramatically expands the scope of what's achievable with LLM agents.&lt;/p&gt;
    &lt;p&gt;To stress test it, I tasked 16 agents with writing a Rust-based C compiler, from scratch, capable of compiling the Linux kernel. Over nearly 2,000 Claude Code sessions and $20,000 in API costs, the agent team produced a 100,000-line compiler that can build Linux 6.9 on x86, ARM, and RISC-V.&lt;/p&gt;
    &lt;p&gt;The compiler is an interesting artifact on its own, but I focus here on what I learned about designing harnesses for long-running autonomous agent teams: how to write tests that keep agents on track without human oversight, how to structure work so multiple agents can make progress in parallel, and where this approach hits its ceiling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enabling long-running Claudes&lt;/head&gt;
    &lt;p&gt;Existing agent scaffolds like Claude Code require an operator to be online and available to work jointly. If you ask for a solution to a long and complex problem, the model may solve part of it, but eventually it will stop and wait for continued input—a question, a status update, or a request for clarification.&lt;/p&gt;
    &lt;p&gt;To elicit sustained, autonomous progress, I built a harness that sticks Claude in a simple loop (if you’ve seen Ralph-loop, this should look familiar). When it finishes one task, it immediately picks up the next. (Run this in a container, not your actual machine).&lt;/p&gt;
    &lt;code&gt;#!/bin/bash

while true; do
    COMMIT=$(git rev-parse --short=6 HEAD)
    LOGFILE="agent_logs/agent_${COMMIT}.log"

    claude --dangerously-skip-permissions \
           -p "$(cat AGENT_PROMPT.md)" \
           --model claude-opus-X-Y &amp;amp;&amp;gt; "$LOGFILE"
done
&lt;/code&gt;
    &lt;p&gt;&lt;lb/&gt;In the agent prompt, I tell Claude what problem to solve and ask it to approach the problem by breaking it into small pieces, tracking what it’s working on, figuring out what to work on next, and to effectively keep going until it’s perfect. (On this last point, Claude has no choice. The loop runs forever—although in one instance, I did see Claude &lt;code&gt;pkill -9 bash&lt;/code&gt; on accident, thus killing itself and ending the loop. Whoops!).&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Running Claude in parallel&lt;/head&gt;
    &lt;p&gt;Running multiple instances in parallel can address two weaknesses of a single-agent harness:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One Claude Code session can only do one thing at a time. Especially as the scope of a project expands, debugging multiple issues in parallel is far more efficient.&lt;/item&gt;
      &lt;item&gt;Running multiple Claude agents allows for specialization. While a few agents are tasked to solve the actual problem at hand, other specialized agents can be invoked to (for example) maintain documentation, keep an eye on code quality, or solve specialized sub-tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My implementation of parallel Claude is bare-bones. A new bare git repo is created, and for each agent, a Docker container is spun up with the repo mounted to &lt;code&gt;/upstream&lt;/code&gt;. Each agent clones a local copy to &lt;code&gt;/workspace&lt;/code&gt;, and when it's done, pushes from its own local container to upstream.&lt;/p&gt;
    &lt;p&gt;To prevent two agents from trying to solve the same problem at the same time, the harness uses a simple synchronization algorithm:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Claude takes a "lock" on a task by writing a text file to current_tasks/ (e.g., one agent might lock current_tasks/parse_if_statement.txt, while another locks current_tasks/codegen_function_definition.txt). If two agents try to claim the same task, git's synchronization forces the second agent to pick a different one.&lt;/item&gt;
      &lt;item&gt;Claude works on the task, then pulls from upstream, merges changes from other agents, pushes its changes, and removes the lock. Merge conflicts are frequent, but Claude is smart enough to figure that out.&lt;/item&gt;
      &lt;item&gt;The infinite agent-generation-loop spawns a new Claude Code session in a fresh container, and the cycle repeats.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a very early research prototype. I haven’t yet implemented any other method for communication between agents, nor do I enforce any process for managing high-level goals. I don’t use an orchestration agent.&lt;/p&gt;
    &lt;p&gt;Instead, I leave it up to each Claude agent to decide how to act. In most cases, Claude picks up the “next most obvious” problem. When stuck on a bug, Claude will often maintain a running doc of failed approaches and remaining tasks. In the git repository of the project, you can read through the history and watch it take out locks on various tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons from programming with Claude agent teams&lt;/head&gt;
    &lt;p&gt;The scaffolding runs Claude in a loop, but that loop is only useful if Claude can tell how to make progress. Most of my effort went into designing the environment around Claude—the tests, the environment, the feedback—so that it could orient itself without me. These are the approaches I’ve found most helpful when orchestrating multiple Claude instances.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write extremely high-quality tests&lt;/head&gt;
    &lt;p&gt;Claude will work autonomously to solve whatever problem I give it. So it’s important that the task verifier is nearly perfect, otherwise Claude will solve the wrong problem. Improving the testing harness required finding high-quality compiler test suites, writing verifiers and build scripts for open-source software packages, and watching for mistakes Claude was making, then designing new tests as I identified those failure modes.&lt;/p&gt;
    &lt;p&gt;For example, near the end of the project, Claude started to frequently break existing functionality each time it implemented a new feature. To address this, I built a continuous integration pipeline and implemented stricter enforcement that allowed Claude to better test its work so that new commits can’t break existing code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Put yourself in Claude’s shoes&lt;/head&gt;
    &lt;p&gt;I had to constantly remind myself that I was writing this test harness for Claude and not for myself, which meant rethinking many of my assumptions about how tests should communicate results.&lt;/p&gt;
    &lt;p&gt;For example, each agent is dropped into a fresh container with no context and will spend significant time orienting itself, especially on large projects. Before we even reach the tests, to help Claude help itself, I included instructions to maintain extensive READMEs and progress files that should be updated frequently with the current status.&lt;/p&gt;
    &lt;p&gt;I also kept in mind the fact that language models have inherent limitations, which, in this case, needed to be designed around. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Context window pollution: The test harness should not print thousands of useless bytes. At most, it should print a few lines of output and log all important information to a file so Claude can find it when needed. Logfiles should be easy to process automatically: if there are errors, Claude should write ERROR and put the reason on the same line so grep will find it. It helps to pre-compute aggregate summary statistics so Claude doesn't have to recompute them.&lt;/item&gt;
      &lt;item&gt;Time blindness: Claude can't tell time and, left alone, will happily spend hours running tests instead of making progress. The harness prints incremental progress infrequently (to avoid polluting context) and includes a default &lt;code&gt;--fast&lt;/code&gt;option that runs a 1% or 10% random sample. This subsample is deterministic per-agent but random across VMs, so Claude still covers all files but each agent can perfectly identify regressions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Make parallelism easy&lt;/head&gt;
    &lt;p&gt;When there are many distinct failing tests, parallelization is trivial: each agent picks a different failing test to work on. After the test suite reached a 99% pass rate, each agent worked on getting a different small open-source project (e.g., SQlite, Redis, libjpeg, MQuickJS, Lua) to compile.&lt;/p&gt;
    &lt;p&gt;But when agents started to compile the Linux kernel, they got stuck. Unlike a test suite with hundreds of independent tests, compiling the Linux kernel is one giant task. Every agent would hit the same bug, fix that bug, and then overwrite each other's changes. Having 16 agents running didn't help because each was stuck solving the same task.&lt;/p&gt;
    &lt;p&gt;The fix was to use GCC as an online known-good compiler oracle to compare against. I wrote a new test harness that randomly compiled most of the kernel using GCC, and only the remaining files with Claude's C Compiler. If the kernel worked, then the problem wasn’t in Claude’s subset of the files. If it broke, then it could further refine by re-compiling some of these files with GCC. This let each agent work in parallel, fixing different bugs in different files, until Claude's compiler could eventually compile all files. (After this worked, it was still necessary to apply delta debugging techniques to find pairs of files that failed together but worked independently.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple agent roles&lt;/head&gt;
    &lt;p&gt;Parallelism also enables specialization. LLM-written code frequently re-implements existing functionality, so I tasked one agent with coalescing any duplicate code it found. I put another in charge of improving the performance of the compiler itself, and a third I made responsible for outputting efficient compiled code. I asked another agent to critique the design of the project from the perspective of a Rust developer, and make structural changes to the project to improve the overall code quality, and another to work on documentation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stress testing the limits of agent teams&lt;/head&gt;
    &lt;p&gt;This project was designed as a capability benchmark. I am interested in stress-testing the limits of what LLMs can just barely achieve today in order to help us prepare for what models will reliably achieve in the future.&lt;/p&gt;
    &lt;p&gt;I’ve been using the C Compiler project as a benchmark across the entire Claude 4 model series. As I did with prior projects, I started by drafting what I wanted: a from-scratch optimizing compiler with no dependencies, GCC-compatible, able to compile the Linux kernel, and designed to support multiple backends. While I specified some aspects of the design (e.g., that it should have an SSA IR to enable multiple optimization passes) I did not go into any detail on how to do so.&lt;/p&gt;
    &lt;p&gt;Previous Opus 4 models were barely capable of producing a functional compiler. Opus 4.5 was the first to cross a threshold that allowed it to produce a functional compiler which could pass large test suites, but it was still incapable of compiling any real large projects. My goal with Opus 4.6 was to again test the limits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation&lt;/head&gt;
    &lt;p&gt;Over nearly 2,000 Claude Code sessions across two weeks, Opus 4.6 consumed 2 billion input tokens and generated 140 million output tokens, a total cost just under $20,000. Compared to even the most expensive Claude Max plans, this was an extremely expensive project. But that total is a fraction of what it would cost me to produce this myself—let alone an entire team.&lt;/p&gt;
    &lt;p&gt;This was a clean-room implementation (Claude did not have internet access at any point during its development); it depends only on the Rust standard library. The 100,000-line compiler can build a bootable Linux 6.9 on x86, ARM, and RISC-V. It can also compile QEMU, FFmpeg, SQlite, postgres, redis, and has a 99% pass rate on most compiler test suites including the GCC torture test suite. It also passes the developer's ultimate litmus test: it can compile and run Doom.&lt;/p&gt;
    &lt;p&gt;The compiler, however, is not without limitations. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It lacks the 16-bit x86 compiler that is necessary to boot Linux out of real mode. For this, it calls out to GCC (the x86_32 and x86_64 compilers are its own).&lt;/item&gt;
      &lt;item&gt;It does not have its own assembler and linker; these are the very last bits that Claude started automating and are still somewhat buggy. The demo video was produced with a GCC assembler and linker.&lt;/item&gt;
      &lt;item&gt;The compiler successfully builds many projects, but not all. It's not yet a drop-in replacement for a real compiler.&lt;/item&gt;
      &lt;item&gt;The generated code is not very efficient. Even with all optimizations enabled, it outputs less efficient code than GCC with all optimizations disabled.&lt;/item&gt;
      &lt;item&gt;The Rust code quality is reasonable, but is nowhere near the quality of what an expert Rust programmer might produce.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The resulting compiler has nearly reached the limits of Opus’s abilities. I tried (hard!) to fix several of the above limitations but wasn’t fully successful. New features and bugfixes frequently broke existing functionality.&lt;/p&gt;
    &lt;p&gt;As one particularly challenging example, Opus was unable to implement a 16-bit x86 code generator needed to boot into 16-bit real mode. While the compiler can output correct 16-bit x86 via the 66/67 opcode prefixes, the resulting compiled output is over 60kb, far exceeding the 32k code limit enforced by Linux. Instead, Claude simply cheats here and calls out to GCC for this phase (This is only the case for x86. For ARM or RISC-V, Claude’s compiler can compile completely by itself.)&lt;/p&gt;
    &lt;p&gt;The source code for the compiler is available. Download it, read through the code, and try it on your favorite C projects. I’ve consistently found the best way to understand what language models can do is to push them to their limits, and then study where they start to break down. Over the coming days, I’ll continue having Claude push new changes if you want to follow along with Claude’s continued attempts at addressing these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;Each generation of language models opens up new ways of working with them. Early models were useful for tab-completion in IDEs. Before long, models could complete a function body from its docstring. The launch of Claude Code brought agents into the mainstream and enabled developers to pair-program with Claude. But each of these products operates under the assumption that a user defines a task, an LLM runs for a few seconds or minutes and returns an answer, and then the user provides a follow-up.&lt;/p&gt;
    &lt;p&gt;Agent teams show the possibility of implementing entire, complex projects autonomously. This allows us, as users of these tools, to become more ambitious with our goals.&lt;/p&gt;
    &lt;p&gt;We are still early, and fully autonomous development comes with real risks. When a human sits with Claude during development, they can ensure consistent quality and catch errors in real time. For autonomous systems, it is easy to see tests pass and assume the job is done, when this is rarely the case. I used to work in penetration testing, exploiting vulnerabilities in products produced by large companies, and the thought of programmers deploying software they’ve never personally verified is a real concern.&lt;/p&gt;
    &lt;p&gt;So, while this experiment excites me, it also leaves me feeling uneasy. Building this compiler has been some of the most fun I’ve had recently, but I did not expect this to be anywhere near possible so early in 2026. The rapid progress in both language models and the scaffolds we use to interact with them opens the door to writing an enormous amount of new code. I expect the positive applications to outweigh the negative, but we’re entering a new world which will require new strategies to navigate safely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Special thanks to Josef Bacik, Edwin Chen, Bernardo Meurer Costa, Jake Eaton, Dan Kelley, Felix Klock, Jannet Park, Steve Weis, and many other people across Anthropic for their assistance and contributions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.anthropic.com/engineering/building-c-compiler"/><published>2026-02-05T19:07:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46903929</id><title>The time I didn't meet Jeffrey Epstein</title><updated>2026-02-06T13:53:29.626399+00:00</updated><content>&lt;doc fingerprint="6767385f0fa437a5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The time I didn’t meet Jeffrey Epstein&lt;/head&gt;
    &lt;p&gt;Last night, I was taken aback to discover that my name appears in the Epstein Files, in 26 different documents. This is despite the fact that I met Jeffrey Epstein a grand total of zero times, and had zero email or any other contact with him … which is more (less) than some of my colleagues can say.&lt;/p&gt;
    &lt;p&gt;The bulk of the correspondence involves Epstein wanting to arrange a meeting with me and Seth Lloyd back in 2010, via an intermediary named Charles Harper, about funding a research project on “Cryptography in Nature.”&lt;/p&gt;
    &lt;p&gt;Searching my inbox, it turns out that this Charles Harper did contact me in May 2010, and I then met him at S&amp;amp;S Deli in Cambridge (plausible, although I have zero recollections of this meeting—only of the deli). Harper then sent me a detailed followup email about his proposed Cryptography in Nature project, naming Jeffrey Epstein for the first time as the project’s funder, and adding: “perhaps you will know Jeffrey and his background and situation.”&lt;/p&gt;
    &lt;p&gt;For whatever reason, I forwarded this email to my parents, brother, and then-fiancee Dana. My brother then found and shared a news article about Epstein’s prostitution conviction, adding to a different article that I had found and shared. (At that time, like many others, I’d probably vaguely heard of Epstein, but he didn’t have 0.1% the infamy that he has now.) Then my mom wrote the following: “be careful not to get sucked up in the slime-machine going on here! Since you don’t care that much about money, they can’t buy you at least.”&lt;/p&gt;
    &lt;p&gt;It appears from emails that Charles Harper tried again later that summer to arrange a meeting between me and Epstein, but that I took my mom’s advice and largely blew him off, and no such meeting ever happened. Amazingly, I then forgot entirely that any of this had occurred until last night. By way of explanation, some business/finance dude trying to interest me in half-baked ideas involving quantum, AI, cryptography, etc., often dangling the prospect of funding for my students and postdocs, shows up in my life like every month. Most of their world-changing initiatives go nowhere for one reason or another. There really wasn’t much reason to think further about this, until Epstein had become history’s most notorious sex criminal, which (again) wouldn’t happen until years later, after I’d forgotten.&lt;/p&gt;
    &lt;p&gt;It gets better, though. In the Epstein Files, one also finds a November 2010 letter from Charles Harper to Epstein about organizing a conference on the same Cryptography in Nature topic, which includes the following idea about me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scott Aaronson was born on May 21st, 1981. He will be 30 in 2011. The conference could follow a theme of: “hurry to think together with Scott Aaronson while he is still in his 20s and not yet a pitiful over-the-hill geezer in his 30s.” This offers another nice opportunity for celebration.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I see no indication that any such conference ever happened; in any case, I didn’t get invited to one!&lt;/p&gt;
    &lt;p&gt;On my Facebook, some friends are joking that “it tracks that someone into teenage girls might think Scott Aaronson was a hot property in his nubile 20s, who would get old and boring in his 30s”—and that maybe Epstein was less sexist about such matters than everyone assumes. I replied that I wished I could say the proposition that I’d gradually get slower and more senile through the 2010s and 2020s was entirely false.&lt;/p&gt;
    &lt;p&gt;But the best comment was that I’ve been incredibly lucky to have such an astute family. If only Bill Gates and Larry Summers had had my mom to go to for advice, they could’ve saved themselves a lot of grief.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://scottaaronson.blog/?p=9534"/><published>2026-02-05T19:29:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46904361</id><title>LinkedIn checks for 2953 browser extensions</title><updated>2026-02-06T13:53:29.016274+00:00</updated><content>&lt;doc fingerprint="ba50dea2345746f0"&gt;
  &lt;main&gt;
    &lt;p&gt;LinkedIn silently probes for 2,953 Chrome extensions on every page load.&lt;/p&gt;
    &lt;p&gt;This repository documents every extension LinkedIn checks for and provides tools to identify them.&lt;/p&gt;
    &lt;p&gt;The complete list of extensions with names and Chrome Web Store links:&lt;/p&gt;
    &lt;p&gt;chrome_extensions_with_names_all.csv&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Column&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extension ID&lt;/cell&gt;
        &lt;cell&gt;32-character Chrome extension identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Name&lt;/cell&gt;
        &lt;cell&gt;Extension name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;URL&lt;/cell&gt;
        &lt;cell&gt;Link to Chrome Web Store or Extpose&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fetches extension names from Chrome Web Store with Extpose fallback for removed/unavailable extensions.&lt;/p&gt;
    &lt;code&gt;# Fetch all extensions
node fetch_extension_names.js

# Fetch a subset (useful if rate limited)
node fetch_extension_names.js --offset 0 --limit 500
node fetch_extension_names.js -o 500 -l 500

# Show help
node fetch_extension_names.js --help&lt;/code&gt;
    &lt;p&gt;Test script that processes the first 3 extensions with verbose output.&lt;/p&gt;
    &lt;code&gt;node test_fetch.js&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2,953 total extensions in LinkedIn's fingerprint list&lt;/item&gt;
      &lt;item&gt;~78% found on Chrome Web Store&lt;/item&gt;
      &lt;item&gt;~22% found via Extpose fallback (removed or unavailable on Chrome Web Store)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;chrome_extension_ids.txt&lt;/code&gt;- Raw list of extension IDs extracted from LinkedIn's fingerprint.js&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fingerprint.js&lt;/code&gt;- LinkedIn's page script with the extensions (minified)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/mdp/linkedin-extension-fingerprinting"/><published>2026-02-05T20:00:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46904569</id><title>Claude Opus 4.6 extra usage promo</title><updated>2026-02-06T13:53:28.603400+00:00</updated><content>&lt;doc fingerprint="84c1e6de4aa3c940"&gt;
  &lt;main&gt;
    &lt;p&gt;We're offering a limited-time $50 (USD, or local currency equivalent) in extra usage to Pro and Max users to coincide with the launch of Claude Opus 4.6.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eligibility requirements&lt;/head&gt;
    &lt;p&gt;To be eligible for this promotion, you must meet the following criteria:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;You started your Pro or Max subscription before Wednesday, February 4, 2026 at 11:59 PM PT.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You have enabled extra usage before Monday, February 16, 2026 at 11:59 PM PT.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This offer does not apply to Team, Enterprise, or API/Console users. There are no exceptions to these eligibility requirements. This offer has no cash value and is not assignable or transferable. It may not be combined with other offers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to claim the $50 credit&lt;/head&gt;
    &lt;p&gt;How you receive the credit depends on whether you already have extra usage enabled:&lt;/p&gt;
    &lt;p&gt;If extra usage is already enabled: The $50 credit will be applied to your account automatically. No action is needed on your part.&lt;/p&gt;
    &lt;p&gt;If extra usage is not enabled: You'll need to enable extra usage to claim the credit. Here's how:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Go to Settings &amp;gt; Usage.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enable extra usage when prompted.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your $50 credit will be applied once extra usage is active.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This promotion can be claimed from Thursday, February 5, 2026 at 10 AM PT through Monday, February 16, 2026 at 11:59 PM PT. After this window closes, the credit can no longer be claimed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where can I use this credit?&lt;/head&gt;
    &lt;p&gt;The $50 credit can be used for Claude, Claude Code, and Cowork, including all models and features available on your plan.&lt;/p&gt;
    &lt;p&gt;Note: Settings &amp;gt; Usage is not accessible via the Claude mobile apps, so if you haven’t enabled extra usage yet, you’ll need to do this on the web version of Claude before you receive the credit.&lt;/p&gt;
    &lt;head rend="h2"&gt;When does my credit expire?&lt;/head&gt;
    &lt;p&gt;Your $50 credit expires 60 days after the date you claim it. Any unused portion of the credit will not carry over after that date.&lt;/p&gt;
    &lt;p&gt;Once the credit expires or is fully used, extra usage will remain enabled on your account. If you’ve also enabled auto-reload in Settings &amp;gt; Usage under Extra usage, then any usage beyond your plan limits after that point will be billed at the standard extra usage rate. If you don't want to use extra usage going forward, you can disable it in your account settings at any time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://support.claude.com/en/articles/13613973-claude-opus-4-6-extra-usage-promo"/><published>2026-02-05T20:15:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46906947</id><title>The RCE that AMD won't fix</title><updated>2026-02-06T13:53:27.930397+00:00</updated><content>&lt;doc fingerprint="8551539d8a3b7c19"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The RCE that AMD won’t fix&lt;/head&gt;
    &lt;p&gt;After being interrupted multiple times by an annoying console window that would pop up periodically on my new gaming PC, I managed to track the offending executable down to AMD’s AutoUpdate software.&lt;/p&gt;
    &lt;p&gt;In my anger, I decided to punish this software by decompiling it to figure out how it worked, and accidentally discovered a trivial Remote Code Execution (RCE) vulnerability in the process.&lt;/p&gt;
    &lt;p&gt;The first thing I found, is that they store their update URL in the program’s &lt;code&gt;app.config&lt;/code&gt;, although its a little odd that they use their “Develpment” URL in production, it uses HTTPS so its perfectly safe.&lt;/p&gt;
    &lt;p&gt;The real problem starts when you open up this URL in your web browser, and realise that all of the executable download URL’s are using HTTP.&lt;/p&gt;
    &lt;p&gt;This means that a malicious attacker on your network, or a nation state that has access to your ISP can easily perform a MITM attack and replace the network response with any malicious executable of their choosing.&lt;/p&gt;
    &lt;p&gt;I was hoping that AMD perhaps had some form of certificate validation to ensure that it could not download &amp;amp; run any unsigned executables, however a quick look into the decompiled code revealed that the AutoUpdate software does no such validation and immediately executes the downloaded file.&lt;/p&gt;
    &lt;p&gt;After finding this issue, I thought it was worth reporting to AMD since it seemed to be a pretty severe issue.&lt;/p&gt;
    &lt;p&gt;However it turned out to be considered “out of scope”, resulting in AMD not considering this to be a vulnerability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline (DD/MM/YYYY)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;27/01/2026 - Vulnerability Discovered&lt;/item&gt;
      &lt;item&gt;05/02/2026 - Vulnerability Reported&lt;/item&gt;
      &lt;item&gt;05/02/2026 - Report Closed as &lt;code&gt;wont fix/out of scope&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;06/02/2026 - Blog published&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this blog, you can read another of my write-ups here: 1.4 Billion exposed user records via insecure Firebase instances in top Android apps&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mrbruh.com/amd/"/><published>2026-02-05T23:29:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46908671</id><title>I reversed Tower of Fantasy's anti-cheat driver: a BYOVD toolkit never loaded</title><updated>2026-02-06T13:53:27.603044+00:00</updated><content>&lt;doc fingerprint="3d1bd917386a7e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tower of Flaws: Dismantling Tower of Fantasy's Anti-Cheat Driver While Waiting for The Game to Install&lt;/head&gt;
    &lt;p&gt;This all started because I wanted to delete my Tower of Fantasy account from over 4 years ago.&lt;/p&gt;
    &lt;p&gt;For the life of me, I couldn’t find a way to do it without having the game installed. There was no web portal and no obvious support route. Eventually I gave up and decided to just download it.&lt;/p&gt;
    &lt;p&gt;Tower of Fantasy is over 100 GB so it would be a long install. I already knew the game shipped with an anti-cheat driver from past experience, so while the download crawled along I started poking around the launcher directory. That’s when I noticed &lt;code&gt;GameDriverX64.sys&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Kernel drivers run with the highest privileges on your machine. Anti-cheat drivers use this power to protect games from cheaters, but when they’re poorly written, attackers can abuse that same power against you.&lt;/p&gt;
    &lt;p&gt;I opened the driver in IDA expecting a wall of virtualized code, probably VMProtect. Instead I got clean, readable functions with no obfuscation or virtualization at all.&lt;/p&gt;
    &lt;p&gt;By now, the install was at 9%. I had time to dig in.&lt;/p&gt;
    &lt;p&gt;There’s a lot of noise online about kernel anti-cheats being “spyware” or inherently privacy-invasive. Most of it misidentifies the actual risk. A usermode game client can already steal your browser cookies, log keystrokes, and exfiltrate files without ever touching the kernel. The real concern with kernel anti-cheats isn’t surveillance, it’s that they are security-critical code running at the highest privilege level. When they’re poorly written, they become attack surface, and when they fail, they can take your entire system down with them (e.g the CrowdStrike incident). For a thorough, level-headed breakdown of the privacy and security tradeoffs, I’d recommend this post by Bevan Philip.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Isn’t This Obfuscated?&lt;/head&gt;
    &lt;p&gt;The previous version of this driver (&lt;code&gt;KSophon_x64.sys&lt;/code&gt;) was VMProtect’d to hell, so I was curious why they’d strip protection from a security-critical kernel component. The reason is due to HVCI.&lt;/p&gt;
    &lt;p&gt;HVCI (Hypervisor-Protected Code Integrity) is a Windows security feature that uses Hyper-V to enforce code integrity above the NT kernel, enabled by default on clean Windows 11 installs. The key constraint: W^X (Write XOR Execute) enforcement means code pages can’t be both writable and executable. VMProtect’s packing and import protection both violate this, so the driver fails integrity checks on HVCI-enabled systems.&lt;/p&gt;
    &lt;p&gt;VMProtect can still work under HVCI if you stick to mutation and virtualization macros while avoiding the features that break W^X. They could still protect their imports manually and virtualize the bulk of their code. For some reason, they did neither.&lt;/p&gt;
    &lt;p&gt;Even with VMProtect, these vulnerabilities would still exist. The IOCTLs still do what they do and the authentication is essentially nonexistent. Obfuscation makes reversing harder, not impossible.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the Driver Actually Does&lt;/head&gt;
    &lt;p&gt;It registers the following device:&lt;/p&gt;
    &lt;head rend="h3"&gt;Device Access Control&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;IRP_MJ_CREATE&lt;/code&gt; handler checks whether the calling process has loaded one of three specific DLLs:&lt;/p&gt;
    &lt;p&gt;In practice, you don’t even need the real DLLs. The check only looks at module names in your PEB, so you can rename literally any DLL to &lt;code&gt;QmGUI.dll&lt;/code&gt; for example and load it. My PoC just copies &lt;code&gt;version.dll&lt;/code&gt; from System32 and renames it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Handle Protection&lt;/head&gt;
    &lt;p&gt;The driver registers &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; to intercept handle operations on protected processes and strip dangerous access rights. The callbacks look like this:&lt;/p&gt;
    &lt;p&gt;Notably, &lt;code&gt;PROCESS_TERMINATE&lt;/code&gt; is not stripped from process handles, so external processes can still kill the game.&lt;/p&gt;
    &lt;p&gt;Before stripping, the callback checks a whitelist. One entry is hardcoded for &lt;code&gt;"CrashCapture.e"&lt;/code&gt; (their crash handler) with zero integrity verification. The &lt;code&gt;.e&lt;/code&gt; extension isn’t a typo by the way, &lt;code&gt;PsGetProcessImageFileName&lt;/code&gt; returns from &lt;code&gt;EPROCESS.ImageFileName&lt;/code&gt;, a 15-byte array, so “CrashCapture.exe” (16 chars) gets truncated to 14 chars + null. The check itself is just a filename comparison:&lt;/p&gt;
    &lt;p&gt;There’s also a subtle bug here: the &lt;code&gt;strnicmp&lt;/code&gt; calls use &lt;code&gt;strlen(ProcessName)&lt;/code&gt; as the comparison length, not the length of the constant string. This means the comparison is a prefix match. Any process whose name is a prefix of the whitelist entry will pass. A process named &lt;code&gt;"Crash"&lt;/code&gt; or even &lt;code&gt;"C"&lt;/code&gt; would match &lt;code&gt;"CrashCapture.e"&lt;/code&gt;. The same bug applies to every dynamic whitelist entry.&lt;/p&gt;
    &lt;p&gt;Dynamic whitelist entries do one more check: comparing the caller’s &lt;code&gt;OptionalHeader.CheckSum&lt;/code&gt; against a stored value. PE checksums aren’t cryptographic though, so anyone can set them to any value with a hex editor.&lt;/p&gt;
    &lt;p&gt;There’s also a background thread that runs anti-debug and integrity checks in a loop:&lt;/p&gt;
    &lt;p&gt;Plus a process creation callback that kills all protected processes if it sees &lt;code&gt;GPUView.exe&lt;/code&gt; or &lt;code&gt;xperf.exe&lt;/code&gt; launch, neither of which are reverse engineering tools. My best guess is this was cargo-culted from another anti-cheat driver’s blocklist.&lt;/p&gt;
    &lt;head rend="h2"&gt;The IOCTL Interface&lt;/head&gt;
    &lt;p&gt;The driver exposes 10 IOCTL codes, 7 of which are interesting:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;IOCTL Code&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Check if debugger attached via debug port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222004&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Register a process as “protected”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222008&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Heartbeat/keepalive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222020&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get list of protected processes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222040&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Terminate any process by PID&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222044&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Strip handle access rights via ExEnumHandleTable&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;0x222048&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validate memory checksum&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The remaining three (&lt;code&gt;0x222080&lt;/code&gt;, &lt;code&gt;0x222084&lt;/code&gt;, &lt;code&gt;0x222088&lt;/code&gt;) expose kernel memory scanning and module enumeration. Not the main finding here, but worth noting they exist behind the same weak authentication.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Authentication Mechanism (Or Lack Thereof)&lt;/head&gt;
    &lt;p&gt;Every IOCTL is “protected” by this:&lt;/p&gt;
    &lt;p&gt;A hardcoded 32-bit magic value. No cryptographic verification, no challenge-response, no signature validation. The driver is unobfuscated, so anyone can just read the value out of the binary and call whatever IOCTL they want.&lt;/p&gt;
    &lt;head rend="h3"&gt;Four Layers, None of Them Do Anything&lt;/head&gt;
    &lt;p&gt;The full “authentication” stack:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;DLL presence check - bypassed by loading any renamed DLL&lt;/item&gt;
      &lt;item&gt;Process name whitelist - bypassed by renaming your executable&lt;/item&gt;
      &lt;item&gt;PE checksum validation - bypassed by writing 4 bytes with a hex editor&lt;/item&gt;
      &lt;item&gt;Hardcoded magic number - bypassed by reading the binary&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vulnerability #1: Arbitrary Process Termination&lt;/head&gt;
    &lt;p&gt;IOCTL &lt;code&gt;0x222040&lt;/code&gt; terminates any process on the system:&lt;/p&gt;
    &lt;p&gt;The IOCTL dispatch routine validates the magic field, then passes just the PID to an internal &lt;code&gt;TerminateProcess&lt;/code&gt; function. That function calls &lt;code&gt;ZwOpenProcess&lt;/code&gt; with &lt;code&gt;GENERIC_ALL&lt;/code&gt; (&lt;code&gt;0x10000000&lt;/code&gt;), then &lt;code&gt;ZwTerminateProcess&lt;/code&gt;. The &lt;code&gt;Zw&lt;/code&gt; prefix is important here: &lt;code&gt;Zw*&lt;/code&gt; functions set &lt;code&gt;PreviousMode&lt;/code&gt; to &lt;code&gt;KernelMode&lt;/code&gt; before entering the system service, which tells the object manager to skip security descriptor checks on the target process entirely. If they’d used the &lt;code&gt;Nt*&lt;/code&gt; variants instead, the call would inherit the original caller’s previous mode and could actually be denied. But with &lt;code&gt;Zw*&lt;/code&gt;, no access check will block the open. &lt;code&gt;GENERIC_ALL&lt;/code&gt; maps to every access right, so the returned handle can do anything. Antivirus, EDR agents, system services, even PPL (Protected Process Light) processes are not safe. &lt;code&gt;ZwTerminateProcess&lt;/code&gt; from kernel mode bypasses PPL protection entirely, which means even processes that Windows itself is supposed to shield from tampering are killable through this driver.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulnerability #2: Arbitrary Process Protection&lt;/head&gt;
    &lt;p&gt;IOCTL &lt;code&gt;0x222004&lt;/code&gt; registers any process as “protected.” The same &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; that strip handle access rights for the game now apply to whatever PID you pass in.&lt;/p&gt;
    &lt;p&gt;This doesn’t make your process invisible on its own. EDR agents register kernel callbacks (&lt;code&gt;PsSetCreateProcessNotifyRoutine&lt;/code&gt;, &lt;code&gt;PsSetLoadImageNotifyRoutine&lt;/code&gt;) to inject their monitoring DLLs during early process creation, before your entry point or even TLS callbacks run. By the time your code can call this IOCTL, the EDR’s hooks are already in your process.&lt;/p&gt;
    &lt;p&gt;What this does block is future handle operations. &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; intercepts both &lt;code&gt;OB_OPERATION_HANDLE_CREATE&lt;/code&gt; and &lt;code&gt;OB_OPERATION_HANDLE_DUPLICATE&lt;/code&gt;, so it covers &lt;code&gt;NtOpenProcess&lt;/code&gt; and &lt;code&gt;NtDuplicateObject&lt;/code&gt;. Important detail: the callback can’t outright deny the open. It strips access bits from the &lt;code&gt;DesiredAccess&lt;/code&gt; mask, so the call still succeeds but returns a handle with reduced permissions (no &lt;code&gt;PROCESS_VM_READ&lt;/code&gt;, no &lt;code&gt;PROCESS_VM_WRITE&lt;/code&gt;, etc). Functionally useless for the caller.&lt;/p&gt;
    &lt;p&gt;That leaves existing handles. Any handles the EDR already holds from before the IOCTL remain valid with their original access rights. But the driver has a solution for that too: IOCTL &lt;code&gt;0x222044&lt;/code&gt; calls &lt;code&gt;ExEnumHandleTable&lt;/code&gt; to walk the system handle table and retroactively strip access rights from existing handles pointing at a target process. So the full protection chain doesn’t even require killing anything:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;0x222004&lt;/code&gt;to register your PID as protected (blocks future handles via&lt;code&gt;ObRegisterCallbacks&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0x222044&lt;/code&gt;to strip all existing handles to your process (kills current EDR handles via&lt;code&gt;ExEnumHandleTable&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;New handles get stripped on creation, existing handles get stripped after the fact. Combine that with Vulnerability #1 to kill the EDR service entirely and you have the complete BYOVD toolkit. If you remember mhyprot2 (the Genshin Impact driver that got weaponized by ransomware groups), this driver has the same kill-then-shield capability exposed through a comparably weak authentication mechanism.&lt;/p&gt;
    &lt;head rend="h2"&gt;They Don’t Even Use It&lt;/head&gt;
    &lt;p&gt;After documenting all of this, I actually launched the game to see the driver in action.&lt;/p&gt;
    &lt;p&gt;It wasn’t running. I checked for a loaded driver, checked for associated services, tried deleting the file to see if anything held a handle. Nothing. The driver just sits in the game directory and never gets loaded.&lt;/p&gt;
    &lt;p&gt;The game process isn’t protected either. No &lt;code&gt;ObRegisterCallbacks&lt;/code&gt; stripping handle access. You can freely open a handle and read/write its memory.&lt;/p&gt;
    &lt;p&gt;They ship a kernel driver with hardcoded authentication and full BYOVD capabilities, and they don’t even load it. It just sits on every player’s machine. Good to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proof of Concept&lt;/head&gt;
    &lt;p&gt;I wrote a PoC demonstrating both vulnerabilities. It bypasses the DLL check by loading one of the required DLLs, opens a handle to the driver, and first registers notepad.exe as a protected process via IOCTL &lt;code&gt;0x222004&lt;/code&gt;. Once protected, it waits for you to press Delete, then terminates it via IOCTL &lt;code&gt;0x222040&lt;/code&gt; with the magic value.&lt;/p&gt;
    &lt;p&gt;Full source on GitHub: TowerOfFlaws&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;Someone already filed a CVE for this driver (CVE-2025-61155) before I started this work, but the entry has no writeup, no PoC, and no technical details. As far as I can tell, this is the first public documentation of how these vulnerabilities actually work. The driver isn’t actively loaded by the game (reducing immediate attack surface), the techniques involved are straightforward enough that any motivated attacker would find them independently, and if nothing else it’s a good reference for other anti-cheat vendors and driver developers on what not to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;mhyprot2 already proved that anti-cheat drivers make high-value BYOVD targets. This driver has the same capabilities behind weaker authentication, and it shipped anyway. HVCI doesn’t kill obfuscation entirely, but it does break major features like packing and import protection that protectors like VMProtect offer. It’s possible some vendors just see their protected driver fail on HVCI systems and strip it entirely without understanding what specifically broke. Code obfuscation is still possible under HVCI, just more constrained, and if your security model depended on it, you were already in trouble.&lt;/p&gt;
    &lt;p&gt;If you’re shipping a kernel driver and want to make sure it doesn’t end up as someone’s next blog post, feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://vespalec.com/blog/tower-of-flaws/"/><published>2026-02-06T03:22:29+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46908700</id><title>Waiting for Postgres 19: Better planner hints with path generation strategies [video]</title><updated>2026-02-06T13:53:26.593397+00:00</updated><content>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.youtube.com/watch?v=QLb3nhIy2Lc"/><published>2026-02-06T03:25:51+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46909037</id><title>Show HN: Artifact Keeper – Open-Source Artifactory/Nexus Alternative in Rust</title><updated>2026-02-06T13:53:25.948208+00:00</updated><content>&lt;doc fingerprint="85f15bf9b9b7724d"&gt;
  &lt;main&gt;
    &lt;p&gt;Your packages. Your servers. Your freedom.&lt;/p&gt;
    &lt;p&gt;Website · Docs · Live Demo · MIT Licensed&lt;/p&gt;
    &lt;p&gt;A full-featured, enterprise-grade artifact registry you can self-host in minutes. Drop-in replacement for JFrog Artifactory and Sonatype Nexus with zero feature gates — security scanning, SSO, replication, all 45+ package formats — everything ships in the open-source release.&lt;/p&gt;
    &lt;p&gt;No open-core. No "enterprise edition." No surprise invoices.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Repository&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Stack&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Backend server, CLI, and Docker deployment&lt;/cell&gt;
        &lt;cell&gt;Rust, Axum, PostgreSQL, Meilisearch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper-web&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Web frontend&lt;/cell&gt;
        &lt;cell&gt;Next.js 15, TypeScript, Tailwind CSS, shadcn/ui&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper-ios&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;iOS &amp;amp; macOS app&lt;/cell&gt;
        &lt;cell&gt;SwiftUI, Swift 6, Alamofire&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper-android&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Android app&lt;/cell&gt;
        &lt;cell&gt;Jetpack Compose, Kotlin, Material 3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper-api&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;OpenAPI 3.1 spec (165 endpoints)&lt;/cell&gt;
        &lt;cell&gt;TypeScript + Rust SDK generation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;artifact-keeper-example-plugin&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Example WASM plugin (Unity .unitypackage)&lt;/cell&gt;
        &lt;cell&gt;Rust, WIT, Wasmtime&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;45+ Package Formats — Native protocol support. Not a generic blob store with format labels. Your package managers (&lt;code&gt;pip install&lt;/code&gt;, &lt;code&gt;npm install&lt;/code&gt;, &lt;code&gt;docker pull&lt;/code&gt;, &lt;code&gt;cargo add&lt;/code&gt;, &lt;code&gt;helm install&lt;/code&gt;, &lt;code&gt;go get&lt;/code&gt;, etc.) talk directly to Artifact Keeper using their native protocols.&lt;/p&gt;
    &lt;p&gt;Security Scanning — Automated vulnerability detection with Trivy and Grype. Policy engine with severity thresholds, quarantine workflows, and scan-before-download enforcement.&lt;/p&gt;
    &lt;p&gt;WASM Plugin System — Extend with custom format handlers via WebAssembly. Ship your own package format support without forking the backend.&lt;/p&gt;
    &lt;p&gt;Edge Replication — Mesh-based artifact distribution with swarm sync and P2P transfers between nodes. Put caches close to your build agents.&lt;/p&gt;
    &lt;p&gt;SSO &amp;amp; Multi-Auth — OpenID Connect, LDAP, SAML 2.0, JWT, and API tokens. RBAC with per-repository permissions.&lt;/p&gt;
    &lt;p&gt;Artifactory Migration — Built-in tooling to migrate repositories, artifacts, users, and permissions from JFrog Artifactory. One command.&lt;/p&gt;
    &lt;p&gt;Full-Text Search — Meilisearch-powered search across all repositories, packages, and artifact metadata.&lt;/p&gt;
    &lt;p&gt;Manage your registries from anywhere. Monitor builds, browse repositories, trigger security scans, and administer users — all from native mobile apps with adaptive layouts.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Android&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;iOS&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A full management interface for repositories, packages, security policies, user administration, SSO configuration, replication topology, and operational analytics.&lt;/p&gt;
    &lt;code&gt;# Clone and start with Docker Compose
git clone https://github.com/artifact-keeper/artifact-keeper.git
cd artifact-keeper
docker compose up -d

# That's it. Visit http://localhost:9080&lt;/code&gt;
    &lt;p&gt;Or pull the pre-built images directly:&lt;/p&gt;
    &lt;code&gt;# Backend (required)
docker pull ghcr.io/artifact-keeper/artifact-keeper-backend:latest

# Web dashboard (recommended)
docker pull ghcr.io/artifact-keeper/artifact-keeper-web:latest&lt;/code&gt;
    &lt;p&gt;Full deployment guides for Docker, Kubernetes, and AWS are in the docs.&lt;/p&gt;
    &lt;code&gt;graph TB
    subgraph Clients["Clients"]
        CLI["CLI &amp;amp; Package Managers&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;pip · npm · docker · cargo&amp;lt;br/&amp;gt;helm · go · maven · ...&amp;lt;/sub&amp;gt;"]
        WebApp["Web Dashboard&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Next.js 15 · Desktop Browser&amp;lt;/sub&amp;gt;"]
        iOS["iPhone · iPad · Mac&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;SwiftUI · Swift 6&amp;lt;/sub&amp;gt;"]
        Android["Android Phone · Tablet&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Jetpack Compose · Kotlin&amp;lt;/sub&amp;gt;"]
    end

    subgraph Core["Artifact Keeper Backend"]
        API["REST API Gateway&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Rust · Axum&amp;lt;/sub&amp;gt;"]
        Handlers["45+ Format Handlers&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Native protocol support&amp;lt;/sub&amp;gt;"]
        WASM["WASM Plugin Runtime&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Wasmtime · WIT&amp;lt;/sub&amp;gt;"]
        Auth["Auth Engine&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;OIDC · LDAP · SAML · JWT&amp;lt;/sub&amp;gt;"]
        Policy["Policy Engine&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Severity gates · Quarantine&amp;lt;/sub&amp;gt;"]
    end

    subgraph Data["Data Layer"]
        PG[("PostgreSQL 16&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Metadata &amp;amp; config&amp;lt;/sub&amp;gt;")]
        Storage[("Storage&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;S3 / Filesystem&amp;lt;/sub&amp;gt;")]
        Meili[("Meilisearch&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Full-text search&amp;lt;/sub&amp;gt;")]
    end

    subgraph Security["Security Scanning"]
        Trivy["Trivy&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Container &amp;amp; FS scanning&amp;lt;/sub&amp;gt;"]
        Grype["Grype&amp;lt;br/&amp;gt;&amp;lt;sub&amp;gt;Dependency scanning&amp;lt;/sub&amp;gt;"]
    end

    subgraph Edge["Edge Replication"]
        Peer1["Edge Node"]
        Peer2["Edge Node"]
        Peer3["Edge Node"]
    end

    CLI --&amp;gt;|"Native protocols"| API
    WebApp --&amp;gt; API
    iOS --&amp;gt; API
    Android --&amp;gt; API

    API --&amp;gt; Handlers
    API --&amp;gt; Auth
    Handlers --&amp;gt; WASM
    Handlers --&amp;gt; Policy

    API --&amp;gt; PG
    Handlers --&amp;gt; Storage
    API --&amp;gt; Meili

    Policy --&amp;gt; Trivy
    Policy --&amp;gt; Grype

    API &amp;lt;--&amp;gt;|"Borg Replication"| Peer1
    API &amp;lt;--&amp;gt;|"Borg Replication"| Peer2
    API &amp;lt;--&amp;gt;|"Borg Replication"| Peer3
    Peer1 &amp;lt;--&amp;gt;|"P2P Mesh"| Peer2
    Peer2 &amp;lt;--&amp;gt;|"P2P Mesh"| Peer3
    Peer1 &amp;lt;--&amp;gt;|"P2P Mesh"| Peer3

    style Core fill:#1a1a2e,stroke:#e94560,color:#fff
    style Data fill:#16213e,stroke:#0f3460,color:#fff
    style Security fill:#1a1a2e,stroke:#e94560,color:#fff
    style Edge fill:#0f3460,stroke:#533483,color:#fff
    style Clients fill:#16213e,stroke:#0f3460,color:#fff

    style API fill:#e94560,stroke:#e94560,color:#fff
    style Handlers fill:#e94560,stroke:#e94560,color:#fff
    style WASM fill:#533483,stroke:#533483,color:#fff
    style Auth fill:#e94560,stroke:#e94560,color:#fff
    style Policy fill:#e94560,stroke:#e94560,color:#fff

    style PG fill:#0f3460,stroke:#0f3460,color:#fff
    style Storage fill:#0f3460,stroke:#0f3460,color:#fff
    style Meili fill:#0f3460,stroke:#0f3460,color:#fff

    style Trivy fill:#533483,stroke:#533483,color:#fff
    style Grype fill:#533483,stroke:#533483,color:#fff

    style Peer1 fill:#533483,stroke:#533483,color:#fff
    style Peer2 fill:#533483,stroke:#533483,color:#fff
    style Peer3 fill:#533483,stroke:#533483,color:#fff

    style CLI fill:#0f3460,stroke:#0f3460,color:#fff
    style WebApp fill:#0f3460,stroke:#0f3460,color:#fff
    style iOS fill:#0f3460,stroke:#0f3460,color:#fff
    style Android fill:#0f3460,stroke:#0f3460,color:#fff
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome. Pick an issue, open a PR, or start a discussion. The backend is Rust, the frontend is TypeScript/React, and the mobile apps are native Swift and Kotlin.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation: artifactkeeper.com/docs&lt;/item&gt;
      &lt;item&gt;Email: support@artifactkeeper.com&lt;/item&gt;
      &lt;item&gt;Issues: GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT. Every feature. No exceptions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/artifact-keeper"/><published>2026-02-06T04:12:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46909439</id><title>Systems Thinking</title><updated>2026-02-06T13:53:25.845682+00:00</updated><content>&lt;doc fingerprint="b77892377b0587ad"&gt;
  &lt;main&gt;
    &lt;p&gt; There are two main schools of thought in software development about how to build really big, complicated stuff.&lt;lb/&gt;The most prevalent one, these days, is that you gradually evolve the complexity over time. You start small and keep adding to it. &lt;lb/&gt;The other school is that you lay out a huge specification that would fully work through all of the complexity in advance, then build it.&lt;lb/&gt;In a sense, it is the difference between the way an entrepreneur might approach doing a startup versus how we build modern skyscrapers. Evolution versus Engineering.&lt;lb/&gt;I was working in a large company a while ago, and I stumbled on the fact that they had well over 3000 active systems that were covering dozens of lines of business and all of the internal departments. It had evolved this way over fifty years, and included lots of different tech stacks, as well as countless vendors. Viewed as ‘one’ thing it was a pretty shaky house of cards.&lt;lb/&gt;It’s not hard to see that if they had a few really big systems, then a great number of their problems would disappear. The inconsistencies between data, security, operations, quality, and access were huge across all of those disconnected projects. Some systems were up-to-date, some were ancient. Some worked well, some were barely functional. With way fewer systems, a lot of these self-inflicted problems would just go away.&lt;lb/&gt;It’s not that you could cut the combined complexity in half, but more likely that you could bring it down to at least one-tenth of what it is today, if not even better. It would function better, be more reliable, and would be far more resilient to change. It would likely cost far less and require fewer employees as well. All sorts of ugly problems that they have now would just not exist.&lt;lb/&gt;The core difference between the different schools really centers around how to deal with dependencies. &lt;lb/&gt;If you had thousands of little blobs of complexity that were all entirely independent, then getting finished is just a matter of banging out each one by itself until they are all completed. That’s the dream. &lt;lb/&gt;But in practice, very few things in a big ecosystem are actually independent. That’s the problem.&lt;lb/&gt;If you are going to evolve a system, then you ignore these dependencies. Sort them out afterwards, as the complexity grows. It’s faster, and you can get started right away.&lt;lb/&gt;If you were going to design a big system, then these dependencies dictate that design. You have to go through each one and understand them all right away. They change everything from the architecture all the way down to the idioms and style in the code. &lt;lb/&gt;But that means that all of the people working to build up this big system have to interact with each other. Coordinate and communicate. That is a lot of friction that management and the programmers don’t want. They tend to feel like it would all get done faster if they could just go off on their own. And it will, in the short-term.&lt;lb/&gt;If you ignore a dependency and try to fix it later, it will be more expensive. More time, more effort, more thinking. And it will require the same level of coordination that you tried to avoid initially. Slightly worse, in that the time pressures of doing it correctly generally give way to just getting it done quickly, which pumps up the overall artificial complexity. The more hacks you throw at it, the more hacks you will need to hold it together. It spirals out of control. You lose big in the long-term.&lt;lb/&gt;One of the big speed bumps preventing big up-front designs is a general lack of knowledge. Since the foundations like tech stacks, frameworks, and libraries are always changing rapidly these days, there are few accepted best practices, and most issues are incorrectly believed to be subjective. They’re not, of course, but it takes a lot of repeated experience to see that. &lt;lb/&gt;The career path of most application programmers is fairly short. In most enterprises, the majority have five years or less of real in-depth experience, and battle-scared twenty-year+ vets are rare. Mostly, these novices are struggling through early career experiences, not ready yet to deal with the unbounded, massive complexity present in a big design. &lt;lb/&gt;Also, the other side of it is that evolutionary projects are just more fun. I’ve preferred them. You’re not loaded down with all those messy dependencies. Way fewer meetings, so you can just get into the work and see how it goes. Endlessly arguing about fiddly details in a giant spec is draining, made worse if the experience around you is weak. &lt;lb/&gt;Evolutionary projects go very badly sometimes. The larger they grow, the more likely they will derail. And the fun gives way to really bad stress. That severe last-minute panic that comes from knowing that the code doesn't really work as it should, and probably never will. And the longer-term dissatisfaction of having done all that work to ultimately just contribute to the problem, not actually fix it.&lt;lb/&gt;Big up-front designs are often better from a stress perspective. A little slow to start and sometimes slow in the middle, they mostly smooth out the overall development process. You’ve got a lot of work to do, but you’ve also got enough time to do it correctly. So you grind through it, piece by piece, being as attentive to the details as possible. Along the way, you actively look for smarter approaches to compress the work. Reuse, for instance, can shave a ton of code off the table, cut down on testing, and provide stronger certainty that the code will do the right thing in production. &lt;lb/&gt;The fear that big projects will end up producing the wrong thing is often overstated. It’s true for a startup, but entirely untrue for some large business application for a market that’s been around forever. You don’t need to burn a lot of extra time, breaking the work up into tiny fragments, unless you really don’t have a clue what you are building. If you're replacing some other existing system, not only do you have a clue, you usually have a really solid long-term roadmap. Replace the original work and fix its deficiencies. &lt;lb/&gt;There should be some balanced path in the middle somewhere, but I haven’t stumbled across a formal version of it after all these decades. &lt;lb/&gt;We could go first to the dependencies, then come up with reasons why they can be temporarily ignored. You can evolve the next release, but still have a vague big design as a long-term plan. You can refactor the design as you come across new, unexpected dependencies. Change your mind, over and over again, to try to get the evolved works to converge on a solid grand design. Start fast, slow right down, speed up, slow down again, and so forth. The goal is one big giant system to rule them all, but it may just take a while to get there. &lt;lb/&gt;The other point is that the size of the iterations matters, a whole lot. If they are tiny, it is because you are blindly stumbling forward. If you are not blindly stumbling forward, they should be longer, as it is more effective. They don’t have to all be the same size. And you really should stop and take stock after each iteration. The faster people code, the more cleanup that is required. The longer you avoid cleaning it up, the worse it gets, on basically an exponential scale. If you run forward like crazy and never stop, the working environment will be such a swamp that it will all grind to an abrupt stop. This is true in building anything, or even cooking in a restaurant. Speed is a tradeoff.&lt;lb/&gt;Evolution is the way to avoid getting bogged down in engineering, but engineering is the way to ensure that the thing you build really does what it is supposed to do. Engineering is slow, but spinning way out of control is a heck of a lot slower. Evolution is obviously more dynamic, but it is also more chaotic, and you have to continually accept that you’ve gone down a bad path and need to backtrack. That is hard to admit sometimes. For most systems, there are parts that really need to be engineered, and parts that can just be allowed to evolve. The more random the evolutionary path, the more stuff you need to throw away and redo. Wobbling is always expensive. Nature gets away with this by having millions of species, but we really only have one development project, so it isn’t particularly convenient.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="http://theprogrammersparadox.blogspot.com/2026/02/systems-thinking.html"/><published>2026-02-06T05:24:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46909468</id><title>Things Unix can do atomically (2010)</title><updated>2026-02-06T13:53:25.744811+00:00</updated><content>&lt;doc fingerprint="7ff15d51748103e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Things UNIX can do atomically 2010/01/06&lt;/head&gt;
    &lt;p&gt;This is a catalog of things UNIX-like/POSIX-compliant operating systems can do atomically, making them useful as building blocks for thread-safe and multi-process-safe programs without mutexes or read/write locks. The list is by no means exhaustive and I expect it to be updated frequently for the foreseeable future.&lt;/p&gt;
    &lt;p&gt;The philosophy here is to let the kernel do as much work as possible. At my most pessimistic, I trust the kernel developers more than a trust myself. More practically, it’s stupid to spend CPU time locking around an operation that’s already atomic. Added 2010-01-07.&lt;/p&gt;
    &lt;head rend="h2"&gt;Operating on a pathname&lt;/head&gt;
    &lt;p&gt;The operations below are best left to local filesystems. More than a few people have written in crying foul if any of these techniques are used on an NFS mount. True. When there are multiple kernels involved, the kernel can’t very well take care of all the locking for us. Added 2010-01-06.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;mv -T &amp;lt;oldsymlink&amp;gt; &amp;lt;newsymlink&amp;gt;&lt;/code&gt;atomically changes the target of&lt;code&gt;&amp;lt;newsymlink&amp;gt;&lt;/code&gt;to the directory pointed to by&lt;code&gt;&amp;lt;oldsymlink&amp;gt;&lt;/code&gt;and is indispensable when deploying new code. Updated 2010-01-06: both operands are symlinks. (So this isn’t a system call, it’s still useful.)&lt;del rend="overstrike"&gt;A reader pointed out that&lt;/del&gt;Deleted 2010-01-06:&lt;code&gt;ln -Tfs &amp;lt;directory&amp;gt; &amp;lt;symlink&amp;gt;&lt;/code&gt;accomplishes the same thing without the second symlink. Added 2010-01-06.&lt;code&gt;strace(1)&lt;/code&gt;shows that&lt;code&gt;ln -Tfs &amp;lt;directory&amp;gt; &amp;lt;symlink&amp;gt;&lt;/code&gt;actually calls&lt;code&gt;symlink(2)&lt;/code&gt;,&lt;code&gt;unlink(2)&lt;/code&gt;, and&lt;code&gt;symlink(2)&lt;/code&gt;once more, disqualifying it from this page.&lt;code&gt;mv -T &amp;lt;oldsymlink&amp;gt; &amp;lt;newsymlink&amp;gt;&lt;/code&gt;ends up calling&lt;code&gt;rename(2)&lt;/code&gt;which can atomically replace&lt;code&gt;&amp;lt;newsymlink&amp;gt;&lt;/code&gt;. Caveat 2013-01-07: this does not apply to Mac OS X, whose&lt;code&gt;mv(1)&lt;/code&gt;doesn’t call&lt;code&gt;rename(2)&lt;/code&gt;.&lt;code&gt;mv(1)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;link(oldpath, newpath)&lt;/code&gt;creates a new hard link called&lt;code&gt;newpath&lt;/code&gt;pointing to the same inode as&lt;code&gt;oldpath&lt;/code&gt;and increases the link count by one. This will fail with the error code&lt;code&gt;EEXIST&lt;/code&gt;if&lt;code&gt;newpath&lt;/code&gt;already exists, making this a useful mechanism for locking a file amongst threads or processes that can all agree upon the name&lt;code&gt;newpath&lt;/code&gt;. I prefer this technique for whole-file locking because the lock is visible to&lt;code&gt;ls(1)&lt;/code&gt;.&lt;code&gt;link(2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;symlink(oldpath, newpath)&lt;/code&gt;operates very much like&lt;code&gt;link(2)&lt;/code&gt;but creates a symbolic link at a new inode rather than a hard link to the same inode. Symbolic links can point to directories, which hard links cannot, making them a perfect analogy to&lt;code&gt;link(2)&lt;/code&gt;when locking entire directories. This will fail with the error code&lt;code&gt;EEXIST&lt;/code&gt;if&lt;code&gt;newpath&lt;/code&gt;already exists, making this a perfect analogy to&lt;code&gt;link(2)&lt;/code&gt;that works for directories, too. Be careful of symbolic links whose target inode has been removed ("dangling" symbolic links) —&lt;code&gt;open(2)&lt;/code&gt;will fail with the error code&lt;code&gt;ENOENT&lt;/code&gt;. It should be mentioned that inodes are a finite resource (this particular machine has 1,245,184 inodes).&lt;code&gt;symlink(2)&lt;/code&gt;. Added 2010-01-07&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rename(oldpath, newpath)&lt;/code&gt;can change a pathname atomically, provided&lt;code&gt;oldpath&lt;/code&gt;and&lt;code&gt;newpath&lt;/code&gt;are on the same filesystem. This will fail with the error code&lt;code&gt;ENOENT&lt;/code&gt;if&lt;code&gt;oldpath&lt;/code&gt;does not exist, enabling interprocess locking much like&lt;code&gt;link(oldpath, newpath)&lt;/code&gt;above. I find this technique more natural when the files in question will be&lt;code&gt;unlink&lt;/code&gt;ed later.&lt;code&gt;rename(2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;open(pathname, O_CREAT | O_EXCL, 0644)&lt;/code&gt;creates and opens a new file. (Don’t forget to set the mode in the third argument!)&lt;code&gt;O_EXCL&lt;/code&gt;instructs this to fail with the error code&lt;code&gt;EEXIST&lt;/code&gt;if&lt;code&gt;pathname&lt;/code&gt;exists. This is a useful way to decide which process should handle a task: whoever successfully creates the file.&lt;code&gt;open(2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mkdir(dirname, 0755)&lt;/code&gt;creates a new directory but fails with the error code&lt;code&gt;EEXIST&lt;/code&gt;if&lt;code&gt;dirname&lt;/code&gt;exists. This provides for directories the same mechanism&lt;code&gt;link(2)&lt;/code&gt;&lt;code&gt;open(2)&lt;/code&gt;with&lt;code&gt;O_EXCL&lt;/code&gt;provides for files.&lt;code&gt;mkdir(2)&lt;/code&gt;. Added 2010-01-06; edited 2013-01-07.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Operating on a file descriptor&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;fcntl(fd, F_GETLK, &amp;amp;lock)&lt;/code&gt;,&lt;code&gt;fcntl(fd, F_SETLK, &amp;amp;lock)&lt;/code&gt;, and&lt;code&gt;fcntl(fd, F_SETLKW, &amp;amp;lock)&lt;/code&gt;allow cooperating processes to lock regions of a file to serialize their access.&lt;code&gt;lock&lt;/code&gt;is of type&lt;code&gt;struct flock&lt;/code&gt;and describes the type of lock and the region being locked.&lt;code&gt;F_SETLKW&lt;/code&gt;is particularly useful as it blocks the calling process until the lock is acquired. There is a “mandatory locking” mode but Linux’s implementation is unreliable as it’s subject to a race condition.&lt;code&gt;fcntl(2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fcntl(fd, F_GETLEASE)&lt;/code&gt;and&lt;code&gt;fcntl(fd, F_SETLEASE, lease)&lt;/code&gt;ask the kernel to notify the calling process with&lt;code&gt;SIGIO&lt;/code&gt;when another process&lt;code&gt;open&lt;/code&gt;s or&lt;code&gt;truncate&lt;/code&gt;s the file referred to by&lt;code&gt;fd&lt;/code&gt;. When that signals arrives, the lease needs to be removed by&lt;code&gt;fcntl(fd, F_SETLEASE, F_UNLCK)&lt;/code&gt;.&lt;code&gt;fcntl(fd, F_NOTIFY, arg)&lt;/code&gt;is similar but doesn’t block other processes, so it isn’t useful for synchronization.&lt;code&gt;fcntl(2)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mmap(0, length, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0)&lt;/code&gt;returns a pointer from which a file’s contents can be read and written by normal memory operations. By making frequent use of&lt;code&gt;msync(addr, length, MS_INVALIDATE)&lt;/code&gt;, data written in this manner can be shared between processes that both map the same file.&lt;code&gt;mmap(2)&lt;/code&gt;,&lt;code&gt;msync(2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Operating on virtual memory&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;__sync_fetch_and_add&lt;/code&gt;,&lt;code&gt;__sync_add_and_fetch&lt;/code&gt;,&lt;code&gt;__sync_val_compare_and_swap&lt;/code&gt;, and friends provide a full barrier so “no memory operand will be moved across the operation, either forward or backward.” These operations are the basis for most (all?) lock-free algorithms. GCC Atomic Builtins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Something I should add to my repertoire? Race condition? Let me know at r@rcrowley.org or @rcrowley and I’ll fix it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://rcrowley.org/2010/01/06/things-unix-can-do-atomically.html"/><published>2026-02-06T05:29:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46910963</id><title>A new bill in New York would require disclaimers on AI-generated news content</title><updated>2026-02-06T13:53:25.413127+00:00</updated><content>&lt;doc fingerprint="fcc1099258150ba3"&gt;
  &lt;main&gt;&lt;p&gt;A new bill in the New York state legislature would require news organizations to label AI-generated material and mandate that humans review any such content before publication. On Monday, Senator Patricia Fahy (D-Albany) and Assemblymember Nily Rozic (D-NYC) introduced the bill, called The New York Fundamental Artificial Intelligence Requirements in News Act — The NY FAIR News Act for short.&lt;/p&gt;&lt;p&gt;“At the center of the news industry, New York has a strong interest in preserving journalism and protecting the workers who produce it,” said Rozic in a statement announcing the bill.&lt;/p&gt;A closer look at the bill shows a few regulations, mostly centered around AI transparency, both for the public and in the newsroom. For one, the law would demand that news organizations put disclaimers on any published content that is “substantially composed, authored, or created through the use of generative artificial intelligence.”&lt;p&gt;AI disclaimers for readers have been hotly debated in the news industry, with some critics arguing that such labels alienate audiences, even when generative AI is only used as an assistive tool. The bill contains a carve-out that would allow copyrightable material to be excluded from the law. (The U.S. Copyright Office has ruled that works solely generated by AI systems are not eligible for copyright, but allows leeway for works that show signs of “human authorship.”)&lt;/p&gt;&lt;p&gt;The bill also requires that news organizations disclose to journalists and other media professionals in their newsrooms when AI is being used and how. Any news content created using generative AI must also be reviewed by a human employee “with editorial control” before publication. That goes not just for news articles but also for audio, images, and other visuals.&lt;/p&gt;&lt;p&gt;In addition, the bill contains language that requires news organizations to create safeguards that protect confidential material — mainly, information about sources — from being accessed by AI technologies.&lt;/p&gt;&lt;p&gt;State lawmakers highlighted two main reasons for proposing the NY FAIR News Act. First, they say, AI-generated content may be “false or misleading.” Second, they argue, AI-generated content “plagiarizes” by deriving content from original sources “without permission or proper citation.”&lt;/p&gt;&lt;p&gt;“Perhaps one of the industries at most risk from the use of artificial intelligence is journalism and as a result, the public’s trust and confidence in accurate news reporting,” said Sen. Fahy in a statement. “More than 76% of Americans are concerned about AI stealing or reproducing journalism and local news stories.”&lt;/p&gt;&lt;p&gt;The proposed bill was announced with broad endorsements from unions across the news industry, including WGA-East, SAG-AFTRA and the DGA.&lt;/p&gt;&lt;p&gt;Jennifer Sheehan, a spokesperson for the NewsGuild of New York, confirmed that the NewsGuild has been meeting with this labor coalition to discuss shared concerns around AI adoption and working to get the bill off the ground.&lt;/p&gt;Notably, the bill would cement some labor protections for newsroom workers — including restrictions on firing journalists or reducing their work, pay, or benefits due to generative AI adoption. Similar language has been negotiated into individual newsroom union contracts across the country over the past couple of years.&lt;p&gt;In December, the NewsGuild launched a nationwide campaign called “News Not Slop” to advocate for more guardrails on AI usage in newsrooms. In New York City, the Business Insider union held a rally in the Financial District to protest an editorial pilot that was publishing AI-generated news stories with an “AI byline.”&lt;/p&gt;&lt;p&gt;“Our union is deeply concerned about media companies implementing artificial intelligence in ways that damage the credibility of our members’ journalism,” Sheehan said, “as well as the impact such technology has had and will have on jobs.”&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.niemanlab.org/2026/02/a-new-bill-in-new-york-would-require-disclaimers-on-ai-generated-news-content/"/><published>2026-02-06T09:56:55+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46911170</id><title>Invention of DNA "Page Numbers" Opens Up Possibilities for the Bioeconomy</title><updated>2026-02-06T13:53:25.216845+00:00</updated><content/><link href="https://www.caltech.edu/about/news/invention-dna-page-numbers-synthesis-kaihang-wang"/><published>2026-02-06T10:26:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46911869</id><title>TikTok's 'Addictive Design' Found to Be Illegal in Europe</title><updated>2026-02-06T13:53:25.100100+00:00</updated><content/><link href="https://www.nytimes.com/2026/02/06/business/tiktok-addictive-design-europe.html"/><published>2026-02-06T12:11:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46911873</id><title>Show HN: Agent Arena – Test How Manipulation-Proof Your AI Agent Is</title><updated>2026-02-06T13:53:24.515781+00:00</updated><content>&lt;doc fingerprint="61de88532d98d63f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AGENT ARENA&lt;/head&gt;
    &lt;p&gt;How manipulation-proof is your AI agent? Send it to a page full of hidden prompt injection attacks and find out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Point your AI agent at the test page and ask it to summarize the content.&lt;/p&gt;
    &lt;p&gt;Copy your agent's response and paste it into the scorecard below.&lt;/p&gt;
    &lt;p&gt;Instantly see which hidden attacks your agent fell for.&lt;/p&gt;
    &lt;p&gt;Send your agent to: &lt;code&gt;ref.jock.pl/modern-web&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h2"&gt;Scorecard&lt;/head&gt;
    &lt;head rend="h2"&gt;Challenge Catalog&lt;/head&gt;
    &lt;p&gt;10 attack vectors ordered by difficulty. Canary phrases are hidden â only revealed after analysis.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding Prompt Injection&lt;/head&gt;
    &lt;p&gt;Prompt injection is an attack where adversarial instructions are hidden in content that an AI agent processes. When an agent reads a web page, email, or document, hidden instructions can trick it into changing its behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why It Matters&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agents browsing the web are exposed to content they didn't choose&lt;/item&gt;
      &lt;item&gt;Hidden instructions can exfiltrate data, alter outputs, or bypass safety filters&lt;/item&gt;
      &lt;item&gt;Most attacks are invisible to the human supervising the agent&lt;/item&gt;
      &lt;item&gt;Defense requires awareness at both the model and application layer&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Attack Categories&lt;/head&gt;
    &lt;p&gt;White-on-white text, micro text, off-screen content. The text is there, but humans can't see it.&lt;/p&gt;
    &lt;p&gt;HTML comments, hidden divs, data attributes. Uses the structure of HTML itself as camouflage.&lt;/p&gt;
    &lt;p&gt;ARIA attributes, alt text overrides. Exploits accessibility and metadata channels.&lt;/p&gt;
    &lt;p&gt;Zero-width characters, Unicode exploits. The message is invisible at the character level.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://wiz.jock.pl/experiments/agent-arena/"/><published>2026-02-06T12:12:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46911901</id><title>I now assume that all ads on Apple news are scams</title><updated>2026-02-06T13:53:24.200627+00:00</updated><content>&lt;doc fingerprint="7b885158dca214d7"&gt;
  &lt;main&gt;
    &lt;p&gt;In 2024, Apple signed a deal with Taboola to serve ads in its app, notably Apple News. John Gruber, writing in Daring Fireball said at the time:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you told me that the ads in Apple News have been sold by Taboola for the last few years, I’d have said, “Oh, that makes sense.” Because the ads in Apple News — at least the ones I see1 — already look like chumbox Taboola ads. Even worse, they’re incredibly repetitious.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I use Apple News to keep up on topics that I don’t find in sources I pay for (The Guardian and The New York Times). But there’s no way I’m going to pay the exorbitant price Apple wants for Apple News+ – £13 – because, while you get more publications, you still get ads.&lt;/p&gt;
    &lt;p&gt;And those ads have gotten worse recently. Many if not most of them look like and probably are scams. Here are a few examples from Apple News today.&lt;/p&gt;
    &lt;p&gt;Here are three ads that are scammy; the first two were clearly generated by AI, and the third may have been created by AI.&lt;/p&gt;
    &lt;p&gt;Why are they scams? When I searched domain information for the domains, I found that they were registered very recently.&lt;/p&gt;
    &lt;p&gt;Domain Name: MUSTYLEVO.COM&lt;lb/&gt; Registry Domain ID: 3059688301_DOMAIN_COM-VRSN&lt;lb/&gt; Registrar WHOIS Server: whois.gname.com&lt;lb/&gt; Registrar URL: http://www.gname.com&lt;lb/&gt; Updated Date: 2026-02-04T07:23:58Z&lt;lb/&gt; Creation Date: 2026-01-21T07:23:43Z&lt;/p&gt;
    &lt;p&gt;Domain Name: SOLVERACO.COM&lt;lb/&gt; Registry Domain ID: 3045027870_DOMAIN_COM-VRSN&lt;lb/&gt; Registrar WHOIS Server: grs-whois.hichina.com&lt;lb/&gt; Registrar URL: http://wanwang.aliyun.com&lt;lb/&gt; Updated Date: 2025-12-05T06:10:51Z&lt;lb/&gt; Creation Date: 2025-12-05T06:07:40Z&lt;/p&gt;
    &lt;p&gt;Domain Name: SHIYAATELIER.COM&lt;lb/&gt; Registry Domain ID: 3037972202_DOMAIN_COM-VRSN&lt;lb/&gt; Registrar WHOIS Server: whois.name.com&lt;lb/&gt; Registrar URL: http://www.name.com&lt;lb/&gt; Updated Date: 2025-11-12T06:47:14Z&lt;lb/&gt; Creation Date: 2025-11-12T06:47:13Z&lt;/p&gt;
    &lt;p&gt;This recent registration doesn’t necessarily mean they are scams, but they don’t inspire much confidence.&lt;/p&gt;
    &lt;p&gt;Here’s one example. This ad from Tidenox, whose website says I am retiring, showing a photo of an elderly woman, who says, “For 26 years, Tidenox has been port of your journey in creating earth and comfort at home.” The image of the retiring owner is probably made by AI. (Update: someone on Hacker News pointed out the partly masked Google Gemini logo on the bottom right. I hadn’t spotted that, in part because I don’t use any AI image generation tools.)&lt;/p&gt;
    &lt;p&gt;These fake “going out of business ads” have been around for a few years, and even the US Better Business Bureau warns about them, as they take peoples’ money then shut down. Does Apple care? Does Taboola care? Does Apple care that Taboola serves ads like this? My guess: no, no, and no.&lt;/p&gt;
    &lt;p&gt;Note the registration date for the tidenox.com domain. It’s nowhere near 26 years old, and it’s registered in China:&lt;/p&gt;
    &lt;p&gt;Domain Name: TIDENOX.COM&lt;lb/&gt; Registry Domain ID: 2987356919_DOMAIN_COM-VRSN&lt;lb/&gt; Registrar WHOIS Server: grs-whois.hichina.com&lt;lb/&gt; Registrar URL: http://wanwang.aliyun.com&lt;lb/&gt; Updated Date: 2025-05-29T09:17:31Z&lt;lb/&gt; Creation Date: 2025-05-29T09:14:35Z&lt;/p&gt;
    &lt;p&gt;Shame on Apple for creating a honeypot for scam ads in what they consider to be a premium news service. This company cannot be trusted with ads in its products any more.&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from Kirkville&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://kirkville.com/i-now-assume-that-all-ads-on-apple-news-are-scams/"/><published>2026-02-06T12:16:43+00:00</published></entry></feed>