<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-11-04T23:34:36.509558+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45806348</id><title>When stick figures fought</title><updated>2025-11-04T23:34:43.993800+00:00</updated><content/><link href="https://animationobsessive.substack.com/p/when-stick-figures-fought"/><published>2025-11-04T00:48:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45806903</id><title>My Truck Desk</title><updated>2025-11-04T23:34:43.734431+00:00</updated><content>&lt;doc fingerprint="6f48b816f30396dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Photograph courtesy of Bud Smith.&lt;/p&gt;
    &lt;p&gt;After eight glorious weeks of freedom, I got rehired.&lt;/p&gt;
    &lt;p&gt;First thing I did was walk over to the machine shop to look for my F-150. The oil stain was there but the truck wasn’t. It wasn’t in the rock lot where the bulldozers parked either.&lt;/p&gt;
    &lt;p&gt;Who would have stooped so low as to co-opt that piece of shit? It had no heat and no air-conditioning. The radio bubbled static. Door handles were missing. Floorboards, fenders, and frame all rusted and rotted. It certainly hadn’t been what could be called roadworthy. And, my God, the smell.&lt;/p&gt;
    &lt;p&gt;I went into the machine shop. One of the welders lifted his hood and told me the bad news—they’d had to move the truck for a rebar delivery and the engine on that old thing finally blew, so the truck got dragged to the scrapyard.&lt;/p&gt;
    &lt;p&gt;In a dusty corner, I saw a pile of salvaged tools from the truck. I took some wrenches and my tape measure but didn’t see what I was really looking for—my Truck Desk®. Oh well.&lt;/p&gt;
    &lt;p&gt;I caught a ride out to the unit with the foreman and the rest of the crew. Our goal for the day was to unbolt components from a heat exchanger and fly them off with a crane. Once the exchanger was apart and inspected, we’d begin our real repairs.&lt;/p&gt;
    &lt;p&gt;The morning went well. The mornings always go well. Everybody knows what they’re doing. We’re professionals, equals. Same pay. Same benefits. All working together toward retirement. We look out for each other. Whoever has the hardest task in this crew today could be the foreman tomorrow, and vice versa. Nobody wants to be the boss, so our bosses are the best kind.&lt;/p&gt;
    &lt;p&gt;At first break we packed into our truck and drove shoulder-to-shoulder back to the trailer compound for coffee. During the five-minute drive, I couldn’t help but think how good I’d had it when I had the luxury of using that piece of shit F-150.&lt;/p&gt;
    &lt;p&gt;See, the truck nobody else wanted had been my office. I’d built a portable desk inside it. My truck desk, I called it. A couple of planks screwed together, our union sticker slapped on, the whole deal sealed with shellac. I’d built the desk so it slid into the bottom of the steering wheel and sat across the armrests. I used to hang back at the job and sneak in some creative work while the rest of the crew went to break. My desk—which I’d taken far too long to build and perfect through many prototypes—had been stowed behind the driver’s seat when the truck was hauled off by the wrecker.&lt;/p&gt;
    &lt;p&gt;Back at the break trailer, I took my old seat and joined in on the jokes, insults, tall tales. That trailer was, to me, the best place for storytelling in the world—but, as always, it was too loud, too raucous, too fun to do any writing or reading, which is all I ever want to do on break. At lunch, I retreated into the relative quiet of the machine shop. I sat down by the drill press and took out my cell phone and started writing. Just like I used to do.&lt;/p&gt;
    &lt;p&gt;For nearly two decades I’ve worked off and on at this petrochemical plant as a mechanic and welder. The union dispatched me here: When it gets slow, I get laid off; when work picks up, I boomerang back. And the whole time, I’ve written stories and parts of my novels during breaks—fifteen minutes for coffee and then half an hour for lunch. I’ve also made use of the heaven-sent delays brought on by lightning, severe rainstorms, evacuations, permitting problems, equipment issues, and so on. I’m thankful for each and every delay that happens on this construction site, and, believe me, there are many.&lt;/p&gt;
    &lt;p&gt;Most artists I know are like this. Finding time to make art while working another job, or taking care of loved ones. They improvise. They get better. They get worse. They get better again.&lt;/p&gt;
    &lt;p&gt;Really it mostly comes down to that first thing: finding time. When I talk to people who want to find more time, I repeat something an old-timer said to me early on: “You’ve gotta make your own conditions.”&lt;/p&gt;
    &lt;p&gt;What does that mean? Well. Is it raining? You can either stand out in the rain and get wet, or you can find a coil of tie-wire and hang up tarps for a hooch.&lt;/p&gt;
    &lt;p&gt;There’s another expression I like, which goes: “Let your wallet be your guide.” I try to remember that every time I feel the urge to quit my job and never return.&lt;/p&gt;
    &lt;p&gt;So ever since cell phones got smart, I’ve sat somewhere quiet, semi-on-the-clock, texting myself poems, paragraphs that became stories and novels, and things about my life, or I should say just life, like this thing you’re reading right now.&lt;/p&gt;
    &lt;p&gt;Writing on my cell phone, pecking away, was good enough for many years, but then after a rightfully humbling decade of manual labor, I started having irrational fantasies about convenience and comfort.&lt;/p&gt;
    &lt;p&gt;Of course I have a desk in my apartment, but I couldn’t help myself. Somehow I’d gotten seduced by the prospect of attaining my very own cubicle amid this massive junkyard full of toxic waste.&lt;/p&gt;
    &lt;p&gt;One day I walked into the payroll trailer where the secretaries and site manager sat. There wasn’t an explicit sign that said NO CONTRACTORS ALLOWED, but it was an unspoken rule. The trailer had a few unused old cubicles tucked to the side. I sat down in one and happily pecked away with my thumbs. Every break for a week I went in and worked on my writing. After a few days I started to feel like I should hang pictures of my mom and dad and my wife inside it. But I didn’t dare.&lt;/p&gt;
    &lt;p&gt;Then things really heated up. I brought in a Bluetooth keyboard and wrote a whole story that day on my breaks. There was no going back. My heart soared. I thought I should adopt a brown dog with a bandanna around his neck just so I could thumbtack his picture to the cubicle wall. I hadn’t interacted with any of the office staff, but they’d seen me. They’d followed my oily bootprints down the hallway and begun to leer. Who is this diesel-stinking contractor? He’s probably the one who’s been eating Janelle’s Oreos. He raided the mango-kiwi yogurt from the fridge. He glommed all the sporks. I knew my cubicle dreams were over the morning I found the site manager waiting in “my” cubicle.&lt;/p&gt;
    &lt;p&gt;“What are you doing here?” he asked.&lt;/p&gt;
    &lt;p&gt;In all my years working at that place, I’d never seen the site manager out on the site. I’m not sure he knew what it was or where it was. You went to him to order tools; he was the one who said no. I’d only ever seen him at a urinal or buying bacon and eggs off the lunch truck. But if I had ever seen him out on the site, it would have never occurred to me to ask him what he was doing there. He was wearing a blue polo shirt and khakis, and I was in his world—and he was asking.&lt;/p&gt;
    &lt;p&gt;“Office work,” I said.&lt;/p&gt;
    &lt;p&gt;“What kind, exactly?”&lt;/p&gt;
    &lt;p&gt;How can you explain literary fiction to a site manager?&lt;/p&gt;
    &lt;p&gt;“Little bit of everything,” I said.&lt;/p&gt;
    &lt;p&gt;I started writing in the machine shop again. It wasn’t the same. Once I’d been infected by the cubicle virus, there was no going back. Out of scrap lumber I gathered from various dumpsters, I built a proper desk for myself in the northeast corner of the shop. That desk was a huge leap forward in possibility and productivity. In the evenings, if I wrote something by hand or on my typewriter at home, I could now use my time at work to retype it at my shop desk.&lt;/p&gt;
    &lt;p&gt;The shop desk was not ideal. Some days I arrived to find someone had disassembled a small motor on top of it, gaskets and hardware spread out on newspaper. Other times I found pneumatic guns taken apart, or electrical devices with wiring splayed in a colorful tangle, or—fair enough—important blueprints laid out the entire length of the desk.&lt;/p&gt;
    &lt;p&gt;Right around this time I first saw the F-150. One of the workers had abandoned it by the shop. I put a battery in. That lasted one shift. Then I took an alternator out of another junk truck and, lo and behold, I had my own four wheels. The fan belt screamed. The engine smoked. The brakes worked when they wanted to. It was mine that whole dangerous year.&lt;/p&gt;
    &lt;p&gt;Then, one day, my luck changed.&lt;/p&gt;
    &lt;p&gt;A crate full of chain falls got delivered. It was a glorious crate, made of sanded spruce. I unscrewed some of the planking and built my first Truck Desk prototype.&lt;/p&gt;
    &lt;p&gt;It was made of three boards cut at twenty-four inches. Light and compact. Sealed with shellac. It slid into the bottom of the steering wheel, one side supported by a curved rebar I welded into a nut that fit exactly in a recess on the driver’s door. The center console supported the other side of the desk. I kept it stored behind the seat. Whenever break time came and the crew drove back to the trailer compound, I stayed parked on the unit and got at least ten extra minutes to write.&lt;/p&gt;
    &lt;p&gt;Now that I had my Truck Desk, that vehicle was my very own rolling cubicle.&lt;/p&gt;
    &lt;p&gt;Having that truck reminded me of when I lived on 173rd Street in New York City. Back then I used to drive around endlessly looking for street parking. I would see men and women sitting in their cars. They weren’t leaving, though; they were reading a book or a magazine, smoking cigarettes, playing Sudoku, scribbling love letters. They were the wisest men and women in the entire city, using their vehicles as a kind of office down on the street, a sanctuary where they could do their real work.&lt;/p&gt;
    &lt;p&gt;After the F-150 was scrapped, I never got a replacement truck. I never found that first Truck Desk either, even when I called the scrapyard.&lt;/p&gt;
    &lt;p&gt;What I did do, though, was go over to the carpenter’s side of the shop and cut a scaffold plank at twenty-nine inches. This simple plank fits across the armrests of whatever Chevy or Ford pickup the crew has that day. This dramatic redesign of Truck Desk into Truck Plank® took all of ten seconds. I didn’t bother with the sticker or shellac.&lt;/p&gt;
    &lt;p&gt;The years on the job have rolled on. Now editors send me Word documents with comments and questions and tracked changes. I bring my backpack to work with my laptop inside.&lt;/p&gt;
    &lt;p&gt;Every morning, when I find out what crew I’m in, I bring that plank with me. I stick it on the dashboard and climb into the driver’s seat. I drive us all out to the job and at break time I take them to the trailer. I clean my hands with pumice wipes and sit alone in whoever’s truck it is that day, pulling the plank off the dashboard and setting it across the armrests. Within a minute or so, I’ve got the laptop out and I’m working. If somebody from the crew is still in the back seat, bandanna over their eyes, snoozing, I do my best to keep extra quiet. And if they begin to snore, I don’t let that bother me at all.&lt;/p&gt;
    &lt;p&gt;Bud Smith is the author of the novel Teenager and the story collection Double Bird, among other books. Mighty, a novel, is forthcoming from Knopf in spring 2027. His story “Skyhawks” appears in the new Fall issue of The Paris Review.&lt;/p&gt;
    &lt;p&gt;Last / Next Article&lt;/p&gt;
    &lt;p&gt;Share&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.theparisreview.org/blog/2025/10/29/truck-desk/"/><published>2025-11-04T02:37:01+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45807775</id><title>Tell HN: X is opening any tweet link in a webview whether you press it or not</title><updated>2025-11-04T23:34:43.070905+00:00</updated><content>&lt;doc fingerprint="d49d99132ed61bc5"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Just saw the CEO of Substack celebrating traffic from X/Twitter shooting up thinking they stopped suppressing tweets with links[0]. Actually, this traffic is because now any time you open a tweet with a link, the in-app webview loads in the background, and displays when you press the link.&lt;/p&gt;
      &lt;p&gt;I run an ecom store that gets a lot of its customers from Twitter. I was also shocked to see my traffic double or triple overnight and thought the algorithm had blessed me and my business. Soon realized what was actually happening. Thought other traffic-monitors might appreciate this explanation.&lt;/p&gt;
      &lt;p&gt;Meanwhile Nikita Bier is pretending they never suppressed tweets with links to begin with, offering the alternative explanation: "a common complaint is that posts with links tend to get lower reach. This is because the web browser covers the post and people forget to Like or Reply. So X doesn't get a clear signal whether the content is any good"[1]. A bit of a rewriting of history since Elon and his mom both tweeted about how it wasn't fair to use his platform to promote other links/platforms, even banning people who shared profiles of other social networks (including Paul Graham for a period). They suppressed all links shortly after.&lt;/p&gt;
      &lt;p&gt;[0] https://x.com/cjgbest/status/1985464687350485092&lt;/p&gt;
      &lt;p&gt;[1] https://x.com/nikitabier/status/1979994223224209709&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://news.ycombinator.com/item?id=45807775"/><published>2025-11-04T05:53:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45808998</id><title>Bloom filters are good for search that does not scale</title><updated>2025-11-04T23:34:42.849832+00:00</updated><content>&lt;doc fingerprint="7b8d83e54decc003"&gt;
  &lt;main&gt;
    &lt;p&gt;A great blog post from 2013 describes using bloom filters to build a space-efficient full text search index for small numbers of documents. The algorithm is simple: Per document, create a bloom filter of all its words. To query, simply check each document's bloom filter for the query terms.&lt;/p&gt;
    &lt;p&gt;With a query time complexity of O(number-of-documents), we can forget about using this on big corpuses, right? In this blog post I propose a way of scaling the technique to large document corpuses (e.g. the web) and discuss why that is a bad idea.&lt;/p&gt;
    &lt;p&gt;Fun fact: There is a nice implementation of this exact algorithm that is still used in the wild. But let's get into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why even try this?&lt;/head&gt;
    &lt;p&gt;The bloom filter index's big selling point is its small size. It allows static websites with dozens of pages to ship a full text search index to the client that is as small as a small image. An equivalent inverted index, which is the traditional textbook approach for keyword-based full text search, would be multiple times bigger.&lt;/p&gt;
    &lt;p&gt;But index size is not only relevant on small blog websites. If we could scale this technique to larger document corpuses and achive similar space savings, that would be huge!&lt;/p&gt;
    &lt;p&gt;The main thing that seems to stand in our way is query performance. Instead of always checking every document's bloom filter, we will try to construct an index that only checks a small subset of filters, but still finds all matching documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Ideas that don't work at all&lt;/head&gt;
    &lt;p&gt;Look, I brainstormed a bunch of ideas for how to improve the bloom filter based index. I will quickly go over two of them, because identifying and discarding ideas that will not work is an important part of science and engineering.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sort the filters&lt;/head&gt;
    &lt;p&gt;If we sort the filters by some metric, for example by the most to least significant bits, then we can use a binary search algorithm or something like that, right? - Wrong.&lt;/p&gt;
    &lt;p&gt;We can construct a simple counter example to show that this does not work. Here the query is matched by the first and the last filter in a sorted list of filters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tree of filters&lt;/head&gt;
    &lt;p&gt;Plain sorting does not work, but what if we structure our big set of filters into a tree? Imagine it sort of like this, but much bigger.&lt;/p&gt;
    &lt;p&gt;At each branch node, we construct an aggregate filter that encodes all documents that are reachable from the branch. Aggregate filters are constructed by a simple bitwise or of the other filters like this.&lt;/p&gt;
    &lt;p&gt;When we get a query, we first check it against the top level branch filters. If a filter does not match e.g. the filter for document 6-10, we can discard that entire branch of the tree for this query.&lt;/p&gt;
    &lt;p&gt;Ideally we would like to search as few branches of the tree as possible to improve performance. How many branches we do need to search, depends heavily on the partitioning of the documents. Intuitively, we can think of it like this: branch A should contain all documents that contain the words "dog", "cat", "bird". Branch B should contain documents with "car", "bus", "plane". The fewer branches each word is contained in the better.&lt;/p&gt;
    &lt;p&gt;Here comes the problem: What if there is a document that says "I took my cat on the bus today"?&lt;lb/&gt; This breaks our assumtion above, and suddenly for the query "bus" we need to search both branches. You can imagine this happening for almost every word in the dicionary across all branches, because language is complex and lets us say so many different things in many different contexts.&lt;/p&gt;
    &lt;p&gt;Or in other words: Text documents are high-dimensional.&lt;lb/&gt; I recommend reading about the curse of dimensionality for an intuition of what issues this implies. Here it means that it is basically impossible to cluster text documents into disjunct subsets without significant overlap. For our seach index that means that even when using this tree, we would still need to search almost every document for every query.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inverted Index of Bloom Filters&lt;/head&gt;
    &lt;p&gt;The problem with our previous tree-based idea is that there is so much overlap between text documents. But I know one book that only contains every word exactly once: The dictionary. We can construct a search tree of the entire dictionary, again based on bloom filters. Each leaf represents a set of words. At each leaf we keep a list of pointers to every document's filter that contains one of those words.&lt;/p&gt;
    &lt;p&gt;Maybe not so incidentally, this looks a lot like an inverted index. And it works! For any query term we can walk the tree to the leaf that contains the query term and then we match only against the filters at that leaf. Instead of a hash table, as in the inverted index, our index uses a tree for the dictionary, but fundamentally it does a similar thing.&lt;/p&gt;
    &lt;p&gt;The big difference is that the tree can be smaller than the hash table. Remember, size is the main reason to attempt this at all. Not only is there no empty space in a tree, but we also encode all the words in our bloom filters instead of storing them outright. Modern bloom filters (actually called Xor filters) require about ten bits per element [1], much less than the 8 bits per character required to store a full word.&lt;/p&gt;
    &lt;p&gt;As an aside, bloom filters are indeed already used in full text search for large-ish datasets, but in the form of skip-indexes. In a skip index, a bloom filter is used to quickly check if a large chunk of data contains a value (e.g. a word) at all. That way, a database can avoid reading chunks of data that do not contain any records for a given query. Until very recently this technique was used by the Clickhouse OLAP system for full text search [2]. It has been superseeded by a proper inverted index in 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why all of this is still a bad idea&lt;/head&gt;
    &lt;p&gt;We did it! We have a working idea for a bloom filter based search index that works for large document corpuses. The query time complexity is not as good as for an inverted index, but it is logarithmic with the number of documents. That is good enough if you ask me. So why do I write that it is still a bad idea?&lt;/p&gt;
    &lt;p&gt;Let's think about what allows the bloom filter based index to be small again. Instead of storing the entire dictionary in our index, we use bloom filters that require about ten bits per word. Ten bits per word. Ten bits per every word. Not unique word. Every word in our document corpus (except duplicates in the same document). To make our math exceedingly simple, let's say the english dictionary has about 500 Thousand unique words and every document contains 1000 distinct words. At ten bits per entry for a bloom filter, that makes each document's filter about 1.25kb. Assume that words are on average ten characters long, then the dictionary will require 5mb for the text alone. We can assume another 4mb for the inverted index's hash table to get a lower bound of 9mb for the inverted index. Both indexes require similar amounts of space for document ids and pointers, so relative to the inverted index, our bloom filter index grows by 1.25kb per document. Divide 9mb by 1.25kb and you find out that at only 7200 documents the inverted index becomes more space efficient than the bloom filter index. Of course the real numbers will be different and we are ignoring some things here, but the trend will stay the same.&lt;/p&gt;
    &lt;p&gt;What is going on here is that while an inverted index must store every word in the dictionary exactly once, sharing the space when a word is reused, bloom filters do not share space amongst each other. Every document's bloom filter must encode all words in the document from scratch. If a word is contained in thousands of documents, that requires much more space than simply storing the word in plain text.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you have a small number of documents relative to the size of your dictionary, bloom filters can indeed achieve a much smaller full text search index than is possible traditionally.&lt;/p&gt;
    &lt;p&gt;Bloom filters are space efficient when compressing a large dictionary into a small number of filters. As more filters share the same dictionary, this efficiency decreases. Intuitively this is because bloom filters cannot share information amongst each other. Each filter must encode its entire dictionary from scratch. An inverted index does not do this. It only stores the dictionary once and shares it for all documents, so it gets more space efficient with the number of documents.&lt;/p&gt;
    &lt;p&gt;More generally, there is no synergy between bloom filters. Each filter on its own is efficient, but as a whole system, a different approach might be more efficient. We can transfer this insight to other problem domains as well. For example, imagine a content moderation system on a social media platform that allows blocking individual accounts. If we have one global blocklist on our platform, a bloom filter can be an efficient (though maybe not ideal) implementation of this. But allow every user to create their own blocklist and a different design will be more scaleable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://notpeerreviewed.com/blog/bloom-filters/"/><published>2025-11-04T09:25:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45809193</id><title>What is a manifold?</title><updated>2025-11-04T23:34:42.544833+00:00</updated><content>&lt;doc fingerprint="ba91984004c1afe9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Is a Manifold?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Standing in the middle of a field, we can easily forget that we live on a round planet. We’re so small in comparison to the Earth that from our point of view, it looks flat.&lt;/p&gt;
    &lt;p&gt;The world is full of such shapes — ones that look flat to an ant living on them, even though they might have a more complicated global structure. Mathematicians call these shapes manifolds. Introduced by Bernhard Riemann in the mid-19th century, manifolds transformed how mathematicians think about space. It was no longer just a physical setting for other mathematical objects, but rather an abstract, well-defined object worth studying in its own right.&lt;/p&gt;
    &lt;p&gt;This new perspective allowed mathematicians to rigorously explore higher-dimensional spaces — leading to the birth of modern topology, a field dedicated to the study of mathematical spaces like manifolds. Manifolds have also come to occupy a central role in fields such as geometry, dynamical systems, data analysis and physics.&lt;/p&gt;
    &lt;p&gt;Today, they give mathematicians a common vocabulary for solving all sorts of problems. They’re as fundamental to mathematics as the alphabet is to language. “If I know Cyrillic, do I know Russian?” said Fabrizio Bianchi, a mathematician at the University of Pisa in Italy. “No. But try to learn Russian without learning Cyrillic.”&lt;/p&gt;
    &lt;p&gt;So what are manifolds, and what kind of vocabulary do they provide?&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideas Taking Shape&lt;/head&gt;
    &lt;p&gt;For millennia, geometry meant the study of objects in Euclidean space, the flat space we see around us. “Until the 1800s, ‘space’ meant ‘physical space,’” said José Ferreirós, a philosopher of science at the University of Seville in Spain — the analogue of a line in one dimension, or a flat plane in two dimensions.&lt;/p&gt;
    &lt;p&gt;In Euclidean space, things behave as expected: The shortest distance between any two points is a straight line. A triangle’s angles add up to 180 degrees. The tools of calculus are reliable and well defined.&lt;/p&gt;
    &lt;p&gt;But by the early 19th century, some mathematicians had started exploring other kinds of geometric spaces — ones that aren’t flat but rather curved like a sphere or saddle. In these spaces, parallel lines might eventually intersect. A triangle’s angles might add up to more or less than 180 degrees. And doing calculus can become a lot less straightforward.&lt;/p&gt;
    &lt;p&gt;The mathematical community struggled to accept (or even understand) this shift in geometric thinking.&lt;/p&gt;
    &lt;p&gt;But some mathematicians wanted to push these ideas even further. One of them was Bernhard Riemann, a shy young man who had originally planned to study theology — his father was a pastor — before being drawn to mathematics. In 1849, he decided to pursue his doctorate under the tutelage of Carl Friedrich Gauss, who had been studying the intrinsic properties of curves and surfaces, independent of the space surrounding them.&lt;/p&gt;
    &lt;p&gt;In 1854, Riemann was required to deliver a lecture to secure a teaching position at the University of Göttingen. His assigned topic: the foundations of geometry. On June 10, despite a fear of public speaking, he described a new theory in which he generalized Gauss’ ideas about the geometry of surfaces to an arbitrary number of dimensions (and even to infinite dimensions).&lt;/p&gt;
    &lt;p&gt;Gauss was immediately impressed with the lecture, which involved not just math but also philosophy and physics. But most mathematicians found Riemann’s ideas too vague and abstract to be of much use. “Many scientists and philosophers were saying, ‘This is nonsense,’” Ferreirós said. And so, for decades, the work was largely ignored. Riemann’s lecture didn’t appear in print until 1868, two years after his death.&lt;/p&gt;
    &lt;p&gt;But by the end of the 19th century, mathematical greats like Henri Poincaré had recognized the importance of Riemann’s ideas. And in 1915, Albert Einstein used them in his general theory of relativity, bringing them out of the realm of philosophical abstraction and into the real world. By the middle of the 20th century, they had become a mathematical staple.&lt;/p&gt;
    &lt;p&gt;Riemann had introduced a concept that could encompass all possible geometries, in any number of dimensions. A concept that would change how mathematicians view space.&lt;/p&gt;
    &lt;p&gt;A manifold.&lt;/p&gt;
    &lt;head rend="h2"&gt;Charted Territory&lt;/head&gt;
    &lt;p&gt;The term “manifold” comes from Riemann’s Mannigfaltigkeit, which is German for “variety” or “multiplicity.”&lt;/p&gt;
    &lt;p&gt;A manifold is a space that looks Euclidean when you zoom in on any one of its points. For instance, a circle is a one-dimensional manifold. Zoom in anywhere on it, and it will look like a straight line. An ant living on the circle will never know that it’s actually round. But zoom in on a figure eight, right at the point where it crosses itself, and it will never look like a straight line. The ant will realize at that intersection point that it’s not in a Euclidean space. A figure eight is therefore not a manifold.&lt;/p&gt;
    &lt;p&gt;Similarly, in two dimensions, the surface of the Earth is a manifold; zoom in far enough anywhere on it, and it’ll look like a flat 2D plane. But the surface of a double cone — a shape consisting of two cones connected at their tips — is not a manifold.&lt;/p&gt;
    &lt;p&gt;Manifolds address a problem that mathematicians would otherwise have to deal with: A shape’s properties can change depending on the nature and dimension of the space it lives in (and how it sits in that space). For instance, lay a piece of string on a table, and connect its ends without lifting it. You’ll get a simple loop. Now hold the string in the air and tie its ends together. By considering the string in three dimensions, you can pass it over and under itself before you connect the ends, creating all sorts of knots beyond the simple loop. They all represent the same one-dimensional manifold — the looped string — but they have different properties when considered in two versus three dimensions.&lt;/p&gt;
    &lt;p&gt;Mathematicians avoid such ambiguities by focusing on the manifold’s intrinsic properties. The defining property of manifolds — that at any point, they look Euclidean — is immensely helpful on that front. Because it’s possible to think about any small patch of the manifold in terms of Euclidean space, mathematicians can use traditional calculus techniques to, say, compute its area or volume, or describe movement on it.&lt;/p&gt;
    &lt;p&gt;To do this, mathematicians divide a given manifold into several overlapping patches and represent each with a “chart” — a set of some number of coordinates (equal to the manifold’s dimension) that tell you where you are on the manifold. Crucially, you also need to write down rules that describe how the coordinates of overlapping charts relate to one another. The collection of all these charts is called an atlas.&lt;/p&gt;
    &lt;p&gt;You can then use this atlas — whose charts translate smaller regions of your potentially complicated manifold into familiar Euclidean space — to measure and explore the manifold one patch at a time. If you want to understand how a function behaves on a manifold, or get a sense of its global structure, you can break the problem up into pieces, solve each piece on a different chart, in Euclidean space, and then stitch together the results from all the charts in the atlas to get the full answer you’re seeking.&lt;/p&gt;
    &lt;p&gt;Today, this approach is ubiquitous throughout math and physics.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manifold Uses&lt;/head&gt;
    &lt;p&gt;Manifolds are crucial to our understanding of the universe, for one. In his general theory of relativity, Einstein described space-time as a four-dimensional manifold, and gravity as that manifold’s curvature. And the three-dimensional space we see around us is also a manifold — one that, as manifolds do, appears Euclidean to those of us living within it, even though we’re still trying to figure out its global shape.&lt;/p&gt;
    &lt;p&gt;Even in cases where manifolds don’t seem to be present, mathematicians and physicists try to rewrite their problems in the language of manifolds to make use of their helpful properties. “So much of physics comes down to understanding geometry,” said Jonathan Sorce, a theoretical physicist at Princeton University. “And often in surprising ways.”&lt;/p&gt;
    &lt;p&gt;Consider a double pendulum, which consists of one pendulum hanging from the end of another. Small changes in the double pendulum’s initial conditions lead it to carve out very different trajectories through space, making its behavior hard to predict and understand. But if you represent the configuration of the pendulum with just two angles (one describing the position of each of its arms), then the space of all possible configurations looks like a doughnut, or torus — a manifold. Each point on this torus represents one possible state of the pendulum; paths on the torus represent the trajectories the pendulum might follow through space. This allows researchers to translate their physical questions about the pendulum into geometric ones, making them more intuitive and easier to solve. This is also how they study the movements of fluids, robots, quantum particles and more.&lt;/p&gt;
    &lt;p&gt;Similarly, mathematicians often view the solutions to complicated algebraic equations as a manifold to better understand their properties. And they analyze high-dimensional datasets — such as those recording the activity of thousands of neurons in the brain — by looking at how those data points might sit on a lower-dimensional manifold.&lt;/p&gt;
    &lt;p&gt;Asking how scientists use manifolds is akin to asking how they use numbers, Sorce said. “They are at the foundation of everything.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/what-is-a-manifold-20251103/"/><published>2025-11-04T09:58:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45810430</id><title>Chaining FFmpeg with a Browser Agent</title><updated>2025-11-04T23:34:41.962039+00:00</updated><link href="https://100x.bot/a/chaining-ffmpeg-with-browser-agent"/><published>2025-11-04T12:52:54+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45811093</id><title>Show HN: A CSS-Only Terrain Generator</title><updated>2025-11-04T23:34:41.587064+00:00</updated><content>&lt;doc fingerprint="109996e321a537ae"&gt;
  &lt;main&gt;
    &lt;p&gt;CSS Terrain Generator Regenerate Restart Undo Redo Import Export Heightmap CSS VOX TXT PNG Copy Embed Open Codepen Download Code move raise lower about world size ✕ ✕ landmass coverage small medium large terrain type pampas hilly alpinist biome temperate arctic desert camera settings rotate x 45° tilt y 60° zoom 50% pan x 0px lift y 0px animate reset to defaults minimap heightmap matrix v0.0.1 Regenerate&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://terra.layoutit.com"/><published>2025-11-04T13:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45811447</id><title>Optimizing Datalog for the GPU</title><updated>2025-11-04T23:34:41.503470+00:00</updated><content/><link href="https://danglingpointers.substack.com/p/optimizing-datalog-for-the-gpu"/><published>2025-11-04T14:31:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812000</id><title>How devtools map minified JS code back to your TypeScript source code</title><updated>2025-11-04T23:34:41.095028+00:00</updated><content>&lt;doc fingerprint="2e607229161f0532"&gt;
  &lt;main&gt;
    &lt;p&gt;Source maps are the main piece in the jigsaw puzzle of mapping symbols and locations from "built" JavaScript files back to the original source code. When you debug minified JavaScript in your browser's DevTools and see the original source with proper variable names and formatting, you're witnessing source maps in action.&lt;/p&gt;
    &lt;p&gt;For example, when your browser encounters an error at &lt;code&gt;bundle.min.js:1:27698&lt;/code&gt;, the source map translates this to &lt;code&gt;src/index.ts:73:16&lt;/code&gt;, revealing exactly where the issue occurred in your original TypeScript code:&lt;/p&gt;
    &lt;p&gt;But how does this actually work under the hood? In this post, we'll take a deep dive into the internals of source maps, exploring their format, encoding mechanisms, and how devtools use them to bridge the gap between production code and developer-friendly sources.&lt;/p&gt;
    &lt;head rend="h2"&gt;The TypeScript Build Pipeline&lt;/head&gt;
    &lt;p&gt;Modern JavaScript builds typically involve three main stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transpilation: TypeScript → JavaScript&lt;/item&gt;
      &lt;item&gt;Bundling: Combining modules into a single file&lt;/item&gt;
      &lt;item&gt;Minification: Compressing code for production&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At each stage, source maps preserve the connection back to the original code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stage 0: Source TS files&lt;/head&gt;
    &lt;p&gt;The original TypeScript source files with full type annotations.&lt;/p&gt;
    &lt;head rend="h4"&gt;Source Files&lt;/head&gt;
    &lt;quote&gt;1export function add(a: number, b: number): number {2 return a + b;3}&lt;/quote&gt;
    &lt;quote&gt;1import { add } from './add';23export function computeFibonacci(n: number): number {4 if (n &amp;lt;= 1) return n;5 return add(computeFibonacci(n - 1), computeFibonacci(n - 2));6}&lt;/quote&gt;
    &lt;quote&gt;1import { computeFibonacci } from './fibonacci';23const result = computeFibonacci(10);4console.log(`Fibonacci(10) = ${result}`);&lt;/quote&gt;
    &lt;p&gt;No source map at this stage&lt;/p&gt;
    &lt;head rend="h2"&gt;The Source Map File Format&lt;/head&gt;
    &lt;p&gt;Source maps use JSON format, typically with a &lt;code&gt;.js.map&lt;/code&gt; extension. Let's examine a source map structure from our &lt;code&gt;add.js.map&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;{
  "version": 3,
  "file": "add.js",
  "sourceRoot": "",
  "sources": ["add.ts"],
  "names": ["add", "a", "b"],
  "mappings": "AAAA,OAAO,SAAS,IAAI,CAAC,EAAE;EACrB,OAAO,IAAI;AACb"
}
&lt;/code&gt;
    &lt;head rend="h5"&gt;Fields Breakdown:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;version&lt;/code&gt;: Indicates the source map version (currently always&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;file&lt;/code&gt;: The generated file name this source map corresponds to.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sourceRoot&lt;/code&gt;: Optional prefix for all source URLs. Useful when sources are hosted elsewhere.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sources&lt;/code&gt;: Array of original source file paths from which the generated file was built.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sourcesContent&lt;/code&gt;: Optional array containing the actual source code. This allows DevTools to display sources even if the original files aren't accessible. Usually disabled in production builds.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;names&lt;/code&gt;: Array of original identifiers (variable names, function names, etc.) that appear in the source. Referenced by the mappings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;mappings&lt;/code&gt;: The compressed mapping data. This is the heart of the source map and uses VLQ encoding. More on this below.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Understanding the Mappings: VLQ Encoding&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;mappings&lt;/code&gt; field is where the real magic happens. It contains the actual position mappings between every token in the generated JavaScript file and its corresponding location in the original source files.&lt;/p&gt;
    &lt;p&gt;Essentially, it answers the question: "For this character at line X, column Y in the minified file, where was it originally located?"&lt;/p&gt;
    &lt;p&gt;This mapping data tracks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The file path and name of the original source file&lt;/item&gt;
      &lt;item&gt;The exact line and column in the source file&lt;/item&gt;
      &lt;item&gt;The original variable/function name (if renamed during minification)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But instead of storing this as a massive JSON array of positions, which would be larger than the minified code itself, source maps use a highly compressed format. Here's what the encoded string looks like:&lt;/p&gt;
    &lt;code&gt;"AAAA,OAAO,SAAS,IAAI,CAAC,EAAE;EACrB,OAAO,IAAI;AACb"
&lt;/code&gt;
    &lt;p&gt;To keep file sizes manageable, mappings use Variable Length Quantity (VLQ) encoding with Base64 characters. Let's break this down.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Mapping Structure&lt;/head&gt;
    &lt;p&gt;The mappings string is a series of segments separated by commas and semicolons:&lt;/p&gt;
    &lt;code&gt;"segment,segment,segment;segment,segment;segment"
&lt;/code&gt;
    &lt;p&gt;We'll see significance of commas and semicolons shortly, but first, what is a "segment"?&lt;/p&gt;
    &lt;p&gt;Each segment represents a mapping from a position in the generated file to a position in the source file. Segments come in three flavors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1 value: This referenced column doesn't map to any source (e.g., webpack-generated code)&lt;/p&gt;
        &lt;code&gt;[generatedColumn]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;4 values: This is the most common case, mapping a position in the generated file to a position in the source file:&lt;/p&gt;
        &lt;code&gt;[generatedColumn, sourceFileIndex, sourceLine, sourceColumn]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;5 values: Same as 4, plus a reference to the original name of the variable/function:&lt;/p&gt;
        &lt;code&gt;[generatedColumn, sourceFileIndex, sourceLine, sourceColumn, nameIndex]&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most common case is 4 values (basic position mapping). The 5th value is only added when a variable or function was renamed during minification.&lt;/p&gt;
    &lt;p&gt;But wait, notice that segments only contain the column in the generated file, not the line number. How does the decoder know which line a segment belongs to?&lt;/p&gt;
    &lt;p&gt;The answer lies in the structure: semicolons act as line breaks. The position of segments between semicolons determines their line number in the generated file.&lt;/p&gt;
    &lt;p&gt;This is why empty lines in the generated file still need semicolons, they maintain the line count even with no mappings.&lt;/p&gt;
    &lt;p&gt;Let's see how this works with a real example:&lt;/p&gt;
    &lt;p&gt;Notice how the decoded values give relative positions, each value represents the difference from the previous position, not absolute coordinates. This is crucial: instead of encoding large column numbers like 27698 in minified files, source maps only store small deltas like +7 or +15, making the encoded strings much more compact.&lt;/p&gt;
    &lt;p&gt;Now that we understand the mapping structure, let's see how these numbers actually get transformed into the Base64 alphabet characters we see in the mappings string.&lt;/p&gt;
    &lt;head rend="h3"&gt;How VLQ Encoding Works&lt;/head&gt;
    &lt;p&gt;VLQ (Variable Length Quantity) encoding is an efficient way to represent numbers using as few bytes as possible. It's perfect for source maps because most position differences are small numbers.&lt;/p&gt;
    &lt;p&gt;The encoding process has three main steps:&lt;/p&gt;
    &lt;p&gt;1. Encode the sign bit&lt;/p&gt;
    &lt;p&gt;Since we need to handle both positive and negative differences (code can move backward), VLQ uses the least significant bit (LSB) to encode the sign:&lt;/p&gt;
    &lt;code&gt;Positive number: LSB = 0
Negative number: LSB = 1

Examples:
 5 → binary: 101 → with sign bit: 1010 (LSB=0 for positive)
-5 → binary: 101 → with sign bit: 1011 (LSB=1 for negative)
&lt;/code&gt;
    &lt;p&gt;2. Split into 5-bit groups&lt;/p&gt;
    &lt;p&gt;Each Base64 character can represent 6 bits, but we need 1 bit as a "continuation" flag to indicate if more characters follow. This leaves 5 bits for data:&lt;/p&gt;
    &lt;code&gt;[continuation bit][5 data bits]
       ↑              ↑
   1 = more coming    actual value bits
   0 = last character
&lt;/code&gt;
    &lt;p&gt;3. Convert to Base64&lt;/p&gt;
    &lt;p&gt;Map each 6-bit value to a Base64 character:&lt;/p&gt;
    &lt;code&gt;A=0, B=1, C=2... Z=25, a=26, b=27... z=51, 0=52, 1=53... 9=61, +=62, /=63
&lt;/code&gt;
    &lt;head rend="h6"&gt;Example&lt;/head&gt;
    &lt;p&gt;Lets go through the steps to encode the number 7:&lt;/p&gt;
    &lt;p&gt;That's why in our mapping example, the value 7 is encoded as 'O'!&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I hope this deep dive into JavaScript source maps has shed light on how they function under the hood and adds to your appreciation for the amount of position data they efficiently encode.&lt;/p&gt;
    &lt;p&gt;P.S. Stay tuned: source maps support is coming to parca-agent and Polar Signals Cloud, bringing the same debugging magic to your performance profiling workflow!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.polarsignals.com/blog/posts/2025/11/04/javascript-source-maps-internals"/><published>2025-11-04T15:21:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812024</id><title>This Day in 1988, the Morris worm infected 10% of the Internet within 24 hours</title><updated>2025-11-04T23:34:40.887594+00:00</updated><content>&lt;doc fingerprint="ff40223f0be08a6f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;37 years ago this week, the Morris worm infected 10% of the Internet within 24 hours — worm slithered out and sparked a new era in cybersecurity&lt;/head&gt;
    &lt;p&gt;The Internet contracted worms a year before the World Wide Web was even a thing.&lt;/p&gt;
    &lt;p&gt;This week in 1988, Cornell graduate student Robert Tappan Morris unleashed his eponymous worm upon the Internet. The wave of infections grew to 10% of the entire Internet within 24 hours, causing astronomically expensive damage for the time. However, the pioneering Morris worm malware wasn’t made with malice, says an FBI retrospective on the “programming error.” It was designed to gauge the size of the Internet, resulting in a classic case of unintended consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Morris worm dissection&lt;/head&gt;
    &lt;p&gt;Known to be something of a prankster, Morris must have felt some foreboding about releasing his ‘innocent’ program into the wild. Evidence of this comes from his release method. “He released it by hacking into an MIT computer from his Cornell terminal in Ithaca, New York,” according to the FBI.&lt;/p&gt;
    &lt;p&gt;The Morris worm was written in C and targeted BSD UNIX systems, like VAX and Sun-3 machines. Specifically, the FBI writes, it “exploited a backdoor in the Internet’s electronic mail system and a bug in the ‘finger’ program that identified network users.” In contrast to computer viruses, the worm Morris had devised had no need of a host program, but could self-replicate and spread autonomously.&lt;/p&gt;
    &lt;p&gt;Thankfully, the Morris worm wasn’t written to cause damage to files. Due to those unintended consequences, though, it precipitated massive slowdowns, and messaging delays and system crashes were common symptoms. It became a computer news sensation in the worst possible way. Just to get rid of the worm in a timely fashion, some institutions ended up wiping complete systems and unplugging networks for as long as a week.&lt;/p&gt;
    &lt;p&gt;Among the Morris worm's casualties were prestigious institutions such as Berkeley, Harvard, Princeton, Stanford, Johns Hopkins, NASA, and the Lawrence Livermore National Laboratory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whodunit?&lt;/head&gt;
    &lt;p&gt;Experts worked hard to find a fix, and while they did so, the question of who was behind the worm came to the fore. Understandably, whoever created and unleashed this worm needed to feel some consequences, and thus, the FBI was brought in.&lt;/p&gt;
    &lt;p&gt;Apparently, Morris sought to anonymously explain and apologize for the worm, but an inadvertent slip of his initials by a friend landed Morris in it.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;FBI interviews and computer file analysis would subsequently confirm Morris was the culprit. He was indicted under the rather freshly inked Computer Fraud and Abuse Act of 1986. After a court appearance for his misdemeanors in 1989, Morris ended up not with jail time, but with a fine, probation, and 400 hours of community service to complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Computer worms have been around longer than the World Wide Web&lt;/head&gt;
    &lt;p&gt;Back in November 1988, the Internet bore little resemblance to what it is today. For example, the World Wide Web (WWW) wasn’t even a thing. Though the WWW would soon form the core experience for the first tide of surfers in the 90s.&lt;/p&gt;
    &lt;p&gt;At the time, the Internet’s backbone was the NSFNET, the recent successor to ARPANET. Its purpose was mostly to expand the prior backbone’s reach beyond military and defense institutions, and it more broadly embraced academia. While we are here, it is worth mentioning that NSFNET was decommissioned in 1995, and succeeded by the commercial Internet, which emerged in the 1990s off the back of private ISPs and commercial backbones.&lt;/p&gt;
    &lt;p&gt;So, when we talk about 10% of the Internet being paralyzed by the Morris Worm, contemporary estimates are that about 6,000 of the approximately 60,000 connected systems were infected and impacted. Moreover, when we highlighted the potentially massive costs of this first worm propagating, estimates range from $100,000 to millions of dollars.&lt;/p&gt;
    &lt;p&gt;Computer worms have remained a scary phenomenon in recent times. For example, we reported on the first-generation AI worm, the Morris II generative AI worm, last year.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;sb5k&lt;/header&gt;I was working at DEC when the worm slithered its way across the Internet, as part of an engineering team. I also helped manage our Ultrix systems; our IT department knew VMS only.Reply&lt;lb/&gt;I don't remember which CPU was in our systems, but the worm was not able to run on our systems, but I did find it dropped in them.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gaston404&lt;/header&gt;I completely disagree with the tone of the article. Depicting this as an accident without consequences and limited effect is simply incorrect.Reply&lt;lb/&gt;Back then as a part time job I managed some of the traffic routing through Washington DC. Mail relays were shutdown and backed up queues were spooked off to tape. By today’s standards the volume of traffic may seem trivial but when many of these links ran at 56kbps or less. It was a mess. The main way administrators communicated with each other was email. This also affected collaboration between University researchers and access to the NSF super computer centers.&lt;lb/&gt;At the time rumors maintained that Morris used exploits that he learned from his father who had a consulting agreement with the NSA. So if this is true there is a certain level of non-originality.&lt;lb/&gt;On one hand stronger persecution may have reduced follow on internet crime. On the other hand the fragility demonstrated by this crime, resulted in the creation of procedures to deal with outages. If anything the naive sense of trusted collaboration that pervaded the Internet started to fade.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;derekullo&lt;/header&gt;In 9 years, Tiktok has infected over 90% of the internet!Reply&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;DS426&lt;/header&gt;Reply&lt;quote/&gt;The next big social media craze is probably just around the corner. I shutter to think how ludicrous it will be.derekullo said:In 9 years, Tiktok has infected over 90% of the internet!&lt;lb/&gt;Much slower but also much more insidious!&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/cyber-security/on-this-day-in-1988-the-morris-worm-slithered-out-and-sparked-a-new-era-in-cybersecurity-10-percent-of-the-internet-was-infected-within-24-hours"/><published>2025-11-04T15:23:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45812606</id><title>Pg_lake: Postgres with Iceberg and data lake access</title><updated>2025-11-04T23:34:40.175004+00:00</updated><content>&lt;doc fingerprint="fb9ba072642955ea"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; integrates Iceberg and data lake files into Postgres. With the &lt;code&gt;pg_lake&lt;/code&gt; extensions, you can use Postgres as a stand-alone lakehouse system that supports transactions and fast queries on Iceberg tables, and can directly work with raw data files in object stores like S3.&lt;/p&gt;
    &lt;p&gt;At a high level, &lt;code&gt;pg_lake&lt;/code&gt; lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and modify Iceberg tables directly from PostgreSQL, with full transactional guarantees and query them from other engines&lt;/item&gt;
      &lt;item&gt;Query and import data from Parquet, CSV, JSON, and Iceberg files stored in S3 or other compatible object stores&lt;/item&gt;
      &lt;item&gt;Export query results back to S3 in Parquet, CSV, or JSON formats using COPY commands&lt;/item&gt;
      &lt;item&gt;Read geospatial formats supported by GDAL, such as GeoJSON and Shapefiles&lt;/item&gt;
      &lt;item&gt;Use compression transparently with .gz and .zst&lt;/item&gt;
      &lt;item&gt;Use the built-in map type for semi-structured or key–value data&lt;/item&gt;
      &lt;item&gt;Combine heap, Iceberg, and external Parquet/CSV/JSON files in the same SQL queries and modifications — all with full transactional guarantees and no SQL limitations&lt;/item&gt;
      &lt;item&gt;Infer table columns and types from external data sources such as Iceberg, Parquet, JSON, and CSV files&lt;/item&gt;
      &lt;item&gt;Leverage DuckDB’s query engine underneath for fast execution without leaving Postgres&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are two ways to set up &lt;code&gt;pg_lake&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using Docker, for an easy, ready-to-run test environment.&lt;/item&gt;
      &lt;item&gt;Building from source, for a manual setup or development use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both approaches include the PostgreSQL extensions, the &lt;code&gt;pgduck_server&lt;/code&gt; application and setting up S3-compatible storage.&lt;/p&gt;
    &lt;p&gt;Follow the Docker README to set up and run &lt;code&gt;pg_lake&lt;/code&gt; with Docker.&lt;/p&gt;
    &lt;p&gt;Once you’ve built and installed the required components, you can initialize &lt;code&gt;pg_lake&lt;/code&gt; inside Postgres.&lt;/p&gt;
    &lt;p&gt;Create all required extensions at once using &lt;code&gt;CASCADE&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;CREATE EXTENSION pg_lake CASCADE;
NOTICE:  installing required extension "pg_lake_table"
NOTICE:  installing required extension "pg_lake_engine"
NOTICE:  installing required extension "pg_extension_base"
NOTICE:  installing required extension "pg_lake_iceberg"
NOTICE:  installing required extension "pg_lake_copy"
CREATE EXTENSION&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; is a standalone process that implements the Postgres wire-protocol (locally), and underneath uses &lt;code&gt;DuckDB&lt;/code&gt; to execute queries.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;pgduck_server&lt;/code&gt; it starts listening to port &lt;code&gt;5332&lt;/code&gt; on unix domain socket:&lt;/p&gt;
    &lt;code&gt;pgduck_server
LOG pgduck_server is listening on unix_socket_directory: /tmp with port 5332, max_clients allowed 10000
&lt;/code&gt;
    &lt;p&gt;As &lt;code&gt;pgduck_server&lt;/code&gt; implements Postgres wire protocol, you can access it via &lt;code&gt;psql&lt;/code&gt; on port &lt;code&gt;5332&lt;/code&gt; and host &lt;code&gt;/tmp&lt;/code&gt; and run commands via DuckDB.&lt;/p&gt;
    &lt;p&gt;For example, you can get the DuckDB version:&lt;/p&gt;
    &lt;code&gt;psql -p 5332 -h /tmp

select version() as duckdb_version; 
duckdb_version 
---------------- 
v1.3.2 (1 row)&lt;/code&gt;
    &lt;p&gt;You can also provide some additional settings while starting the server, to see all:&lt;/p&gt;
    &lt;code&gt;pgduck_server --help
&lt;/code&gt;
    &lt;p&gt;There are some important settings that should be adjusted, especially on production systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--memory_limit&lt;/code&gt;: Optionally specify the maximum memory of pgduck_server similar to DuckDB's memory_limit, the default is 80 percent of the system memory&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--init_file_path &amp;lt;path&amp;gt;&lt;/code&gt;: Execute all statements in this file on start-up&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--cache_dir&lt;/code&gt;: Specify the directory to use to cache remote files (from S3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pgduck_server&lt;/code&gt; relies on the DuckDB secrets manager for credentials and it follows the credentials chain by default for AWS and GCP. Make sure your cloud credentials are configured properly — for example, by setting them in ~/.aws/credentials.&lt;/p&gt;
    &lt;p&gt;Once you set up the credential chain, you should set the &lt;code&gt;pg_lake_iceberg.default_location_prefix&lt;/code&gt;. This is the location where Iceberg tables are stored:&lt;/p&gt;
    &lt;code&gt;SET pg_lake_iceberg.default_location_prefix TO 's3://testbucketpglake';&lt;/code&gt;
    &lt;p&gt;You can also set the credentials on &lt;code&gt;pgduck_server&lt;/code&gt; for local development with &lt;code&gt;minio&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can create Iceberg tables by adding &lt;code&gt;USING iceberg&lt;/code&gt; to your &lt;code&gt;CREATE TABLE&lt;/code&gt; statements.&lt;/p&gt;
    &lt;code&gt;CREATE TABLE iceberg_test USING iceberg 
      AS SELECT 
            i as key, 'val_'|| i  as val
         FROM 
            generate_series(0,99)i;&lt;/code&gt;
    &lt;p&gt;Then, query it:&lt;/p&gt;
    &lt;code&gt;SELECT count(*) FROM iceberg_test;
 count 
-------
   100
(1 row)&lt;/code&gt;
    &lt;p&gt;You can then see the Iceberg metadata location:&lt;/p&gt;
    &lt;code&gt;SELECT table_name, metadata_location FROM iceberg_tables;


    table_name     |                                                metadata_location
-------------------+--------------------------------------------------------------------------------------------------------------------
 iceberg_test      | s3://testbucketpglake/postgres/public/test/435029/metadata/00001-f0c6e20a-fd1c-4645-87c9-c0c64b92992b.metadata.json&lt;/code&gt;
    &lt;p&gt;You can import or export data directly using &lt;code&gt;COPY&lt;/code&gt; in Parquet, CSV, or newline-delimited JSON formats.  The format is automatically inferred from the file extension, or you can specify it explicitly with &lt;code&gt;COPY&lt;/code&gt; options like &lt;code&gt;WITH (format 'csv', compression 'gzip')&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;-- Copy data from Postgres to S3 with format parquet
-- Read from any data source, including iceberg tables, heap tables or any query results
COPY (SELECT * FROM iceberg_test) TO 's3://testbucketpglake/parquet_data/iceberg_test.parquet';

-- Copy back from S3 to any table in Postgres
-- This example copies into an iceberg table, but could be heap table as well
COPY iceberg_test FROM 's3://testbucketpglake/parquet_data/iceberg_test.parquet';&lt;/code&gt;
    &lt;p&gt;You can create a foreign table directly from a file or set of files without having to specify column names or types.&lt;/p&gt;
    &lt;code&gt;-- use the files under the path, can use * for all files
CREATE FOREIGN TABLE parquet_table() 
SERVER pg_lake 
OPTIONS (path 's3://testbucketpglake/parquet_data/*.parquet');

-- note that we infer the columns from the file
\d parquet_table
              Foreign table "public.parquet_table"
 Column |  Type   | Collation | Nullable | Default | FDW options 
--------+---------+-----------+----------+---------+-------------
 key    | integer |           |          |         | 
 val    | text    |           |          |         | 
Server: pg_lake
FDW options: (path 's3://testbucketpglake/parquet_data/*.parquet')

-- and, query it
select count(*) from parquet_table;
 count 
-------
   100
(1 row)
&lt;/code&gt;
    &lt;p&gt;A &lt;code&gt;pg_lake&lt;/code&gt; instance consists of two main components: PostgreSQL with the pg_lake extensions and pgduck_server.&lt;/p&gt;
    &lt;p&gt;Users connect to PostgreSQL to run SQL queries, and the &lt;code&gt;pg_lake&lt;/code&gt; extensions integrate with Postgres’s hooks to handle query planning, transaction boundaries, and overall orchestration of execution.&lt;/p&gt;
    &lt;p&gt;Behind the scenes, parts of query execution are delegated to DuckDB through pgduck_server, a separate multi-threaded process that implements the PostgreSQL wire protocol (locally). This process runs DuckDB together with our duckdb_pglake extension, which adds PostgreSQL-compatible functions and behavior.&lt;/p&gt;
    &lt;p&gt;Users typically don’t need to be aware of &lt;code&gt;pgduck_server&lt;/code&gt;; it operates transparently to improve performance. When appropriate, &lt;code&gt;pg_lake&lt;/code&gt; delegates scanning of the data and the computation to DuckDB’s highly parallel, column-oriented execution engine.&lt;/p&gt;
    &lt;p&gt;This separation also avoids the threading and memory-safety limitations that would arise from embedding DuckDB directly inside the Postgres process, which is designed around process isolation rather than multi-threaded execution. Moreover, it lets us interact with the query engine directly by connecting to it using standard Postgres clients.&lt;/p&gt;
    &lt;p&gt;The team behind pg_lake has a lot of experience building Postgres extensions (e.g. Citus, pg_cron, pg_documentdb). Over time, we’ve learned that large, monolithic PostgreSQL extensions are harder to evolve and maintain.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; follows a modular design built around a set of interoperating components — mostly implemented as PostgreSQL extensions, others as supporting services or libraries.&lt;lb/&gt; Each part focuses on a well-defined layer, such as table and metadata management, catalog and object store integration, query execution, or data format handling. This approach makes it easier to extend, test, and evolve the system, while keeping it familiar to anyone with a PostgreSQL background.&lt;/p&gt;
    &lt;p&gt;The current set of components are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pg_lake_iceberg: a PostgreSQL extension that implements the Iceberg specification&lt;/item&gt;
      &lt;item&gt;pg_lake_table: a PostgreSQL extension that implements a foreign data wrapper to query files in object storage&lt;/item&gt;
      &lt;item&gt;pg_lake_copy: a PostgreSQL extension that implements COPY to/from your data lake&lt;/item&gt;
      &lt;item&gt;pg_lake_engine: a common module for different pg_lake extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_base: A foundational building block for other extensions&lt;/item&gt;
      &lt;item&gt;pg_extension_updater: An extension for updating all extensions on start-up. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_lake_benchmark: a PostgreSQL extension that performs various benchmarks on lake tables. See README.md.&lt;/item&gt;
      &lt;item&gt;pg_map: A generic map type generator&lt;/item&gt;
      &lt;item&gt;pgduck_server: a stand-alone server that loads DuckDB into the same server machine and exposes DuckDB via the PostgreSQL protocol&lt;/item&gt;
      &lt;item&gt;duckdb_pglake: a DuckDB extension that adds missing PostgreSQL functions to DuckDB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; development started in early 2024 at Crunchy Data with the goal of bringing Iceberg to PostgreSQL. The first few months were focused on building a robust integration of an external query engine (DuckDB). To get to market early, we made the query/import/export features available to Crunchy Bridge customers as Crunchy Bridge for Analytics.&lt;/p&gt;
    &lt;p&gt;Next, we started building a comprehensive implementation of the Iceberg (v2) protocol with support for transactions and almost all PostgreSQL features. In November 2024, we relaunched Crunchy Bridge for Analytics as Crunchy Data Warehouse available on Crunchy Bridge and on-premises.&lt;/p&gt;
    &lt;p&gt;In June 2025, Crunchy Data was acquired by Snowflake. Following the acquisition, Snowflake decided to open source the project as &lt;code&gt;pg_lake&lt;/code&gt; in November 2025. The initial version is 3.0 because of the two prior generations. If you’re currently a Crunchy Data Warehouse user there will be an automatic upgrade path, though some names will change.&lt;/p&gt;
    &lt;p&gt;Full project documentation can be found in the docs directory.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Snowflake Inc. All rights reserved. Licensed under the Apache 2.0 license.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;pg_lake&lt;/code&gt; is dependent on third-party projects Apache Avro and DuckDB. During build, &lt;code&gt;pg_lake&lt;/code&gt; applies patches to Avro and certain DuckDB extensions in order to provide the &lt;code&gt;pg_lake&lt;/code&gt; functionality. The source code associated with the Avro and DuckDB extensions is downloaded from the applicable upstream repos and the source code associated with those projects remains under the original licenses. If you are packaging or redistributing packages that include &lt;code&gt;pg_lake&lt;/code&gt;, please note that you should review those upstream license terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/Snowflake-Labs/pg_lake"/><published>2025-11-04T16:12:27+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813310</id><title>Launch HN: Plexe (YC X25) – Build production-grade ML models from prompts</title><updated>2025-11-04T23:34:39.594225+00:00</updated><content>&lt;doc fingerprint="d1d50ecbdfaae78"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;AI Data Scientist&lt;/p&gt;
    &lt;p&gt;Your Agentic ML Engineering&lt;/p&gt;
    &lt;p&gt;Team&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Turn your raw data into engineered AI solutions.&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;Custom ML Models&lt;/p&gt;
    &lt;p&gt;Data Dashboards&lt;/p&gt;
    &lt;p&gt;API Endpoints&lt;/p&gt;
    &lt;p&gt;Batch Jobs&lt;/p&gt;
    &lt;p&gt;File Upload&lt;/p&gt;
    &lt;p&gt;Database Connectors&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and it will build a production-ready model thatâs engineered for your exact business challenge.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;Spotlight&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;As Seen On&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Read what the media is saying about us&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Featured in BIâs 10 Most Exciting AI Startups from YC Spring 2025&lt;/p&gt;
    &lt;p&gt;Plexe AI Redefines Credit Underwriting With Real-Time ML Models&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Plexe Launches to Bring Custom AI Models to Every Business&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Plexe featured in European Startups at Y Combinator&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Read More&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;Solutions&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;What Plexe Can Build For You&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Tailored ML solutions for your industry, deployed instantly.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;Select your industry:&lt;/p&gt;
    &lt;p&gt;Finance &amp;amp; Banking&lt;/p&gt;
    &lt;p&gt;E-commerce&lt;/p&gt;
    &lt;p&gt;Logistics&lt;/p&gt;
    &lt;p&gt;Cybersecurity&lt;/p&gt;
    &lt;p&gt;Stop fraud before it drains your revenue.&lt;/p&gt;
    &lt;p&gt;Protect your customers and your bottom line with AI that spots suspicious activity before it becomes a problem. &lt;/p&gt;
    &lt;p&gt;Lend with confidence.&lt;/p&gt;
    &lt;p&gt;Make smarter credit decisions by accurately understanding whoâs truly creditworthy.&lt;/p&gt;
    &lt;p&gt;Keep your best customers from leaving.&lt;/p&gt;
    &lt;p&gt;Identify early signs of churn so you can act before valuable relationships are lost.&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;FAQ&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Questions? Weâve Got Answers.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Everything you need to know about using Plexe, from building your first model to deploying at scale.&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Who owns the models?&lt;/p&gt;
    &lt;p&gt;Where can I use Plexe?&lt;/p&gt;
    &lt;p&gt;Do you have a free version?&lt;/p&gt;
    &lt;p&gt;Can I use Plexe without my own data?&lt;/p&gt;
    &lt;p&gt;How secure is my data?&lt;/p&gt;
    &lt;p&gt;Can Plexe integrate with my existing tools?&lt;/p&gt;
    &lt;p&gt;Do you offer annual or enterprise pricing?&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your data into your competitive advantage.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Letâs Build Something Incredible Together.&lt;/p&gt;
    &lt;p&gt;Whether youâre starting from scratch or scaling to millions of users, Plexe is your AI engineering team, ready to turn your ideas into real solutions.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Â© 2025 Plexe Ltd. All rights reserved.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;How It Works&lt;/p&gt;
    &lt;p&gt;From Prompt to Production: The Plexe Workflow&lt;/p&gt;
    &lt;p&gt;Get Instant, Actionable Data Insights&lt;/p&gt;
    &lt;p&gt;Simply connect your data, Plexe checks quality, and spots the patterns that matter most. Youâll see whatâs working, whatâs not, and where the real opportunities are hiding. No code, no setup, no fuss.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Give me quick insights on ecommerce fraud dataset&lt;/p&gt;
    &lt;p&gt;Quick Insights on E-commerce Fraud Dataset&lt;/p&gt;
    &lt;p&gt;ð Key Findings:&lt;/p&gt;
    &lt;p&gt;Data Quality: â Excellent - No missing values across all fields&lt;/p&gt;
    &lt;p&gt;Fraud Rate: ð Very low at -1% (mean of 0.01025), which is typical for fraud datasets&lt;/p&gt;
    &lt;p&gt;ð Notable Patterns&lt;/p&gt;
    &lt;p&gt;Transaction Amounts:&lt;/p&gt;
    &lt;p&gt;Average: $90.59 but with high variation (std: $116.23)&lt;/p&gt;
    &lt;p&gt;Ask anything about models or datasets...&lt;/p&gt;
    &lt;p&gt;Turn Ideas into Deployable ML Models&lt;/p&gt;
    &lt;p&gt;Tell Plexe what you want to achieve, in plain language and weâll build a production-ready model thatâs engineered for your exact business challenge. Whether itâs predicting churn or fraud detection, youâll go from idea to working AI in hours, not months.&lt;/p&gt;
    &lt;p&gt;Create Model&lt;/p&gt;
    &lt;p&gt;Build a custom AI model for your specific needs in a few simple steps&lt;/p&gt;
    &lt;p&gt;Describe your modelâs purpose&lt;/p&gt;
    &lt;p&gt;Explain what you want your model to do in detail. Be specific about what you want to predict and what data it should use.&lt;/p&gt;
    &lt;p&gt;Model Intent&lt;/p&gt;
    &lt;p&gt;Build me a product recommendations model for my ecommerce website&lt;/p&gt;
    &lt;p&gt;Model Name&lt;/p&gt;
    &lt;p&gt;build-product-recommendations&lt;/p&gt;
    &lt;p&gt;Generate&lt;/p&gt;
    &lt;p&gt;A unique identifier for your model. Use lowercase letters, numbers, and hyphens only.&lt;/p&gt;
    &lt;p&gt;Full Transparency, Built In&lt;/p&gt;
    &lt;p&gt;We believe you should always know what your AI is doing and why. Plexe gives you clear performance metrics, training details, and easy-to-read explanations so you can trust every prediction your model makes.&lt;/p&gt;
    &lt;p&gt;Funding Prediction Model&lt;/p&gt;
    &lt;p&gt;completed&lt;/p&gt;
    &lt;p&gt;Retrain Model&lt;/p&gt;
    &lt;p&gt;Download Model&lt;/p&gt;
    &lt;p&gt;Performance&lt;/p&gt;
    &lt;p&gt;Overview&lt;/p&gt;
    &lt;p&gt;Technical Details&lt;/p&gt;
    &lt;p&gt;API Usage&lt;/p&gt;
    &lt;p&gt;Model Performance&lt;/p&gt;
    &lt;p&gt;Training performance, metrics and behavior insights.&lt;/p&gt;
    &lt;p&gt;Training Performance&lt;/p&gt;
    &lt;p&gt;Mean Absolute Error&lt;/p&gt;
    &lt;p&gt;0.2083&lt;/p&gt;
    &lt;p&gt;Training Details&lt;/p&gt;
    &lt;p&gt;Preprocessing&lt;/p&gt;
    &lt;p&gt;One-hot encoding for categorical variables proj_a, proj_b, funder and quarter.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.plexe.ai/"/><published>2025-11-04T17:07:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813343</id><title>NoLongerEvil-Thermostat – Nest Generation 1 and 2 Firmware</title><updated>2025-11-04T23:34:39.011111+00:00</updated><content>&lt;doc fingerprint="1eabf0080969b3e8"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;WARNING: EXPERIMENTAL SOFTWARE&lt;p&gt;This project is currently in the experimental/testing phase. Do NOT use this firmware on any thermostat that is critical for your heating or cooling needs. Flashing this firmware may brick your device or cause unexpected behavior. Only proceed if you have a backup thermostat or can afford to have your device non-functional during testing.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;This directory contains the tools and firmware needed to flash custom firmware to Nest Thermostat devices using the OMAP DFU (Device Firmware Update) interface.&lt;/p&gt;
    &lt;p&gt;This firmware loader uses the OMAP bootloader interface to flash custom bootloader and kernel images to Nest Thermostat devices. The device must be put into DFU mode to accept new firmware.&lt;/p&gt;
    &lt;p&gt;Important: After flashing this firmware, your device will no longer contact Nest/Google servers. It will operate independently and connect to the NoLongerEvil platform instead, giving you complete control over your thermostat.&lt;/p&gt;
    &lt;p&gt;The custom firmware flashes the device with modified bootloader and kernel components that redirect all network traffic from the original Nest/Google servers to a server we specify. This server hosts a reverse-engineered replica of their API, allowing the thermostat to function independently while giving you complete control over your device data and settings.&lt;/p&gt;
    &lt;p&gt;By intercepting the communication layer, the thermostat believes it's communicating with the official Nest infrastructure, but instead connects to the NoLongerEvil platform. This approach ensures full compatibility with the device's existing software while breaking free from Google's cloud dependency.&lt;/p&gt;
    &lt;code&gt;git clone --recurse-submodules https://github.com/codykociemba/NoLongerEvil-Thermostat.git
cd NoLongerEvil-Thermostat&lt;/code&gt;
    &lt;p&gt;Before building, you'll need to install some required packages:&lt;/p&gt;
    &lt;code&gt;sudo apt-get update
sudo apt-get install build-essential libusb-1.0-0-dev gcc&lt;/code&gt;
    &lt;p&gt;First, install Xcode Command Line Tools:&lt;/p&gt;
    &lt;code&gt;xcode-select --install&lt;/code&gt;
    &lt;p&gt;Then install libusb using Homebrew (the build script will attempt to install this automatically if missing):&lt;/p&gt;
    &lt;code&gt;# Install Homebrew if you don't have it
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install libusb
brew install libusb&lt;/code&gt;
    &lt;code&gt;chmod +x build.sh
./build.sh&lt;/code&gt;
    &lt;p&gt;The build script will automatically detect your operating system (Linux, macOS, or Windows) and build the appropriate binary.&lt;/p&gt;
    &lt;p&gt;IMPORTANT: You must start the installer script BEFORE rebooting the device.&lt;/p&gt;
    &lt;code&gt;chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;code&gt;chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;p&gt;Note for macOS: You may need to grant USB permissions. If you encounter permission issues, check System Preferences → Security &amp;amp; Privacy.&lt;/p&gt;
    &lt;p&gt;The script will wait for the device to enter DFU mode.&lt;/p&gt;
    &lt;p&gt;Follow these steps carefully:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Charge the device - Ensure your Nest Thermostat is properly charged (at least 50% battery recommended)&lt;/item&gt;
      &lt;item&gt;Remove from wall - Remove the Nest from its back plate/wall mount&lt;/item&gt;
      &lt;item&gt;Connect via USB - Plug the Nest into your computer using a micro USB cable&lt;/item&gt;
      &lt;item&gt;Wait for the installer - Make sure the &lt;code&gt;install.sh&lt;/code&gt;script is running and waiting&lt;/item&gt;
      &lt;item&gt;Reboot the device - Press and hold down on the display for 10-15 seconds until the device reboots&lt;/item&gt;
      &lt;item&gt;DFU mode active - Once it reboots, the device will enter DFU mode and the installer script will recognize it and begin flashing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The firmware installer will automatically detect the device and flash the custom bootloader (x-load, u-boot) and kernel (uImage).&lt;/p&gt;
    &lt;p&gt;After the firmware is flashed successfully, you should see our logo on the device screen:&lt;/p&gt;
    &lt;p&gt;Important:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Keep the device plugged in via USB&lt;/item&gt;
      &lt;item&gt;Wait for the device to complete its boot sequence (this may take 3-4 minutes)&lt;/item&gt;
      &lt;item&gt;Do not disconnect or power off the device during this time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once the device has fully rebooted:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Visit https://nolongerevil.com in your web browser&lt;/item&gt;
      &lt;item&gt;Register an account (or sign in if you already have one)&lt;/item&gt;
      &lt;item&gt;Navigate to your Dashboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You will see a "No devices" screen that prompts you for an entry code.&lt;/p&gt;
    &lt;p&gt;To link your Nest device to your NoLongerEvil account:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On your Nest device, navigate to: Settings → Nest App → Get Entry Code&lt;/item&gt;
      &lt;item&gt;The device will display a unique entry code&lt;/item&gt;
      &lt;item&gt;Enter this code on the NoLongerEvil dashboard&lt;/item&gt;
      &lt;item&gt;Your device is now linked and ready to use!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The firmware installation process installs three components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;x-load.bin - First-stage bootloader (X-Loader for OMAP)&lt;/item&gt;
      &lt;item&gt;u-boot.bin - Second-stage bootloader (Das U-Boot) loaded at address 0x80100000&lt;/item&gt;
      &lt;item&gt;uImage - Linux kernel image loaded at address 0x80A00000&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After flashing, the device jumps to execution at 0x80100000 (u-boot).&lt;/p&gt;
    &lt;p&gt;This tool provides low-level access to the device's boot process. Use responsibly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only use on devices you own&lt;/item&gt;
      &lt;item&gt;Improper firmware can brick your device (Don't sue me bro)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project builds upon the excellent work of several security researchers and developers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;grant-h / ajb142 - omap_loader, the USB bootloader tool used to flash OMAP devices&lt;/item&gt;
      &lt;item&gt;exploiteers (GTVHacker) - Original research and development of the Nest DFU attack, which demonstrated the ability to flash custom firmware to Nest devices gen 1 &amp;amp; gen 2&lt;/item&gt;
      &lt;item&gt;FULU and all bounty backers - For funding the Nest Learning Thermostat Gen 1/2 bounty and supporting the right-to-repair movement&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without their groundbreaking research, open-source contributions, and advocacy for device ownership rights, this work would not be possible. Thank you!&lt;/p&gt;
    &lt;p&gt;We are committed to transparency and the right-to-repair movement. The firmware images and backend API server code will be open sourced soon, allowing the community to audit, improve, and self-host their own infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/codykociemba/NoLongerEvil-Thermostat"/><published>2025-11-04T17:10:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813719</id><title>SocketAddrV6 is not roundtrip serializable</title><updated>2025-11-04T23:34:38.598122+00:00</updated><content>&lt;doc fingerprint="223dd8174c01bad0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;&lt;code&gt;SocketAddrV6&lt;/code&gt; is not roundtrip serializable&lt;/head&gt;
    &lt;p&gt;A few weeks ago at Oxide, we encountered a bug where a particular, somewhat large, data structure was erroring on serialization to JSON via &lt;code&gt;serde&lt;/code&gt;. The problem was that JSON only supports map keys that are strings or numbers, and the data structure had an infrequently-populated map with keys that were more complex than that1.&lt;/p&gt;
    &lt;p&gt;We fixed the bug, but a concern still remained: what if some other map that was empty most of the time had a complex key in it? The easiest way to guard against this is by generating random instances of the data structure and attempting to serialize them, checking that this operation doesn’t panic. The most straightforward way to do this is with property-based testing, where you define:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a way to generate random instances of a particular type, and&lt;/item&gt;
      &lt;item&gt;given a failing input, a way to shrink it down to a minimal failing value.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern property-based testing frameworks like &lt;code&gt;proptest&lt;/code&gt;, which we use at Oxide, combine these two algorithms into a single strategy, through a technique known as integrated shrinking. (For a more detailed overview, see my monad tutorial, where I talk about the undesirable performance characteristics of monadic composition when it comes to integrated shrinking.)&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;proptest&lt;/code&gt; library has a notion of a canonical strategy for a type, expressed via the &lt;code&gt;Arbitrary&lt;/code&gt; trait. The easiest way to define &lt;code&gt;Arbitrary&lt;/code&gt; instances for large, complex types is to use a derive macro. Annotate your type with the macro:&lt;/p&gt;
    &lt;code&gt;use test_strategy::Arbitrary;

#[derive(Arbitrary)]
struct MyType {
    id: String,
    data: BTreeMap&amp;lt;String, MyInnerType&amp;gt;,
}

#[derive(Arbitrary)]
struct MyInnerType {
    value: usize,
    // ...
}
&lt;/code&gt;
    &lt;p&gt;As long as all the fields have &lt;code&gt;Arbitrary&lt;/code&gt; defined for them—and the &lt;code&gt;proptest&lt;/code&gt; library defines the trait for most types in the standard library—your type has a working random generator and shrinker associated with it. It’s pretty neat!&lt;/p&gt;
    &lt;p&gt;I put together an &lt;code&gt;Arbitrary&lt;/code&gt; implementation for our very complex type, then wrote a property-based test to ensure that it serializes properly:&lt;/p&gt;
    &lt;code&gt;use test_strategy::proptest;

// The outermost struct is called PlanningReport.
#[proptest]
fn planning_report_json_serialize(planning_report: PlanningReport) {
    serde_json::to_string(&amp;amp;planning_report).unwrap();
}
&lt;/code&gt;
    &lt;p&gt;And, running it:&lt;/p&gt;
    &lt;code&gt;% cargo nextest run -p nexus-types planning_report
        PASS [   4.879s] nexus-types deployment::planning_report::tests::planning_report_json_serialize
────────────
     Summary [   4.880s] 1 test run: 1 passed, 24 skipped
&lt;/code&gt;
    &lt;p&gt;The test passed!&lt;/p&gt;
    &lt;p&gt;But while we’re here, surely we should also be able to deserialize a &lt;code&gt;PlanningReport&lt;/code&gt;, and then ensure that we get the same value back, right? We’ve already done the hard part, so let’s go ahead and add this test:&lt;/p&gt;
    &lt;code&gt;#[proptest]
fn planning_report_json_roundtrip(planning_report: PlanningReport) {
    let json = serde_json::to_string(&amp;amp;planning_report).unwrap();
    let deserialized: PlanningReport = serde_json::from_str(&amp;amp;json).unwrap();
    prop_assert_eq!(
        planning_report,
        deserialized,
        "input and output are equal"
    );
}
&lt;/code&gt;
    &lt;p&gt;And…&lt;/p&gt;
    &lt;code&gt;% cargo nextest run -p nexus-types planning_report
        FAIL [   3.688s] nexus-types deployment::planning_report::tests::planning_report_json_roundtrip
  stderr ───
  minimal failing input: input = _PlanningReportJsonRoundtripArgs {
      [... many fields omitted for brevity]
      mgs_updates: PlanningMgsUpdatesStepReport {
          blocked_mgs_updates: [],
          pending_mgs_updates: PendingMgsUpdates {
              by_baseboard: {
                  BaseboardId {
                      part_number: "",
                      serial_number: "",
                  }: PendingMgsUpdate {
                      sp_type: Sled,
                      slot_id: 0,
                      details: HostPhase1(
                          PendingMgsUpdateHostPhase1Details {
                              expected_active_phase_1_slot: A,
                              expected_boot_disk: A,
                              sled_agent_address: [::ffff:0.0.0.0]:0,
                          },
                      ),
                  },
              },
          },
      },
      [...]
  }
&lt;/code&gt;
    &lt;p&gt;The roundtrip test failed!&lt;/p&gt;
    &lt;p&gt;Why in the world did the test fail? My first idea was to try and do a textual diff of the &lt;code&gt;Debug&lt;/code&gt; outputs of the two data structures. In this case, I tried out the &lt;code&gt;pretty_assertions&lt;/code&gt; library, with something like:&lt;/p&gt;
    &lt;code&gt;fn planning_report_json_roundtrip(planning_report: PlanningReport) {
    // ...
    pretty_assertions::assert_eq!(planning_report, deserialized);
}
&lt;/code&gt;
    &lt;p&gt;And the output I got was:&lt;/p&gt;
    &lt;code&gt;% cargo nextest run -p nexus-types planning_report
[...]
Test failed: assertion failed: `(left == right)`

Diff &amp;lt; left / right &amp;gt; :
 PlanningReport {
     [...]
     mgs_updates: PlanningMgsUpdatesStepReport {
         blocked_mgs_updates: [],
         pending_mgs_updates: PendingMgsUpdates {
             by_baseboard: {
                 BaseboardId {
                     part_number: "",
                     serial_number: "",
                 }: PendingMgsUpdate {
                     baseboard_id: BaseboardId {
                         part_number: "",
                         serial_number: "",
                     },
                     sp_type: Sled,
                     slot_id: 0,
                     details: HostPhase1(
                         PendingMgsUpdateHostPhase1Details {
                             expected_active_phase_1_slot: A,
                             expected_boot_disk: A,
                             sled_agent_address: [::ffff:0.0.0.0]:0,
                         },
                     ),
                 },
             },
         },
     },
     [...]
 }
&lt;/code&gt;
    &lt;p&gt;There’s nothing in the output! No &lt;code&gt;&amp;lt;&lt;/code&gt; or &lt;code&gt;&amp;gt;&lt;/code&gt; as would typically be printed. It’s as if there wasn’t a difference at all, and yet the assertion failing indicated the before and after values just weren’t the same.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is going on?#&lt;/head&gt;
    &lt;p&gt;We have one clue to go by: the integrated shrinking algorithm in &lt;code&gt;proptest&lt;/code&gt; tries to shrink maps down to empty ones. But it looks like the &lt;code&gt;pending_mgs_updates&lt;/code&gt; map is non-empty. This means that something in either the &lt;code&gt;BaseboardId&lt;/code&gt; key or the &lt;code&gt;PendingMgsUpdate&lt;/code&gt; value was suspicious.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;PendingMgsUpdate&lt;/code&gt; is defined as:&lt;/p&gt;
    &lt;code&gt;// These types had many more fields -- most have been omitted for brevity.
pub struct PendingMgsUpdate {
    pub baseboard_id: Arc&amp;lt;BaseboardId&amp;gt;,
    pub sp_type: SpType,
    pub slot_id: u16,
    pub details: PendingMgsUpdateDetails,
}

pub enum PendingMgsUpdateDetails {
    // ...
    HostPhase1(PendingMgsUpdateHostPhase1Details),
}

pub struct PendingMgsUpdateHostPhase1Details {
    pub expected_active_phase_1_slot: M2Slot,
    pub expected_boot_disk: M2Slot,
    pub sled_agent_address: SocketAddrV6,
}
&lt;/code&gt;
    &lt;p&gt;Most of these types were pretty simple. The only one that looked even remotely suspicious was the &lt;code&gt;SocketAddrV6&lt;/code&gt;, which ostensibly represents an IPv6 address plus a port number.&lt;/p&gt;
    &lt;p&gt;What’s going on with the &lt;code&gt;SocketAddrV6&lt;/code&gt;? Does the &lt;code&gt;Arbitrary&lt;/code&gt; implementation for it do something weird? Well, let’s look at it:&lt;/p&gt;
    &lt;code&gt;arbitrary!(SocketAddrV6, SMapped&amp;lt;(Ipv6Addr, u16, u32, u32), Self&amp;gt;;
    static_map(any::&amp;lt;(Ipv6Addr, u16, u32, u32)&amp;gt;(),
        |(a, b, c, d)| Self::new(a, b, c, d))
);
&lt;/code&gt;
    &lt;p&gt;Like a lot of abstracted-out library code it looks a bit strange, but at its core it seems to be simple enough:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;generate four values: an &lt;code&gt;Ipv6Addr&lt;/code&gt;, a&lt;code&gt;u16&lt;/code&gt;, a&lt;code&gt;u32&lt;/code&gt;, and another&lt;code&gt;u32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;then pass them in to &lt;code&gt;SocketAddrV6::new&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;Ipv6Addr&lt;/code&gt; is self-explanatory, and the &lt;code&gt;u16&lt;/code&gt; is probably the port number. But what are these last two values? Let’s look at the &lt;code&gt;SocketAddrV6::new&lt;/code&gt; constructor:&lt;/p&gt;
    &lt;code&gt;pub const fn new(
    ip: Ipv6Addr,
    port: u16,
    flowinfo: u32,
    scope_id: u32,
) -&amp;gt; SocketAddrV6 {
    SocketAddrV6 { ip, port, flowinfo, scope_id }
}
&lt;/code&gt;
    &lt;p&gt;What in the world are these two &lt;code&gt;flowinfo&lt;/code&gt; and &lt;code&gt;scope_id&lt;/code&gt; values? They look mighty suspicious.&lt;/p&gt;
    &lt;p&gt;A thing that caught my eye was the “Textual representation” section of the &lt;code&gt;SocketAddrV6&lt;/code&gt;, which defined the representation as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A left square bracket (&lt;code&gt;[&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;The textual representation of an IPv6 address&lt;/item&gt;
      &lt;item&gt;Optionally, a percent sign (&lt;code&gt;%&lt;/code&gt;) followed by the scope identifier encoded as a decimal integer&lt;/item&gt;
      &lt;item&gt;A right square bracket (&lt;code&gt;]&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;A colon (&lt;code&gt;:&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;The port, encoded as a decimal integer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note what’s missing from this representation: the &lt;code&gt;flowinfo&lt;/code&gt; field!&lt;/p&gt;
    &lt;p&gt;We finally have a theory for what’s going on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;proptest&lt;/code&gt;generated a&lt;code&gt;SocketAddrV6&lt;/code&gt;with a non-zero&lt;code&gt;flowinfo&lt;/code&gt;field.&lt;/item&gt;
      &lt;item&gt;When we went to serialize this field as JSON, we used the textual representation, which dropped the &lt;code&gt;flowinfo&lt;/code&gt;field.&lt;/item&gt;
      &lt;item&gt;When we deserialized it, the &lt;code&gt;flowinfo&lt;/code&gt;field was set to zero.&lt;/item&gt;
      &lt;item&gt;As a result, the before and after values were no longer equal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why did this not show up in the textual diff of the &lt;code&gt;Debug&lt;/code&gt; values? For most types in Rust, the &lt;code&gt;Debug&lt;/code&gt; representation breaks out all the fields and their values. But for &lt;code&gt;SocketAddrV6&lt;/code&gt;, the &lt;code&gt;Debug&lt;/code&gt; implementation (quite reasonably) forwards to the &lt;code&gt;Display&lt;/code&gt; implementation. So the &lt;code&gt;flowinfo&lt;/code&gt; field is completely hidden, and the only way to look at it is through the &lt;code&gt;flowinfo&lt;/code&gt; method. Whoops.&lt;/p&gt;
    &lt;p&gt;How can we test this theory? The easiest way is to generate random values of &lt;code&gt;SocketAddrV6&lt;/code&gt; where &lt;code&gt;flowinfo&lt;/code&gt; is always set to zero, and see if that passes our roundtrip tests. The &lt;code&gt;proptest&lt;/code&gt; ecosystem has pretty good support for generating and using this kind of non-canonical strategy. Let’s try it out:&lt;/p&gt;
    &lt;code&gt;// This defines a strategy where flowinfo is always 0.
fn socket_addr_v6_without_flowinfo() -&amp;gt; impl Strategy&amp;lt;Value = SocketAddrV6&amp;gt; {
    any::&amp;lt;(Ipv6Addr, u16, u32)&amp;gt;().prop_map(
        |(addr, port, scope_id)| SocketAddrV6::new(addr, port, 0, scope_id),
    )
}

// Then, we can use this function like so.
#[derive(Arbitrary)]
pub struct PendingMgsUpdateHostPhase1Details {
    pub expected_active_phase_1_slot: M2Slot,
    pub expected_boot_disk: M2Slot,
    #[strategy(socket_addr_v6_without_flowinfo())]
    pub sled_agent_address: SocketAddrV6,
}
&lt;/code&gt;
    &lt;p&gt;Pretty straightforward, and similar to how &lt;code&gt;serde&lt;/code&gt; lets you provide custom implementations through &lt;code&gt;#[serde(with = ...)]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let’s test it out again:&lt;/p&gt;
    &lt;code&gt;% cargo nextest run -p nexus-types planning_report
        PASS [   4.828s] nexus-types deployment::planning_report::tests::planning_report_json_roundtrip
────────────
     Summary [   4.829s] 1 test run: 1 passed, 24 skipped
&lt;/code&gt;
    &lt;p&gt;All right, looks like our theory is confirmed! We can now merrily be on our way… right?&lt;/p&gt;
    &lt;p&gt;This little adventure left us with more questions than answers, though:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What does this &lt;code&gt;flowinfo&lt;/code&gt;field mean?&lt;/item&gt;
      &lt;item&gt;A &lt;code&gt;SocketAddrV4&lt;/code&gt;is just an&lt;code&gt;Ipv4Addr&lt;/code&gt;plus a port; why is a&lt;code&gt;SocketAddrV6&lt;/code&gt;different?&lt;/item&gt;
      &lt;item&gt;Why is the &lt;code&gt;flowinfo&lt;/code&gt;not part of the textual representation?&lt;code&gt;Ipv4Addr&lt;/code&gt;,&lt;code&gt;Ipv6Addr&lt;/code&gt;, and&lt;code&gt;SocketAddrV4&lt;/code&gt;are all roundtrip serializable. Why is&lt;code&gt;SocketAddrV6&lt;/code&gt;not?&lt;/item&gt;
      &lt;item&gt;Also: what is the &lt;code&gt;scope_id&lt;/code&gt;field?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;But what is &lt;code&gt;flowinfo&lt;/code&gt;, anyway?#&lt;/head&gt;
    &lt;p&gt;The best place to start looking is in the IETF Request for Comments (RFCs)2 that specify IPv6. The Rust documentation for &lt;code&gt;flowinfo&lt;/code&gt; helpfully links to RFC 2460, section 6 and section 7.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;flowinfo&lt;/code&gt; field is actually a combination of two fields that are part of every IPv6 packet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a 20-bit Flow Label, and&lt;/item&gt;
      &lt;item&gt;an 8-bit Traffic Class3.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Section 6 of the RFC says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Flow Labels&lt;/p&gt;
      &lt;p&gt;The 20-bit Flow Label field in the IPv6 header may be used by a source to label sequences of packets for which it requests special handling by the IPv6 routers, such as non-default quality of service or “real-time” service. This aspect of IPv6 is, at the time of writing, still experimental and subject to change as the requirements for flow support in the Internet become clearer. […]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And section 7:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Traffic Classes&lt;/p&gt;
      &lt;p&gt;The 8-bit Traffic Class field in the IPv6 header is available for use by originating nodes and/or forwarding routers to identify and distinguish between different classes or priorities of IPv6 packets. At the point in time at which this specification is being written, there are a number of experiments underway in the use of the IPv4 Type of Service and/or Precedence bits to provide various forms of “differentiated service” for IP packets […].&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Traffic Classes#&lt;/head&gt;
    &lt;p&gt;Let’s look at the Traffic Class field first. This field is similar to IPv4’s differentiated services code point (DSCP), and is meant to provide quality of service (QoS) over the network. (For example, prioritizing low-latency gaming and video conferencing packets over bulk downloads.)&lt;/p&gt;
    &lt;p&gt;The DSCP field in IPv4 is not part of a &lt;code&gt;SocketAddrV4&lt;/code&gt;, but the Traffic Class—through the &lt;code&gt;flowinfo&lt;/code&gt; field—is part of a &lt;code&gt;SocketAddrV6&lt;/code&gt;. Why is that the case? Rust’s definition of &lt;code&gt;SocketAddrV6&lt;/code&gt; mirrors the &lt;code&gt;sockaddr_in6&lt;/code&gt; defined by RFC 2553, section 3.3:&lt;/p&gt;
    &lt;code&gt;struct sockaddr_in6 {
    sa_family_t     sin6_family;    /* AF_INET6 */
    in_port_t       sin6_port;      /* transport layer port # */
    uint32_t        sin6_flowinfo;  /* IPv6 traffic class &amp;amp; flow info */
    struct in6_addr sin6_addr;      /* IPv6 address */
    uint32_t        sin6_scope_id;  /* set of interfaces for a scope */
};
&lt;/code&gt;
    &lt;p&gt;Similarly, Rust’s &lt;code&gt;SocketAddrV4&lt;/code&gt; mirrors the &lt;code&gt;sockaddr_in&lt;/code&gt; struct. There isn’t a similar RFC for &lt;code&gt;sockaddr_in&lt;/code&gt;; the de facto standard is Berkeley sockets, designed in 1983. The Linux man page for &lt;code&gt;sockaddr_in&lt;/code&gt; defines it as:&lt;/p&gt;
    &lt;code&gt;struct sockaddr_in {
    sa_family_t     sin_family;     /* AF_INET */
    in_port_t       sin_port;       /* Port number */
    struct in_addr  sin_addr;       /* IPv4 address */
};
&lt;/code&gt;
    &lt;p&gt;So &lt;code&gt;sin6_flowinfo&lt;/code&gt;, which includes the Traffic Class, is part of &lt;code&gt;sockaddr_in6&lt;/code&gt;, but the very similar DSCP field is not part of &lt;code&gt;sockaddr_in&lt;/code&gt;. Why? I’m not entirely sure about this, but here’s an attempt to reconstruct a history:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QoS was not originally part of the 1980s Berkeley sockets specification.&lt;/item&gt;
      &lt;item&gt;DSCP came about much later (RFC 2474, 1998).&lt;/item&gt;
      &lt;item&gt;Because C structs do not provide encapsulation, the &lt;code&gt;sockaddr_in&lt;/code&gt;definition was set in stone and couldn’t be changed.&lt;/item&gt;
      &lt;item&gt;So instead, the DSCP field is set as an option on the socket, via &lt;code&gt;setsockopt&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;By the time IPv6 came around, it was pretty clear that QoS was important, so the Traffic Class was baked into the &lt;code&gt;sockaddr_in6&lt;/code&gt;struct.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(Even if &lt;code&gt;sockaddr_in&lt;/code&gt; could be extended to have this field, would it be a good idea to do so? Put a pin in this for now.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Flow Labels#&lt;/head&gt;
    &lt;p&gt;RFC 2460 says that the Flow Label is “experimental and subject to change”. The RFC was written back in 1998, over a quarter-century ago—has anyone found a use for it since then?&lt;/p&gt;
    &lt;p&gt;RFC 6437, published in 2011, attempts to specify semantics for IPv6 Flow Labels. Section 2 of the RFC says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The 20-bit Flow Label field in the IPv6 header [RFC2460] is used by a node to label packets of a flow. […] Packet classifiers can use the triplet of Flow Label, Source Address, and Destination Address fields to identify the flow to which a particular packet belongs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The RFC says that Flow Labels can potentially be used by routers for load balancing, where they can use the triplet source address, destination address, flow label to figure out that a series of packets are all associated with each other. But this is an internal implementation detail generated by the source program, and not something IPv6 users copy/pasting an address generally have to think about. So it makes sense that it isn’t part of the textual representation.&lt;/p&gt;
    &lt;p&gt;RFC 6294 surveys Flow Label use cases, and some of the ones mentioned are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;as a pseudo-random value that can be used as part of a hash key for load balancing, or&lt;/item&gt;
      &lt;item&gt;as extra QoS bits on top of the 8 bits provided by the Traffic Class field.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But this Stack Exchange answer by Andrei Korshikov says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Nowadays […] there [are] no clear advantages of additional 20-bit QoS field over existent Traffic Class (Differentiated Class of Service) field. So “Flow Label” is still waiting for its meaningful usage.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my view, putting &lt;code&gt;flowinfo&lt;/code&gt; in &lt;code&gt;sockaddr_in6&lt;/code&gt; was an understandable choice given the optimism around QoS in 1998, but it was a bit of a mistake in hindsight. The Flow Label field never found widespread adoption, and the Traffic Class field is more of an application-level concern. In general, I think there should be a separation between types that are losslessly serializable and types that are not, and &lt;code&gt;sockaddr_in6&lt;/code&gt; violates this expectation. Making the Traffic Class (QoS) a socket option, like in IPv4, avoids these serialization issues.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is &lt;code&gt;scope_id&lt;/code&gt;?#&lt;/head&gt;
    &lt;p&gt;What about the other additional field, &lt;code&gt;scope_id&lt;/code&gt;? What does it mean, and why does it not have to be zeroed out?&lt;/p&gt;
    &lt;p&gt;The documentation for a &lt;code&gt;SocketAddrV6&lt;/code&gt; says that in its textual representation, the scope identifier is included after the IPv6 address and a &lt;code&gt;%&lt;/code&gt; character, within square brackets. So, for example, the following code sample:&lt;/p&gt;
    &lt;code&gt;let addr = SocketAddrV6::new("::1".parse().unwrap(), 80, 0, 42);
println!("{}", addr);
&lt;/code&gt;
    &lt;p&gt;prints out &lt;code&gt;[::1%42]:80&lt;/code&gt;. What does this field mean?&lt;/p&gt;
    &lt;p&gt;The reason &lt;code&gt;scope_id&lt;/code&gt; exists has to do with link-local addressing. Imagine you connect two computers directly to each other via, say, an Ethernet cable. There isn’t a central server telling the computers which addresses to use, or anything similar—in this situation, how can the two computers talk to each other?&lt;/p&gt;
    &lt;p&gt;To address this issue, OS vendors came up with the idea to just assign random addresses on each end of the link. The behavior is defined in RFC 3927, section 2.1:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When a host wishes to configure an IPv4 Link-Local address, it selects an address using a pseudo-random number generator with a uniform distribution in the range from 169.254.1.0 to 169.254.254.255 inclusive.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(You might have seen these 169.254 addresses on your home computers if your router is down. Those are link-local addresses.)&lt;/p&gt;
    &lt;p&gt;Sounds simple enough, right? But there is a pretty big problem with this approach: what if a computer has more than one interface on which a link-local address has been established? When a program tries to send some data over the network, the computer has to know which interface to send the data out on. But with multiple link-local interfaces, the outbound one becomes ambiguous. This is described in section 6.3 of the RFC:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Address Ambiguity&lt;/p&gt;
      &lt;p&gt;Application software run on a multi-homed host that supports IPv4 Link-Local address configuration on more than one interface may fail.&lt;/p&gt;
      &lt;p&gt;This is because application software assumes that an IPv4 address is unambiguous, that it can refer to only one host. IPv4 Link-Local addresses are unique only on a single link. A host attached to multiple links can easily encounter a situation where the same address is present on more than one interface, or first on one interface, later on another; in any case associated with more than one host. […]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The IPv6 protocol designers took this lesson to heart. Every time an IPv6-capable computer connects to a network, it establishes a link-local address starting with &lt;code&gt;fe80::&lt;/code&gt;. (You should be able to see this address via &lt;code&gt;ip addr&lt;/code&gt; on Linux, or your OS’s equivalent.) But if you’re connected to multiple networks, all of them will have addresses beginning with &lt;code&gt;fe80::&lt;/code&gt;. Now if an application wants to establish a connection to a computer in this &lt;code&gt;fe80::&lt;/code&gt; range, how can it tell the OS which interface to use?&lt;/p&gt;
    &lt;p&gt;That’s exactly where &lt;code&gt;scope_id&lt;/code&gt; comes in: it allows the &lt;code&gt;SocketAddrV6&lt;/code&gt; to specify which network interface to use. Each interface has an index associated with it, which you can see on Linux with &lt;code&gt;ip addr&lt;/code&gt;. When I run that command, I see:&lt;/p&gt;
    &lt;code&gt;% ip addr
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
2: enp4s0f0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 9000 qdisc mq state UP group default qlen 1000
    [...]
    inet6 fe80::11fe:b754:2233:afb9/64 scope link noprefixroute
       valid_lft forever preferred_lft forever
3: enp4s0f1: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc mq state DOWN group default qlen 1000
4: wlp13s0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; listed here are all the indexes that can be used as the scope ID. Let’s try pinging our address:&lt;/p&gt;
    &lt;code&gt;% ping6 fe80::11fe:b754:2233:afb9
ping6: Warning: IPv6 link-local address on ICMP datagram socket may require ifname or scope-id =&amp;gt; use: address%&amp;lt;ifname|scope-id&amp;gt;
PING fe80::11fe:b754:2233:afb9 (fe80::11fe:b754:2233:afb9) 56 data bytes
&lt;/code&gt;
    &lt;p&gt;Aha! The warning tells us that for a link-local address, the scope ID needs to be specified. Let’s try that using the &lt;code&gt;%&lt;/code&gt; syntax:&lt;/p&gt;
    &lt;code&gt;% ping6 fe80::11fe:b754:2233:afb9%2
PING fe80::11fe:b754:2233:afb9%2 (fe80::11fe:b754:2233:afb9%enp4s0f0) 56 data bytes
64 bytes from fe80::11fe:b754:2233:afb9%enp4s0f0: icmp_seq=1 ttl=64 time=0.050 ms
64 bytes from fe80::11fe:b754:2233:afb9%enp4s0f0: icmp_seq=2 ttl=64 time=0.052 ms
&lt;/code&gt;
    &lt;p&gt;Success! What if we try a different scope ID?&lt;/p&gt;
    &lt;code&gt;% ping6 fe80::11fe:b754:2233:afb9%3
PING fe80::11fe:b754:2233:afb9%3 (fe80::11fe:b754:2233:afb9%enp4s0f1) 56 data bytes
^C
--- fe80::11fe:b754:2233:afb9%3 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2080ms
&lt;/code&gt;
    &lt;p&gt;This makes sense: the address is only valid for scope ID 2 (the &lt;code&gt;enp4s0f0&lt;/code&gt; interface). When we told &lt;code&gt;ping6&lt;/code&gt; to use a different scope, 3, the address was no longer reachable. This neatly solves the 169.254 problem with IPv4 addresses.&lt;/p&gt;
    &lt;p&gt;Since scope IDs can help disambiguate the interface on which a connection ought to be made, it does make sense to include this field in &lt;code&gt;SocketAddrV6&lt;/code&gt;, as well as in its textual representation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scope ID portability#&lt;/head&gt;
    &lt;p&gt;The keen-eyed among you may have noticed that the &lt;code&gt;ping6&lt;/code&gt; commands above printed out an alternate representation: &lt;code&gt;fe80::11fe:b754:2233:afb9%enp4s0f0&lt;/code&gt;. The &lt;code&gt;enp4s0f0&lt;/code&gt; at the end is the network interface that corresponds to the numeric scope ID. Many programs can handle this representation, but Rust’s &lt;code&gt;SocketAddrV6&lt;/code&gt; can’t.&lt;/p&gt;
    &lt;p&gt;Another thing you might have noticed is that the scope ID only makes sense on a particular computer. A scope ID such as &lt;code&gt;2&lt;/code&gt; means different things on different computers. So the scope ID is roundtrip serializable, but not portable across machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;In this post we started off by looking at a somewhat strange inconsistency and ended up deep in the IPv6 specification. In our case, the &lt;code&gt;SocketAddrV6&lt;/code&gt; instances were always for internal services talking to each other without any QoS considerations, so &lt;code&gt;flowinfo&lt;/code&gt; was always zero. Given that knowledge, we were okay adjusting the property-based tests to always generate instances where &lt;code&gt;flowinfo&lt;/code&gt; was set to zero. (Here’s the PR as landed.)&lt;/p&gt;
    &lt;p&gt;Still, it raises questions: Should we wrap &lt;code&gt;SocketAddrV6&lt;/code&gt; in a newtype that enforces this constraint? Should &lt;code&gt;serde&lt;/code&gt; provide a non-standard alternate serializer that also includes the &lt;code&gt;flowinfo&lt;/code&gt; field? Should &lt;code&gt;Debug&lt;/code&gt; not forward to &lt;code&gt;Display&lt;/code&gt; when &lt;code&gt;Display&lt;/code&gt; hides fields? Should Rust have had separate types from the start? (Probably too late now.) And should Berkeley sockets not have included &lt;code&gt;flowinfo&lt;/code&gt; at all, given that it makes the type impossible to represent as text without loss?&lt;/p&gt;
    &lt;p&gt;The lesson it really drives home for me is how important the principle of least surprise can be. Both &lt;code&gt;Ipv4Addr&lt;/code&gt; and &lt;code&gt;Ipv6Addr&lt;/code&gt; have lossless textual representations, and &lt;code&gt;SocketAddrV4&lt;/code&gt; does as well. By analogy it would seem like &lt;code&gt;SocketAddrV6&lt;/code&gt; would, too, and yet it does not!&lt;/p&gt;
    &lt;p&gt;IPv6 learned so much from IPv4’s mistakes, and yet its designers couldn’t help but make some mistakes of their own. This makes sense: the designers could only see the problems they were solving then, just as we can only see those we’re solving now—and just as we encounter problems with their solutions, future generations will encounter problems with ours.&lt;/p&gt;
    &lt;p&gt;Thanks to Fiona, and several of my colleagues at Oxide, for reviewing drafts of this post.&lt;/p&gt;
    &lt;p&gt;Discuss on Hacker News and Lobsters.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;This is why our Rust map crate where keys can borrow from values,&lt;/p&gt;&lt;code&gt;iddqd&lt;/code&gt;, serializes its maps as lists or sequences. ↩︎&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Requests for Discussion we use at Oxide are inspired by RFCs, though we use a slightly different term (RFD) to convey the fact that our documents are less set in stone than IETF RFCs are. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The two fields sum up to 28 bits, and the&lt;/p&gt;&lt;code&gt;flowinfo&lt;/code&gt;field is a&lt;code&gt;u32&lt;/code&gt;, so there’s four bits remaining. I couldn’t find documentation for these four bits anywhere—they appear to be unused padding in the&lt;code&gt;u32&lt;/code&gt;. If you know about these bits, please let me know! ↩︎&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://sunshowers.io/posts/socketaddrv6-not-roundtrip/"/><published>2025-11-04T17:43:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45813767</id><title>Codemaps: Understand Code, Before You Vibe It</title><updated>2025-11-04T23:34:38.395227+00:00</updated><content>&lt;doc fingerprint="3fc096052cb3ae5f"&gt;
  &lt;main&gt;
    &lt;quote&gt;“Your code is your understanding of the problem you’re exploring. So it’s only when you have your code in your head that you really understand the problem.” — Paul Graham&lt;/quote&gt;
    &lt;p&gt;Software development only becomes engineering with understanding. Your ability to reason through your most challenging coding tasks is constrained by your mental model of how things work — in other words, how quickly and how well you onboard to any codebase for solving any problem. However most AI vibe coding tools are aimed at relieving you of that burden by reading → thinking → writing the code for you, increasing the separation from you and your code. This is fine for low value, commodity tasks, but absolutely unacceptable for the hard, sensitive, and high value work that defines real engineering.&lt;/p&gt;
    &lt;p&gt;We all need more AI that turns your brain ON, not OFF.&lt;/p&gt;
    &lt;p&gt;Today we are announcing Windsurf Codemaps, which are first-of-its-kind AI-annotated structured maps of your code, powered by SWE-1.5 and Claude Sonnet 4.5. Building on our popular work from DeepWiki and Ask Devin, Codemaps is the next step in hyper-contextualized codebase understanding, grounded in precise code navigation.&lt;/p&gt;
    &lt;p&gt;Every engineering task — debugging, refactors, new features — starts with understanding. Great engineers aren’t just good at writing code; they’re good at reading it, building mental models that span files, layers, and systems.&lt;/p&gt;
    &lt;p&gt;But modern codebases are sprawling: hundreds of files, multiple services, dense abstractions. Based on own experience and deep conversations with our customers across the Fortune 500, even top engineers spend much of their deep-work time finding and remembering what matters.&lt;/p&gt;
    &lt;p&gt;It’s a huge tax on productivity:&lt;/p&gt;
    &lt;p&gt;This is the frontier that AI coding tools haven’t yet solved. Onboarding isn’t even a onetime cost, you pay it every time you switch context and codebases. The faster and better you understand your codebase, the faster and better you’ll be able to fix it yourself, or prompt agents to do it.&lt;/p&gt;
    &lt;p&gt;Until today, the standard approach by Copilot, Claude Code, Codex, and even Windsurf Cascade, was to have you ask questions of a generalist agent with access to your code in a typical chat experience. But those solutions don’t solve focused onboarding and strongly grounded navigation to onboard, debug, and better context engineer for your codebase.&lt;/p&gt;
    &lt;p&gt;At Cognition, we’ve been investing far more deeply in understanding:&lt;/p&gt;
    &lt;p&gt;Codemaps is our next investment in tooling that makes engineers the best versions of themselves.&lt;/p&gt;
    &lt;p&gt;When you first open Codemaps (click the new maps icon or Cmd+Shift+C in Windsurf) with a codebase opened in Windsurf, you can enter in a prompt for the task you are trying to do, or take one of the automatic suggestions. You can choose a Fast (SWE-1.5) or Smart (Sonnet 4.5) model to generate your Codemap. Every Codemap is snapshots your code and respects ZDR.&lt;/p&gt;
    &lt;p&gt;Based on our demos to customers, you will experience Codemaps best on your own codebase and asking a question about how or where some functionality works. In our dogfooding, we find particular effectiveness tracing through client-server problems or a data pipeline or debugging auth/security issues:&lt;/p&gt;
    &lt;p&gt;If all you wanted was to quickly jump through grouped and nested parts of your code that related to your question, this is already an improvement compared to asking the same question in Cascade, where answers are not as densely linked to the exact lines of code.&lt;/p&gt;
    &lt;p&gt;You can also toggle over to a visually drawn Codemap, which performs the same functions when you click on individual nodes: they send you to the exact part of the codebase you clicked on.&lt;/p&gt;
    &lt;p&gt;However, if you want a little more context, then you can hit “See more” in any section to expand our “trace guide” that gives a more descriptive explanation of what groups the discovered lines together.&lt;/p&gt;
    &lt;p&gt;Finally, inside Cascade you can also reference a codemap for the agent with &lt;code&gt;@{codemap}&lt;/code&gt; (all of it, or a particular subsection) in your prompt to provide more specific context and dramatically improve the performance of your agent for your task.&lt;/p&gt;
    &lt;p&gt;We feel that the popular usage of “vibe coding” has strayed far from the original intent, into a blanket endorsement of plowing through any and all AI generated code slop. If you look at the difference between the most productive vs the problematic AI-assisted coders, the productive ones can surf the vibes of code that they understand well, whereas people get into trouble when the code they generate and maintain starts to outstrip their ability to understand it.&lt;/p&gt;
    &lt;p&gt;To understand is to be accountable. As AI takes on more of the easy work, the hard problems left to humans are the ones that demand real comprehension: debugging complex systems, refactoring legacy code, making architecture decisions. In this new era, the engineer’s role shifts from authoring to accountability — you might not write every line, but you’re still responsible for what ships. That accountability depends on understanding what the AI produced, why it changed, and whether it’s safe. Codemaps closes that gap by giving both the human and the AI a shared picture of the system: how it’s structured, how data flows, where dependencies live. Codemaps is our latest Fast Agent, but as we discussed in the Semi-Async Valley of Death, our goal isn't just about speed, it is to help your human engineers stay in flow, stay on top of their code, and to move faster and more confidently on the hardest problems, never shipping slop that they don't understand.&lt;/p&gt;
    &lt;p&gt;Augment engineers for high value work, relieve them of low value work. The other local minima that the coding agent industry has gotten stuck in is in the general messaging of replacing engineers for low value work and not having any solutions for the hardest tasks apart from “pls ultrathink high, no mistakes”, which only gives autonomy to the agent, at the expense of the engineer. The long history of human-machine collaboration teaches us that we can always do more with the synergy rather than humans-alone or AI-alone. Our view is that the AI product that engineers will love most is the one that makes them better at their job, not the one that tries to replace them with a sloppy facsimile of themselves.&lt;/p&gt;
    &lt;p&gt;With Codemaps, we are now exposing to humans some of the indexing and analysis we do inside of our coding agents. These artifacts are sharable today across teams for learning and discussion, but we have yet to benchmark how much better they can make our coding agents like Devin and Cascade in solving challenging tasks on their own. We also see opportunities for connecting and annotating codemaps, as well as defining an open &lt;code&gt;.codemap&lt;/code&gt; protocol that can be used by other code agents and custom tooling built by you. Complementing our Fast Context feature, this is an advancement in human-readable automatic context engineering.&lt;/p&gt;
    &lt;p&gt;You can try Codemaps on the latest versions of Windsurf, or DeepWiki!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cognition.ai/blog/codemaps"/><published>2025-11-04T17:47:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816041</id><title>Send this article to your friend who still thinks the cloud is a good idea</title><updated>2025-11-04T23:34:38.266928+00:00</updated><content/><link href="https://rameerez.com/send-this-article-to-your-friend-who-still-thinks-the-cloud-is-a-good-idea/"/><published>2025-11-04T21:22:15+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816195</id><title>74% of CEOs worry AI failures could cost them their jobs</title><updated>2025-11-04T23:34:37.876645+00:00</updated><content>&lt;doc fingerprint="bb5bd26b2eb375f7"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;News&lt;/item&gt;
      &lt;item&gt;2 min read&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;74% of CEOs worry AI failures could cost them their jobs: Report&lt;/head&gt;
    &lt;p&gt; According to Dataiku’s Global AI Confessions Report: CEO Edition, 74% of CEOs fear losing their jobs within two years if they fail to deliver AI-driven results. Nearly 70% of executives expect leadership shakeups due to failed AI strategies, while 54% acknowledge that competitors have a stronger AI advantage. With AI governance failures, regulatory hurdles, and AI’s growing role in decision-making, leaders face mounting pressure to act. The report underscores the urgent need for CEOs to drive AI execution or risk being replaced. &lt;/p&gt;
    &lt;p&gt; Global CEOs are under mounting pressure to turn AI ambition into measurable business gains, or risk losing their jobs. According to the Global AI Confessions Report: CEO Edition by Dataiku, 74% of CEOs fear job loss within two years if they fail to deliver AI-driven results. Conducted by The Harris Poll, the study surveyed over 500 CEOs across the US, UK, France, and Germany, exposing a growing crisis in executive accountability and AI strategy execution.&lt;lb/&gt;AI-driven leadership shakeups on the horizon&lt;lb/&gt;AI’s expanding role in leadership and decision-making&lt;lb/&gt;The report highlights a radical shift in corporate governance, with AI challenging traditional leadership structures:&lt;lb/&gt;The 'AI commodity trap' and governance failures&lt;lb/&gt;While CEOs recognise AI’s potential, many overlook critical risks:&lt;lb/&gt;Regulatory uncertainty stalls AI adoption&lt;lb/&gt;AI strategy now a defining factor in CEO and corporate survival&lt;lb/&gt;With 78% of CEOs prioritising AI as a core business goal for 2025, and 83% acknowledging AI’s impact on investor confidence, leadership now hinges on AI execution. Failure to deliver AI-driven results won’t just cost companies their competitive edge, it could cost CEOs their jobs. &lt;/p&gt;
    &lt;p&gt;AI-driven leadership shakeups on the horizon&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;70% of CEOs predict AI failures will lead to executive oustings this year, signaling an industry-wide shift in leadership expectations.&lt;/item&gt;
      &lt;item&gt;54% acknowledge that competitors already have more advanced AI strategies, increasing pressure to accelerate AI adoption.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI’s expanding role in leadership and decision-making&lt;/p&gt;
    &lt;p&gt;The report highlights a radical shift in corporate governance, with AI challenging traditional leadership structures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;94% of CEOs believe AI agents could provide equal or better counsel than human board members.&lt;/item&gt;
      &lt;item&gt;89% say AI can develop stronger strategic plans than at least one of their executive leaders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The 'AI commodity trap' and governance failures&lt;/p&gt;
    &lt;p&gt;While CEOs recognise AI’s potential, many overlook critical risks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;87% assume off-the-shelf AI is as effective as custom-built solutions, risking poor execution in complex industries.&lt;/item&gt;
      &lt;item&gt;35% of AI initiatives are suspected to be ‘AI washing,’ where projects prioritise optics over actual business impact.&lt;/item&gt;
      &lt;item&gt;94% of CEOs believe employees use AI tools like ChatGPT, Claude, and Midjourney without company approval (“shadow AI”), exposing governance failures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Regulatory uncertainty stalls AI adoption&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;80% of CEOs fear AI could harm employees, and 83% worry about unintended customer impact.&lt;/item&gt;
      &lt;item&gt;37% have delayed AI projects due to unclear regulations, while 32% have canceled initiatives altogether.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AI strategy now a defining factor in CEO and corporate survival&lt;/p&gt;
    &lt;p&gt;With 78% of CEOs prioritising AI as a core business goal for 2025, and 83% acknowledging AI’s impact on investor confidence, leadership now hinges on AI execution. Failure to deliver AI-driven results won’t just cost companies their competitive edge, it could cost CEOs their jobs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://cfo.economictimes.indiatimes.com/news/74-of-ceos-worry-ai-failures-could-cost-them-their-jobs-report/118923383"/><published>2025-11-04T21:39:56+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816879</id><title>Patching 68K Software – SimpleText</title><updated>2025-11-04T23:34:37.637253+00:00</updated><content>&lt;doc fingerprint="b2497ef403174708"&gt;
  &lt;main&gt;
    &lt;p&gt;Someone asked to have SimpleText open a smaller text window at startup. Initially, I assumed this would be a fairly easy fix by just overwriting a few constant values in SimpleText code. It turned out to be a pain -- but I learned a lot along the way.&lt;lb/&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;lb/&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;lb/&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;lb/&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;lb/&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;lb/&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;lb/&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;lb/&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;lb/&gt;A couple of other tricks.&lt;lb/&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;lb/&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;lb/&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;lb/&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;lb/&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;lb/&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;lb/&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;lb/&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;lb/&gt;Attached is the hacked version of SimpleText.&lt;lb/&gt;- David&lt;/p&gt;
    &lt;p&gt;You need to have the code editor (from one of the Apple developer CDs) in your ResEdit preference file in order to disassemble code resources within ResEdit. Then, open each 'CODE' resource of SimpleText and search for _SizeWindow (A91D). Or, just skim the code until you see system calls that look like they have something to do with windows. Here is a nice chunk in routine Anon48. A new window is created, then it looks at the main device, looks at the size of the menu bar (MBarHeight), sets the port, and moves and sizes the window.&lt;/p&gt;
    &lt;p&gt;But, uh oh, earlier in the code it checks what type of window it is opening. A patch is going to need to avoid resizing code for pictures, video, and the about box.&lt;/p&gt;
    &lt;p&gt;I could not locate any obvious constants to adjust. Instead, I would need to inject a more complicated routine that detects whether it was a text window and substitute a new rectangle for the size. But, you cannot simply insert code, as it would move all subsequent code down, and the jumps (subroutine calls) that cross over that location would now jump to the wrong spots. So, I need to jump out of their code to my own (appended at the end of the code resource) and return.&lt;/p&gt;
    &lt;p&gt;For example, SimpleText's code to get the size of MBarHeight can easily be performed elsewhere. My routine need only return the MBarHeight in register D0 before returning. That gives me 8 bytes to overwrite with my jump.&lt;/p&gt;
    &lt;p&gt;Here is my replacement code. It still is only 8 bytes. But, I now jump to my subroutine, check the result, and jump over their resizing code if my routine says it is changing the window size.&lt;/p&gt;
    &lt;p&gt;In my subroutine, I make sure all the needed information is in registers (which I checked that SimpleText was not using), I call my various functions and then perform any work that was lost from overwriting or jumping over the original code. Specifically, I get the MBarHeight into D0 and set the current port to the window.&lt;/p&gt;
    &lt;p&gt;Easy! But, later in the code, SimpleText reads the content of whatever document is being opened and once again resizes the window. So, I needed to patch later code as well. How could I determine at that point that a replacement window size was being used? I simply store the result of the first subroutine (see SetRecentResult above) and then check it on later calls.&lt;/p&gt;
    &lt;p&gt;Where could I store this information? it is not possible to add global variables and the registers are all reused by SimpleText between the first and second routines. Well, you can store a variable (or an entire structure with many variables) within the code itself. Here is my little workaround for CodeWarrior.&lt;/p&gt;
    &lt;p&gt;A couple of other tricks.&lt;/p&gt;
    &lt;p&gt;1. The system routine GetHandleSize has some glue code (they intercept the call in a local library before calling the system). I needed this call, but didn't want to add the weight of CodeWarrior's libary. So, I defined the direct call to GetHandleSize (I didn't need the glue fix).&lt;/p&gt;
    &lt;p&gt;2. You can pass any of the scratch registers (D0-D2, A0-A1, FP0-FP3) to a C function. The way of defining that in CodeWarrior is noted below. You cannot use any other registers. To make debugging easier, I wrote the original subroutine as a true C function, and the register-&amp;gt;C function as a wrapper.&lt;/p&gt;
    &lt;p&gt;3. CodeWarrior does not support BSR for some reason. Use JSR instead. Also, a called routine must be placed before the caller routine in order to generate a short relative JSR rather than absolute address. See my 'RecentResult' example above, where the routines that call RecentResult are placed after it in code.&lt;/p&gt;
    &lt;p&gt;4. SimpleText stores literals (strings, constants) at the end of the 'CODE' resource. After that is where I placed my code. Unfortunately, this breaks disassembly in ResEdit. Below, do you see 'A9FF'? That's the '_Debugger' trap call. It is follow by the rest of the code, and the MacsBug symbols for "SimpleTextWindowChoicePrep'&lt;/p&gt;
    &lt;p&gt;I then needed to hand compute the JSR patched in the original SimpleText code to this location at the end of the code resource.&lt;/p&gt;
    &lt;p&gt;5. To make it easy to redefine window sizes in the future, I added a resource.&lt;/p&gt;
    &lt;p&gt;The ResEdit definition of this resource is the TMPL. By the way, I have experienced corruption twice with ResEdit 2.1.3. Perhaps it has a bug with templates?&lt;/p&gt;
    &lt;p&gt;I doubt this information will be useful to most people. However, it may help avoid some frustrating issues for those few people that attempt patching old software.&lt;/p&gt;
    &lt;p&gt;Attached is the hacked version of SimpleText.&lt;/p&gt;
    &lt;p&gt;- David&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tinkerdifferent.com/threads/patching-68k-software-simpletext.4793/"/><published>2025-11-04T22:59:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816946</id><title>Border Patrol agent testifies sandwich thrown at him "exploded all over,"</title><updated>2025-11-04T23:34:37.455770+00:00</updated><content>&lt;doc fingerprint="5acee4dce501667d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Border Patrol agent testifies sandwich thrown at him "exploded all over," left mustard stains&lt;/head&gt;
    &lt;p&gt;A Customs and Border Patrol Agent testified in federal court on Tuesday that a sandwich "exploded all over him" after a D.C. man threw a sub-style sandwich at him during an expletive-filled rant in protest of President Trump's federal policing crackdown and National Guard deployment in the nation's capital this summer.&lt;/p&gt;
    &lt;p&gt;Sean Dunn, who was charged with one count of misdemeanor assault charges, has become a symbol of resistance against Mr. Trump's D.C. policing policies, sat in a crowded courtroom as spectators, jury members, and even witnesses attempted to keep a straight face during the first day of his criminal trial.&lt;/p&gt;
    &lt;p&gt;"He did it, he threw the sandwich," Dunn's attorney Julia Gatto said in her opening statement to a D.C. jury, adding their client has "very strong feelings" about the Trump administration's federal takeover of D.C.&lt;/p&gt;
    &lt;p&gt;Gatto called the viral sub-toss a "harmless gesture that did not, could not, cause injury."&lt;/p&gt;
    &lt;p&gt;Customs and Border Patrol Agent Gregory Lairmore, who was hit with the footlong sub, testified Tuesday about his experience dealing with Dunn, who can be seen in video of the incident yelling at the agent and other officers in his vicinity before throwing the wrapped sandwich at the officer's chest. Dunn attempted to flee on foot before being apprehended, documents and video of the incident shows.&lt;/p&gt;
    &lt;p&gt;According to charging documents, Dunn yelled, "F*** you! You f***ing fascists! Why are you here? I don't want you in my city," before crossing the street. He later returned and threw the sandwich.&lt;/p&gt;
    &lt;p&gt;Lairmore, who says he caught most of Dunn's ire before catching the sandwich in his ballistic vest, saying at one point Dunn was "red-faced" and "enraged" by the police presence at a crowded intersection in Northwest Washington, D.C.&lt;/p&gt;
    &lt;p&gt;To laughs in the room, Lairmore walked the jury through the "baseball pitch" of a sub throw, as another witness put it.&lt;/p&gt;
    &lt;p&gt;Lairmore said he "could feel it through his ballistic vest" and it "exploded all over" him after the Subway stack hit him. He said he "could smell the onions and mustard" on his uniform, and even had an onion string hanging by his police radio later that night. The fast-food mustard, he said, stained his shirt.&lt;/p&gt;
    &lt;p&gt;Dunn's attorneys later pressed Lairmore on two gag gifts that his coworkers bought him after the incident, including a plush submarine sandwich and a "felony footlong" patch that Lairmore said he put on his lunchbox.&lt;/p&gt;
    &lt;p&gt;They also pressed Lairmore on why there are no evidentiary photos of stains on his shirt or of the sandwich after it was thrown, only a video posted to social media platform Instagram from a bystander showing the sandwich mostly intact. Lairmore said the Metropolitan Police Department in D.C. took over the investigation after Dunn was detained, and Lairmore said the sandwich appeared at least "bent and out of shape" in its wrapper.&lt;/p&gt;
    &lt;p&gt;Federal prosecutors failed to secure a felony indictment from a grand jury in Washington in the immediate aftermath of the incident, and instead charged Dunn with a federal misdemeanor assault charge for allegedly assaulting, resisting, opposing, impeding, intimidating and interfering with a federal officer.&lt;/p&gt;
    &lt;p&gt;After his arrest, Dunn was fired from his job as a paralegal in the Justice Department. According to a Justice Department source, Dunn worked at the Office of International Affairs within the department's Criminal Division as a paralegal.&lt;/p&gt;
    &lt;p&gt;In a post on X announcing Dunn's firing, Attorney General Pam Bondi called Dunn "an example of the Deep State we have been up against."&lt;/p&gt;
    &lt;p&gt;Jeanine Pirro, the U.S. Attorney for Washington, D.C., said in a video announcing the arrest that her office is "going to back the police to the hilt. So there, stick your Subway sandwich somewhere else."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cbsnews.com/news/sean-dunn-trial-dc-sandwich-thrower-testimony-onions-mustard/"/><published>2025-11-04T23:07:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45816981</id><title>WASM 45% slower than Native Code</title><updated>2025-11-04T23:34:37.031658+00:00</updated><content>&lt;doc fingerprint="6f09809a4d733ca8"&gt;
  &lt;main&gt;
    &lt;p&gt;spacing=nonfrench&lt;/p&gt;
    &lt;head rend="h1"&gt;Not So Fast: &lt;lb/&gt;Analyzing the Performance of WebAssembly vs. Native Code&lt;/head&gt;
    &lt;head rend="h6"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;All major web browsers now support WebAssembly, a low-level bytecode intended to serve as a compilation target for code written in languages like C and C++. A key goal of WebAssembly is performance parity with native code; previous work reports near parity, with many applications compiled to WebAssembly running on average slower than native code. However, this evaluation was limited to a suite of scientific kernels, each consisting of roughly 100 lines of code. Running more substantial applications was not possible because compiling code to WebAssembly is only part of the puzzle: standard Unix APIs are not available in the web browser environment. To address this challenge, we build Browsix-Wasm, a significant extension to Browsix [29] that, for the first time, makes it possible to run unmodified WebAssembly-compiled Unix applications directly inside the browser. We then use Browsix-Wasm to conduct the first large-scale evaluation of the performance of WebAssembly vs. native. Across the SPEC CPU suite of benchmarks, we find a substantial performance gap: applications compiled to WebAssembly run slower by an average of 45% (Firefox) to 55% (Chrome), with peak slowdowns of (Firefox) and (Chrome). We identify the causes of this performance degradation, some of which are due to missing optimizations and code generation issues, while others are inherent to the WebAssembly platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;1 Introduction&lt;/head&gt;
    &lt;p&gt;Web browsers have become the most popular platform for running user-facing applications, and until recently, JavaScript was the only programming language supported by all major web browsers. Beyond its many quirks and pitfalls from the perspective of programming language design, JavaScript is also notoriously difficult to compile efficiently [12, 17, 31, 30]. Applications written in or compiled to JavaScript typically run much slower than their native counterparts. To address this situation, a group of browser vendors jointly developed WebAssembly.&lt;/p&gt;
    &lt;p&gt;WebAssembly is a low-level, statically typed language that does not require garbage collection, and supports interoperability with JavaScript. The goal of WebAssembly is to serve as a universal compiler target that can run in a browser [16, 15, 18].111The WebAssembly standard is undergoing active development, with ongoing efforts to extend WebAssembly with features ranging from SIMD primitives and threading to tail calls and garbage collection. This paper focuses on the initial and stable version of WebAssembly [18], which is supported by all major browsers. Towards this end, WebAssembly is designed to be fast to compile and run, to be portable across browsers and architectures, and to provide formal guarantees of type and memory safety. Prior attempts at running code at native speed in the browser [13, 14, 4, 38], which we discuss in related work, do not satisfy all of these criteria.&lt;/p&gt;
    &lt;p&gt;WebAssembly is now supported by all major browsers [34, 8] and has been swiftly adopted by several programming languages. There are now backends for C, C++, C#, Go, and Rust [39, 24, 2, 1] that target WebAssembly. A curated list currently includes more than a dozen others [10]. Today, code written in these languages can be safely executed in browser sandboxes across any modern device once compiled to WebAssembly.&lt;/p&gt;
    &lt;p&gt;A major goal of WebAssembly is to be faster than JavaScript. For example, the paper that introduced WebAssembly [18] showed that when a C program is compiled to WebAssembly instead of JavaScript (asm.js), it runs 34% faster in Google Chrome. That paper also showed that the performance of WebAssembly is competitive with native code: of the 24 benchmarks evaluated, the running time of seven benchmarks using WebAssembly is within 10% of native code, and almost all of them are less than slower than native code. Figure 1 shows that WebAssembly implementations have continuously improved with respect to these benchmarks. In 2017, only seven benchmarks performed within 1.1 of native, but by 2019, this number increased to 13.&lt;/p&gt;
    &lt;p&gt;These results appear promising, but they beg the question: are these 24 benchmarks representative of WebAssembly’s intended use cases?&lt;/p&gt;
    &lt;head rend="h5"&gt;The Challenge of Benchmarking WebAssembly&lt;/head&gt;
    &lt;p&gt;The aforementioned suite of 24 benchmarks is the PolybenchC benchmark suite [5], which is designed to measure the effect of polyhedral loop optimizations in compilers. All the benchmarks in the suite are small scientific computing kernels rather than full applications (e.g., matrix multiplication and LU Decomposition); each is roughly 100 LOC. While WebAssembly is designed to accelerate scientific kernels on the Web, it is also explicitly designed for a much richer set of full applications.&lt;/p&gt;
    &lt;p&gt;The WebAssembly documentation highlights several intended use cases [7], including scientific kernels, image editing, video editing, image recognition, scientific visualization, simulations, programming language interpreters, virtual machines, and POSIX applications. Therefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC do not imply that it will perform well given a different kind of application.&lt;/p&gt;
    &lt;p&gt;We argue that a more comprehensive evaluation of WebAssembly should rely on an established benchmark suite of large programs, such as the SPEC CPU benchmark suites. In fact, the SPEC CPU 2006 and 2017 suite of benchmarks include several applications that fall under the intended use cases of WebAssembly: eight benchmarks are scientific applications (e.g., 433.milc, 444.namd, 447.dealII, 450.soplex, and 470.lbm), two benchmarks involve image and video processing (464.h264ref and 453.povray), and all of the benchmarks are POSIX applications.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it is not possible to simply compile a sophisticated native program to WebAssembly. Native programs, including the programs in the SPEC CPU suites, require operating system services, such as a filesystem, synchronous I/O, and processes, which WebAssembly and the browser do not provide. The SPEC benchmarking harness itself requires a file system, a shell, the ability to spawn processes, and other Unix facilities. To overcome these limitations when porting native applications to the web, many programmers painstakingly modify their programs to avoid or mimic missing operating system services. Modifying well-known benchmarks, such as SPEC CPU, would not only be time consuming but would also pose a serious threat to validity.&lt;/p&gt;
    &lt;p&gt;The standard approach to running these applications today is to use Emscripten, a toolchain for compiling C and C++ to WebAssembly [39]. Unfortunately, Emscripten only supports the most trivial system calls and does not scale up to large-scale applications. For example, to enable applications to use synchronous I/O, the default Emscripten MEMFS filesystem loads the entire filesystem image into memory before the program begins executing. For SPEC, these files are too large to fit into memory.&lt;/p&gt;
    &lt;p&gt;A promising alternative is to use Browsix, a framework that enables running unmodified, full-featured Unix applications in the browser [28, 29]. Browsix implements a Unix-compatible kernel in JavaScript, with full support for processes, files, pipes, blocking I/O, and other Unix features. Moreover, it includes a C/C++ compiler (based on Emscripten) that allows programs to run in the browser unmodified. The Browsix case studies include complex applications, such as LaTeX, which runs entirely in the browser without any source code modifications.&lt;/p&gt;
    &lt;p&gt;Unfortunately, Browsix is a JavaScript-only solution, since it was built before the release of WebAssembly. Moreover, Browsix suffers from high performance overhead, which would be a significant confounder while benchmarking. Using Browsix, it would be difficult to tease apart the poorly performing benchmarks from performance degradation introduced by Browsix.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contributions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; • &lt;p&gt;Browsix-Wasm: We develop Browsix-Wasm, a significant extension to and enhancement of Browsix that allows us to compile Unix programs to WebAssembly and run them in the browser with no modifications. In addition to integrating functional extensions, Browsix-Wasm incorporates performance optimizations that drastically improve its performance, ensuring that CPU-intensive applications operate with virtually no overhead imposed by Browsix-Wasm ().&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; • &lt;p&gt;Browsix-SPEC: We develop Browsix-SPEC, a harness that extends Browsix-Wasm to allow automated collection of detailed timing and hardware on-chip performance counter information in order to perform detailed measurements of application performance ().&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; • &lt;p&gt;Performance Analysis of WebAssembly: Using Browsix-Wasm and Browsix-SPEC, we conduct the first comprehensive performance analysis of WebAssembly using the SPEC CPU benchmark suite (both 2006 and 2017). This evaluation confirms that WebAssembly does run faster than JavaScript (on average 1.3 faster across SPEC CPU). However, contrary to prior work, we find a substantial gap between WebAssembly and native performance: code compiled to WebAssembly runs on average 1.55 slower in Chrome and 1.45 slower in Firefox than native code ().&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; • &lt;p&gt;Root Cause Analysis and Advice for Implementers: We conduct a forensic analysis with the aid of performance counter results to identify the root causes of this performance gap. We find the following results:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt; 1. &lt;p&gt;The instructions produced by WebAssembly have more loads and stores than native code (2.02 more loads and 2.30 more stores in Chrome; 1.92 more loads and 2.16 more stores in Firefox). We attribute this to reduced availability of registers, a sub-optimal register allocator, and a failure to effectively exploit a wider range of x86 addressing modes.&lt;/p&gt;&lt;/item&gt;&lt;item&gt; 2. &lt;p&gt;The instructions produced by WebAssembly have more branches, because WebAssembly requires several dynamic safety checks.&lt;/p&gt;&lt;/item&gt;&lt;item&gt; 3. &lt;p&gt;Since WebAssembly generates more instructions, it leads to more L1 instruction cache misses.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We provide guidance to help WebAssembly implementers focus their optimization efforts in order to close the performance gap between WebAssembly and native code ().&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; 1. &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Browsix-Wasm and Browsix-SPEC are available at https://browsix.org.&lt;/p&gt;
    &lt;head rend="h2"&gt;2 From Browsix to Browsix-Wasm&lt;/head&gt;
    &lt;p&gt;Browsix [29] mimics a Unix kernel within the browser and includes a compiler (based on Emscripten [39, 33]) that compiles native programs to JavaScript. Together, they allow native programs (in C, C++, and Go) to run in the browser and freely use operating system services, such as pipes, processes, and a filesystem. However, Browsix has two major limitations that we must overcome. First, Browsix compiles native code to JavaScript and not WebAssembly. Second, the Browsix kernel has significant performance issues. In particular, several common system calls have very high overhead in Browsix, which makes it hard to compare the performance of a program running in Browsix to that of a program running natively. We address these limitations by building a new in-browser kernel called Browsix-Wasm, which supports WebAssembly programs and eliminates the performance bottlenecks of Browsix.&lt;/p&gt;
    &lt;head rend="h5"&gt;Emscripten Runtime Modifications&lt;/head&gt;
    &lt;p&gt;Browsix modifies the Emscripten compiler to allow processes (which run in WebWorkers) to communicate with the Browsix kernel (which runs on the main thread of a page). Since Browsix compiles native programs to JavaScript, this is relatively straightforward: each process’ memory is a buffer that is shared with the kernel (a SharedArrayBuffer), thus system calls can directly read and write process memory. However, this approach has two significant drawbacks. First, it precludes growing the heap on-demand; the shared memory must be sized large enough to meet the high-water-mark heap size of the application for the entire life of the process. Second, JavaScript contexts (like the main context and each web worker context) have a fixed limit on their heap sizes, which is currently approximately 2.2 GB in Google Chrome [6]. This cap imposes a serious limitation on running multiple processes: if each process reserves a 500 MB heap, Browsix would only be able to run at most four concurrent processes. A deeper problem is that WebAssembly memory cannot be shared across WebWorkers and does not support the Atomic API, which Browsix processes use to wait for system calls.&lt;/p&gt;
    &lt;p&gt;Browsix-Wasm uses a different approach to process-kernel communication that is also faster than the Browsix approach. Browsix-Wasm modifies the Emscripten runtime system to create an auxiliary buffer (of 64MB) for each process that is shared with the kernel, but is distinct from process memory. Since this auxiliary buffer is a SharedArrayBuffer the Browsix-Wasm process and kernel can use Atomic API for communication. When a system call references strings or buffers in the process’s heap (e.g., writev or stat), its runtime system copies data from the process memory to the shared buffer and sends a message to the kernel with locations of the copied data in auxiliary memory. Similarly, when a system call writes data to the auxiliary buffer (e.g., read), its runtime system copies the data from the shared buffer to the process memory at the memory specified. Moreover, if a system call specifies a buffer in process memory for the kernel to write to (e.g., read), the runtime allocates a corresponding buffer in auxiliary memory and passes it to the kernel. In case the system call is either reading or writing data of size more than 64MB, Browsix-Wasm divides this call into several calls such that each call only reads or writes at maximum 64MB of data. The cost of these memory copy operations is dwarfed by the overall cost of the system call invocation, which involves sending a message between process and kernel JavaScript contexts. We show in §4.2.1 that Browsix-Wasm has negligible overhead.&lt;/p&gt;
    &lt;head rend="h5"&gt;Performance Optimization&lt;/head&gt;
    &lt;p&gt;While building Browsix-Wasm and doing our preliminary performance evaluation, we discovered several performance issues in parts of the Browsix kernel. Left unresolved, these performance issues would be a threat to the validity of a performance comparison between WebAssembly and native code. The most serious case was in the shared filesystem component included with Browsix/Browsix-Wasm, BrowserFS. Originally, on each append operation on a file, BrowserFS would allocate a new, larger buffer, copying the previous and new contents into the new buffer. Small appends could impose substantial performance degradation. Now, whenever a buffer backing a file requires additional space, BrowserFS grows the buffer by at least 4 KB. This change alone decreased the time the 464.h264ref benchmark spent in Browsix from 25 seconds to under 1.5 seconds. We made a series of improvements that reduce overhead throughout Browsix-Wasm. Similar, if less dramatic, improvements were made to reduce the number of allocations and the amount of copying in the kernel implementation of pipes.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 Browsix-SPEC&lt;/head&gt;
    &lt;p&gt;To reliably execute WebAssembly benchmarks while capturing performance counter data, we developed Browsix-SPEC. Browsix-SPEC works with Browsix-Wasm to manage spawning browser instances, serving benchmark assets (e.g., the compiled WebAssembly programs and test inputs), spawning perf processes to record performance counter data, and validating benchmark outputs.&lt;/p&gt;
    &lt;p&gt;We use Browsix-SPEC to run three benchmark suites to evaluate WebAssembly’s performance: SPEC CPU2006, SPEC CPU2017, and PolyBenchC. These benchmarks are compiled to native code using Clang 4.0, and WebAssembly using Browsix-Wasm. We made no modifications to Chrome or Firefox, and the browsers are run with their standard sandboxing and isolation features enabled. Browsix-Wasm is built on top of standard web platform features and requires no direct access to host resources – instead, benchmarks make standard HTTP requests to Browsix-SPEC.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.1 Browsix-SPEC Benchmark Execution&lt;/head&gt;
    &lt;p&gt;Figure 2 illustrates the key pieces of Browsix-SPEC in play when running a benchmark, such as 401.bzip2 in Chrome. First (1), the Browsix-SPEC benchmark harness launches a new browser instance using a WebBrowser automation tool, Selenium.222https://www.seleniumhq.org/ (2) The browser loads the page’s HTML, harness JS, and Browsix-Wasm kernel JS over HTTP from the benchmark harness. (3) The harness JS initializes the Browsix-Wasm kernel and starts a new Browsix-Wasm process executing the runspec shell script (not shown in Figure 2). runspec in turn spawns the standard specinvoke (not shown), compiled from the C sources provided in SPEC 2006. specinvoke reads the speccmds.cmd file from the Browsix-Wasm filesystem and starts 401.bzip2 with the appropriate arguments. (4) After the WebAssembly module has been instantiated but before the benchmark’s main function is invoked, the Browsix-Wasm userspace runtime does an XHR request to Browsix-SPEC to begin recording performance counter stats. (5) The benchmark harness finds the Chrome thread corresponding to the Web Worker 401.bzip2 process and attaches perf to the process. (6) At the end of the benchmark, the Browsix-Wasm userspace runtime does a final XHR to the benchmark harness to end the perf record process. When the runspec program exits (after potentially invoking the test binary several times), the harness JS POSTs (7) a tar archive of the SPEC results directory to Browsix-SPEC. After Browsix-SPEC receives the full results archive, it unpacks the results to a temporary directory and validates the output using the cmp tool provided with SPEC 2006. Finally, Browsix-SPEC kills the browser process and records the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 Evaluation&lt;/head&gt;
    &lt;p&gt;We use Browsix-Wasm and Browsix-SPEC to evaluate the performance of WebAssembly using three benchmark suites: SPEC CPU2006, SPEC CPU2017, and PolyBenchC. We include PolybenchC benchmarks for comparison with the original WebAssembly paper [18], but argue that these benchmarks do not represent typical workloads. The SPEC benchmarks are representative and require Browsix-Wasm to run successfully. We run all benchmarks on a 6-Core Intel Xeon E5-1650 v3 CPU with hyperthreading and 64 GB of RAM running Ubuntu 16.04 with Linux kernel v4.4.0. We run all benchmarks using two state-of-the-art browsers: Google Chrome 74.0 and Mozilla Firefox 66.0. We compile benchmarks to native code using Clang 4.0333The flags to Clang are -O2 -fno-strict-aliasing. and to WebAssembly using Browsix-Wasm (which is based on Emscripten with Clang 4.0).444Browsix-Wasm runs Emscripten with the flags -O2 -s TOTAL_MEMORY=1073741824 -s ALLOW_MEMORY_GROWTH=1 -fno-strict-aliasing. Each benchmark was executed five times. We report the average of all running times and the standard error. The execution time measured is the difference between wall clock time when the program starts, i.e. after WebAssembly JIT compilation concludes, and when the program ends.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.1 PolyBenchC Benchmarks&lt;/head&gt;
    &lt;p&gt;Haas et al. [18] used PolybenchC to benchmark WebAssembly implementations because the PolybenchC benchmarks do not make system calls. As we have already argued, the PolybenchC benchmarks are small scientific kernels that are typically used to benchmark polyhedral optimization techniques, and do not represent larger applications. Nevertheless, it is still valuable for us to run PolybenchC with Browsix-Wasm, because it demonstrates that our infrastructure for system calls does not have any overhead. Figure 3(a) shows the execution time of the PolyBenchC benchmarks in Browsix-Wasm and when run natively. We are able to reproduce the majority of the results from the original WebAssembly paper [18]. We find that Browsix-Wasm imposes a very low overhead: an average of 0.2% and a maximum of 1.2%.&lt;/p&gt;
    &lt;head rend="h3"&gt;4.2 SPEC Benchmarks&lt;/head&gt;
    &lt;p&gt;We now evaluate Browsix-Wasm using the C/C++ benchmarks from SPEC CPU2006 and SPEC CPU2017 (the new C/C++ benchmarks and the speed benchmarks), which use system calls extensively. We exclude four data points that either do not compile to WebAssembly555400.perlbench, 403.gcc, 471.omnetpp, and 456.hmmer from SPEC CPU2006 do not compile with Emscripten. or allocate more memory than WebAssembly allows.666From SPEC CPU2017, the ref dataset of 638.imagick_s and 657.xz_s require more than 4 GB RAM. However, these benchmarks do work with their test dataset. Table 1 shows the absolute execution times of the SPEC benchmarks when running with Browsix-Wasm in both Chrome and Firefox, and when running natively.&lt;/p&gt;
    &lt;p&gt;WebAssembly performs worse than native for all benchmarks except for 429.mcf and 433.milc. In Chrome, WebAssembly’s maximum overhead is 2.5 over native and 7 out of 15 benchmarks have a running time within 1.5 of native. In Firefox, WebAssembly is within 2.08 of native and performs within 1.5 of native for 7 out of 15 benchmarks. On average, WebAssembly is 1.55 slower than native in Chrome, and 1.45 slower than native in Firefox. Table 2 shows the time required to compile the SPEC benchmarks using Clang and Chrome. (To the best of our knowledge, Firefox cannot report WebAssembly compile times.) In all cases, the compilation time is negligible compared to the execution time. However, the Clang compiler is orders of magnitude slower than the WebAssembly compiler. Finally, note that Clang compiles benchmarks from C++ source code, whereas Chrome compiles WebAssembly, which is a simpler format than C++.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Benchmark&lt;/cell&gt;
        &lt;cell&gt;Native&lt;/cell&gt;
        &lt;cell role="head"&gt;Google Chrome&lt;/cell&gt;
        &lt;cell role="head"&gt;Mozilla Firefox&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;401.bzip2&lt;/cell&gt;
        &lt;cell&gt;370 0.6&lt;/cell&gt;
        &lt;cell&gt;864 6.4&lt;/cell&gt;
        &lt;cell&gt;730 1.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;429.mcf&lt;/cell&gt;
        &lt;cell&gt;221 0.1&lt;/cell&gt;
        &lt;cell&gt;180 0.9&lt;/cell&gt;
        &lt;cell&gt;184 0.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;433.milc&lt;/cell&gt;
        &lt;cell&gt;375 2.6&lt;/cell&gt;
        &lt;cell&gt;369 0.5&lt;/cell&gt;
        &lt;cell&gt;378 0.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;444.namd&lt;/cell&gt;
        &lt;cell&gt;271 0.8&lt;/cell&gt;
        &lt;cell&gt;369 9.1&lt;/cell&gt;
        &lt;cell&gt;373 1.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;445.gobmk&lt;/cell&gt;
        &lt;cell&gt;352 2.1&lt;/cell&gt;
        &lt;cell&gt;537 0.8&lt;/cell&gt;
        &lt;cell&gt;549 3.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;450.soplex&lt;/cell&gt;
        &lt;cell&gt;179 3.7&lt;/cell&gt;
        &lt;cell&gt;265 1.2&lt;/cell&gt;
        &lt;cell&gt;238 0.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;453.povray&lt;/cell&gt;
        &lt;cell&gt;110 1.9&lt;/cell&gt;
        &lt;cell&gt;275 1.3&lt;/cell&gt;
        &lt;cell&gt;229 1.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;458.sjeng&lt;/cell&gt;
        &lt;cell&gt;358 1.4&lt;/cell&gt;
        &lt;cell&gt;602 2.5&lt;/cell&gt;
        &lt;cell&gt;580 2.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;462.libquantum&lt;/cell&gt;
        &lt;cell&gt;330 0.8&lt;/cell&gt;
        &lt;cell&gt;444 0.2&lt;/cell&gt;
        &lt;cell&gt;385 0.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;464.h264ref&lt;/cell&gt;
        &lt;cell&gt;389 0.7&lt;/cell&gt;
        &lt;cell&gt;807 11.0&lt;/cell&gt;
        &lt;cell&gt;733 2.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;470.lbm&lt;/cell&gt;
        &lt;cell&gt;209 1.1&lt;/cell&gt;
        &lt;cell&gt;248 0.3&lt;/cell&gt;
        &lt;cell&gt;249 0.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;473.astar&lt;/cell&gt;
        &lt;cell&gt;299 0.5&lt;/cell&gt;
        &lt;cell&gt;474 3.5&lt;/cell&gt;
        &lt;cell&gt;408 1.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;482.sphinx3&lt;/cell&gt;
        &lt;cell&gt;381 7.1&lt;/cell&gt;
        &lt;cell&gt;834 1.8&lt;/cell&gt;
        &lt;cell&gt;713 3.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;641.leela_s&lt;/cell&gt;
        &lt;cell&gt;466 2.7&lt;/cell&gt;
        &lt;cell&gt;825 4.6&lt;/cell&gt;
        &lt;cell&gt;717 1.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;644.nab_s&lt;/cell&gt;
        &lt;cell&gt;2476 11&lt;/cell&gt;
        &lt;cell&gt;3639 5.6&lt;/cell&gt;
        &lt;cell&gt;3829 6.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Slowdown: geomean&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;1.55&lt;/cell&gt;
        &lt;cell&gt;1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Slowdown: median&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;1.53&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Benchmark&lt;/cell&gt;
        &lt;cell&gt;Clang 4.0&lt;/cell&gt;
        &lt;cell role="head"&gt;Google Chrome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;401.bzip2&lt;/cell&gt;
        &lt;cell&gt;1.9 0.018&lt;/cell&gt;
        &lt;cell&gt;0.53 0.005&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;429.mcf&lt;/cell&gt;
        &lt;cell&gt;0.3 0.003&lt;/cell&gt;
        &lt;cell&gt;0.15 0.005&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;433.milc&lt;/cell&gt;
        &lt;cell&gt;2.2 0.02&lt;/cell&gt;
        &lt;cell&gt;0.3 0.003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;444.namd&lt;/cell&gt;
        &lt;cell&gt;4.6 0.02&lt;/cell&gt;
        &lt;cell&gt;0.78 0.004&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;445.gobmk&lt;/cell&gt;
        &lt;cell&gt;12.1 0.2&lt;/cell&gt;
        &lt;cell&gt;1.4 0.014&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;450.soplex&lt;/cell&gt;
        &lt;cell&gt;6.9 0.01&lt;/cell&gt;
        &lt;cell&gt;1.2 0.009&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;453.povray&lt;/cell&gt;
        &lt;cell&gt;15.3 0.03&lt;/cell&gt;
        &lt;cell&gt;1.2 0.012&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;458.sjeng&lt;/cell&gt;
        &lt;cell&gt;1.9 0.01&lt;/cell&gt;
        &lt;cell&gt;0.35 0.001&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;462.libquantum&lt;/cell&gt;
        &lt;cell&gt;6.9 0.03&lt;/cell&gt;
        &lt;cell&gt;0.15 0.002&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;464.h264ref&lt;/cell&gt;
        &lt;cell&gt;10.3 0.06&lt;/cell&gt;
        &lt;cell&gt;1.0 0.03&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;470.lbm&lt;/cell&gt;
        &lt;cell&gt;0.3 0.001&lt;/cell&gt;
        &lt;cell&gt;0.14 0.004&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;473.astar&lt;/cell&gt;
        &lt;cell&gt;0.73 0.005&lt;/cell&gt;
        &lt;cell&gt;0.24 0.004&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;482.sphinx3&lt;/cell&gt;
        &lt;cell&gt;3.0 0.04&lt;/cell&gt;
        &lt;cell&gt;0.48 0.007&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;641.leela_s&lt;/cell&gt;
        &lt;cell&gt;4.3 0.05&lt;/cell&gt;
        &lt;cell&gt;0.74 0.003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;644.nab_s&lt;/cell&gt;
        &lt;cell&gt;4.1 0.03&lt;/cell&gt;
        &lt;cell&gt;0.41 0.001&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;4.2.1 Browsix-Wasm Overhead&lt;/head&gt;
    &lt;p&gt;It is important to rule out the possibility that the slowdown that we report is due to poor performance in our implementation of Browsix-Wasm. In particular, Browsix-Wasm implements system calls without modifying the browser, and system calls involve copying data (§2), which may be costly. To quantify the overhead of Browsix-Wasm, we instrumented its system calls to measure all time spent in Browsix-Wasm. Figure 4 shows the percentage of time spent in Browsix-Wasm in Firefox using the SPEC benchmarks. For 14 of the 15 benchmarks, the overhead is less than 0.5%. The maximum overhead is 1.2%. On average, the overhead of Browsix-Wasm is only 0.2%. Therefore, we conclude that Browsix-Wasm has negligible overhead and does not substantially affect the performance counter results of programs executed in WebAssembly.&lt;/p&gt;
    &lt;head rend="h4"&gt;4.2.2 Comparison of WebAssembly and asm.js&lt;/head&gt;
    &lt;p&gt;A key claim in the original work on WebAssembly was that it is significantly faster than asm.js. We now test that claim using the SPEC benchmarks. For this comparison, we modified Browsix-Wasm to also support processes compiled to asm.js. The alternative would have been to benchmark the asm.js processes using the original Browsix. However, as we discussed earlier, Browsix has performance problems that would have been a threat to the validity of our results. Figure 5 shows the speedup of the SPEC benchmarks using WebAssembly, relative to their running time using asm.js using both Chrome and Firefox. WebAssembly outperforms asm.js in both browsers: the mean speedup is 1.54 in Chrome and 1.39 in Firefox.&lt;/p&gt;
    &lt;p&gt;Since the performance difference between Chrome and Firefox is substantial, in Figure 6 we show the speedup of each benchmark by selecting the best-performing browser for WebAssembly and the best-performing browser of asm.js (i.e., they may be different browsers). These results show that WebAssembly consistently performs better than asm.js, with a mean speedup of 1.3. Haas et al. [18] also found that WebAssembly gives a mean speedup of 1.3 over asm.js using PolyBenchC.&lt;/p&gt;
    &lt;head rend="h2"&gt;5 Case Study: Matrix Multiplication&lt;/head&gt;
    &lt;p&gt;In this section, we illustrate the performance differences between WebAssembly and native code using a C function that performs matrix multiplication, as shown in Figure 7(a). Three matrices are provided as arguments to the function, and the results of A () and B () are stored in C (), where are constants defined in the program.&lt;/p&gt;
    &lt;p&gt;In WebAssembly, this function is – slower than native in both Chrome and Firefox with a variety of matrix sizes (Figure 8). We compiled the function with -O2 and disabled automatic vectorization, since WebAssembly does not support vectorized instructions.&lt;/p&gt;
    &lt;p&gt;Figure 7(b) shows native code generated for the matmul function by clang-4.0. Arguments are passed to the function in the rdi, rsi, and rdx registers, as specified in the System V AMD64 ABI calling convention [9]. Lines 7(b) - 7(b) are the body of the first loop with iterator i stored in r8d. Lines 7(b) - 7(b) contain the body of the second loop with iterator k stored in r9d. Lines 7(b) - 7(b) comprise the body of the third loop with iterator j stored in rcx. Clang is able to eliminate a cmp instruction in the inner loop by initializing rcx with , incrementing rcx on each iteration at line 7(b), and using jne to test the zero flag of the status register, which is set to 1 when rcx becomes 0.&lt;/p&gt;
    &lt;p&gt;Figure 7(c) shows x86-64 code JITed by Chrome for the WebAssembly compiled version of matmul. This code has been modified slightly – nops in the generated code have been removed for presentation. Function arguments are passed in the rax, rcx, and rdx registers, following Chrome’s calling convention. At lines 1– 3, the contents of registers rax, rdx, and rcx are stored on the stack, due to registers spills at lines 7 - 9. Lines 7–45 are the body of the first loop with iterator i stored in edi. Lines 18–42 contain the body of second loop with iterator k stored in r11. Lines 27–39 are the body of the third loop with iterator j stored in eax. To avoid memory loads due to register spilling at lines 7– 9 in the first iteration of the first loop, an extra jump is generated at line 5. Similarly, extra jumps are generated for the second and third loops at line 16 and line 25 respectively.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.1 Differences&lt;/head&gt;
    &lt;p&gt;The native code JITed by Chrome has more instructions, suffers from increased register pressure, and has extra branches compared to Clang-generated native code.&lt;/p&gt;
    &lt;head rend="h4"&gt;5.1.1 Increased Code Size&lt;/head&gt;
    &lt;p&gt;The number of instructions in the code generated by Chrome (Figure 7(c)) is 53, including nops, while clang generated code (Figure 7(b)) consists of only 28 instructions. The poor instruction selection algorithm of Chrome is one of the reasons for increased code size.&lt;/p&gt;
    &lt;p&gt;Additionally, Chrome does not take advantage of all available memory addressing modes for x86 instructions. In Figure 7(b) Clang uses the add instruction at line 7(b) with register addressing mode, loading from and writing to a memory address in the same operation. Chrome on the other hand loads the address in ecx, adds the operand to ecx, finally storing ecx at the address, requiring 3 instructions rather than one on lines 3537.&lt;/p&gt;
    &lt;head rend="h4"&gt;5.1.2 Increased Register Pressure&lt;/head&gt;
    &lt;p&gt;Code generated by Clang in Figure 7(b) does not generate any spills and uses only 10 registers. On the other hand, the code generated by Chrome (Figure 7(c)) uses 13 general purpose registers – all available registers (r13 and r10 are reserved by V8). As described in Section 5.1.1, eschewing the use of the register addressing mode of the add instruction requires the use of a temporary register. All of this register inefficiency compounds, introducing three register spills to the stack at lines 1–3. Values stored on the stack are loaded again into registers at lines 7–9 and line 18.&lt;/p&gt;
    &lt;head rend="h4"&gt;5.1.3 Extra Branches&lt;/head&gt;
    &lt;p&gt;Clang (Figure 7(b)) generates code with a single branch per loop by inverting the loop counter (line 7(b)). In contrast, Chrome (Figure 7(c)) generates more straightforward code, which requires a conditional jump at the start of the loop. In addition, Chrome generates extra jumps to avoid memory loads due to register spills in the first iteration of a loop. For example, the jump at line 5 avoids the spills at lines 7– 9.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 Performance Analysis&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;perf Event&lt;/cell&gt;
        &lt;cell&gt;Wasm Summary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;all-loads-retired (r81d0) (Figure 9(a))&lt;/cell&gt;
        &lt;cell&gt;Increased register&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;all-stores-retired (r82d0) (Figure 9(b))&lt;/cell&gt;
        &lt;cell&gt;pressure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;branches-retired (r00c4) (Figure 9(c))&lt;/cell&gt;
        &lt;cell&gt;More branch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;conditional-branches (r01c4) (Figure 9(d))&lt;/cell&gt;
        &lt;cell&gt;statements&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;instructions-retired (r1c0) (Figure 9(e))&lt;/cell&gt;
        &lt;cell&gt;Increased code size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;cpu-cycles (Figure 9(f))&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;L1-icache-load-misses (Figure 10)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We use Browsix-SPEC to record measurements from all supported performance counters on our system for the SPEC CPU benchmarks compiled to WebAssembly and executed in Chrome and Firefox, and the SPEC CPU benchmarks compiled to native code (Section 3).&lt;/p&gt;
    &lt;p&gt;Table 3 lists the performance counters we use here, along with a summary of the impact of Browsix-Wasm performance on these counters compared to native. We use these results to explain the performance overhead of WebAssembly over native code. Our analysis shows that the inefficiences described in Section 5 are pervasive and translate to reduced performance across the SPEC CPU benchmark suite.&lt;/p&gt;
    &lt;head rend="h3"&gt;6.1 Increased Register Pressure&lt;/head&gt;
    &lt;p&gt;This section focuses on two performance counters that show the effect of increased register pressure. Figure 9(a) presents the number of load instructions retired by WebAssembly-compiled SPEC benchmarks in Chrome and Firefox, relative to the number of load instructions retired in native code. Similarly, Figure 9(b) shows the number of store instructions retired. Note that a “retired” instruction is an instruction which leaves the instruction pipeline and its results are correct and visible in the architectural state (that is, not speculative).&lt;/p&gt;
    &lt;p&gt;Code generated by Chrome has 2.02 more load instructions retired and 2.30 more store instructions retired than native code. Code generated by Firefox has 1.92 more load instructions retired and 2.16 more store instructions retired than native code. These results show that the WebAssembly-compiled SPEC CPU benchmarks suffer from increased register pressure and thus increased memory references. Below, we outline the reasons for this increased register pressure.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.1.1 Reserved Registers&lt;/head&gt;
    &lt;p&gt;In Chrome, matmul generates three register spills but does not use two x86-64 registers: r13 and r10 (Figure 7(c), lines 7– 9). This occurs because Chrome reserves these two registers.777https://github.com/v8/v8/blob/7.4.1/src/x64/register-x64.h For the JavaScript garbage collector, Chrome reserves r13 to point to an array of GC roots at all times. In addition, Chrome uses r10 and xmm13 as dedicated scratch registers. Similarly, Firefox reserves r15 as a pointer to the start of the heap, and r11 and xmm15 are JavaScript scratch registers.888https://hg.mozilla.org/mozilla-central/file/tip/js/src/jit/x64/Assembler-x64.h None of these registers are available to WebAssembly code.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.1.2 Poor Register Allocation&lt;/head&gt;
    &lt;p&gt;Beyond a reduced set of registers available to allocate, both Chrome and Firefox do a poor job of allocating the registers they have. For example, the code generated by Chrome for matmul uses 12 registers while the native code generated by Clang only uses 10 registers (Section 5.1.2). This increased register usage—in both Firefox and Chrome—is because of their use of fast but not particularly effective register allocators. Chrome and Firefox both use a linear scan register allocator [36], while Clang uses a greedy graph-coloring register allocator [3], which consistently generates better code.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.1.3 x86 Addressing Modes&lt;/head&gt;
    &lt;p&gt;The x86-64 instruction set offers several addressing modes for each operand, including a register mode, where the instruction reads data from register or writes data to a register, and memory address modes like register indirect or direct offset addressing, where the operand resides in a memory address and the instruction can read from or write to that address. A code generator could avoid unnecessary register pressure by using the latter modes. However, Chrome does not take advantage of these modes. For example, the code generated by Chrome for matmul does not use the register indirect addressing mode for the add instruction (Section 5.1.2), creating unnecessary register pressure.&lt;/p&gt;
    &lt;head rend="h3"&gt;6.2 Extra Branch Instructions&lt;/head&gt;
    &lt;p&gt;This section focuses on two performance counters that measure the number of branch instructions executed. Figure 9(c) shows the number of branch instructions retired by WebAssembly, relative to the number of branch instructions retired in native code. Similarly, Figure 9(d) shows the number of conditional branch instructions retired. In Chrome, there are and more unconditional and conditional branch instructions retired respectively, whereas in Firefox, there are and more retired. These results show that all the SPEC CPU benchmarks incur extra branches, and we explain why below.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.2.1 Extra Jump Statements for Loops&lt;/head&gt;
    &lt;p&gt;As with matmul (Section 5.1.3), Chrome generates unnecessary jump statements for loops, leading to significantly more branch instructions than Firefox.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.2.2 Stack Overflow Checks Per Function Call&lt;/head&gt;
    &lt;p&gt;A WebAssembly program tracks the current stack size with a global variable that it increases on every function call. The programmer can define the maximum stack size for the program. To ensure that a program does not overflow the stack, both Chrome and Firefox add stack checks at the start of each function to detect if the current stack size is less than the maximum stack size. These checks includes extra comparison and conditional jump instructions, which must be executed on every function call.&lt;/p&gt;
    &lt;head rend="h4"&gt;6.2.3 Function Table Indexing Checks&lt;/head&gt;
    &lt;p&gt;WebAssembly dynamically checks all indirect calls to ensure that the target is a valid function and that the function’s type at runtime is the same as the type specified at the call site. In a WebAssembly module, the function table stores the list of functions and their types, and the code generated by WebAssembly uses the function table to implement these checks. These checks are required when calling function pointers and virtual functions in C/C++. The checks lead to extra comparison and conditional jump instructions, which are executed before every indirect function call.&lt;/p&gt;
    &lt;head rend="h3"&gt;6.3 Increased Code Size&lt;/head&gt;
    &lt;p&gt;The code generated by Chrome and Firefox is considerably larger than the code generated by Clang. We use three performance counters to measure this effect. (i) Figure 9(e) shows the number of instructions retired by benchmarks compiled to WebAssembly and executed in Chrome and Firefox relative to the number of instructions retired in native code. Similarly, Figure 9(f) shows the relative number of CPU cycles spent by benchmarks compiled to WebAssembly, and Figure 10 shows the relative number of L1 instruction cache load misses.&lt;/p&gt;
    &lt;p&gt;Figure 9(e) shows that Chrome executes an average of 1.80 more instructions than native code and Firefox executes an average of 1.75 more instructions than native code. Due to poor instruction selection, a poor register allocator generating more register spills (Section 6.1), and extra branch statements (Section 6.2), the size of generated code for WebAssembly is greater than native code, leading to more instructions being executed. This increase in the number of instructions executed leads to increased L1 instruction cache misses in Figure 10. On average, Chrome suffers 2.83 more I-cache misses than native code, and Firefox suffers from 2.04 more L1 instruction cache misses than native code. More cache misses means that more CPU cycles are spent waiting for the instruction to be fetched.&lt;/p&gt;
    &lt;p&gt;We note one anomaly: although 429.mcf has 1.6 more instructions retired in Chrome than native code and 1.5 more instructions retired in Firefox than native code, it runs faster than native code. Figure 3(b) shows that its slowdown relative to native is 0.81 in Chrome and 0.83 in Firefox. The reason for this anomaly is attributable directly to its lower number of L1 instruction cache misses. 429.mcf contains a main loop and most of the instructions in the loop fit in the L1 instruction cache. Similarly, 433.milc performance is better due to fewer L1 instruction cache misses. In 450.soplex there are 4.6 more L1 instruction cache misses in Chrome and Firefox than native because of several virtual functions being executed, leading to more indirect function calls.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Performance Counter&lt;/cell&gt;
        &lt;cell&gt;Chrome&lt;/cell&gt;
        &lt;cell&gt;Firefox&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;all-loads-retired&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
        &lt;cell&gt;1.92&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;all-stores-retired&lt;/cell&gt;
        &lt;cell&gt;2.30&lt;/cell&gt;
        &lt;cell&gt;2.16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;branch-instructions-retired&lt;/cell&gt;
        &lt;cell&gt;1.75&lt;/cell&gt;
        &lt;cell&gt;1.65&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;conditional-branches&lt;/cell&gt;
        &lt;cell&gt;1.65&lt;/cell&gt;
        &lt;cell&gt;1.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;instructions-retired&lt;/cell&gt;
        &lt;cell&gt;1.80&lt;/cell&gt;
        &lt;cell&gt;1.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;cpu-cycles&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;1.38&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;L1-icache-load-misses&lt;/cell&gt;
        &lt;cell&gt;2.83&lt;/cell&gt;
        &lt;cell&gt;2.04&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;6.4 Discussion&lt;/head&gt;
    &lt;p&gt;It is worth asking if the performance issues identified here are fundamental. We believe that two of the identified issues are not: that is, they could be ameliorated by improved implementations. WebAssembly implementations today use register allocators (§6.1.2) and code generators (§6.2.1) that perform worse than Clang’s counterparts. However, an offline compiler like Clang can spend considerably more time to generate better code, whereas WebAssembly compilers must be fast enough to run online. Therefore, solutions adopted by other JITs, such as further optimizing hot code, are likely applicable here [19, 32].&lt;/p&gt;
    &lt;p&gt;The four other issues that we have identified appear to arise from the design constraints of WebAssembly: the stack overflow checks (§6.2.2), indirect call checks (§6.2.3), and reserved registers (§6.1.1) have a runtime cost and lead to increased code size (§6.3). Unfortunately, these checks are necessary for WebAssembly’s safety guarantees. A redesigned WebAssembly, with richer types for memory and function pointers [23], might be able to perform some of these checks at compile time, but that could complicate the implementation of compilers that produce WebAssembly. Finally, a WebAssembly implementation in a browser must interoperate with a high-performance JavaScript implementation, which may impose its own constraints. For example, current JavaScript implementations reserve a few registers for their own use, which increases register pressure on WebAssembly.&lt;/p&gt;
    &lt;head rend="h2"&gt;7 Related Work&lt;/head&gt;
    &lt;head rend="h5"&gt;Precursors to WebAssembly&lt;/head&gt;
    &lt;p&gt;There have been several attempts to execute native code in browsers, but none of them met all the design criteria of WebAssembly.&lt;/p&gt;
    &lt;p&gt;ActiveX [13] allows web pages to embed signed x86 libraries, however these binaries have unrestricted access to the Windows API. In contrast, WebAssembly modules are sandboxed. ActiveX is now a deprecated technology.&lt;/p&gt;
    &lt;p&gt;Native Client [37, 11] (NaCl) adds a module to a web application that contains platform specific machine code. NaCl introduced sandboxing techniques to execute platform specific machine code at near native speed. Since NaCl relies on static validation of machine code, it requires code generators to follow certain patterns, hence, supporting only a subset of the x86, ARM, and MIPS instructions sets in the browser. To address the inherent portability issue of NaCl, Portable NaCl (PNaCl) [14] uses LLVM Bitcode as a binary format. However, PNaCl does not provide significant improvement in compactness over NaCl and still exposes compiler and/or platform-specific details such as the call stack layout. Both have been deprecated in favor of WebAssembly.&lt;/p&gt;
    &lt;p&gt;asm.js is a subset of JavaScript designed to be compiled efficiently to native code. asm.js uses type coercions to avoid the dynamic type system of JavaScript. Since asm.js is a subset of JavaScript, adding all native features to asm.js such as 64-bit integers will first require extending JavaScript. Compared to asm.js, WebAssembly provides several improvements: (i) WebAssembly binaries are compact due to its lightweight representation compared to JavaScript source, (ii) WebAssembly is more straightforward to validate, (iii) WebAssembly provides formal guarantees of type safety and isolation, and (iv) WebAssembly has been shown to provide better performance than asm.js.&lt;/p&gt;
    &lt;p&gt;WebAssembly is a stack machine, which is similar to the Java Virtual Machine [21] and the Common Language Runtime [25]. However, WebAssembly is very different from these platforms. For example WebAssembly does not support objects and does not support unstructured control flow.&lt;/p&gt;
    &lt;p&gt;The WebAssembly specification defines its operational semantics and type system. This proof was mechanized using the Isabelle theorem prover, and that mechanization effort found and addressed a number of issues in the specification [35]. RockSalt [22] is a similar verification effort for NaCl. It implements the NaCl verification toolchain in Coq, along with a proof of correctness with respect to a model of the subset of x86 instructions that NaCl supports.&lt;/p&gt;
    &lt;head rend="h5"&gt;Analysis of SPEC Benchmarks using performance counters&lt;/head&gt;
    &lt;p&gt;Several papers use performance counters to analyze the SPEC benchmarks. Panda et al. [26] analyze the SPEC CPU2017 benchmarks, applying statistical techniques to identify similarities among benchmarks. Phansalkar et al. perform a similar study on SPEC CPU2006 [27]. Limaye and Adegija identify workload differences between SPEC CPU2006 and SPEC CPU2017 [20].&lt;/p&gt;
    &lt;head rend="h2"&gt;8 Conclusions&lt;/head&gt;
    &lt;p&gt;This paper performs the first comprehensive performance analysis of WebAssembly. We develop Browsix-Wasm, a significant extension of Browsix, and Browsix-SPEC, a harness that enables detailed performance analysis, to let us run the SPEC CPU2006 and CPU2017 benchmarks as WebAssembly in Chrome and Firefox. We find that the mean slowdown of WebAssembly vs. native across SPEC benchmarks is 1.55 for Chrome and 1.45 for Firefox, with peak slowdowns of 2.5 in Chrome and 2.08 in Firefox. We identify the causes of these performance gaps, providing actionable guidance for future optimization efforts.&lt;/p&gt;
    &lt;head rend="h5"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;We thank the reviewers and our shepherd, Eric Eide, for their constructive feedback. This work was partially supported by NSF grants 1439008 and 1413985.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[1] Blazor. https://blazor.net/. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[2] Compiling from Rust to WebAssembly. https://developer.mozilla.org/en-US/docs/WebAssembly/Rust_to_wasm. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[3] LLVM Reference Manual. https://llvm.org/docs/CodeGenerator.html.&lt;/item&gt;
      &lt;item&gt;[4] NaCl and PNaCl. https://developer.chrome.com/native-client/nacl-and-pnacl. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[5] PolyBenchC: the polyhedral benchmark suite. http://web.cs.ucla.edu/~pouchet/software/polybench/. [Online; accessed 14-March-2017].&lt;/item&gt;
      &lt;item&gt;[6] Raise Chrome JS heap limit? - Stack Overflow. https://stackoverflow.com/questions/43643406/raise-chrome-js-heap-limit. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[7] Use cases. https://webassembly.org/docs/use-cases/.&lt;/item&gt;
      &lt;item&gt;[8] WebAssembly. https://webassembly.org/. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[9] System V Application Binary Interface AMD64 Architecture Processor Supplement. https://software.intel.com/sites/default/files/article/402129/mpx-linux64-abi.pdf, 2013.&lt;/item&gt;
      &lt;item&gt;[10] Steve Akinyemi. A curated list of languages that compile directly to or have their VMs in WebAssembly. https://github.com/appcypher/awesome-wasm-langs. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[11] Jason Ansel, Petr Marchenko, Úlfar Erlingsson, Elijah Taylor, Brad Chen, Derek L. Schuff, David Sehr, Cliff L. Biffle, and Bennet Yee. Language-independent Sandboxing of Just-in-time Compilation and Self-modifying Code. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’11, pages 355–366. ACM, 2011.&lt;/item&gt;
      &lt;item&gt;[12] Michael Bebenita, Florian Brandner, Manuel Fahndrich, Francesco Logozzo, Wolfram Schulte, Nikolai Tillmann, and Herman Venter. SPUR: A Trace-based JIT Compiler for CIL. In Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA ’10, pages 708–725. ACM, 2010.&lt;/item&gt;
      &lt;item&gt;[13] David A Chappell. Understanding ActiveX and OLE. Microsoft Press, 1996.&lt;/item&gt;
      &lt;item&gt;[14] Alan Donovan, Robert Muth, Brad Chen, and David Sehr. PNaCl: Portable Native Client Executables. https://css.csail.mit.edu/6.858/2012/readings/pnacl.pdf, 2010.&lt;/item&gt;
      &lt;item&gt;[15] Brendan Eich. From ASM.JS to WebAssembly. https://brendaneich.com/2015/06/from-asm-js-to-webassembly/, 2015. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[16] Eric Elliott. What is WebAssembly? https://tinyurl.com/o5h6daj, 2015. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[17] Andreas Gal, Brendan Eich, Mike Shaver, David Anderson, David Mandelin, Mohammad R. Haghighat, Blake Kaplan, Graydon Hoare, Boris Zbarsky, Jason Orendorff, Jesse Ruderman, Edwin W. Smith, Rick Reitmaier, Michael Bebenita, Mason Chang, and Michael Franz. Trace-based Just-in-time Type Specialization for Dynamic Languages. In Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’09, pages 465–478. ACM, 2009.&lt;/item&gt;
      &lt;item&gt;[18] Andreas Haas, Andreas Rossberg, Derek L. Schuff, Ben L. Titzer, Michael Holman, Dan Gohman, Luke Wagner, Alon Zakai, and JF Bastien. Bringing the Web Up to Speed with WebAssembly. In Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2017, pages 185–200. ACM, 2017.&lt;/item&gt;
      &lt;item&gt;[19] Thomas Kotzmann, Christian Wimmer, Hanspeter Mössenböck, Thomas Rodriguez, Kenneth Russell, and David Cox. Design of the Java HotSpot Client Compiler for Java 6. ACM Trans. Archit. Code Optim., 5(1):7:1–7:32, 2008.&lt;/item&gt;
      &lt;item&gt;[20] Ankur Limaye and Tosiron Adegbija. A Workload Characterization of the SPEC CPU2017 Benchmark Suite. In 2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 149–158, 2018.&lt;/item&gt;
      &lt;item&gt;[21] Tim Lindholm, Frank Yellin, Gilad Bracha, and Alex Buckley. The Java Virtual Machine Specification, Java SE 8 Edition. Addison-Wesley Professional, 1st edition, 2014.&lt;/item&gt;
      &lt;item&gt;[22] Greg Morrisett, Gang Tan, Joseph Tassarotti, Jean-Baptiste Tristan, and Edward Gan. RockSalt: Better, Faster, Stronger SFI for the x86. In Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’12, pages 395–404. ACM, 2012.&lt;/item&gt;
      &lt;item&gt;[23] Greg Morrisett, David Walker, Karl Crary, and Neal Glew. From System F to Typed Assembly Language. ACM Trans. Program. Lang. Syst., 21(3):527–568, 1999.&lt;/item&gt;
      &lt;item&gt;[24] Richard Musiol. A compiler from Go to JavaScript for running Go code in a browser. https://github.com/gopherjs/gopherjs, 2016. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[25] George C. Necula, Scott McPeak, Shree P. Rahul, and Westley Weimer. CIL: Intermediate Language and Tools for Analysis and Transformation of C Programs. In R. Nigel Horspool, editor, Compiler Construction, pages 213–228. Springer, 2002.&lt;/item&gt;
      &lt;item&gt;[26] Reena Panda, Shuang Song, Joseph Dean, and Lizy K. John. Wait of a Decade: Did SPEC CPU 2017 Broaden the Performance Horizon? In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 271–282, 2018.&lt;/item&gt;
      &lt;item&gt;[27] Aashish Phansalkar, Ajay Joshi, and Lizy K. John. Analysis of Redundancy and Application Balance in the SPEC CPU2006 Benchmark Suite. In Proceedings of the 34th Annual International Symposium on Computer Architecture, ISCA ’07, pages 412–423. ACM, 2007.&lt;/item&gt;
      &lt;item&gt;[28] Bobby Powers, John Vilk, and Emery D. Berger. Browsix: Unix in your browser tab. https://browsix.org.&lt;/item&gt;
      &lt;item&gt;[29] Bobby Powers, John Vilk, and Emery D. Berger. Browsix: Bridging the Gap Between Unix and the Browser. In Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS ’17, pages 253–266. ACM, 2017.&lt;/item&gt;
      &lt;item&gt;[30] Gregor Richards, Sylvain Lebresne, Brian Burg, and Jan Vitek. An Analysis of the Dynamic Behavior of JavaScript Programs. In Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’10, pages 1–12. ACM, 2010.&lt;/item&gt;
      &lt;item&gt;[31] Marija Selakovic and Michael Pradel. Performance Issues and Optimizations in JavaScript: An Empirical Study. In Proceedings of the 38th International Conference on Software Engineering, ICSE ’16, pages 61–72. ACM, 2016.&lt;/item&gt;
      &lt;item&gt;[32] Toshio Suganuma, Toshiaki Yasue, Motohiro Kawahito, Hideaki Komatsu, and Toshio Nakatani. A Dynamic Optimization Framework for a Java Just-in-time Compiler. In Proceedings of the 16th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, OOPSLA ’01, pages 180–195. ACM, 2001.&lt;/item&gt;
      &lt;item&gt;[33] Luke Wagner. asm.js in Firefox Nightly | Luke Wagner’s Blog. https://blog.mozilla.org/luke/2013/03/21/asm-js-in-firefox-nightly/. [Online; accessed 21-May-2019].&lt;/item&gt;
      &lt;item&gt;[34] Luke Wagner. A WebAssembly Milestone: Experimental Support in Multiple Browsers. https://hacks.mozilla.org/2016/03/a-webassembly-milestone/, 2016. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[35] Conrad Watt. Mechanising and Verifying the WebAssembly Specification. In Proceedings of the 7th ACM SIGPLAN International Conference on Certified Programs and Proofs, CPP 2018, pages 53–65. ACM, 2018.&lt;/item&gt;
      &lt;item&gt;[36] Christian Wimmer and Michael Franz. Linear Scan Register Allocation on SSA Form. In Proceedings of the 8th Annual IEEE/ACM International Symposium on Code Generation and Optimization, CGO ’10, pages 170–179. ACM, 2010.&lt;/item&gt;
      &lt;item&gt;[37] Bennet Yee, David Sehr, Greg Dardyk, Brad Chen, Robert Muth, Tavis Ormandy, Shiki Okasaka, Neha Narula, and Nicholas Fullagar. Native Client: A Sandbox for Portable, Untrusted x86 Native Code. In IEEE Symposium on Security and Privacy (Oakland’09), IEEE, 2009.&lt;/item&gt;
      &lt;item&gt;[38] Alon Zakai. asm.js. http://asmjs.org/. [Online; accessed 5-January-2019].&lt;/item&gt;
      &lt;item&gt;[39] Alon Zakai. Emscripten: An LLVM-to-JavaScript Compiler. In Proceedings of the ACM International Conference Companion on Object Oriented Programming Systems Languages and Applications Companion, OOPSLA ’11, pages 301–312. ACM, 2011.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ar5iv.labs.arxiv.org/html/1901.09056"/><published>2025-11-04T23:13:06+00:00</published></entry></feed>