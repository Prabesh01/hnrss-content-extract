<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-18T20:35:03.518845+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45609986</id><title>When you opened a screen shot of a video in Paint, the video was playing in it</title><updated>2025-10-18T20:35:13.333812+00:00</updated><content>&lt;doc fingerprint="377fcdfb29c88adf"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;@ChenCravat In an old version of Windows (Windows 98 iirc) if you took screenshot of a video from media player and paste it into paint, and resume media player, video would play inside paint. Do you why it happened? It is still bugging me to this day.&lt;/p&gt;
      &lt;p&gt;‚Äî Yasar Arabaci @ysar.bsky.social (@y_arabaci) July 18, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the tricks for video playback is to use a green screen, more technically known as color-keying or chroma-keying.&lt;/p&gt;
    &lt;p&gt;The media player program didn‚Äôt render the video pixels to the screen. Rather, it followed this recipe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Draw solid green where you want the video to go.&lt;/item&gt;
      &lt;item&gt;Render the video pixels to a graphics surface shared with the graphics card.&lt;/item&gt;
      &lt;item&gt;Tell the graphics card that whenever it sees a green pixel about to be written to the screen, it should substitute a pixel from that shared graphics surface.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Surface&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;üèñÔ∏è&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;‚Üì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;‚Üí&lt;/cell&gt;
        &lt;cell&gt;Graphics card&lt;/cell&gt;
        &lt;cell&gt;‚Üí&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;üèñÔ∏è&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Desktop&lt;/cell&gt;
        &lt;cell&gt;Monitor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are a few advantages to this approach.&lt;/p&gt;
    &lt;p&gt;One is that the shared graphics surface need not have the same pixel format as the user‚Äôs main display. Therefore, you can specify that the shared graphics surface have a pixel format that matches that of the video, avoiding the need to do any pixel format conversions.&lt;/p&gt;
    &lt;p&gt;Another is that you can update the content without having to go through a full paint cycle. You just update the shared graphics surface, and the results are on the screen at the next frame. This lets you update the video at 60 frames per second from a background thread, which works even if the UI thread is busy or sluggish.&lt;/p&gt;
    &lt;p&gt;You can do even better if you create two shared graphics surfaces. The first one holds the contents of the video frame you want the user to see right now. And the second one is where you create the contents of the video frame you want the user to see next. And then at the vertical blank, you tell the video card to switch to the second shared graphics surface (known as ‚Äúflipping‚Äù), and the entire screen updates at once with no tearing. While the second surface is on the screen, you can render the next frame to the first surface, and then flip again at the next vertical blank. Repeat this process for each frame of the video.&lt;/p&gt;
    &lt;p&gt;A media player program of this era typically negotiated with the graphics card (via DirectDraw) to get one of these magic graphic surfaces and configure it to use it as replacement pixels. These special surfaces were called ‚Äúoverlays‚Äù because they appeared to overlay the desktop.&lt;/p&gt;
    &lt;p&gt;When you took a screen shot, you got the pixels that Windows gave to the video card as the contents of the desktop. If an overlay is active, then these are not the same pixels that came out of the video card and sent to your monitor. The computer never sees these monitor pixels; they are something generated on the fly by the graphics card and sent directly to the monitor. Your screen shot was a screen shot of the desktop screen, and it contains green pixels where the video would go.&lt;/p&gt;
    &lt;p&gt;Now, when you load the image into Paint or any other image viewer, Windows sends those green pixels to the video card, but if the media player is still running, then its overlay is still active, and if you put Paint in the same place that the media player window is, then the green pixels in Paint get changed into the pixels of the active video. The video card doesn‚Äôt know that the pixels came from Paint. Its job is to look for green pixels in a certain region of the screen and change them into the pixels from the shared surface.&lt;/p&gt;
    &lt;p&gt;If you move the Paint window to another position where it doesn‚Äôt overlap the media player, or if the media player isn‚Äôt playing a video, you will see the bitmap‚Äôs true nature: It‚Äôs just a bunch of green pixels.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs go back to the green screen analogy: Imagine you are visiting a television studio while the presenter is giving a weather report. The presenter is standing in front of a green screen, but the image that goes out to viewers contains an animating weather map where the green backdrop would normally appear. You take a picture of the green screen with your phone, and you hold up the phone to the television camera. What do the viewers at home see? Do they see a phone with a picture of a green screen? No, they see a phone with an animating weather map! And if you move your phone around, what the viewers at home see is different parts of the weather map being inserted into your phone. When you get home, your friends tell you, ‚ÄúWow, how did you do that? You took a still picture of a weather map, and when you held it up, it was animating!‚Äù&lt;/p&gt;
    &lt;p&gt;Now, while overlays are better than going through paint cycles, they still have their problems. For example, if a window moves over the media player, and it happens to have green pixels, then the video will play in that other window. If you move the media player window, it needs to move the overlay to match the media player‚Äôs new location, and in practice there is some lag to this tracking, causing it to look jerky. Also, there is a limit on the number of overlays supported by a graphics card, so if they‚Äôre all used up, then the media player has to go through the old software rendering path.&lt;/p&gt;
    &lt;p&gt;Nowadays, video rendering is no longer done with overlays. Instead, content is rendered to graphic surfaces that are associated with window. The desktop compositors takes the graphics content of all the windows, including their composition visuals, and combines them to form a full desktop image that is sent to the monitor. The desktop compositor understands window positions, so when you move the window, the composition visuals automatically move with them, so you don‚Äôt get the phenomenon of the overlay lagging the window position. The desktop compositor also understands visual transformations, so that when you hit Alt+Tab or hover over the taskbar button, the animating video is automatically resized and repositioned to match the preview thumbnail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://devblogs.microsoft.com/oldnewthing/20251014-00/?p=111681"/><published>2025-10-16T19:57:41+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45611252</id><title>K8s with 1M nodes</title><updated>2025-10-18T20:35:13.023563+00:00</updated><content>&lt;doc fingerprint="e798c941e1211d92"&gt;
  &lt;main&gt;
    &lt;p&gt;This is an effort to create a fully functional Kubernetes cluster with 1 million active nodes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Several years ago at OpenAI I helped author Scaling Kubernetes to 7500 Nodes which remains one of the CNCF‚Äôs most popular blog posts. Alibaba made a post about running Kubernetes clusters with 10K nodes. Google made a post about 15K nodes with Bayer Crop Science. Fast forward to today, GKE supports running some clusters up to 65K nodes, and AWS recently announced support for clusters up to 100K nodes.&lt;/p&gt;
    &lt;p&gt;In online forums and in my own conversations with peers, I‚Äôve encountered a lot of debate about how big a Kubernetes cluster can get. What tends to be lacking from these discussions is hard data and evidence-backed justifications. I‚Äôve worked with engineers reluctant to push things beyond what they‚Äôve seen before because they‚Äôre fearful or uncertain of what may go wrong. Or when something does go wrong, the response is to scale down the cluster rather than understand and address the bottleneck.&lt;/p&gt;
    &lt;p&gt;The spirit of the k8s-1m project is to identify the hard blockers to scalability. What are the actual limits of how hard a particular system can be pushed, and why are those the limits? What would it take to overcome those limits to make the system even more scalable? What types of workloads can be run at greater scale than others?&lt;/p&gt;
    &lt;p&gt;Many limitations are imposed by software. As software engineers, it‚Äôs on us to work through those limitations. Once we‚Äôve done all we can on the software side and the remaining bottlenecks are in hardware, only then should we wait on next generation hardware to achieve our goals.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have ambitions to commercialize this effort. Certainly not as-is. I‚Äôm fine making the code freely available. Realistically, while all of the claims and results in this project are true and verifiable, each large-scale Kubernetes cluster has its own unique workload demands that may or may not work well with this design. So I wouldn‚Äôt classify this as "production-ready" or recommend that anyone use it for mission-critical workloads.&lt;/p&gt;
    &lt;p&gt;At the same time, I hope that the proposed designs here inspire the Kubernetes community with what is possible and influence the future roadmap of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Components&lt;/head&gt;
    &lt;p&gt;Early on in this project, I asked ChatGPT ‚ÄúI want to scale Kubernetes to 1 million nodes. What types of problems would I need to overcome?‚Äù&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a portion of what I got:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scaling Kubernetes to 1 million nodes is a formidable challenge and involves overcoming a variety of technical hurdles. Here are the key areas you would need to address:&lt;/p&gt;
      &lt;p&gt;etcd Scalability and Performance: etcd is the backbone of Kubernetes' storage, handling all API object data. With 1 million nodes, the volume of data managed by etcd will increase significantly. Optimizing etcd‚Äôs performance, including efficient data partitioning, tuning the database to handle high loads, and possibly looking into alternatives or supplements to etcd for certain types of data, is crucial.&lt;/p&gt;
      &lt;p&gt;kube-apiserver Performance: The Kubernetes kube-apiserver would face enormous loads in a cluster of this size. Optimizing its performance includes horizontal scaling (running multiple kube-apiserver instances), rate limiting, and efficient caching mechanisms. Also, minimizing the watch events and reducing the overall number of requests to the kube-apiserver is important.&lt;/p&gt;
      &lt;p&gt;Networking: Handling network traffic for 1 million nodes is extremely challenging. This includes the management of IP addresses, efficient routing, and possibly segmenting the network to prevent it from becoming a bottleneck. Solutions like Cilium, which uses eBPF technology for networking and security, might be necessary.&lt;/p&gt;
      &lt;p&gt;Scheduling and Resource Management: The scheduler will have increased latency as it tries to place pods optimally across a vast number of nodes. Enhancing the scheduler to handle large scale operations efficiently, possibly by implementing custom schedulers for specific tasks, is required.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It goes on to list other challenges, including Logging &amp;amp; Monitoring, Security, and Storage management.&lt;/p&gt;
    &lt;p&gt;I think these first items are the most critical. There are a wide range of options for support for logging or PVCs, but if you can‚Äôt run etcd or schedule pods or have the pods talk to one another, you‚Äôre not getting anywhere.&lt;/p&gt;
    &lt;p&gt;Below I‚Äôm going to discuss the three major areas I tackled.&lt;/p&gt;
    &lt;head rend="h3"&gt;Networking&lt;/head&gt;
    &lt;p&gt;The common challenges with networking in Kubernetes are IP address space, service discovery, and network policies (aka firewalling). Compared to later challenges, configuring networking to support 1M nodes turned out to be relatively easy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pod IPs&lt;/head&gt;
    &lt;p&gt;It can be a challenge to plan IP address space in large clusters. Efficient routing usually means routing a contiguous CIDR to each individual node, which means you‚Äôre pre-allocating how many IP addresses (and thus pods) each node can serve. Some nodes that are intended for lots of small workloads end up constrained by the number of available IP addresses before they run out of other hardware resources. A 10.0.0.0/8 has 16 million IPs. For a 1-million node cluster, that leaves just 15 pod IPs per node, which is likely not enough.&lt;/p&gt;
    &lt;p&gt;The answer is to use IPv6 thoroughly and exclusively. The enormous IPv6 address space means that there‚Äôs plenty of room for every pod to have its own globally accessible IP address.&lt;/p&gt;
    &lt;p&gt;Kubernetes has great support for IPv6, and it requires no code changes to create a fully functioning Kubernetes cluster that uses IPv6 exclusively.&lt;/p&gt;
    &lt;p&gt;My goal is for each node to have an IPv6 address prefix with a range large enough so each pod on that node can have its own IP out of that range. Plus, of course, at least one for the host itself.&lt;/p&gt;
    &lt;p&gt;Of course, using IPv6 requires support from your compute vendor. All of the major cloud providers (and even many less-major) support IPv6 to some degree or another. And with public IPv6, it‚Äôs trivial to create a single cluster that spans multiple clouds.&lt;/p&gt;
    &lt;p&gt;I primarily focused on AWS, GCP, and Vultr. (Scoff if you want at Vultr, but they have cheap compute and I am self-bootstrapping this project.) But each one has slightly different twists on its support for IPv6 addressing inside a VM. To give a sense of the range of situations, let me briefly describe each below:&lt;/p&gt;
    &lt;p&gt;Vultr: Each node gets a /64. The primary IP of the node is the ::1 of that range, and the server automatically receives all traffic for any IP in the full /64 range.&lt;/p&gt;
    &lt;p&gt;GCP: Each node gets a /96. The primary IP of the node is some random IP within that range. The server must send valid NDP RA packets for the IPs that it wants to receive traffic for.&lt;/p&gt;
    &lt;p&gt;AWS: Each node gets a /128. You can add a /80 prefix (that comes from a different range) via an API call to an existing NIC or VM. (The ‚ÄòCreate‚Äô API looks like it can support setting both an IPv6 address and an IPv6 range at creation time, but you‚Äôll get an error). The server must send valid NDP RA packets for the IPs it wants to receive traffic for, and all outgoing packets must use the one MAC address that matches the primary IP.&lt;/p&gt;
    &lt;p&gt;To satisfy the intersection of these requirements, particularly the requirement about MAC addresses, I create one bridge for all of the pods' interfaces to share. But leave the host interface separate, and enable forwarding to handle traffic between the bridge and the host interface. A host-local IPAM is set to a /96 IPv6 prefix of what I get from the provider. This gives us a full 2^32 IPs per node, plenty of space for pods.&lt;/p&gt;
    &lt;p&gt;Because these are global public IPv6 addresses, no special routing is necessary. No packet encapsulation or NAT is used. Traffic from each pod is correctly sourced from its true origin pod IP, regardless of destination.&lt;/p&gt;
    &lt;head rend="h4"&gt;IPv4-only external service dependencies&lt;/head&gt;
    &lt;p&gt;If you only have an IPv6 address, then you can only reach other IPv6 addresses on the internet. Anything that is IPv4-only isn‚Äôt directly accessible.&lt;/p&gt;
    &lt;p&gt;Most services I used in this project worked fine: Ubuntu packages, PyPi packages, the docker.io registry. The main exception was GitHub. Github.com remains stubbornly IPv4-only. Tsk tsk.&lt;/p&gt;
    &lt;p&gt;Many AWS services have dual-stack endpoints but notably for this project Elastic Container Registry (ECR) does not. Tsk tsk to them as well.&lt;/p&gt;
    &lt;p&gt;For IPv6 devices to reach IPv4 hosts, most cloud providers offer some sort of NAT64 gateway. You can also roll your own gateway on a Linux VM. I over-engineered this a bit with a custom WireGuard server. All VMs connect via WireGuard to this server and use it as an IPv4 gateway.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network Policies&lt;/head&gt;
    &lt;p&gt;High-level, I hand-waved over this problem and did not use network policies between workloads.&lt;/p&gt;
    &lt;p&gt;1 million nodes would have 1 million separate IPv6 prefixes, which is far too many individual entries for any firewall solution to support. Security-minded folks: clutch your pearls when I say that I do not use extensive firewalling to prevent access into the cluster from the Internet. I do use firewall rules to limit to a select few number of ports that I know need to be reached, but otherwise we must rely on other techniques to safeguard unauthorized inbound access to servers and pods.&lt;/p&gt;
    &lt;p&gt;Thorough use of TLS covers most use cases for this project. The enormous size of the IPv6 address space also makes scanning impractical. Cilium, kube-proxy, or other network plugins could also limit which pods can reach which pods, but at significant cost of additional watches on the control plane.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre using one single vendor for all of your nodes, it may be plausible that all nodes still get ipv6 ranges out of 1 or a few larger spaces, a count low enough that could be reasonably installed as firewall rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network flow needs (# of TCP connections)&lt;/head&gt;
    &lt;p&gt;Both kube-apiservers and etcd support both HTTP/2 and gRPC. Many individual requests and streams are multiplexed over a single TCP connection. Kubernetes sets a default HTTP/2 limit of 100 concurrent requests (or technically streams) per TCP connection. (HTTP/2 can support far more than that, but as you add more streams you run into performance problems like head-of-line blocking). So each kubelet needs at least 1 connection to the kube-apiserver control plane. And you can expect 1 more connection for kube-proxy, or any similar CNI like Cilium or Calico. With 1M nodes, that means each kube-apiserver is supporting at least 2 million TCP connections. With 8 kube-apiservers, each server would be supporting 250K connections to kubelets.&lt;/p&gt;
    &lt;p&gt;Linux itself can support this number of connections with some light tuning. And of course make sure you have allowed yourself enough file descriptors. Nevertheless it may be more than your network provider can support. Azure, for example, documents that it can support a maximum of 500k inbound and 500k outbound connections per VM. GCP and AWS do not publish limits, but there are limits in any system to both the total number of concurrent connections as well as the rate of new connections being made.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing state&lt;/head&gt;
    &lt;p&gt;When I talk about ‚Äúmanaging state,‚Äù I mean the API surface that Kubernetes exposes for interacting with resources. With careful tuning, the kube-apiserver can scale to sufficiently high levels of throughput. etcd, however, is the bottleneck. In this section, I‚Äôll outline why that is and describe a replacement implementation that can meet the demands of a million-node cluster.&lt;/p&gt;
    &lt;head rend="h4"&gt;kube-apiservers vs etcd&lt;/head&gt;
    &lt;p&gt;First a quick overview about the ways you work with state in Kubernetes. Any number of clients interact with kube-apiservers, which then in turn interact with etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers are stateless. etcd is the persistent store for all of Kubernetes resources. All CRUD operations you send to a kube-apiserver are actually persisted by etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers have seven common verbs for state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;create&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;get&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;list&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;update&lt;/code&gt;(aka&lt;code&gt;replace&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;patch&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;delete&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;etcd has four:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;put&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;range&lt;/code&gt;- includes&lt;code&gt;get&lt;/code&gt;with a null&lt;code&gt;range_end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deleteRange&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;patch&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt; all result in an etcd &lt;code&gt;put&lt;/code&gt; operation. (A &lt;code&gt;delete&lt;/code&gt; is just a &lt;code&gt;put&lt;/code&gt; with a null value). etcd doesn‚Äôt support any partial updates of values, only putting the entire value. So all operations that involve modifying a resource result in a new etcd &lt;code&gt;put&lt;/code&gt; of the entire resource contents.&lt;/p&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;watch&lt;/code&gt; can, but often doesn‚Äôt, result in an etcd &lt;code&gt;watch&lt;/code&gt;. More on that below.&lt;/p&gt;
    &lt;head rend="h4"&gt;Meeting the QPS needs for a 1M node cluster&lt;/head&gt;
    &lt;p&gt;Kubelets interact with the kube-apiserver primarily through two resource types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Node&lt;/code&gt;- the resource representing a server for running pods&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;- a lightweight heartbeat object updated by&lt;code&gt;kubelet`&lt;/code&gt;to signal liveness&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Lease is critical: if it isn‚Äôt refreshed in time, the NodeController marks the node as &lt;code&gt;NotReady&lt;/code&gt;. By default, each kubelet updates its &lt;code&gt;Lease&lt;/code&gt; every 10 seconds. At a scale of 1 million nodes, that alone translates to 100K writes per second just to keep the nodes "alive."&lt;/p&gt;
    &lt;p&gt;Adding in the constant churn of other resources, the system needs to sustain on the order of many hundreds of thousands of writes per second, plus a significant volume of reads.&lt;/p&gt;
    &lt;p&gt;For kube-apiserver, this is manageable. It‚Äôs stateless, so QPS can be scaled out simply by running more replicas. If one instance can‚Äôt handle the load, more can be added, and traffic will spread across them.&lt;/p&gt;
    &lt;p&gt;For etcd, things are different. Etcd is stateful, which makes scaling QPS much harder.&lt;/p&gt;
    &lt;head rend="h4"&gt;etcd is too slow&lt;/head&gt;
    &lt;p&gt;Using the etcd-benchmark tool, I measured about 50K writes/sec out of a single etcd instance backed by NVMe storage. Importantly, adding replicas doesn‚Äôt help. Write throughput actually drops with more members since each write must be coordinated across a quorum of replicas to maintain consistency. So with the typical 3-replica setup, effective write QPS is even lower than the benchmarked 50K/s. That‚Äôs nowhere near what‚Äôs needed to support a 1M-node cluster.&lt;/p&gt;
    &lt;p&gt;At first glance, 50K QPS seems surprisingly low given modern hardware capabilities. A single NVMe drive can do over 1M 4K writes per second, and a single DDR5 DIMM can push 10x more than that. So why is etcd is far behind raw hardware limits?&lt;/p&gt;
    &lt;p&gt;The answer lies in etcd‚Äôs interface and guarantees. For one thing, etcd is ensuring that all writes are durable to disk. For every &lt;code&gt;put&lt;/code&gt; or &lt;code&gt;delete&lt;/code&gt; call, etcd ensures the change is written to disk via &lt;code&gt;fsync&lt;/code&gt; before acknowledging success. This helps ensure that there is never any data loss if the host crashes or loses power. But that durability drastically reduces the number of IOPS that a modern NVMe drive can support.&lt;/p&gt;
    &lt;p&gt;Plus, etcd has a pretty broad interface surface area:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It is a key value store and so of course supports reads, writes, and deletes of single objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can support querying a range of sorted keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It keeps history for all changes, so you can query for an older version of a particular key, or even a range of keys. Older changes eventually get ‚Äúcompacted‚Äù to reduce state size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It has a notion of ‚Äúwatches‚Äù, meaning it can stream out all of the changes that affect a particular key or range of keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It also has a ‚Äúlease‚Äù API, where keys can be attached to a TTL that will cause them to expire if not renewed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It supports transactions, supporting an atomic If/Then/Else.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementing all of those interfaces can make for complex software. Beyond simple puts and deletes, etcd must support transactions, maintain multi-versioned history, and enforce Raft-based consensus across replicas.&lt;/p&gt;
    &lt;p&gt;These features are what give Kubernetes its consistency and reliability, but they also impose strict constraints on performance. Intuitively, strong consistency means more serialization: operations can‚Äôt always be parallelized freely. Writes often need to follow a carefully ordered path through Raft, the WAL, and compaction, ensuring that every replica agrees on state before acknowledging success.&lt;/p&gt;
    &lt;p&gt;The result is raw hardware capable of millions of writes per second, but etcd delivering orders of magnitude less due to the interfaces and guarantees it must uphold.&lt;/p&gt;
    &lt;p&gt;But do we need all of these things?&lt;/p&gt;
    &lt;head rend="h5"&gt;Reduce durability and eliminate replicas&lt;/head&gt;
    &lt;p&gt;Perhaps my spiciest take from this entire project: most clusters don‚Äôt actually need the level of reliability and durability that etcd provides.&lt;/p&gt;
    &lt;p&gt;As we‚Äôll see in the next section, the majority of writes in a Kubernetes cluster are for ephemeral resources.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Kubernetes&lt;/p&gt;&lt;code&gt;Events&lt;/code&gt;may only stick around for minutes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;objects typically expire within tens of seconds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the cluster is disrupted, restoring these objects is almost never useful, and certainly not to the precision of their last few milliseconds of updates. Even for longer-lived objects, Kubernetes is designed to reconcile automatically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Nodes&lt;/code&gt;continually refresh status via&lt;code&gt;kubelet&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Controllers will bring&lt;/p&gt;&lt;code&gt;DaemonSet&lt;/code&gt;and&lt;code&gt;Deployment&lt;/code&gt;status back in sync with actual&lt;code&gt;Pods&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we stopped &lt;code&gt;fsync&lt;/code&gt;-ing these ephemeral writes, or even stopped writing them altogether and just relied on RAM, clusters could process far more operations and perform substantially better.&lt;/p&gt;
    &lt;p&gt;In fact, even full control plane data loss isn‚Äôt catastrophic in some environments. Many clusters are ephemeral themselves, with all configuration encoded in Terraform, Helm, or GitOps. In those cases, rebuilding is often easier than preserving every last write. Some organizations already treat Kubernetes clusters as cattle.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre not mad yet, let me push you a little further: you probably don‚Äôt need etcd replicas at all.&lt;/p&gt;
    &lt;p&gt;In the 5 years I ran Kubernetes clusters at OpenAI, we never once had an unplanned VM outage on an etcd VM. etcd‚Äôs resource needs are tiny. The database is limited to 8GB. CPU is no more than 2-4 cores. Most cloud providers can do live migration on VMs this small. With network-attached storage like EBS, recovery is straightforward: spin up a replacement VM, attach the volume, and resume operation with zero data loss.&lt;/p&gt;
    &lt;p&gt;If you had just 1 etcd instance and that went down, your Kubernetes cluster control plane would go down. Pods would still stay running. Nodes would still be reachable. It‚Äôs possible that you could still serve traffic. If etcd used EBS, recovery would be the time to start a new VM and attach the volume, with no data loss.&lt;/p&gt;
    &lt;p&gt;Yes, running a single etcd instance is a single point of failure. But failures are rare and the practical impact is often negligible. Meanwhile, etcd replicas come with a significant performance cost. For many workloads, that tradeoff simply isn‚Äôt worth it.&lt;/p&gt;
    &lt;p&gt;Always stop writing &lt;code&gt;Event&lt;/code&gt; and &lt;code&gt;Lease&lt;/code&gt; to disk. Beyond that, you have some options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You don‚Äôt need durability: Run one replica, keep all state in memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can tolerate losing a few ms of updates: Run a single replica with a network-attached disk, but without&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You‚Äôd rather avoid data loss: Run multiple replicas in case one goes down, but don‚Äôt bother writing changes to disk. Rely on the uptime of the other replicas to keep from losing data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You‚Äôre paranoid about data loss: Run a single replica with a network-attached disk, and enable&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Reduce the interface&lt;/head&gt;
    &lt;p&gt;As I described, etcd has a pretty broad interface surface area. But does Kubernetes actually use all of those features?&lt;/p&gt;
    &lt;p&gt;To measure this, I wrote a small tool called etcd proxy, The proxy sits between Kubernetes and etcd, transparently forwarding all traffic while logging every request and response.&lt;/p&gt;
    &lt;p&gt;With that in place, I spun up a Kubernetes cluster and ran Sonobuoy, the standard conformance test suite of Kubernetes. Sonobuoy systematically exercises the full API surface of Kubernetes, ensuring compliance with upstream expectations. Running it through the proxy produced a complete, real-world trace of the requests and workloads that etcd must handle in a conforming cluster.&lt;/p&gt;
    &lt;p&gt;It turns out that Kubernetes actually uses just a small amount of the etcd interface.&lt;/p&gt;
    &lt;p&gt;There‚Äôs of course &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;range&lt;/code&gt;, and &lt;code&gt;watch&lt;/code&gt; queries, but they all follow a few patterns.&lt;/p&gt;
    &lt;head rend="h6"&gt;Txn-Put&lt;/head&gt;
    &lt;p&gt;Kubernetes does do Txn queries, but they‚Äôre always of this form:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "SOMEKEY",
        "modRevision": "SOMEREV",
        "target": "MOD"
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "SOMEKEY",
          "value": "..."
        }
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "SOMEKEY"
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;In other words, do a &lt;code&gt;put&lt;/code&gt; if the &lt;code&gt;modRev&lt;/code&gt; of this key is set to this particular value, otherwise just return me the current version. And this makes sense, because Kubernetes is often patching or updating existing resources but turning that into a &lt;code&gt;put&lt;/code&gt; of the full resource safely means that the underlying resource must not have changed in between.&lt;/p&gt;
    &lt;head rend="h6"&gt;Leases&lt;/head&gt;
    &lt;p&gt;Note that Kubernetes Leases are not the same as etcd Leases. Kubernetes leases are implemented as regular K/V‚Äôs in etcd. Kubernetes makes very few etcd Leases.&lt;/p&gt;
    &lt;p&gt;The main area where Kubernetes uses etcd leases is on Events objects, e.g.:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.Lease/LeaseGrant",
  "request": {
    "TTL": "3660"
  },
  "response": {
    "ID": "7587883212297104637",
    "TTL": "3660"
  }
}
{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "/registry/events/NAMESPACE/SOMEEVENT",
        "modRevision": "205",
        "target": "MOD"
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
        }
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
          "lease": "7587883212297104637",
          "value": "..."
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;The purpose of this is to manage some sane TTL on events. It‚Äôs not critical to the consistency model of Kubernetes.&lt;/p&gt;
    &lt;head rend="h6"&gt;Ranges&lt;/head&gt;
    &lt;p&gt;etcd could be implemented as a simple hash-table with O(1) insertion time, if it weren‚Äôt for range queries. Range queries return a sorted list of keys within a given span, which requires storing data in a sorted structure. Inserting into a sorted list or B-Tree is O(log n). In my view, supporting Range is thus the most difficult constraint that etcd must implement to be Kubernetes compatible. Nevertheless, it is critical.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can take advantage of the predictable structure of the keyspace:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]$NAME&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Range queries are typically scoped to be either within a particular namespace, or across all namespaces for a given resource Kind. Kubernetes never performs a range query that spans across multiple resource Kinds (e.g., Pods and ConfigMaps together).&lt;/p&gt;
    &lt;p&gt;This introduces an opportunity: rather than one global B-tree for the entire keyspace, we can maintain separate B-trees per resource Kind. That shrinks the effective n in O(log n) to just the number of objects of a single kind, improving both inserts and queries.&lt;/p&gt;
    &lt;p&gt;Another wrinkle is the use of &lt;code&gt;limit&lt;/code&gt; on range queries. Kubernetes rarely needs to retrieve all objects at once; queries often return only 500, 1,000, or 10,000 results at a time. However, range responses are also expected to include a count field representing the total number of remaining objects. This undermines the benefit of &lt;code&gt;limit&lt;/code&gt;, since even merely counting all remaining keys can still be expensive.&lt;/p&gt;
    &lt;p&gt;In practice, though, Kubernetes doesn‚Äôt rely on &lt;code&gt;count&lt;/code&gt; being exact. It only needs to know that there are more than &lt;code&gt;limit&lt;/code&gt; results available. This looser requirement leaves room for approximation, and is one area where further optimizations are possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;mem_etcd: custom in-memory etcd&lt;/head&gt;
    &lt;p&gt;I built a new program called mem_etcd that implements the etcd interface but with the simplifications described above. Written in Rust, it provides fully correct semantics for the etcd APIs that Kubernetes depends on.&lt;/p&gt;
    &lt;p&gt;mem_etcd maintains two main data structures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A hash map storing the full keyspace&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A B-tree indexing the keys within each prefix.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each value also stores the non-compacted revision history for that key. This design makes writes to existing keys O(1), while writes to new keys and range queries are O(log n) (where n is the number of resources of that &lt;code&gt;Kind&lt;/code&gt;). &lt;code&gt;Range&lt;/code&gt; queries also require additional linear work up to the query‚Äôs limit.&lt;/p&gt;
    &lt;p&gt;Despite its name, mem_etcd can provide durability by writing a write-ahead log (WAL) to disk. Each prefix of &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]&lt;/code&gt; is written to its own separate file. By default, files are written in &lt;code&gt;buffered&lt;/code&gt; mode, so &lt;code&gt;put&lt;/code&gt; calls can complete before the data is durably written to disk. This behavior can be changed with a CLI flag that enables &lt;code&gt;fsync&lt;/code&gt;, forcing all writes to be flushed to disk before the &lt;code&gt;put&lt;/code&gt; completes.  You can also configure some prefixes to not be written to disk at all.&lt;/p&gt;
    &lt;code&gt;% (cd /tmpfs ; etcd-3.5.16 --snapshot-count=9999999999 --quota-backend-bytes=9999999999) &amp;amp;
% parallel -j $X --results out_{#}.txt './benchmark put --total 10000000 --clients 1000 --conns 10 --key-space-size 10000000 --key-size=48 --val-size=1024' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;These tests were run on a pair of &lt;code&gt;c4d-standard-192-lssd&lt;/code&gt; instances, with one VM running mem_etcd and the other running the client benchmark. In these results, you can easily observe how badly enabling &lt;code&gt;fsync&lt;/code&gt; negatively impacts throughput and latency. Note that the baseline comparison of etcd is a single replica of etcd v3.5.16 running on a tmpfs (ram-based) disk. This should be an optimal environment for etcd as there is no actual disk involved and &lt;code&gt;fsync&lt;/code&gt;, while still being a syscall, is otherwise a no-op. mem_etcd is storing its WAL on a local NVMe, what GCE calls Titanium SSD. Though the instance type has 16 local disks, only 1 is used for this test.&lt;/p&gt;
    &lt;code&gt;% timeout 10 parallel -j $X --results out_{#}.txt   './etcd-lease-flood -num-keys 1000 -workers 100 -key-prefix {#}' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;etcd-lease-flood&lt;/code&gt; is a custom benchmark designed to simulate the dominant type of load in a large Kubernetes cluster. Each client creates 100 &lt;code&gt;Lease&lt;/code&gt; objects directly in etcd, using the same protobuf encoding as Kubernetes. For each &lt;code&gt;Lease&lt;/code&gt;, the client repeatedly issues &lt;code&gt;put&lt;/code&gt; updates in a tight loop, attempting to update the &lt;code&gt;Lease&lt;/code&gt; as quickly as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Watch()&lt;/head&gt;
    &lt;p&gt;There are several different types of watches and each has different performance characteristics. Let‚Äôs unpack them.&lt;/p&gt;
    &lt;p&gt;https://kubernetes.io/docs/reference/using-api/api-concepts/#semantics-for-watch has some useful details about how the kube-apiserver handles the parameters of your watch:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;resourceVersion unset: Get State and Start at Most Recent&lt;/p&gt;&lt;lb/&gt;Start a watch at the most recent resource version, which must be consistent (in detail: served from etcd via a quorum read). To establish initial state, the watch begins with synthetic "Added" events of all resources instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"0`": Get State and Start at Any&lt;/p&gt;&lt;lb/&gt;Start a watch at any resource version; the most recent resource version available is preferred, but not required‚Ä¶. To establish initial state, the watch begins with synthetic "Added" events for all resource instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"{value other than 0}`": Start at Exact&lt;/p&gt;&lt;lb/&gt;Start a watch at an exact resource version. The watch events are for all changes after the provided resource version. Unlike "Get State and Start at Most Recent" and "Get State and Start at Any", the watch is not started with synthetic "Added" events for the provided resource version.&lt;/quote&gt;
    &lt;p&gt;Let‚Äôs re-format the important points into a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;resourceVersion unset&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion=0&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Served from kube-apiserver state (instead of etcd)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚ùå&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Includes an initial list&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚ùå&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So a &lt;code&gt;watch&lt;/code&gt; is often preceded by a &lt;code&gt;list&lt;/code&gt;. The &lt;code&gt;list&lt;/code&gt; provides a snapshot-in-time of a set of resources, marked with a revision number. Then you start a watch from that revision number, which will then stream to you all of the changes that have occurred since that revision.&lt;/p&gt;
    &lt;p&gt;When &lt;code&gt;resourceVersion&lt;/code&gt; is set, a watch against the kube-apiserver does not create a new watch against etcd. At startup time, a kube-apiserver creates &lt;code&gt;watch&lt;/code&gt; streams against etcd for each of the well-known standard resources. Any time a client creates a watch, the kube-apiserver handles that stream itself based on the one etcd watch stream it maintains. So while client watches can be expensive for kube-apiservers, it adds no additional load to etcd. You can horizontally scale more kube-apiservers.&lt;/p&gt;
    &lt;p&gt;Furthermore, watches are not really that bad for etcd. A watch has a beginning and an end range, and those ranges fit within the same prefixes as Range queries. With each Put we need to do a log(n) lookup in the list of watches to find watches that could match that key. But there are far far fewer watches than objects. The n is small and is done asynchronously after the write is committed anyway, so it does not affect the request time to complete a write.&lt;/p&gt;
    &lt;p&gt;Watches do create network amplification. For each write into etcd, there may be N corresponding watches for that object. That results in a lot of outbound network traffic from etcd. The kube-apiservers are on the receiving end of these watches. kube-apiservers are consolidating their own watches, but etcd is still sending a copy of the data to each kube-apiserver. While adding more kube-apiserver replicas does help with many Kubernetes scalability problems, each replica does put additional pressure on the etcd NIC. The network throughput of etcd is the most immediate hardware bottleneck of large-scale Kubernetes clusters. However, these demands are limited to just between etcd and the kube-apiservers. In a single datacenter with modern hardware there‚Äôs still plenty of potential interconnect that could be established amongst these servers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Watches per node&lt;/head&gt;
    &lt;p&gt;By scaling up the number of nodes I was able to observe how many watches each node creates. Per kubelet + kube-proxy, I observe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;4 watches of&lt;/p&gt;
        &lt;code&gt;configmaps&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;2 watches each of&lt;/p&gt;&lt;code&gt;pods&lt;/code&gt;,&lt;code&gt;secrets&lt;/code&gt;,&lt;code&gt;services&lt;/code&gt;,&lt;code&gt;nodes&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;1 watch each of&lt;/p&gt;&lt;code&gt;namespaces&lt;/code&gt;,&lt;code&gt;endpoints&lt;/code&gt;,&lt;code&gt;csidrivers&lt;/code&gt;,&lt;code&gt;runtimeclasses&lt;/code&gt;,&lt;code&gt;endpointslices&lt;/code&gt;,&lt;code&gt;networkpolicies&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That‚Äôs 18 watches per node, so 18M watches for 1M nodes. These are only against the kube-apiserver and do not passthrough to etcd directly. With enough kube-apiservers we should be fine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Update()&lt;/head&gt;
    &lt;p&gt;Let‚Äôs revisit our 1M kubelet Lease requirement. Kubelet is issuing an &lt;code&gt;Update&lt;/code&gt; (aka &lt;code&gt;Replace&lt;/code&gt;, aka PUT) of its Lease resource every 10 seconds.&lt;/p&gt;
    &lt;p&gt;This is an old Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:41:50.646654Z"&lt;/code&gt;
    &lt;p&gt;This is the body of calling Update() when renewing that Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:51:50.650000Z"&lt;/code&gt;
    &lt;p&gt;Note the &lt;code&gt;renewTime&lt;/code&gt; has been updated  to something 10 seconds later. (&lt;code&gt;renewTime&lt;/code&gt; is in fact always set to 40 seconds in the future, so we can tolerate some amount of failed or slow lease updates).&lt;/p&gt;
    &lt;p&gt;The other key field is the &lt;code&gt;resourceVersion&lt;/code&gt;. When a client sends an &lt;code&gt;Update()&lt;/code&gt; to a kube-apiserver, it includes the same &lt;code&gt;resourceVersion&lt;/code&gt; from the previous version of the resource it‚Äôs updating. This is for safety to ensure that no other client has updated the resource in-between.  Every time a resource is updated on the server, the server assigns the new resource a monotonically-increasing new resourceVersion.  An Update operation must include a resourceVersion that indicates what the old version of the resource it thinks it‚Äôs replacing. That way we‚Äôre not accidentally overwriting some other change that has happened in-between.&lt;/p&gt;
    &lt;p&gt;You‚Äôd think that kube-apiserver could simply convert this Update operation into a &lt;code&gt;Txn-Put&lt;/code&gt; operation in etcd, passing through this command in a straightforward and stateless way. Unfortunately kube-apiserver‚Äôs Update implementation also always needs to obtain the entire Old version of this resource. There‚Äôs a few reasons for this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Server-side fields: some resources have fields such as ‚Äòstatus‚Äô and ‚ÄòmanagedFields‚Äô that are only ever updated by the server.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Admission checks: the Admission check interface takes both the old and the new resource.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So to keep Update calls performant, kube-apiserver will maintain Watch caches of most commonly-used resources. When an Update occurs, it‚Äôll pull the old version from its local watch cache. If for some reason the old version is not in the watch cache, then kube-apiserver will first issue a Range to etcd to get the old resource before calling &lt;code&gt;Txn-Put&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Having to do two synchronous calls to etcd for each Update would double our QPS needs and latency, so it‚Äôs much better if we can rely on an up-to-date watch cache.&lt;/p&gt;
    &lt;p&gt;However this introduces a new requirement and constraint for 1M nodes: kube-apiservers must be able to ‚ÄúWatch‚Äù at a rate of at least 100K events/sec.&lt;/p&gt;
    &lt;p&gt;In my testing this is where things get a little tight.&lt;/p&gt;
    &lt;head rend="h5"&gt;Caching and locking&lt;/head&gt;
    &lt;p&gt;kube-apiserver is deserializing (and, more critically, allocating memory for) 100K nested dictionaries per second. It stores these in a cache, backed by a B-Tree protected with a RWMutex. That RWMutex is under heavy contention:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that are attempting to read the cache for the old objects.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that complete (&lt;code&gt;GuaranteedUpdate()&lt;/code&gt;finalizer) are writing the new value into the cache&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Events from the etcd Watch stream is also writing new values into the cache&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adding more kube-apiservers helps reduce the contention caused by Update, but it doesn‚Äôt reduce the watch load - each kube-apiserver still needs to be able to keep up with the full watch stream of every change that occurs. And adding more kube-apiserver replicas puts additional strain on etcd - most critically its ability to push copies of the watch stream out to the network to each kube-apiserver.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a relatively recent change that the kube-apiserver cache is backed by a B-Tree. Previously it was backed by a hash map. This was enabled with feature flag &lt;code&gt;BtreeWatchCache&lt;/code&gt; which became &lt;code&gt;true&lt;/code&gt; by default in Kubernetes 1.32. As far as I can tell, the motivation to move to B-Tree was for faster &lt;code&gt;List()&lt;/code&gt; response. Remember that &lt;code&gt;List()&lt;/code&gt; needs to return items in sorted order, so keeping the items in a B-Tree will make that much faster. But &lt;code&gt;Get()&lt;/code&gt; and &lt;code&gt;Update()&lt;/code&gt; of existing items is now O(n log n) instead of O(1).&lt;/p&gt;
    &lt;p&gt;In my testing, I was unable to get the B-Tree-based cache to scale much more beyond 40K updates per second on a c4a-standard-72 GCP instance. The cache gets stale, unable to keep up with the stream of watch events, too much time being spent waiting for cache lock.&lt;/p&gt;
    &lt;p&gt;With the old hashmap-based cache and 11x kube-apiservers there‚Äôs enough replicas to handle the Update() load of 100K Lease updates per second.&lt;/p&gt;
    &lt;head rend="h5"&gt;Garbage collection&lt;/head&gt;
    &lt;p&gt;kube-apiservers parse and decode all resources into their individual fields. Resources with lots of fields thus create a lot of tiny objects in Go, and that puts pressure on garbage collection. Adding more kube-apiserver replicas won‚Äôt help if they all are watching the same resource event streams. There‚Äôs no real cure, but setting &lt;code&gt;GOMEMLIMIT&lt;/code&gt; and &lt;code&gt;GOGC&lt;/code&gt; can help.&lt;/p&gt;
    &lt;p&gt;I set &lt;code&gt;GOMEMLIMIT&lt;/code&gt; to a number 10-20% less than memory I have on-hand, and set &lt;code&gt;GOGC&lt;/code&gt; up to a few hundred.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scheduler&lt;/head&gt;
    &lt;p&gt;It doesn‚Äôt do any good to have a 1-million node cluster if you can‚Äôt schedule pods on it. The Kubernetes scheduler is a pretty common bottleneck for large jobs. I ran a benchmark, scheduling 50K pods on 50K nodes, and it took about 4.5 minutes. That‚Äôs already uncomfortably long.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Warning&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;If you‚Äôre creating pods with some sort of replication controller like Deployment, DaemonSet, or StatefulSet, which can be a bottleneck even before the scheduler. DaemonSet creates a burst of 500 pods at a time and then waits for the Watch stream to show that those are created before proceeding (the rate depends on many factors but expect &amp;lt;5K/sec). The scheduler doesn‚Äôt even get a chance to run until those pods are created.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For this 1-million node cluster project, I set an ambitious goal of being able to schedule 1 million pods in 1 minute. Admittedly the number is somewhat arbitrary, but the symmetry with all those m's seemed nice.&lt;/p&gt;
    &lt;p&gt;I also wanted to keep full compatibility with the standard kube-scheduler. It would be far easier to write a simplified scheduler from scratch that scales impressively in narrow scenarios but then fails spectacularly in real-world use cases. There‚Äôs a lot of complexity in the existing scheduler that arises from being battle-tested across lots of different production environments. Stripping away those pesky features to make a ‚Äúfaster‚Äù scheduler would be misleading.&lt;/p&gt;
    &lt;p&gt;So, we‚Äôre going to preserve the functionality and implementation of the kube-scheduler as much as we can. What‚Äôs getting in our way to making it more scalable?&lt;/p&gt;
    &lt;p&gt;kube-scheduler works by keeping state of all nodes, and then has a O(n*p) loop, where for each pod it evaluates it against every node. First it filters out nodes that the pod wouldn‚Äôt fit at all. Then, for each remaining node, it calculates a score on how well that node would match the pod. The pod is then scheduled to the highest-scoring node, or a random choice among the highest-scoring nodes if there‚Äôs a tie.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Tip&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;The kube-scheduler has some techniques to improve performance:&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is parallelizable. And to be fair, the scheduler does parallelize the filtering and generation of scores of nodes against a particular pod. But the scheduler is still burdened by having to do it for all nodes. This isn‚Äôt just parallelizable, this can also be distributable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Basic design: shard on nodes&lt;/head&gt;
    &lt;p&gt;It‚Äôs akin to the classic scatter/gather design of a distributed search system. Think of each node as a document in the corpus, and each pod as a search query. The query is fanned out to many shards, each responsible for a fraction of the documents. Each shard selects its top candidate(s) and sends them back to a central gatherer to ultimately identify the overall top result.&lt;/p&gt;
    &lt;p&gt;Generic scatter-gather design pattern&lt;/p&gt;
    &lt;p&gt;The key difference is that in search, documents are read-only and thus queries can be evaluated in parallel without conflicts. In scheduling, however, executing a decision actually modifies the documents (i.e. allocates node resources). If two pods are scheduled in parallel to the same node, one may succeed while the other must fail due to insufficient resources.&lt;/p&gt;
    &lt;p&gt;Nevertheless, for a large cluster it‚Äôs reasonable to design for ‚Äúoptimistic concurrency‚Äù. That is, presume that multiple pods can be scheduled at the same time without conflicts. We still check to see if a conflict arises before committing. And if a conflict occurs, we still do the correct thing by ‚Äúrolling back‚Äù the other pod, e.g. it has to be re-scheduled. But the chance and resulting impact of this happening is low - low enough that you get much higher throughput by running in parallel and absorbing the wasted effort if it does happen.&lt;/p&gt;
    &lt;p&gt;So my initial architecture idea of the distributed scheduler:&lt;/p&gt;
    &lt;p&gt;Scatter-gather as a Kubernetes pod scheduler pattern&lt;/p&gt;
    &lt;p&gt;The Relay starts a watch on unscheduled pods against the kube-apiserver. As streams of pods come in, the Relay forwards them to different schedulers.&lt;lb/&gt; Each Scheduler is responsible for doing filtering and scoring against its own subset of the overall nodes, then sending back to Relay the top-winning node and score.&lt;/p&gt;
    &lt;p&gt;The Relay then aggregates these scores, picks the overall winner, sends a true/false back to the Scheduler, and that true/false dictates whether the scheduler should actually bind the pod to that node.&lt;/p&gt;
    &lt;p&gt;The scheduler here is thus a slightly modified version of the upstream kube-scheduler. It has a custom gRPC server endpoint for receiving the new pod. It has custom code to know which of the overall nodes it is responsible for. And it has a custom Permit extension point for sending the proposed node back to the Relay. The Permit extension point runs after the nodes are filtered and scored, but before the pod is bound. Permit extensions return ‚Äòtrue‚Äô or ‚Äòfalse‚Äô to approve whether or not the pod should be scheduled on the specified node.&lt;/p&gt;
    &lt;p&gt;This is the basic design and it works pretty well. It doesn‚Äôt quite work up to 1M node scale - we‚Äôll talk about that next - but it delivers a much more scalable scheduler solution than what exists today, while preserving all of the nuanced complex battle-tested logic of the current system.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs scheduler is effectively O(n x p), where n is the number of nodes and p the number of pods. That complexity becomes untenable as n grows. The sharded approach helps counteract the scaling problem: if you have n nodes, then you can shard the work across r replicas where r is some factor of n, turning that large factor back into something more tractable.&lt;/p&gt;
    &lt;p&gt;I should mention there‚Äôs one fairly large exception, and that‚Äôs pod evictions. Pod eviction can occur when there‚Äôs a new pod to schedule, but there‚Äôs not enough available resources currently on the cluster to schedule that pod. When this occurs, the scheduler does a scan across all pods currently running in the cluster, trying to identify a set of lower-priority pods that, if they were to be killed, would leave enough space for this new pod. To be fair, I didn‚Äôt implement this. You could squint at the current approach and imagine how we could also distribute the work of eviction calculation, but I didn‚Äôt do it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The painful long tail: running large distributed systems in reality&lt;/head&gt;
    &lt;p&gt;On my hardware, a single scheduler was able to filter and score a pod against 1K nodes in about 1ms. So we could do 1K pods on 1K nodes in 1s. Remember the goal was 1M pods on 1M nodes in 60s. Recall that the overall work is O(n x p) (each pod has to be evaluated against each node), so going from 1K pods and nodes to 1M pods &amp;amp; 1M nodes is not a factor of 1K more work, but 1K*1K, or 1 million times more work. Even allowing ourselves 60s instead of 1s, we‚Äôre going to need a lot more schedulers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Add more relays and distribute the score gathering&lt;/head&gt;
    &lt;p&gt;In fact, we‚Äôre going to need so many schedulers that a single relay simply doesn‚Äôt have enough network bandwidth to send to all of them in enough time. We need multiple relays. In fact we actually need multiple levels of relays to reach all of the schedulers.&lt;/p&gt;
    &lt;p&gt;Similarly the gathering stage, of collecting all scores and determining a winner, can also be distributed. Each scheduler and relay has a Score &lt;code&gt;Gather`&lt;/code&gt; endpoint, and it‚Äôs determined via a hash of the pod name to determine which scheduler is responsible for gathering the scores of a particular pod.&lt;/p&gt;
    &lt;p&gt;Here is a simplified example of what more relays looks like. This is with a fanout of 3, while in reality I used a fanout of 10. I was aiming to maximize but not exceed the maximum transmit throughput of each NIC to transmit 1M * 4K of Pod data in 60 seconds.&lt;/p&gt;
    &lt;p&gt;Note that it‚Äôs packed. Not all schedulers are on the same level.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fight long-tail latency&lt;/head&gt;
    &lt;p&gt;My goal was to see linear time reductions as I added more replicas. In reality, I started hitting a plateau, where no matter how many more replicas I added, things remained the same or even got worse. While on average, most of the schedulers were doing less work and thus finishing more quickly, it became more frequent to see one or two stragglers that were not faster at all. This was a problem because I needed all schedulers to report back their best node before we could pick a winner.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a well-known Google paper by Jeff Dean called The Tail of Scale that talks about exactly this problem. Our servers aren‚Äôt running real-time OSes and software. They are busy with all sorts of miscellaneous background tasks; observability, upgrades, garbage collection. Garbage collection is a big problem in Golang if you‚Äôre trying to write tightly coordinated software. It interrupts a currently-running task or defers a queued one. Suddenly a task that usually takes 300 microseconds spikes to take 1 millisecond. With enough individual servers, inevitably someone is always taking that 1 millisecond. If you have tightly timed coordinated systems that rely on everyone to respond before proceeding, 99% of your servers are waiting for that long-tail 1% to finish.&lt;/p&gt;
    &lt;p&gt;So I implemented a few things to reduce this problem:&lt;/p&gt;
    &lt;p&gt;Use pinned CPUs. It‚Äôs a way to ensure that cpu cores are dedicated to one single container‚Äôs processes, not context switching between various random processes. In kubelet this is done via ‚ÄúCPU Manager Policy‚Äù. Just specifying this made my tasks much more consistently performing.&lt;/p&gt;
    &lt;p&gt;Tweak garbage collection. Increasing GOGC above 100 can reduce the amount of overall time spent in garbage collection at the expense of using more memory. Using an aggressive GOGC plus a GOMEMLIMIT near towards your actual memory limit is a fantastic way to ensure that you only do GC when you really need to. I use a GOGC value of 800 and a GOMEMLIMIT set to 90% of the container‚Äôs memory limit.&lt;/p&gt;
    &lt;p&gt;Give up on stragglers. Simply don‚Äôt wait for the last N% to respond. This can effectively cut off your long tail. Beware that if there‚Äôs one consistently slow node, then this can create a feedback loop, hammering that server with more and more requests will just make it slower until it totally melts down.&lt;/p&gt;
    &lt;p&gt;Really you should just go read the The Tail of Scale paper, it covers several other possible scenarios and fixes.&lt;/p&gt;
    &lt;p&gt;One thing that I did not do is overlap workloads across multiple servers. I could‚Äôve assigned each kube node to multiple shards, so that any one of them can calculate and score the pods on that node. I worried this would result in too many cases of data inconsistency, where the node became over-subscribed with pods because the various shards were not consistent with one-another about which pods had been scheduled on that node.&lt;/p&gt;
    &lt;head rend="h5"&gt;Replace watcher with AdmissionWebHook&lt;/head&gt;
    &lt;p&gt;This one remains a bit of a mystery. The kube-scheduler typically learns about pods to schedule by doing a watch with fieldSelector ‚Äòspec.nodeName=‚Äô (meaning, pods that have no current nodeName set). When creating a lot of pods quickly (over &amp;gt;5K/sec), the watch stream would frequently stall for tens of seconds at a time.&lt;/p&gt;
    &lt;p&gt;This was one particularly bad example:&lt;/p&gt;
    &lt;p&gt;Sometimes even though there‚Äôd be plenty of pods to schedule, the watch stream had stalled so badly that the scheduler would be starved of pods to process.&lt;/p&gt;
    &lt;p&gt;To overcome this, I made the scheduler take a rather extreme change of interface. Rather than creating a watch, I made the scheduler a ValidatingWebhook. This made it so the kube-apiserver would hit an HTTP endpoint on the scheduler with every new pod that was created, in line with the create request. Typical Validating Webhooks are used for security, to approve or deny some resource fields from being set by particular clients. In this case, the scheduler approved all pods. It was merely a way for it to learn about every new pod faster (and synchronously) than by using a watch stream.&lt;/p&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;Created a 100K node cluster and then timed how long it takes to schedule 100K pods. The pods have no nodeSelector or affinity.&lt;/p&gt;
    &lt;p&gt;Each scheduler ran on a dedicated c4d-standard-32; that is 32 AMD Turin cores and 128GB of DDR5 RAM. Experiments where dist-schedulers that had more than 1 replica also had 1 dedicated dist-scheduler-relay VM.&lt;/p&gt;
    &lt;p&gt;Each dist-scheduler replica is configured to run 30 separate internal schedulers, each with a parallelism setting of ‚Äò2‚Äô.&lt;/p&gt;
    &lt;p&gt;The default-scheduler is kube-scheduler 1.32.3 with no modifications.&lt;/p&gt;
    &lt;p&gt;One puzzling result is how much better a 1x dist-scheduler performed than the default-scheduler. Adjusting the &lt;code&gt;parallelism&lt;/code&gt; setting had no impact on the performance nor the CPU-seconds, which seemed to peak at about 20 (leaving 12 cores free).&lt;/p&gt;
    &lt;p&gt;Note that adding replicas to dist-scheduler did result in a more-or-less linear time improvement. In other words, doubling the number of dist-scheduler replicas results in a halving of the time to completion. This trend continues to the 256x replica/1M pod scale as we‚Äôll see in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experiments&lt;/head&gt;
    &lt;head rend="h3"&gt;1M nodes, 1M pods with kwok&lt;/head&gt;
    &lt;head rend="h4"&gt;Test setup:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 5x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;kube-scheduler and kube-controller-manager v1.32.4 each run as a separate process on the same VMs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;code&gt;feature-gates=kube:BtreeWatchCache=false&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-16 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For dist-scheduler: 285x c4d-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 7x c4a-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Both running kubelet via k3s v1.32.4+k3s1&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kwok: 100x pods running a modified version of kwok v0.6.0&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pod scheduler: 289 replicas (8670 AMD Turin cores) of custom distributed scheduler implementation, consisting of 256 schedulers and 29 relays.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Start all VMs. Wait for kwok and dist-scheduler to be fully running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M nodes via make_nodes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for 1M nodes and 1M leases to be present in the etcd database&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M pods via create-pods&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for all pods to have a spec.nodeName&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;In the below graphs, the green line annotation indicates when the first pod was created and the red line indicates when the millionth pod was scheduled.&lt;/p&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h5"&gt;scheduler&lt;/head&gt;
    &lt;head rend="h3"&gt;Kwok vs kubelet&lt;/head&gt;
    &lt;p&gt;So far all of these experiments are run using kwok instead of real kubelets. But how realistic is that? It‚Äôs possible kwok doesn‚Äôt generate nearly the same pattern of load as real kubelets, and so these experiments wouldn‚Äôt be representative of a real-life cluster with 1M nodes each running kubelet.&lt;/p&gt;
    &lt;p&gt;Unfortunately running 1M real kubelets is beyond my budget. But maybe we can run a smaller-scale experiment with real kubelets and measure how its workload compares to an equivalent-sized kwok cluster.&lt;/p&gt;
    &lt;p&gt;With some careful configuration, I can run a test of a 100K kubelet cluster. The trick is to run many kubelets at once all on the same VM. They each run in separate Linux namespaces off of the host. They each have their own IPv6 address and range from which they can allocate pod addresses. They each run their own copy of containerd with which to allocate nested pods.&lt;/p&gt;
    &lt;p&gt;Deploying and managing 100K kubelet containers across lots of VMs sounds difficult. If only there were software to orchestrate this‚Ä¶ Aha! We can create a Kubernetes Deployment of kubelets!&lt;/p&gt;
    &lt;p&gt;There are still some single points of contention because of the shared kernel. Kube-proxy by default uses iptables, and changes to iptables are done with a mutex. Nftables is more performant and more friendly to concurrency but nevertheless remains a bottleneck. So we will do better to have lots of small VMs rather than fewer big ones to spread out our concurrency constraints.&lt;/p&gt;
    &lt;p&gt;Additionally, for the IPv6 subnetworks of each kubelet to be reachable from the cloud provider, we need to propagate neighbor advertisement packets. I deploy ndppd on each VM (as a DaemonSet) to do this.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 6x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-8 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For kubelet-in-pod: 426x c4d-highmem-8‚Äôs (3408 AMD Turin cores and 27264GiB of RAM)&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Running k3s v1.32.4+k3s1&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 2x c4a-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;kubelet-pod: Running a slightly modified version of the k3s v1.32.4+k3s1 image where&lt;/p&gt;&lt;code&gt;libjansson&lt;/code&gt;is installed, which adds json support to nftables, required for kubelet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for cluster to boot and all VM nodes to go Ready&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy Deployment of kubelet-as-pod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scale up to 100K replicas&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tear down and re-create cluster&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy kwok. Create 100K kwok nodes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion: How large can a Kubernetes cluster be?&lt;/head&gt;
    &lt;p&gt;The truth is that cluster size matters far less than the rate of operations on any single resource Kind‚Äîespecially creates and updates. Operations on different Kinds are isolated: each runs in its own goroutine protected by its own mutex. You can even shard across multiple etcd clusters by resource kind, so cross-kind modifications scale relatively independently.&lt;/p&gt;
    &lt;p&gt;The biggest source of writes is ususally Lease updates that keep Nodes alive. That makes cluster size fundamentally constrained by how quickly the system can process those updates.&lt;/p&gt;
    &lt;p&gt;A standard etcd setup on modern hardware sustains roughly 50,000 modifications per second. With careful sharding (separate etcd clusters for Nodes, Leases, and Pods), you could likely support around 500,000 nodes with standard etcd.&lt;/p&gt;
    &lt;p&gt;Replacing etcd with a more scalable backend shifts the bottleneck to the kube-apiserver‚Äôs watch cache. Each resource Kind today is guarded by a single RWMutex over a B-tree. Replacing that with a hash map can likely support ~100,000 events/second, enough to support 1 million nodes on current hardware. To go beyond that, increase the Lease interval (e.g., &amp;gt;10s) to reduce modification rate.&lt;/p&gt;
    &lt;p&gt;At scale, the biggest aggregate limiter is Go‚Äôs garbage collector. The kube-apiserver creates and discards vast numbers of small objects when parsing and decoding resources, and this churn drives GC pressure. Adding more kube-apiserver replicas doesn‚Äôt help, since all of them are subscribed to the same event streams.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to run yourself&lt;/head&gt;
    &lt;p&gt;See the RUNNING file for instructions on how to run a cluster yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://bchess.github.io/k8s-1m/"/><published>2025-10-16T22:04:57+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45613898</id><title>Titan submersible‚Äôs $62 SanDisk memory card found undamaged at wreckage site</title><updated>2025-10-18T20:35:12.820343+00:00</updated><content>&lt;doc fingerprint="a50f2c926b2b1f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tragic Titan submersible‚Äôs $62 SanDisk memory card found undamaged at wreckage site ‚Äî 12 stills and nine videos have been recovered, but none from the fateful OceanGate implosion&lt;/head&gt;
    &lt;p&gt;The specialist camera was rated to 6,000m, but the lens and some of its components were probably damaged by the implosion.&lt;/p&gt;
    &lt;p&gt;Recovery teams working on the Titan submersible have found the vessel's specialist stills and video camera intact. Fascinatingly, while there was some damage to the camera‚Äôs housing and internal components, tech and science enthusiast Scott Manley reveals that the internal SD card was ‚Äúundamaged.‚Äù Contents of the memory card have since been investigated, and 12 stills and nine videos have been recovered.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The recovery teams found a hardened underwater camera in the wreckage of the Titan submersible, and inside the casing was an undamaged SD card. pic.twitter.com/QCOtdcS7dUOctober 15, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Click 'See more' for images.&lt;/p&gt;
    &lt;p&gt;In the images, you can see a SubC-branded Rayfin Mk2 Benthic Camera, recovered from the wreckage of the ill-fated submersible operated by OceanGate. This still and video camera is rated to withstand depths up to 6,000m (19,685 feet, 3,281 fathoms). The titanium and synthetic sapphire crystal constructed device features both onboard and expansion memory (the titular SD card).&lt;/p&gt;
    &lt;head rend="h2"&gt;Casing intact, lens shattered but remained in place&lt;/head&gt;
    &lt;p&gt;Inside the camera's tube-like form, it is observed that the PCBs had suffered some slight damage. For example, connectors between two boards were sheared off, and some surface mount components were similarly damaged.&lt;/p&gt;
    &lt;p&gt;In some images of the PCB that are shared, you will notice details are blurred at the request of the Canada-based underwater imaging specialist and Rayfin Mk2 Benthic Camera manufacturer. However, SubC‚Äôs requested trade secret obfuscation hasn‚Äôt stopped internet sleuths asserting that they know exactly what components have been redacted.&lt;/p&gt;
    &lt;p&gt;Picking through Manley's Tweet thread replies, the key PCBs in the camera were likely an Inforce 6601 System on Module (SoM) based on the Qualcomm SD820 processor, which comes with 4GB of RAM on board and 64GB of UFS storage. Another component is thought to be the Teensy 4.0 or 3.2, which acted as an MCU. Last but not least, the undamaged SD card is almost certainly a SanDisk Extreme Pro 512GB ($62.99 at the time of writing), though it was de-branded in the photos.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data recovery process and results&lt;/head&gt;
    &lt;p&gt;With an undamaged SD card, of course, investigators (and others) were interested in what details of the tragedy may have been captured and stored by this camera system. The first step was to make ‚Äúan exact binary image of the SD card‚Äù so the original could be left untampered.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Check out Manley‚Äôs Tweet thread, containing investigator report screengrabs, to take in the full gamut of back and forth between the data forensics investigator, Canada‚Äôs Transportation Safety Board of Canada (TSB), and SubC. To cut a long technical story short, though, the parties eventually met up at a lab/office in Newfoundland, where a recovered NVRAM chip and SD card image were interfaced with a ‚Äúsurrogate SoM board.‚Äù This did the trick, and 12 still and nine videos were recovered!&lt;/p&gt;
    &lt;p&gt;Recovered images were at a resolution of 4,056 x 3,040 pixels, implying a pretty common 12.3MP sensor was used by the SubC Rayfin Mk2 Benthic Camera. Videos were at a more standard 3,840 x 2,160 pixels ‚Äì commonly referred to as 4K Ultra HD (UHD) video.&lt;/p&gt;
    &lt;p&gt;Somewhat disappointingly, the images and videos shared in the report were taken in the vicinity of the ROV shop at the Marine Institute, also in Newfoundland. The location was the logistical base for Titanic dive missions. No deep-sea shenanigans around the Titanic wreck were revealed. Manley explains in his Twitter thread that ‚Äúthe camera had been configured to dump data onto an external storage device, so nothing was found from the accident dive.‚Äù Nothing particularly pertinent to the tragic accident, that is.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/pc-components/microsd-cards/tragic-oceangate-titan-submersibles-usd62-sandisk-memory-card-found-undamaged-at-wreckage-site-12-stills-and-nine-videos-have-been-recovered-but-none-from-the-fateful-implosion"/><published>2025-10-17T06:39:09+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45619108</id><title>Is Postgres read heavy or write heavy?</title><updated>2025-10-18T20:35:12.488362+00:00</updated><content>&lt;doc fingerprint="ea4bf8ff29727bda"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Postgres Read Heavy or Write Heavy? (And Why You Should You Care)&lt;/head&gt;
    &lt;p&gt;10 min readMore by this author&lt;/p&gt;
    &lt;p&gt;When someone asks about Postgres tuning, I always say ‚Äúit depends‚Äù. What ‚Äúit‚Äù is can vary widely but one major factor is the read and write traffic of a Postgres database. Today let‚Äôs dig into knowing if your Postgres database is read heavy or write heavy.&lt;/p&gt;
    &lt;p&gt;Of course write heavy or read heavy can largely be inferred from your business logic. Social media app - read heavy. IoT logger - write heavy. But ‚Ä¶. Many of us have mixed use applications. Knowing your write and read load can help you make other decisions about tuning and architecture priorities with your Postgres fleet.&lt;/p&gt;
    &lt;p&gt;Understanding whether a Postgres database is read-heavy or write-heavy is paramount for effective database administration and performance tuning. For example, a read-heavy database might benefit more from extensive indexing, query caching, and read replicas, while a write-heavy database might require optimizations like faster storage, efficient WAL (Write-Ahead Log) management, table design considerations (such as fill factor and autovacuum tuning) and careful consideration of transaction isolation levels.&lt;/p&gt;
    &lt;p&gt;By reviewing a detailed read/write estimation, you can gain valuable insights into the underlying workload characteristics, enabling informed decisions for optimizing resource allocation and improving overall database performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Read and writes are not really equal&lt;/head&gt;
    &lt;p&gt;The challenge here in looking at Postgres like this is that reads and writes are not really equal.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Postgres reads data in whole 8kb units, called blocks on disk or pages once they‚Äôre part of the shared memory. The cost of reading is much lower than writing. Since the most frequently used data generally resides in the shared buffers or the OS cache, many queries never need additional physical IO and can return results just from memory.&lt;/item&gt;
      &lt;item&gt;Postgres writes by comparison are a little more complicated. When changing an individual tuple, Postgres needs to write data to WAL defining what happens. If this is the first write after a checkpoint, this could include a copy of the full data page. This also can involve writing additional data for any index changes, toast table changes, or toast table indexes. This is the direct write cost of a single database change, which is done before the commit is accepted. There is also the IO cost for writing out all dirty page buffers, but this is generally done in the background by the background writer. In addition to these write IO costs, the data pages need to be in memory in order to make changes, so every write operation also has potential read overhead as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That being said - I‚Äôve worked on a query using internal table statistics that loosely estimates read load and write load.&lt;/p&gt;
    &lt;head rend="h2"&gt;Query Postgres for read and write traffic&lt;/head&gt;
    &lt;p&gt;This query leverages Postgres‚Äô internal metadata to provide an estimation of the number of disk pages (or blocks) that have been directly affected by changes to a given number of tuples (rows). This estimation is crucial for understanding the read/write profile of a database, which in turn can inform optimization strategies (see below).&lt;/p&gt;
    &lt;p&gt;The query's logic is broken down into several Common Table Expressions (CTEs) to enhance readability and modularity:&lt;/p&gt;
    &lt;p&gt;ratio_target CTE:&lt;/p&gt;
    &lt;p&gt;This initial CTE is designed to establish a predefined threshold. It allows the user to specify a target ratio of read pages per write page. This ratio serves as the primary criteria for classifying a database or table as either read-heavy or write-heavy.&lt;/p&gt;
    &lt;p&gt;I‚Äôve set the ratio in the query to 5 reads : 1 write, which means that roughly 20% of the database activity would be writes in this case. This is a bit of a fudge factor number and the exact definition of what makes up a write-heavy database may differ. If you set to 100, it would consider 100 reads to be equivalent to 1 write, or 1%; this is to allow you to tweak the definitions here for the classifications.&lt;/p&gt;
    &lt;p&gt;By defining this threshold explicitly, the query provides a flexible mechanism for evaluating different performance characteristics based on specific application requirements. For instance, a higher ratio_target might indicate a preference for read-intensive operations, while a lower one might suggest a workload dominated by writes.&lt;/p&gt;
    &lt;p&gt;table_list CTE&lt;/p&gt;
    &lt;p&gt;This CTE is responsible for the core calculations necessary to determine the read and write page counts. It performs the following key functions:&lt;/p&gt;
    &lt;p&gt;Total read pages:&lt;/p&gt;
    &lt;p&gt;It calculates the total number of pages that are typically read for the tables under consideration. This metric is fundamental to assessing the read demand placed on the database.&lt;/p&gt;
    &lt;p&gt;Estimated changed pages for writes:&lt;/p&gt;
    &lt;p&gt;To estimate the number of pages affected by write operations, the table_list CTE utilizes the existing relpages (total pages) and reltuples (total tuples) statistics from the pg_class system catalog. By calculating the ratio of relpages to reltuples, the query derives an estimated density of tuples per page. This density is then applied to the observed number of tuple writes to project how many physical pages were likely impacted by these write operations. This approach provides a practical way to infer disk I/O related to writes without needing to track every individual page modification.&lt;/p&gt;
    &lt;p&gt;Final comparison and classification&lt;/p&gt;
    &lt;p&gt;After the table_list CTE has computed the estimated read pages and write-affected pages, the final stage of the query involves a comparative analysis. The calculated number of read pages is directly compared against the estimated number of write pages. Based on this comparison, and in conjunction with the ratio_target defined earlier, the query then classifies each table (or the database as a whole) into one of several categories. These categories typically include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read-heavy: This classification is applied when the proportion of read pages significantly outweighs the write pages, based on the defined ratio_target.&lt;/item&gt;
      &lt;item&gt;Write-heavy: Conversely, this classification indicates that write operations are more prevalent, with a higher number of write-affected pages relative to read pages.&lt;/item&gt;
      &lt;item&gt;Other scenarios: The query can also identify other scenarios, such as balanced workloads where read and write operations are roughly equivalent, or cases where the data volume is too low to make a definitive classification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The read/write Postgres query:&lt;/p&gt;
    &lt;code&gt;WITH
ratio_target AS (SELECT 5 AS ratio),
table_list AS (SELECT
 s.schemaname,
 s.relname AS table_name,
 -- Sum of heap and index blocks read from disk (from pg_statio_user_tables)
 si.heap_blks_read + si.idx_blks_read AS blocks_read,
 -- Sum of all write operations (tuples) (from pg_stat_user_tables)
s.n_tup_ins + s.n_tup_upd + s.n_tup_del AS write_tuples,
relpages * (s.n_tup_ins + s.n_tup_upd + s.n_tup_del ) / (case when reltuples = 0 then 1 else reltuples end) as blocks_write
FROM
 -- Join the user tables statistics view with the I/O statistics view
 pg_stat_user_tables AS s
JOIN pg_statio_user_tables AS si ON s.relid = si.relid
JOIN pg_class c ON c.oid = s.relid
WHERE
 -- Filter to only show tables that have had some form of read or write activity
(s.n_tup_ins + s.n_tup_upd + s.n_tup_del) &amp;gt; 0
AND
 (si.heap_blks_read + si.idx_blks_read) &amp;gt; 0
 )
SELECT *,
 CASE
   -- Handle case with no activity
   WHEN blocks_read = 0 and blocks_write = 0 THEN
     'No Activity'
   -- Handle write-heavy tables
   WHEN blocks_write * ratio &amp;gt; blocks_read THEN
     CASE
       WHEN blocks_read = 0 THEN 'Write-Only'
       ELSE
         ROUND(blocks_write :: numeric / blocks_read :: numeric, 1)::text || ':1 (Write-Heavy)'
     END
   -- Handle read-heavy tables
   WHEN blocks_read &amp;gt; blocks_write * ratio THEN
     CASE
       WHEN blocks_write = 0 THEN 'Read-Only'
       ELSE
         '1:' || ROUND(blocks_read::numeric / blocks_write :: numeric, 1)::text || ' (Read-Heavy)'
     END
   -- Handle balanced tables
   ELSE
     '1:1 (Balanced)'
 END AS activity_ratio
FROM table_list, ratio_target
ORDER BY
 -- Order by the most active tables first (sum of all operations)
 (blocks_read + blocks_write) DESC;
&lt;/code&gt;
    &lt;p&gt;Results will look something like this:&lt;/p&gt;
    &lt;code&gt;schemaname |¬† table_name ¬† | blocks_read | write_tuples | blocks_write | ratio | ¬† ¬† activity_ratio

- -----------+---------------+-------------+--------------+--------------+-------+------------------------

public ¬† ¬† | audit_logs¬† ¬† | ¬† ¬† ¬† ¬† ¬† 2 |¬† ¬† ¬† 1500000 |¬† ¬† ¬† ¬† 18519 | ¬† ¬† 5 | 9259.5:1 (Write-Heavy)
public ¬† ¬† | orders¬† ¬† ¬† ¬† | ¬† ¬† ¬† ¬† ¬† 8 |¬† ¬† ¬† ¬† ¬† ¬† 4 | ¬† ¬† ¬† ¬† ¬† -0 | ¬† ¬† 5 | Read-Only
public ¬† ¬† | articles¬† ¬† ¬† | ¬† ¬† ¬† ¬† ¬† 2 | ¬† ¬† ¬† ¬† ¬† 10 |¬† ¬† ¬† ¬† ¬† ¬† 1 | ¬† ¬† 5 | 0.5:1 (Write-Heavy)
public ¬† ¬† | user_profiles | ¬† ¬† ¬† ¬† ¬† 1 |¬† ¬† ¬† ¬† ¬† ¬† 3 | ¬† ¬† ¬† ¬† ¬† -0 | ¬† ¬† 5 | Read-Only
&lt;/code&gt;
    &lt;head rend="h3"&gt;pg_stat_statements&lt;/head&gt;
    &lt;p&gt;Another way to look at read and write traffic is through the pg_stat_statements extension. It aggregates statistics for every unique query run on your database. It also will collect data about Postgres queries row by row.&lt;/p&gt;
    &lt;p&gt;While the above query accounts for a bit more distribution in workload, pg_stat_statements is also a good checkpoint for traffic volume.&lt;/p&gt;
    &lt;code&gt;SELECT
  SUM(CASE WHEN query ILIKE 'SELECT%' THEN rows ELSE 0 END) AS rows_read,
   SUM(CASE WHEN query ILIKE 'INSERT%' OR query ILIKE 'UPDATE%' OR query ILIKE 'DELETE%' THEN rows ELSE 0 END) AS rows_written
FROM pg_stat_statements;

 cache_hits | disk_reads | rows_read | rows_written
------------+------------+-----------+--------------
      27586 |        998 |    443628 |           30
(1 row)
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance Tuning for High Write Traffic in Postgres&lt;/head&gt;
    &lt;p&gt;For write-heavy systems, the bottleneck is often I/O and transaction throughput. You're constantly writing to the disk, which is slower than reading from memory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Faster Storage: The most direct way to improve write performance is to use faster storage, such as NVMe SSDs, and provision more I/O operations per second (IOPS).&lt;/item&gt;
      &lt;item&gt;Postgres 18 now has asynchronous I/O which should be more performant than traditional methods.&lt;/item&gt;
      &lt;item&gt;More RAM: While reads benefit from RAM for caching too, writes also benefit from a larger shared_buffers pool, which can hold more dirty pages before they need to be flushed to disk.&lt;/item&gt;
      &lt;item&gt;I/O burst systems: Many cloud based systems come with extra I/O out of the box, so looking at these numbers may also be helpful.&lt;/item&gt;
      &lt;item&gt;Minimize Indexes: While essential for reads, every index needs to be updated during a write operation. Over-indexing can significantly slow down writes so remove unused indexes.&lt;/item&gt;
      &lt;item&gt;Utilizing HOT updates: Postgres has a performance improvement for frequently updated rows that are indexed, so adjusting fill factor to take advantage of this could be worth looking into.&lt;/item&gt;
      &lt;item&gt;Tune the WAL (Write-Ahead Log): The WAL is where every change is written before it's committed to the main database files. Tuning parameters like wal_buffers can reduce the number of disk flushes and improve write performance.&lt;/item&gt;
      &lt;item&gt;Optimize Checkpoints: Checkpoints sync the data from shared memory to disk. Frequent or large checkpoints can cause I/O spikes. Adjusting checkpoint_timeout and checkpoint_completion_target can smooth out these events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance tuning for read traffic&lt;/head&gt;
    &lt;p&gt;For read-heavy systems, the primary goal is to get data to the user as quickly as possible and ideally have much data in the buffer cache so it is not reading from disk.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Effective Caching: Ensure your shared_buffers and effective_cache_size are configured to take advantage of available RAM. This lets Postgres keep frequently accessed data in memory, avoiding costly disk reads.&lt;/item&gt;
      &lt;item&gt;Optimize Queries and Indexes: Use EXPLAIN ANALYZE to pinpoint slow SELECT queries and add indexes on columns used in WHERE clauses, JOIN conditions, and ORDER BY statements. Remember, indexes speed up lookups at the cost of slower writes.&lt;/item&gt;
      &lt;item&gt;Scaling out with read replicas: A read replica is a copy of your primary database that's kept in sync asynchronously. All write operations go to the primary, but you can distribute read queries across one or more replicas. This distributes the read load, offloads traffic from your primary server, and can dramatically improve read throughput without impacting your write performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Most Postgres databases are read heavy&lt;/head&gt;
    &lt;p&gt;Most Postgres databases are going to be far more read heavy than write heavy. I estimate just based on experience that 10:1 reads to writes is probably something where it is starting to get write heavy. Of course, there are outliers to this.&lt;/p&gt;
    &lt;p&gt;The right scaling strategy depends entirely on your workload. By proactively monitoring your Postgres stats using internal statistics in the Postgres catalog, you can make informed decisions that will keep your database healthy and your application fast.&lt;/p&gt;
    &lt;p&gt;Co-authored with Elizabeth Christensen&lt;/p&gt;
    &lt;head rend="h2"&gt;Related Articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is Postgres Read Heavy or Write Heavy? (And Why You Should You Care)&lt;p&gt;10 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;PostGIS Performance: Indexing and EXPLAIN&lt;p&gt;3 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres Migrations Using Logical Replication&lt;p&gt;7 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres 18: OLD and NEW Rows in the RETURNING Clause&lt;p&gt;2 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres‚Äô Original Project Goals: The Creators Totally Nailed It&lt;p&gt;9 min read&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.crunchydata.com/blog/is-postgres-read-heavy-or-write-heavy-and-why-should-you-care"/><published>2025-10-17T17:06:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45619329</id><title>Andrej Karpathy ‚Äì¬†It will take a decade to work through the issues with agents</title><updated>2025-10-18T20:35:11.895890+00:00</updated><content>&lt;doc fingerprint="2771005f5ee40ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;The Andrej Karpathy episode.&lt;/p&gt;
    &lt;p&gt;Andrej explains why reinforcement learning is terrible (but everything else is much worse), why model collapse prevents LLMs from learning the way humans do, why AGI will just blend into the previous ~2.5 centuries of 2% GDP growth, why self driving took so long to crack, and what he sees as the future of education.&lt;/p&gt;
    &lt;p&gt;Watch on YouTube; listen on Apple Podcasts or Spotify.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sponsors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Labelbox helps you get data that is more detailed, more accurate, and higher signal than you could get by default, no matter your domain or training paradigm. Reach out today at labelbox.com/dwarkesh&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mercury helps you run your business better. It‚Äôs the banking platform we use for the podcast ‚Äî we love that we can see our accounts, cash flows, AR, and AP all in one place. Apply online in minutes at mercury.com&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google‚Äôs Veo 3.1 update is a notable improvement to an already great model. Veo 3.1‚Äôs generations are more coherent and the audio is even higher-quality. If you have a Google AI Pro or Ultra plan, you can try it in Gemini today by visiting https://gemini.google&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Timestamps&lt;/head&gt;
    &lt;p&gt;(00:00:00) ‚Äì AGI is still a decade away&lt;/p&gt;
    &lt;p&gt;(00:29:45) ‚Äì LLM cognitive deficits&lt;/p&gt;
    &lt;p&gt;(00:49:38) ‚Äì How do humans learn?&lt;/p&gt;
    &lt;p&gt;(01:06:25) ‚Äì AGI will blend into 2% GDP growth&lt;/p&gt;
    &lt;p&gt;(01:32:50) ‚Äì Evolution of intelligence &amp;amp; culture&lt;/p&gt;
    &lt;p&gt;(01:42:55) - Why self driving took so long&lt;/p&gt;
    &lt;p&gt;(01:56:20) - Future of education&lt;/p&gt;
    &lt;head rend="h3"&gt;Transcript&lt;/head&gt;
    &lt;head rend="h3"&gt;00:00:00 ‚Äì AGI is still a decade away&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:00:00&lt;/p&gt;
    &lt;p&gt;Today I‚Äôm speaking with Andrej Karpathy. Andrej, why do you say that this will be the decade of agents and not the year of agents?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:00:07&lt;/p&gt;
    &lt;p&gt;First of all, thank you for having me here. I‚Äôm excited to be here.&lt;/p&gt;
    &lt;p&gt;The quote you‚Äôve just mentioned, ‚ÄúIt‚Äôs the decade of agents,‚Äù is actually a reaction to a pre-existing quote. I‚Äôm not actually sure who said this but they were alluding to this being the year of agents with respect to LLMs and how they were going to evolve. I was triggered by that because there‚Äôs some over-prediction going on in the industry. In my mind, this is more accurately described as the decade of agents.&lt;/p&gt;
    &lt;p&gt;We have some very early agents that are extremely impressive and that I use daily‚ÄîClaude and Codex and so on‚Äîbut I still feel there‚Äôs so much work to be done. My reaction is we‚Äôll be working with these things for a decade. They‚Äôre going to get better, and it‚Äôs going to be wonderful. I was just reacting to the timelines of the implication.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:00:58&lt;/p&gt;
    &lt;p&gt;What do you think will take a decade to accomplish? What are the bottlenecks?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:01:02&lt;/p&gt;
    &lt;p&gt;Actually making it work. When you‚Äôre talking about an agent, or what the labs have in mind and maybe what I have in mind as well, you should think of it almost like an employee or an intern that you would hire to work with you. For example, you work with some employees here. When would you prefer to have an agent like Claude or Codex do that work?&lt;/p&gt;
    &lt;p&gt;Currently, of course they can‚Äôt. What would it take for them to be able to do that? Why don‚Äôt you do it today? The reason you don‚Äôt do it today is because they just don‚Äôt work. They don‚Äôt have enough intelligence, they‚Äôre not multimodal enough, they can‚Äôt do computer use and all this stuff.&lt;/p&gt;
    &lt;p&gt;They don‚Äôt do a lot of the things you‚Äôve alluded to earlier. They don‚Äôt have continual learning. You can‚Äôt just tell them something and they‚Äôll remember it. They‚Äôre cognitively lacking and it‚Äôs just not working. It will take about a decade to work through all of those issues.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:01:44&lt;/p&gt;
    &lt;p&gt;Interesting. As a professional podcaster and a viewer of AI from afar, it‚Äôs easy for me to identify what‚Äôs lacking: continual learning is lacking, or multimodality is lacking. But I don‚Äôt really have a good way of trying to put a timeline on it. If somebody asks how long continual learning will take, I have no prior about whether this is a project that should take 5 years, 10 years, or 50 years. Why a decade? Why not one year? Why not 50 years?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:02:16&lt;/p&gt;
    &lt;p&gt;This is where you get into a bit of my own intuition, and doing a bit of an extrapolation with respect to my own experience in the field. I‚Äôve been in AI for almost two decades. It‚Äôs going to be 15 years or so, not that long. You had Richard Sutton here, who was around for much longer. I do have about 15 years of experience of people making predictions, of seeing how they turned out. Also I was in the industry for a while, I was in research, and I‚Äôve worked in the industry for a while. I have a general intuition that I have left from that.&lt;/p&gt;
    &lt;p&gt;I feel like the problems are tractable, they‚Äôre surmountable, but they‚Äôre still difficult. If I just average it out, it just feels like a decade to me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:02:57&lt;/p&gt;
    &lt;p&gt;This is quite interesting. I want to hear not only the history, but what people in the room felt was about to happen at various different breakthrough moments. What were the ways in which their feelings were either overly pessimistic or overly optimistic? Should we just go through each of them one by one?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:03:16&lt;/p&gt;
    &lt;p&gt;That‚Äôs a giant question because you‚Äôre talking about 15 years of stuff that happened. AI is so wonderful because there have been a number of seismic shifts where the entire field has suddenly looked a different way. I‚Äôve maybe lived through two or three of those. I still think there will continue to be some because they come with almost surprising regularity.&lt;/p&gt;
    &lt;p&gt;When my career began, when I started to work on deep learning, when I became interested in deep learning, this was by chance of being right next to Geoff Hinton at the University of Toronto. Geoff Hinton, of course, is the godfather figure of AI. He was training all these neural networks. I thought it was incredible and interesting. This was not the main thing that everyone in AI was doing by far. This was a niche little subject on the side. That‚Äôs maybe the first dramatic seismic shift that came with the AlexNet and so on.&lt;/p&gt;
    &lt;p&gt;AlexNet reoriented everyone, and everyone started to train neural networks, but it was still very per-task, per specific task. Maybe I have an image classifier or I have a neural machine translator or something like that. People became very slowly interested in agents. People started to think, ‚ÄúOkay, maybe we have a check mark next to the visual cortex or something like that, but what about the other parts of the brain, and how can we get a full agent or a full entity that can interact in the world?‚Äù&lt;/p&gt;
    &lt;p&gt;The Atari deep reinforcement learning shift in 2013 or so was part of that early effort of agents, in my mind, because it was an attempt to try to get agents that not just perceive the world, but also take actions and interact and get rewards from environments. At the time, this was Atari games.&lt;/p&gt;
    &lt;p&gt;I feel that was a misstep. It was a misstep that even the early OpenAI that I was a part of adopted because at that time, the zeitgeist was reinforcement learning environments, games, game playing, beat games, get lots of different types of games, and OpenAI was doing a lot of that. That was another prominent part of AI where maybe for two or three or four years, everyone was doing reinforcement learning on games. That was all a bit of a misstep.&lt;/p&gt;
    &lt;p&gt;What I was trying to do at OpenAI is I was always a bit suspicious of games as being this thing that would lead to AGI. Because in my mind, you want something like an accountant or something that‚Äôs interacting with the real world. I just didn‚Äôt see how games add up to it. My project at OpenAI, for example, was within the scope of the Universe project, on an agent that was using keyboard and mouse to operate web pages. I really wanted to have something that interacts with the actual digital world that can do knowledge work.&lt;/p&gt;
    &lt;p&gt;It just so turns out that this was extremely early, way too early, so early that we shouldn‚Äôt have been working on that. Because if you‚Äôre just stumbling your way around and keyboard mashing and mouse clicking and trying to get rewards in these environments, your reward is too sparse and you just won‚Äôt learn. You‚Äôre going to burn a forest computing, and you‚Äôre never going to get something off the ground. What you‚Äôre missing is this power of representation in the neural network.&lt;/p&gt;
    &lt;p&gt;For example, today people are training those computer-using agents, but they‚Äôre doing it on top of a large language model. You have to get the language model first, you have to get the representations first, and you have to do that by all the pre-training and all the LLM stuff.&lt;/p&gt;
    &lt;p&gt;I feel maybe loosely speaking, people kept trying to get the full thing too early a few times, where people really try to go after agents too early, I would say. That was Atari and Universe and even my own experience. You actually have to do some things first before you get to those agents. Now the agents are a lot more competent, but maybe we‚Äôre still missing some parts of that stack.&lt;/p&gt;
    &lt;p&gt;I would say those are the three major buckets of what people were doing: training neural nets per-tasks, trying the first round of agents, and then maybe the LLMs and seeking the representation power of the neural networks before you tack on everything else on top.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:07:02&lt;/p&gt;
    &lt;p&gt;Interesting. If I were to steelman the Sutton perspective, it would be that humans can just take on everything at once, or even animals can take on everything at once. Animals are maybe a better example because they don‚Äôt even have the scaffold of language. They just get thrown out into the world, and they just have to make sense of everything without any labels.&lt;/p&gt;
    &lt;p&gt;The vision for AGI then should just be something which looks at sensory data, looks at the computer screen, and it just figures out what‚Äôs going on from scratch. If a human were put in a similar situation and had to be trained from scratch‚Ä¶ This is like a human growing up or an animal growing up. Why shouldn‚Äôt that be the vision for AI, rather than this thing where we‚Äôre doing millions of years of training?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:07:41&lt;/p&gt;
    &lt;p&gt;That‚Äôs a really good question. Sutton was on your podcast and I saw the podcast and I had a write-up about that podcast that gets into a bit of how I see things. I‚Äôm very careful to make analogies to animals because they came about by a very different optimization process. Animals are evolved, and they come with a huge amount of hardware that‚Äôs built in.&lt;/p&gt;
    &lt;p&gt;For example, my example in the post was the zebra. A zebra gets born, and a few minutes later it‚Äôs running around and following its mother. That‚Äôs an extremely complicated thing to do. That‚Äôs not reinforcement learning. That‚Äôs something that‚Äôs baked in. Evolution obviously has some way of encoding the weights of our neural nets in ATCGs, and I have no idea how that works, but it apparently works.&lt;/p&gt;
    &lt;p&gt;Brains just came from a very different process, and I‚Äôm very hesitant to take inspiration from it because we‚Äôre not actually running that process. In my post, I said we‚Äôre not building animals. We‚Äôre building ghosts or spirits or whatever people want to call it, because we‚Äôre not doing training by evolution. We‚Äôre doing training by imitation of humans and the data that they‚Äôve put on the Internet.&lt;/p&gt;
    &lt;p&gt;You end up with these ethereal spirit entities because they‚Äôre fully digital and they‚Äôre mimicking humans. It‚Äôs a different kind of intelligence. If you imagine a space of intelligences, we‚Äôre starting off at a different point almost. We‚Äôre not really building animals. But it‚Äôs also possible to make them a bit more animal-like over time, and I think we should be doing that.&lt;/p&gt;
    &lt;p&gt;One more point. I do feel Sutton has a very... His framework is, ‚ÄúWe want to build animals.‚Äù I think that would be wonderful if we can get that to work. That would be amazing. If there were a single algorithm that you can just run on the Internet and it learns everything, that would be incredible. I‚Äôm not sure that it exists and that‚Äôs certainly not what animals do, because animals have this outer loop of evolution.&lt;/p&gt;
    &lt;p&gt;A lot of what looks like learning is more like maturation of the brain. I think there‚Äôs very little reinforcement learning for animals. A lot of the reinforcement learning is more like motor tasks; it‚Äôs not intelligence tasks. So I actually kind of think humans don‚Äôt really use RL, roughly speaking.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:09:52&lt;/p&gt;
    &lt;p&gt;Can you repeat the last sentence? A lot of that intelligence is not motor task‚Ä¶it‚Äôs what, sorry?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:09:54&lt;/p&gt;
    &lt;p&gt;A lot of the reinforcement learning, in my perspective, would be things that are a lot more motor-like, simple tasks like throwing a hoop. But I don‚Äôt think that humans use reinforcement learning for a lot of intelligence tasks like problem-solving and so on. That doesn‚Äôt mean we shouldn‚Äôt do that for research, but I just feel like that‚Äôs what animals do or don‚Äôt.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:10:17&lt;/p&gt;
    &lt;p&gt;I‚Äôm going to take a second to digest that because there are a lot of different ideas. Here‚Äôs one clarifying question I can ask to understand the perspective. You suggest that evolution is doing the kind of thing that pre-training does in the sense of building something which can then understand the world.&lt;/p&gt;
    &lt;p&gt;The difference is that evolution has to be titrated in the case of humans through three gigabytes of DNA. That‚Äôs very unlike the weights of a model. Literally, the weights of the model are a brain, which obviously does not exist in the sperm and the egg. So it has to be grown. Also, the information for every single synapse in the brain simply cannot exist in the three gigabytes that exist in the DNA.&lt;/p&gt;
    &lt;p&gt;Evolution seems closer to finding the algorithm which then does the lifetime learning. Now, maybe the lifetime learning is not analogous to RL, to your point. Is that compatible with the thing you were saying, or would you disagree with that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:11:17&lt;/p&gt;
    &lt;p&gt;I think so. I would agree with you that there‚Äôs some miraculous compression going on because obviously, the weights of the neural net are not stored in ATCGs. There‚Äôs some dramatic compression. There are some learning algorithms encoded that take over and do some of the learning online. I definitely agree with you on that. I would say I‚Äôm a lot more practically minded. I don‚Äôt come at it from the perspective of, let‚Äôs build animals. I come from it from the perspective of, let‚Äôs build useful things. I have a hard hat on, and I‚Äôm just observing that we‚Äôre not going to do evolution, because I don‚Äôt know how to do that.&lt;/p&gt;
    &lt;p&gt;But it does turn out we can build these ghosts, spirit-like entities, by imitating internet documents. This works. It‚Äôs a way to bring you up to something that has a lot of built-in knowledge and intelligence in some way, similar to maybe what evolution has done. That‚Äôs why I call pre-training this crappy evolution. It‚Äôs the practically possible version with our technology and what we have available to us to get to a starting point where we can do things like reinforcement learning and so on.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:12:15&lt;/p&gt;
    &lt;p&gt;Just to steelman the other perspective, after doing this Sutton interview and thinking about it a bit, he has an important point here. Evolution does not give us the knowledge, really. It gives us the algorithm to find the knowledge, and that seems different from pre-training.&lt;/p&gt;
    &lt;p&gt;Perhaps the perspective is that pre-training helps build the kind of entity which can learn better. It teaches meta-learning, and therefore it is similar to finding an algorithm. But if it‚Äôs ‚ÄúEvolution gives us knowledge, pre-training gives us knowledge,‚Äù that analogy seems to break down.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:12:42&lt;/p&gt;
    &lt;p&gt;It‚Äôs subtle and I think you‚Äôre right to push back on it, but basically the thing that pre-training is doing, you‚Äôre getting the next-token predictor over the internet, and you‚Äôre training that into a neural net. It‚Äôs doing two things that are unrelated. Number one, it‚Äôs picking up all this knowledge, as I call it. Number two, it‚Äôs actually becoming intelligent.&lt;/p&gt;
    &lt;p&gt;By observing the algorithmic patterns in the internet, it boots up all these little circuits and algorithms inside the neural net to do things like in-context learning and all this stuff. You don‚Äôt need or want the knowledge. I think that‚Äôs probably holding back the neural networks overall because it‚Äôs getting them to rely on the knowledge a little too much sometimes.&lt;/p&gt;
    &lt;p&gt;For example, I feel agents, one thing they‚Äôre not very good at, is going off the data manifold of what exists on the internet. If they had less knowledge or less memory, maybe they would be better. What I think we have to do going forward‚Äîand this would be part of the research paradigms‚Äîis figure out ways to remove some of the knowledge and to keep what I call this cognitive core. It‚Äôs this intelligent entity that is stripped from knowledge but contains the algorithms and contains the magic of intelligence and problem-solving and the strategies of it and all this stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:13:50&lt;/p&gt;
    &lt;p&gt;There‚Äôs so much interesting stuff there. Let‚Äôs start with in-context learning. This is an obvious point, but I think it‚Äôs worth just saying it explicitly and meditating on it. The situation in which these models seem the most intelligent‚Äîin which I talk to them and I‚Äôm like, ‚ÄúWow, there‚Äôs really something on the other end that‚Äôs responding to me thinking about things‚Äîis if it makes a mistake it‚Äôs like, ‚ÄúOh wait, that‚Äôs the wrong way to think about it. I‚Äôm backing up.‚Äù All that is happening in context. That‚Äôs where I feel like the real intelligence is that you can visibly see.&lt;/p&gt;
    &lt;p&gt;That in-context learning process is developed by gradient descent on pre-training. It spontaneously meta-learns in-context learning, but the in-context learning itself is not gradient descent, in the same way that our lifetime intelligence as humans to be able to do things is conditioned by evolution but our learning during our lifetime is happening through some other process.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:14:42&lt;/p&gt;
    &lt;p&gt;I don‚Äôt fully agree with that, but you should continue your thought.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:14:44&lt;/p&gt;
    &lt;p&gt;Well, I‚Äôm very curious to understand how that analogy breaks down.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:14:48&lt;/p&gt;
    &lt;p&gt;I‚Äôm hesitant to say that in-context learning is not doing gradient descent. It‚Äôs not doing explicit gradient descent. In-context learning is pattern completion within a token window. It just turns out that there‚Äôs a huge amount of patterns on the internet. You‚Äôre right, the model learns to complete the pattern, and that‚Äôs inside the weights. The weights of the neural network are trying to discover patterns and complete the pattern. There‚Äôs some adaptation that happens inside the neural network, which is magical and just falls out from the internet just because there‚Äôs a lot of patterns.&lt;/p&gt;
    &lt;p&gt;I will say that there have been some papers that I thought were interesting that look at the mechanisms behind in-context learning. I do think it‚Äôs possible that in-context learning runs a small gradient descent loop internally in the layers of the neural network. I recall one paper in particular where they were doing linear regression using in-context learning. Your inputs into the neural network are XY pairs, XY, XY, XY that happen to be on the line. Then you do X and you expect Y. The neural network, when you train it in this way, does linear regression.&lt;/p&gt;
    &lt;p&gt;Normally when you would run linear regression, you have a small gradient descent optimizer that looks at XY, looks at an error, calculates the gradient of the weights and does the update a few times. It just turns out that when they looked at the weights of that in-context learning algorithm, they found some analogies to gradient descent mechanics. In fact, I think the paper was even stronger because they hardcoded the weights of a neural network to do gradient descent through attention and all the internals of the neural network.&lt;/p&gt;
    &lt;p&gt;That‚Äôs just my only pushback. Who knows how in-context learning works, but I think that it‚Äôs probably doing a bit of some funky gradient descent internally. I think that that‚Äôs possible. I was only pushing back on your saying that it‚Äôs not doing in-context learning. Who knows what it‚Äôs doing, but it‚Äôs probably maybe doing something similar to it, but we don‚Äôt know.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:16:39&lt;/p&gt;
    &lt;p&gt;So then it‚Äôs worth thinking okay, if in-context learning and pre-training are both implementing something like gradient descent, why does it feel like with in-context learning we‚Äôre getting to this continual learning, real intelligence-like thing? Whereas you don‚Äôt get the analogous feeling just from pre-training. You could argue that.&lt;/p&gt;
    &lt;p&gt;If it‚Äôs the same algorithm, what could be different? One way you could think about it is, how much information does the model store per information it receives from training? If you look at pre-training, if you look at Llama 3 for example, I think it‚Äôs trained on 15 trillion tokens. If you look at the 70B model, that would be the equivalent of 0.07 bits per token that it sees in pre-training, in terms of the information in the weights of the model compared to the tokens it reads. Whereas if you look at the KV cache and how it grows per additional token in in-context learning, it‚Äôs like 320 kilobytes. So that‚Äôs a 35 million-fold difference in how much information per token is assimilated by the model. I wonder if that‚Äôs relevant at all.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:17:46&lt;/p&gt;
    &lt;p&gt;I kind of agree. The way I usually put this is that anything that happens during the training of the neural network, the knowledge is only a hazy recollection of what happened in training time. That‚Äôs because the compression is dramatic. You‚Äôre taking 15 trillion tokens and you‚Äôre compressing it to just your final neural network of a few billion parameters. Obviously it‚Äôs a massive amount of compression going on. So I refer to it as a hazy recollection of the internet documents.&lt;/p&gt;
    &lt;p&gt;Whereas anything that happens in the context window of the neural network‚Äîyou‚Äôre plugging in all the tokens and building up all those KV cache representations‚Äîis very directly accessible to the neural net. So I compare the KV cache and the stuff that happens at test time to more like a working memory. All the stuff that‚Äôs in the context window is very directly accessible to the neural net.&lt;/p&gt;
    &lt;p&gt;There‚Äôs always these almost surprising analogies between LLMs and humans. I find them surprising because we‚Äôre not trying to build a human brain directly. We‚Äôre just finding that this works and we‚Äôre doing it. But I do think that anything that‚Äôs in the weights, it‚Äôs a hazy recollection of what you read a year ago. Anything that you give it as a context at test time is directly in the working memory. That‚Äôs a very powerful analogy to think through things.&lt;/p&gt;
    &lt;p&gt;When you, for example, go to an LLM and you ask it about some book and what happened in it, like Nick Lane‚Äôs book or something like that, the LLM will often give you some stuff which is roughly correct. But if you give it the full chapter and ask it questions, you‚Äôre going to get much better results because it‚Äôs now loaded in the working memory of the model. So a very long way of saying I agree and that‚Äôs why.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:19:11&lt;/p&gt;
    &lt;p&gt;Stepping back, what is the part about human intelligence that we have most failed to replicate with these models?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:19:20&lt;/p&gt;
    &lt;p&gt;Just a lot of it. So maybe one way to think about it, I don‚Äôt know if this is the best way, but I almost feel like ‚Äî again, making these analogies imperfect as they are ‚Äî we‚Äôve stumbled by with the transformer neural network, which is extremely powerful, very general. You can train transformers on audio, or video, or text, or whatever you want, and it just learns patterns and they‚Äôre very powerful, and it works really well. That to me almost indicates that this is some piece of cortical tissue. It‚Äôs something like that, because the cortex is famously very plastic as well. You can rewire parts of brains. There were the slightly gruesome experiments with rewiring the visual cortex to the auditory cortex, and this animal learned fine, et cetera.&lt;/p&gt;
    &lt;p&gt;So I think that this is cortical tissue. I think when we‚Äôre doing reasoning and planning inside the neural networks, doing reasoning traces for thinking models, that‚Äôs kind of like the prefrontal cortex. Maybe those are like little checkmarks, but I still think there are many brain parts and nuclei that are not explored. For example, there‚Äôs a basal ganglia doing a bit of reinforcement learning when we fine-tune the models on reinforcement learning. But where‚Äôs the hippocampus? Not obvious what that would be. Some parts are probably not important. Maybe the cerebellum is not important to cognition, its thoughts, so maybe we can skip some of it. But I still think there‚Äôs, for example, the amygdala, all the emotions and instincts. There‚Äôs probably a bunch of other nuclei in the brain that are very ancient that I don‚Äôt think we‚Äôve really replicated.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know that we should be pursuing the building of an analog of a human brain. I‚Äôm an engineer mostly at heart. Maybe another way to answer the question is that you‚Äôre not going to hire this thing as an intern. It‚Äôs missing a lot of it because it comes with a lot of these cognitive deficits that we all intuitively feel when we talk to the models. So it‚Äôs not fully there yet. You can look at it as not all the brain parts are checked off yet.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:21:16&lt;/p&gt;
    &lt;p&gt;This is maybe relevant to the question of thinking about how fast these issues will be solved. Sometimes people will say about continual learning, ‚ÄúLook, you could easily replicate this capability. Just as in-context learning emerged spontaneously as a result of pre-training, continual learning over longer horizons will emerge spontaneously if the model is incentivized to recollect information over longer horizons, or horizons longer than one session.‚Äù So if there‚Äôs some outer loop RL which has many sessions within that outer loop, then this continual learning where it fine-tunes itself, or it writes to an external memory or something, will just emerge spontaneously. Do you think things like that are plausible? I just don‚Äôt have a prior over how plausible that is. How likely is that to happen?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:22:07&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know that I fully resonate with that. These models, when you boot them up and they have zero tokens in the window, they‚Äôre always restarting from scratch where they were. So I don‚Äôt know in that worldview what it looks like. Maybe making some analogies to humans‚Äîjust because I think it‚Äôs roughly concrete and interesting to think through‚ÄîI feel like when I‚Äôm awake, I‚Äôm building up a context window of stuff that‚Äôs happening during the day. But when I go to sleep, something magical happens where I don‚Äôt think that context window stays around. There‚Äôs some process of distillation into the weights of my brain. This happens during sleep and all this stuff.&lt;/p&gt;
    &lt;p&gt;We don‚Äôt have an equivalent of that in large language models. That‚Äôs to me more adjacent to when you talk about continual learning and so on as absent. These models don‚Äôt really have a distillation phase of taking what happened, analyzing it obsessively, thinking through it, doing some synthetic data generation process and distilling it back into the weights, and maybe having a specific neural net per person. Maybe it‚Äôs a LoRA. It‚Äôs not a full-weight neural network. It‚Äôs just some small sparse subset of the weights that are changed.&lt;/p&gt;
    &lt;p&gt;But we do want to create ways of creating these individuals that have very long context. It‚Äôs not only remaining in the context window because the context windows grow very, very long. Maybe we have some very elaborate, sparse attention over it. But I still think that humans obviously have some process for distilling some of that knowledge into the weights. We‚Äôre missing it. I do also think that humans have some very elaborate, sparse attention scheme, which I think we‚Äôre starting to see some early hints of. DeepSeek v3.2 just came out and I saw that they have sparse attention as an example, and this is one way to have very, very long context windows. So I feel like we are redoing a lot of the cognitive tricks that evolution came up with through a very different process. But we‚Äôre going to converge on a similar architecture cognitively.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:24:02&lt;/p&gt;
    &lt;p&gt;In 10 years, do you think it‚Äôll still be something like a transformer, but with much more modified attention and more sparse MLPs and so forth?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:24:10&lt;/p&gt;
    &lt;p&gt;The way I like to think about it is translation invariance in time. So 10 years ago, where were we? 2015. In 2015, we had convolutional neural networks primarily, residual networks just came out. So remarkably similar, I guess, but quite a bit different still. The transformer was not around. All these more modern tweaks on the transformer were not around. Maybe some of the things that we can bet on, I think in 10 years by translational equivariance, is that we‚Äôre still training giant neural networks with a forward backward pass and update through gradient descent, but maybe it looks a bit different, and it‚Äôs just that everything is much bigger.&lt;/p&gt;
    &lt;p&gt;Recently I went back all the way to 1989 which was a fun exercise for me, a few years ago, because I was reproducing Yann LeCun‚Äôs 1989 convolutional network, which was the first neural network I‚Äôm aware of trained via gradient descent, like modern neural network trained gradient descent on digit recognition. I was just interested in how I could modernize this. How much of this is algorithms? How much of this is data? How much of this progress is compute and systems? I was able to very quickly halve the learning just by time traveling by 33 years.&lt;/p&gt;
    &lt;p&gt;So if I time travel by algorithms 33 years, I could adjust what Yann LeCun did in 1989, and I could halve the error. But to get further gains, I had to add a lot more data, I had to 10x the training set, and then I had to add more computational optimizations. I had to train for much longer with dropout and other regularization techniques.&lt;/p&gt;
    &lt;p&gt;So all these things have to improve simultaneously. We‚Äôre probably going to have a lot more data, we‚Äôre probably going to have a lot better hardware, probably going to have a lot better kernels and software, we‚Äôre probably going to have better algorithms. All of those, it‚Äôs almost like no one of them is winning too much. All of them are surprisingly equal. This has been the trend for a while.&lt;/p&gt;
    &lt;p&gt;So to answer your question, I expect differences algorithmically to what‚Äôs happening today. But I do also expect that some of the things that have stuck around for a very long time will probably still be there. It‚Äôs probably still a giant neural network trained with gradient descent. That would be my guess.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:26:16&lt;/p&gt;
    &lt;p&gt;It‚Äôs surprising that all of those things together only halved the error, 30 years of progress‚Ä¶. Maybe half is a lot. Because if you halve the error, that actually means that‚Ä¶&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:26:30&lt;/p&gt;
    &lt;p&gt;Half is a lot. But I guess what was shocking to me is everything needs to improve across the board: architecture, optimizer, loss function. It also has improved across the board forever. So I expect all those changes to be alive and well.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:26:43&lt;/p&gt;
    &lt;p&gt;Yeah. I was about to ask you a very similar question about nanochat. Since you just coded it up recently, every single step in the process of building a chatbot is fresh in your RAM. I‚Äôm curious if you had similar thoughts about, ‚ÄúOh, there was no one thing that was relevant to going from GPT-2 to nanochat.‚Äù What are some surprising takeaways from the experience?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:27:08&lt;/p&gt;
    &lt;p&gt;Of building nanochat? So nanochat is a repository I released. Was it yesterday or the day before? I can‚Äôt remember.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:27:15&lt;/p&gt;
    &lt;p&gt;We can see the sleep deprivation that went into the‚Ä¶&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:27:18&lt;/p&gt;
    &lt;p&gt;It‚Äôs trying to be the simplest complete repository that covers the whole pipeline end-to-end of building a ChatGPT clone. So you have all of the steps, not just any individual step, which is a bunch. I worked on all the individual steps in the past and released small pieces of code that show you how that‚Äôs done in an algorithmic sense, in simple code. But this handles the entire pipeline. In terms of learning, I don‚Äôt know that I necessarily found something that I learned from it. I already had in my mind how you build it. This is just the process of mechanically building it and making it clean enough so that people can learn from it and that they find it useful.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:28:04&lt;/p&gt;
    &lt;p&gt;What is the best way for somebody to learn from it? Is it to just delete all the code and try to reimplement from scratch, try to add modifications to it?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:28:10&lt;/p&gt;
    &lt;p&gt;That‚Äôs a great question. Basically it‚Äôs about 8,000 lines of code that takes you through the entire pipeline. I would probably put it on the right monitor. If you have two monitors, you put it on the right. You want to build it from scratch, you build it from the start. You‚Äôre not allowed to copy-paste, you‚Äôre allowed to reference, you‚Äôre not allowed to copy-paste. Maybe that‚Äôs how I would do it.&lt;/p&gt;
    &lt;p&gt;But I also think the repository by itself is a pretty large beast. When you write this code, you don‚Äôt go from top to bottom, you go from chunks and you grow the chunks, and that information is absent. You wouldn‚Äôt know where to start. So it‚Äôs not just a final repository that‚Äôs needed, it‚Äôs the building of the repository, which is a complicated chunk-growing process. So that part is not there yet. I would love to add that probably later this week. It‚Äôs probably a video or something like that. Roughly speaking, that‚Äôs what I would try to do. Build the stuff yourself, but don‚Äôt allow yourself copy-paste.&lt;/p&gt;
    &lt;p&gt;I do think that there‚Äôs two types of knowledge, almost. There‚Äôs the high-level surface knowledge, but when you build something from scratch, you‚Äôre forced to come to terms with what you don‚Äôt understand and you don‚Äôt know that you don‚Äôt understand it.&lt;/p&gt;
    &lt;p&gt;It always leads to a deeper understanding. It‚Äôs the only way to build. If I can‚Äôt build it, I don‚Äôt understand it. That‚Äôs a Feynman quote, I believe. I 100% have always believed this very strongly, because there are all these micro things that are just not properly arranged and you don‚Äôt really have the knowledge. You just think you have the knowledge. So don‚Äôt write blog posts, don‚Äôt do slides, don‚Äôt do any of that. Build the code, arrange it, get it to work. It‚Äôs the only way to go. Otherwise, you‚Äôre missing knowledge.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:29:45 ‚Äì LLM cognitive deficits&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:29:45&lt;/p&gt;
    &lt;p&gt;You tweeted out that coding models were of very little help to you in assembling this repository. I‚Äôm curious why that was.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:29:53&lt;/p&gt;
    &lt;p&gt;I guess I built the repository over a period of a bit more than a month. I would say there are three major classes of how people interact with code right now. Some people completely reject all of LLMs and they are just writing by scratch. This is probably not the right thing to do anymore.&lt;/p&gt;
    &lt;p&gt;The intermediate part, which is where I am, is you still write a lot of things from scratch, but you use the autocomplete that‚Äôs available now from these models. So when you start writing out a little piece of it, it will autocomplete for you and you can just tap through. Most of the time it‚Äôs correct, sometimes it‚Äôs not, and you edit it. But you‚Äôre still very much the architect of what you‚Äôre writing. Then there‚Äôs the vibe coding: ‚ÄúHi, please implement this or that,‚Äù enter, and then let the model do it. That‚Äôs the agents.&lt;/p&gt;
    &lt;p&gt;I do feel like the agents work in very specific settings, and I would use them in specific settings. But these are all tools available to you and you have to learn what they‚Äôre good at, what they‚Äôre not good at, and when to use them. So the agents are pretty good, for example, if you‚Äôre doing boilerplate stuff. Boilerplate code that‚Äôs just copy-paste stuff, they‚Äôre very good at that. They‚Äôre very good at stuff that occurs very often on the Internet because there are lots of examples of it in the training sets of these models. There are features of things where the models will do very well.&lt;/p&gt;
    &lt;p&gt;I would say nanochat is not an example of those because it‚Äôs a fairly unique repository. There‚Äôs not that much code in the way that I‚Äôve structured it. It‚Äôs not boilerplate code. It‚Äôs intellectually intense code almost, and everything has to be very precisely arranged. The models have so many cognitive deficits. One example, they kept misunderstanding the code because they have too much memory from all the typical ways of doing things on the Internet that I just wasn‚Äôt adopting. The models, for example‚ÄîI don‚Äôt know if I want to get into the full details‚Äîbut they kept thinking I‚Äôm writing normal code, and I‚Äôm not.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:31:49&lt;/p&gt;
    &lt;p&gt;Maybe one example?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:31:51&lt;/p&gt;
    &lt;p&gt;You have eight GPUs that are all doing forward, backwards. The way to synchronize gradients between them is to use a Distributed Data Parallel container of PyTorch, which automatically as you‚Äôre doing the backward, it will start communicating and synchronizing gradients. I didn‚Äôt use DDP because I didn‚Äôt want to use it, because it‚Äôs not necessary. I threw it out and wrote my own synchronization routine that‚Äôs inside the step of the optimizer. The models were trying to get me to use the DDP container. They were very concerned. This gets way too technical, but I wasn‚Äôt using that container because I don‚Äôt need it and I have a custom implementation of something like it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:32:26&lt;/p&gt;
    &lt;p&gt;They just couldn‚Äôt internalize that you had your own.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:32:28&lt;/p&gt;
    &lt;p&gt;They couldn‚Äôt get past that. They kept trying to mess up the style. They‚Äôre way too over-defensive. They make all these try-catch statements. They keep trying to make a production code base, and I have a bunch of assumptions in my code, and it‚Äôs okay. I don‚Äôt need all this extra stuff in there. So I feel like they‚Äôre bloating the code base, bloating the complexity, they keep misunderstanding, they‚Äôre using deprecated APIs a bunch of times. It‚Äôs a total mess. It‚Äôs just not net useful. I can go in, I can clean it up, but it‚Äôs not net useful.&lt;/p&gt;
    &lt;p&gt;I also feel like it‚Äôs annoying to have to type out what I want in English because it‚Äôs too much typing. If I just navigate to the part of the code that I want, and I go where I know the code has to appear and I start typing out the first few letters, autocomplete gets it and just gives you the code. This is a very high information bandwidth to specify what you want. You point to the code where you want it, you type out the first few pieces, and the model will complete it.&lt;/p&gt;
    &lt;p&gt;So what I mean is, these models are good in certain parts of the stack. There are two examples where I use the models that I think are illustrative. One was when I generated the report. That‚Äôs more boilerplate-y, so I partially vibe-coded some of that stuff. That was fine because it‚Äôs not mission-critical stuff, and it works fine.&lt;/p&gt;
    &lt;p&gt;The other part is when I was rewriting the tokenizer in Rust. I‚Äôm not as good at Rust because I‚Äôm fairly new to Rust. So there‚Äôs a bit of vibe coding going on when I was writing some of the Rust code. But I had a Python implementation that I fully understand, and I‚Äôm just making sure I‚Äôm making a more efficient version of it, and I have tests so I feel safer doing that stuff. They increase accessibility to languages or paradigms that you might not be as familiar with. I think they‚Äôre very helpful there as well. There‚Äôs a ton of Rust code out there, the models are pretty good at it. I happen to not know that much about it, so the models are very useful there.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:34:23&lt;/p&gt;
    &lt;p&gt;The reason this question is so interesting is because the main story people have about AI exploding and getting to superintelligence pretty rapidly is AI automating AI engineering and AI research. They‚Äôll look at the fact that you can have Claude Code and make entire applications, CRUD applications, from scratch and think, ‚ÄúIf you had this same capability inside of OpenAI and DeepMind and everything, just imagine a thousand of you or a million of you in parallel, finding little architectural tweaks.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs quite interesting to hear you say that this is the thing they‚Äôre asymmetrically worse at. It‚Äôs quite relevant to forecasting whether the AI 2027-type explosion is likely to happen anytime soon.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:35:05&lt;/p&gt;
    &lt;p&gt;That‚Äôs a good way of putting it, and you‚Äôre getting at why my timelines are a bit longer. You‚Äôre right. They‚Äôre not very good at code that has never been written before, maybe it‚Äôs one way to put it, which is what we‚Äôre trying to achieve when we‚Äôre building these models.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:35:19&lt;/p&gt;
    &lt;p&gt;Very naive question, but the architectural tweaks that you‚Äôre adding to nanochat, they‚Äôre in a paper somewhere, right? They might even be in a repo somewhere. Is it surprising that they aren‚Äôt able to integrate that into whenever you‚Äôre like, ‚ÄúAdd RoPE embeddings‚Äù or something, they do that in the wrong way?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:35:42&lt;/p&gt;
    &lt;p&gt;It‚Äôs tough. They know, but they don‚Äôt fully know. They don‚Äôt know how to fully integrate it into the repo and your style and your code and your place, and some of the custom things that you‚Äôre doing and how it fits with all the assumptions of the repository. They do have some knowledge, but they haven‚Äôt gotten to the place where they can integrate it and make sense of it.&lt;/p&gt;
    &lt;p&gt;A lot of the stuff continues to improve. Currently, the state-of-the-art model that I go to is the GPT-5 Pro, and that‚Äôs a very powerful model. If I have 20 minutes, I will copy-paste my entire repo and I go to GPT-5 Pro, the oracle, for some questions. Often it‚Äôs not too bad and surprisingly good compared to what existed a year ago.&lt;/p&gt;
    &lt;p&gt;Overall, the models are not there. I feel like the industry is making too big of a jump and is trying to pretend like this is amazing, and it‚Äôs not. It‚Äôs slop. They‚Äôre not coming to terms with it, and maybe they‚Äôre trying to fundraise or something like that. I‚Äôm not sure what‚Äôs going on, but we‚Äôre at this intermediate stage. The models are amazing. They still need a lot of work. For now, autocomplete is my sweet spot. But sometimes, for some types of code, I will go to an LLM agent.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:36:53&lt;/p&gt;
    &lt;p&gt;Here‚Äôs another reason this is really interesting. Through the history of programming, there have been many productivity improvements‚Äîcompilers, linting, better programming languages‚Äîwhich have increased programmer productivity but have not led to an explosion. That sounds very much like the autocomplete tab, and this other category is just automation of the programmer. It‚Äôs interesting you‚Äôre seeing more in the category of the historical analogies of better compilers or something.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:37:26&lt;/p&gt;
    &lt;p&gt;Maybe this gets to one other thought. I have a hard time differentiating where AI begins and stops because I see AI as fundamentally an extension of computing in a pretty fundamental way. I see a continuum of this recursive self-improvement or speeding up programmers all the way from the beginning: code editors, syntax highlighting, or checking even of the types, like data type checking‚Äîall these tools that we‚Äôve built for each other.&lt;/p&gt;
    &lt;p&gt;Even search engines. Why aren‚Äôt search engines part of AI? Ranking is AI. At some point, Google, even early on, was thinking of themselves as an AI company doing Google Search engine, which is totally fair.&lt;/p&gt;
    &lt;p&gt;I see it as a lot more of a continuum than other people do, and it‚Äôs hard for me to draw the line. I feel like we‚Äôre now getting a much better autocomplete, and now we‚Äôre also getting some agents which are these loopy things, but they go off-rails sometimes. What‚Äôs going on is that the human is progressively doing a bit less and less of the low-level stuff. We‚Äôre not writing the assembly code because we have compilers. Compilers will take my high-level language in C and write the assembly code.&lt;/p&gt;
    &lt;p&gt;We‚Äôre abstracting ourselves very, very slowly. There‚Äôs this what I call ‚Äúautonomy slider,‚Äù where more and more stuff is automated‚Äîof the stuff that can be automated at any point in time‚Äîand we‚Äôre doing a bit less and less and raising ourselves in the layer of abstraction over the automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:40:05 ‚Äì RL is terrible&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:40:05&lt;/p&gt;
    &lt;p&gt;Let‚Äôs talk about RL a bit. You tweeted some very interesting things about this. Conceptually, how should we think about the way that humans are able to build a rich world model just from interacting with our environment, and in ways that seem almost irrespective of the final reward at the end of the episode?&lt;/p&gt;
    &lt;p&gt;If somebody is starting a business, and at the end of 10 years, she finds out whether the business succeeded or failed, we say that she‚Äôs earned a bunch of wisdom and experience. But it‚Äôs not because the log probs of every single thing that happened over the last 10 years are up-weighted or down-weighted. Something much more deliberate and rich is happening. What is the ML analogy, and how does that compare to what we‚Äôre doing with LLMs right now?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:40:47&lt;/p&gt;
    &lt;p&gt;Maybe the way I would put it is that humans don‚Äôt use reinforcement learning, as I said. I think they do something different. Reinforcement learning is a lot worse than I think the average person thinks. Reinforcement learning is terrible. It just so happens that everything that we had before it is much worse because previously we were just imitating people, so it has all these issues.&lt;/p&gt;
    &lt;p&gt;In reinforcement learning, say you‚Äôre solving a math problem, because it‚Äôs very simple. You‚Äôre given a math problem and you‚Äôre trying to find the solution. In reinforcement learning, you will try lots of things in parallel first. You‚Äôre given a problem, you try hundreds of different attempts. These attempts can be complex. They can be like, ‚ÄúOh, let me try this, let me try that, this didn‚Äôt work, that didn‚Äôt work,‚Äù etc. Then maybe you get an answer. Now you check the back of the book and you see, ‚ÄúOkay, the correct answer is this.‚Äù You can see that this one, this one, and that one got the correct answer, but these other 97 of them didn‚Äôt. Literally what reinforcement learning does is it goes to the ones that worked really well and every single thing you did along the way, every single token gets upweighted like, ‚ÄúDo more of this.‚Äù&lt;/p&gt;
    &lt;p&gt;The problem with that is people will say that your estimator has high variance, but it‚Äôs just noisy. It‚Äôs noisy. It almost assumes that every single little piece of the solution that you made that arrived at the right answer was the correct thing to do, which is not true. You may have gone down the wrong alleys until you arrived at the right solution. Every single one of those incorrect things you did, as long as you got to the correct solution, will be upweighted as, ‚ÄúDo more of this.‚Äù It‚Äôs terrible. It‚Äôs noise.&lt;/p&gt;
    &lt;p&gt;You‚Äôve done all this work only to find, at the end, you get a single number of like, ‚ÄúOh, you did correct.‚Äù Based on that, you weigh that entire trajectory as like, upweight or downweight. The way I like to put it is you‚Äôre sucking supervision through a straw. You‚Äôve done all this work that could be a minute of rollout, and you‚Äôre sucking the bits of supervision of the final reward signal through a straw and you‚Äôre broadcasting that across the entire trajectory and using that to upweight or downweight that trajectory. It‚Äôs just stupid and crazy.&lt;/p&gt;
    &lt;p&gt;A human would never do this. Number one, a human would never do hundreds of rollouts. Number two, when a person finds a solution, they will have a pretty complicated process of review of, ‚ÄúOkay, I think these parts I did well, these parts I did not do that well. I should probably do this or that.‚Äù They think through things. There‚Äôs nothing in current LLMs that does this. There‚Äôs no equivalent of it. But I do see papers popping out that are trying to do this because it‚Äôs obvious to everyone in the field.&lt;/p&gt;
    &lt;p&gt;The first imitation learning, by the way, was extremely surprising and miraculous and amazing, that we can fine-tune by imitation on humans. That was incredible. Because in the beginning, all we had was base models. Base models are autocomplete. It wasn‚Äôt obvious to me at the time, and I had to learn this. The paper that blew my mind was InstructGPT, because it pointed out that you can take the pretrained model, which is autocomplete, and if you just fine-tune it on text that looks like conversations, the model will very rapidly adapt to become very conversational, and it keeps all the knowledge from pre-training. This blew my mind because I didn‚Äôt understand that stylistically, it can adjust so quickly and become an assistant to a user through just a few loops of fine-tuning on that kind of data. It was very miraculous to me that that worked. So incredible. That was two to three years of work.&lt;/p&gt;
    &lt;p&gt;Now came RL. And RL allows you to do a bit better than just imitation learning because you can have these reward functions and you can hill-climb on the reward functions. Some problems have just correct answers, you can hill-climb on that without getting expert trajectories to imitate. So that‚Äôs amazing. The model can also discover solutions that a human might never come up with. This is incredible. Yet, it‚Äôs still stupid.&lt;/p&gt;
    &lt;p&gt;We need more. I saw a paper from Google yesterday that tried to have this reflect &amp;amp; review idea in mind. Was it the memory bank paper or something? I don‚Äôt know. I‚Äôve seen a few papers along these lines. So I expect there to be some major update to how we do algorithms for LLMs coming in that realm. I think we need three or four or five more, something like that.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:44:54&lt;/p&gt;
    &lt;p&gt;You‚Äôre so good at coming up with evocative phrases. ‚ÄúSucking supervision through a straw.‚Äù It‚Äôs so good.&lt;/p&gt;
    &lt;p&gt;You‚Äôre saying the problem with outcome-based reward is that you have this huge trajectory, and then at the end, you‚Äôre trying to learn every single possible thing about what you should do and what you should learn about the world from that one final bit. Given the fact that this is obvious, why hasn‚Äôt process-based supervision as an alternative been a successful way to make models more capable? What has been preventing us from using this alternative paradigm?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:45:29&lt;/p&gt;
    &lt;p&gt;Process-based supervision just refers to the fact that we‚Äôre not going to have a reward function only at the very end. After you‚Äôve done 10 minutes of work, I‚Äôm not going to tell you you did well or not well. I‚Äôm going to tell you at every single step of the way how well you‚Äôre doing. The reason we don‚Äôt have that is it‚Äôs tricky how you do that properly. You have partial solutions and you don‚Äôt know how to assign credit. So when you get the right answer, it‚Äôs just an equality match to the answer. It‚Äôs very simple to implement. If you‚Äôre doing process supervision, how do you assign in an automatable way, a partial credit assignment? It‚Äôs not obvious how you do it.&lt;/p&gt;
    &lt;p&gt;Lots of labs are trying to do it with these LLM judges. You get LLMs to try to do it. You prompt an LLM, ‚ÄúHey, look at a partial solution of a student. How well do you think they‚Äôre doing if the answer is this?‚Äù and they try to tune the prompt.&lt;/p&gt;
    &lt;p&gt;The reason that this is tricky is quite subtle. It‚Äôs the fact that anytime you use an LLM to assign a reward, those LLMs are giant things with billions of parameters, and they‚Äôre gameable. If you‚Äôre reinforcement learning with respect to them, you will find adversarial examples for your LLM judges, almost guaranteed. So you can‚Äôt do this for too long. You do maybe 10 steps or 20 steps, and maybe it will work, but you can‚Äôt do 100 or 1,000. I understand it‚Äôs not obvious, but basically the model will find little cracks. It will find all these spurious things in the nooks and crannies of the giant model and find a way to cheat it.&lt;/p&gt;
    &lt;p&gt;One example that‚Äôs prominently in my mind, this was probably public, if you‚Äôre using an LLM judge for a reward, you just give it a solution from a student and ask it if the student did well or not. We were training with reinforcement learning against that reward function, and it worked really well. Then, suddenly, the reward became extremely large. It was a massive jump, and it did perfect. You‚Äôre looking at it like, ‚ÄúWow, this means the student is perfect in all these problems. It‚Äôs fully solved math.‚Äù&lt;/p&gt;
    &lt;p&gt;But when you look at the completions that you‚Äôre getting from the model, they are complete nonsense. They start out okay, and then they change to ‚Äúdhdhdhdh.‚Äù It‚Äôs just like, ‚ÄúOh, okay, let‚Äôs take two plus three and we do this and this, and then dhdhdhdh.‚Äù You‚Äôre looking at it, and it‚Äôs like, this is crazy. How is it getting a reward of one or 100%? You look at the LLM judge, and it turns out that ‚Äúdhdhdhdh‚Äù is an adversarial example for the model, and it assigns 100% probability to it.&lt;/p&gt;
    &lt;p&gt;It‚Äôs just because this is an out-of-sample example to the LLM. It‚Äôs never seen it during training, and you‚Äôre in pure generalization land. It‚Äôs never seen it during training, and in the pure generalization land, you can find these examples that break it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:47:52&lt;/p&gt;
    &lt;p&gt;You‚Äôre basically training the LLM to be a prompt injection model.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:47:56&lt;/p&gt;
    &lt;p&gt;Not even that. Prompt injection is way too fancy. You‚Äôre finding adversarial examples, as they‚Äôre called. These are nonsensical solutions that are obviously wrong, but the model thinks they are amazing.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:48:07&lt;/p&gt;
    &lt;p&gt;To the extent you think this is the bottleneck to making RL more functional, then that will require making LLMs better judges, if you want to do this in an automated way. Is it just going to be some sort of GAN-like approach where you have to train models to be more robust?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:48:22&lt;/p&gt;
    &lt;p&gt;The labs are probably doing all that. The obvious thing is, ‚Äúdhdhdhdh‚Äù should not get 100% reward. Okay, well, take ‚Äúdhdhdhdh,‚Äù put it in the training set of the LLM judge, and say this is not 100%, this is 0%. You can do this, but every time you do this, you get a new LLM, and it still has adversarial examples. There‚Äôs an infinity of adversarial examples.&lt;/p&gt;
    &lt;p&gt;Probably if you iterate this a few times, it‚Äôll probably be harder and harder to find adversarial examples, but I‚Äôm not 100% sure because this thing has a trillion parameters or whatnot. I bet you the labs are trying. I still think we need other ideas.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:48:57&lt;/p&gt;
    &lt;p&gt;Interesting. Do you have some shape of what the other idea could be?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:49:02&lt;/p&gt;
    &lt;p&gt;This idea of a review solution encompassing synthetic examples such that when you train on them, you get better, and meta-learn it in some way. I think there are some papers that I‚Äôm starting to see pop out. I am only at a stage of reading abstracts because a lot of these papers are just ideas. Someone has to make it work on a frontier LLM lab scale in full generality because when you see these papers, they pop up, and it‚Äôs just a bit noisy. They‚Äôre cool ideas, but I haven‚Äôt seen anyone convincingly show that this is possible. That said, the LLM labs are fairly closed, so who knows what they‚Äôre doing now.&lt;/p&gt;
    &lt;head rend="h3"&gt;00:49:38 ‚Äì How do humans learn?&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 00:49:38&lt;/p&gt;
    &lt;p&gt;I can conceptualize how you would be able to train on synthetic examples or synthetic problems that you have made for yourself. But there seems to be another thing humans do‚Äîmaybe sleep is this, maybe daydreaming is this‚Äîwhich is not necessarily to come up with fake problems, but just to reflect.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not sure what the ML analogy is for daydreaming or sleeping, or just reflecting. I haven‚Äôt come up with a new problem. Obviously, the very basic analogy would just be fine-tuning on reflection bits, but I feel like in practice that probably wouldn‚Äôt work that well. Do you have some take on what the analogy of this thing is?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:50:17&lt;/p&gt;
    &lt;p&gt;I do think that we‚Äôre missing some aspects there. As an example, let‚Äôs take reading a book. Currently when LLMs are reading a book, what that means is we stretch out the sequence of text, and the model is predicting the next token, and it‚Äôs getting some knowledge from that. That‚Äôs not really what humans do. When you‚Äôre reading a book, I don‚Äôt even feel like the book is exposition I‚Äôm supposed to be attending to and training on. The book is a set of prompts for me to do synthetic data generation, or for you to get to a book club and talk about it with your friends. It‚Äôs by manipulating that information that you actually gain that knowledge. We have no equivalent of that with LLMs. They don‚Äôt really do that. I‚Äôd love to see during pre-training some stage that thinks through the material and tries to reconcile it with what it already knows, and thinks through it for some amount of time and gets that to work. There‚Äôs no equivalence of any of this. This is all research.&lt;/p&gt;
    &lt;p&gt;There are some subtle‚Äîvery subtle that I think are very hard to understand‚Äîreasons why it‚Äôs not trivial. If I can just describe one: why can‚Äôt we just synthetically generate and train on it? Because every synthetic example, if I just give synthetic generation of the model thinking about a book, you look at it and you‚Äôre like, ‚ÄúThis looks great. Why can‚Äôt I train on it?‚Äù You could try, but the model will get much worse if you continue trying. That‚Äôs because all of the samples you get from models are silently collapsed. Silently‚Äîit is not obvious if you look at any individual example of it‚Äîthey occupy a very tiny manifold of the possible space of thoughts about content. The LLMs, when they come off, they‚Äôre what we call ‚Äúcollapsed.‚Äù They have a collapsed data distribution. One easy way to see it is to go to ChatGPT and ask it, ‚ÄúTell me a joke.‚Äù It only has like three jokes. It‚Äôs not giving you the whole breadth of possible jokes. It knows like three jokes. They‚Äôre silently collapsed.&lt;/p&gt;
    &lt;p&gt;You‚Äôre not getting the richness and the diversity and the entropy from these models as you would get from humans. Humans are a lot noisier, but at least they‚Äôre not biased, in a statistical sense. They‚Äôre not silently collapsed. They maintain a huge amount of entropy. So how do you get synthetic data generation to work despite the collapse and while maintaining the entropy? That‚Äôs a research problem.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:52:20&lt;/p&gt;
    &lt;p&gt;Just to make sure I understood, the reason that the collapse is relevant to synthetic data generation is because you want to be able to come up with synthetic problems or reflections which are not already in your data distribution?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:52:32&lt;/p&gt;
    &lt;p&gt;I guess what I‚Äôm saying is, say we have a chapter of a book and I ask an LLM to think about it, it will give you something that looks very reasonable. But if I ask it 10 times, you‚Äôll notice that all of them are the same.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:52:44&lt;/p&gt;
    &lt;p&gt;You can‚Äôt just keep scaling ‚Äúreflection‚Äù on the same amount of prompt information and then get returns from that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:52:54&lt;/p&gt;
    &lt;p&gt;Any individual sample will look okay, but the distribution of it is quite terrible. It‚Äôs quite terrible in such a way that if you continue training on too much of your own stuff, you actually collapse.&lt;/p&gt;
    &lt;p&gt;I think that there‚Äôs possibly no fundamental solution to this. I also think humans collapse over time. These analogies are surprisingly good. Humans collapse during the course of their lives. This is why children, they haven‚Äôt overfit yet. They will say stuff that will shock you because you can see where they‚Äôre coming from, but it‚Äôs just not the thing people say, because they‚Äôre not yet collapsed. But we‚Äôre collapsed. We end up revisiting the same thoughts. We end up saying more and more of the same stuff, and the learning rates go down, and the collapse continues to get worse, and then everything deteriorates.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:53:39&lt;/p&gt;
    &lt;p&gt;Have you seen this super interesting paper that dreaming is a way of preventing this kind of overfitting and collapse? The reason dreaming is evolutionary adaptive is to put you in weird situations that are very unlike your day-to-day reality, so as to prevent this kind of overfitting.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:53:55&lt;/p&gt;
    &lt;p&gt;It‚Äôs an interesting idea. I do think that when you‚Äôre generating things in your head and then you‚Äôre attending to it, you‚Äôre training on your own samples, you‚Äôre training on your synthetic data. If you do it for too long, you go off-rails and you collapse way too much. You always have to seek entropy in your life. Talking to other people is a great source of entropy, and things like that. So maybe the brain has also built some internal mechanisms for increasing the amount of entropy in that process. That‚Äôs an interesting idea.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:54:25&lt;/p&gt;
    &lt;p&gt;This is a very ill-formed thought so I‚Äôll just put it out and let you react to it. The best learners that we are aware of, which are children, are extremely bad at recollecting information. In fact, at the very earliest stages of childhood, you will forget everything. You‚Äôre just an amnesiac about everything that happens before a certain year date. But you‚Äôre extremely good at picking up new languages and learning from the world. Maybe there‚Äôs some element of being able to see the forest for the trees.&lt;/p&gt;
    &lt;p&gt;Whereas if you compare it to the opposite end of the spectrum, you have LLM pre-training, where these models will literally be able to regurgitate word-for-word what is the next thing in a Wikipedia page. But their ability to learn abstract concepts really quickly, the way a child can, is much more limited. Then adults are somewhere in between, where they don‚Äôt have the flexibility of childhood learning, but they can memorize facts and information in a way that is harder for kids. I don‚Äôt know if there‚Äôs something interesting about that spectrum.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:55:19&lt;/p&gt;
    &lt;p&gt;I think there‚Äôs something very interesting about that, 100%. I do think that humans have a lot more of an element, compared to LLMs, of seeing the forest for the trees. We‚Äôre not actually that good at memorization, which is actually a feature. Because we‚Äôre not that good at memorization, we‚Äôre forced to find patterns in a more general sense.&lt;/p&gt;
    &lt;p&gt;LLMs in comparison are extremely good at memorization. They will recite passages from all these training sources. You can give them completely nonsensical data. You can hash some amount of text or something like that, you get a completely random sequence. If you train on it, even just for a single iteration or two, it can suddenly regurgitate the entire thing. It will memorize it. There‚Äôs no way a person can read a single sequence of random numbers and recite it to you.&lt;/p&gt;
    &lt;p&gt;That‚Äôs a feature, not a bug, because it forces you to only learn the generalizable components. Whereas LLMs are distracted by all the memory that they have of the pre-training documents, and it‚Äôs probably very distracting to them in a certain sense. So that‚Äôs why when I talk about the cognitive core, I want to remove the memory, which is what we talked about. I‚Äôd love to have them have less memory so that they have to look things up, and they only maintain the algorithms for thought, and the idea of an experiment, and all this cognitive glue of acting.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:56:36&lt;/p&gt;
    &lt;p&gt;And this is also relevant to preventing model collapse?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:56:41&lt;/p&gt;
    &lt;p&gt;Let me think. I‚Äôm not sure. It‚Äôs almost like a separate axis. The models are way too good at memorization, and somehow we should remove that. People are much worse, but it‚Äôs a good thing.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:56:57&lt;/p&gt;
    &lt;p&gt;What is a solution to model collapse? There are very naive things you could attempt. The distribution over logits should be wider or something. There are many naive things you could try. What ends up being the problem with the naive approaches?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:57:11&lt;/p&gt;
    &lt;p&gt;That‚Äôs a great question. You can imagine having a regularization for entropy and things like that. I guess they just don‚Äôt work as well empirically because right now the models are collapsed. But I will say most of the tasks that we want from them don‚Äôt actually demand diversity. That‚Äôs probably the answer to what‚Äôs going on.&lt;/p&gt;
    &lt;p&gt;The frontier labs are trying to make the models useful. I feel like the diversity of the outputs is not so much... Number one, it‚Äôs much harder to work with and evaluate and all this stuff, but maybe it‚Äôs not what‚Äôs capturing most of the value.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:57:42&lt;/p&gt;
    &lt;p&gt;In fact, it‚Äôs actively penalized. If you‚Äôre super creative in RL, it‚Äôs not good.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:57:48&lt;/p&gt;
    &lt;p&gt;Yeah. Or maybe if you‚Äôre doing a lot of writing, help from LLMs and stuff like that, it‚Äôs probably bad because the models will silently give you all the same stuff. They won‚Äôt explore lots of different ways of answering a question.&lt;/p&gt;
    &lt;p&gt;Maybe this diversity, not as many applications need it so the models don‚Äôt have it. But then it‚Äôs a problem at synthetic data generation time, et cetera. So we‚Äôre shooting ourselves in the foot by not allowing this entropy to maintain in the model. Possibly the labs should try harder.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:58:17&lt;/p&gt;
    &lt;p&gt;I think you hinted that it‚Äôs a very fundamental problem, it won‚Äôt be easy to solve. What‚Äôs your intuition for that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:58:24&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know if it‚Äôs super fundamental. I don‚Äôt know if I intended to say that. I do think that I haven‚Äôt done these experiments, but I do think that you could probably regularize the entropy to be higher. So you‚Äôre encouraging the model to give you more and more solutions, but you don‚Äôt want it to start deviating too much from the training data. It‚Äôs going to start making up its own language. It‚Äôs going to start using words that are extremely rare, so it‚Äôs going to drift too much from the distribution.&lt;/p&gt;
    &lt;p&gt;So I think controlling the distribution is just tricky. It‚Äôs probably not trivial in that sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:58:58&lt;/p&gt;
    &lt;p&gt;How many bits should the optimal core of intelligence end up being if you just had to make a guess? The thing we put on the von Neumann probes, how big does it have to be?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 00:59:10&lt;/p&gt;
    &lt;p&gt;It‚Äôs really interesting in the history of the field because at one point everything was very scaling-pilled in terms of like, ‚ÄúOh, we‚Äôre gonna make much bigger models, trillions of parameter models.‚Äù What the models have done in size is they‚Äôve gone up and now they‚Äôve come down. State-of-the-art models are smaller. Even then, I think they memorized way too much. So I had a prediction a while back that I almost feel like we can get cognitive cores that are very good at even a billion parameters.&lt;/p&gt;
    &lt;p&gt;If you talk to a billion parameter model, I think in 20 years, you can have a very productive conversation. It thinks and it‚Äôs a lot more like a human. But if you ask it some factual question, it might have to look it up, but it knows that it doesn‚Äôt know and it might have to look it up and it will just do all the reasonable things.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 00:59:54&lt;/p&gt;
    &lt;p&gt;That‚Äôs surprising that you think it‚Äôll take a billion parameters. Because already we have billion parameter models or a couple billion parameter models that are very intelligent.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:00:02&lt;/p&gt;
    &lt;p&gt;Well, state-of-the-art models are like a trillion parameters. But they remember so much stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:00:06&lt;/p&gt;
    &lt;p&gt;Yeah, but I‚Äôm surprised that in 10 years, given the pace‚Ä¶ We have gpt-oss-20b. That‚Äôs way better than GPT-4 original, which was a trillion plus parameters. Given that trend, I‚Äôm surprised you think in 10 years the cognitive core is still a billion parameters. I‚Äôm surprised you‚Äôre not like, ‚ÄúOh it‚Äôs gonna be like tens of millions or millions.‚Äù&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:00:30&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the issue, the training data is the internet, which is really terrible. There‚Äôs a huge amount of gains to be made because the internet is terrible. Even the internet, when you and I think of the internet, you‚Äôre thinking of like The Wall Street Journal. That‚Äôs not what this is. When you‚Äôre looking at a pre-training dataset in the frontier lab and you look at a random internet document, it‚Äôs total garbage. I don‚Äôt even know how this works at all. It‚Äôs some like stock tickers, symbols, it‚Äôs a huge amount of slop and garbage from like all the corners of the internet. It‚Äôs not like your Wall Street Journal article, that‚Äôs extremely rare. So because the internet is so terrible, we have to build really big models to compress all that. Most of that compression is memory work instead of cognitive work.&lt;/p&gt;
    &lt;p&gt;But what we really want is the cognitive part, delete the memory. I guess what I‚Äôm saying is that we need intelligent models to help us refine even the pre-training set to just narrow it down to the cognitive components. Then I think you get away with a much smaller model because it‚Äôs a much better dataset and you could train it on it. But probably it‚Äôs not trained directly on it, it‚Äôs probably distilled from a much better model still.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:01:35&lt;/p&gt;
    &lt;p&gt;But why is the distilled version still a billion?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:01:39&lt;/p&gt;
    &lt;p&gt;I just feel like distillation works extremely well. So almost every small model, if you have a small model, it‚Äôs almost certainly distilled.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:01:46&lt;/p&gt;
    &lt;p&gt;Right, but why is the distillation in 10 years not getting below 1 billion?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:01:50&lt;/p&gt;
    &lt;p&gt;Oh, you think it should be smaller than a billion? I mean, come on, right? I don‚Äôt know. At some point it should take at least a billion knobs to do something interesting. You‚Äôre thinking it should be even smaller?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:02:01&lt;/p&gt;
    &lt;p&gt;Yeah. If you look at the trend over the last few years of just finding low-hanging fruit and going from trillion plus models to models that are literally two orders of magnitude smaller in a matter of two years and having better performance, it makes me think the sort of core of intelligence might be even way, way smaller. Plenty of room at the bottom, to paraphrase Feynman.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:02:22&lt;/p&gt;
    &lt;p&gt;I feel like I‚Äôm already contrarian by talking about a billion parameter cognitive core and you‚Äôre outdoing me. Maybe we could get a little bit smaller. I do think that practically speaking, you want the model to have some knowledge. You don‚Äôt want it to be looking up everything because then you can‚Äôt think in your head. You‚Äôre looking up way too much stuff all the time. Some basic curriculum needs to be there for knowledge, but it doesn‚Äôt have esoteric knowledge.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:02:48&lt;/p&gt;
    &lt;p&gt;We‚Äôre discussing what plausibly could be the cognitive core. There‚Äôs a separate question which is what will be the size of frontier models over time? I‚Äôm curious if you have predictions. We had increasing scale up to maybe GPT 4.5 and now we‚Äôre seeing decreasing or plateauing scale. There are many reasons this could be going on. Do you have a prediction going forward? Will the biggest models be bigger, will they be smaller, will they be the same?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:03:14&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have a super strong prediction. The labs are just being practical. They have a flops budget and a cost budget. It just turns out that pre-training is not where you want to put most of your flops or your cost. That‚Äôs why the models have gotten smaller. They are a bit smaller, the pre-training stage is smaller, but they make it up in reinforcement learning, mid-training, and all this stuff that follows. They‚Äôre just being practical in terms of all the stages and how you get the most bang for the buck.&lt;/p&gt;
    &lt;p&gt;Forecasting that trend is quite hard. I do still expect that there‚Äôs so much low-hanging fruit. That‚Äôs my basic expectation. I have a very wide distribution here.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:03:51&lt;/p&gt;
    &lt;p&gt;Do you expect the low-hanging fruit to be similar in kind to the kinds of things that have been happening over the last two to five years? If I look at nanochat versus nanoGPT and the architectural tweaks you made, is that the flavor of things you expect to continue to keep happening? You‚Äôre not expecting any giant paradigm shifts.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:04:11&lt;/p&gt;
    &lt;p&gt;For the most part, yeah. I expect the datasets to get much, much better. When you look at the average datasets, they‚Äôre extremely terrible. They‚Äôre so bad that I don‚Äôt even know how anything works. Look at the average example in the training set: factual mistakes, errors, nonsensical things. Somehow when you do it at scale, the noise washes away and you‚Äôre left with some of the signal. Datasets will improve a ton.&lt;/p&gt;
    &lt;p&gt;Everything gets better. Our hardware, all the kernels for running the hardware and maximizing what you get with the hardware. Nvidia is slowly tuning the hardware itself, Tensor Cores, all that needs to happen and will continue to happen. All the kernels will get better and utilize the chip to the max extent. All the algorithms will probably improve over optimization, architecture, and all the modeling components of how everything is done and what the algorithms are that we‚Äôre even training with. I do expect that nothing dominates. Everything plus 20%. This is roughly what I‚Äôve seen.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:06:25 ‚Äì AGI will blend into 2% GDP growth&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:06:25&lt;/p&gt;
    &lt;p&gt;People have proposed different ways of charting how much progress we‚Äôve made towards full AGI. If you can come up with some line, then you can see where that line intersects with AGI and where that would happen on the x-axis. People have proposed it‚Äôs the education level. We had a high schooler, and then they went to college with RL, and they‚Äôre going to get a Ph.D.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:06:44&lt;/p&gt;
    &lt;p&gt;I don‚Äôt like that one.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:06:45&lt;/p&gt;
    &lt;p&gt;Or they‚Äôll propose horizon length. Maybe they can do tasks that take a minute, they can do those autonomously. Then they can autonomously do tasks that take an hour, a human an hour, a human a week. How do you think about the relevant y-axis here? How should we think about how AI is making progress?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:07:05&lt;/p&gt;
    &lt;p&gt;I have two answers to that. Number one, I‚Äôm almost tempted to reject the question entirely because I see this as an extension of computing. Have we talked about how to chart progress in computing, or how do you chart progress in computing since the 1970s or whatever? What is the y-axis? The whole question is funny from that perspective a little bit.&lt;/p&gt;
    &lt;p&gt;When people talk about AI and the original AGI and how we spoke about it when OpenAI started, AGI was a system you could go to that can do any economically valuable task at human performance or better. That was the definition. I was pretty happy with that at the time. I‚Äôve stuck to that definition forever, and then people have made up all kinds of other definitions. But I like that definition.&lt;/p&gt;
    &lt;p&gt;The first concession that people make all the time is they just take out all the physical stuff because we‚Äôre just talking about digital knowledge work. That‚Äôs a pretty major concession compared to the original definition, which was any task a human can do. I can lift things, etc. AI can‚Äôt do that, obviously, but we‚Äôll take it. What fraction of the economy are we taking away by saying, ‚ÄúOh, only knowledge work?‚Äù I don‚Äôt know the numbers. I feel about 10% to 20%, if I had to guess, is only knowledge work, someone could work from home and perform tasks, something like that. It‚Äôs still a really large market. What is the size of the economy, and what is 10% or 20%? We‚Äôre still talking about a few trillion dollars, even in the US, of market share or work. So it‚Äôs still a very massive bucket.&lt;/p&gt;
    &lt;p&gt;Going back to the definition, what I would be looking for is to what extent is that definition true? Are there jobs or lots of tasks? If we think of tasks as not jobs but tasks. It‚Äôs difficult because the problem is society will refactor based on the tasks that make up jobs, based on what‚Äôs automatable or not. Today, what jobs are replaceable by AI? A good example recently was Geoff Hinton‚Äôs prediction that radiologists would not be a job anymore, and this turned out to be very wrong in a bunch of ways. Radiologists are alive and well and growing, even though computer vision is really, really good at recognizing all the different things that they have to recognize in images. It‚Äôs just a messy, complicated job with a lot of surfaces and dealing with patients and all this stuff in the context of it.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know that by that definition AI has made a huge dent yet. Some of the jobs that I would be looking for have some features that make it very amenable to automation earlier than later. As an example, call center employees often come up, and I think rightly so. Call center employees have a number of simplifying properties with respect to what‚Äôs automatable today. Their jobs are pretty simple. It‚Äôs a sequence of tasks, and every task looks similar. You take a phone call with a person, it‚Äôs 10 minutes of interaction or whatever it is, probably a bit longer. In my experience, a lot longer. You complete some task in some scheme, and you change some database entries around or something like that. So you keep repeating something over and over again, and that‚Äôs your job.&lt;/p&gt;
    &lt;p&gt;You do want to bring in the task horizon‚Äîhow long it takes to perform a task‚Äîand then you want to also remove context. You‚Äôre not dealing with different parts of services of companies or other customers. It‚Äôs just the database, you, and a person you‚Äôre serving. It‚Äôs more closed, it‚Äôs more understandable, it‚Äôs purely digital. So I would be looking for those things.&lt;/p&gt;
    &lt;p&gt;But even there, I‚Äôm not looking at full automation yet. I‚Äôm looking for an autonomy slider. I expect that we are not going to instantly replace people. We‚Äôre going to be swapping in AIs that do 80% of the volume. They delegate 20% of the volume to humans, and humans are supervising teams of five AIs doing the call center work that‚Äôs more rote. I would be looking for new interfaces or new companies that provide some layer that allows you to manage some of these AIs that are not yet perfect. Then I would expect that across the economy. A lot of jobs are a lot harder than a call center employee.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:11:02&lt;/p&gt;
    &lt;p&gt;With radiologists, I‚Äôm totally speculating and I have no idea what the actual workflow of a radiologist involves. But one analogy that might be applicable is when Waymos were first being rolled out, there‚Äôd be a person sitting in the front seat, and you just had to have them there to make sure that if something went really wrong, they‚Äôre there to monitor. Even today, people are still watching to make sure things are going well. Robotaxi, which was just deployed, still has a person inside it.&lt;/p&gt;
    &lt;p&gt;Now we could be in a similar situation where if you automate 99% of a job, that last 1% the human has to do is incredibly valuable because it‚Äôs bottlenecking everything else. If it were the case with radiologists, where the person sitting in the front of Waymo has to be specially trained for years in order to provide the last 1%, their wages should go up tremendously because they‚Äôre the one thing bottlenecking wide deployment. Radiologists, I think their wages have gone up for similar reasons, if you‚Äôre the last bottleneck and you‚Äôre not fungible. A Waymo driver might be fungible with others. So you might see this thing where your wages go up until you get to 99% and then fall just like that when the last 1% is gone. And I wonder if we‚Äôre seeing similar things with radiology or salaries of call center workers or anything like that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:12:17&lt;/p&gt;
    &lt;p&gt;That‚Äôs an interesting question. I don‚Äôt think we‚Äôre currently seeing that with radiology. I think radiology is not a good example. I don‚Äôt know why Geoff Hinton picked on radiology because I think it‚Äôs an extremely messy, complicated profession.&lt;/p&gt;
    &lt;p&gt;I would be a lot more interested in what‚Äôs happening with call center employees today, for example, because I would expect a lot of the rote stuff to be automatable today. I don‚Äôt have first-level access to it but I would be looking for trends of what‚Äôs happening with the call center employees. Some of the things I would also expect is that maybe they are swapping in AI, but then I would still wait for a year or two because I would potentially expect them to pull back and rehire some of the people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:13:00&lt;/p&gt;
    &lt;p&gt;There‚Äôs been evidence that that‚Äôs already been happening generally in companies that have been adopting AI, which I think is quite surprising.&lt;/p&gt;
    &lt;p&gt;I also found what was really surprising. AGI, right? A thing which would do everything. We‚Äôll take out physical work, but it should be able to do all knowledge work. What you would have naively anticipated is that the way this progression would happen is that you take a little task that a consultant is doing, you take that out of the bucket. You take a little task that an accountant is doing, you take that out of the bucket. Then you‚Äôre just doing this across all knowledge work.&lt;/p&gt;
    &lt;p&gt;But instead, if we do believe we‚Äôre on the path of AGI with the current paradigm, the progression is very much not like that. It does not seem like consultants and accountants are getting huge productivity improvements. It‚Äôs very much like programmers are getting more and more chiseled away at their work. If you look at the revenues of these companies, discounting normal chat revenue‚Äîwhich is similar to Google or something‚Äîjust looking at API revenues, it‚Äôs dominated by coding. So this thing which is ‚Äúgeneral‚Äù, which should be able to do any knowledge work, is just overwhelmingly doing only coding. It‚Äôs a surprising way that you would expect the AGI to be deployed.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:14:13&lt;/p&gt;
    &lt;p&gt;There‚Äôs an interesting point here. I do believe coding is the perfect first thing for these LLMs and agents. That‚Äôs because coding has always fundamentally worked around text. It‚Äôs computer terminals and text, and everything is based around text. LLMs, the way they‚Äôre trained on the Internet, love text. They‚Äôre perfect text processors, and there‚Äôs all this data out there. It‚Äôs a perfect fit.&lt;/p&gt;
    &lt;p&gt;We also have a lot of infrastructure pre-built for handling code and text. For example, we have Visual Studio Code or your favorite IDE showing you code, and an agent can plug into that. If an agent has a diff where it made some change, we suddenly have all this code already that shows all the differences to a code base using a diff. It‚Äôs almost like we‚Äôve pre-built a lot of the infrastructure for code.&lt;/p&gt;
    &lt;p&gt;Contrast that with some of the things that don‚Äôt enjoy that at all. As an example, there are people trying to build automation not for coding, but for slides. I saw a company doing slides. That‚Äôs much, much harder. The reason it‚Äôs much harder is because slides are not text. Slides are little graphics, they‚Äôre arranged spatially, and there‚Äôs a visual component to it. Slides don‚Äôt have this pre-built infrastructure. For example, if an agent is to make a change to your slides, how does a thing show you the diff? How do you see the diff? There‚Äôs nothing that shows diffs for slides. Someone has to build it. Some of these things are not amenable to AIs as they are, which are text processors, and code surprisingly is.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:15:48&lt;/p&gt;
    &lt;p&gt;I‚Äôm not sure that alone explains it. I personally have tried to get LLMs to be useful in domains which are just pure language-in, language-out, like rewriting transcripts, coming up with clips based on transcripts. It‚Äôs very plausible that I didn‚Äôt do every single possible thing I could do. I put a bunch of good examples in context, but maybe I should have done some kind of fine-tuning.&lt;/p&gt;
    &lt;p&gt;Our mutual friend, Andy Matuschak, told me that he tried 50 billion things to try to get models to be good at writing spaced repetition prompts. Again, very much language-in, language-out tasks, the kind of thing that should be dead center in the repertoire of these LLMs. He tried in-context learning with a few-shot examples. He tried supervised fine-tuning and retrieval. He could not get them to make cards to his satisfaction.&lt;/p&gt;
    &lt;p&gt;So I find it striking that even in language-out domains, it‚Äôs very hard to get a lot of economic value out of these models separate from coding. I don‚Äôt know what explains it.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:16:57&lt;/p&gt;
    &lt;p&gt;That makes sense. I‚Äôm not saying that anything text is trivial. I do think that code is pretty structured. Text is maybe a lot more flowery, and there‚Äôs a lot more entropy in text, I would say. I don‚Äôt know how else to put it. Also code is hard, and so people feel quite empowered by LLMs, even from simple knowledge. I don‚Äôt know that I have a very good answer. Obviously, text makes it much, much easier, but it doesn‚Äôt mean that all text is trivial.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:17:36 ‚Äì ASI&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:17:36&lt;/p&gt;
    &lt;p&gt;How do you think about superintelligence? Do you expect it to feel qualitatively different from normal humans or human companies?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:17:45&lt;/p&gt;
    &lt;p&gt;I see it as a progression of automation in society. Extrapolating the trend of computing, there will be a gradual automation of a lot of things, and superintelligence will an extrapolation of that. We expect more and more autonomous entities over time that are doing a lot of the digital work and then eventually even the physical work some amount of time later. Basically I see it as just automation, roughly speaking.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:18:10&lt;/p&gt;
    &lt;p&gt;But automation includes the things humans can already do, and superintelligence implies things humans can‚Äôt do.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:18:16&lt;/p&gt;
    &lt;p&gt;But one of the things that people do is invent new things, which I would just put into the automation if that makes sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:18:20&lt;/p&gt;
    &lt;p&gt;But I guess, less abstractly and more qualitatively, do you expect something to feel like‚Ä¶ Because this thing can either think so fast, or has so many copies, or the copies can merge back into themselves, or is much smarter, any number of advantages an AI might have, will the civilization in which these AIs exist will just feel qualitatively different from humans?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:18:51&lt;/p&gt;
    &lt;p&gt;I think it will. It is fundamentally automation, but it will be extremely foreign. It will look really strange. Like you mentioned, we can run all of this on a computer cluster and much faster.&lt;/p&gt;
    &lt;p&gt;Some of the scenarios that I start to get nervous about when the world looks like that is this gradual loss of control and understanding of what‚Äôs happening. I think that‚Äôs the most likely outcome, that there will be a gradual loss of understanding. We‚Äôll gradually layer all this stuff everywhere, and there will be fewer and fewer people who understand it. Then there will be a gradual loss of control and understanding of what‚Äôs happening. That to me seems the most likely outcome of how all this stuff will go down.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:19:31&lt;/p&gt;
    &lt;p&gt;Let me probe on that a bit. It‚Äôs not clear to me that loss of control and loss of understanding are the same things. A board of directors at TSMC, Intel‚Äîname a random company‚Äîthey‚Äôre just prestigious 80-year-olds. They have very little understanding, and maybe they don‚Äôt practically actually have control.&lt;/p&gt;
    &lt;p&gt;A better example is the President of the United States. The President has a lot of fucking power. I‚Äôm not trying to make a good statement about the current operant, or maybe I am, but the actual level of understanding is very different from the level of control.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:20:06&lt;/p&gt;
    &lt;p&gt;I think that‚Äôs fair. That‚Äôs a good pushback. I think I expect loss of both.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:20:15&lt;/p&gt;
    &lt;p&gt;How come? Loss of understanding is obvious, but why loss of control?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:20:20&lt;/p&gt;
    &lt;p&gt;We‚Äôre really far into a territory where I don‚Äôt know what this looks like, but if I were to write sci-fi novels, they would look along the lines of not even a single entity that takes over everything, but multiple competing entities that gradually become more and more autonomous. Some of them go rogue and the others fight them off. It‚Äôs this hot pot of completely autonomous activity that we‚Äôve delegated to. I feel it would have that flavor.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:20:52&lt;/p&gt;
    &lt;p&gt;It is not the fact that they are smarter than us that is resulting in the loss of control. It‚Äôs the fact that they are competing with each other, and whatever arises out of that competition leads to the loss of control.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:21:06&lt;/p&gt;
    &lt;p&gt;A lot of these things, they will be tools to people, they‚Äôre acting on behalf of people or something like that. So maybe those people are in control, but maybe it‚Äôs a loss of control overall for society in the sense of outcomes we want. You have entities acting on behalf of individuals that are still roughly seen as out of control.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:21:30&lt;/p&gt;
    &lt;p&gt;This is a question I should have asked earlier. We were talking about how currently it feels like when you‚Äôre doing AI engineering or AI research, these models are more in the category of compiler rather than in the category of a replacement.&lt;/p&gt;
    &lt;p&gt;At some point, if you have AGI, it should be able to do what you do. Do you feel like having a million copies of you in parallel results in some huge speed-up of AI progress? If that does happen, do you expect to see an intelligence explosion once we have a true AGI? I‚Äôm not talking about LLMs today.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:22:01&lt;/p&gt;
    &lt;p&gt;I do, but it‚Äôs business as usual because we‚Äôre in an intelligence explosion already and have been for decades. It‚Äôs basically the GDP curve that is an exponential weighted sum over so many aspects of the industry. Everything is gradually being automated and has been for hundreds of years. The Industrial Revolution is automation and some of the physical components and tool building and all this stuff. Compilers are early software automation, et cetera. We‚Äôve been recursively self-improving and exploding for a long time.&lt;/p&gt;
    &lt;p&gt;Another way to see it is that Earth was a pretty boring place if you don‚Äôt look at the biomechanics and so on, and looked very similar. If you look from space, we‚Äôre in the middle of this firecracker event, but we‚Äôre seeing it in slow motion. I definitely feel like this has already happened for a very long time. Again, I don‚Äôt see AI as a distinct technology with respect to what has already been happening for a long time.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:23:00&lt;/p&gt;
    &lt;p&gt;You think it‚Äôs continuous with this hyper-exponential trend?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:23:03&lt;/p&gt;
    &lt;p&gt;Yes. That‚Äôs why this was very interesting to me, because I was trying to find AI in the GDP for a while. I thought that GDP should go up. But then I looked at some of the other technologies that I thought were very transformative, like computers or mobile phones or et cetera. You can‚Äôt find them in GDP. GDP is the same exponential.&lt;/p&gt;
    &lt;p&gt;Even the early iPhone didn‚Äôt have the App Store, and it didn‚Äôt have a lot of the bells and whistles that the modern iPhone has. So even though we think of 2008, when the iPhone came out, as this major seismic change, it‚Äôs actually not. Everything is so spread out and it so slowly diffuses that everything ends up being averaged up into the same exponential. It‚Äôs the exact same thing with computers. You can‚Äôt find them in the GDP like, ‚ÄúOh, we have computers now.‚Äù That‚Äôs not what happened, because it‚Äôs such slow progression.&lt;/p&gt;
    &lt;p&gt;With AI we‚Äôre going to see the exact same thing. It‚Äôs just more automation. It allows us to write different kinds of programs that we couldn‚Äôt write before, but AI is still fundamentally a program. It‚Äôs a new kind of computer and a new kind of computing system. But it has all these problems, it‚Äôs going to diffuse over time, and it‚Äôs still going to add up to the same exponential. We‚Äôre still going to have an exponential that‚Äôs going to get extremely vertical. It‚Äôs going to be very foreign to live in that kind of an environment.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:24:10&lt;/p&gt;
    &lt;p&gt;Are you saying that, if you look at the trend before the Industrial Revolution to now, you have a hyper-exponential where you go from 0% growth to then 10,000 years ago, 0.02% growth, and to now when we‚Äôre at 2% growth. That‚Äôs a hyper-exponential. Are you saying if you‚Äôre charting AI on there, then AI takes you to 20% growth or 200% growth?&lt;/p&gt;
    &lt;p&gt;Or are you saying that if you look at the last 300 years, what you‚Äôve been seeing is that you have technology after technology‚Äîcomputers, electrification, steam engines, railways, et cetera‚Äîbut the rate of growth is the exact same, it‚Äôs 2%. Are you saying the rate of growth will go up?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:24:46&lt;/p&gt;
    &lt;p&gt;The rate of growth has also stayed roughly constant, right?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:24:49&lt;/p&gt;
    &lt;p&gt;Only over the last 200, 300 years. But over the course of human history it‚Äôs exploded. It‚Äôs gone from 0% to faster, faster, faster. Industrial explosion, 2%.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:25:01&lt;/p&gt;
    &lt;p&gt;For a while I tried to find AI or look for AI in the GDP curve, and I‚Äôve convinced myself that this is false. Even when people talk about recursive self-improvement and labs and stuff like that, this is business as usual. Of course it‚Äôs going to recursively self-improve, and it‚Äôs been recursively self-improving.&lt;/p&gt;
    &lt;p&gt;LLMs allow the engineers to work much more efficiently to build the next round of LLM, and a lot more of the components are being automated and tuned and et cetera. All the engineers having access to Google Search is part of it. All the engineers having an IDE, all of them having autocomplete or having Claude code, et cetera, it‚Äôs all just part of the same speed-up of the whole thing. It‚Äôs just so smooth.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:25:41&lt;/p&gt;
    &lt;p&gt;Just to clarify, you‚Äôre saying that the rate of growth will not change. The intelligence explosion will show up as it just enabled us to continue staying on the 2% growth trajectory, just as the Internet helped us stay on the 2% growth trajectory.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:25:53&lt;/p&gt;
    &lt;p&gt;Yes, my expectation is that it stays in the same pattern.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:25:58&lt;/p&gt;
    &lt;p&gt;Just to throw the opposite argument against you, my expectation is that it blows up because I think true AGI‚Äîand I‚Äôm not talking about LLM coding bots, I‚Äôm talking about actual replacement of a human in a server‚Äîis qualitatively different from these other productivity-improving technologies because it‚Äôs labor itself.&lt;/p&gt;
    &lt;p&gt;I think we live in a very labor-constrained world. If you talk to any startup founder or any person, you can be like, what do you need more of? You need really talented people. And if you have billions of extra people who are inventing stuff, integrating themselves, making companies bottom start to finish, that feels qualitatively different from a single technology. It‚Äôs as if you get 10 billion extra people on the planet.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:26:44&lt;/p&gt;
    &lt;p&gt;Maybe a counterpoint. I‚Äôm pretty willing to be convinced one way or another on this point. But I will say, for example, computing is labor. Computing was labor. Computers, a lot of jobs disappeared because computers are automating a bunch of digital information processing that you now don‚Äôt need a human for. So computers are labor, and that has played out.&lt;/p&gt;
    &lt;p&gt;Self-driving as an example is also computers doing labor. That‚Äôs already been playing out. It‚Äôs still business as usual.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:27:13&lt;/p&gt;
    &lt;p&gt;You have a machine which is spitting out more things like that at potentially faster pace. Historically, we have examples of the growth regime changing where you went from 0.2% growth to 2% growth. It seems very plausible to me that a machine which is then spitting out the next self-driving car and the next Internet and whatever‚Ä¶&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:27:33&lt;/p&gt;
    &lt;p&gt;I see where it‚Äôs coming from. At the same time, I do feel like people make this assumption of, ‚ÄúWe have God in a box, and now it can do everything,‚Äù and it just won‚Äôt look like that. It‚Äôs going to be able to do some of the things. It‚Äôs going to fail at some other things. It‚Äôs going to be gradually put into society, and we‚Äôll end up with the same pattern. That is my prediction.&lt;/p&gt;
    &lt;p&gt;This assumption of suddenly having a completely intelligent, fully flexible, fully general human in a box, and we can dispense it at arbitrary problems in society, I don‚Äôt think that we will have this discrete change. I think we‚Äôll arrive at the same kind of gradual diffusion of this across the industry.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:28:14&lt;/p&gt;
    &lt;p&gt;It often ends up being misleading in these conversations. I don‚Äôt like to use the word intelligence in this context because intelligence implies you think there‚Äôll be a single superintelligence sitting in a server and it‚Äôll divine how to come up with new technologies and inventions that cause this explosion. That‚Äôs not what I‚Äôm imagining when I‚Äôm imagining 20% growth. I‚Äôm imagining that there are billions of very smart human-like minds, potentially, or that‚Äôs all that‚Äôs required.&lt;/p&gt;
    &lt;p&gt;But the fact that there‚Äôs hundreds of millions of them, billions of them, each individually making new products, figuring out how to integrate themselves into the economy. If a highly experienced smart immigrant came to the country, you wouldn‚Äôt need to figure out how we integrate them in the economy. They figure it out. They could start a company, they could make inventions, or increase productivity in the world.&lt;/p&gt;
    &lt;p&gt;We have examples, even in the current regime, of places that have had 10-20% economic growth. If you just have a lot of people and less capital in comparison to the people, you can have Hong Kong or Shenzhen or whatever with decades of 10% plus growth. There‚Äôs a lot of really smart people who are ready to make use of the resources and do this period of catch-up because we‚Äôve had this discontinuity, and I think AI might be similar.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:29:33&lt;/p&gt;
    &lt;p&gt;I understand, but I still think that you‚Äôre presupposing some discrete jump. There‚Äôs some unlock that we‚Äôre waiting to claim. And suddenly we‚Äôre going to have geniuses in data centers. I still think you‚Äôre presupposing some discrete jump that has no historical precedent that I can‚Äôt find in any of the statistics and that I think probably won‚Äôt happen.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:29:52&lt;/p&gt;
    &lt;p&gt;I mean, the Industrial Revolution is such a jump. You went from 0.2% growth to 2% growth. I‚Äôm just saying you‚Äôll see another jump like that.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:30:00&lt;/p&gt;
    &lt;p&gt;I‚Äôm a little bit suspicious, I would have to take a look. For example, some of the logs are not very good from before the Industrial Revolution. I‚Äôm a bit suspicious of it but I don‚Äôt have strong opinions. You‚Äôre saying that this was a singular event that was extremely magical. You‚Äôre saying that maybe there‚Äôs going to be another event that‚Äôs going to be just like that, extremely magical. It will break the paradigm, and so on.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:30:23&lt;/p&gt;
    &lt;p&gt;I actually don‚Äôt think‚Ä¶ The crucial thing with the Industrial Revolution was that it was not magical. If you just zoomed in, what you would see in 1770 or 1870 is not that there was some key invention. But at the same time, you did move the economy to a regime where the progress was much faster and the exponential 10x‚Äôd. I expect a similar thing from AI where it‚Äôs not like there‚Äôs going to be a single moment where we‚Äôve made the crucial invention.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:30:51&lt;/p&gt;
    &lt;p&gt;It‚Äôs an overhang that‚Äôs being unlocked. Like maybe there‚Äôs a new energy source. There‚Äôs some unlock‚Äîin this case, some kind of a cognitive capacity‚Äîand there‚Äôs an overhang of cognitive work to do.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:31:02&lt;/p&gt;
    &lt;p&gt;That‚Äôs right.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:31:03&lt;/p&gt;
    &lt;p&gt;You‚Äôre expecting that overhang to be filled by this new technology when it crosses the threshold.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:31:06&lt;/p&gt;
    &lt;p&gt;Maybe one way to think about it is throughout history, a lot of growth comes because people come up with ideas, and then people are out there doing stuff to execute those ideas and make valuable output. Through most of this time, the population has been exploding. That has been driving growth.&lt;/p&gt;
    &lt;p&gt;For the last 50 years, people have argued that growth has stagnated. The population in frontier countries has also stagnated. I think we go back to the exponential growth in population that causes hyper-exponential growth in output.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:31:37&lt;/p&gt;
    &lt;p&gt;It‚Äôs really hard to tell. I understand that viewpoint. I don‚Äôt intuitively feel that viewpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:32:50 ‚Äì Evolution of intelligence &amp;amp; culture&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:32:50&lt;/p&gt;
    &lt;p&gt;You recommended Nick Lane‚Äôs book to me. On that basis, I also found it super interesting and I interviewed him. I have some questions about thinking about intelligence and evolutionary history.&lt;/p&gt;
    &lt;p&gt;Now that you, over the last 20 years of doing AI research, you maybe have a more tangible sense of what intelligence is, what it takes to develop it. Are you more or less surprised as a result that evolution just spontaneously stumbled upon it?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:33:19&lt;/p&gt;
    &lt;p&gt;I love Nick Lane‚Äôs books. I was just listening to his podcast on the way up here. With respect to intelligence and its evolution, it‚Äôs very, very recent. I am surprised that it evolved.&lt;/p&gt;
    &lt;p&gt;I find it fascinating to think about all the worlds out there. Say there‚Äôs a thousand planets like Earth and what they look like. I think Nick Lane was here talking about some of the earliest parts. He expects very similar life forms, roughly speaking, and bacteria-like things in most of them. There are a few breaks in there. The evolution of intelligence intuitively feels to me like it should be a fairly rare event.&lt;/p&gt;
    &lt;p&gt;Maybe you should base it on how long something has existed. If bacteria were around for 2 billion years and nothing happened, then going to eukaryote is probably pretty hard because bacteria came up quite early in Earth‚Äôs evolution or history. How long have we had animals? Maybe a couple hundred million years, multicellular animals that run around, crawl, et cetera. That‚Äôs maybe 10% of Earth‚Äôs lifespan. Maybe on that timescale it‚Äôs not too tricky. It‚Äôs still surprising to me, intuitively, that it developed. I would maybe expect just a lot of animal-like life forms doing animal-like things. The fact that you can get something that creates culture and knowledge and accumulates it is surprising to me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:34:42&lt;/p&gt;
    &lt;p&gt;There‚Äôs a couple of interesting follow-ups. If you buy the Sutton perspective that the crux of intelligence is animal intelligence‚Ä¶ The quote he said is ‚ÄúIf you got to the squirrel, you‚Äôd be most of the way to AGI.‚Äù&lt;/p&gt;
    &lt;p&gt;We got to squirrel intelligence right after the Cambrian explosion 600 million years ago. It seems like what instigated that was the oxygenation event 600 million years ago. But immediately the intelligence algorithm was there to make the squirrel intelligence. It‚Äôs suggestive that animal intelligence was like that. As soon as you had the oxygen in the environment, you had the eukaryote, you could just get the algorithm. Maybe it was an accident that evolution stumbled upon it so fast, but I don‚Äôt know if that suggests that at the end it‚Äôs going to be quite simple.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:35:31&lt;/p&gt;
    &lt;p&gt;It‚Äôs so hard to tell with any of this stuff. You can base it a bit on how long something has existed or how long it feels like something has been bottlenecked. Nick Lane is very good about describing this very apparent bottleneck in bacteria and archaea. For two billion years, nothing happened. There‚Äôs extreme diversity of biochemistry, and yet nothing grows to become animals. Two billion years.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know that we‚Äôve seen exactly that kind of an equivalent with animals and intelligence, to your point. We could also look at it with respect to how many times we think certain intelligence has individually sprung up.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:36:07&lt;/p&gt;
    &lt;p&gt;That‚Äôs a really good thing to investigate.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:36:09&lt;/p&gt;
    &lt;p&gt;One thought on that. There‚Äôs hominid intelligence, and then there‚Äôs bird intelligence. Ravens, etc., are extremely clever, but their brain parts are quite distinct, and we don‚Äôt have that much in common. That‚Äôs a slight indication of maybe intelligence springing up a few times. In that case, you‚Äôd expect it more frequently.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:36:32&lt;/p&gt;
    &lt;p&gt;A former guest, Gwern, and Carl Shulman, they‚Äôve made a really interesting point about that. Their perspective is that the scalable algorithm which humans have and primates have, arose in birds as well, and maybe other times as well. But humans found an evolutionary niche which rewarded marginal increases in intelligence and also had a scalable brain algorithm that could achieve those increases in intelligence.&lt;/p&gt;
    &lt;p&gt;For example, if a bird had a bigger brain, it would just collapse out of the air. It‚Äôs very smart for the size of its brain, but it‚Äôs not in a niche which rewards the brain getting bigger. It‚Äôs maybe similar to some really smart‚Ä¶&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy&lt;/p&gt;
    &lt;p&gt;Like dolphins?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel&lt;/p&gt;
    &lt;p&gt;Exaclty, humans, we have hands that reward being able to learn how to do tool use. We can externalize digestion, more energy to the brain, and that kicks off the flywheel.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:37:28&lt;/p&gt;
    &lt;p&gt;Also stuff to work with. I‚Äôm guessing it would be harder if I were a dolphin. How do you have fire? The universe of things you can do in water, inside water, is probably lower than what you can do on land, just chemically.&lt;/p&gt;
    &lt;p&gt;I do agree with this viewpoint of these niches and what‚Äôs being incentivized. I still find it miraculous. I would have expected things to get stuck on animals with bigger muscles. Going through intelligence is a really fascinating breaking point.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:38:02&lt;/p&gt;
    &lt;p&gt;The way Gwern put it is the reason it was so hard is that it‚Äôs a very tight line between being in a situation where something is so important to learn that it‚Äôs not worth distilling the exact right circuits directly back into your DNA, versus it‚Äôs not important enough to learn at all. It has to be something that incentivizes building the algorithm to learn in a lifetime.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:38:28&lt;/p&gt;
    &lt;p&gt;You have to incentivize some kind of adaptability. You want environments that are unpredictable so evolution can‚Äôt bake your algorithms into your weights. A lot of animals are pre-baked in this sense. Humans have to figure it out at test time when they get born. You want these environments that change really rapidly, where you can‚Äôt foresee what will work well. You create intelligence to figure it out at test time.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:38:55&lt;/p&gt;
    &lt;p&gt;Quintin Pope had this interesting blog post where he‚Äôs saying the reason he doesn‚Äôt expect a sharp takeoff is that humans had the sharp takeoff where 60,000 years ago we seem to have had the cognitive architectures that we have today. 10,000 years ago, agricultural revolution, modernity. What was happening in that 50,000 years? You had to build this cultural scaffold where you can accumulate knowledge over generations.&lt;/p&gt;
    &lt;p&gt;This is an ability that exists for free in the way we do AI training. In many cases they are literally distilled. If you retrain a model, they can be trained on each other, they can be trained on the same pre-training corpus, they don‚Äôt literally have to start from scratch. There‚Äôs a sense in which it took humans a long time to get this cultural loop going, but it just comes for free with the way we do LLM training.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:39:45&lt;/p&gt;
    &lt;p&gt;Yes and no. Because LLMs don‚Äôt really have the equivalent of culture. Maybe we‚Äôre giving them way too much and incentivizing not to create it or something like that. But the invention of culture and of written record and of passing down notes between each other, I don‚Äôt think there‚Äôs an equivalent of that with LLMs right now. LLMs don‚Äôt really have culture right now and it‚Äôs one of the impediments I would say.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:40:05&lt;/p&gt;
    &lt;p&gt;Can you give me some sense of what LLM culture might look like?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:40:09&lt;/p&gt;
    &lt;p&gt;In the simplest case it would be a giant scratchpad that the LLM can edit and as it‚Äôs reading stuff or as it‚Äôs helping out with work, it‚Äôs editing the scratchpad for itself. Why can‚Äôt an LLM write a book for the other LLMs? That would be cool. Why can‚Äôt other LLMs read this LLM‚Äôs book and be inspired by it or shocked by it or something like that? There‚Äôs no equivalence for any of this stuff.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:40:29&lt;/p&gt;
    &lt;p&gt;Interesting. When would you expect that kind of thing to start happening? Also, multi-agent systems and a sort of independent AI civilization and culture?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:40:40&lt;/p&gt;
    &lt;p&gt;There are two powerful ideas in the realm of multi-agent that have both not been really claimed or so on. The first one I would say is culture and LLMs having a growing repertoire of knowledge for their own purposes.&lt;/p&gt;
    &lt;p&gt;The second one looks a lot more like the powerful idea of self-play. In my mind it‚Äôs extremely powerful. Evolution has a lot of competition driving intelligence and evolution. In AlphaGo more algorithmically, AlphaGo is playing against itself and that‚Äôs how it learns to get really good at Go. There‚Äôs no equivalent of self-playing LLMs, but I would expect that to also exist. No one has done it yet. Why can‚Äôt an LLM for example, create a bunch of problems that another LLM is learning to solve? Then the LLM is always trying to serve more and more difficult problems, stuff like that.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a bunch of ways to organize it. It‚Äôs a realm of research, but I haven‚Äôt seen anything that convincingly claims both of those multi-agent improvements. We‚Äôre mostly in the realm of a single individual agent, but that will change. In the realm of culture also, I would also bucket organizations. We haven‚Äôt seen anything like that convincingly either. That‚Äôs why we‚Äôre still early.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:41:53&lt;/p&gt;
    &lt;p&gt;Can you identify the key bottleneck that‚Äôs preventing this kind of collaboration between LLMs?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:41:59&lt;/p&gt;
    &lt;p&gt;Maybe the way I would put it is, some of these analogies work and they shouldn‚Äôt, but somehow, remarkably, they do. A lot of the smaller models, or the dumber models, remarkably resemble a kindergarten student, or an elementary school student or high school student. Somehow, we still haven‚Äôt graduated enough where this stuff can take over. My Claude Code or Codex, they still feel like this elementary-grade student. I know that they can take PhD quizzes, but they still cognitively feel like a kindergarten or an elementary school student.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think they can create culture because they‚Äôre still kids. They‚Äôre savant kids. They have perfect memory of all this stuff. They can convincingly create all kinds of slop that looks really good. But I still think they don‚Äôt really know what they‚Äôre doing and they don‚Äôt really have the cognition across all these little checkboxes that we still have to collect.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:42:55 - Why self driving took so long&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:42:55&lt;/p&gt;
    &lt;p&gt;You‚Äôve talked about how you were at Tesla leading self-driving from 2017 to 2022. And you firsthand saw this progress from cool demos to now thousands of cars out there actually autonomously doing drives. Why did that take a decade? What was happening through that time?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:43:11&lt;/p&gt;
    &lt;p&gt;One thing I will almost instantly push back on is that this is not even near done, in a bunch of ways that I‚Äôm going to get to. Self-driving is very interesting because it‚Äôs definitely where I get a lot of my intuitions because I spent five years on it. It has this entire history where the first demos of self-driving go all the way to the 1980s. You can see a demo from CMU in 1986. There‚Äôs a truck that‚Äôs driving itself on roads.&lt;/p&gt;
    &lt;p&gt;Fast forward. When I was joining Tesla, I had a very early demo of Waymo. It basically gave me a perfect drive in 2014 or something like that, so a perfect Waymo drive a decade ago. It took us around Palo Alto and so on because I had a friend who worked there. I thought it was very close and then it still took a long time.&lt;/p&gt;
    &lt;p&gt;For some kinds of tasks and jobs and so on, there‚Äôs a very large demo-to-product gap where the demo is very easy, but the product is very hard. It‚Äôs especially the case in cases like self-driving where the cost of failure is too high. Many industries, tasks, and jobs maybe don‚Äôt have that property, but when you do have that property, that definitely increases the timelines.&lt;/p&gt;
    &lt;p&gt;For example, in software engineering, I do think that property does exist. For a lot of vibe coding, it doesn‚Äôt. But if you‚Äôre writing actual production-grade code, that property should exist, because any kind of mistake leads to a security vulnerability or something like that. Millions and hundreds of millions of people‚Äôs personal Social Security numbers get leaked or something like that. So in software, people should be careful, kind of like in self-driving. In self-driving, if things go wrong, you might get injured. There are worse outcomes. But in software, it‚Äôs almost unbounded how terrible something could be.&lt;/p&gt;
    &lt;p&gt;I do think that they share that property. What takes the long amount of time and the way to think about it is that it‚Äôs a march of nines. Every single nine is a constant amount of work. Every single nine is the same amount of work. When you get a demo and something works 90% of the time, that‚Äôs just the first nine. Then you need the second nine, a third nine, a fourth nine, a fifth nine. While I was at Tesla for five years or so, we went through maybe three nines or two nines. I don‚Äôt know what it is, but multiple nines of iteration. There are still more nines to go.&lt;/p&gt;
    &lt;p&gt;That‚Äôs why these things take so long. It‚Äôs definitely formative for me, seeing something that was a demo. I‚Äôm very unimpressed by demos. Whenever I see demos of anything, I‚Äôm extremely unimpressed by that. If it‚Äôs a demo that someone cooked up as a showing, it‚Äôs worse. If you can interact with it, it‚Äôs a bit better. But even then, you‚Äôre not done. You need the actual product. It‚Äôs going to face all these challenges when it comes in contact with reality and all these different pockets of behavior that need patching.&lt;/p&gt;
    &lt;p&gt;We‚Äôre going to see all this stuff play out. It‚Äôs a march of nines. Each nine is constant. Demos are encouraging. It‚Äôs still a huge amount of work to do. It is a critical safety domain, unless you‚Äôre doing vibe coding, which is all nice and fun and so on. That‚Äôs why this also enforced my timelines from that perspective.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:46:25&lt;/p&gt;
    &lt;p&gt;It‚Äôs very interesting to hear you say that, that the safety guarantees you need from software are not dissimilar to self-driving. What people will often say is that self-driving took so long because the cost of failure is so high. A human makes a mistake on average every 400,000 miles or every seven years. If you had to release a coding agent that couldn‚Äôt make a mistake for at least seven years, it would be much harder to deploy.&lt;/p&gt;
    &lt;p&gt;But your point is that if you made a catastrophic coding mistake, like breaking some important system every seven years...&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:46:56&lt;/p&gt;
    &lt;p&gt;Very easy to do.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:46:57&lt;/p&gt;
    &lt;p&gt;In fact, in terms of wall clock time, it would be much less than seven years because you‚Äôre constantly outputting code like that. In terms of tokens, it would be seven years. But in terms of wall clock time...&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:47:09&lt;/p&gt;
    &lt;p&gt;In some ways, it‚Äôs a much harder problem. Self-driving is just one of thousands of things that people do. It‚Äôs almost like a single vertical, I suppose. Whereas when we‚Äôre talking about general software engineering, it‚Äôs even more... There‚Äôs more surface area.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:47:20&lt;/p&gt;
    &lt;p&gt;There‚Äôs another objection people make to that analogy, which is that with self-driving, what took a big fraction of that time was solving the problem of having basic perception that‚Äôs robust, building representations, and having a model that has some common sense so it can generalize to when it sees something that‚Äôs slightly out of distribution. If somebody‚Äôs waving down the road this way, you don‚Äôt need to train for it. The thing will have some understanding of how to respond to something like that.&lt;/p&gt;
    &lt;p&gt;These are things we‚Äôre getting for free with LLMs or VLMs today, so we don‚Äôt have to solve these very basic representation problems. So now deploying AIs across different domains will sort of be like deploying a self-driving car with current models to a different city, which is hard but not like a 10-year-long task.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:48:07&lt;/p&gt;
    &lt;p&gt;I‚Äôm not 100% sure if I fully agree with that. I don‚Äôt know how much we‚Äôre getting for free. There‚Äôs still a lot of gaps in understanding what we are getting. We‚Äôre definitely getting more generalizable intelligence in a single entity, whereas self-driving is a very special-purpose task that requires. In some sense building a special-purpose task is maybe even harder in a certain sense because it doesn‚Äôt fall out from a more general thing that you‚Äôre doing at scale, if that makes sense.&lt;/p&gt;
    &lt;p&gt;But the analogy still doesn‚Äôt fully resonate because the LLMs are still pretty fallible and they have a lot of gaps that still need to be filled in. I don‚Äôt think that we‚Äôre getting magical generalization completely out of the box, in a certain sense.&lt;/p&gt;
    &lt;p&gt;The other aspect that I wanted to return to is that self-driving cars are nowhere near done still. The deployments are pretty minimal. Even Waymo and so on has very few cars. They‚Äôre doing that roughly speaking because they‚Äôre not economical. They‚Äôve built something that lives in the future. They‚Äôve had to pull back the future, but they had to make it uneconomical. There are all these costs, not just marginal costs for those cars and their operation and maintenance, but also the capex of the entire thing. Making it economical is still going to be a slog for them.&lt;/p&gt;
    &lt;p&gt;Also, when you look at these cars and there‚Äôs no one driving, I actually think it‚Äôs a little bit deceiving because there are very elaborate teleoperation centers of people kind of in a loop with these cars. I don‚Äôt have the full extent of it, but there‚Äôs more human-in-the-loop than you might expect. There are people somewhere out there beaming in from the sky. I don‚Äôt know if they‚Äôre fully in the loop with the driving. Some of the time they are, but they‚Äôre certainly involved and there are people. In some sense, we haven‚Äôt actually removed the person, we‚Äôve moved them to somewhere where you can‚Äôt see them.&lt;/p&gt;
    &lt;p&gt;I still think there will be some work, as you mentioned, going from environment to environment. There are still challenges to make self-driving real. But I do agree that it‚Äôs definitely crossed a threshold where it kind of feels real, unless it‚Äôs really teleoperated. For example, Waymo can‚Äôt go to all the different parts of the city. My suspicion is that it‚Äôs parts of the city where you don‚Äôt get good signal. Anyway, I don‚Äôt know anything about the stack. I‚Äôm just making stuff up.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:50:23&lt;/p&gt;
    &lt;p&gt;You led self-driving for five years at Tesla.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:50:27&lt;/p&gt;
    &lt;p&gt;Sorry, I don‚Äôt know anything about the specifics of Waymo. By the way, I love Waymo and I take it all the time. I just think that people are sometimes a little bit too naive about some of the progress and there‚Äôs still a huge amount of work. Tesla took in my mind a much more scalable approach and the team is doing extremely well. I‚Äôm kind of on the record for predicting how this thing will go. Waymo had an early start because you can package up so many sensors. But I do think Tesla is taking the more scalable strategy and it‚Äôs going to look a lot more like that. So this will still have to play out and hasn‚Äôt. But I don‚Äôt want to talk about self-driving as something that took a decade because it didn‚Äôt take it yet, if that makes sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:51:08&lt;/p&gt;
    &lt;p&gt;Because one, the start is at 1980 and not 10 years ago, and then two, the end is not here yet.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:51:14&lt;/p&gt;
    &lt;p&gt;The end is not near yet because when we‚Äôre talking about self-driving, usually in my mind it‚Äôs self-driving at scale. People don‚Äôt have to get a driver‚Äôs license, etc.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:51:22&lt;/p&gt;
    &lt;p&gt;I‚Äôm curious to bounce two other ways in which the analogy might be different. The reason I‚Äôm especially curious about this is because the question of how fast AI is deployed, how valuable it is when it‚Äôs early on is potentially the most important question in the world right now. If you‚Äôre trying to model what the year 2030 looks like, this is the question you ought to have some understanding of.&lt;/p&gt;
    &lt;p&gt;Another thing you might think is one, you have this latency requirement with self-driving. I have no idea what the actual models are, but I assume it‚Äôs like tens of millions of parameters or something, which is not the necessary constraint for knowledge work with LLMs. Maybe it might be with computer use and stuff.&lt;/p&gt;
    &lt;p&gt;But the other big one is, maybe more importantly, on this capex question. Yes, there is additional cost to serving up an additional copy of a model, but the opex of a session is quite low and you can amortize the cost of AI into the training run itself, depending on how inference scaling goes and stuff. But it‚Äôs certainly not as much as building a whole new car to serve another instance of a model. So the economics of deploying more widely are much more favorable.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:52:37&lt;/p&gt;
    &lt;p&gt;I think that‚Äôs right. If you‚Äôre sticking to the realm of bits, bits are a million times easier than anything that touches the physical world. I definitely grant that. Bits are completely changeable, arbitrarily reshuffleable at a very rapid speed. You would expect a much faster adaptation also in the industry and so on. What was the first one?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:52:59&lt;/p&gt;
    &lt;p&gt;The latency requirements and its implications for model size?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:53:02&lt;/p&gt;
    &lt;p&gt;I think that‚Äôs roughly right. I also think that if we are talking about knowledge work at scale, there will be some latency requirements, practically speaking, because we‚Äôre going to have to create a huge amount of compute and serve that.&lt;/p&gt;
    &lt;p&gt;The last aspect that I very briefly want to also talk about is all the rest of it. What does society think about it? What are the legal ramifications? How is it working legally? How is it working insurance-wise? What are those layers of it and aspects of it? What is the equivalent of people putting a cone on a Waymo? There are going to be equivalents of all that. So I feel like self-driving is a very nice analogy that you can borrow things from. What is the equivalent of a cone in the car? What is the equivalent of a teleoperating worker who‚Äôs hidden away and all the aspects of it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:53:53&lt;/p&gt;
    &lt;p&gt;Do you have any opinions on what this implies about the current AI buildout, which would 10x the amount of available compute in the world in a year or two and maybe more than 100x it by the end of the decade. If the use of AI will be lower than some people naively predict, does that mean that we‚Äôre overbuilding compute or is that a separate question?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:15&lt;/p&gt;
    &lt;p&gt;Kind of like what happened with railroads.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:54:18&lt;/p&gt;
    &lt;p&gt;With what, sorry?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:19&lt;/p&gt;
    &lt;p&gt;Was it railroads or?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:54:20&lt;/p&gt;
    &lt;p&gt;Yeah, it was.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:54:21&lt;/p&gt;
    &lt;p&gt;Yeah. There‚Äôs historical precedent. Or was it with the telecommunication industry? Pre-paving the internet that only came a decade later and creating a whole bubble in the telecommunications industry in the late ‚Äò90s.&lt;/p&gt;
    &lt;p&gt;I understand I‚Äôm sounding very pessimistic here. I‚Äôm actually optimistic. I think this will work. I think it‚Äôs tractable. I‚Äôm only sounding pessimistic because when I go on my Twitter timeline, I see all this stuff that makes no sense to me. There‚Äôs a lot of reasons for why that exists. A lot of it is honestly just fundraising. It‚Äôs just incentive structures. A lot of it may be fundraising. A lot of it is just attention, converting attention to money on the internet, stuff like that. There‚Äôs a lot of that going on, and I‚Äôm only reacting to that.&lt;/p&gt;
    &lt;p&gt;But I‚Äôm still overall very bullish on technology. We‚Äôre going to work through all this stuff. There‚Äôs been a rapid amount of progress. I don‚Äôt know that there‚Äôs overbuilding. I think we‚Äôre going to be able to gobble up what, in my understanding, is being built. For example, Claude Code or OpenAI Codex and stuff like that didn‚Äôt even exist a year ago. Is that right? This is a miraculous technology that didn‚Äôt exist. There‚Äôs going to be a huge amount of demand, as we see the demand in ChatGPT already and so on.&lt;/p&gt;
    &lt;p&gt;So I don‚Äôt know that there‚Äôs overbuilding. I‚Äôm just reacting to some of the very fast timelines that people continue to say incorrectly. I‚Äôve heard many, many times over the course of my 15 years in AI where very reputable people keep getting this wrong all the time. I want this to be properly calibrated, and some of this also has geopolitical ramifications and things like that with some of these questions. I don‚Äôt want people to make mistakes in that sphere of things. I do want us to be grounded in the reality of what technology is and isn‚Äôt.&lt;/p&gt;
    &lt;head rend="h3"&gt;01:56:20 - Future of education&lt;/head&gt;
    &lt;p&gt;Dwarkesh Patel 01:56:20&lt;/p&gt;
    &lt;p&gt;Let‚Äôs talk about education and Eureka. One thing you could do is start another AI lab and then try to solve those problems. I‚Äôm curious what you‚Äôre up to now, and why not AI research itself?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:56:33&lt;/p&gt;
    &lt;p&gt;I guess the way I would put it is I feel some amount of determinism around the things that AI labs are doing. I feel like I could help out there, but I don‚Äôt know that I would uniquely improve it. My personal big fear is that a lot of this stuff happens on the side of humanity, and that humanity gets disempowered by it. I care not just about all the Dyson spheres that we‚Äôre going to build and that AI is going to build in a fully autonomous way, I care about what happens to humans. I want humans to be well off in the future.&lt;/p&gt;
    &lt;p&gt;I feel like that‚Äôs where I can a lot more uniquely add value than an incremental improvement in the frontier lab. I‚Äôm most afraid of something depicted in movies like WALL-E or Idiocracy or something like that, where humanity is on the side of this stuff. I want humans to be much, much better in this future. To me, this is through education that you can achieve this.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:57:35&lt;/p&gt;
    &lt;p&gt;So what are you working on there?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:57:36&lt;/p&gt;
    &lt;p&gt;The easiest way I can describe it is we‚Äôre trying to build the Starfleet Academy. I don‚Äôt know if you‚Äôve watched Star Trek.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:57:44&lt;/p&gt;
    &lt;p&gt;I haven‚Äôt.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:57:44&lt;/p&gt;
    &lt;p&gt;Starfleet Academy is this elite institution for frontier technology, building spaceships, and graduating cadets to be the pilots of these spaceships and whatnot. So I just imagine an elite institution for technical knowledge and a kind of school that‚Äôs very up-to-date and a premier institution.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 01:58:05&lt;/p&gt;
    &lt;p&gt;A category of questions I have for you is explaining how one teaches technical or scientific content well, because you are one of the world masters at it. I‚Äôm curious both about how you think about it for content you‚Äôve already put out there on YouTube, but also, to the extent it‚Äôs any different, how you think about it for Eureka.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 01:58:25&lt;/p&gt;
    &lt;p&gt;With respect to Eureka, one thing that is very fascinating to me about education is that I do think education will pretty fundamentally change with AIs on the side. It has to be rewired and changed to some extent.&lt;/p&gt;
    &lt;p&gt;I still think that we‚Äôre pretty early. There‚Äôs going to be a lot of people who are going to try to do the obvious things. Have an LLM and ask it questions. Do all the basic things that you would do via prompting right now. It‚Äôs helpful, but it still feels to me a bit like slop. I‚Äôd like to do it properly, and I think the capability is not there for what I would want. What I‚Äôd want is an actual tutor experience.&lt;/p&gt;
    &lt;p&gt;A prominent example in my mind is I was recently learning Korean, so language learning. I went through a phase where I was learning Korean by myself on the internet. I went through a phase where I was part of a small class in Korea taking Korean with a bunch of other people, which was really funny. We had a teacher and 10 people or so taking Korean. Then I switched to a one-on-one tutor.&lt;/p&gt;
    &lt;p&gt;I guess what was fascinating to me was, I think I had a really good tutor, but just thinking through what this tutor was doing for me and how incredible that experience was and how high the bar is for what I want to build eventually. Instantly from a very short conversation, she understood where I am as a student, what I know and don‚Äôt know. She was able to probe exactly the kinds of questions or things to understand my world model. No LLM will do that for you 100% right now, not even close. But a tutor will do that if they‚Äôre good. Once she understands, she really served me all the things that I needed at my current sliver of capability. I need to be always appropriately challenged. I can‚Äôt be faced with something too hard or too trivial, and a tutor is really good at serving you just the right stuff.&lt;/p&gt;
    &lt;p&gt;I felt like I was the only constraint to learning. I was always given the perfect information. I‚Äôm the only constraint. I felt good because I‚Äôm the only impediment that exists. It‚Äôs not that I can‚Äôt find knowledge or that it‚Äôs not properly explained or etc. It‚Äôs just my ability to memorize and so on. This is what I want for people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:00:27&lt;/p&gt;
    &lt;p&gt;How do you automate that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:00:29&lt;/p&gt;
    &lt;p&gt;Very good question. At the current capability, you don‚Äôt. That‚Äôs why I think it‚Äôs not actually the right time to build this kind of an AI tutor. I still think it‚Äôs a useful product, and lots of people will build it, but the bar is so high and the capability is not there. Even today, I would say ChatGPT is an extremely valuable educational product. But for me, it was so fascinating to see how high the bar is. When I was with her, I almost felt like there‚Äôs no way I can build this.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:01:02&lt;/p&gt;
    &lt;p&gt;But you are building it, right?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:01:03&lt;/p&gt;
    &lt;p&gt;Anyone who‚Äôs had a really good tutor is like, ‚ÄúHow are you going to build this?‚Äù I‚Äôm waiting for that capability.&lt;/p&gt;
    &lt;p&gt;I did some AI consulting for computer vision. A lot of times, the value that I brought to the company was telling them not to use AI. I was the AI expert, and they described the problem, and I said, ‚ÄúDon‚Äôt use AI.‚Äù This is my value add. I feel like it‚Äôs the same in education right now, where I feel like for what I have in mind, it‚Äôs not yet the time, but the time will come. For now, I‚Äôm building something that looks maybe a bit more conventional that has a physical and digital component and so on. But it‚Äôs obvious how this should look in the future.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:01:43&lt;/p&gt;
    &lt;p&gt;To the extent you‚Äôre willing to say, what is the thing you hope will be released this year or next year?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:01:49&lt;/p&gt;
    &lt;p&gt;I‚Äôm building the first course. I want to have a really, really good course, the obvious state-of-the-art destination you go to to learn, AI in this case. That‚Äôs just what I‚Äôm familiar with, so it‚Äôs a really good first product to get to be really good at it. So that‚Äôs what I‚Äôm building. Nanochat, which you briefly mentioned, is a capstone project of LLM101N, which is a class that I‚Äôm building. That‚Äôs a really big piece of it. But now I have to build out a lot of the intermediates, and then I have to hire a small team of TAs and so on and build the entire course.&lt;/p&gt;
    &lt;p&gt;One more thing that I would say is that many times, when people think about education, they think more about what I would say is a softer component of diffusing knowledge. I have something very hard and technical in mind. In my mind, education is the very difficult technical process of building ramps to knowledge. In my mind, nanochat is a ramp to knowledge because it‚Äôs very simple. It‚Äôs the super simplified full-stack thing. If you give this artifact to someone and they look through it, they‚Äôre learning a ton of stuff. It‚Äôs giving you a lot of what I call eurekas per second, which is understanding per second. That‚Äôs what I want, lots of eurekas per second. So to me, this is a technical problem of how do we build these ramps to knowledge.&lt;/p&gt;
    &lt;p&gt;So I almost think of Eureka as maybe not that different from some of the frontier labs or some of the work that‚Äôs going on there. I want to figure out how to build these ramps very efficiently so that people are never stuck and everything is always not too hard or not too trivial, and you have just the right material to progress.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:03:25&lt;/p&gt;
    &lt;p&gt;You‚Äôre imagining in the short term that instead of a tutor being able to probe your understanding, if you have enough self-awareness to be able to probe yourself, you‚Äôre never going to be stuck. You can find the right answer between talking to the TA or talking to an LLM and looking at the reference implementation. It sounds like automation or AI is not a significant part. So far, the big alpha here is your ability to explain AI codified in the source material of the class. That‚Äôs fundamentally what the course is.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:04:00&lt;/p&gt;
    &lt;p&gt;You always have to be calibrated to what capability exists in the industry. A lot of people are going to pursue just asking ChatGPT, etc. But I think right now, for example, if you go to ChatGPT and you say, teach me AI, there‚Äôs no way. It‚Äôs going to give you some slop. AI is never going to write nanochat right now. But nanochat is a really useful intermediate point. I‚Äôm collaborating with AI to create all this material, so AI is still fundamentally very helpful.&lt;/p&gt;
    &lt;p&gt;Earlier on, I built CS231n at Stanford, which I think was the first deep learning class at Stanford, which became very popular. The difference in building out 231n then and LLM101N now is quite stark. I feel really empowered by the LLMs as they exist right now, but I‚Äôm very much in the loop. They‚Äôre helping me build the materials, I go much faster. They‚Äôre doing a lot of the boring stuff, etc. I feel like I‚Äôm developing the course much faster, and it‚Äôs LLM-infused, but it‚Äôs not yet at a place where it can creatively create the content. I‚Äôm still there to do that. The trickiness is always calibrating yourself to what exists.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:05:04&lt;/p&gt;
    &lt;p&gt;When you imagine what is available through Eureka in a couple of years, it seems like the big bottleneck is going to be finding Karpathys in field after field who can convert their understanding into these ramps.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:05:18&lt;/p&gt;
    &lt;p&gt;It would change over time. Right now, it would be hiring faculty to help work hand-in-hand with AI and a team of people probably to build state-of-the-art courses. Over time maybe some of the TAs can become AIs. You just take all the course materials and then I think you could serve a very good automated TA for the student when they have more basic questions or something like that. But I think you‚Äôll need faculty for the overall architecture of a course and making sure that it fits. So I see a progression of how this will evolve. Maybe at some future point I‚Äôm not even that useful and AI is doing most of the design much better than I could. But I still think that‚Äôs going to take some time to play out.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:05:59&lt;/p&gt;
    &lt;p&gt;Are you imagining that people who have expertise in other fields are then contributing courses, or do you feel like it‚Äôs quite essential to the vision that you, given your understanding of how you want to teach, are the one designing the content? Sal Khan is narrating all the videos on Khan Academy. Are you imagining something like that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:06:20&lt;/p&gt;
    &lt;p&gt;No, I will hire faculty because there are domains in which I‚Äôm not an expert. That‚Äôs the only way to offer the state-of-the-art experience for the student ultimately. I do expect that I would hire faculty, but I will probably stick around in AI for some time. I do have something more conventional in mind for the current capability than what people would probably anticipate.&lt;/p&gt;
    &lt;p&gt;When I‚Äôm building Starfleet Academy, I do probably imagine a physical institution, and maybe a tier below that a digital offering that is not the state-of-the-art experience you would get when someone comes in physically full-time and we work through material from start to end and make sure you understand it. That‚Äôs the physical offering. The digital offering is a bunch of stuff on the internet and maybe some LLM assistant. It‚Äôs a bit more gimmicky in a tier below, but at least it‚Äôs accessible to 8 billion people.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:07:08&lt;/p&gt;
    &lt;p&gt;I think you‚Äôre basically inventing college from first principles for the tools that are available today and just selecting for people who have the motivation and the interest of really engaging with material.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:07:26&lt;/p&gt;
    &lt;p&gt;There‚Äôs going to have to be a lot of not just education but also re-education. I would love to help out there because the jobs will probably change quite a bit. For example, today a lot of people are trying to upskill in AI specifically. I think it‚Äôs a really good course to teach in this respect. Motivation-wise, before AGI motivation is very simple to solve because people want to make money. This is how you make money in the industry today. Post-AGI is a lot more interesting possibly because if everything is automated and there‚Äôs nothing to do for anyone, why would anyone go to a school?&lt;/p&gt;
    &lt;p&gt;I often say that pre-AGI education is useful. Post-AGI education is fun. In a similar way, people go to the gym today. We don‚Äôt need their physical strength to manipulate heavy objects because we have machines that do that. They still go to the gym. Why do they go to the gym? Because it‚Äôs fun, it‚Äôs healthy, and you look hot when you have a six-pack. It‚Äôs attractive for people to do that in a very deep, psychological, evolutionary sense for humanity. Education will play out in the same way. You‚Äôll go to school like you go to the gym.&lt;/p&gt;
    &lt;p&gt;Right now, not that many people learn because learning is hard. You bounce from material. Some people overcome that barrier, but for most people, it‚Äôs hard. It‚Äôs a technical problem to solve. It‚Äôs a technical problem to do what my tutor did for me when I was learning Korean. It‚Äôs tractable and buildable, and someone should build it. It‚Äôs going to make learning anything trivial and desirable, and people will do it for fun because it‚Äôs trivial. If I had a tutor like that for any arbitrary piece of knowledge, it‚Äôs going to be so much easier to learn anything, and people will do it. They‚Äôll do it for the same reasons they go to the gym.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:09:17&lt;/p&gt;
    &lt;p&gt;That sounds different from using‚Ä¶ So post-AGI, you‚Äôre using this as entertainment or as self-betterment. But it sounded like you had a vision also that this education is relevant to keeping humanity in control of AI. That sounds different. Is it entertaining for some people, but then empowerment for some others? How do you think about that?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:09:41&lt;/p&gt;
    &lt;p&gt;I do think eventually it‚Äôs a bit of a losing game, if that makes sense. It is in the long term. In the long term, which is longer than maybe most people in the industry think about, it‚Äôs a losing game. I do think people can go so far and we‚Äôve barely scratched the surface of how much a person can go. That‚Äôs just because people are bouncing off of material that‚Äôs too easy or too hard. People will be able to go much further. Anyone will speak five languages because why not? Because it‚Äôs so trivial. Anyone will know all the basic curriculum of undergrad, et cetera.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:10:18&lt;/p&gt;
    &lt;p&gt;Now that I‚Äôm understanding the vision, that‚Äôs very interesting. It has a perfect analog in gym culture. I don‚Äôt think 100 years ago anybody would be ripped. Nobody would have been able to just spontaneously bench two plates or three plates or something. It‚Äôs very common now because of this idea of systematically training and lifting weights in the gym, or systematically training to be able to run a marathon, which is a capability most humans would not spontaneously have. You‚Äôre imagining similar things for learning across many different domains, much more intensely, deeply, faster.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:10:54&lt;/p&gt;
    &lt;p&gt;Exactly. I am betting a bit implicitly on some of the timelessness of human nature. It will be desirable to do all these things, and I think people will look up to it as they have for millennia. This will continue to be true. There‚Äôs some evidence of that historically. If you look at, for example, aristocrats, or you look at ancient Greece or something like that, whenever you had little pocket environments that were post-AGI in a certain sense, people have spent a lot of their time flourishing in a certain way, either physically or cognitively. I feel okay about the prospects of that.&lt;/p&gt;
    &lt;p&gt;If this is false and I‚Äôm wrong and we end up in a WALL-E or Idiocracy future, then I don‚Äôt even care if there are Dyson spheres. This is a terrible outcome. I really do care about humanity. Everyone has to just be superhuman in a certain sense.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:11:52&lt;/p&gt;
    &lt;p&gt;It‚Äôs still a world in which that is not enabling us to‚Ä¶ It‚Äôs like the culture world, right? You‚Äôre not fundamentally going to be able to transform the trajectory of technology or influence decisions by your own labor or cognition alone. Maybe you can influence decisions because the AI is asking for your approval, but it‚Äôs not because I‚Äôve invented something or I‚Äôve come up with a new design that I‚Äôm really influencing the future.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:12:21&lt;/p&gt;
    &lt;p&gt;Maybe. I think there will be a transitional period where we are going to be able to be in the loop and advance things if we understand a lot of stuff. In the long-term, that probably goes away. It might even become a sport. Right now you have powerlifters who go extreme in this direction. What is powerlifting in a cognitive era? Maybe it‚Äôs people who are really trying to make Olympics out of knowing stuff. If you have a perfect AI tutor, maybe you can get extremely far. I feel that the geniuses of today are barely scratching the surface of what a human mind can do, I think.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:12:59&lt;/p&gt;
    &lt;p&gt;I love this vision. I also feel like the person you have the most product-market fit with is me because my job involves having to learn different subjects every week, and I am very excited.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:13:17&lt;/p&gt;
    &lt;p&gt;I‚Äôm similar, for that matter. A lot of people, for example, hate school and want to get out of it. I really liked school. I loved learning things, et cetera. I wanted to stay in school. I stayed all the way until Ph.D. and then they wouldn‚Äôt let me stay longer, so I went to the industry. Roughly speaking, I love learning, even for the sake of learning, but I also love learning because it‚Äôs a form of empowerment and being useful and productive.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:13:39&lt;/p&gt;
    &lt;p&gt;You also made a point that was subtle and I want to spell it out. With what‚Äôs happened so far with online courses, why haven‚Äôt they already enabled us to enable every single human to know everything? They‚Äôre just so motivation-laden because there are no obvious on-ramps and it‚Äôs so easy to get stuck. If you had this thing instead‚Äîlike a really good human tutor‚Äîit would just be such an unlock from a motivation perspective.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:14:10&lt;/p&gt;
    &lt;p&gt;I think so. It feels bad to bounce from material. It feels bad. You get negative reward from sinking an amount of time in something and it doesn‚Äôt pan out, or being completely bored because what you‚Äôre getting is too easy or too hard. When you do it properly, learning feels good. It‚Äôs a technical problem to get there. For a while, it‚Äôs going to be AI plus human collab, and at some point, maybe it‚Äôs just AI.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:14:36&lt;/p&gt;
    &lt;p&gt;Can I ask some questions about teaching well? If you had to give advice to another educator in another field that you‚Äôre curious about to make the kinds of YouTube tutorials you‚Äôve made. Maybe it might be especially interesting to talk about domains where you can‚Äôt test someone‚Äôs technical understanding by having them code something up or something. What advice would you give them?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:14:58&lt;/p&gt;
    &lt;p&gt;That‚Äôs a pretty broad topic. There are 10‚Äì20 tips and tricks that I semi-consciously do probably. But a lot of this comes from my physics background. I really, really did enjoy my physics background. I have a whole rant on how everyone should learn physics in early school education because early school education is not about accumulating knowledge or memory for tasks later in the industry. It‚Äôs about booting up a brain. Physics uniquely boots up the brain the best because some of the things that they get you to do in your brain during physics is extremely valuable later.&lt;/p&gt;
    &lt;p&gt;The idea of building models and abstractions and understanding that there‚Äôs a first-order approximation that describes most of the system, but then there‚Äôre second-order, third-order, fourth-order terms that may or may not be present. The idea that you‚Äôre observing a very noisy system, but there are these fundamental frequencies that you can abstract away. When a physicist walks into the class and they say, ‚ÄúAssume there‚Äôs a spherical cow,‚Äù everyone laughs at that, but this is brilliant. It‚Äôs brilliant thinking that‚Äôs very generalizable across the industry because a cow can be approximated as a sphere in a bunch of ways.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a really good book, for example, Scale. It‚Äôs from a physicist talking about biology. Maybe this is also a book I would recommend reading. You can get a lot of really interesting approximations and chart scaling laws of animals. You can look at their heartbeats and things like that, and they line up with the size of the animal and things like that. You can talk about an animal as a volume. You can talk about the heat dissipation of that, because your heat dissipation grows as the surface area, which is growing as a square. But your heat creation or generation is growing as a cube. So I just feel like physicists have all the right cognitive tools to approach problem solving in the world.&lt;/p&gt;
    &lt;p&gt;So because of that training, I always try to find the first-order terms or the second-order terms of everything. When I‚Äôm observing a system or a thing, I have a tangle of a web of ideas or knowledge in my mind. I‚Äôm trying to find, what is the thing that matters? What is the first-order component? How can I simplify it? How can I have a simplest thing that shows that thing, shows it in action, and then I can tack on the other terms?&lt;/p&gt;
    &lt;p&gt;Maybe an example from one of my repos that I think illustrates it well is called micrograd. I don‚Äôt know if you‚Äôre familiar with this. So micrograd is 100 lines of code that shows backpropagation. You can create neural networks out of simple operations like plus and times, et cetera. Lego blocks of neural networks. You build up a computational graph and you do a forward pass and a backward pass to get the gradients. Now, this is at the heart of all neural network learning.&lt;/p&gt;
    &lt;p&gt;So micrograd is a 100 lines of pretty interpretable Python code, and it can do forward and backward arbitrary neural networks, but not efficiently. So micrograd, these 100 lines of Python, are everything you need to understand how neural networks train. Everything else is just efficiency. Everything else is efficiency. There‚Äôs a huge amount of work to get efficiency. You need your tensors, you lay them out, you stride them, you make sure your kernels, orchestrating memory movement correctly, et cetera. It‚Äôs all just efficiency, roughly speaking. But the core intellectual piece of neural network training is micrograd. It‚Äôs 100 lines. You can easily understand it. It‚Äôs a recursive application of chain rule to derive the gradient, which allows you to optimize any arbitrary differentiable function.&lt;/p&gt;
    &lt;p&gt;So I love finding these small-order terms and serving them on a platter and discovering them. I feel like education is the most intellectually interesting thing because you have a tangle of understanding and you‚Äôre trying to lay it out in a way that creates a ramp where everything only depends on the thing before it. I find that this untangling of knowledge is just so intellectually interesting as a cognitive task. I love doing it personally, but I just have a fascination with trying to lay things out in a certain way. Maybe that helps me.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:18:41&lt;/p&gt;
    &lt;p&gt;It also makes the learning experience so much more motivated. Your tutorial on the transformer begins with bigrams, literally a lookup table from, ‚ÄúHere‚Äôs the word right now, or here‚Äôs the previous word, here‚Äôs the next word.‚Äù It‚Äôs literally just a lookup table.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:18:58&lt;/p&gt;
    &lt;p&gt;That‚Äôs the essence of it, yeah.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:18:59&lt;/p&gt;
    &lt;p&gt;It‚Äôs such a brilliant way, starting with a lookup table and then going to a transformer. Each piece is motivated. Why would you add that? Why would you add the next thing? You could memorize the attention formula, but having an understanding of why every single piece is relevant, what problem it solves.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:19:13&lt;/p&gt;
    &lt;p&gt;You‚Äôre presenting the pain before you present a solution, and how clever is that? You want to take the student through that progression. There are a lot of other small things that make it nice and engaging and interesting. Always prompting the student.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a lot of small things like that are important and a lot of good educators will do this. How would you solve this? I‚Äôm not going to present the solution before you guess. That would be wasteful. That‚Äôs a little bit of a‚Ä¶I don‚Äôt want to swear but it‚Äôs a dick move towards you to present you with the solution before I give you a shot to try to come up with it yourself.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:19:51&lt;/p&gt;
    &lt;p&gt;Because if you try to come up with it yourself, you get a better understanding of what the action space is, what the objective is, and then why only this action fulfills that objective.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:20:03&lt;/p&gt;
    &lt;p&gt;You have a chance to try it yourself, and you have an appreciation when I give you the solution. It maximizes the amount of knowledge per new fact added.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:20:11&lt;/p&gt;
    &lt;p&gt;Why do you think, by default, people who are genuine experts in their field are often bad at explaining it to somebody ramping up?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:20:24&lt;/p&gt;
    &lt;p&gt;It‚Äôs the curse of knowledge and expertise. This is a real phenomenon, and I suffered from it myself as much as I try not to. But you take certain things for granted, and you can‚Äôt put yourself in the shoes of new people who are just starting out. This is pervasive and happens to me as well.&lt;/p&gt;
    &lt;p&gt;One thing that‚Äôs extremely helpful. As an example, someone was trying to show me a paper in biology recently, and I just instantly had so many terrible questions. What I did was I used ChatGPT to ask the questions with the paper in the context window. It worked through some of the simple things. Then I shared the thread to the person who wrote that paper or worked on that work. I felt like if they could see the dumb questions I had, it might help them explain better in the future.&lt;/p&gt;
    &lt;p&gt;For my material, I would love it if people shared their dumb conversations with ChatGPT about the stuff that I‚Äôve created because it really helps me put myself again in the shoes of someone who‚Äôs starting out.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:21:19&lt;/p&gt;
    &lt;p&gt;Another trick that just works astoundingly well. If somebody writes a paper or a blog post or an announcement, it is in 100% of cases that just the narration or the transcription of how they would explain it to you over lunch is way more, not only understandable, but actually also more accurate and scientific, in the sense that people have a bias to explain things in the most abstract, jargon-filled way possible and to clear their throat for four paragraphs before they explain the central idea. But there‚Äôs something about communicating one-on-one with a person which compels you to just say the thing.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:22:07&lt;/p&gt;
    &lt;p&gt;Just say the thing. I saw that tweet, I thought it was really good. I shared it with a bunch of people. I noticed this many, many times.&lt;/p&gt;
    &lt;p&gt;The most prominent example is that I remember back in my PhD days doing research. You read someone‚Äôs paper, and you work to understand what it‚Äôs doing. Then you catch them, you‚Äôre having beers at the conference later, and you ask them, ‚ÄúSo this paper, what were you doing? What is the paper about?‚Äù&lt;/p&gt;
    &lt;p&gt;They will just tell you these three sentences that perfectly captured the essence of that paper and totally give you the idea. And you didn‚Äôt have to read the paper. It‚Äôs only when you‚Äôre sitting at the table with a beer or something, and they‚Äôre like, ‚ÄúOh yeah, the paper is just, you take this idea, you take that idea and try this experiment and you try out this thing.‚Äù They have a way of just putting it conversationally just perfectly. Why isn‚Äôt that the abstract?&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:22:51&lt;/p&gt;
    &lt;p&gt;Exactly. This is coming from the perspective of how somebody who‚Äôs trying to explain an idea should formulate it better. What is your advice as a student to other students, if you don‚Äôt have a Karpathy who is doing the exposition of an idea? If you‚Äôre reading a paper from somebody or reading a book, what strategies do you employ to learn material you‚Äôre interested in in fields you‚Äôre not an expert at?&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:23:20&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know that I have unique tips and tricks, to be honest. It‚Äôs a painful process. One thing that has always helped me quite a bit is‚ÄîI had a small tweet about this‚Äîlearning things on demand is pretty nice. Learning depth-wise. I do feel you need a bit of alternation of learning depth-wise, on demand‚Äîyou‚Äôre trying to achieve a certain project that you‚Äôre going to get a reward from‚Äîand learning breadth-wise, which is just, ‚ÄúOh, let‚Äôs do whatever 101, and here‚Äôs all the things you might need.‚Äù Which is a lot of school‚Äîdoes breadth-wise learning, like, ‚ÄúOh, trust me, you‚Äôll need this later,‚Äù that kind of stuff. Okay, I trust you. I‚Äôll learn it because I guess I need it. But I love the kind of learning where you‚Äôll get a reward out of doing something, and you‚Äôre learning on demand.&lt;/p&gt;
    &lt;p&gt;The other thing that I‚Äôve found extremely helpful. This is an aspect where education is a bit more selfless, but explaining things to people is a beautiful way to learn something more deeply. This happens to me all the time. It probably happens to other people too because I realize if I don‚Äôt really understand something, I can‚Äôt explain it. I‚Äôm trying and I‚Äôm like, ‚ÄúOh, I don‚Äôt understand this.‚Äù It‚Äôs so annoying to come to terms with that. You can go back and make sure you understood it. It fills these gaps of your understanding. It forces you to come to terms with them and to reconcile them.&lt;/p&gt;
    &lt;p&gt;I love to re-explain things and people should be doing that more as well. That forces you to manipulate the knowledge and make sure that you know what you‚Äôre talking about when you‚Äôre explaining it.&lt;/p&gt;
    &lt;p&gt;Dwarkesh Patel 02:24:48&lt;/p&gt;
    &lt;p&gt;That‚Äôs an excellent note to close on. Andrej, that was great.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy 02:24:51&lt;/p&gt;
    &lt;p&gt;Thank you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dwarkesh.com/p/andrej-karpathy"/><published>2025-10-17T17:24:18+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45622365</id><title>New Work by Gary Larson</title><updated>2025-10-18T20:35:11.792013+00:00</updated><content>&lt;doc fingerprint="32709077d7a86c66"&gt;
  &lt;main&gt;
    &lt;p&gt;I don‚Äôt want to mislead anyone here. This corner of the website‚Äî‚ÄúNew Stuff‚Äù‚Äîis not a resurrection of The Far Side daily cartoons. (Well, not exactly, anyway‚Äîlike the proverbial tiger and its stripes, I‚Äôm pretty much stuck with my sense of humor. Aren‚Äôt we all?) The thing is, I thoroughly enjoyed my career as a syndicated cartoonist, and I hope, in spirit at least, we had some laughs together. But after fifteen years of meeting deadlines, well, blah blah blah ‚Ä¶ you know the rest. The day after I retired from syndication, it felt good not to draw on a deadline. And after moving on to other interests, drawing just wasn‚Äôt on my to-do list. Things change. But then a few years ago‚Äîand returning to the subject at hand‚Äîsomething happened in my life, and it started with a clogged pen.&lt;/p&gt;
    &lt;p&gt;Despite my retirement, I still had intermittent connections to cartooning, including my wife‚Äôs and my personal Christmas card. Once a year, I‚Äôd sit myself down to take on Santa, and every year it began with the same ritual: me cursing at, and then cleaning out, my clogged pen. (Apparently, the concept of cleaning it before putting it away each year was just too elusive for me.) As problems go, this is admittedly not exactly on the scale of global warming, but in the small world of my studio, it was cataclysmic. Okay, highly annoying.&lt;/p&gt;
    &lt;p&gt;So a few years ago‚Äîfinally fed up with my once-loyal but now reliably traitorous pen‚ÄîI decided to try a digital tablet. I knew nothing about these devices but hoped it would just get me through my annual Christmas card ordeal. I got one, fired it up, and lo and behold, something totally unexpected happened: within moments, I was having fun drawing again. I was stunned at all the tools the thing offered, all the creative potential it contained. I simply had no idea how far these things had evolved. Perhaps fittingly, the first thing I drew was a caveman.&lt;/p&gt;
    &lt;p&gt;The ‚ÄúNew Stuff‚Äù that you‚Äôll see here is the result of my journey into the world of digital art. Believe me, this has been a bit of a learning curve for me. I hail from a world of pen and ink, and suddenly I was feeling like I was sitting at the controls of a 747. (True, I don‚Äôt get out much.) But as overwhelmed as I was, there was still something familiar there‚Äîa sense of adventure. That had always been at the core of what I enjoyed most when I was drawing The Far Side, that sense of exploring, reaching for something, taking some risks, sometimes hitting a home run and sometimes coming up with ‚ÄúCow tools.‚Äù (Let‚Äôs not get into that.) But as a jazz teacher once said to me about improvisation, ‚ÄúYou want to try and take people somewhere where they might not have been before.‚Äù I think that my approach to cartooning was similar‚ÄîI‚Äôm just not sure if even I knew where I was going. But I was having fun.&lt;/p&gt;
    &lt;p&gt;So here goes. I‚Äôve got my coffee, I‚Äôve got this cool gizmo, and I‚Äôve got no deadlines. And‚Äîto borrow from Sherlock Holmes‚Äîthe game is afoot.&lt;/p&gt;
    &lt;p&gt;Again, please remember, I‚Äôm just exploring, experimenting, and trying stuff. New Stuff. I have just one last thing to say before I go: thank you, clogged pen.&lt;/p&gt;
    &lt;p&gt;‚ÄîGary Larson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.thefarside.com/new-stuff"/><published>2025-10-17T21:34:37+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45624888</id><title>AMD's Chiplet APU: An Overview of Strix Halo</title><updated>2025-10-18T20:35:11.575674+00:00</updated><content>&lt;doc fingerprint="aff85855953a28bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Hello you fine Internet folks!&lt;/p&gt;
    &lt;p&gt;Today we are looking at AMD‚Äôs largest client APU to date, Strix Halo. This is an APU designed to be a true all-in-one mobile processor, able to handle high end CPU and GPU workloads without compromise. Offering a TDP range of 55W to 120W, the chip targets a far higher power envelope compared to standard Strix Point, but eschews the need for dedicated graphics.&lt;/p&gt;
    &lt;p&gt;To get y‚Äôall all caught up on the history and specifications of this APU, AMD first announced Strix Halo at CES 2025 earlier this year to much fanfare. Strix Halo is AMD‚Äôs first chiplet APU in the consumer market with AMD using Strix Halo as a bit of a show piece for what both CPU and GPU performance can look like with a sufficiently large APU.&lt;/p&gt;
    &lt;p&gt;AMD‚Äôs Strix Halo can be equipped with dual 8 core Zen 5 CCDs for a total of 16 cores that feature the same 512b FPU as the desktop parts. This is a change from the more mainstream and monolithic Strix Point APU which has ‚Äúdouble-pumped‚Äù 256b FPUs similar to Zen 4 for use with AVX512 code. What is similar to the more mainstream Strix Point is the same 5.1GHz max boost clock which is a 600MHz deficit compared to the desktop flagship Zen 5 CPU, the Ryzen 9 9950X.&lt;/p&gt;
    &lt;p&gt;Moving to the 3rd die on a Strix Halo package, a RDNA 3.5 iGPU takes up the majority of the SoC die with 40 compute units, 32MB of Infinity Cache, and a boost clock of up to 2.9GHz placing raw compute capability somewhere between the RX 7600 XT and RX 7700.&lt;/p&gt;
    &lt;p&gt;To feed this chip, AMD has equipped Strix Halo with a 256b LPDDR5X-8000 memory bus, which provides up to 256GB/s shared between all of the components. This is slightly lower than the 288GB/s available to the RX 7600 XT but is much higher than any other APU we have tested.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;A massive thank you to both Asus and HP for sending over a ROG Flow Z13 (2025) and a ZBook Ultra G1a 14‚Äù for testing which were both equipped with an AMD Ryzen AI Max+ 395. All of the gaming tests were done on the Flow Z13 due to that being a more gaming focused device and all of the microbenchmarking was done on the ZBook Ultra G1a.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Subsystem from the CPU‚Äôs Perspective&lt;/head&gt;
    &lt;p&gt;Starting with the memory latency from Zen 5‚Äôs perspective, we see that the latency difference between Strix Point and Strix Halo is negligible with Strix Point at ~128ns of memory latency and Strix Halo at ~123ns of memory latency. However, as you can see the CPU does not have access to the 32MB of Infinity Cache on the IO die. This behavior was confirmed by Mahesh Subramony during our interview about Strix Halo at CES 2025.&lt;/p&gt;
    &lt;p&gt;While the 123ns DRAM latency seen here is quite good for a mobile part, desktop processors like our 9950X fare much better at 75-80ns.&lt;/p&gt;
    &lt;p&gt;Moving on to memory bandwidth, we see Strix Halo fall into a category of its own of the SoCs we have tested.&lt;/p&gt;
    &lt;p&gt;When doing read-modify-add operations across both CCDs, the 16 Zen 5 cores can pull over 175GB/s of bandwidth from the memory with reads being no slouch at 124GB/s across both CCDs.&lt;/p&gt;
    &lt;p&gt;However, looking at the bandwidth of a single CCD and just like the desktop CPUs a single Strix-Halo CCD only has a 32 byte per cycle read link to the IO die. And just like the desktop chips, the chip to chip link runs at ~2000MHz, which caps out the single CCD read at 64GB/s. Unlike the desktop chips, the write link is 32 bytes per cycle and we are seeing about 43GB/s for the write bandwidth. That brings the total theoretical single CCD bandwidth to 128GB/s and the observed bandwidth is just over 103GB/s.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU‚Äôs Performance&lt;/head&gt;
    &lt;p&gt;The performance of Strix Halo‚Äôs CPU packs quite a bit more of a punch than Strix Point‚Äôs CPU.&lt;/p&gt;
    &lt;p&gt;Strix Halo‚Äôs CPU can match a last generation desktop flagship CPU, the 7950X, in Integer performance despite a 11.7% clock speed delta. And nearly matches AMD current desktop flagship CPU, the 9950X, in Floating Point performance again with a 11.7% clock speed deficit.&lt;/p&gt;
    &lt;p&gt;Looking at the SPEC CPU 2017 Integer subtests and while Strix Halo can‚Äôt quite match the desktop 9950X, likely due to the higher memory latency of Strix Halo‚Äôs LPDDR5X bus, it does get close in a number of subtests.&lt;/p&gt;
    &lt;p&gt;Moving to the FP subtests and the story is similar to the Integer subtests but Strix Halo can get even closer to the 9950X and even beat it in the fotonik3d subtest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory from the GPU‚Äôs Perspective&lt;/head&gt;
    &lt;p&gt;Moving to the GPU side of things and this is where Strix Halo really shines. The laptop we used as a comparison to Strix Halo was the HP Omen Transcend 14 2025 with a 5070M equipped which maxed out at about 75 Watts for the GPU.&lt;/p&gt;
    &lt;p&gt;Strix Halo has over double the memory bandwidth of any of the other mobile SoCs that we have tested. However, the RTX 5070 Mobile does have about 50% more memory bandwidth compared to Strix Halo.&lt;/p&gt;
    &lt;p&gt;Looking at the caches of Strix Halo, the Infinity Cache, AKA MALL, is able to deliver over 40% higher bandwidth compared to the 5070M‚Äôs L2 while having 33% more capacity. Plus Strix Halo has a 4MB L2 which is capable of providing 2.5TB/s of bandwidth to the GPU.&lt;/p&gt;
    &lt;p&gt;Moving to the latency, the more complex cache layout of Strix Halo does give it a latency advantage after the 128KB with Strix Halo‚Äôs L2 being significantly lower latency than the 5070M‚Äôs L2 and the larger 32MB MALL that Strix Halo has a similar latency to the 5070M‚Äôs L2. And Strix Halo‚Äôs memory latency is about 35% lower than the 5070M‚Äôs memory latency.&lt;/p&gt;
    &lt;head rend="h2"&gt;The GPU‚Äôs Compute Throughput&lt;/head&gt;
    &lt;p&gt;Looking at the floating point throughput, we see that Strix Halo unsurprisingly has about 2.5x the throughput of Strix Point considering it has about two and a half times the number of Compute Units. Strix Halo oftentimes can match or even pull ahead of the 5070 Mobile in terms of throughput. I will note that the FP16 results for the 5070 Mobile are half of what I would expect; the FP16:FP32 ratio for the 5070 Mobile should be 1:1 so I am not positive about what is going on there.&lt;/p&gt;
    &lt;p&gt;Moving to the integer throughput and we see the 5070 Mobile soundly pulling ahead of the Radeon 8060S.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPU Performance&lt;/head&gt;
    &lt;p&gt;Looking at the GPU performance, we see Strix Halo once again shine, with a staggering level of performance available for an iGPU, courtesy of the large CU count paired with relatively high memory bandwidth. Our comparison suite includes several recent iGPU‚Äôs from Intel/AMD, along with the newest generation RTX 5070 Mobile @ 75W to act as a reference for mid to high-range laptop dedicated graphics, and the antiquated GTX 1050 as a reference for budget dedicated graphics.&lt;/p&gt;
    &lt;p&gt;Looking at Fluid X3D for a compute-heavy workload, we can see the Radeon 8060S absolutely obliterates the other iGPU‚Äôs from Intel/AMD, putting itself firmly in a class above. The 5070 is no slouch though, and still holds a substantial 64.1% lead largely due to the higher memory bandwidth afforded to the 5070M.&lt;/p&gt;
    &lt;p&gt;Switching to gaming workloads with Cyberpunk 2077, we start with a benchmark conducted while on battery power. The gap with other iGPU‚Äôs is still wide, but now the 5070M is limited to 55W and exhibits 7.5% worse performance at 1080p low settings when compared to the Radeon 8060S.&lt;/p&gt;
    &lt;p&gt;Finally, moving to wall power and allowing both the Radeon 8060S and 5070M to access the full power limit in CP2077, we can see that the 8060S still pulls ahead at 1080p low by 2.5%, while at 1440p medium we see a reversal, with the 5070M commanding an 8.3% lead. Overall the two provide a comparable experience in Cyberpunk 2077, with changes in settings or power limits adjusting the lead between the two. This is a seriously impressive turnaround for an iGPU working against dedicated graphics, and demonstrates the versatility of the chip in workloads like gaming, where iGPU‚Äôs have traditionally struggled.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Strix Halo follows in the footsteps of many other companies in the goal of designing an SoC for desktop and laptop usage that is truly all encompassing. The CPU and GPU performance is truly a class above standard low power laptop chips, and is even able to compete with larger systems boasting dedicated graphics. CPU performance is especially impressive with a comparable showing to the desktop Zen 5 CPUs. GPU performance is comparable to mid range dedicated graphics, while still offering the efficiency and integration of an iGPU. High end dedicated graphics still have a place above Strix Halo, but the versatility of this design for smaller form factor devices is class leading.&lt;/p&gt;
    &lt;p&gt;However, this is not to say that Strix Halo is perfect. I was hoping to have a section dedicated to the ML performance of Strix Halo in this article, however AMD only just released preview support for Strix Halo in the ROCm 7.0.2 release which came out about a week ago from time of publication. As a result of the long delay between the launch of Strix Halo and the release of ROCm 7.0.2, the ML performance will have to wait until a future article.&lt;/p&gt;
    &lt;p&gt;However, putting aside ROCm, Strix Halo is a very, very cool piece of technology and I would love to see successors to Strix Halo with newer CPU and GPU IP and possibly even larger memory buses similar to Apple‚Äôs Max and Ultra series of SoCs with 512b and 1024b memory respectively. AMD has a formula for building bigger APUs with Strix Halo, which opens the door to a lot of interesting hardware possibilities in the future.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://chipsandcheese.com/p/amds-chiplet-apu-an-overview-of-strix"/><published>2025-10-18T04:26:19+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45625229</id><title>Chen-Ning Yang, Nobel laureate, dies at 103</title><updated>2025-10-18T20:35:10.757396+00:00</updated><content>&lt;doc fingerprint="73451408afd08a0a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China's first Nobel laureate, Yang Chen-Ning, dies, aged 103&lt;/head&gt;
    &lt;p&gt;Yang Chen-Ning, a world-renowned physicist and Nobel laureate, passed away in Beijing on Saturday at 103.&lt;/p&gt;
    &lt;p&gt;Yang, an academician of the Chinese Academy of Sciences, professor at Tsinghua University, and the honorary president of the Institute for Advanced Study at Tsinghua, died after an illness, the university said in an obituary, calling the late professor "immortal".&lt;/p&gt;
    &lt;p&gt;Together with his colleague Tsung-dao Lee, Yang was awarded the Nobel Prize in Physics in 1957 for their theory of parity non-conservation in weak interaction.&lt;/p&gt;
    &lt;p&gt;He was often ranked alongside Albert Einstein as one of the 20th century's greatest physicists.&lt;/p&gt;
    &lt;p&gt;Born in Hefei, Anhui province, in 1922, Yang moved with his family to Tsinghua in 1929. He enrolled at the National Southwestern Associated University in 1938 and later entered the graduate school of Tsinghua University in 1942, earning a master's degree in science in 1944. In 1945, he went to the United States for further studies as a Tsinghua University government-sponsored student, attending the University of Chicago, where he received his PhD in 1948 and remained for postdoctoral work.&lt;/p&gt;
    &lt;p&gt;He joined the Institute for Advanced Study in Princeton in 1949, becoming a permanent member in 1952 and a professor in 1955. In 1966, he was appointed as the Albert Einstein Professor of Physics at the State University of New York at Stony Brook, working there until 1999.&lt;/p&gt;
    &lt;p&gt;Since 1986, he had been a visiting professor at the Chinese University of Hong Kong. From 1997, he served as the honorary director of the newly established Center for Advanced Study ‚Äî now the Institute for Advanced Study ‚Äî at Tsinghua University and became a Tsinghua professor in 1999.&lt;/p&gt;
    &lt;p&gt;Yang, having made seminal contributions to modern physics, is recognized as one of the most eminent physicists of the 20th century. His work with Robert Mills on the "Yang-Mills theory" laid the foundation for the Standard Model of particle physics and is regarded as one of the cornerstones of modern physics, comparable in significance to Maxwell's equations and Einstein's theory of general relativity.&lt;/p&gt;
    &lt;p&gt;In collaboration with Tsung-Dao Lee, he proposed the non-conservation of parity in weak interactions, a revolutionary idea for which they were jointly awarded the Nobel Prize in Physics in 1957, becoming the first Chinese Nobel laureate.&lt;/p&gt;
    &lt;p&gt;Yang was a foreign member of more than ten academies of sciences worldwide and received honorary doctoral degrees from over twenty renowned universities.&lt;/p&gt;
    &lt;p&gt;Yang maintained a deep affinity for his homeland and made outstanding contributions to China's scientific and educational development. His first visit to the People's Republic of China in 1971 helped initiate a wave of visits by overseas Chinese scholars, earning him recognition as a pioneer in building academic bridges between China and the United States.&lt;/p&gt;
    &lt;p&gt;He later proposed the restoration and strengthening of basic scientific research to China's central leadership and personally raised funds to establish a committee for educational exchange with China ‚Äî sponsoring nearly a hundred Chinese scholars for further studies in the US. Many of those scholars later became key figures in China's scientific and technological advancement. Yang played a significant role in promoting domestic scientific exchange and progress, offering crucial advice on major national scientific projects and policies.&lt;/p&gt;
    &lt;p&gt;Upon his return to Tsinghua, he dedicated himself to the development of the Institute for Advanced Study, investing immense effort into the growth of basic disciplines like physics and the cultivation of talent at Tsinghua, significantly impacting the reform and development of China's higher education.&lt;/p&gt;
    &lt;p&gt;The life of Professor Yang was that of an immortal legend ‚Äî exploring the unknown with a timeless echo of a heart devoted to his nation, the obituary said.&lt;/p&gt;
    &lt;p&gt;Yang's century-long journey constitutes an eternal chapter shining among the stars of humanity, it said.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Musical medley: Super Bund Music Festival kicks off in Shanghai&lt;/item&gt;
      &lt;item&gt;Physicist Chen-Ning Yang dies at 103&lt;/item&gt;
      &lt;item&gt;China improves regulations on personal information outbound transfer&lt;/item&gt;
      &lt;item&gt;Generative AI users in China reach 515m&lt;/item&gt;
      &lt;item&gt;China Focus: China achieves numerous breakthroughs in space exploration quest&lt;/item&gt;
      &lt;item&gt;Rediscovering the magnetism of Yan'an for China's youth&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.chinadaily.com.cn/a/202510/18/WS68f3170ea310f735438b5bf2.html"/><published>2025-10-18T05:47:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626037</id><title>Fast calculation of the distance to cubic Bezier curves on the GPU</title><updated>2025-10-18T20:35:09.978947+00:00</updated><content>&lt;doc fingerprint="7ebb8b95b86f333a"&gt;
  &lt;main&gt;
    &lt;p&gt;B√©zier curves are a core building block of text and 2D shapes rendering. There are several approaches to rendering them, but one especially challenging problem, both mathematically and technically, is computing the distance to a B√©zier curve. For quadratic curves (one control point), this is fairly accessible, but for cubic (two control points) we're going to see why it is so hard.&lt;/p&gt;
    &lt;p&gt;Having this distance field opens up many rendering possibilities. It's hard, but it's possible; here is a live proof:&lt;/p&gt;
    &lt;p&gt;In this visualization, I'm borrowing your device resources to compute the distance to the curve for every single pixel. The yellow points are the control points of the curve (in white) and the blue zone is a representation of the distance field.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;All the demos and code in this article are self-contained GLSL fragment shaders. Most of the code can be found in the article, but feel free to inspect the source code of any of these WebGL demo for the complete code. They can be run verbatim using ShaderWorkshop.&lt;/p&gt;
    &lt;head rend="h2"&gt;The basic maths&lt;/head&gt;
    &lt;p&gt;In a previous article, we explained that a B√©zier curve can be expressed as a polynomial. In our case, a cubic polynomial:&lt;/p&gt;
    &lt;p&gt;Where a, b, c and d are the vector coefficients derived from the start (P_0), end (P_3), and control points (P_1, P_2) using the following formulas (you can refer to the previous article for details):&lt;/p&gt;
    &lt;p&gt;For a given point p in 2D space, the distance to that B√©zier curve can be expressed as a length between our curve and p:&lt;/p&gt;
    &lt;p&gt;Our goal is to find the t value where d(t) is the smallest.&lt;/p&gt;
    &lt;p&gt;The length formula has an annoying square root, so we start with the distance squared for simplicity, which we are going to unroll:&lt;/p&gt;
    &lt;p&gt;The derivative of that function will allow us to identify critical points: that is, points where the distance starts growing or reducing. Said differently, solving D'(t)=0 will identify all the maximums and minimums (we're interested in the latter) of D(t) (and thus d(t) as well).&lt;/p&gt;
    &lt;p&gt;It is a bit convoluted in our case but straightforward to compute:&lt;/p&gt;
    &lt;p&gt;A polynomial, this time of degree 5, emerges here. For conciseness, we can express D'(t) polynomial coefficients as a bunch of dot products:&lt;/p&gt;
    &lt;p&gt;Finally, we notice that solving D'(t)=0 is equivalent to solving D'(t)/2 = 0, so we simplify the expression:&lt;/p&gt;
    &lt;p&gt;Assuming we are able to solve this equation, we will get at most 5 values of t, among which we should find the shortest distance from p to the curve. Since t is bound within 0 and 1 (start and end of the curve), we will also have to test the distance at these locations.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We could also compute the 2nd derivative in order to differentiate minimums from maximums, but simply evaluating the 5(+2) potential t values and keeping the smallest works just fine.&lt;/p&gt;
    &lt;p&gt;The red dot in the blue field is a random point in space. The red lines show which distances are evaluated (at most 5+2) to find the smallest one.&lt;/p&gt;
    &lt;head rend="h3"&gt;Translated to GLSL code&lt;/head&gt;
    &lt;p&gt;Transposing these formulas into code gives us this base template code:&lt;/p&gt;
    &lt;code&gt;float bezier_distance(vec2 p, vec2 p0, vec2 p1, vec2 p2, vec2 p3) {
    // Start by testing the distance to the boundary points at t=0 (p0) and t=1 (p3)
    vec2 dp0 = p0 - p,
         dp3 = p3 - p;
    float dist = min(dot(dp0, dp0), dot(dp3, dp3));

    // Bezier cubic points to polynomial coefficients
    vec2 a = -p0 + 3.0*(p1 - p2) + p3,
         b = 3.0 * (p0 - 2.0*p1 + p2),
         c = 3.0 * (p1 - p0),
         d = p0;

    // Solve D'(t)=0 where D(t) is the distance squared
    vec2 dmp = d - p;
    float da = 3.0 * dot(a, a),
          db = 5.0 * dot(a, b),
          dc = 4.0 * dot(a, c) + 2.0 * dot(b, b),
          dd = 3.0 * (dot(a, dmp) + dot(b, c)),
          de = 2.0 * dot(b, dmp) + dot(c, c),
          df = dot(c, dmp);

    float roots[5];
    int count = root_find5(roots, da, db, dc, dd, de, df);
    for (int i = 0; i &amp;lt; count; i++) {
        float t = roots[i];
        // Evaluate the distance to our point p and keep the smallest
        vec2 dp = ((a * t + b) * t + c) * t + dmp;
        dist = min(dist, dot(dp, dp));
    }

    // We've been working with the squared distance so far, it's time to get its
    // square root
    return sqrt(dist);
}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dot(dp,dp)&lt;/code&gt; is a shorthand for the length squared, of course cheaper than
computing &lt;code&gt;length()&lt;/code&gt; which contains a square root.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;We assume here the root finder only returns the roots that are within [0,1].&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;root_find5()&lt;/code&gt; is our 5th degree root finder, that is the function that gives us
all the t (at most 5) which satisfy:&lt;/p&gt;
    &lt;p&gt;But before we are able to solve that, we need to study the simpler 2nd degree polynomial solving:&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quadratic polynomial equations&lt;/head&gt;
    &lt;p&gt;Diving into the rabbit hole of solving polynomial numerically will lead you to insanity. But we still have to scratch the surface because superior degree solvers usually rely on it.&lt;/p&gt;
    &lt;p&gt;My favorite quadratic root finding formula is the super simple one introduced by 3Blue1Brown, which involves locating a mid point m from which you get the 2 surrounding roots r:&lt;/p&gt;
    &lt;p&gt;In GLSL, a code to cover most common corner cases would look like this:&lt;/p&gt;
    &lt;code&gt;// Return true if x is not a NaN nor an infinite
// highp is probably mandatory to force IEEE 754 compliance
bool isfinite(highp float x) { return (floatBitsToUint(x) &amp;amp; 0x7f800000u) != 0x7f800000u; }

// Quadratic: solve ax¬≤+bx+c=0
int root_find2(out float r[5], float a, float b, float c) {
    int count = 0;
    float m = -b / (2.*a);
    float d = m*m - c/a;
    if (!isfinite(m) || !isfinite(d)) { // a is (probably) too small
        // Linear: solve bx+c=0
        float s = -c / b;
        if (isfinite(s))
            r[count++] = s;
        return count;
    }
    if (d &amp;lt; 0.) // no root
        return count;
    if (d == 0.) {
        r[count++] = m; // single root
        return count;
    }
    float z = sqrt(d);
    r[count++] = m - z;
    r[count++] = m + z;
    return count;
}
&lt;/code&gt;
    &lt;p&gt;Not quite as straightforward as the math formula, isn't it?&lt;/p&gt;
    &lt;p&gt;We cannot know in advance whether the division is going to succeed, so we do run divisions and only then check if they failed (and assume a reason for the failing). This is much more reliable than an arbitrary epsilon value. We also try to avoid duplicated roots.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The roots are automatically sorted because z is always positive.&lt;/p&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;isfinite()&lt;/code&gt; may not be as reliable because in GLSL "NaNs are not required
to be generated", meaning some edge case may not be supported depending on
the hardware, drivers, and the current weather in Yokohama.&lt;/p&gt;
    &lt;p&gt;As much as I like it, this implementation might not be the most stable numerically (even though I don't have have strong data to back this claim). Instead, we may prefer the formula from Numerical Recipes:&lt;/p&gt;
    &lt;p&gt;Leading to the following alternative implementation:&lt;/p&gt;
    &lt;code&gt;int root_find2(out float r[5], float a, float b, float c) {
    int count = 0;
    float d = b*b - 4.*a*c;
    if (d &amp;lt; 0.)
        return count;
    if (d == 0.) {
        float s = -.5 * b / a;
        if (isfinite(s))
            r[count++] = s;
        return count;
    }
    float h = sqrt(d);
    float q = -.5 * (b + (b &amp;gt; 0. ? h : -h));
    float r0 = q/a, r1 = c/q;
    if (isfinite(r0)) r[count++] = r0;
    if (isfinite(r1)) r[count++] = r1;
    return count;
}
&lt;/code&gt;
    &lt;p&gt;This is not perfect at all (especially with the b¬≤-4ac part). There are actually many other possible implementations, and this HAL CNRS paper shows how near impossible it is to make a correct one. It is an interesting but depressing read, especially since it "only" covers IEEE 754 floats, and we have no such guarantee on GPUs. We also don't have &lt;code&gt;fma()&lt;/code&gt; in
WebGL, which greatly limits improvements. For now, it will have to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quintic polynomial equations: attempt 1&lt;/head&gt;
    &lt;p&gt;Solving polynomials of degree 5 cannot be solved analytically like quadratics. And even if they were, we probably wouldn't do it because of numerical instability. Typically, in my experience, analytical 3rd degree polynomials solver do not provide reliable results.&lt;/p&gt;
    &lt;p&gt;The first iterative algorithm I picked was the Aberth‚ÄìEhrlich method. Nowadays, more appropriate algorithms exist, but at the time I started messing up with these problems (several years ago), it was a fairly good contender. This video explores how it works.&lt;/p&gt;
    &lt;p&gt;The convergence to the roots is quick, and it's overall simple to implement. But it's not without flaws. The main problem is that it works in complex space. We can't ignore the complex roots because they all "respond" to each others. And filtering these roots out at the end implies some unreliable arbitrary threshold mechanism (we keep the root only when the imaginary part is close to 0).&lt;/p&gt;
    &lt;p&gt;The initialization process also annoyingly requires you to come up with a guess at what the roots are, and doesn't provide anything relevant. Aberth-Ehrlich works by refining these initial roots, similar to a more elaborate Newton iterations algorithm. Choosing better initial estimates leads to a faster convergence (meaning less iterations).&lt;/p&gt;
    &lt;p&gt;The Cauchy bound specifies a space by defining the radius of a disk (complex numbers are in 2D space) where all the roots of a polynomial should lie within. We are going to use it for the initial guess, and more specifically its "tight" version (which unfortunately relies on &lt;code&gt;pow()&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Since Aberth-Ehrlich is a refinement and not just a shrinking process, we define and use an inner disk that has half the area of Cauchy bound disk. That way, we're more likely to start with initial guesses spread in the "middle" of the roots; this is where the ‚àö2 comes from in the formula below.&lt;/p&gt;
    &lt;code&gt;#define K5_0 vec2( 0.951056516295154,  0.309016994374947)
#define K5_1 vec2( 0.000000000000000,  1.000000000000000)
#define K5_2 vec2(-0.951056516295154,  0.309016994374948)
#define K5_3 vec2(-0.587785252292473, -0.809016994374947)
#define K5_4 vec2( 0.587785252292473, -0.809016994374948)

int root_find5_aberth(out float roots[5], float a, float b, float c, float d, float e, float f) {
    // Initial candidates set mid-way of the tight Cauchy bound estimate
    float r = (1.0 + max_5(
        pow(abs(b/a), 1.0/5.0),
        pow(abs(c/a), 1.0/4.0),
        pow(abs(d/a), 1.0/3.0),
        pow(abs(e/a), 1.0/2.0),
            abs(f/a))) / sqrt(2.0);

    // Spread in a circle
    vec2 r0 = r * K5_0,
         r1 = r * K5_1,
         r2 = r * K5_2,
         r3 = r * K5_3,
         r4 = r * K5_4;
&lt;/code&gt;
    &lt;p&gt;The circle constants are generated with the following script:&lt;/p&gt;
    &lt;code&gt;import math
import sys

n = int(sys.argv[1])
for k in range(n):
    angle = 2 * math.pi / n
    off = math.pi / (2 * n)
    z = angle * k + off
    c, s = math.cos(z), math.sin(z)
    print(f"#define K{n}_{k} vec2({c:18.15f}, {s:18.15f})")
&lt;/code&gt;
    &lt;p&gt;Next, it's basically a simple iterative process. Unrolling everything for degree 5 looks like this:&lt;/p&gt;
    &lt;code&gt;#define close_to_zero(x) (abs(x) &amp;lt; eps)

// This also filters out roots out of the [0,1] range
#define ADD_ROOT_IF_REAL(r) if (close_to_zero(r.y) &amp;amp;&amp;amp; r.x &amp;gt;= 0. &amp;amp;&amp;amp; r.x &amp;lt;= 1.) roots[count++] = r.x

#define SMALL_OFF(off) (dot(off, off) &amp;lt;= eps*eps)

/* Complex multiply, divide, inverse */
vec2 c_mul(vec2 a, vec2 b) { return mat2(a, -a.y, a.x) * b; }
vec2 c_div(vec2 a, vec2 b) { return mat2(a, a.y, -a.x) * b / dot(b, b); }
vec2 c_inv(vec2 z)         { return vec2(z.x, -z.y) / dot(z, z); }

// Compute f(x)/f'(x): complex polynomial evaluation (y) divided by their
// derivatives (q) using Horner's method in one pass
vec2 c_poly5d4(float a, float b, float c, float d, float e, float f, vec2 x) {
    vec2 y =       a*x  + vec2(b, 0), q =       a*x  + y;
         y = c_mul(y,x) + vec2(c, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(d, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(e, 0); q = c_mul(q,x) + y;
         y = c_mul(y,x) + vec2(f, 0);
    return c_div(y, q);
}

vec2 sum_of_inv(vec2 z0, vec2 z1, vec2 z2, vec2 z3, vec2 z4) { return c_inv(z0 - z1) + c_inv(z0 - z2) + c_inv(z0 - z3) + c_inv(z0 - z4); }

int root_find5_aberth(out float roots[5], float a, float b, float c, float d, float e, float f) {
    if (close_to_zero(a))
        return root_find4_aberth(roots, b, c, d, e, f);

    // Code snip: see previous snippet
    // float r = ...
    // vec2 r0, r1, r2, ... 

    for (int m = 0; m &amp;lt; 16; m++) {
        vec2 d0 = c_poly5d4(a, b, c, d, e, f, r0),
             d1 = c_poly5d4(a, b, c, d, e, f, r1),
             d2 = c_poly5d4(a, b, c, d, e, f, r2),
             d3 = c_poly5d4(a, b, c, d, e, f, r3),
             d4 = c_poly5d4(a, b, c, d, e, f, r4);

        vec2 off0 = c_div(d0, vec2(1,0) - c_mul(d0, sum_of_inv(r0, r1, r2, r3, r4))),
             off1 = c_div(d1, vec2(1,0) - c_mul(d1, sum_of_inv(r1, r0, r2, r3, r4))),
             off2 = c_div(d2, vec2(1,0) - c_mul(d2, sum_of_inv(r2, r0, r1, r3, r4))),
             off3 = c_div(d3, vec2(1,0) - c_mul(d3, sum_of_inv(r3, r0, r1, r2, r4))),
             off4 = c_div(d4, vec2(1,0) - c_mul(d4, sum_of_inv(r4, r0, r1, r2, r3)));

        r0 -= off0;
        r1 -= off1;
        r2 -= off2;
        r3 -= off3;
        r4 -= off4;

        if (SMALL_OFF(off0) &amp;amp;&amp;amp; SMALL_OFF(off1) &amp;amp;&amp;amp; SMALL_OFF(off2) &amp;amp;&amp;amp; SMALL_OFF(off3) &amp;amp;&amp;amp; SMALL_OFF(off4))
            break;
    }

    int count = 0;
    ADD_ROOT_IF_REAL(r0);
    ADD_ROOT_IF_REAL(r1);
    ADD_ROOT_IF_REAL(r2);
    ADD_ROOT_IF_REAL(r3);
    ADD_ROOT_IF_REAL(r4);
    return count;
}
&lt;/code&gt;
    &lt;p&gt;When the main coefficient is too small, we fall back on the 4th degree (and so on until we reach the analytic quadratic). The 4th and 3rd degree versions of this function are easy to guess (they're pretty much identical, just removing one coefficient at each degree).&lt;/p&gt;
    &lt;p&gt;We're also hardcoding a maximum of 16 iterations here because it's usually enough. To have an idea of how many iterations are required in practice, following is a visualization of the heat map of the number of iterations for every pixel:&lt;/p&gt;
    &lt;p&gt;The big picture and the weaknesses of the algorithm should be pretty obvious by now. Among all drawbacks of this approach, there are also surprising pathological cases where the algorithm is not performing well. Fortunately, there were some progress on the state of the art in recent years.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving quintic polynomial equations: the state of the art&lt;/head&gt;
    &lt;p&gt;In 2022, Cem Yuksel published a new algorithm for polynomial root solving. Initially I had my reservations because the official implementation had a few shortcomings on some edge cases, which made me question its reliability. It's also optimized for CPU computation and is, to my very personal taste, overly complex.&lt;/p&gt;
    &lt;p&gt;Fortunately, Christoph Peters showed that it was possible on the GPU by implementing it for very large degrees, and without any recursion. Inspired by that, I decided to unroll it myself for degree 5.&lt;/p&gt;
    &lt;p&gt;One core difference with Aberth approach is that it is designed for arbitrary ranges. In our case this is actually convenient because, due to how B√©zier curves are defined, we are only interested in roots between 0 and 1. We will need to adjust the Quadratic function to work in this range, as well as keeping the roots ordered:&lt;/p&gt;
    &lt;code&gt;     }
     float h = sqrt(d);
     float q = -.5 * (b + (b &amp;gt; 0. ? h : -h));
-    float r0 = q/a, r1 = c/q;
-    if (isfinite(r0)) r[count++] = r0;
-    if (isfinite(r1)) r[count++] = r1;
+    vec2 v = vec2(q/a, c/q);
+    if (v.x &amp;gt; v.y) v.xy = v.yx; // keep them ordered
+    if (isfinite(v.x) &amp;amp;&amp;amp; v.x &amp;gt;= 0. &amp;amp;&amp;amp; v.x &amp;lt;= 1.) r[count++] = v.x;
+    if (isfinite(v.y) &amp;amp;&amp;amp; v.y &amp;gt;= 0. &amp;amp;&amp;amp; v.y &amp;lt;= 1.) r[count++] = v.y;
     return r;
 }
&lt;/code&gt;
    &lt;p&gt;The core logic of the algorithm relies on a cascade of derivatives for every degree. Christoph Peters provides an analytic formula to obtain the derivative for any degree. This is a huge helper when we need to work for an arbitrary degree, but in our case we can just differentiate manually:&lt;/p&gt;
    &lt;p&gt;Since we're only interested in the roots, similar to what we did to D(t), we can simplify some of these expressions:&lt;/p&gt;
    &lt;p&gt;The purpose of that cascade of derivatives is to cut the curve into monotonic segments. In practice, the core function looks like this:&lt;/p&gt;
    &lt;code&gt;int root_find5_cy(out float r[5], float a, float b, float c, float d, float e, float f) {
    float r2[5], r3[5], r4[5];
    int n = root_find2(r2,          10.*a, 4.*b,    c);            // degree 2
    n = cy_find5(r3, r2, n, 0. ,0., 10.*a, 6.*b, 3.*c,   d);       // degree 3
    n = cy_find5(r4, r3, n,     0.,  5.*a, 4.*b, 3.*c, d+d, e);    // degree 4
    n = cy_find5(r,  r4, n,             a,    b,    c,   d, e, f); // degree 5
    reutnr n;
}
&lt;/code&gt;
    &lt;p&gt;We could unroll &lt;code&gt;cy_find3&lt;/code&gt;, &lt;code&gt;cy_find4&lt;/code&gt;, and &lt;code&gt;cy_find5&lt;/code&gt;, but to keep the
code simple, the degree 3 to 5 will share the same function, with leading
coefficients set to 0 (hopefully the compiler does its job properly).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;cy_find5&lt;/code&gt; relies on roots found (at most 4) at previous stages to define
intervals of search:&lt;/p&gt;
    &lt;p&gt;Such an approach has the nice side effect of keeping the roots ordered.&lt;/p&gt;
    &lt;p&gt;The solver itself is not that complex either:&lt;/p&gt;
    &lt;code&gt;float poly5(float a, float b, float c, float d, float e, float f, float t) {
     return ((((a * t + b) * t + c) * t + d) * t + e) * t + f;
}

// Quintic: solve ax‚Åµ+bx‚Å¥+cx¬≥+dx¬≤+ex+f=0
iint cy_find5(out float r[5], float r4[5], int n, float a, float b, float c, float d, float e, float f) {
    int count = 0;
    vec2 p = vec2(0, poly5(a,b,c,d,e,f, 0.));
    for (int i = 0; i &amp;lt;= n; i++) {
        float x = i == n ? 1. : r4[i],
              y = poly5(a,b,c,d,e,f, x);
        if (p.y * y &amp;gt; 0.)
            continue;
        float v = bisect5(a,b,c,d,e,f, vec2(p.x,x), vec2(p.y,y));
        r[count++] = v;
        p = vec2(x, y);
    }
    return count;
}
&lt;/code&gt;
    &lt;p&gt;The last brick of the algorithm is the Newton bisection, the slowest part:&lt;/p&gt;
    &lt;code&gt;// Newton bisection
//
// a,b,c,d,e,f: 5th degree polynomial parameters
// t: x-axis boundaries
// v: respectively f(t.x) and f(t.y)
float bisect5(float a, float b, float c, float d, float e, float f, vec2 t, vec2 v) {
    float x = (t.x+t.y) * .5; // mid point
    float s = v.x &amp;lt; v.y ? 1. : -1.; // sign flip
    for (int i = 0; i &amp;lt; 32; i++) {
        // Evaluate polynomial (y) and its derivative (q) using Horner's method in one pass
        float y = a*x + b, q = a*x + y;
              y = y*x + c; q = q*x + y;
              y = y*x + d; q = q*x + y;
              y = y*x + e; q = q*x + y;
              y = y*x + f;

        t = s*y &amp;lt; 0. ? vec2(x, t.y) : vec2(t.x, x);
        float next = x - y/q; // Newton iteration
        next = next &amp;gt;= t.x &amp;amp;&amp;amp; next &amp;lt;= t.y ? next : (t.x+t.y) * .5;
        if (abs(next - x) &amp;lt; eps)
            return next;
        x = next;
    }
    return x;
}
&lt;/code&gt;
    &lt;p&gt;And that's pretty much it. Looking at its heat map, it has a completely different look than Aberth:&lt;/p&gt;
    &lt;p&gt;The number of iterations might be larger but it is much faster (I observed a factor 3 on my machine), the code is shorter, and actually more reliable.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The scale used to represent the heat map is not the same as the one used in Aberth, but it is identical with the method presented in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploring ITP convergence&lt;/head&gt;
    &lt;p&gt;The bisection being the hot loop, it is interesting to ponder how to make this faster. A while back, Raph Levien hypothesized about how the ITP method could perform. Out of curiosity, I gave it a chance. The function is designed to work like a bisection, claiming to be as performant in the worst case.&lt;/p&gt;
    &lt;p&gt;There isn't a lot of code, and the paper provides a pseudo-code. But implementing it was actually challenging in many ways.&lt;/p&gt;
    &lt;p&gt;First of all, the authors didn't seem to find relevant to mention that it only works if f(a)&amp;lt;0&amp;lt;f(b). If f(a)&amp;gt;0&amp;gt;f(b), you're pretty much on your own. It requires just 2 lines of adjustments but figuring out this shortcoming of the algorithm was particularly unexpected.&lt;/p&gt;
    &lt;p&gt;Another bothering aspect concerns the parameters: K_1, K_2 and n_0. The paper proposes those:&lt;/p&gt;
    &lt;p&gt;I played with them for a while and couldn't find any set that would make a real difference, so I ended up with the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For performance reasons, reducing K_2 to a value of 2 saves a call to &lt;code&gt;pow()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;For K_1, CRAN seems to suggest \frac{0.2}{b-a} so I went along with it&lt;/item&gt;
      &lt;item&gt;And for n_0, well 1 or 2 seem to be the usual parameter.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the end, the function looks like this:&lt;/p&gt;
    &lt;code&gt;// ITP algorithm (2020) by Oliveira &amp;amp; Takahashi
// "An Enhancement of the Bisection Method Average Performance Preserving Minmax Optimality"
//
// a,b,c,d,e,f: 5th degree polynomial parameters
// t: x-axis boundaries (a and b in the paper)
// v: respectively f(a) and f(b) in the paper (evaluation of the function with t.x and t.y)
float itp5(float a, float b, float c, float d, float e, float f, vec2 t, vec2 v) {
    float diff = t.y-t.x;

    // K1 and n0 suggested by CRAN
    float K1 = .2 / diff;
    int n0 = 1;

    // The paper has the assumption that f(a)&amp;lt;0&amp;lt;f(b) but we want to
    // support f(a)&amp;gt;0&amp;gt;f(b) too, so we keep a sign flip
    float s = v.x &amp;lt; v.y ? 1. : -1.;

    // Using log(ab)=log(a)+log(b): log2(x/(2Œµ)) &amp;lt;=&amp;gt; log2(x/Œµ)-1
    int nh = int(ceil(log2(diff/eps)-1.)); // n_{1/2} (half point)
    int n_max = nh + n0;

    // Œµ 2^(n_max-k) = Œµ 2^n_max 2^-k = Œµ 2^n_max ¬Ω^k
    // ¬Ω^k is done iteratively in the loop, simplifying the arithmetic
    float q = eps * float(1&amp;lt;&amp;lt;n_max);

    while (diff &amp;gt; eps+eps) {
        // Interpolation
        float xf = (v.y*t.x - v.x*t.y) / (v.y-v.x); // Regula-Falsi

        // Truncation
        float xh = (t.x+t.y) * .5; // x half point
        float sigma = sign(xh - xf);
        float delta = K1*diff*diff; // save a pow() by forcing K2=2
        float xt = delta &amp;lt;= abs(xh - xf) ? xf + sigma*delta : xh; // xt: truncation of xf

        // Projection
        float r = q - diff*.5;
        float x = abs(xt-xh) &amp;lt;= r ? xt : xh-sigma*r;

        // Updating
        float y = poly5(a,b,c,d,e,f, x);
        float side = s*y;
        if      (side &amp;gt; 0.) t.y=x, v.y=y;
        else if (side &amp;lt; 0.) t.x=x, v.x=y;
        else                return x;

        diff = t.y-t.x;
        q *= .5;
    }
    return (t.x+t.y) * .5;
}
&lt;/code&gt;
    &lt;p&gt;This function can be used as a drop'in replacement for &lt;code&gt;bisect5&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I had a lot of expectations about it, but in the end it requires more iterations than the bisection we implemented. The paper claims to perform at least as good as a bisection, but our &lt;code&gt;bisect5&lt;/code&gt; is driven by the derivatives so it converges
much faster. Here is the heat map with &lt;code&gt;itp5&lt;/code&gt; instead of &lt;code&gt;bisect5&lt;/code&gt;:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The naive unrolled version of Cem Yuksel paper definitely is, so far, the best choice for our problem. I have still concerns about how to implement a good quadratic formula, and I have my reservations about various edge cases. There is also still room for improvements in the cubic solver (degree 3) because it's still a special case where analytical formulas exist, but in general this implementation is satisfying.&lt;/p&gt;
    &lt;p&gt;The next step is to work with chains of B√©zier curves to make up complex shapes (such as font glyphs). It will lead us to build a signed distance field. This is not trivial at all and mandates one or several dedicated articles. We will hopefully study these subjects in the not-so-distant future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.pkh.me/p/46-fast-calculation-of-the-distance-to-cubic-bezier-curves-on-the-gpu.html"/><published>2025-10-18T09:25:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626130</id><title>./watch</title><updated>2025-10-18T20:35:09.739544+00:00</updated><link href="https://dotslashwatch.com/"/><published>2025-10-18T09:55:06+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626961</id><title>Lux: A luxurious package manager for Lua</title><updated>2025-10-18T20:35:09.154431+00:00</updated><content>&lt;doc fingerprint="ac056e38d961f788"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;A luxurious package manager for Lua.&lt;/head&gt;
    &lt;p&gt;Key Features ‚Ä¢ How To Use ‚Ä¢ Comparison with Luarocks ‚Ä¢ Related Projects ‚Ä¢ Contributing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and manage Lua projects &lt;list rend="ul"&gt;&lt;item&gt;Easily manage dependencies, build steps and more through the &lt;code&gt;lux.toml&lt;/code&gt;file.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Easily manage dependencies, build steps and more through the &lt;/item&gt;
      &lt;item&gt;Parallel builds and installs üöÄ&lt;/item&gt;
      &lt;item&gt;Add/remove dependencies with simple CLI commands&lt;/item&gt;
      &lt;item&gt;Automatic generation of rockspecs &lt;list rend="ul"&gt;&lt;item&gt;Say goodbye to managing 10 different rockspec files in your source code üéâ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Integrated code formatting via &lt;code&gt;lx fmt&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;stylua&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Easily specify compatible Lua versions &lt;list rend="ul"&gt;&lt;item&gt;Lux will take care of Lua header installation automatically&lt;/item&gt;&lt;item&gt;Forget about users complaining they have the wrong Lua headers installed on their system&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatic EmmyLua/LuaCATS based type checking via &lt;code&gt;lx check&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;emmylua-analyzer-rust&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Automatic code linting via &lt;code&gt;lx lint&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;luacheck&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Powerful lockfile support &lt;list rend="ul"&gt;&lt;item&gt;Makes for fully reproducible developer environments.&lt;/item&gt;&lt;item&gt;Makes Lux easy to integrate with Nix!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fully compatible &lt;list rend="ul"&gt;&lt;item&gt;Works with existing luarocks packages.&lt;/item&gt;&lt;item&gt;Have a complex rockspec that you don't want to rewrite to TOML? No problem! Lux allows the creation of an &lt;code&gt;extra.rockspec&lt;/code&gt;file, everything just works.&lt;/item&gt;&lt;item&gt;Have a very complex build script? Lux can shell out to &lt;code&gt;luarocks&lt;/code&gt;if it knows it has to preserve maximum compatibility.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatically adds project dependencies to a &lt;code&gt;.luarc.json&lt;/code&gt;file so they can be picked up by&lt;code&gt;lua-language-server&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Lux, while generally functional, is a work in progress and does not have a &lt;code&gt;1.0&lt;/code&gt; release yet.&lt;/p&gt;
    &lt;p&gt;Feel free to consult the documentation on how to get started with Lux!&lt;/p&gt;
    &lt;p&gt;It features a tutorial and several guides to make you good at managing Lua projects.&lt;/p&gt;
    &lt;p&gt;As this project is still a work in progress, some luarocks features have not been (fully) implemented yet. On the other hand, lux has some features that are not present in luarocks.&lt;/p&gt;
    &lt;p&gt;The following table provides a brief comparison:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;lux&lt;/cell&gt;
        &lt;cell role="head"&gt;luarocks v3.12.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;project format&lt;/cell&gt;
        &lt;cell&gt;TOML / Lua&lt;/cell&gt;
        &lt;cell&gt;Lua&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;add/remove dependencies&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;parallel builds/installs&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;proper lockfile support with integrity checks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå (basic, dependency versions only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;run tests with busted&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;linting with luacheck&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;code formatting with stylua&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;automatic lua detection/installation&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;default build specs&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;custom build backends&lt;/cell&gt;
        &lt;cell&gt;‚úÖ1&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;rust-mlua&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (builtin)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;treesitter-parser&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (builtin)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install multiple packages with a single command&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install packages using version constraints&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;auto-detect external dependencies and Lua headers with &lt;code&gt;pkg-config&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;resolve multiple versions of the same dependency at runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pack and upload pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org manifest namespaces&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org dev packages&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;versioning&lt;/cell&gt;
        &lt;cell&gt;SemVer2&lt;/cell&gt;
        &lt;cell&gt;arbitrary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rockspecs with CVS/Mercurial/SVN/SSCM sources&lt;/cell&gt;
        &lt;cell&gt;‚ùå (YAGNI3)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;static type checking&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;generate a &lt;code&gt;.luarc&lt;/code&gt; file with dependencies&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;git dependencies in local projects&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Lux includes the following packages and libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The main CLI for interacting with projects and installing Lua packages from the command line.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua&lt;/code&gt;: The Lux Lua API, which provides:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;lux.loader&lt;/code&gt;for resolving dependencies on&lt;code&gt;require&lt;/code&gt;at runtime.&lt;/item&gt;&lt;item&gt;A work-in-progress API for embedding Lux into Lua applications. We provide builds of &lt;code&gt;lux-lua&lt;/code&gt;for Lua 5.1, 5.2, 5.3, 5.4 and Luajit.&lt;code&gt;lux-cli&lt;/code&gt;uses&lt;code&gt;lux-lua&lt;/code&gt;for commands like&lt;code&gt;lx lua&lt;/code&gt;,&lt;code&gt;lx run&lt;/code&gt;and&lt;code&gt;lx path&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lib&lt;/code&gt;: The Lux library for Rust. A dependency of&lt;code&gt;lux-cli&lt;/code&gt;and&lt;code&gt;lux-lua&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We do not yet provide a way to install &lt;code&gt;lux-lua&lt;/code&gt; as a Lua library using Lux.
See #663.
Lux can detect a lux-lua installation using pkg-config
or via the &lt;code&gt;LUX_LIB_DIR&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Our pre-built binary release artifacts are bundled with &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dependencies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gnupg&lt;/code&gt;,&lt;code&gt;libgpg-error&lt;/code&gt;and&lt;code&gt;gpgme&lt;/code&gt;(*nix only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building without the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;lua&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libgit2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;openssl&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building with the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;perl&lt;/code&gt;and&lt;code&gt;perl-core&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;make&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To link &lt;code&gt;gpgme&lt;/code&gt; statically on Linux and macOS, set the environment variable
&lt;code&gt;SYSTEM_DEPS_LINK=static&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We usually recommend building with the &lt;code&gt;vendored&lt;/code&gt; feature enabled,
to statically link &lt;code&gt;lua&lt;/code&gt;, &lt;code&gt;libgit2&lt;/code&gt; and &lt;code&gt;openssl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;SYSTEM_DEPS_LINK="static" cargo build --locked --profile release --features vendored&lt;/code&gt;
    &lt;p&gt;Or, to build with dynamically linked libraries:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release&lt;/code&gt;
    &lt;p&gt;On Windows/MSVC, you must disable the &lt;code&gt;gpgme&lt;/code&gt; feature:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release --no-default-features --features lua54,vendored&lt;/code&gt;
    &lt;p&gt;You can build &lt;code&gt;lux-lua&lt;/code&gt; for a given Lua version with:&lt;/p&gt;
    &lt;code&gt;cargo xtask51 dist-lua # lux-lua for Lua 5.1
cargo xtask52 dist-lua # for Lua 5.2
cargo xtask53 dist-lua # ...
cargo xtask54 dist-lua
cargo xtaskjit dist-lua&lt;/code&gt;
    &lt;p&gt;This will install &lt;code&gt;lux-lua&lt;/code&gt; to &lt;code&gt;target/dist/share/lux-lua/&amp;lt;lua&amp;gt;/lux.so&lt;/code&gt;
and a pkg-config &lt;code&gt;.pc&lt;/code&gt; file to &lt;code&gt;target/dist/lib/lux-lua*.pc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To build completions:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-completions&lt;/code&gt;
    &lt;p&gt;To build man pages:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-man&lt;/code&gt;
    &lt;p&gt;To build the binary distributions for your platform, bundled with completions, man pages and &lt;code&gt;lux-lua&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-package&lt;/code&gt;
    &lt;p&gt;If you would like to use the latest version of lux with Nix, you can import our flake. It provides an overlay and packages for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The Lux CLI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua51&lt;/code&gt;The Lux Lua API for Lua 5.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua52&lt;/code&gt;The Lux Lua API for Lua 5.2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua53&lt;/code&gt;The Lux Lua API for Lua 5.3&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua54&lt;/code&gt;The Lux Lua API for Lua 5.4&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-luajit&lt;/code&gt;The Lux Lua API for Luajit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a &lt;code&gt;lux-lua&lt;/code&gt; build and &lt;code&gt;pkg-config&lt;/code&gt; in a Nix devShell,
Lux will auto-detect &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;luarocks - The original Lua package manager&lt;/item&gt;
      &lt;item&gt;rocks.nvim - A Neovim plugin manager that uses &lt;code&gt;luarocks&lt;/code&gt;under the hood, and will soon be undergoing a rewrite to use Lux instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Credits go to the Luarocks team for maintaining luarocks and luarocks.org for as long as they have. Without their prior work Lux would not be possible.&lt;/p&gt;
    &lt;p&gt;Contributions are more than welcome! See CONTRIBUTING.md for a guide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lux is licensed under LGPL-3.0+.&lt;/item&gt;
      &lt;item&gt;The Lux logo ¬© 2025 by Kai Jakobi is licensed under CC BY-NC-SA 4.0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Supported via a compatibility layer that uses luarocks as a backend. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mostly compatible with the luarocks version parser, which allows an arbitrary number of version components. To comply with SemVer, we treat anything after the third version component (except for the specrev) as a prerelease/build version. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/lumen-oss/lux"/><published>2025-10-18T12:53:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45626985</id><title>SQL Anti-Patterns</title><updated>2025-10-18T20:35:09.002910+00:00</updated><content/><link href="https://datamethods.substack.com/p/sql-anti-patterns-you-should-avoid"/><published>2025-10-18T12:56:50+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627048</id><title>Using CUE to unify IoT sensor data</title><updated>2025-10-18T20:35:08.582685+00:00</updated><content>&lt;doc fingerprint="b6805a100b045a72"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using CUE to unify IoT sensor data&lt;/head&gt;
    &lt;p&gt;I‚Äôve been building a home automation system that processes motion sensor data from various IKEA Zigbee devices. Each sensor type sends a different data structure, and I needed to extract consistent information from all of them without sacrificing type safety.&lt;/p&gt;
    &lt;p&gt;I spent a long time trying to make Home Assistant work for my setup. The visual automation builder felt limiting once I needed conditional logic beyond simple triggers, and writing automations in YAML became tedious. Adding complex logic like correlating motion across multiple rooms or applying time-weighted logic was difficult. I ended up fighting the system rather than solving the problem.&lt;/p&gt;
    &lt;p&gt;My current home automation setup looks like this:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TR√ÖDFRI   ‚îÇ  ‚îÇ  VALLHORN   ‚îÇ
‚îÇ   Sensor    ‚îÇ  ‚îÇ   Sensor    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         Zigbee ‚îÇ
                ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ  zigbee2mqtt     ‚îÇ
       ‚îÇ  (Zigbee bridge) ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ   MQTT Broker    ‚îÇ
       ‚îÇ                  ‚îÇ
       ‚îÇ Topics:          ‚îÇ
       ‚îÇ  bedroom/motion  ‚îÇ
       ‚îÇ  hallway/motion  ‚îÇ
       ‚îÇ  kitchen/motion  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ         ‚îÇ
            ‚îÇ         ‚îÇ JSON messages
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Node-RED  ‚îÇ      ‚îÇ     Home     ‚îÇ
‚îÇ              ‚îÇ      ‚îÇ   Assistant  ‚îÇ
‚îÇ  - Visual    ‚îÇ      ‚îÇ              ‚îÇ
‚îÇ    flows     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  - Visual    ‚îÇ
‚îÇ  - Custom    ‚îÇ      ‚îÇ    automata  ‚îÇ
‚îÇ    logic     ‚îÇ      ‚îÇ  - YAML      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚îÇ HTTP API
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Philips Hue Hub  ‚îÇ
                    ‚îÇ                  ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                    ‚îÇ  ‚îÇBulb‚îÇ  ‚îÇBulb‚îÇ  ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                    ‚îÇ                  ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
                    ‚îÇ  ‚îÇSensor‚îÇ        ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;With Home Assistant and Node-RED handling some of my automation logic, and controlling devices through various hubs like Philips Hue. This works for many people, but I wanted more control and flexibility that I knew I could get with a custom Go service. I could build exactly what I wanted with the language I already knew.&lt;/p&gt;
    &lt;p&gt;I‚Äôm building a Go service to replace this entire automation layer‚ÄîHome Assistant, Node-RED, and the device-specific hubs. The service subscribes directly to MQTT topics where zigbee2mqtt publishes sensor data, processes that data, and sends commands back to control lights and other devices.&lt;/p&gt;
    &lt;head rend="h2"&gt;The system architecture #&lt;/head&gt;
    &lt;p&gt;The sensors connect via Zigbee, a low-power wireless protocol common in home automation. I‚Äôm running zigbee2mqtt which bridges between Zigbee devices and an MQTT broker. My Go service subscribes to MQTT topics and processes the JSON messages that sensors publish whenever their state changes.&lt;/p&gt;
    &lt;p&gt;Currently, I‚Äôm using two different sensor types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IKEA TR√ÖDFRI motion sensor ‚Äì the older model that reports light levels as a boolean&lt;/item&gt;
      &lt;item&gt;IKEA VALLHORN motion sensor ‚Äì the newer model with precise lux readings and motion timing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eventually, I‚Äôll add others including Philips Hue sensors. Each type has its own unique data structure with different fields. I‚Äôd seen this pattern before‚Äîsmall differences that compound into maintenance headaches as the codebase grows.&lt;/p&gt;
    &lt;p&gt;The zigbee2mqtt documentation shows what each sensor exposes. The TR√ÖDFRI reports light levels as a boolean &lt;code&gt;illuminance_above_threshold&lt;/code&gt;, whilst the VALLHORN gives you actual lux readings in an &lt;code&gt;illuminance&lt;/code&gt; field. The VALLHORN tells you precisely how many seconds have passed since motion stopped via &lt;code&gt;no_occupancy_since&lt;/code&gt;; the TR√ÖDFRI doesn‚Äôt provide this at all. Both detect motion with an &lt;code&gt;occupancy&lt;/code&gt; boolean, but handling these differences in pure Go means type switches, assertions, and duplicated logic across every method that touches sensor data.&lt;/p&gt;
    &lt;p&gt;From the beginning I knew CUE would let me validate sensor data at the boundary and transform it into consistent structures, meaning the rest of my code doesn‚Äôt need to know which sensor type it‚Äôs processing. CUE handles the differences between sensor types in both directions, but this article focuses on the inbound transformation (I‚Äôll cover outbound MQTT messages in a follow-up article). This article shows my CUE implementation alongside what a pure Go approach might have looked like, demonstrating where CUE removes the boilerplate.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pure Go approach and its limitations #&lt;/head&gt;
    &lt;p&gt;Here‚Äôs what processing sensor data might have looked like if I‚Äôd built this system in pure Go without CUE:&lt;/p&gt;
    &lt;code&gt;// Separate structs for each sensor's raw data
type TradfriRawData struct {
    Battery                   int  `json:"battery"`
    Occupancy                 bool `json:"occupancy"`
    IlluminanceAboveThreshold bool `json:"illuminance_above_threshold"`
    Linkquality               int  `json:"linkquality"`
}

type VallhornRawData struct {
    Battery          int   `json:"battery"`
    Occupancy        bool  `json:"occupancy"`
    Illuminance      int   `json:"illuminance"`
    NoOccupancySince *int  `json:"no_occupancy_since,omitempty"` // Optional, pointer to handle null
    Linkquality      int   `json:"linkquality"`
    Voltage          int   `json:"voltage"`
}

type MotionSensorMessage struct {
    SensorID   string `json:"sensor_id"`
    Timestamp  string `json:"timestamp"`
    Floor      string `json:"floor"`
    SensorType string `json:"sensor_type"`
    RawData    any    `json:"raw_data"`
}

// Processing requires routing based on sensor type
func ProcessMQTTMessage(topic string, payload []byte, metadata SensorMetadata) (*MotionSensorMessage, error) {
    var rawData any

    switch metadata.SensorType {
    case "ikea_tradfri_motion":
        var tradfri TradfriRawData
        if err := json.Unmarshal(payload, &amp;amp;tradfri); err != nil {
            return nil, fmt.Errorf("failed to unmarshal tradfri data: %w", err)
        }
        rawData = tradfri

    case "ikea_vallhorn_motion":
        var vallhorn VallhornRawData
        if err := json.Unmarshal(payload, &amp;amp;vallhorn); err != nil {
            return nil, fmt.Errorf("failed to unmarshal vallhorn data: %w", err)
        }
        rawData = vallhorn

    default:
        return nil, fmt.Errorf("unknown sensor type: %s", metadata.SensorType)
    }

    return &amp;amp;MotionSensorMessage{
        SensorID:   metadata.SensorID,
        Timestamp:  time.Now().Format(time.RFC3339),
        Floor:      metadata.Floor,
        SensorType: metadata.SensorType,
        RawData:    rawData,
    }, nil
}

// Every method needs type switches
func (m *MotionSensorMessage) IsMotionDetected() (bool, error) {
    switch data := m.RawData.(type) {
    case TradfriRawData:
        return data.Occupancy, nil
    case VallhornRawData:
        return data.Occupancy, nil
    default:
        return false, fmt.Errorf("unknown sensor data type")
    }
}

func (m *MotionSensorMessage) IsLightSufficient() (bool, bool, error) {
    switch data := m.RawData.(type) {
    case TradfriRawData:
        // Field is inverted - true means dark
        return true, !data.IlluminanceAboveThreshold, nil

    case VallhornRawData:
        // Consider light sufficient if illuminance is above 50 lux
        return true, data.Illuminance &amp;gt; 50, nil

    default:
        return false, false, fmt.Errorf("unknown sensor data type")
    }
}

func (m *MotionSensorMessage) GetSecondsSinceMotion() (int, bool, error) {
    vallhorn, ok := m.RawData.(VallhornRawData)
    if !ok {
        return 0, false, nil // Not supported on this sensor
    }

    if vallhorn.Occupancy {
        return 0, false, nil // Motion detected, field not present
    }

    if vallhorn.NoOccupancySince == nil {
        return 0, false, nil // Field is null
    }

    return *vallhorn.NoOccupancySince, true, nil
}
&lt;/code&gt;
    &lt;p&gt;Typed structs don‚Äôt solve the fundamental problem. Every method that extracts data requires type switches to handle the union. Optional fields like &lt;code&gt;NoOccupancySince&lt;/code&gt; need pointers to distinguish between ‚Äúnot present‚Äù, ‚Äúnull‚Äù, and ‚Äúhas a value‚Äù. The VALLHORN sensor provides precise timing about how long a room has been empty, but extracting this value requires checking the sensor type, verifying occupancy is false, checking the pointer isn‚Äôt nil, and dereferencing it.&lt;/p&gt;
    &lt;p&gt;Add a third sensor type and you extend every type switch with another case. When IKEA releases a firmware update that changes a field, you hunt through multiple functions to fix it. I knew I‚Äôd be adding more sensor types and dealing with firmware variations, so I chose CUE from the start rather than fighting this complexity.&lt;/p&gt;
    &lt;p&gt;You could build similar behaviour in pure Go with interfaces and careful abstraction. CUE handles this declaratively. You define schemas with constraints and transformations, and CUE validates the incoming data whilst extracting normalised structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;What CUE provides #&lt;/head&gt;
    &lt;p&gt;CUE is a configuration language that handles everything from defining schemas and generating configuration files to validating data and computing transformations. It‚Äôs used for API definitions, build configurations, policy validation, and more. In this system, I use it to validate sensor data at the boundary and transform it into normalised structures that my Go code can work with consistently.&lt;/p&gt;
    &lt;p&gt;Instead of validating battery levels with scattered if statements through your Go code, the schema catches invalid data at the boundary:&lt;/p&gt;
    &lt;code&gt;battery: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=100
&lt;/code&gt;
    &lt;p&gt;CUE definitions (prefixed with &lt;code&gt;#&lt;/code&gt;) work like type definitions in Go. You can reference them and compose them with other schemas, but they don‚Äôt produce output by themselves. This lets you build a union type for all supported sensors:&lt;/p&gt;
    &lt;code&gt;#MotionSensor: #IkeaTradfriMotionSensorStatus | #IkeaVallhornMotionSensorStatus
&lt;/code&gt;
    &lt;p&gt;The language handles both validation and transformation. This means you can turn the TR√ÖDFRI‚Äôs boolean light sensor and the VALLHORN‚Äôs lux reading into a single &lt;code&gt;light_sufficient&lt;/code&gt; field without writing Go type switches.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building the schemas #&lt;/head&gt;
    &lt;p&gt;I started with a base schema that all sensors share:&lt;/p&gt;
    &lt;code&gt;#MotionSensorBase: {
    sensor_id!: string
    timestamp?: string
    floor!: "basement" | "ground" | "upstairs" | "attic"
    sensor_type!: string
    raw_data!: _
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;!&lt;/code&gt; operator marks required fields, whilst &lt;code&gt;?&lt;/code&gt; marks optional ones.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;raw_data&lt;/code&gt; field uses CUE‚Äôs top type &lt;code&gt;_&lt;/code&gt;, which accepts any value. This allows each sensor type to define its own structure for &lt;code&gt;raw_data&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Each sensor gets its own schema that composes the base with sensor-specific constraints:&lt;/p&gt;
    &lt;code&gt;#IkeaTradfriMotionSensorStatus: #MotionSensorBase &amp;amp; {
    sensor_type: "ikea_tradfri_motion"
    raw_data: {
        battery: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=100
        occupancy: bool
        illuminance_above_threshold: bool
        linkquality: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=255
    }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;&amp;amp;&lt;/code&gt; operator composes schemas. Each sensor gets its own constraints that match IKEA‚Äôs hardware specifications.&lt;/p&gt;
    &lt;p&gt;The VALLHORN schema includes additional fields that the older TR√ÖDFRI doesn‚Äôt support:&lt;/p&gt;
    &lt;code&gt;#IkeaVallhornMotionSensorStatus: #MotionSensorBase &amp;amp; {
    sensor_type: "ikea_vallhorn_motion"
    raw_data: {
        battery: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=100
        occupancy: bool
        illuminance: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=65535
        linkquality: int &amp;amp; &amp;gt;=0 &amp;amp; &amp;lt;=255

        // Only present when occupancy is false
        no_occupancy_since?: null | int &amp;amp; &amp;gt;=0

        voltage: int
    }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;no_occupancy_since&lt;/code&gt; field is optional (marked with &lt;code&gt;?&lt;/code&gt;) and can be either null or a positive integer. When motion stops, the sensor starts counting seconds and reports them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extracting normalised data #&lt;/head&gt;
    &lt;p&gt;The extraction schemas contain the conditional logic that would otherwise live in Go type switches. Here‚Äôs motion detection:&lt;/p&gt;
    &lt;code&gt;#ExtractMotionFromSensor: {
    sensor: #MotionSensor

    motion_detected: bool
    if sensor.sensor_type == "ikea_tradfri_motion" ||
       sensor.sensor_type == "ikea_vallhorn_motion" {
        motion_detected: sensor.raw_data.occupancy
    }

    // Optional field for precise timing on supported sensors
    no_occupancy_seconds?: int

    if sensor.sensor_type == "ikea_vallhorn_motion" {
        // Only present when occupancy is false and sensor supports it
        if sensor.raw_data.occupancy == false &amp;amp;&amp;amp;
           sensor.raw_data.no_occupancy_since != _|_ &amp;amp;&amp;amp;
           sensor.raw_data.no_occupancy_since != null {
            no_occupancy_seconds: sensor.raw_data.no_occupancy_since
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;In Go, you‚Äôd write a method on each sensor struct to extract this data. Add a new sensor and you update every extraction method. With CUE, you add one conditional block to the schema and the extraction works everywhere you use it.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no_occupancy_seconds&lt;/code&gt; field shows how CUE handles sensor-specific features. The VALLHORN provides this timing information, the TR√ÖDFRI doesn‚Äôt. Making the field optional means sensors without this capability simply don‚Äôt include it. Automation logic can check if it exists and use it for precise timing (‚Äúturn off lights if no motion for 10 minutes‚Äù), or fall back to basic occupancy detection when it‚Äôs not available.&lt;/p&gt;
    &lt;p&gt;Light detection demonstrates how CUE normalises different measurement approaches:&lt;/p&gt;
    &lt;code&gt;#ExtractAmbientLightFromSensor: {
    sensor: #MotionSensor

    light_sufficient?: bool
    has_ambient_sensor: bool

    if sensor.sensor_type == "ikea_tradfri_motion" {
        if sensor.raw_data.illuminance_above_threshold != _|_ {
            has_ambient_sensor: true
            // This field is inverted - true means dark
            light_sufficient: !sensor.raw_data.illuminance_above_threshold
        }
    }

    if sensor.sensor_type == "ikea_vallhorn_motion" {
        if sensor.raw_data.illuminance != _|_ {
            has_ambient_sensor: true
            // 50 lux is sufficient for movement without lights
            light_sufficient: sensor.raw_data.illuminance &amp;gt; 50
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;The TR√ÖDFRI‚Äôs boolean gets inverted (it indicates darkness rather than light). The VALLHORN‚Äôs lux reading gets compared to a threshold. Application code doesn‚Äôt need to know about these differences. It just checks &lt;code&gt;light_sufficient&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Go code works with CUE #&lt;/head&gt;
    &lt;p&gt;With CUE handling validation and transformation, the Go code becomes generic. In my Go code, I‚Äôve decided to wrap optional fields in an Option type that provides convenience helpers like &lt;code&gt;IsPresent()&lt;/code&gt; and &lt;code&gt;MustGet()&lt;/code&gt; methods for safely handling values that may or may not exist.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the actual implementation:&lt;/p&gt;
    &lt;code&gt;// Generic processor that works for ALL sensor types
type MotionSensorProcessor struct {
    cueCtx                    *cue.Context
    extractMotionSchema       cue.Value  // #ExtractMotionFromSensor
    extractAmbientLightSchema cue.Value  // #ExtractAmbientLightFromSensor
}

// This function works for Vallhorn, Tradfri, or any future sensor
func (p *MotionSensorProcessor) ProcessSensorData(sensorData MotionSensor, statusSchema cue.Value) error {
    // Encode to CUE and validate against the sensor-specific schema
    sensorDataAsCUE := p.cueCtx.Encode(sensorData)
    if err := statusSchema.Unify(sensorDataAsCUE).Validate(); err != nil {
        return fmt.Errorf("schema validation failed: %w", err)
    }

    // Extract motion event using CUE's extraction schema
    extractMotion := map[string]any{"sensor": sensorData}
    extractMotionAsCUE := p.cueCtx.Encode(extractMotion)
    unifiedMotion := p.extractMotionSchema.Unify(extractMotionAsCUE)

    // Decode back to a clean Go struct
    var motionEvent MotionEvent
    if err := unifiedMotion.Decode(&amp;amp;motionEvent); err != nil {
        return fmt.Errorf("failed to decode motion event: %w", err)
    }

    // Identical code for all sensor types
    if motionEvent.MotionDetected {
        handleMotionDetected(sensorData.Floor)
    }

    // Works for Vallhorn (which has the field) and gracefully handles
    // Tradfri (which doesn't) via the Option type
    if motionEvent.NoOccupancySeconds.IsPresent() {
        seconds := motionEvent.NoOccupancySeconds.MustGet()
        if seconds &amp;gt; 600 {
            handleNoMotionTimeout(sensorData.Floor, seconds)
        }
    }

    // Extract ambient light reading using CUE
    extractAmbientLight := map[string]any{"sensor": sensorData}
    extractAmbientLightAsCUE := p.cueCtx.Encode(extractAmbientLight)
    unifiedAmbientLight := p.extractAmbientLightSchema.Unify(extractAmbientLightAsCUE)

    var lightReading AmbientLightReading
    if err := unifiedAmbientLight.Decode(&amp;amp;lightReading); err != nil {
        return fmt.Errorf("failed to decode ambient light: %w", err)
    }

    // Handles both Tradfri's boolean and Vallhorn's lux reading
    // CUE normalised them both into a single light_sufficient field
    if lightReading.HasAmbientSensor &amp;amp;&amp;amp; lightReading.LightSufficient.IsPresent() {
        if !lightReading.LightSufficient.MustGet() {
            handleDarkness(sensorData.Floor)
        }
    }

    return nil
}

// Vallhorn-specific handler - ONLY handles unmarshalling
type VallhornHandler struct {
    processor    *MotionSensorProcessor
    statusSchema cue.Value  // #IkeaVallhornMotionSensorStatus
}

func (h *VallhornHandler) HandleMQTT(topic string, payload []byte) error {
    sensorID, floor, err := parseSensorTopic(topic)
    if err != nil {
        return fmt.Errorf("failed to parse topic: %w", err)
    }

    // THIS IS THE ONLY VALLHORN-SPECIFIC CODE
    var rawData VallhornRawData
    if err := json.Unmarshal(payload, &amp;amp;rawData); err != nil {
        return fmt.Errorf("failed to unmarshal: %w", err)
    }

    // Build the generic sensor data structure
    sensorData := MotionSensor{
        SensorID:   sensorID,
        Timestamp:  time.Now().Format(time.RFC3339),
        Floor:      floor,
        SensorType: "ikea_vallhorn_motion",
        RawData:    rawData,
    }

    // Everything from here is generic and reusable
    return h.processor.ProcessSensorData(sensorData, h.statusSchema)
}

// Tradfri-specific handler - ONLY handles unmarshalling
type TradfriHandler struct {
    processor    *MotionSensorProcessor
    statusSchema cue.Value  // #IkeaTradfriMotionSensorStatus
}

func (h *TradfriHandler) HandleMQTT(topic string, payload []byte) error {
    sensorID, floor, err := parseSensorTopic(topic)
    if err != nil {
        return fmt.Errorf("failed to parse topic: %w", err)
    }

    // THIS IS THE ONLY TRADFRI-SPECIFIC CODE
    var rawData TradfriRawData
    if err := json.Unmarshal(payload, &amp;amp;rawData); err != nil {
        return fmt.Errorf("failed to unmarshal: %w", err)
    }

    // Build the generic sensor data structure
    sensorData := MotionSensor{
        SensorID:   sensorID,
        Timestamp:  time.Now().Format(time.RFC3339),
        Floor:      floor,
        SensorType: "ikea_tradfri_motion",
        RawData:    rawData,
    }

    // Same generic processor works for Tradfri too
    return h.processor.ProcessSensorData(sensorData, h.statusSchema)
}
&lt;/code&gt;
    &lt;p&gt;Each handler‚Äôs sensor-specific code is about five lines: just unmarshalling JSON into the correct struct type. Everything else is generic processing. The same &lt;code&gt;ProcessSensorData&lt;/code&gt; function validates, extracts motion events, extracts ambient light readings, and executes business logic without knowing which specific sensor it‚Äôs processing.&lt;/p&gt;
    &lt;p&gt;The extraction schemas in CUE contain all the conditional logic. The Go code just encodes, unifies, and decodes. No type switches. No conditional chains.&lt;/p&gt;
    &lt;head rend="h2"&gt;What you gain #&lt;/head&gt;
    &lt;p&gt;When an MQTT message arrives, validation against the CUE schema happens at the boundary. Invalid data gets rejected with specific error messages about what‚Äôs wrong rather than causing runtime panics deep in the application.&lt;/p&gt;
    &lt;p&gt;The schema files document the exact structure and valid ranges for each sensor. Adding a new sensor type means defining one schema and making the necessary changes to the extraction schemas for motion and ambient light. The existing generic processing code continues to work.&lt;/p&gt;
    &lt;p&gt;That said, CUE validates structure but tracking application state (like whether a room has been unoccupied for 10 minutes) still requires Go code. CUE tells you what the current state is, not what to do with it. Time-series analysis, rate limiting, and state machines live in your application layer.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a learning curve if you‚Äôre coming from Protobuf, JSON Schema or Go structs. CUE‚Äôs syntax looks familiar but behaves differently, especially around definitions versus concrete values. The documentation helps, but expect some initial friction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why CUE instead of alternatives #&lt;/head&gt;
    &lt;p&gt;JSON Schema can validate incoming data but can‚Äôt transform it. You‚Äôd still need Go type switches to extract normalised structures from different sensor types.&lt;/p&gt;
    &lt;p&gt;Protocol Buffers require a compilation step and don‚Äôt handle optional fields as elegantly. The &lt;code&gt;no_occupancy_since&lt;/code&gt; field that‚Äôs sometimes present, sometimes null, and sometimes absent would need Protobuf‚Äôs wrapper types (like &lt;code&gt;google.protobuf.Int32Value&lt;/code&gt;) or custom handling logic.&lt;/p&gt;
    &lt;p&gt;CUE does both validation and transformation in one tool. The same schemas that validate incoming MQTT messages also define the extraction logic that normalises the data. You maintain one set of schemas that handles both concerns.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs next #&lt;/head&gt;
    &lt;p&gt;I built this system with CUE from the start, and the flexibility it provides compared to a pure Go solution has worked well for this use case. The schemas sit at the boundary where MQTT messages enter the system. Every sensor type gets validated, and the rest of the codebase works with clean, normalised structures. The system currently handles four motion sensors across four floors, controlling lights on two floors.&lt;/p&gt;
    &lt;p&gt;Adding new sensor types means defining a schema and updating the extraction logic. When I add Philips Hue sensors, I‚Äôll extend the &lt;code&gt;#MotionSensor&lt;/code&gt; union type and make the necessary changes to the extraction schemas for motion and ambient light detection. The generic processing code won‚Äôt need to change.&lt;/p&gt;
    &lt;p&gt;This article covered inbound sensor data transformation. But in the coming weeks, I‚Äôll be writing more about this system, starting with how CUE handles the outbound side: generating device-specific MQTT commands for different bulb types. There‚Äôs a lot more to explore around building automation rules, managing state, and handling edge cases in a distributed sensor network.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thanks #&lt;/head&gt;
    &lt;p&gt;Thanks to Paul, Roger and Daniel from the CUE team for their help and support with CUE and this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://aran.dev/posts/cue/using-cue-to-unify-iot-sensor-data/"/><published>2025-10-18T13:08:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627324</id><title>Ripgrep 15.0</title><updated>2025-10-18T20:35:08.005703+00:00</updated><content>&lt;doc fingerprint="3cab1bfb2c6333d9"&gt;
  &lt;main&gt;
    &lt;p&gt;ripgrep 15 is a new major version release of ripgrep that mostly has bug fixes,&lt;lb/&gt; some minor performance improvements and minor new features.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In case you haven't heard of it before, ripgrep is a line-oriented search&lt;/p&gt;&lt;lb/&gt;tool that recursively searches the current directory for a regex pattern.&lt;lb/&gt;By default, ripgrep will respect gitignore rules and automatically skip&lt;lb/&gt;hidden files/directories and binary files.&lt;/quote&gt;
    &lt;p&gt;Here are some highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Several bugs around gitignore matching have been fixed. This includes&lt;lb/&gt;a commonly reported bug related to applying gitignore rules from parent&lt;lb/&gt;directories.&lt;/item&gt;
      &lt;item&gt;A memory usage regression when handling very large gitignore files has been&lt;lb/&gt;fixed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rg -vf file&lt;/code&gt;, where&lt;code&gt;file&lt;/code&gt;is empty, now matches everything.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;-r/--replace&lt;/code&gt;flag now works with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A subset of Jujutsu (&lt;code&gt;jj&lt;/code&gt;) repositories are now treated as if they were git&lt;lb/&gt;repositories. That is, ripgrep will respect&lt;code&gt;jj&lt;/code&gt;'s gitignores.&lt;/item&gt;
      &lt;item&gt;Globs can now use nested curly braces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platform support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;aarch64&lt;/code&gt;for Windows now has release artifacts.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;powerpc64&lt;/code&gt;no longer has release artifacts generated for it. The CI&lt;lb/&gt;release workflow stopped working, and I didn't deem it worth my time to&lt;lb/&gt;debug it. If someone wants this and can test it, I'd be happy to add it&lt;lb/&gt;back.&lt;/item&gt;
      &lt;item&gt;ripgrep binaries are now compiled with full LTO enabled. You may notice&lt;lb/&gt;small performance improvements from this and a modest decrease in binary&lt;lb/&gt;size.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PERF #2111:&lt;lb/&gt;Don't resolve helper binaries on Windows when&lt;code&gt;-z/--search-zip&lt;/code&gt;isn't used.&lt;/item&gt;
      &lt;item&gt;PERF #2865:&lt;lb/&gt;Avoid using path canonicalization on Windows when emitting hyperlinks.&lt;/item&gt;
      &lt;item&gt;PERF #3184:&lt;lb/&gt;Improve performance of large values with&lt;code&gt;-A/--after-context&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bug fixes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BUG #829,&lt;lb/&gt;BUG #2731,&lt;lb/&gt;BUG #2747,&lt;lb/&gt;BUG #2770,&lt;lb/&gt;BUG #2778,&lt;lb/&gt;BUG #2836,&lt;lb/&gt;BUG #2933,&lt;lb/&gt;BUG #3067:&lt;lb/&gt;Fix bug related to gitignores from parent directories.&lt;/item&gt;
      &lt;item&gt;BUG #1332,&lt;lb/&gt;BUG #3001:&lt;lb/&gt;Make&lt;code&gt;rg -vf file&lt;/code&gt;where&lt;code&gt;file&lt;/code&gt;is empty match everything.&lt;/item&gt;
      &lt;item&gt;BUG #2177:&lt;lb/&gt;Ignore a UTF-8 BOM marker at the start of&lt;code&gt;.gitignore&lt;/code&gt;(and similar files).&lt;/item&gt;
      &lt;item&gt;BUG #2750:&lt;lb/&gt;Fix memory usage regression for some truly large gitignore files.&lt;/item&gt;
      &lt;item&gt;BUG #2944:&lt;lb/&gt;Fix a bug where the "bytes searched" in&lt;code&gt;--stats&lt;/code&gt;output could be incorrect.&lt;/item&gt;
      &lt;item&gt;BUG #2990:&lt;lb/&gt;Fix a bug where ripgrep would mishandle globs that ended with a&lt;code&gt;.&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #2094,&lt;lb/&gt;BUG #3076:&lt;lb/&gt;Fix bug with&lt;code&gt;-m/--max-count&lt;/code&gt;and&lt;code&gt;-U/--multiline&lt;/code&gt;showing too many matches.&lt;/item&gt;
      &lt;item&gt;BUG #3100:&lt;lb/&gt;Preserve line terminators when using&lt;code&gt;-r/--replace&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3108:&lt;lb/&gt;Fix a bug where&lt;code&gt;-q --files-without-match&lt;/code&gt;inverted the exit code.&lt;/item&gt;
      &lt;item&gt;BUG #3131:&lt;lb/&gt;Document inconsistency between&lt;code&gt;-c/--count&lt;/code&gt;and&lt;code&gt;--files-with-matches&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3135:&lt;lb/&gt;Fix rare panic for some classes of large regexes on large haystacks.&lt;/item&gt;
      &lt;item&gt;BUG #3140:&lt;lb/&gt;Ensure hyphens in flag names are escaped in the roff text for the man page.&lt;/item&gt;
      &lt;item&gt;BUG #3155:&lt;lb/&gt;Statically compile PCRE2 into macOS release artifacts on&lt;code&gt;aarch64&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3173:&lt;lb/&gt;Fix ancestor ignore filter bug when searching whitelisted hidden files.&lt;/item&gt;
      &lt;item&gt;BUG #3178:&lt;lb/&gt;Fix bug causing incorrect summary statistics with&lt;code&gt;--json&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3179:&lt;lb/&gt;Fix gitignore bug when searching absolute paths with global gitignores.&lt;/item&gt;
      &lt;item&gt;BUG #3180:&lt;lb/&gt;Fix a panicking bug when using&lt;code&gt;-U/--multiline&lt;/code&gt;and&lt;code&gt;-r/--replace&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature enhancements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many enhancements to the default set of file types available for filtering.&lt;/item&gt;
      &lt;item&gt;FEATURE #1872:&lt;lb/&gt;Make&lt;code&gt;-r/--replace&lt;/code&gt;work with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2708:&lt;lb/&gt;Completions for the fish shell take ripgrep's config file into account.&lt;/item&gt;
      &lt;item&gt;FEATURE #2841:&lt;lb/&gt;Add&lt;code&gt;italic&lt;/code&gt;to the list of available style attributes in&lt;code&gt;--color&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2842:&lt;lb/&gt;Directories containing&lt;code&gt;.jj&lt;/code&gt;are now treated as git repositories.&lt;/item&gt;
      &lt;item&gt;FEATURE #2849:&lt;lb/&gt;When using multithreading, schedule files to search in order given on CLI.&lt;/item&gt;
      &lt;item&gt;FEATURE #2943:&lt;lb/&gt;Add&lt;code&gt;aarch64&lt;/code&gt;release artifacts for Windows.&lt;/item&gt;
      &lt;item&gt;FEATURE #3024:&lt;lb/&gt;Add&lt;code&gt;highlight&lt;/code&gt;color type, for styling non-matching text in a matching line.&lt;/item&gt;
      &lt;item&gt;FEATURE #3048:&lt;lb/&gt;Globs in ripgrep (and the&lt;code&gt;globset&lt;/code&gt;crate) now support nested alternates.&lt;/item&gt;
      &lt;item&gt;FEATURE #3096:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in bash and fish.&lt;/item&gt;
      &lt;item&gt;FEATURE #3102:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in zsh.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/BurntSushi/ripgrep/releases/tag/15.0.0"/><published>2025-10-18T13:44:02+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627394</id><title>Root System Drawings</title><updated>2025-10-18T20:35:06.732234+00:00</updated><content>&lt;doc fingerprint="fbf286432d583069"&gt;
  &lt;main&gt;
    &lt;p&gt;Javascript Required To experience full interactivity, please enable Javascript in your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://images.wur.nl/digital/collection/coll13/search"/><published>2025-10-18T13:52:24+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45627692</id><title>Flowistry: An IDE plugin for Rust that focuses on relevant code</title><updated>2025-10-18T20:35:06.247725+00:00</updated><content>&lt;doc fingerprint="b283f24420974767"&gt;
  &lt;main&gt;
    &lt;p&gt;Flowistry is a tool that analyzes the information flow of Rust programs. Flowistry understands whether it's possible for one piece of code to affect another. Flowistry integrates into the IDE to provide a "focus mode" which helps you focus on the code that's related to your current task.&lt;/p&gt;
    &lt;p&gt;For example, this GIF shows the focus mode when reading a function that unions two sets together:&lt;/p&gt;
    &lt;p&gt;When the user clicks a given variable or expression, Flowistry fades out all code that does not influence that code, and is not influenced by that code. For example, &lt;code&gt;orig_len&lt;/code&gt; is not influenced by the for-loop, while &lt;code&gt;set.len()&lt;/code&gt; is.&lt;/p&gt;
    &lt;p&gt;Flowistry can be helpful when you're reading a function with a lot of code. For example, this GIF shows a real function in the Rust compiler. If you want to understand the role of a specific argument to the function, then Flowistry can filter out most of the code as irrelevant:&lt;/p&gt;
    &lt;p&gt;The algorithm that powers Flowistry was published in the paper "Modular Information Flow through Ownership" at PLDI 2022.&lt;/p&gt;
    &lt;p&gt;Table of contents&lt;/p&gt;
    &lt;p&gt;Flowistry is available as a VSCode plugin. You can install Flowistry from the Visual Studio Marketplace or the Open VSX Registry. In VSCode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the Extensions pane by clicking this button in the left margin:&lt;/item&gt;
      &lt;item&gt;Search for "Flowistry" and then click "Install".&lt;/item&gt;
      &lt;item&gt;Open a Rust workspace and wait for the tool to finish installing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note on platform support: Flowistry does not yet support NixOS. Flowistry cannot provide pre-built binaries for ARM targets like M1 Macs, so Flowistry must be installed from scratch on these targets (this is done for you, but will take a few more minutes than usual).&lt;/p&gt;
    &lt;p&gt;Alternatively, you can install it from source:&lt;/p&gt;
    &lt;code&gt;# Install flowistry binaries
git clone https://github.com/willcrichton/flowistry
cd flowistry
cargo install --path crates/flowistry_ide

# Install vscode extension
cd ide
npm install
npm run build
ln -s $(pwd) ~/.vscode/extensions/flowistry
&lt;/code&gt;
    &lt;p&gt;If you are interested in the underlying analysis, you can use the &lt;code&gt;flowistry&lt;/code&gt; crate published to crates.io: https://crates.io/crates/flowistry&lt;/p&gt;
    &lt;p&gt;The documentation is published here: https://willcrichton.net/flowistry/flowistry/&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Docs.rs doesn't support documentation for crates that use&lt;/p&gt;&lt;code&gt;#![feature(rustc_private)]&lt;/code&gt;so we have to host it ourselves.&lt;/quote&gt;
    &lt;p&gt;Note that the latest Flowistry has a Maximum Supported Rust Version of Rust 1.73. Flowistry is not guaranteed to work with features implemented after 1.73.&lt;/p&gt;
    &lt;p&gt;Once you have installed Flowistry, open a Rust workspace in VSCode. You should see this icon in the bottom toolbar:&lt;/p&gt;
    &lt;p&gt;Flowistry starts up by type-checking your codebase. This may take a few minutes if you have many dependencies.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Flowistry type-checking results are cached in the&lt;/p&gt;&lt;code&gt;target/flowistry&lt;/code&gt;directory. If you delete this folder, Flowistry will have to recompute types. Also for a large codebase this directory may take up a fair amount of disk space.&lt;/quote&gt;
    &lt;p&gt;Once Flowistry has booted up, the loading icon will disappear. Then you can enter focus mode by running the "Toggle focus mode" command. By default the keyboard shortcut is Ctrl+R Ctrl+A (‚åò+R ‚åò+A on Mac), or you can use the Flowistry context menu:&lt;/p&gt;
    &lt;p&gt;In focus mode, Flowistry will automatically compute the information flow within a given function once you put your cursor there. Once Flowistry has finished analysis, the status bar will look like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Flowistry can be a bit slow for larger functions. It may take up to 15 seconds to finish the analysis.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Flowistry infers what you want to focus on based on your cursor. So if you click on a variable, you should see the focus region of that variable. Flowistry will highlight the focused code in gray, and then fade out code outside the focus region. For example, because the user's cursor is on &lt;code&gt;view_projection&lt;/code&gt;, that variable is highlighted in gray, and its focus region is shown.&lt;/p&gt;
    &lt;p&gt;Sometimes you want to keep the focus region where it is, and click on other code to inspect it without changing focus. For this purpose, Flowistry has a concept of a "mark". Once you have selected code to focus on, you can run the "Set mark" command (Ctrl+R Ctrl+S / ‚åò+R ‚åò+S). Then a mark is set at your cursor's current position, and the focus will stay there until you run the "Unset mark" command (Ctrl+R Ctrl+D / ‚åò+R ‚åò+D).&lt;/p&gt;
    &lt;p&gt;If you want to modify all the code in the focus region, e.g. to comment it out or copy it, then you can run the "Select focused region" command (Ctrl+R Ctrl+T / ‚åò+R ‚åò+T). This will add the entire focus region into your editor's selection.&lt;/p&gt;
    &lt;p&gt;Flowistry is an active research project into the applications of information flow analysis for Rust. It is continually evolving as we experiment with analysis techniques and interaction paradigms. So it's not quite as polished or efficient as tools like Rust Analyzer, but we hope you can still find it useful! Nevertheless, there are a number of important limitations you should understand when using Flowistry to avoid being surprised.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, please file a Github issue, join our Discord, or DM @wcrichton on Twitter.&lt;/p&gt;
    &lt;p&gt;When your code has references, Flowistry needs to understand what that reference points-to. Flowistry uses Rust's lifetime information to determine points-to information. However, data structures that use interior mutability such as &lt;code&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;&lt;/code&gt; explicitly do not share lifetimes between pointers to the same data. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let x = Arc::new(Mutex::new(0));
let y = x.clone();
*x.lock().unwrap() = 1;
println!("{}", y.lock().unwrap());&lt;/code&gt;
    &lt;p&gt;Flowistry can determine that &lt;code&gt;*x.lock().unwrap() = 1&lt;/code&gt; is a mutation to &lt;code&gt;x&lt;/code&gt;, but it can not determine that it is a mutation to &lt;code&gt;y&lt;/code&gt;. So if you focus on &lt;code&gt;y&lt;/code&gt;, the assignment to 1 would be faded out, even though it is relevant to the value of &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We are researching methods to overcome this limitation, but for now just be aware that this is the main case where Flowistry is known to provide an incorrect answer.&lt;/p&gt;
    &lt;p&gt;Flowistry's analysis tries to include all code that could have an influence on a focal point. This analysis makes a number of assumptions for both practical and fundamental reasons. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let mut v = vec![1, 2, 3];
let x = v.get_mut(0);
println!("{:?} {}", v, x);&lt;/code&gt;
    &lt;p&gt;If you focus on &lt;code&gt;v&lt;/code&gt; on line 3, it will include &lt;code&gt;v.get_mut(0)&lt;/code&gt; as an operation that could have modified &lt;code&gt;v&lt;/code&gt;. The reason is that Flowistry does not actually analyze the bodies of called functions, but rather approximates based on their type signatures. Because &lt;code&gt;get_mut&lt;/code&gt; takes &lt;code&gt;&amp;amp;mut self&lt;/code&gt; as input, it assumes that the vector could be modified.&lt;/p&gt;
    &lt;p&gt;In general, you should use focus mode as a pruning tool. If code is faded out, then you don't have to read it (minus the limitation mentioned above!). If it isn't faded out, then it might be relevant to your task.&lt;/p&gt;
    &lt;p&gt;Flowistry works by analyzing the MIR graph for a given function using the Rust compiler's API. Then the IDE extension lifts the analysis results from the MIR level back to the source level. However, a lot of information about the program is lost in the journey from source code to MIR.&lt;/p&gt;
    &lt;p&gt;For example, if the source contains an expression &lt;code&gt;foo.whomp.bar().baz()&lt;/code&gt;, it's possible that a temporary variable is only generated for the expression &lt;code&gt;foo.whomp.bar()&lt;/code&gt;. So if the user selects &lt;code&gt;foo&lt;/code&gt;, Flowistry may not be able to determine that this corresponds to the MIR place that represents &lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is why the IDE extension highlights the focused code in gray, so you can understand what your cursor's selection actually maps to.&lt;/p&gt;
    &lt;p&gt;Flowistry analyzes a single function at a time. If a function contains other functions, e.g. &lt;code&gt;fn&lt;/code&gt; definitions, or closures, or implicitly via async, then Flowistry will only show you focus regions within the smallest function body containing your cursor. This is usually well defined for function definitions and closures, but may be confusing for async since that depends on how rustc decides to carve up your async function.&lt;/p&gt;
    &lt;p&gt;If rustup fails, especially with an error like "could not rename downloaded file", this is probably because Flowistry is running rustup concurrently with another tool (like rust-analyzer). Until rustup#988 is resolved, there is unfortunately no automated way around this.&lt;/p&gt;
    &lt;p&gt;To solve the issue, go to the command line and run:&lt;/p&gt;
    &lt;code&gt;rustup toolchain install nightly-2023-08-25 -c rust-src -c rustc-dev -c llvm-tools-preview
&lt;/code&gt;
    &lt;p&gt;Then go back to VSCode and click "Continue" to let Flowistry continue installing.&lt;/p&gt;
    &lt;p&gt;Rust Analyzer does not support MIR and the borrow checker, which are essential parts of Flowistry's analysis. That fact is unlikely to change for a long time, so Flowistry is a standalone tool.&lt;/p&gt;
    &lt;p&gt;See Limitations for known issues. If that doesn't explain what you're seeing, please post it in the unexpected highlights issue or ask on Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/willcrichton/flowistry"/><published>2025-10-18T14:33:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628190</id><title>Attention is a luxury good</title><updated>2025-10-18T20:35:06.101618+00:00</updated><content>&lt;doc fingerprint="f09fb1166f69ed9a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Attention is a luxury good&lt;/head&gt;
    &lt;p&gt;Luxury goods are special: they are scarce and expensive, and they earn us status with some folks because it shows that we paid more than we needed to.&lt;/p&gt;
    &lt;p&gt;Luxury isn‚Äôt about quality, suitability or performance. Luxury isn‚Äôt a more accurate watch or a faster processor. Luxury is a marker that we can afford to do something others might consider wasteful.&lt;/p&gt;
    &lt;p&gt;A Birkin bag is a luxury good, and so is reading an entire non-fiction book, listening to a public radio broadcast or attending a concert when we could stay at home and listen for free.&lt;/p&gt;
    &lt;p&gt;By ‚Äòwasting‚Äô our attention on nuance, narrative, experiences and everything except the checkbox takeaway, we‚Äôre sending a message to ourselves and others. A message about allocating our time to something beyond optimized performance or survival.&lt;/p&gt;
    &lt;p&gt;If you‚Äôve signed up to offer an attention-luxury good, you undermine it when you also try to make it quick and convenient.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://seths.blog/2025/10/attention-is-a-luxury-good/"/><published>2025-10-18T15:39:47+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628283</id><title>Picturing Mathematics</title><updated>2025-10-18T20:35:05.923110+00:00</updated><content>&lt;doc fingerprint="d0851df7d6d09a8"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôm a great believer in low-tech math. I don‚Äôt like to rely on things a computer tells me; what if there‚Äôs a bug in the code? I prefer trusting things that I can check for myself. At the same time, I‚Äôm keenly aware of the limits of my imagination even when it‚Äôs aided by paper and pencil. Sometimes I need a computer to show me things I can imagine myself imagining but don‚Äôt yet know how to imagine.&lt;/p&gt;
    &lt;p&gt;ILLUSTRATING MATH TOGETHER&lt;/p&gt;
    &lt;p&gt;In 2016, the Institute for Computational and Experimental Research in Mathematics (ICERM) in Providence, Rhode Island hosted a workshop called Illustrating Mathematics with the hope of bringing together researchers who, like me, study mathematical abstractions that can be brought to life by appropriate visuals. The workshop spawned a community that has held meetings at ICERM from time to time and has been running a webinar series since 2023.&lt;/p&gt;
    &lt;p&gt;I‚Äôve spoken twice at the webinar. Back in 2024, I gave a brief mathematical eulogy for the brilliant mathematical explorer Roger Antonsen, now sadly deceased (though you can‚Äôt tell that he‚Äôs deceased from his website), who had a unique knack for coming up with cool visuals related to every topic we ever discussed. The striking figure below, arising from a deterministic model of a one-dimensional gas I‚Äôd proposed, is just one instance among many dozens he created as part of our email conversations.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;On October 10th, 2025, I spoke at the webinar for a second time, even more briefly: I gave a five-minute ‚Äúshow-and-ask‚Äù pitch as a warmup-act for the phenomenal math explainer/animator Grant Sanderson (aka 3Blue1Brown). My lightning talk was entitled ‚ÄúEvolving cross sections of Ford spheres‚Äù, and it was my way of testing the waters of the webinar crowd. I wondered: if I described a compelling mathematical object that nobody has illustrated yet in a fully satisfying manner, or at least not in a way that I find fully satisfying, and I shared with other webinar attendees my vision of how one could make that mathematical object more available to the brain by way of the eye, then could I convince others, more skilled than I in the art of computer-assisted illustration, to bring my vision into reality?&lt;/p&gt;
    &lt;p&gt;The answer proved to be a resounding ‚ÄúYes!‚Äù Roice Nelson (with whom I‚Äôve corresponded in the past) was one of several people who expressed interest, and Roice and I have moved forward with this project. Arguably I shouldn‚Äôt be spending my time this way‚ÄîI don‚Äôt plan to write any research articles on the Ford spheres. I just think that they‚Äôre cool things that other people would find interesting if they were better publicized. And they got stuck in my head like a catchy tune.&lt;/p&gt;
    &lt;p&gt;AN 87-YEAR-OLD FRACTAL&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure you‚Äôve heard of fractals‚Äîthey had a moment back in the 1980s that basically never ended, with fractals penetrating not just the sciences and geek culture but popular culture as well, culminating in a line about frozen fractals in a stirring power ballad in the 2013 Disney movie Frozen. The Ford spheres form a three-dimensional fractal that not enough mathematicians know about, even though Lester Ford described it in a charming article called, simply, ‚ÄúFractions‚Äù, back in 1938‚Äîthirty-seven years before Benoit Mandelbrot coined the term ‚Äúfractal‚Äù.&lt;/p&gt;
    &lt;p&gt;There are actually many arrangements that are called Ford sphere arrangements nowadays, but the one Ford himself described looks like this:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;This image is a still from a video made by Sam Wells and Aidan Donahue. The video gives some intuition for the fractal, but (to quote another Disney movie heroine) I want more.&lt;/p&gt;
    &lt;p&gt;What makes the Ford spheres worthy of study? From a research perspective, they‚Äôre descendants of a more famous two-dimensional fractal Ford wrote about in his 1938 article. The Ford circles are geometrical surrogates for the rational numbers, and the way the circles nestle against one another turns out to reflect important facts in number theory.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;It stands to reason that the analogous three-dimensional fractals would have secrets to teach us as well.&lt;/p&gt;
    &lt;p&gt;ALL OVER THE PLACE BUT ALMOST NOWHERE&lt;/p&gt;
    &lt;p&gt;Another thing that makes the Ford spheres worthy of illustration is the way they offer math-loving non-mathematicians the chance to have their minds blown by the counterintuitive behavior of countable dense sets. The primordial example of such a set is the set of rational numbers: as elements of the real line, the rational numbers are all over the place but they‚Äôre also almost nowhere. I‚Äôve chosen my phrasing to be provocative and a little paradoxical, but in a certain mathematical sense, it‚Äôs true: hardly any real numbers are rational, but no tiniest stretch of the real line is free of them. If you zoom in on (say) the square root of two, no matter how far in you zoom, you‚Äôll keep on seeing rational numbers with ever-bigger numerators and denominators. Ford circles give geometric meaning to that bigness: the bigger those numerators and denominators are, the tinier the corresponding circles are.&lt;/p&gt;
    &lt;p&gt;All the Ford circles are tangent to a single horizontal line. One way to think about the Ford circles is as what you get when you try to pack together as many circles as you can above that line. You start by drawing evenly-spaced circles tangent to the number line at the points . . . , ‚àí2, ‚àí1, 0, 1, 2, . . . I‚Äôll just show the two circles that touch the line at 0 and 1 and hereafter ignore all the circles to the left or right of them:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add a circle to fill the gap between the 0-circle and the 1-circle, tangent to the line at 1/2):&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add more circles to fill the new gaps with tangencies at 1/3 and 2/3:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add even more circles to fill the newer gaps with tangencies at 1/4, 2/5, 3/5, and 3/4:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;If you continue this process, the circles you‚Äôll draw are precisely the Ford circles, all tangent to the line, and the points of tangency will be all the rational numbers and nothing else.&lt;/p&gt;
    &lt;p&gt;Now imagine that, having drawn in the Ford circles (or as many of them as you have the patience to draw), you add to your picture a horizontal line parallel to, but slightly above, the line we were talking about before. This new line will intersect some of the circles. If you move that new line downward slightly, it‚Äôll intersect more of the circles. As you continue to move the new line further downward, closer and closer to the original line (which I‚Äôll call the ‚Äúlimit line‚Äù), you start to intersect more and more circles.&lt;/p&gt;
    &lt;p&gt;FROM TWO TO THREE&lt;/p&gt;
    &lt;p&gt;Ford also described a similar fractal one dimension up. We have infinitely many spheres, all tangent to the x, y plane, and the points of tangency correspond exactly to the points (x, y) with x and y rational. Here‚Äôs Ford‚Äôs sketch showing four of the infinitely many spheres:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;(Yeah, four is a lot less than infinity, but cut Ford some slack: this was before computers.)&lt;/p&gt;
    &lt;p&gt;I want to picture this complicated three-dimensional object by way of its two-dimensonal cross-sections. Here‚Äôs one of the animations Roice sent me a few days ago, as part of our ongoing work:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;It shows what you get when you intersect the Ford spheres with a moving plane that approaches, without ever reaching, the limit plane that all the Ford spheres touch (analogous to the limit line that all the Ford circles touch). As time passes in the video and the moving plane moves on, we see a mix of growing disks and shrinking disks; the shrinking disks are cross-sections of the spheres whose centers the plane has already passed through, while the growing disks are cross-sections of the spheres whose centers still lie just a bit ahead of us. The picture becomes frothier and frothier, approaching the limit of infinite fractal frothiness.&lt;/p&gt;
    &lt;p&gt;The video is just a rough cut, but already I can see features of the image that I didn‚Äôt expect: halos and solar arches, one might call them. Perhaps one of you will find a way to make rigorous mathematics of what your eyes are telling you, but even if not, I hope the animation gives you visual pleasure.&lt;/p&gt;
    &lt;p&gt;WHY I BOTHER&lt;/p&gt;
    &lt;p&gt;If this essay inspires any of you to drop by one of the monthly meetings of the Illustrating Math webinar, visit the webinar link. Or, if you‚Äôre feeling brave and want to pitch an idea or to give a five-minute presentation of any kind, go to the show-and-ask signup sheet. Or if you just want to see what other cool visuals Roice has created, check out his website.&lt;/p&gt;
    &lt;p&gt;I realized shortly before I published this essay that there is a connection to my research, though it‚Äôs not a direct link, and that it was probably subconsciously driving me to explore the Ford spheres. Two decades ago I was looking at ‚Äúrotor-router blobs‚Äù that gave rise to images like this one, generated by Tobias Friedrich and Lionel Levine:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre like me, your eyes and brain see ghostly circles (or near-circles), forming bands separated by lines of orange fire. The trouble is, those near-circles are very much creations of your eyes and brain, intermediated by software called ImageMagick. The task of figuring out what details at the pixel-level create ghostly near-circles in my brain defeated me. Such are the frustrations of ‚Äúdigital pointillism‚Äù: when we zoom in, we tend to lose sight of what we are trying to understand! It‚Äôs the problem faced by creators of monumental paintings: you have to stand close to the canvas to paint your strokes or dots or whatever, but when you stand close it‚Äôs easy to literally lose sight of the big picture. I‚Äôd like to try looking at those blobs again sometime, once I have the tools and the skills to ‚Äúinterrogate‚Äù such pictures more effectively than I could in the past.&lt;/p&gt;
    &lt;p&gt;A smaller-scale version of this gap in my skill-set manifests itself for the Ford spheres. Those halos and solar arches exist in my brain (and I hope in yours), but what do they correspond to at the pixel level? I don‚Äôt know how to ask the picture to tell me, but I‚Äôm hoping I can learn.&lt;/p&gt;
    &lt;p&gt;I‚Äôll finish by mentioning one last reason for bringing the Ford spheres from the world of fancy to the world of the senses: videos like these can convey to non-mathematicians, in a way that words and symbols can‚Äôt, what makes math so addictive to those of us who love it.&lt;/p&gt;
    &lt;p&gt;Thanks to David Jacobi and Roice Nelson.&lt;/p&gt;
    &lt;p&gt;REFERENCES&lt;/p&gt;
    &lt;p&gt;L. R. Ford, Fractions, American Mathematical Monthly, 45, 586‚Äì601.&lt;/p&gt;
    &lt;p&gt;S. Northshield, Ford circles and spheres, 2015.&lt;/p&gt;
    &lt;p&gt;C. Pickover, Beauty and Gaussian Rational Numbers, Chapter 103 (pages 243-247) in: ‚ÄúWonders of Numbers: Adventures in Mathematics, Mind, and Meaning‚Äù, Oxford University Press, 2001.&lt;/p&gt;
    &lt;p&gt;S. Wells and A. Donahue, Ford spheres, 2021.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mathenchant.wordpress.com/2025/10/18/picturing-mathematics/"/><published>2025-10-18T15:52:39+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628391</id><title>Tinnitus Neuromodulator</title><updated>2025-10-18T20:35:05.088287+00:00</updated><content>&lt;doc fingerprint="b64980b949699afa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;User Stories&lt;/head&gt;
      &lt;p&gt;Write your own here. Click the blue bullets ‚óè to load associated settings.&lt;/p&gt;
      &lt;p&gt;‚óè Been loving this project, this website for years. The sounds, the stories, the community. &amp;lt;3 This particular "noise" for me is a real lifesaver. It always helps me with (tension-) headaches. Thank you so much.&lt;lb/&gt; ‚óè I couldn't think for two days straight; it was so bad and kept getting worse. This quieted the constant chatter and noise down to a mild hum.&lt;lb/&gt; ‚óè This and a special Dreaming Nautilus almost put me to sleep for some weird reason.&lt;lb/&gt; ‚óè Neural Hack sounds so funny, that one time I laughed so hard at night listening to this, that my mom came upstairs and whooped me. All jokes aside, if anyone has trouble managing their tinnitus, then this is the one for you! 8.7 out of 10.&lt;lb/&gt; ‚óè Yes, this is pretty wonderful. I only need 3 of the sliders for it to work, thank you. Why not make a standalone device for this?&lt;lb/&gt; ‚óè It takes a moment to adjust, but once you do, all your problems warble away.&lt;lb/&gt; ‚óè I feel like I'm on an alien planet!&lt;lb/&gt; ‚óè St√©phane and Steve have created a nearly magical tinnitus solution. Whenever my tinnitus starts acting up, I pop on these settings for a few minutes, and when I take the headphones off... Silence. Absolute silence. I have no idea how this works, but it's genius!&lt;lb/&gt; ‚óè An absolute godsend! It's just tickly enough to distract me from the noise in my ears, but it doesn't crowd out my actual thoughts.&lt;lb/&gt; ‚óè You wouldn't think such an exotic, high-pitched, and beepy sound generator would be calming, but when you find the right spot and let it go for a few minutes, then pause the audio, it's so much quieter‚Äînot gone, but still a relief compared to before. Even just letting it play and noticing your ear ringing contributing to the music instead of being a distracting annoyance is calming.&lt;lb/&gt; ‚óè Once properly calibrated, after a few minutes of listening, it helps reduce the perception of tinnitus‚Äîand even after removing the headphones, the sensation almost seems to vanish. myNoise is a great tool to use when needed.&lt;lb/&gt; ‚óè I have infrequent tinnitus in one ear that gets really annoying when I'm sick. I tried this on a whim, and I straight-up can't hear it anymore! What a godsend.&lt;lb/&gt; ‚óè I've been using this neuromodulator on a daily basis for many weeks and this is by far the best help I could ever find to release my tinnitus 24/7 head drill. It doesn‚Äôt get rid of it completely, but it tones it down massively - just leaves a soft background hum that lets you live your life. Honestly, it feels like magic. No idea how it works this well. Huge thanks to the creators - you guys rock!&lt;lb/&gt; ‚óè I use the tinnitus neuromodulator when things get rough. It doesn't just mask the tinnitus, it transports me into a space where I can focus again. For a while, I feel clear. What gets me is this: when I take off the headphones, the tinnitus is gone. Not reduced‚Äîgone. Then, slowly, it creeps back. That minute of silence is everything.&lt;lb/&gt; ‚óè If you have basic audio knowledge, this website is a much better option than commercial tinnitus therapies based on overpriced hearing aids. Thanks to this website, I‚Äôm reconfiguring my relationship with tinnitus in a much more organic and spontaneous way.&lt;lb/&gt; ‚óè My tinnitus is an infrequent problem. When it gets bad, though, it gets horrible. This just saved my whole morning.&lt;lb/&gt; ‚óè I don't have tinnitus, but I love the strangeness of this generator. It sounds really fascinating and seems to help me focus. By the way, I guess this is what an aural representation (a strange tone poem?) of a ketamine trip would sound like :D&lt;lb/&gt; ‚óè I use myNoise to go to sleep.&lt;lb/&gt; ‚óè This doesn't so much drown out my tinnitus as blend with it, but it's still a fantastic change from the usual! I've had tinnitus my entire life, and as a kid, I never understood why people liked silence so much since it was so loud! Weird alien beep-boops is the new silence.&lt;lb/&gt; ‚óè Wow! I don't have tinnitus, but this is stunning!&lt;lb/&gt; ‚óè This is so satisfying to listen to, and I don't even have tinnitus.&lt;lb/&gt; ‚óè It's the best website for listening to sounds while doing productive activities.&lt;lb/&gt; ‚óè I have found "trance" to work well with outside high-pitched noises like old TVs. Nice job.&lt;lb/&gt; ‚óè This is crazy, it actually works. Thankfully my tinnitus is not constant, but very annoying when it's present. It actually makes it disappear, even after I've closed the sound generator, for quite some time.&lt;lb/&gt; ‚óè This is the first time in a while that I've been able to just sit and be content without that infernal ringing hijacking my thoughts. Definitely not something I'd be able to listen to for longer periods, but it's amazing how well my tinnitus just blends in and becomes a part of the experience.&lt;lb/&gt; ‚óè This app and noise really help me in primary school, and since I am sometimes cleverer than my peers, this helps.&lt;lb/&gt; ‚óè Thanks so much for this site. My tinnitus fluctuates a lot, and when it's bad, I rely on sounds/maskers like these for work and at night.&lt;lb/&gt; ‚óè It helps me write an interesting Egyptian story and gives a nice effect.&lt;lb/&gt; ‚óè I was using this to help me focus because I have really bad tinnitus, and it makes it so hard to focus! As soon as it started playing, it drowned out my tinnitus and made me able to think way better! The sounds seem to go all around me, and it's just so helpful. I definitely recommend MyNoise!&lt;lb/&gt; ‚óè Against all odds, it apparently works. I have tested it a couple of times today by playing the audio on headphones for 5-10 minutes, and there is a significant relief in my tinnitus for shorter periods of time. I will try running the audio for a longer time to see if I can have a longer-lasting effect.&lt;lb/&gt; ‚óè It's tingling my brain.&lt;lb/&gt; ‚óè I must be doing it wrong. Unfortunately, the sound in my head got louder as I was adjusting the sliders.&lt;lb/&gt; ‚óè I needed this sound every day in 2022 when I was having major mental issues and anxiety, which created a feedback loop for my tinnitus (tinnitus increased anxiety, anxiety increased tinnitus.) with the help of this sound generator I managed to reduce my anxiety and tinnitus to the point where it's nothing more than a passing thought every now and then, that I come back here to get rid of. THANKS: )&lt;lb/&gt; ‚óè I love this soo much.&lt;lb/&gt; ‚óè I love this beyond belief! Thank you :) &lt;lb/&gt; ‚óè It's funny, the raw noises should really make me angry but they work. I don't get it. Why does it work better than any other tinnitus blocker.&lt;lb/&gt; ‚óè I don't have tinnitus; I have MS. Certain sounds have started to bother me a lot, even waking me up. I can't sleep for longer than 2 hours some nights. It's not fun. I usually have to turn some white noise on loud enough to mask the sounds but that hurts sometimes, and is hard to sleep with. This sound however is pretty weird. I don't have to crank it up loud, and it works. I'm actually sleepy.&lt;lb/&gt; ‚óè It helps me focus on reading. I like it ;)&lt;lb/&gt; ‚óè My form of tinnitus is masked by many of your noise generators. I prefer natural sounds, but this did remarkably let me forget the constant, piercing ring in my right ear rather than just mask it.&lt;lb/&gt; ‚óè Dude thank you, I've had musical ear syndrome since I was a kid, and a life of listening to heavy metal and rock and roll at high volume on headphones and going to shows has made tinnitus worse. Now I get musical ear syndrome all the time, it can be distracting as well a bit concerning but this soundscape cured it instantly. Thank you! This saved me from a mental breakdown.&lt;lb/&gt; ‚óè All points of connectivity reaching all other points.&lt;lb/&gt; ‚óè Works wonders to cancel out the Tinnitus on both of my ears. It seems to make my ringing go away for short periods of time also. &lt;lb/&gt; ‚óè Who else heard an iphone alarm and horror movie riffs? -TDR&lt;lb/&gt; ‚óè This is a god - send! My brain never quiets down, and this lovely soundscape is a perfect solution for letting my brain latch on to something while I do things I need to. Thank you so much!&lt;lb/&gt; ‚óè It might not relieve my tinnitus, but it definitely helps me focus on my homework!&lt;lb/&gt; ‚óè Works for my ADHD as well as tinnitus :3&lt;lb/&gt; ‚óè This is the ONLY thing I've found that actually helps with my tinnitus. 10-20 minutes of listening, and I'm free from it for an hour or even longer. Thank you so, so much for this.&lt;lb/&gt; ‚óè I'm still highly skeptical if this truly works but, with the noises I selected, I like it. &lt;lb/&gt; ‚óè I just recommended this to a friend with tinnitus who used to leave the TV on all night, and we found this present that I hope works better!&lt;lb/&gt; ‚óè This helps me break free from the world. Great noise.&lt;lb/&gt; ‚óè Relief from tinnitus, for the first time in years! I can't handle listening to it for much longer than twenty minutes (because I'm bored by generated noise after a while), but when I take my headphones off, my tinnitus is so massively diminished I can hardly believe it. Compared to everything else I've unsuccessfully tried for my tinnitus, this feels like actual magic--audiomancy?&lt;lb/&gt; ‚óè Wow, this actually works! I didn't expect it, but it does, and while not completely canceling out my tinnitus, works much better than songs or such.&lt;lb/&gt; ‚óè The absolute relief I feel right now has me reeling. My Jaw dropped instantly when I plugged this baby in. My head feels about half the weight it usually does just from honestly maybe a minute total of listening to this. I didn't think there would be such a solution to tinnitus but this might just really improve my quality of life, especially in the falling asleep department! Thank you, thank you!&lt;lb/&gt; ‚óè This sound helps my tinnitus a lot and it also kind of tickles my ears a bit!&lt;lb/&gt; ‚óè Suffering from tinitus for many years, this is helping me tremendously. My tinitus sounds resemble the high pitch noise you get from old TV sets. Listening to this setting for just a couple of minutes brings me relief for up to an hour afterwards, sometimes even longer because I forget to concentrate on it. Just amazing!&lt;lb/&gt; ‚óè I started using MyNoise's ocean sounds years ago to help me sleep due to severe misophonia, it blocks the noise from the street and I don't need to sleep with earplugs or headphones anymore. Recently a covid infection damaged my ear nerves and caused intolerable sound sensitivity accompanied of tinnitus and hyperacusis. If it wasn't for these sound generators I would have lost my sanity.&lt;lb/&gt; ‚óè I have autism and difficulty focusing. I've never liked listening to things like this generator, but for the first time I quite like this! I love the way that I can hear the sounds travel from each side of my brain. It's pretty awesome to listen to when you've got synesthesia. &lt;lb/&gt; ‚óè I have no tinnitus but "Neural Drops" + RPG Evil Charm on the "Monsters" preset + Canyon drone around 1 kHz is my best mix for listening to it before lucid dreaming with out-of-body experience.&lt;lb/&gt; ‚óè For a few years I've used myNoise to block out the sounds of busy city life and to get to sleep. More recently in late 2022 I contracted covid which unfortunately left me with hearing damage and significant tinnitus in one ear. The only thing that has been able to relieve the horrible ringing so far are some of the neuromodulator tracks here, and for that I am grateful. &lt;lb/&gt; ‚óè As an autistic person with pretty debilitating hyperacusis (I pretty much can't function without ear plugs) the onset of some mild tinnitus was distressing to say the least. White noise is usually the recommended go-to for tinnitus but it's painful to me. High pitched fuzzy or low constant humming sounds are literal agony. This is varied enough to ease and accommodate both issues. Miraculous!&lt;lb/&gt; ‚óè Well it just works. I get into music stuff lately after a long session of mixing, mastering or drum take your ear just start to deafen sometimes with headaches. I used this as a tool to "reset" my ear by listening to it at a barely heard loudness and turning it down even more after I can hear it clearly, just rinse and repeat!&lt;lb/&gt; ‚óè Just wanted to add my comment to say that yes, in my experience (loud ~9kHz in left ear) this is more than a placebo effect re: tinnitus relief. HOW exciting. Cheers! Breathing deeper and slower too.&lt;lb/&gt; ‚óè When getting the automation right, it takes ages for piano or guitar sounds to get repetitive, sounds of nature and human voices (radio chatter etc) sound completely natural, organic, and at home here without feeling repetitive at all. And sci-fi sounds, and drone like instruments can pretty much do their thing forever and play well off of each other while they do it.&lt;lb/&gt; ‚óè Suffering since 11 months from tinnitus. I destressed my life, did different types of therapy but nothing came close to the effect this neural symphony has. It actually gives me seconds or minutes as close to silence as I wasn't for 11 months. THANK YOU!&lt;lb/&gt; ‚óè Amazing, always helps when silence is a bit too noisy. My uncle who has bad tinnitus commented that it helped even after a few minutes without headphones... Amazingly helpful in this era of being used to hearing fans, fridges, pinging tech and then ringing noises when it's supposed to be quiet.&lt;lb/&gt; ‚óè Wow, so interesting. It fades away quickly. Greetings from Ecuador.&lt;lb/&gt; ‚óè It takes away my tinnitus if I listen to if for a few minutes. I genuinely cannot believe this. &lt;lb/&gt; ‚óè I have had tinnitus for several years and it recently got worse causing anxiety attacks. After discovering this site I found it really works on lessening the tinnitus and I feel so much better! I play it while working from home. Amazing! &lt;lb/&gt; ‚óè I developed a high pitched whistling tinnitus 10 years ago when I was 14 and I thought I lost the sound of silence forever, but listening to this for 20 minutes on shuffle makes my tinnitus disappear completely! Even if it's just for a little while, it feels wonderful to experience true silence again.&lt;lb/&gt; ‚óè Peaceful and relaxing!&lt;lb/&gt; ‚óè Strangely, this also blocks out when my ears begin to ring highly.&lt;lb/&gt; ‚óè I have tinnitus that sounds like a cat purring. The "purring" is annoying and it causes me to think a cat is near me, a car is starting, Etcetera. These settings work strangely well.&lt;lb/&gt; ‚óè I luckily don't have tinnitus but this sounds like I've been taken by aliens! &lt;lb/&gt; ‚óè Yes!! It helps!!&lt;lb/&gt; ‚óè It definitely assisted me with my tinnitus, I feel substantially more relaxed as well 10/10. &lt;lb/&gt; ‚óè Gone! My tinnitus sounds like a dentist's drill. Sometimes it's hard to blend it out, but with this setting it's gone. Thanx, Doc!&lt;lb/&gt; ‚óè Silence! Silence, thank god! I've never felt so much joy.&lt;lb/&gt; ‚óè I dont have tinnitus but this is still a cool sound to study to.&lt;lb/&gt; ‚óè The tinnitus neuromodulator sounds empty and sinister by itself. To make it less empty and sinister, the sinewave stereo slider can combine well with the take it easy generator on the bad trip preset.&lt;lb/&gt; ‚óè This helps with misophonia, too! I put it at a low volume and listen to it during class, it's super helpful.&lt;lb/&gt; ‚óè I was a bit skeptical, but after listening for 10 minutes on a moderately high volume with ear buds, I can say the tinnitus relief is real! I'm now trying to find out how long the relief will last, and how long I should listen to the modulation sounds... &lt;lb/&gt; ‚óè I don't even have tinnitus, but I like listening to this while I write my sci-fi novel!&lt;lb/&gt; ‚óè Playing with settings was a lot of fun! I masked my tinnitus with it!&lt;lb/&gt; ‚óè It was fun playing with the settings and I lowered some and raised some, slowed it down a bit, and it's really wonderful. I think it helps with my tinnitus but it's also just fun to listen to!&lt;lb/&gt; ‚óè Wow! Being able to hear pure silence even only for a few seconds is truly something. Thank you.&lt;lb/&gt; ‚óè Ahhh this is literally so amazing, it actually helped so so so so so so much!! It got so much quieter after I took my headphones off.&lt;lb/&gt; ‚óè This.... Is good. So good. &lt;lb/&gt; ‚óè Amazing, amazing, amazing for my tinnitus. It's not especially loud, but it's very constant when it's quiet, and having this setting is surprisingly relaxing for my mind to not hear it in the background.&lt;lb/&gt; ‚óè Doesn't get rid of the tinnitus, but masks it while I listen to it.&lt;lb/&gt; ‚óè Suddenly had an onset of tinnitus last night. This setting in particular seems to be holding it down for me while I wait for my trip to an ENT doctor.&lt;lb/&gt; ‚óè Tried a lot of relief sound from youtube, but most of them are too high pitched for me and are very painful. But this one is perfect, you saved my life thank you.&lt;lb/&gt; ‚óè I constantly hear ringing whenever it's silent, and it affects my day-to-day tasks. I used this for a while, and I went somewhere silent, and there you go, no more ringing! Mind blown!&lt;lb/&gt; ‚óè Recently I had been dealing with pretty bad tinnitus, it had gotten to the point where I couldn't even fall asleep some nights. After one day of using this, I have had more relief than I have had in weeks. I am very grateful for this program, I have already donated and plan to continue supporting this.&lt;lb/&gt; ‚óè I suffer from tinnitus due to infections, and this is wonderful! &lt;lb/&gt; ‚óè I don't even have tinnitus, I just find it a cool, emotionally neutral background to stay focused at work... It feels like I'm coding on an alien spaceship!&lt;lb/&gt; ‚óè I can't say if this will work for everyone's tinnitus, but it does for mine! After listening for 20 minutes or so, my tinnitus is much reduced and stays that way for up to two hours afterward. I am interested to see if there is any long-term effect from using this generator daily.&lt;lb/&gt; ‚óè Thank you so much! My tinnitus began after a concussion 6 years ago. I have no hearing problems, just issues caused by my brain. I found your website, turned on my computer speakers &amp;amp; immediately my neck &amp;amp; shoulders relaxed as the tinnitus decreased significantly. Initially I listened for about 1/2 hour &amp;amp; when I walked away it was much lower volume than before. Best for me on the default setting.&lt;lb/&gt; ‚óè This generator is completely amazing, if for no other reason it detracts attention away from my tinnitus, which is now chronic for years and years. The Neural Symphony functions are easy to use and adjust. I have discovered that for me I drop all pulses and warble to zero, and then use the automated slider animation. Wide range of high frequencies, ever changing. Bliss.&lt;lb/&gt; ‚óè ‚Üê Click here to hear some crazy hums and beeps! &lt;lb/&gt; ‚óè This was overwhelming for me at first, but I clicked Surprise! a few times and got settings that not only hide my tinnitus but also make a pleasing sound. I love it.&lt;lb/&gt; ‚óè I have tinnitus in scattered frequencies between 8-10k, and this setting on studio monitors (at a quiet volume) works very well to calm my mind, and get my focus off of audio based stimuli. &lt;lb/&gt; ‚óè It works o_0. Tinnitus is not big problem for me because I have it since I was kid, but it's nice feeling to not hear it :D&lt;lb/&gt; ‚óè I loved it, it's kind of magical too.&lt;lb/&gt; ‚óè It's like C3PO had too much sugar lolololololol!&lt;lb/&gt; ‚óè I don't have tinnitus. But this sounds crazy! I love it! 8D&lt;lb/&gt; ‚óè Neuromodulator and Summer Night are about the only things that got me through the worst bout of tinnitus. Thank you so much!&lt;lb/&gt; ‚óè Oh my goodness. You are an absolute godsend; I wish I could have found this sooner. Will def be using this whenever I need it. Thank you so so so so much! &amp;lt;3&lt;lb/&gt; ‚óè I use this daily to focus at work and to get my mind off of my tinnitus. Also, I use this to sleep on my phone, this is the best thing that I've had in a long time.&lt;lb/&gt; ‚óè Thanks -- this doesn't seem to "fix" anything but it does help concentrate with the tinnitus is bothering me. I have great hearing but tinnitus (with high-pitched sounds in one ear and a tiny bit in the other) started a couple weeks ago after my doctor put me on 150mg Effexor. We cut the medication after this happened but the tinnitus is still loud :( Anyway thanks for your work on this site! &lt;lb/&gt; ‚óè I fortunately don't have tinnitus but it helped with my daily earaches anyway.&lt;lb/&gt; ‚óè I thought people were overreacting, my God I was wrong. It truly works.&lt;lb/&gt; ‚óè I have used myNoise numerous times now and I find great relief from my very loud and annoying tinnitus. Waiting to see if any reversal becomes evident. Thank you for this wonderful tool! &lt;lb/&gt; ‚óè It was not working for me at first. The tinnitus would go away and come back after a few seconds. I tweaked the settings so the high pitched noises were louder and more frequent. A day after I am writing this with little to no tinnitus now. Idk if this really works or not but it's worth a try.&lt;lb/&gt; ‚óè I suffer from tinnitus in both the ears and have two distinct high and low frequency patterns. I listened this for about 45 mins, kept away my headphones and then went for a coffee... Guess what I could hear `silence'! Very promising!&lt;lb/&gt; ‚óè I found that all the sounds hurt to listen to and increased or battled with my tinnitus. Disappointing! &lt;lb/&gt; ‚óè This is certainly promising, and I did notice an affect pretty quickly. However, ACRN should be tunable to the frequency of of your tinnitus. The Steve Sequence is kind of all over the place, and mostly tuned lower than my own (6kHz, and pretty common). Would be nice to see something where the frequency can be tuned, and maybe change the waveform (sine, sawtooth, fuzz, etc). [Note from editor : try the Tape Speed Control feature]&lt;lb/&gt; ‚óè Gosh that's impressive, Thank you so much for your work&lt;lb/&gt; ‚óè I don't have tinnitus, but this is really relaxing with the right settings.&lt;lb/&gt; ‚óè I have suffered tinnitus for as long as I can remember, very few things have helped lessen its obnoxious effects, and those that did (like music) were very distracting (I have ADHD too), but this generator works perfectly for me, it helps my tinnitus to the point that I don't even notice it, and even better, its not distracting in any way! Focusing has never been easier.&lt;lb/&gt; ‚óè Try Lake Life (loon calls and lapping shore water) with this - pretty funky. Keep expecting a strange interstellar artifact to light in the depths of the lake and rise in the full moon before me. Fantastical.&lt;lb/&gt; ‚óè Love it with Hydrogen XII.&lt;lb/&gt; ‚óè OH MY GOSH BEAUTIFUL SILENCE&lt;lb/&gt; ‚óè This literally worked in 10 seconds... wow!&lt;lb/&gt; ‚óè This noise generator gave me nausea and vertigo and my tinnitus was unaffected. Am I using it wrong?&lt;lb/&gt; ‚óè I must say that I didn't really have faith in it working to dampen my tinnitus, but I had to eat my words back when I took my headphones off. I was close to shedding tears at finally being close to experiencing silence after all these years of constant noise. I want to thank you for creating this, and I am most definitely going to favorite this and listen to it as much as possible.&lt;lb/&gt; ‚óè While I may not have Tinnitus, this generator is still a very entertaining experience! I sometimes use it to calm myself or focus on writing, which was surprising as I thought it would have the opposite effect!&lt;lb/&gt; ‚óè Yes. Really does improve my tinnitus. Even more, I find it actually quite soothing as ambience for indoor work - less aural fatigue than white noise. For some reason, it even makes house-cleaning less tedious :-) Kudos!&lt;lb/&gt; ‚óè As a musician, I have to deal with tinnitus on the daily. It got bad this morning, and this helped me out a ton.&lt;lb/&gt; ‚óè The tinnitus neuromodulation noises have had a proufound effect on lessening my tinnitus and I thoroughly recommend it to anyone who is seeking relief. Short of a future breakthrough in medication or surgical treatments, this is the best solution I have come across and it makes life that bit more bearable. Many, many, many thanks!&lt;lb/&gt; ‚óè After listening to the Sinescape preset for this generator for only maybe 20 minutes, I found that my rather loud tinnitus because much quieter and almost non-existent immediately after removing my headphones. I'm curious to see how long this lasts, but it's quite a relief even for a little while!&lt;lb/&gt; ‚óè This generator is the first thing that helps with my tinnitus, lowering its pitch and volume until it's almost gone even after I took off the headphones. The relief only lasts for a few hours, but to someone who has a constant high pitched noise in their ears otherwise, those hours mean the world. I cannot thank you enough for your amazing work, Stphane. You're a true blessing to the world.&lt;lb/&gt; ‚óè After listening to this for awhile, my tinnitus was actually GONE after I took off my headphones.I don't know how long this will last but thank you. I will enjoy this sweet, sweet silence while it lasts. Donation coming your way as soon as I get home! Love this site.&lt;lb/&gt; ‚óè My tinnitus is quite high-pitched and fairly constant. If I play this sequence for a little while, it brings it back down under a low roar, if only for a few hours. &amp;lt;3&lt;lb/&gt; ‚óè I cannot express my thanks! Every sound has changed my life in a different way! Thank you SO MUCH! Thank You!!! These settings seem to be the ultimate cure to my T.&lt;lb/&gt; ‚óè ‚Üê extreme warbling&lt;lb/&gt; ‚óè Holy Crap! I was experiencing on of my usual Tinnitus spikes (and you know how annoying this can be). I tried this custom noise take from one of the testimonials and my tinnitus just went down from 10 to 4! Thanks!&lt;lb/&gt; ‚óè Brilliant site I could spend hours here. I have tinnitus a different high pitched frequency in each ear.Slightly spacey space-station? A little dreamy but a little sedating as well and kills the tinnitus.&lt;lb/&gt; ‚óè This works for me - a little dreamy but a little sedating as well.&lt;lb/&gt; ‚óè This is a very strange soundscape, but in an enjoyable way. I personally do not have tinnitus, but I think this is still fun to listen to. You just have to find the right sliders to make sense of this one, hahaha.&lt;lb/&gt; ‚óè That's artificial rain... trickle, and some wind. Very relaxing during computer work! Both stimulating and calming. :)&lt;lb/&gt; ‚óè I have Tinnitus. I also have perfect hearing and from years of being a musician one would anticipate hearing loss. None. I am also a Director for an electric and water utility and I have a lot of concentrating work (reports, analysis, etc). When My Tinnitus is in full flight, it is impossible to concentrate. This is a life saver. I can work all day and feel the calming of the tones. Amazing! &lt;lb/&gt; ‚óè I am sitting in the library trying to work and someone near me has the touch-tones turned all the way up on their phone. This is the only thing that drowns it out. Thank you!!!&lt;lb/&gt; ‚óè Back again, THANK GOD for this. When I'm having a rough day with my T, I just come home, throw this on and chill and just zone out and stuff I love it. Thank you!&lt;lb/&gt; ‚óè May be my imagination, listened for about 15 min, then when taking my speakers away from my ears, my pulsating tinnitus was quieter.&lt;lb/&gt; ‚óè First I found myself heaving a relieved sigh, and then started crying when I found the custom noise that masked my tinnitus. It has been five months of unrelenting high pitched whine, yet I refused to allow myself a pity party, concluding, "It is what it is; suck it up, girl, there is little you can do." But to have this respite is a great blessing. Thank you! &lt;lb/&gt; ‚óè Kudos to all who put this wonderful chaotic neural symphony together! As a tinnitus sufferer I found the sounds had a very soothing effect on me. I will be using this selection again for sure.&lt;lb/&gt; ‚óè I noticed listening to music with different frequencies helps soften my Tinnitus. I have a constant 24/7 14,000hz sawtooth sound in my head for the two years. This program is amazing! &lt;lb/&gt; ‚óè It is soothing but I hear my tinnitus while I am listening to it. I don't really think this would help me in any way, long term. It is soothing though, so thanks for that! &lt;lb/&gt; ‚óè Not for me. Sounds more like star wars. What really helps me is a soft noise plug. Stick in the ear canal and when the sounds starts bothering switch to other ear. This is the only thing that helps me.&lt;lb/&gt; ‚óè This is incredible. I've had a fairly mild, but annoying tinnitus accompanying me for some time now and this Neural Hack makes it seem magically inexistent. I am impressed (and very relaxed).&lt;lb/&gt; ‚óè I have been listening to music with MY Tinnitus frequency embedded into the music. I understand this works for some, not me. After over 6 months, no change. The past two days have been horrible... I am a musician and although I have been playing rock music for over 40 years, I have no hearing damage at all... 20 minutes on customizing my own pattern, I can actually concentrate on my work!&lt;lb/&gt; ‚óè Thank you so much for this incredible site! I'm quite happy to make a donation to you out of simple gratitude for all your efforts putting these sounds online for us, and keeping the site (and your iOS app, myNoise) working. I've experienced persistent tinnitus for a couple decades, now, with almost no respite except after one minutes or so of "Neural Hack" or "Neural Symphony!" Amazing!&lt;lb/&gt; ‚óè I have severe tinnitus and this noise generator may actually change my life.&lt;lb/&gt; ‚óè Thank you so much for this. It provides relief for my tinnitus when nothing else does.&lt;lb/&gt; ‚óè This site is amazing! Brilliant concept and execution! As a Tinnitus sufferer, it has become safe haven for me. Thank you so much, Stephane!&lt;lb/&gt; ‚óè I'm a fairly new comer to myNoise having serendipitously discovered the site only three weeks ago and a very long sufferer of Tinnitus. I have only discovered this particular generator only moments ago and jumped right in to listen. Ironically, I've been having episodes all week so I'll absolutely write the results of my experience after one week and again at two weeks. Today is 8 June '16 &lt;lb/&gt; ‚óè Something very weird happens if I listen to this for 15-20 minutes: in addition to the tinnitus masking, my mind actually "quiets down". The parts of my brain that would normally be wandering seem to be occupied chasing down the bells and warbles, allowing the rest of my attention to focus entirely on the task at hand. Awesome brain hack.&lt;lb/&gt; ‚óè I don't have tinnitus, but I find myself enjoying the bizarre dreamlike discord that using the animation sliders can provide. It's a rather lovely chiming chaos... that is weirdly calming to me. I'm not sure what that says :)&lt;lb/&gt; ‚óè My goodness, I've been waiting for something like this. God Bless!&lt;lb/&gt; ‚óè I guess I am fortunate that I only get ringing in my ears when I take certain prescription medications I can't believe my luck that a few hours after I took the medication I started to get the ringing and came here to find some rain sounds and discovered this new generator. What a gift this is! Thank you, thank you, thank you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://mynoise.net/NoiseMachines/neuromodulationTonesGenerator.php"/><published>2025-10-18T16:08:10+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45628761</id><title>Liva AI (YC S25) Is Hiring</title><updated>2025-10-18T20:35:04.093723+00:00</updated><content>&lt;doc fingerprint="66cee20134c3ed39"&gt;
  &lt;main&gt;
    &lt;p&gt;Scale AI for video and voice data.&lt;/p&gt;
    &lt;p&gt;As part of Liva's engineering team, you'll have end-to-end ownership over projects that define how frontier AI data is collected, validated, and quality-assured. You'll work on a mix of platform engineering and infrastructure development (scaling data platforms, building human-in-the-loop systems, designing evaluation pipelines).&lt;/p&gt;
    &lt;p&gt;You might build a mobile app for data collection one week, design an automated QA pipeline with intelligent validation systems the next, and then improve our annotation platforms. You‚Äôll also play a key role in developing AI agent‚Äìdriven workflows that automate core operations.&lt;/p&gt;
    &lt;p&gt;WHAT YOU‚ÄôLL DO:&lt;/p&gt;
    &lt;p&gt;REQUIRED SKILLS:&lt;/p&gt;
    &lt;p&gt;BENEFITS:&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We‚Äôre fixing that by building the world‚Äôs richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.ycombinator.com/companies/liva-ai/jobs/inrUYH9-founding-engineer"/><published>2025-10-18T17:02:22+00:00</published></entry></feed>