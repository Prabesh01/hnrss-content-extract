<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-12-09T06:17:43.658707+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=46192459</id><title>Paramount launches hostile bid for Warner Bros</title><updated>2025-12-09T06:17:50.773187+00:00</updated><content>&lt;doc fingerprint="3a38c554c43cafac"&gt;
  &lt;main&gt;
    &lt;p&gt;Paramount Skydance is launching a hostile bid to buy Warner Bros. Discovery after it lost out to Netflix in a monthslong bidding war for the legacy assets, the company said Monday.&lt;/p&gt;
    &lt;p&gt;Paramount will go straight to WBD shareholders with an all-cash, $30 per share offer. That's the same bid WBD rejected last week and equates to an enterprise value of $108.4 billion.&lt;/p&gt;
    &lt;p&gt;The offer is backstopped with equity financing from the Ellison family and the private equity firm RedBird Capital as well as $54 billion in debt commitments from Bank of America, Citi and Apollo Global Management, Paramount said in a news release.&lt;/p&gt;
    &lt;p&gt;A portion of the equity financing comes from outside Middle Eastern financing partners including Saudi Arabia's Public Investment Fund, Abu Dhabi's L'imad Holding Company PJSC, and the Qatar Investment Authority. Another portion derives from Jared Kushner's Affinity Partners. Kushner is U.S. President Donald Trump's son-in-law.&lt;/p&gt;
    &lt;p&gt;Those partners have agreed to "forgo any governance rights," including board seats, as part of their non-voting equity investment, according to a Paramount filing. The modifications allow the deal to be outside of the jurisdiction of the Committee on Foreign Investment in the U.S., or CFIUS.&lt;/p&gt;
    &lt;p&gt;Shares of Paramount gained 9% Monday. Warner Bros. Discovery's shares rose about 4% while Netflix was down 3%.&lt;/p&gt;
    &lt;p&gt;"We're really here to finish what we started," Paramount Skydance CEO David Ellison told CNBC's "Squawk on the Street" on Monday. "We put the company in play."&lt;/p&gt;
    &lt;p&gt;Paramount Skydance began its hunt for Warner Bros. Discovery in September, submitting three bids before WBD launched a formal sale process that ultimately brought in other suitors.&lt;/p&gt;
    &lt;p&gt;On Friday, Netflix announced a deal to acquire WBD's studio and streaming assets for a combination of cash and stock, valued at $27.75 per WBD share, or $72 billion. Paramount had been bidding for the entirety of Warner Bros. Discovery, including those assets and the company's TV networks like CNN and TNT Sports.&lt;/p&gt;
    &lt;p&gt;"We're sitting on Wall Street, where cash is still king. We are offering shareholders $17.6 billion more cash than the deal they currently have signed up with Netflix, and we believe when they see what it is currently in our offer that that's what they'll vote for," Ellison said.&lt;/p&gt;
    &lt;p&gt;Ellison said Monday he places a value of $1 per share on the linear cable assets, which are set to trade as a separate public entity called Discovery Global in mid-2026. WBD executives have privately valued the assets closer to $3 per share.&lt;/p&gt;
    &lt;p&gt;Paramount has repeatedly argued to the WBD board of directors that keeping Warner Bros. Discovery whole is in the best interest of its shareholders.&lt;/p&gt;
    &lt;p&gt;Paramount made a bid on Dec. 1 and heard back from WBD that it needed to make certain alterations to the offer, Ellison said Monday. When Paramount made the changes and upped its bid to $30 per share, Ellison never heard back from WBD CEO David Zaslav, he said.&lt;/p&gt;
    &lt;p&gt;Ellison said he told Zaslav via text message that $30 per share wasn't the company's best and final offer, suggesting the company is willing to bid higher still.&lt;/p&gt;
    &lt;p&gt;Ellison argued Paramount's deal will have a shorter regulatory approval process given the company's smaller size and friendly relationship with the Trump administration. He called Trump a believer "in competition" and said Paramount's combination with WBD will be "a real competitor to Netflix, a real competitor to Amazon."&lt;/p&gt;
    &lt;p&gt;Ellison also threw cold water on Netflix's chances of regulatory approval.&lt;/p&gt;
    &lt;p&gt;"Allowing the No. 1 streaming service to combine with the No. 3 streaming service is anticompetitive," Ellison said.&lt;/p&gt;
    &lt;p&gt;CNBC reported Friday that the Trump administration was viewing the deal with "heavy skepticism," and Trump said Sunday that the market share considerations could pose a "problem."&lt;/p&gt;
    &lt;p&gt;Netflix agreed to pay Warner Bros. Discovery $5.8 billion if the deal is not approved, according to a Securities and Exchange Commission filing Friday. Warner Bros. Discovery said it would pay a $2.8 billion breakup fee if it decides to call off the deal to pursue a different merger.&lt;/p&gt;
    &lt;p&gt;Netflix, for its part, once again championed the deal as positive for shareholders, consumers and the media industry as a whole when its top leadership spoke at the UBS Global Media and Communications Conference on Monday.&lt;/p&gt;
    &lt;p&gt;Co-CEO Greg Peters said they recognize the Netflix deal came as a shock but called the Warner Bros. studio and HBO Max content complementary to Netflix's business.&lt;/p&gt;
    &lt;p&gt;Co-CEO Ted Sarandos said the acquisition would protect jobs at a time when layoffs have been rampant across media: "In the offer that Paramount was talking about today, they also were talking about $6 billion of synergies. Where do you think synergies come from? Cutting jobs. So we're not cutting jobs, we're making jobs."&lt;/p&gt;
    &lt;p&gt;— CNBC's Lillian Rizzo contributed to this report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.cnbc.com/2025/12/08/paramount-skydance-hostile-bid-wbd-netflix.html"/><published>2025-12-08T14:16:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46192846</id><title>Strong earthquake hits northern Japan, tsunami warning issued</title><updated>2025-12-09T06:17:50.503135+00:00</updated><content>&lt;doc fingerprint="63a0b14dfe71bc79"&gt;
  &lt;main&gt;
    &lt;p&gt;A magnitude 7.5 earthquake struck northern Japan on Monday. Tsunami advisories have been lifted for the Pacific coastline in northern Japan. But officials have issued an alert for a potential megaquake in northern Japan.&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong tremors felt across the region&lt;/head&gt;
    &lt;p&gt;The earthquake struck off the eastern coast of Aomori Prefecture at 11:15 p.m. on Monday.&lt;/p&gt;
    &lt;p&gt;The Japan Meteorological Agency has downgraded the magnitude of the quake centered off the Pacific coast in Aomori Prefecture to 7.5 from 7.6.&lt;/p&gt;
    &lt;p&gt;The depth has also been adjusted to 54 kilometers, from an initial estimate of 50 kilometers.&lt;/p&gt;
    &lt;p&gt;Tremors with an intensity of upper 6 on the Japanese intensity scale of 0 to 7 were observed in the city of Hachinohe in Aomori Prefecture.&lt;/p&gt;
    &lt;p&gt;As of 1:00 a.m., six people in Aomori have been injured by either falling down or getting hit by falling objects at their homes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tsunami advisories lifted&lt;/head&gt;
    &lt;p&gt;Authorities had issued a tsunami warning for Iwate Prefecture and parts of Hokkaido and Aomori.&lt;/p&gt;
    &lt;p&gt;At Kuji Port in Iwate, a tsunami measuring 70 centimeters was observed. In Hokkaido, a 50-centimeter tsunami was seen in Urakawa Town and a 40-centimeter tsunami was observed at Mutsuogawara Port in Aomori Prefecture.&lt;/p&gt;
    &lt;p&gt;The Japan Meteorological Agency says: it is the first time the agency has issued a tsunami warning since July, when a powerful quake off Kamchatka, Russia, prompted it to issue one for Japan's Pacific coastal areas.&lt;/p&gt;
    &lt;p&gt;Over 3 hours later, authorities downgraded the tsunami warning to advisories. And they lifted all tsunami advisories for the Pacific coastline of northern Japan at 6:20 a.m. on Tuesday.&lt;/p&gt;
    &lt;head rend="h3"&gt;'Long-period ground motions' recorded&lt;/head&gt;
    &lt;p&gt;According to authorities, long-period ground motions were recorded during the Monday earthquake.&lt;/p&gt;
    &lt;p&gt;The motions are slow, large-amplitude seismic waves with frequencies of 2 seconds or longer that occur during a large earthquake. Such motions are known to have a significant impact on high-rise buildings.&lt;/p&gt;
    &lt;p&gt;Strong long-period motions, classified class-3, the second highest in the 4-level scale were observed in the village of Rokkasho in Aomori Prefecture. Such class-3 waves are strong enough to make it difficult for people in a high-rise building to stand up.&lt;/p&gt;
    &lt;head rend="h3"&gt;'An alert for a potential mega quake' issued&lt;/head&gt;
    &lt;p&gt;Officials at Japan's Meteorological Agency have issued an alert for a potential mega quake following Monday's quake.&lt;/p&gt;
    &lt;p&gt;A mega quake could trigger tsunami along Japan's Pacific coast from Hokkaido to Chiba Prefecture.&lt;/p&gt;
    &lt;p&gt;Officials are urging people to check evacuation routes, prepare emergency kits, secure home furniture and confirm backup food, water and portable toilets.&lt;/p&gt;
    &lt;p&gt;People along the Pacific coast in those areas should remain on the alert during the next week, even though an evacuation recommendation will not be issued.&lt;/p&gt;
    &lt;p&gt;The alert is the first since this category of warning was started in 2022.&lt;/p&gt;
    &lt;p&gt;Morikubo Tsukasa, Cabinet Office official for disaster preparedness, has held a news conference over a potential mega quake.&lt;/p&gt;
    &lt;p&gt;Morikubo: Based on the statistics of earthquakes that have occurred around the world so far, there is a possibility that a large-scale earthquake with a magnitude of 8 or higher could occur as a follow-up earthquake along the Japan Trench and the Chishima Trench off Hokkaido. It is unclear whether a large-scale earthquake will occur. But everyone should heed the call to take precaution to protect their own lives.&lt;/p&gt;
    &lt;head rend="h3"&gt;Traffic partially affected on Tuesday&lt;/head&gt;
    &lt;p&gt;East Japan Railway Company says that the Tohoku Shinkansen bullet train services have been suspended between Morioka and Shin-Aomori stations due to the earthquake.&lt;/p&gt;
    &lt;p&gt;The company says that it is checking for any damage to railway tracks and that it plans to resume its services at 3:00 p.m.&lt;/p&gt;
    &lt;p&gt;All Nippon Airways and Japan Airlines say they are operating as usual starting Tuesday.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Companies: No abnormalities at nuclear plants&lt;/head&gt;
    &lt;p&gt;Tokyo Electric Power Company says it has confirmed that there are no abnormalities at the Fukushima Daiichi and Daini nuclear plants.&lt;/p&gt;
    &lt;p&gt;The company says it halted the release of treated and diluted water from the Fukushima Daiichi nuclear power plant at 11:42 pm on Monday, as per predetermined procedures.&lt;/p&gt;
    &lt;p&gt;The facility suffered a triple meltdown during the March 2011 earthquake and tsunami. The water used to cool molten fuel has been mixing with rain and groundwater.&lt;/p&gt;
    &lt;p&gt;That has been treated to remove most radioactive substances, except tritium. It's then diluted, reducing levels of tritium to well below the World Health Organization's guidance for drinking water, before it is released into the ocean.&lt;/p&gt;
    &lt;p&gt;TEPCO also ordered some employees at the facility to evacuate. There have been no reports so far of injuries at the nuclear power plant.&lt;/p&gt;
    &lt;p&gt;Tohoku Electric Power Company says no abnormalities have been detected at the Higashidori nuclear power plant in Aomori Prefecture and the Onagawa plant in Miyagi Prefecture.&lt;/p&gt;
    &lt;p&gt;Hokkaido Electric Power Company says no problems have been found at the Tomari nuclear power plant in the prefecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Government bracing for damages&lt;/head&gt;
    &lt;p&gt;The Japanese government set up a task force at the crisis management center in the prime minister's office at 11:16 p.m. on Monday in response to the earthquake.&lt;/p&gt;
    &lt;p&gt;Prime Minister Takaichi Sanae entered the prime minister's office shortly after 11:50 p.m.&lt;/p&gt;
    &lt;p&gt;She instructed the government to immediately provide information on any tsunami and evacuation orders to the people in an appropriate manner, take thorough measures to prevent harm, such as evacuating residents, and get a grasp of the extent of damage as soon as possible.&lt;/p&gt;
    &lt;p&gt;Takaichi: The central government will work closely with local governments and make the utmost effort to carry out measures, such as emergency response, including rescue for the affected people.&lt;/p&gt;
    &lt;p&gt;Chief Cabinet Secretary Kihara Minoru held a news conference on Tuesday. Kihara said the government continues to assess the extent of the damage.&lt;/p&gt;
    &lt;p&gt;He added that the government is devoting all its efforts to disaster prevention measures, with rescue and relief efforts as its top priority, led by the police, fire departments, Self-Defense Forces, and Japan Coast Guard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expert view on the quake&lt;/head&gt;
    &lt;p&gt;Sakai Shinichi, professor at the Earthquake Research Institute of the University of Tokyo, says: If this was a shallow earthquake centered in the sea, there is a high possibility that a tsunami has already occurred. People should stay away from the coast. It is important to evacuate and to take measures to stay warm.&lt;/p&gt;
    &lt;p&gt;Sakai says: The epicenter may be north of the epicenter area of the 2011 Great East Japan Earthquake. This time, the earthquake is believed to have occurred at the plate boundary, so I think it was a slightly larger earthquake. The magnitude could be revised in the future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www3.nhk.or.jp/nhkworld/en/news/20251209_02/"/><published>2025-12-08T14:50:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46193931</id><title>AMD GPU Debugger</title><updated>2025-12-09T06:17:49.915697+00:00</updated><content>&lt;doc fingerprint="ae1ad8512d473b89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AMD GPU Debugger&lt;/head&gt;
    &lt;p&gt;I’ve always wondered why we don’t have a GPU debugger similar to the one used for CPUs. A tool that allows pausing execution and examining the current state. This capability feels essential, especially since the GPU’s concurrent execution model is much harder to reason about. After searching for solutions, I came across rocgdb, a debugger for AMD’s ROCm environment. Unfortunately, its scope is limited to that environment. Still, this shows it’s technically possible. I then found a helpful series of blog posts by Marcell Kiss, detailing how he achieved this, which inspired me to try to recreate the process myself.&lt;/p&gt;
    &lt;head rend="h1"&gt;Let’s Try To Talk To The GPU Directly&lt;/head&gt;
    &lt;p&gt;The best place to start learning about this is RADV. By tracing what it does, we can find how to do it. Our goal here is to run the most basic shader &lt;code&gt;nop 0&lt;/code&gt; without using Vulkan, aka RADV in our case.&lt;/p&gt;
    &lt;p&gt;First of all, we need to open the DRM file to establish a connection with the KMD, using a simple open(“/dev/dri/cardX”), then we find that it’s calling &lt;code&gt;amdgpu_device_initialize&lt;/code&gt;, which is a function defined in &lt;code&gt;libdrm&lt;/code&gt;, which is a library that acts as middleware between user mode drivers(UMD) like &lt;code&gt;RADV&lt;/code&gt; and and kernel mode drivers(KMD) like amdgpu driver, and then when we try to do some actual work we have to create a context which can be achieved by calling &lt;code&gt;amdgpu_cs_ctx_create&lt;/code&gt; from &lt;code&gt;libdrm&lt;/code&gt; again, next up we need to allocate 2 buffers one of them for our code and the other for writing our commands into, we do this by calling a couple of functions, here’s how I do it:&lt;/p&gt;
    &lt;code&gt;void bo_alloc(amdgpu_t* dev, size_t size, u32 domain, bool uncached, amdgpubo_t* bo) {
 s32    ret         = -1;
 u32    alignment   = 0;
 u32    flags       = 0;
 size_t actual_size = 0;

 amdgpu_bo_handle bo_handle = NULL;
 amdgpu_va_handle va_handle = NULL;
 u64              va_addr   = 0;
 void*            host_addr = NULL;&lt;/code&gt;
    &lt;p&gt;Here we’re choosing the domain and assigning flags based on the params, some buffers we will need uncached, as we will see:&lt;/p&gt;
    &lt;code&gt; if (
   domain != AMDGPU_GEM_DOMAIN_GWS &amp;amp;&amp;amp; domain != AMDGPU_GEM_DOMAIN_GDS &amp;amp;&amp;amp;
   domain != AMDGPU_GEM_DOMAIN_OA) {
  actual_size = (size + 4096 - 1) &amp;amp; 0xFFFFFFFFFFFFF000ULL;
  alignment   = 4096;
  flags       = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED | AMDGPU_GEM_CREATE_VRAM_CLEARED |
          AMDGPU_GEM_CREATE_VM_ALWAYS_VALID;
  flags |=
    uncached ? (domain == AMDGPU_GEM_DOMAIN_GTT) * AMDGPU_GEM_CREATE_CPU_GTT_USWC : 0;
 } else {
  actual_size = size;
  alignment   = 1;
  flags       = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;
 }

 struct amdgpu_bo_alloc_request req = {
  .alloc_size     = actual_size,
  .phys_alignment = alignment,
  .preferred_heap = domain,
  .flags          = flags,
 };

 // memory aquired!!
 ret = amdgpu_bo_alloc(dev-&amp;gt;dev_handle, &amp;amp;req, &amp;amp;bo_handle);
 HDB_ASSERT(!ret, "can't allocate bo");&lt;/code&gt;
    &lt;p&gt;Now we have the memory, we need to map it. I opt to map anything that can be CPU-mapped for ease of use. We have to map the memory to both the GPU and the CPU virtual space. The KMD creates the page table when we open the DRM file, as shown here.&lt;/p&gt;
    &lt;p&gt;So map it to the GPU VM and, if possible, to the CPU VM as well. Here, at this point, there’s a libdrm function that does all of this setup for us and maps the memory, but I found that even when specifying &lt;code&gt;AMDGPU_VM_MTYPE_UC&lt;/code&gt;, it doesn’t always tag the page as uncached, not quite sure if it’s a
bug in my code or something in &lt;code&gt;libdrm&lt;/code&gt; anyways, the function is &lt;code&gt;amdgpu_bo_va_op&lt;/code&gt;, I opted to do it manually here and issue the IOCTL call myself:&lt;/p&gt;
    &lt;code&gt; u32 kms_handle = 0;
 amdgpu_bo_export(bo_handle, amdgpu_bo_handle_type_kms, &amp;amp;kms_handle);

 ret = amdgpu_va_range_alloc(
   dev-&amp;gt;dev_handle,
   amdgpu_gpu_va_range_general,
   actual_size,
   4096,
   0,
   &amp;amp;va_addr,
   &amp;amp;va_handle,
   0);
 HDB_ASSERT(!ret, "can't allocate VA");

 u64 map_flags =
   AMDGPU_VM_PAGE_EXECUTABLE | AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE;
 map_flags |= uncached ? AMDGPU_VM_MTYPE_UC | AMDGPU_VM_PAGE_NOALLOC : 0;

 struct drm_amdgpu_gem_va va = {
  .handle       = kms_handle,
  .operation    = AMDGPU_VA_OP_MAP,
  .flags        = map_flags,
  .va_address   = va_addr,
  .offset_in_bo = 0,
  .map_size     = actual_size,

 };

 ret = drm_ioctl_write_read(dev-&amp;gt;drm_fd, DRM_AMDGPU_GEM_VA, &amp;amp;va, sizeof(va));
 HDB_ASSERT(!ret, "can't map bo in GPU space");
 // ret = amdgpu_bo_va_op(bo_handle, 0, actual_size, va_addr, map_flags,
 // AMDGPU_VA_OP_MAP);

 if (flags &amp;amp; AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) {
  ret = amdgpu_bo_cpu_map(bo_handle, &amp;amp;host_addr);
  HDB_ASSERT(!ret, "can't map bo in CPU space");

  // AMDGPU_GEM_CREATE_VRAM_CLEARED doesn't really memset the memory to 0 anyways for
  // debug I'll just do it manually for now
  memset(host_addr, 0x0, actual_size);
 }

 *bo = (amdgpubo_t){
  .bo_handle = bo_handle,
  .va_handle = va_handle,
  .va_addr   = va_addr,
  .size      = actual_size,
  .host_addr = host_addr,
 };
}&lt;/code&gt;
    &lt;p&gt;Now we have the context and 2 buffers. Next, fill those buffers and send our commands to the KMD, which will then forward them to the Command Processor (CP) in the GPU for processing.&lt;/p&gt;
    &lt;p&gt;Let’s compile our code. We can use clang assembler for that, like this:&lt;/p&gt;
    &lt;code&gt;# https://gitlab.freedesktop.org/martty/radbg-poc/-/blob/master/ll-as.sh
clang -c -x assembler -target amdgcn-amd-amdhsa -mcpu=gfx1100 -o asm.o "$1"
objdump -h asm.o | grep .text | awk '{print "dd if='asm.o' of='asmc.bin' bs=1 count=$[0x" $3 "] skip=$[0x" $6 "] status=none"}' | bash
#rm asm.o&lt;/code&gt;
    &lt;p&gt;The bash script compiles the code, and then we’re only interested in the actual machine code, so we use objdump to figure out the offset and the size of the section and copy it to a new file called asmc.bin, then we can just load the file and write its bytes to the CPU-mapped address of the code buffer.&lt;/p&gt;
    &lt;p&gt;Next up, filling in the commands. This was extremely confusing for me because it’s not well documented. It was mostly learning how &lt;code&gt;RADV&lt;/code&gt; does things and trying to do similar things. Also, shout-out to the folks on the Graphics Programming Discord server for helping me, especially Picoduck. The commands are encoded in a special format called &lt;code&gt;PM4 Packets&lt;/code&gt;, which has multiple types. We only care about &lt;code&gt;Type 3&lt;/code&gt;: each packet has an opcode and the number of bytes it contains.&lt;/p&gt;
    &lt;p&gt;The first thing we need to do is program the GPU registers, then dispatch the shader. Some of those registers are &lt;code&gt;rsrc[1-3]&lt;/code&gt;; those registers are responsible for a number of configurations, pgm_[lo/hi], which hold the pointer to the code buffer and &lt;code&gt;num_thread_[x/y/z]&lt;/code&gt;; those are responsible for the number of threads inside a work group. All of those are set using the &lt;code&gt;set shader register&lt;/code&gt; packets, and here is how to encode them:&lt;/p&gt;
    &lt;p&gt;It’s worth mentioning that we can set multiple registers in 1 packet if they’re consecutive.&lt;/p&gt;
    &lt;code&gt;void pkt3_set_sh_reg(pkt3_packets_t* packets, u32 reg, u32 value) {
 HDB_ASSERT(
   reg &amp;gt;= SI_SH_REG_OFFSET &amp;amp;&amp;amp; reg &amp;lt; SI_SH_REG_END,
   "can't set register outside sh registers span");

 // packet header
 da_append(packets, PKT3(PKT3_SET_SH_REG, 1, 0));
 // offset of the register
 da_append(packets, (reg - SI_SH_REG_OFFSET) / 4);
 da_append(packets, value);
}&lt;/code&gt;
    &lt;p&gt;Then we append the dispatch command:&lt;/p&gt;
    &lt;code&gt;// we're going for 1 thread since we want the simplest case here.

da_append(&amp;amp;pkt3_packets, PKT3(PKT3_DISPATCH_DIRECT, 3, 0) | PKT3_SHADER_TYPE_S(1));
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, 1u);
da_append(&amp;amp;pkt3_packets, dispatch_initiator);&lt;/code&gt;
    &lt;p&gt;Now we want to write those commands into our buffer and send them to the KMD:&lt;/p&gt;
    &lt;code&gt;void dev_submit(
  amdgpu_t*         dev,
  pkt3_packets_t*   packets,
  amdgpu_bo_handle* buffers,
  u32               buffers_count,
  amdgpu_submit_t*  submit
) {
 s32        ret = -1;
 amdgpubo_t ib  = { 0 };

 bo_alloc(dev, pkt3_size(packets), AMDGPU_GEM_DOMAIN_GTT, false, &amp;amp;ib);
 bo_upload(&amp;amp;ib, packets-&amp;gt;data, pkt3_size(packets));

 amdgpu_bo_handle* bo_handles = // +1 for the indirect buffer
   (amdgpu_bo_handle*)malloc(sizeof(amdgpu_bo_handle) * (buffers_count + 1));

 bo_handles[0] = ib.bo_handle;
 for_range(i, 0, buffers_count) {
  bo_handles[i + 1] = buffers[i];
 }

 amdgpu_bo_list_handle bo_list = NULL;
 ret =
   amdgpu_bo_list_create(dev-&amp;gt;dev_handle, buffers_count + 1, bo_handles, NULL, &amp;amp;bo_list);
 HDB_ASSERT(!ret, "can't create a bo list");
 free(bo_handles);

 struct amdgpu_cs_ib_info ib_info = {
  .flags         = 0,
  .ib_mc_address = ib.va_addr,
  .size          = packets-&amp;gt;count,
 };

 struct amdgpu_cs_request req = {
  .flags                  = 0,
  .ip_type                = AMDGPU_HW_IP_COMPUTE,
  .ip_instance            = 0,
  .ring                   = 0,
  .resources              = bo_list,
  .number_of_dependencies = 0,
  .dependencies           = NULL,
  .number_of_ibs          = 1,
  .ibs                    = &amp;amp;ib_info,
  .seq_no                 = 0,
  .fence_info             = { 0 },
 };

 ret = amdgpu_cs_submit(dev-&amp;gt;ctx_handle, 0, &amp;amp;req, 1);
 HDB_ASSERT(!ret, "can't submit indirect buffer request");

 *submit = (amdgpu_submit_t){
    .ib = ib,
    .bo_list = bo_list,
    .fence = {
      .context = dev-&amp;gt;ctx_handle,
      .ip_type = AMDGPU_HW_IP_COMPUTE,
      .ip_instance = 0,
      .ring = 0,
      .fence = req.seq_no,
    },
  };
}&lt;/code&gt;
    &lt;p&gt;Here is a good point to make a more complex shader that outputs something. For example, writing 1 to a buffer.&lt;/p&gt;
    &lt;p&gt;No GPU hangs ?! nothing happened ?! cool, cool, now we have a shader that runs on the GPU, what’s next? Let’s try to hang the GPU by pausing the execution, aka make the GPU trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;TBA/TMA&lt;/head&gt;
    &lt;p&gt;The RDNA3’s ISA manual does mention 2 registers, &lt;code&gt;TBA, TMA&lt;/code&gt;; here’s how they describe them respectively:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Holds the pointer to the current trap handler program address. Per-VMID register. Bit [63] indicates if the trap handler is present (1) or not (0) and is not considered part of the address (bit[62] is replicated into address bit[63]). Accessed via S_SENDMSG_RTN.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Temporary register for shader operations. For example, it can hold a pointer to memory used by the trap handler.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can configure the GPU to enter the trap handler when encountering certain exceptions listed in the RDNA3 ISA manual.&lt;/p&gt;
    &lt;p&gt;We know from Marcell Kiss’s blog posts that we need to compile a trap handler, which is a normal shader the GPU switches to when encountering a &lt;code&gt;s_trap&lt;/code&gt;. The TBA register has a special bit that indicates whether the trap handler is enabled.&lt;/p&gt;
    &lt;p&gt;Since these are privileged registers, we cannot write to them from user space. To bridge this gap for debugging, we can utilize the debugfs interface. Luckily, we have UMR, which uses that debugfs interface, and it’s open source; we copy AMD’s homework here which is great.&lt;/p&gt;
    &lt;head rend="h1"&gt;AMDGPU Debugfs&lt;/head&gt;
    &lt;p&gt;The amdgpu KMD has a couple of files in debugfs under &lt;code&gt;/sys/kernel/debug/dri/{PCI address}&lt;/code&gt;; one of them is &lt;code&gt;regs2&lt;/code&gt;, which is an interface to a &lt;code&gt;amdgpu_debugfs_regs2_write&lt;/code&gt; in the kernel that writes to the registers. It works by simply opening the file, seeking the register’s offset, and then writing; it also performs some synchronisation and writes the value correctly. We need to provide more parameters about the register before writing to the file, tho and do that by using an ioctl call. Here are the ioctl arguments:&lt;/p&gt;
    &lt;code&gt;typedef struct amdgpu_debugfs_regs2_iocdata_v2 {
 __u32 use_srbm, use_grbm, pg_lock;
 struct {
  __u32 se, sh, instance;
 } grbm;
 struct {
  __u32 me, pipe, queue, vmid;
 } srbm;
 __u32 xcc_id;
} regs2_ioc_data_t;&lt;/code&gt;
    &lt;p&gt;The 2 structs are because there are 2 types of registers, GRBM and SRBM, each of which is banked by different constructs; you can learn more about some of them here in the Linux kernel documentation.&lt;/p&gt;
    &lt;p&gt;Turns out our registers here are SBRM registers and banked by VMIDs, meaning each VMID has its own TBA and TMA registers. Cool, now we need to figure out the VMID of our process. As far as I understand, VMIDs are a way for the GPU to identify a specific process context, including the page table base address, so the address translation unit can translate a virtual memory address. The context is created when we open the DRM file. They get assigned dynamically at dispatch time, which is a problem for us; we want to write to those registers before dispatch.&lt;/p&gt;
    &lt;p&gt;We can obtain the VMID of the dispatched process by querying the &lt;code&gt;HW_ID2&lt;/code&gt; register with s_getreg_b32. I do a hack here, by enabling the trap handler in every VMID, and there are 16 of them, the first being special, and used by the KMD and the last 8 allocated to the amdkfd driver. We loop over the remaining VMIDs and write to those registers. This can cause issues to other processes using other VMIDs by enabling trap handlers in them and writing the virtual address of our trap handler, which is only valid within our virtual memory address space. It’s relatively safe tho since most other processes won’t cause a trap1.&lt;/p&gt;
    &lt;p&gt;Now we can write to TMA and TBA, here’s the code:&lt;/p&gt;
    &lt;code&gt;void dev_op_reg32(
  amdgpu_t* dev, gc_11_reg_t reg, regs2_ioc_data_t ioc_data, reg_32_op_t op, u32* value) {
 s32 ret = 0;

 reg_info_t reg_info     = gc_11_regs_infos[reg];
 uint64_t   reg_offset   = gc_11_regs_offsets[reg];
 uint64_t   base_offset  = dev-&amp;gt;gc_regs_base_addr[reg_info.soc_index];
 uint64_t   total_offset = (reg_offset + base_offset);

 // seems like we're multiplying by 4 here because the registers database in UMRs
 // source has them in indexes rather than bytes.
 total_offset *= (reg_info.type == REG_MMIO) ? 4 : 1;

 ret = hdb_ioctl(dev-&amp;gt;regs2_fd, AMDGPU_DEBUGFS_REGS2_IOC_SET_STATE_V2, &amp;amp;ioc_data);
 HDB_ASSERT(!ret, "Failed to set registers state");

 size_t size = lseek(dev-&amp;gt;regs2_fd, total_offset, SEEK_SET);
 HDB_ASSERT(size == total_offset, "Failed to seek register address");

 switch (op) {
 case REG_OP_READ : size = read(dev-&amp;gt;regs2_fd, value, 4); break;
 case REG_OP_WRITE: size = write(dev-&amp;gt;regs2_fd, value, 4); break;
 default          : HDB_ASSERT(false, "unsupported op");
 }

 HDB_ASSERT(size == 4, "Failed to write/read the values to/from the register");
}&lt;/code&gt;
    &lt;p&gt;And here’s how we write to &lt;code&gt;TMA&lt;/code&gt; and &lt;code&gt;TBA&lt;/code&gt;:
If you noticed, I’m using bitfields. I use them because working with them is much easier than macros, and while the byte order is not guaranteed by the C spec, it’s guaranteed by System V ABI, which Linux adheres to.&lt;/p&gt;
    &lt;code&gt;void dev_setup_trap_handler(amdgpu_t* dev, u64 tba, u64 tma) {
 reg_sq_shader_tma_lo_t tma_lo = { .raw = (u32)(tma) };
 reg_sq_shader_tma_hi_t tma_hi = { .raw = (u32)(tma &amp;gt;&amp;gt; 32) };

 reg_sq_shader_tba_lo_t tba_lo = { .raw = (u32)(tba &amp;gt;&amp;gt; 8) };
 reg_sq_shader_tba_hi_t tba_hi = { .raw = (u32)(tba &amp;gt;&amp;gt; 40) };

 tba_hi.trap_en = 1;

 regs2_ioc_data_t ioc_data = {
  .use_srbm = 1,
  .xcc_id   = -1,
 };

 // NOTE(hadi):
 // vmid's get assigned when code starts executing before hand we don't know which vmid
 // will get assigned to our process so we just set all of them
 for_range(i, 1, 9) {
  ioc_data.srbm.vmid = i;
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tba_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TBA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tba_hi.raw);

  dev_op_reg32(dev, REG_SQ_SHADER_TMA_LO, ioc_data, REG_OP_WRITE, &amp;amp;tma_lo.raw);
  dev_op_reg32(dev, REG_SQ_SHADER_TMA_HI, ioc_data, REG_OP_WRITE, &amp;amp;tma_hi.raw);
 }
}&lt;/code&gt;
    &lt;p&gt;Anyway, now that we can write to those registers, if we enable the trap handler correctly, the GPU should hang when we launch our shader if we added &lt;code&gt;s_trap&lt;/code&gt; instruction to it, or we enabled the &lt;code&gt;TRAP_ON_START&lt;/code&gt; bit in rsrc32 register.&lt;/p&gt;
    &lt;p&gt;Now, let’s try to write a trap handler.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Trap Handler&lt;/head&gt;
    &lt;p&gt;If you wrote a different shader that outputs to a buffer, u can try writing to that shader from the trap handler, which is nice to make sure it’s actually being run.&lt;/p&gt;
    &lt;p&gt;We need 2 things: our trap handler and some scratch memory to use when needed, which we will store the address of in the TMA register.&lt;/p&gt;
    &lt;p&gt;The trap handler is just a normal program running in privileged state, meaning we have access to special registers like TTMP[0-15]. When we enter a trap handler, we need to first ensure that the state of the GPU registers is saved, just as the kernel does for CPU processes when context-switching, by saving a copy of the stable registers and the program counter, etc. The problem, tho, is that we don’t have a stable ABI for GPUs, or at least not one I’m aware of, and compilers use all the registers they can, so we need to save everything.&lt;/p&gt;
    &lt;p&gt;AMD GPUs’ Command Processors (CPs) have context-switching functionality, and the amdkfd driver does implement some context-switching shaders. The problem is they’re not documented, and we have to figure them out from the amdkfd driver source and from other parts of the driver stack that interact with it, which is a pain in the ass. I kinda did a workaround here since I didn’t find luck understanding how it works, and some other reasons I’ll discuss later in the post.&lt;/p&gt;
    &lt;p&gt;The workaround here is to use only TTMP registers and a combination of specific instructions to copy the values of some registers, allowing us to use more instructions to copy the remaining registers. The main idea is to make use of the &lt;code&gt;global_store_addtid_b32&lt;/code&gt; instruction, which adds the index of the current thread within the wave to the writing address, aka&lt;/p&gt;
    &lt;p&gt;This allows us to write a unique value per thread using only TTMP registers, which are unique per wave, not per thread3, so we can save the context of a single wave.&lt;/p&gt;
    &lt;p&gt;The problem is that if we have more than 1 wave, they will overlap, and we will have a race condition.&lt;/p&gt;
    &lt;p&gt;Here is the code:&lt;/p&gt;
    &lt;code&gt;start:
 ;; save the STATUS word into ttmp8
 s_getreg_b32 ttmp8, hwreg(HW_REG_STATUS)

 ;; save exec into ttmp[2:3]
 s_mov_b64 ttmp[2:3], exec

 ;; getting the address of our tma buffer
 s_sendmsg_rtn_b64 ttmp[4:5], sendmsg(MSG_RTN_GET_TMA)
 s_waitcnt lgkmcnt(0)

 ;; save vcc
 s_mov_b64 ttmp[6:7], vcc

 ;; enable all threads so they can write their vgpr registers
 s_mov_b64 exec, -1

 ;; FIXME(hadi): this assumes only 1 wave is running
 global_store_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_store_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_store_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_store_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_store_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_store_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_store_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; only first thread is supposed to write sgprs of the wave
 s_mov_b64 exec, 1
 v_mov_b32 v1, s0
 v_mov_b32 v2, s1
 v_mov_b32 v3, s2
 v_mov_b32 v4, s3
 v_mov_b32 v5, s4
 v_mov_b32 v0, 0
 global_store_b32 v0, v1, ttmp[4:5], offset:TMA_SREG_OFFSET glc slc dlc
 global_store_b32 v0, v2, ttmp[4:5], offset:TMA_SREG_OFFSET + 4 glc slc dlc
 global_store_b32 v0, v3, ttmp[4:5], offset:TMA_SREG_OFFSET + 8 glc slc dlc
 global_store_b32 v0, v4, ttmp[4:5], offset:TMA_SREG_OFFSET + 12 glc slc dlc
 global_store_b32 v0, v5, ttmp[4:5], offset:TMA_SREG_OFFSET + 16 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; enable all threads
 s_mov_b64 exec, -1&lt;/code&gt;
    &lt;p&gt;Now that we have those values in memory, we need to tell the CPU: Hey, we got the data, and pause the GPU’s execution until the CPU issues a command. Also, notice we can just modify those from the CPU.&lt;/p&gt;
    &lt;p&gt;Before we tell the CPU, we need to write some values that might help the CPU. Here are they:&lt;/p&gt;
    &lt;code&gt; ;; IDs to identify which parts of the hardware we are running on exactly
 s_getreg_b32 ttmp10, hwreg(HW_REG_HW_ID1)
 s_getreg_b32 ttmp11, hwreg(HW_REG_HW_ID2)
 v_mov_b32 v3, ttmp10
 v_mov_b32 v4, ttmp11
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:TMA_DATA_OFFSET glc slc dlc

 ;; the original vcc mask
 v_mov_b32 v3, ttmp6
 v_mov_b32 v4, ttmp7
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2048 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the original exec mask
 v_mov_b32 v3, ttmp2
 v_mov_b32 v4, ttmp3
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:2056 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; the program counter
 v_mov_b32 v3, ttmp0
 v_mov_b32 v4, ttmp1
 v_and_b32 v4, v4, 0xffff
 global_store_dwordx2 v1, v[3:4], ttmp[4:5], offset:16 glc slc dlc

 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;Now the GPU should just wait for the CPU, and here’s the spin code it’s implemented as described by Marcell Kiss here:&lt;/p&gt;
    &lt;code&gt;SPIN:
 global_load_dword v1, v2, ttmp[4:5] glc slc dlc

SPIN1:
 // I found the bit range of 10 to 15 using trial and error in the
 // isa manual specifies that it's a 6-bit number but the offset 10
 // is just trial and error
  s_getreg_b32 ttmp13, hwreg(HW_REG_IB_STS, 10, 15)
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc1 SPIN1

 v_readfirstlane_b32 ttmp13, v1
 s_and_b32 ttmp13, ttmp13, ttmp13
 s_cbranch_scc0 SPIN

CLEAR:
 v_mov_b32 v2, 0
 v_mov_b32 v1, 0
 global_store_dword v1, v2, ttmp[4:5] glc slc dlc
 s_waitcnt vmcnt(0)&lt;/code&gt;
    &lt;p&gt;The main loop in the CPU is like enable trap handler, then dispatch shader, then wait for the GPU to write some specific value in a specific address to signal all data is there, then examine and display, and tell the GPU all clear, go ahead.&lt;/p&gt;
    &lt;p&gt;Now that our uncached buffers are in play, we just keep looping and checking whether the GPU has written the register values. When it does, the first thing we do is halt the wave by writing into the &lt;code&gt;SQ_CMD&lt;/code&gt; register to allow us to do whatever with the wave without causing any issues, tho if we halt for too long, the GPU CP will reset the command queue and kill the process, but we can change that behaviour by adjusting lockup_timeout parameter of the amdgpu kernel module:&lt;/p&gt;
    &lt;code&gt;reg_sq_wave_hw_id1_t hw1 = { .raw = tma[2] };
reg_sq_wave_hw_id2_t hw2 = { .raw = tma[3] };

reg_sq_cmd_t halt_cmd = {
 .cmd  = 1,
 .mode = 1,
 .data = 1,
};

regs2_ioc_data_t ioc_data = {
 .use_srbm = false,
 .use_grbm = true,
};

dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = true;&lt;/code&gt;
    &lt;p&gt;From here on, we can do whatever with the data we have. All the data we need to build a proper debugger. We will come back to what to do with the data in a bit; let’s assume we did what was needed for now.&lt;/p&gt;
    &lt;p&gt;Now that we’re done with the CPU, we need to write to the first byte in our TMA buffer, since the trap handler checks for that, then resume the wave, and the trap handler should pick it up. We can resume by writing to the &lt;code&gt;SQ_CMD&lt;/code&gt; register again:&lt;/p&gt;
    &lt;code&gt;halt_cmd.mode = 0;
dev_op_reg32(&amp;amp;amdgpu, REG_SQ_CMD, ioc_data, REG_OP_WRITE, &amp;amp;halt_cmd.raw);
gpu_is_halted = false;&lt;/code&gt;
    &lt;p&gt;Then the GPU should continue. We need to restore everything and return the program counter to the original address. Based on whether it’s a hardware trap or not, the program counter may point to the instruction before or the instruction itself. The ISA manual and Marcell Kiss’s posts explain that well, so refer to them.&lt;/p&gt;
    &lt;code&gt;RETURN:
 ;; extract the trap ID from ttmp1
 s_and_b32 ttmp9, ttmp1, PC_HI_TRAP_ID_MASK
 s_lshr_b32 ttmp9, ttmp9, PC_HI_TRAP_ID_SHIFT

 ;; if the trapID == 0, then this is a hardware trap,
 ;; we don't need to fix up the return address
 s_cmpk_eq_u32 ttmp9, 0
 s_cbranch_scc1 RETURN_FROM_NON_S_TRAP

 ;; restore PC
 ;; add 4 to the faulting address, with carry
 s_add_u32 ttmp0, ttmp0, 4
 s_addc_u32 ttmp1, ttmp1, 0

RETURN_FROM_NON_S_TRAP:
 s_load_dwordx4 s[0:3], ttmp[4:5], TMA_SREG_OFFSET glc dlc
 s_load_dword s4, ttmp[4:5], TMA_SREG_OFFSET + 16 glc dlc
 s_waitcnt lgkmcnt(0)

 s_mov_b64 exec, -1
 global_load_addtid_b32 v0, ttmp[4:5], offset:TMA_VREG_OFFSET        glc slc dlc
 global_load_addtid_b32 v1, ttmp[4:5], offset:TMA_VREG_OFFSET + 256  glc slc dlc
 global_load_addtid_b32 v2, ttmp[4:5], offset:TMA_VREG_OFFSET + 512  glc slc dlc
 global_load_addtid_b32 v3, ttmp[4:5], offset:TMA_VREG_OFFSET + 768  glc slc dlc
 global_load_addtid_b32 v4, ttmp[4:5], offset:TMA_VREG_OFFSET + 1024 glc slc dlc
 global_load_addtid_b32 v5, ttmp[4:5], offset:TMA_VREG_OFFSET + 1280 glc slc dlc
 global_load_addtid_b32 v6, ttmp[4:5], offset:TMA_VREG_OFFSET + 1536 glc slc dlc
 s_waitcnt vmcnt(0)

 ;; mask off non-address high bits from ttmp1
 s_and_b32 ttmp1, ttmp1, 0xffff

 ;; restore exec
 s_load_b64 vcc, ttmp[4:5], 2048 glc dlc
 s_load_b64 ttmp[2:3], ttmp[4:5], 2056 glc dlc
 s_waitcnt lgkmcnt(0)
 s_mov_b64 exec, ttmp[2:3]

 ;; restore STATUS.EXECZ, not writable by s_setreg_b32
 s_and_b64 exec, exec, exec

 ;; restore STATUS.VCCZ, not writable by s_setreg_b32
 s_and_b64 vcc, vcc, vcc

 ;; restore STATUS.SCC
 s_setreg_b32 hwreg(HW_REG_STATUS, 0, 1), ttmp8

 s_waitcnt vmcnt(0) lgkmcnt(0) expcnt(0)  ; Full pipeline flush
 ;; return from trap handler and restore STATUS.PRIV
 s_rfe_b64 [ttmp0, ttmp1]&lt;/code&gt;
    &lt;head rend="h1"&gt;SPIR-V&lt;/head&gt;
    &lt;p&gt;Now we can run compiled code directly, but we don’t want people to compile their code manually, then extract the text section, and give it to us. The plan is to take SPIR-V code, compile it correctly, then run it, or, even better, integrate with RADV and let RADV give us more information to work with.&lt;/p&gt;
    &lt;p&gt;My main plan was making like fork RADV and then add then make report for us the vulkan calls and then we can have a better view on the GPU work know the buffers/textures it’s using etc, This seems like a lot more work tho so I’ll keep it in mind but not doing that for now unless someone is willing to pay me for that ;).&lt;/p&gt;
    &lt;p&gt;For now, let’s just use RADV’s compiler &lt;code&gt;ACO&lt;/code&gt;. Luckily, RADV has a &lt;code&gt;null_winsys&lt;/code&gt; mode, aka it will not do actual work or open DRM files, just a fake Vulkan device, which is perfect for our case here, since we care about nothing other than just compiling code. We can enable it by setting the env var &lt;code&gt;RADV_FORCE_FAMILY&lt;/code&gt;, then we just call what we need like this:&lt;/p&gt;
    &lt;code&gt;int32_t hdb_compile_spirv_to_bin(
  const void* spirv_binary,
  size_t size,
  hdb_shader_stage_t stage,
  hdb_shader_t* shader
) {
 setenv("RADV_FORCE_FAMILY", "navi31", 1);
 //  setenv("RADV_DEBUG", "nocache,noopt", 1);
 setenv("ACO_DEBUG", "nocache,noopt", 1);

 VkInstanceCreateInfo i_cinfo = {
  .sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO,
  .pApplicationInfo =
    &amp;amp;(VkApplicationInfo){
      .sType              = VK_STRUCTURE_TYPE_APPLICATION_INFO,
      .pApplicationName   = "HDB Shader Compiler",
      .applicationVersion = 1,
      .pEngineName        = "HDB",
      .engineVersion      = 1,
      .apiVersion         = VK_API_VERSION_1_4,
    },
 };

 VkInstance vk_instance = {};
 radv_CreateInstance(&amp;amp;i_cinfo, NULL, &amp;amp;vk_instance);

 struct radv_instance* instance = radv_instance_from_handle(vk_instance);
 instance-&amp;gt;debug_flags |=
   RADV_DEBUG_NIR_DEBUG_INFO | RADV_DEBUG_NO_CACHE | RADV_DEBUG_INFO;

 uint32_t         n       = 1;
 VkPhysicalDevice vk_pdev = {};
 instance-&amp;gt;vk.dispatch_table.EnumeratePhysicalDevices(vk_instance, &amp;amp;n, &amp;amp;vk_pdev);

 struct radv_physical_device* pdev = radv_physical_device_from_handle(vk_pdev);
 pdev-&amp;gt;use_llvm                    = false;

 VkDeviceCreateInfo d_cinfo = { VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO };
 VkDevice vk_dev = {};
 pdev-&amp;gt;vk.dispatch_table.CreateDevice(vk_pdev, &amp;amp;d_cinfo, NULL, &amp;amp;vk_dev);

 struct radv_device* dev = radv_device_from_handle(vk_dev);

 struct radv_shader_stage radv_stage = {
  .spirv.data = spirv_binary,
  .spirv.size = size,
  .entrypoint = "main",
  .stage      = MESA_SHADER_COMPUTE,
  .layout = {
   .push_constant_size = 16,
  },
  .key = {
   .optimisations_disabled = true,
  },
 };

 struct radv_shader_binary* cs_bin = NULL;
 struct radv_shader*        cs_shader =
   radv_compile_cs(dev, NULL, &amp;amp;radv_stage, true, true, false, true, &amp;amp;cs_bin);

 *shader = (hdb_shader_t){
  .bin              = cs_shader-&amp;gt;code,
  .bin_size         = cs_shader-&amp;gt;code_size,
  .rsrc1            = cs_shader-&amp;gt;config.rsrc1,
  .rsrc2            = cs_shader-&amp;gt;config.rsrc2,
  .rsrc3            = cs_shader-&amp;gt;config.rsrc3,
  .debug_info       = cs_shader-&amp;gt;debug_info,
  .debug_info_count = cs_shader-&amp;gt;debug_info_count,
 };

 return 0;
}&lt;/code&gt;
    &lt;p&gt;Now that we have a well-structured loop and communication between the GPU and the CPU, we can run SPIR-V binaries to some extent. Let’s see how we can make it an actual debugger.&lt;/p&gt;
    &lt;head rend="h1"&gt;An Actual Debugger&lt;/head&gt;
    &lt;p&gt;We talked earlier about CPs natively supporting context-switching, this appears to be compute spcific feature, which prevents from implementing it for other types of shaders, tho, it appears that mesh shaders and raytracing shaders are just compute shaders under the hood, which will allow us to use that functionality. For now debugging one wave feels enough, also we can moify the wave parameters to debug some specific indices.&lt;/p&gt;
    &lt;p&gt;Here’s some of the features&lt;/p&gt;
    &lt;head rend="h2"&gt;Breakpoints and Stepping&lt;/head&gt;
    &lt;p&gt;For stepping, we can use 2 bits: one in &lt;code&gt;RSRC1&lt;/code&gt; and the other in &lt;code&gt;RSRC3&lt;/code&gt;. They’re &lt;code&gt;DEBUG_MODE&lt;/code&gt; and &lt;code&gt;TRAP_ON_START&lt;/code&gt;, respectively. The former enters the trap handler after each instruction, and the latter enters before the first instruction. This means we can automatically enable instruction-level stepping.&lt;/p&gt;
    &lt;p&gt;Regarding breakpoints, I haven’t implemented them, but they’re rather simple to implement here by us having the base address of the code buffer and knowing the size of each instruction; we can calculate the program counter location ahead and have a list of them available to the GPU, and we can do a binary search on the trap handler.&lt;/p&gt;
    &lt;head rend="h2"&gt;Source Code Line Mapping&lt;/head&gt;
    &lt;p&gt;The ACO shader compiler does generate instruction-level source code mapping, which is good enough for our purposes here. By taking the offset4 of the current program counter and indexing into the code buffer, we can retrieve the current instruction and disassemble it, as well as find the source code mapping from the debug info.&lt;/p&gt;
    &lt;head rend="h2"&gt;Address Watching aka Watchpoints&lt;/head&gt;
    &lt;p&gt;We can implement this by marking the GPU page as protected. On a GPU fault, we enter the trap handler, check whether it’s within the range of our buffers and textures, and then act accordingly. Also, looking at the registers, we can find these:&lt;/p&gt;
    &lt;code&gt;typedef union {
 struct {
  uint32_t addr: 16;
 };
 uint32_t raw;
} reg_sq_watch0_addr_h_t;

typedef union {
 struct {
  uint32_t __reserved_0 : 6;
  uint32_t addr: 26;
 };
 uint32_t raw;
} reg_sq_watch0_addr_l_t;&lt;/code&gt;
    &lt;p&gt;which suggests that the hardware already supports this natively, so we don’t even need to do that dance. It needs more investigation on my part, tho, since I didn’t implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Variables Types and Names&lt;/head&gt;
    &lt;p&gt;This needs some serious plumbing, since we need to make NIR(Mesa’s intermediate representation) optimisation passes propagate debug info correctly. I already started on this here. Then we need to make ACO track variables and store the information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vulkan Integration&lt;/head&gt;
    &lt;p&gt;This requires ditching our simple UMD we made earlier and using RADV, which is what should happen eventually, then we have our custom driver maybe pause on before a specific frame, or get triggered by a key, and then ask before each dispatch if to attach to it or not, or something similar, since we have a full proper Vulkan implementation we already have all the information we would need like buffers, textures, push constants, types, variable names, .. etc, that would be a much better and more pleasant debugger to use.&lt;/p&gt;
    &lt;p&gt;Finally, here’s some live footage:&lt;/p&gt;
    &lt;head rend="h1"&gt;Bonus Round&lt;/head&gt;
    &lt;p&gt;Here is an incomplete user-mode page walking code for gfx11, aka rx7900xtx&lt;/p&gt;
    &lt;code&gt;typedef struct {
 u64 valid         : 1;  // 0
 u64 system        : 1;  // 1
 u64 coherent      : 1;  // 2
 u64 __reserved_0  : 3;  // 5
 u64 pte_base_addr : 42; // 47
 u64 pa_rsvd       : 4;  // 51
 u64 __reserved_1  : 2;  // 53
 u64 mall_reuse    : 2;  // 55
 u64 tfs_addr      : 1;  // 56
 u64 __reserved_2  : 1;  // 57
 u64 frag_size     : 5;  // 62
 u64 pte           : 1;  // 63
} pde_t;

typedef struct {
 u64 valid          : 1; // = pte_entry &amp;amp; 1;
 u64 system         : 1; // = (pte_entry &amp;gt;&amp;gt; 1) &amp;amp; 1;
 u64 coherent       : 1; // = (pte_entry &amp;gt;&amp;gt; 2) &amp;amp; 1;
 u64 tmz            : 1; // = (pte_entry &amp;gt;&amp;gt; 3) &amp;amp; 1;
 u64 execute        : 1; // = (pte_entry &amp;gt;&amp;gt; 4) &amp;amp; 1;
 u64 read           : 1; // = (pte_entry &amp;gt;&amp;gt; 5) &amp;amp; 1;
 u64 write          : 1; // = (pte_entry &amp;gt;&amp;gt; 6) &amp;amp; 1;
 u64 fragment       : 5; // = (pte_entry &amp;gt;&amp;gt; 7) &amp;amp; 0x1F;
 u64 page_base_addr : 36;
 u64 mtype          : 2; // = (pte_entry &amp;gt;&amp;gt; 48) &amp;amp; 3;
 u64 prt            : 1; // = (pte_entry &amp;gt;&amp;gt; 51) &amp;amp; 1;
 u64 software       : 2; // = (pte_entry &amp;gt;&amp;gt; 52) &amp;amp; 3;
 u64 pde            : 1; // = (pte_entry &amp;gt;&amp;gt; 54) &amp;amp; 1;
 u64 __reserved_0   : 1;
 u64 further        : 1; // = (pte_entry &amp;gt;&amp;gt; 56) &amp;amp; 1;
 u64 gcr            : 1; // = (pte_entry &amp;gt;&amp;gt; 57) &amp;amp; 1;
 u64 llc_noalloc    : 1; // = (pte_entry &amp;gt;&amp;gt; 58) &amp;amp; 1;
} pte_t;

static inline pde_t decode_pde(u64 pde_raw) {
 pde_t pde         = *((pde_t*)(&amp;amp;pde_raw));
 pde.pte_base_addr = (u64)pde.pte_base_addr &amp;lt;&amp;lt; 6;
 return pde;
}

static inline pte_t decode_pte(u64 pde_raw) {
 pte_t pte          = *((pte_t*)(&amp;amp;pde_raw));
 pte.page_base_addr = (u64)pte.page_base_addr &amp;lt;&amp;lt; 12;
 return pte;
}

static inline u64 log2_range_round_up(u64 s, u64 e) {
 u64 x = e - s - 1;
 return (x == 0 || x == 1) ? 1 : 64 - __builtin_clzll(x);
}

void dev_linear_vram(amdgpu_t* dev, u64 phy_addr, size_t size, void* buf) {
 HDB_ASSERT(!((phy_addr &amp;amp; 3) || (size &amp;amp; 3)), "Must be page aligned address and size");

 size_t offset = lseek(dev-&amp;gt;vram_fd, phy_addr, SEEK_SET);
 HDB_ASSERT(offset == phy_addr, "Couldn't seek to the requested addr");

 offset = read(dev-&amp;gt;vram_fd, buf, size);
 HDB_ASSERT(offset == size, "Couldn't read the full requested size");
}

void dev_decode(amdgpu_t* dev, u32 vmid, u64 va_addr) {
 reg_gcmc_vm_fb_location_base_t fb_base_reg   = { 0 };
 reg_gcmc_vm_fb_location_top_t  fb_top_reg    = { 0 };
 reg_gcmc_vm_fb_offset_t        fb_offset_reg = { 0 };

 regs2_ioc_data_t ioc_data = { 0 };
 dev_op_reg32(
   dev, REG_GCMC_VM_FB_LOCATION_BASE, ioc_data, REG_OP_READ, &amp;amp;fb_base_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_LOCATION_TOP, ioc_data, REG_OP_READ, &amp;amp;fb_top_reg.raw);
 dev_op_reg32(dev, REG_GCMC_VM_FB_OFFSET, ioc_data, REG_OP_READ, &amp;amp;fb_offset_reg.raw);

 u64 fb_offset = (u64)fb_offset_reg.fb_offset;

 // TODO(hadi): add zfb mode support
 bool zfb = fb_top_reg.fb_top + 1 &amp;lt; fb_base_reg.fb_base;
 HDB_ASSERT(!zfb, "ZFB mode is not implemented yet!");

 // printf(
 //   "fb base: 0x%x\nfb_top: 0x%x\nfb_offset: 0x%x\n",
 //   fb_base_reg.raw,
 //   fb_top_reg.raw,
 //   fb_offset_reg.raw);

 gc_11_reg_t pt_start_lo_id = { 0 };
 gc_11_reg_t pt_start_hi_id = { 0 };
 gc_11_reg_t pt_end_lo_id   = { 0 };
 gc_11_reg_t pt_end_hi_id   = { 0 };
 gc_11_reg_t pt_base_hi_id  = { 0 };
 gc_11_reg_t pt_base_lo_id  = { 0 };
 gc_11_reg_t ctx_cntl_id    = { 0 };

 switch (vmid) {
 case 0:
  pt_start_lo_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT0_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT0_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT0_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT0_CNTL;
  break;
 case 1:
  pt_start_lo_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT1_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT1_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT1_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT1_CNTL;
  break;
 case 2:
  pt_start_lo_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT2_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT2_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT2_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT2_CNTL;
  break;
 case 3:
  pt_start_lo_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT3_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT3_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT3_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT3_CNTL;
  break;
 case 4:
  pt_start_lo_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT4_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT4_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT4_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT4_CNTL;
  break;
 case 5:
  pt_start_lo_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT5_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT5_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT5_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT5_CNTL;
  break;
 case 6:
  pt_start_lo_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT6_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT6_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT6_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT6_CNTL;
  break;
 case 7:
  pt_start_lo_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT7_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT7_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 8:
  pt_start_lo_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT8_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT8_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 9:
  pt_start_lo_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT9_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT9_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT7_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT7_CNTL;
  break;
 case 10:
  pt_start_lo_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT10_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT10_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT10_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT10_CNTL;
  break;
 case 11:
  pt_start_lo_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT11_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT11_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT11_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT11_CNTL;
  break;
 case 12:
  pt_start_lo_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT12_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT12_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT12_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT12_CNTL;
  break;
 case 13:
  pt_start_lo_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT13_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT13_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT13_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT13_CNTL;
  break;
 case 14:
  pt_start_lo_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT14_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT14_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT14_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT14_CNTL;
  break;
 case 15:
  pt_start_lo_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_LO32;
  pt_start_hi_id = REG_GCVM_CONTEXT15_PAGE_TABLE_START_ADDR_HI32;
  pt_end_lo_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_LO32;
  pt_end_hi_id   = REG_GCVM_CONTEXT15_PAGE_TABLE_END_ADDR_HI32;
  pt_base_lo_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_LO32;
  pt_base_hi_id  = REG_GCVM_CONTEXT15_PAGE_TABLE_BASE_ADDR_HI32;
  ctx_cntl_id    = REG_GCVM_CONTEXT15_CNTL;
  break;
 default: HDB_ASSERT(false, "Out of range VMID 0-15 trying to access %u", vmid);
 }

 // all the types of the contexts are the same so will just use 0 but pass the correct
 // register enum to the read function
 reg_gcvm_context0_page_table_start_addr_lo32_t pt_start_lo = { 0 };
 reg_gcvm_context0_page_table_start_addr_hi32_t pt_start_hi = { 0 };
 reg_gcvm_context0_page_table_end_addr_lo32_t   pt_end_lo   = { 0 };
 reg_gcvm_context0_page_table_end_addr_hi32_t   pt_end_hi   = { 0 };
 reg_gcvm_context0_page_table_base_addr_lo32_t  pt_base_lo  = { 0 };
 reg_gcvm_context0_page_table_base_addr_hi32_t  pt_base_hi  = { 0 };
 reg_gcvm_context0_cntl_t                       ctx_cntl    = { 0 };

 dev_op_reg32(dev, pt_start_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_lo.raw);
 dev_op_reg32(dev, pt_start_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_start_hi.raw);
 dev_op_reg32(dev, pt_end_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_lo.raw);
 dev_op_reg32(dev, pt_end_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_end_hi.raw);
 dev_op_reg32(dev, pt_base_lo_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_lo.raw);
 dev_op_reg32(dev, pt_base_hi_id, ioc_data, REG_OP_READ, &amp;amp;pt_base_hi.raw);
 dev_op_reg32(dev, ctx_cntl_id, ioc_data, REG_OP_READ, &amp;amp;ctx_cntl.raw);

 u64 pt_start_addr = ((u64)pt_start_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_start_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_end_addr   = ((u64)pt_end_lo.raw &amp;lt;&amp;lt; 12) | ((u64)pt_end_hi.raw &amp;lt;&amp;lt; 44);
 u64 pt_base_addr  = ((u64)pt_base_lo.raw &amp;lt;&amp;lt; 0) | ((u64)pt_base_hi.raw &amp;lt;&amp;lt; 32);
 u32 pt_depth      = ctx_cntl.page_table_depth;
 u32 ptb_size      = ctx_cntl.page_table_block_size;

 HDB_ASSERT(pt_base_addr != 0xffffffffffffffffull, "Invalid page table base addr");

 printf(
   "\tPage Table Start: 0x%lx\n\tPage Table End: 0x%lx\n\tPage Table Base: "
   "0x%lx\n\tPage Table Depth: %u\n\tBlock Size: %u\n",
   pt_start_addr,
   pt_end_addr,
   pt_base_addr,
   pt_depth,
   ptb_size);

 // decode base PDB
 pde_t pde = decode_pde(pt_base_addr);
 pt_base_addr -= fb_offset * !pde.system; // substract only on vram

 u64 pt_last_byte_addr = pt_end_addr + 0xfff; // 0xfff is 1 page
 HDB_ASSERT(
   pt_start_addr &amp;lt;= va_addr || va_addr &amp;lt; pt_last_byte_addr,
   "Invalid virtual address outside the range of the root page table of this vm");

 va_addr -= pt_start_addr;
 //
 // Size of the first PDB depends on the total coverage of the
 // page table and the PAGE_TABLE_BLOCK_SIZE.
 // Entire table takes ceil(log2(total_vm_size)) bits
 // All PDBs except the first one take 9 bits each
 // The PTB covers at least 2 MiB (21 bits)
 // And PAGE_TABLE_BLOCK_SIZE is log2(num 2MiB ranges PTB covers)
 // As such, the formula for the size of the first PDB is:
 //                       PDB1, PDB0, etc.      PTB covers at least 2 MiB
 //                                        Block size can make it cover more
 //   total_vm_bits - (9 * num_middle_pdbs) - (page_table_block_size + 21)
 //
 // we need the total range range here not the last byte addr like above
 u32 total_vaddr_bits = log2_range_round_up(pt_start_addr, pt_end_addr + 0x1000);

 u32 total_pdb_bits = total_vaddr_bits;
 // substract everything from the va_addr to leave just the pdb bits
 total_pdb_bits -= 9 * (pt_depth - 1); // middle PDBs each is 9 bits
 total_pdb_bits -= (ptb_size + 21);    // at least 2mb(21) bits + ptb_size

 // u64 va_mask = (1ull &amp;lt;&amp;lt; total_pdb_bits) - 1;
 // va_mask &amp;lt;&amp;lt;= (total_vaddr_bits - total_pdb_bits);

 // pde_t pdes[8]  = { 0 };
 // u32   curr_pde = 0;
 // u64   pde_addr = 0;
 // u64  loop_pde = pt_base_addr;

 if (pt_depth == 0) { HDB_ASSERT(false, "DEPTH = 0 is not implemented yet"); }

 pde_t curr_pde    = pde;
 u64   entry_bits  = 0;
 s32   curr_depth  = pt_depth;
 bool  pde0_is_pte = false;
 // walk all middle PDEs
 while (curr_depth &amp;gt; 0) {
  // printf("pde(%u):0x%lx \n", curr_depth, curr_pde.pte_base_addr);
  u64 next_entry_addr = 0;

  u32 shift_amount = total_vaddr_bits;
  shift_amount -= total_pdb_bits;
  // for each pdb shift 9 more
  shift_amount -= ((pt_depth - curr_depth) * 9);

  // shift address and mask out unused bits
  u64 next_pde_idx = va_addr &amp;gt;&amp;gt; shift_amount;
  next_pde_idx &amp;amp;= 0x1ff;

  // if on vram we need to apply this offset
  if (!curr_pde.system) curr_pde.pte_base_addr -= fb_offset;

  next_entry_addr = curr_pde.pte_base_addr + next_pde_idx * 8;
  curr_depth--;

  if (!curr_pde.system) {
   dev_linear_vram(dev, next_entry_addr, 8, &amp;amp;entry_bits);
   curr_pde = decode_pde(entry_bits);
   printf(
     "\tPage Dir Entry(%u):\n\t  Addr:0x%lx\n\t  Base: 0x%lx\n\n\t        ↓\n\n",
     curr_depth,
     next_entry_addr,
     curr_pde.pte_base_addr);
  } else {
   HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
  }

  if (!curr_pde.valid) { break; }

  if (curr_pde.pte) {
   // PDB0 can act as a pte
   // also I'm making an assumption here that UMRs code doesn't make
   // that the the PDB0 as PTE path can't have the further bit set
   pde0_is_pte = true;
   break;
  }
 }

 if (pde0_is_pte) { HDB_ASSERT(false, "PDE0 as PTE is not implemented yet"); }

 // page_table_block_size is the number of 2MiB regions covered by a PTB
 // If we set it to 0, then PTB cover 2 MiB
 // If it's 9 PTB cover 1024 MiB
 // pde0_block_fragment_size tells us how many 4 KiB regions each PTE covers
 // If it's 0 PTEs cover 4 KiB
 // If it's 9 PTEs cover 2 MiB
 // So the number of PTEs in a PTB is 2^(9+ptbs-pbfs)
 //
 // size here is actually the log_2 of the size
 u32 pte_page_size  = curr_pde.frag_size;
 u32 ptes_per_ptb   = 9 + ptb_size - pte_page_size;
 u64 pte_index_mask = (1ul &amp;lt;&amp;lt; ptes_per_ptb) - 1;

 u32 pte_bits_count   = pte_page_size + 12;
 u64 page_offset_mask = (1ul &amp;lt;&amp;lt; pte_bits_count) - 1; // minimum of 12

 u64 pte_index = (va_addr &amp;gt;&amp;gt; pte_bits_count) &amp;amp; pte_index_mask;
 u64 pte_addr  = curr_pde.pte_base_addr + pte_index * 8;

 pte_t pte = { 0 };
 if (!curr_pde.system) {
  dev_linear_vram(dev, pte_addr, 8, &amp;amp;entry_bits);
  pte = decode_pte(entry_bits);

  printf("\tPage Table Entry: 0x%lx\n", pte.page_base_addr);
 } else {
  HDB_ASSERT(false, "GTT physical memory access is not implemented yet");
 }

 if (pte.further) { HDB_ASSERT(false, "PTE as PDE walking is not implemented yet"); }
 if (!pte.system) pte.page_base_addr -= fb_offset;

 u64 offset_in_page = va_addr &amp;amp; page_offset_mask;
 u64 physical_addr  = pte.page_base_addr + offset_in_page;
 printf("\tFinal Physical Address: 0x%lx\n", physical_addr);
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Other processes need to have a s_trap instruction or have trap on exception flags set, which is not true for most normal GPU processes. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Available since RDNA3, if I’m not mistaken. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;VGPRs are unique per thread, and SGPRs are unique per wave ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can get that by subtracting the current program counter from the address of the code buffer. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://thegeeko.me/blog/amd-gpu-debugging/"/><published>2025-12-08T16:06:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46194337</id><title>Let's put Tailscale on a jailbroken Kindle</title><updated>2025-12-09T06:17:49.758610+00:00</updated><content>&lt;doc fingerprint="77000a1e01fd3fdc"&gt;
  &lt;main&gt;
    &lt;p&gt;“It’s a rite of passage to run Tailscale on weird devices.”&lt;/p&gt;
    &lt;p&gt;So writes Mitanshu Sukhwani on his blog, detailing the steps for getting Tailscale onto a jailbroken Kindle. Getting there, and seeing a kindle entry with a satisfying green dot in your Tailscale admin console, takes some doing. But take the trip, and you’ll end up with an e-reader that can run some neat unofficial apps, and is more open to third-party and DRM-free ebooks. And with a Tailscale connection, it’s easier to connect to files and a command line on your underpowered little Linux slab.&lt;/p&gt;
    &lt;p&gt;“For me, it's the freedom of being able to do anything with the device I own,” Sukhwani writes by email. “What I can do with the freedom is a different story.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a jailbroken Kindle, exactly?&lt;/head&gt;
    &lt;p&gt;Jailbreaking refers to removing the software restrictions on a device put there by its maker. Getting around these restrictions, typically by gaining “root” or administrative access, allows for accessing operating system internals, running unapproved software, and generally doing more things than a manufacturer intended. With the Kindle, you still get the standard Kindle reading experience, including Amazon's store and the ability to send the Kindle books from apps like Libby. You just add many more options, too.&lt;/p&gt;
    &lt;p&gt;The term gained purchase after the first iPhone’s debut in mid-2007; since then, nearly every device with a restricted environment has gained its own jailbreaking scene, including Kindles (debuting five months after the iPhone).&lt;/p&gt;
    &lt;p&gt;Kindle jailbreaks come along every so often. Right now, an unlocking scheme based on Amazon’s own lockscreen ads, “AdBreak,” is available for all but the most up-to-date Kindles (earlier than firmware version 5.18.5.0.2). I know this because I wrote this paragraph and the next on my 11th-generation Kindle, using the open-source Textadept editor, a Bluetooth keyboard, and Tailscale to move this draft file around.&lt;/p&gt;
    &lt;p&gt;One paragraph doesn’t seem that impressive until you consider that on a standard Kindle, you cannot do any of that. Transferring files by SSH, or Taildrop, is certainly not allowed. And that’s in addition to other upgrades you can get by jailbreaking a Kindle, including the feature-rich, customizable e-reader KOReader, and lots of little apps available in repositories like KindleForge.&lt;/p&gt;
    &lt;p&gt;If your Kindle has been connected to Wi-Fi all this time (as of early December 2025), it may have automatically updated itself and no longer be ready for jailbreaking. If you think it still has a chance, immediately put it into airplane mode and follow along.&lt;/p&gt;
    &lt;p&gt;Obligatory notice here: You’re running a risk of bricking your device (having it become unresponsive and unrecoverable) and voiding your warranty when you do this. That having been noted, let's dig further.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Tailscale adds to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Tailscale isn’t necessary on a jailbroken Kindle, but it really helps. Here are some of the ways Tailscale makes messing about with an opened-up Kindle more fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A persistent IP address (100.xx.yyy.zzz), just like any other Tailscale device, instead of having to remember yet another 192.168.random.number&lt;/item&gt;
      &lt;item&gt;Easier SSH access with magicDNS: ssh root@kindle and you’re in&lt;/item&gt;
      &lt;item&gt;Taildrop for sending files to whatever Kindle directory you want&lt;/item&gt;
      &lt;item&gt;Setting up a self-hosted Calibre Web library with Tailscale, then securely grabbing books from it anywhere with KOReader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key to the Kindle-plus-Tailscale experience is an easier way (SSH and Taildrop) to get epub, mobi, and other e-book and document formats into the /documents folder, ready for your KOReader sessions. Tailscale also helps with setting up some of the key jailbreak apps, saving you from plugging and unplugging the Kindle into a computer via USB cord (and then finding a second USB cord, because the first one never works, for some reason).&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting your Kindle ready&lt;/head&gt;
    &lt;p&gt;What follows is by no means a comprehensive guide to jailbreaking and accessing your Kindle. You will want to read the documentation for each tool and app closely. Pay particular attention to which Kindle you have, which version number of the Kindle firmware it’s running, and how much space you have left on that device.&lt;/p&gt;
    &lt;p&gt;The first step is to check your Kindle’s version number (Settings &amp;gt; Device info) and see if there is a jailbreak method available for it. The Kindle Modding Wiki is the jailbreaking community’s go-to resource. As of this writing, there is a “WinterBreak” process available for Kindles running firmware below 15.18.1, and AdBreak is available for firmwares from 15.18.1 through 5.18.5.0.1.&lt;/p&gt;
    &lt;p&gt;If your Kindle’s version number fits one of those ranges, put it in Airplane mode and move on. If not, you’re going to have to wait until the next jailbreak method comes along.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual jailbreaking part&lt;/head&gt;
    &lt;p&gt;Before you dive in, have a computer (PC, Mac, or Linux) and USB cable that works with your Kindle handy. Have your Kindle on reliable Wi-Fi, like your home network—but don’t take your Kindle off airplane mode if you’ve been keeping it that way.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow these steps to jailbreak your Kindle. The techniques are different, but you may need to do some other tasks, like enable advertisements, or fill your Kindle with junk files to prevent automatic updates midway through the process.&lt;/item&gt;
      &lt;item&gt;Install a hotfix and disable over-the-air updates so that you can keep your Kindle on Wi-Fi and not have its jailbreak undone&lt;/item&gt;
      &lt;item&gt;Install the Kindle Unified Application Launcher (KUAL) and MRPI (MobileRead Package Installer). KUAL is vital to installing most jailbroken apps, including Tailscale.&lt;/item&gt;
      &lt;item&gt;You will almost certainly want to install KOReader, too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Those bits above are standard jailbreaking procedures. If you want Tailscale on your Kindle, you’ll go a bit further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding Tailscale to a jailbroken Kindle&lt;/head&gt;
    &lt;p&gt;Make sure you have KUAL and MRPI installed and working. Next up: install this “simple” version of USBNetworking for Kindle.&lt;/p&gt;
    &lt;p&gt;Before you go further, you’ll want to choose between Mitanshu’s “standard” Tailscale repository, or the fork of it that enables Taildrop. I recommend the Taildrop-enabled fork; if it goes wrong, or stops being updated, it’s fairly easy (relative to doing this kind of project) to wipe it and go back to Mitanshu’s “vanilla” version.Either way, you’ll want to get USB access to your Kindle for this next part. If you toggled on USBNetworking to try it out, toggle it off; you can’t get USB access while it’s running, as its name somewhat implies.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the Tailscale/KUAL repository of your choice using git clone or download a ZIP from the Code button on GitHub&lt;/item&gt;
      &lt;item&gt;Head to Tailscale’s page of static Linux binaries and grab the latest arm (not arm64) release&lt;/item&gt;
      &lt;item&gt;Copy the tailscale and tailscaled binaries from the Tailscale download and place them into the /extensions/tailscale/bin directory of the KUAL/Kindle repository you’ll be copying over&lt;/item&gt;
      &lt;item&gt;Head to your Tailscale admin console and generate an authentication key. Name it something like kindle; you’ll want to enable the “Reusable” and “Pre-approved” options. Copy the key that is generated.&lt;/item&gt;
      &lt;item&gt;Open the file extensions/tailscale/config/auth_key.txt for editing while it is on your (non-Kindle) computer. Paste in the key text you generated.&lt;/item&gt;
      &lt;item&gt;If you’re using the variant with Taildrop, you can set a custom directory in which to deliver Taildrop files by editing extensions/tailscale/config/taildrop_dir.txt; setting /mnt/us/documents makes sense if you’re mostly sending yourself things to read in KOReader.&lt;/item&gt;
      &lt;item&gt;Head into the extensions folder on your computer and copy the tailscale folder you’ve set up into the extensions folder on your Kindle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all that done, open up KUAL on your Kindle. Go into USBNetLite and click USBNetwork Status to ensure it is enabled (tap the Toggle button if not). Go back (with the “/” button at the bottom), tap Tailscale, and first tap Start Tailscaled (note the “d” at the end). Wait about 10 seconds to give the Tailscaled daemon time to start, then tap Start Tailscale.&lt;/p&gt;
    &lt;p&gt;If everything is settled, you should be able to see your Kindle as connected on your Tailscale admin console. Once you’ve finished smiling to yourself, click the three dots on the right-hand side of the Kindle row and select “Disable key expiry.” In most situations, you’re better off not having to patch a new key value into a Kindle text file every few months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enjoy your (slightly) less wonky Kindle&lt;/head&gt;
    &lt;p&gt;With Tailscale installed, it’s easier to get into your Kindle via SSH for file management and installing and configuring other apps. Getting a Bluetooth keyboard to work via the Kindle’s quirky command-line Bluetooth interface would not have been fun using a touchscreen keyboard.&lt;/p&gt;
    &lt;p&gt;Because the Kindle is on your tailnet, it can access anything else you have hosted there. Kindles set up this way can use tools like the Shortcut Browser to become dashboards for Home Assistant, or access a self-hosted Calibre-Web e-book server (with some tweaking).&lt;/p&gt;
    &lt;p&gt;Having Taildrop handy, and having it drop files directly into the documents folder, is probably my favorite upgrade. I was on my phone, at a train station, when I came across Annalee Newitz’s Automatic Noodle at Bookshop.org. I bought it on my phone and downloaded the DRM-free epub file. When I got home, I opened and unlocked my Kindle, sent the epub to the Kindle via Taildrop, then tapped Receive Taildrop Files in the Tailscale app inside KUAL. Epubs, PDFs, comic book archives, DjVu files—they’re all ready to be dropped in.&lt;/p&gt;
    &lt;p&gt;If you’ve gotten Tailscale to run on weird (or just uncommon) devices, we’d more than love to hear about it. Let us know on Reddit, Discord, Bluesky, Mastodon, or LinkedIn.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://tailscale.com/blog/tailscale-jailbroken-kindle"/><published>2025-12-08T16:34:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46194384</id><title>Hunting for North Korean Fiber Optic Cables</title><updated>2025-12-09T06:17:49.632910+00:00</updated><content>&lt;doc fingerprint="2fecda19652117ea"&gt;
  &lt;main&gt;
    &lt;p&gt;Before we go any further, one thing that I want to make clear is that the word assume is going to be doing some heavy lifting throughout this post. This was a rabbit hole that I recently went down and I probably have more questions than answers, but I still wanted to document what I had found so far. If you have additional information or findings you want to share, as always feel free to reach out: contact@nkinternet.com.&lt;/p&gt;
    &lt;p&gt;It all started with a PowerPoint that I came across a few weeks ago. It was presented by the DPRK to the ICAO on the state of their aviation industry and their ADS-B deployment inside North Korea. However, one slide in particular caught my eye because it showed a fiber optic cable running across the country&lt;/p&gt;
    &lt;p&gt;This got me wondering more about the physical layout of the network inside North Korea. From the map we know that there’s a connection between Pyongyang and Odaejin, although given the mountains in the middle of the country it probably isn’t a direct link. There isn’t a lot of information on fiber in North Korea, but there are a few outside sources that help provide clues about how things might be laid out.&lt;/p&gt;
    &lt;p&gt;Historic Fiber Information&lt;/p&gt;
    &lt;p&gt;38North first reported the connection from Russia’s TTK to the DPRK over the Korea–Russia Friendship Bridge back in 2017. Additionally, a picture found on Flickr looking toward Tumangang after the bridge doesn’t show any utility poles and instead seems to display some kind of infrastructure in the grass to the side of the tracks. Assuming this interpretation is correct, the fiber is likely buried underground as it enters the country and passes through the vicinity of Tumangang Station.&lt;/p&gt;
    &lt;p&gt;According to a report from The Nautilus Institute we can gather a few additional details about the internet inside North Korea&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One of the first lines was installed in September 1995 between Pyongyang and Hamhung&lt;/item&gt;
      &lt;item&gt;In February 1998 a link between Pyongyang and Sinuiju was completed&lt;/item&gt;
      &lt;item&gt;As of 2000, DPRK’s operational optical fiber telecom lines included: Pyongyang – Hamhung; Pyongyang – Sinuiju including all cities and counties in North Pyongan Province; Hamhung Rajin-Sonbong; Rajin-Songbong – Hunchun (China), Pyongyang – Nampo.&lt;/item&gt;
      &lt;item&gt;In 2003 the original domestic cell phone network was built for North Korean citizens in Pyongyang, Namp’o, reportedly in all provincial capitals, on the Pyongyang-Myohyangsan tourist highway, and the Pyongyang-Kaesong and Wonsan-Hamhung highways&lt;/item&gt;
      &lt;item&gt;The Kwangmyong network’s data is transmitted via fiber optic cable with a backbone capacity of 2.5 GB per second between all the provinces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on these notes, it starts to paint a picture that the fiber link coming from Russia likely travels down the east coast of the DPRK before connecting to Pyongyang. Several city pairs—Pyongyang–Hamhung and Rajin–Sonbong—line up with earlier deployments of east-coast fiber infrastructure.&lt;/p&gt;
    &lt;p&gt;Kwangmyong Internal Topology&lt;/p&gt;
    &lt;p&gt;The report also notes that all of the provinces in North Korea were connected to the Kwangmyong via fiber. The Kwangmyong for those not familiar is the intranet that most citizens in the DPRK can access as they do not have access to the outside internet. While not much information is available about the Kwangmyong, these notes from Choi Sung, Professor of Computer Science at Namseoul University provides some additional details on how the network is laid how, as well as information on the regional networks that are connected. A map provided in his notes shows some of the main points of the Kwangmyong with three of them located along the northeast of North Korea.&lt;/p&gt;
    &lt;p&gt;Railways, Roads, and Practical Fiber Routing&lt;/p&gt;
    &lt;p&gt;This starts to paint a rough picture of how the network is physically deployed in North Korea but we can also look to some outside sources to get some confirmation. 38North once again provides some great detail on cell phone towers in North Korea. The interesting thing being an apparent line down the east coast which follows major roads and highways but would also in theory have easier access to the fiber back haul to support the cell network.&lt;/p&gt;
    &lt;p&gt;All of this seems to suggest that the fiber lines were run along major roads and railways up the east coast. A map from Beyond Parallel shows the major rail lines, which has the Pyongra line up the east coast.&lt;/p&gt;
    &lt;p&gt;Looking For Clues Along the Railway&lt;/p&gt;
    &lt;p&gt;Some additional digging for pictures from along the line suggest that there is infrastructure deployed along the tracks, although it’s difficult to confirm from pictures exactly what is buried. The following shows what appears to be a junction box at the base of a pole along the line.&lt;/p&gt;
    &lt;p&gt;The line does have a path along it as well with mile markers. While it is used by bikes and pedestrians, it provides a nice path for supporting fiber and other communications runs along the tracks.&lt;/p&gt;
    &lt;p&gt;The Pyongra line also crosses through the mountains at points but it is assumed at certain junctions the fiber was laid along the AH 6/National Highway 7 up the coast as there are parts of the line discovered that do not have a path along the tracks. In these places it is assumed they follow the road, although finding pictures of the highway to further examine is challenging.&lt;/p&gt;
    &lt;p&gt;Lastly at certain stations we can see utility boxes along the side of the track suggesting buried conduits/cables are laid along the tracks.&lt;/p&gt;
    &lt;p&gt;From a video taken in 2012 there does appear to be some signs of objects along the tracks, although difficult to confirm due to the video quality. The screenshot below is the clearest I could find of a rectangular box buried in a clearing along the line.&lt;/p&gt;
    &lt;p&gt;Based on this information of what is confirmed and looking at major cities, it appears there is a route that follows Pyongyang → Wonsan → Hamhung → Chongjin → Rajin → Tumangang which follows the Pyongra line as well as the AH 6/National Highway 7 up the coast. The following map highlights a rough path.&lt;/p&gt;
    &lt;p&gt;Interestingly by mapping out the possible fiber locations we can start to draw conclusions based on other sources. According to a video by Cappy’s Army he proposes that when the US Navy Seals landed in NOrth Korea in 2019 the most likely place this would have occurred is Sinpo. As the goal was to depoy a covert listening device this could also line up with supporting the idea that a fiber backbone runs down the east coast of North Korea as Sinpo would be relatively close.&lt;/p&gt;
    &lt;p&gt;What Does This Mean For the Network?&lt;/p&gt;
    &lt;p&gt;In addition to the fiber link via Russia, the other fiber optic cable into North Korea comes in via China by way of Sinuiju and Dandong. Although we don’t know for sure where servers are deployed inside North Korea, based on the map of Kwangmyong the first assumption is that things are mainly centralized in Pyongyang.&lt;/p&gt;
    &lt;p&gt;Out of the 1,024 IPs assigned to North Korea we observe the following behavior based on the CIDR block:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;175.45.176.0/24 is exclusively routed via China Unicom&lt;/item&gt;
      &lt;item&gt;175.45.177.0/24 is exclusively routed via Russia TransTelekom&lt;/item&gt;
      &lt;item&gt;175.45.178.0/24 is dual-homed and can take either path before crossing into North Korea&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this information in mind, running a traceroute with the TCP flag set gives us a slightly better look at how traffic behaves once it reaches the country. For the following tests we’re going to assume there is a fiber path on the west coming in from China toward Pyongyang, as well as a path on the east side coming from Russia.&lt;/p&gt;
    &lt;p&gt;From the US east coast to 175.45.176.71, the final hop in China before entering North Korea shows roughly 50 ms of additional latency before reaching the DPRK host. This suggests there may be extra devices, distance, or internal routing inside the country before the packet reaches its final destination.&lt;/p&gt;
    &lt;quote&gt;10 103.35.255.254 (103.35.255.254) 234.306 ms 234.082 ms 234.329 ms&lt;lb/&gt;11 * * *&lt;lb/&gt;12 * * *&lt;lb/&gt;13 * * *&lt;lb/&gt;14 175.45.176.71 (175.45.176.71) 296.081 ms 294.795 ms 294.605 ms&lt;lb/&gt;15 175.45.176.71 (175.45.176.71) 282.938 ms 284.446 ms 282.227 ms&lt;/quote&gt;
    &lt;p&gt;Interestingly, running a traceroute to 175.45.177.10 shows a similar pattern in terms of missing hops, but with much lower internal latency. In fact, the ~4 ms difference between the last Russian router and the DPRK host suggests the handoff between Russia and North Korea happens very close—network-wise—to where this device is located. This contrasts with the China path, which appears to take a longer or more complex route before reaching its final destination.&lt;/p&gt;
    &lt;quote&gt;10 188.43.225.153 185.192 ms 183.649 ms 189.089 ms&lt;lb/&gt;11 * *&lt;lb/&gt;12 * *&lt;lb/&gt;13 * *&lt;lb/&gt;14 175.45.177.10 195.996 ms 186.801 ms 186.353 ms&lt;lb/&gt;15 175.45.177.10 188.886 ms 201.103 ms 193.334&lt;/quote&gt;
    &lt;p&gt;If everything is centralized in Pyongyang this would mean the handoff from Russia is completed in Pyongyang as well. However, it could also indicate that 175.45.177.0/24 is not hosted in Pyongyang at all and is instead located closer to the Russia–North Korea border. More testing is definitely required however before any conclusions can be drawn about where these devices physically reside.&lt;/p&gt;
    &lt;p&gt;What can we learn from all of this?&lt;/p&gt;
    &lt;p&gt;Making some assumptions we can get a better idea of how the internet works and is laid out inside North Korea. While not much is officially confirmed using some other sources we can get a possible idea of how things work. As mentioned at the start, the word assume does a lot of heavy lifting. However if you do have other information or ideas feel free to reach out at contact@nkinternet.com&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from North Korean Internet&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nkinternet.com/2025/12/08/hunting-for-north-korean-fiber-optic-cables/"/><published>2025-12-08T16:38:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46194477</id><title>A series of tricks and techniques I learned doing tiny GLSL demos</title><updated>2025-12-09T06:17:49.008140+00:00</updated><content>&lt;doc fingerprint="b2a292151eec032f"&gt;
  &lt;main&gt;
    &lt;p&gt;In the past two months or so, I spent some time making tiny GLSL demos. I wrote an article about the first one, Red Alp. There, I went into details about the whole process, so I recommend to check it out first if you're not familiar with the field.&lt;/p&gt;
    &lt;p&gt;We will look at 4 demos: Moonlight, Entrance 3, Archipelago, and Cutie. But this time, for each demo, we're going to cover one or two things I learned from it. It won't be a deep dive into every aspect because it would be extremely redundant. Instead, I'll take you along a journey of learning experiences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moonlight&lt;/head&gt;
    &lt;code&gt;// Moonlight [460] by bµg
// License: CC BY-NC-SA 4.0
void main(){vec3 o,p,u=vec3((P+P-R)/R.y,1),Q;Q++;for(float d,a,m,i,t;i++&amp;lt;1e2;p=t&amp;lt;7.2?Q:vec3(2,1,0),d=abs(d)*.15+.1,o+=p/m+(t&amp;gt;9.?d=9.,Q:p/d),t+=min(m,d))for(p=normalize(u)*t,p.z-=5e1,m=max(length(p)-1e1,.01),p.z+=T,d=5.-length(p.xy*=mat2(cos(t*.2+vec4(0,33,11,0)))),a=.01;a&amp;lt;1.;a+=a)p.xz*=mat2(8,6,-6,8)*.1,d-=abs(dot(sin(p/a*.6-T*.3),p-p+a)),m+=abs(dot(sin(p/a/5.),p-p+a/5.));o/=4e2;O=vec4(tanh(mix(vec3(-35,-15,8),vec3(118,95,60),o-o*length(u.xy*.5))*.01),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;In Red Alp, I used volumetric raymarching to go through the clouds and fog, and it took quite a significant part of the code to make the absorption and emission convincing. But there is an alternative technique that is surprisingly simpler.&lt;/p&gt;
    &lt;p&gt;In the raymarching loop, the color contribution at each iteration becomes 1/d or c/d where d is the density of the material at the current ray position, and c an optional color tint if you don't want to work in grayscale level. Some variants exist, for example 1/d^2, but we'll focus on 1/d.&lt;/p&gt;
    &lt;head rend="h3"&gt;1/d explanation&lt;/head&gt;
    &lt;p&gt;Let's see how it looks in practice with a simple cube raymarch where we use this peculiar contribution:&lt;/p&gt;
    &lt;code&gt;void main() {
    float d, t;
    vec3 o, p,
         u = normalize(vec3(P+P-R,R.y)); // screen to world coordinate

    for (int i = 0; i &amp;lt; 30; i++) {
        p = u * t; // ray position

        p.z -= 3.; // take a step back

        // Rodriguez rotation with an arbitrary angle of π/2
        // and unaligned axis
        vec3 a = normalize(cos(T+vec3(0,2,4)));
        p = a*dot(a,p)-cross(a,p);

        // Signed distance function of a cube of size 1
        p = abs(p)-1.;
        d = length(max(p,0.)) + min(max(p.x,max(p.y,p.z)),0.);

        // Maxed out to not enter the solid
        d = max(d,.001);

        t += d; // stepping forward by that distance

        // Our mysterious contribution to the output
        o += 1./d;
    }

    // Arbitrary scale within visible range
    O = vec4(o/200., 1);
}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The signed function of the cube is from the classic Inigo Quilez page. For the rotation you can refer to Xor or Blackle article. For the general understanding of the code, see my previous article on Red Alp.&lt;/p&gt;
    &lt;p&gt;The first time I saw it, I wondered whether it was a creative take, or if it was backed by physical properties.&lt;/p&gt;
    &lt;p&gt;Let's simplify the problem with the following figure:&lt;/p&gt;
    &lt;p&gt;The glowing object sends photons that spread all around it. The further we go from the object, the more spread these photons are, basically following the inverse square law 1/r^2, which gives the photons density, where r is the distance to the target object.&lt;/p&gt;
    &lt;p&gt;Let's say we send a ray and want to know how many photons are present along the whole path. We have to "sum", or rather integrate, all these photons density measures along the ray. Since we are doing a discrete sampling (the dots on the figure), we need to interpolate the photons density between each sampling point as well.&lt;/p&gt;
    &lt;p&gt;Given two arbitrary sampling points and their corresponding distance d_n and d_{n+1}, any intermediate distance can be linearly interpolated with r=\mathrm{mix}(d_n,d_{n+1},t) where t is within [0,1]. Applying the inverse square law from before (1/r^2), the integrated photons density between these 2 points can be expressed with this formula (in t range):&lt;/p&gt;
    &lt;p&gt;t being normalized, the \Delta t is here to covers the actual segment distance. With the help of Sympy we can do the integration:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a, b, D, t = symbols('a b D t', real=True)
&amp;gt;&amp;gt;&amp;gt; mix = a*(1-t) + b*t
&amp;gt;&amp;gt;&amp;gt; D * integrate(1/mix**2, (t,0,1)).simplify()
 D
───
a⋅b
&lt;/code&gt;
    &lt;p&gt;So the result of this integration is:&lt;/p&gt;
    &lt;p&gt;Now the key is that in the loop, \Delta t stepping is actually d_{n+1}, so we end up with:&lt;/p&gt;
    &lt;p&gt;And we find back our mysterious 1/d. It's "physically correct", assuming vacuum space. Of course, reality is more complex, and we don't even need to stick to that formula, but it was nice figuring out that this simple fraction is a fairly good model of reality.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going through the object&lt;/head&gt;
    &lt;p&gt;In the cube example we didn't go through the object, using &lt;code&gt;max(d, .001)&lt;/code&gt;. But
if we were to add some transparency, we could have used &lt;code&gt;d = A*abs(d)+B&lt;/code&gt;
instead, where &lt;code&gt;A&lt;/code&gt; could be interpreted as absorption and &lt;code&gt;B&lt;/code&gt; the pass-through,
or transparency.&lt;/p&gt;
    &lt;p&gt;I first saw this formula mentioned in Xor article on volumetric. To understand it a bit better, here is my intuitive take: the &lt;code&gt;+B&lt;/code&gt; causes a
potential penetration into the solid at the next iteration, which wouldn't
happen otherwise (or only very marginally). When inside the solid, the &lt;code&gt;abs(d)&lt;/code&gt;
causes the ray to continue further (by the amount of the distance to the closest
edge). Then the multiplication by &lt;code&gt;A&lt;/code&gt; makes sure we don't penetrate too fast
into it; it's the absorption, or "damping".&lt;/p&gt;
    &lt;p&gt;This is basically the technique I used in Moonlight to avoid the complex absorption/emission code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entrance 3&lt;/head&gt;
    &lt;code&gt;// Entrance 3 [465] by bµg
// License: CC BY-NC-SA 4.0
#define V for(s++;d&amp;lt;l&amp;amp;&amp;amp;s&amp;gt;.001;q=abs(p+=v*s)-45.,b=abs(p+vec3(mod(T*5.,80.)-7.,45.+sin(T*10.)*.2,12))-vec3(1,7,1),d+=s=min(max(p.y,-min(max(abs(p.y+28.)-17.,abs(p.z+12.)-4.),max(q.x,max(q.y,q.z)))),max(b.x,max(b.y,b.z))))
void main(){float d,s,r=1.7,l=2e2;vec3 b,v=b-.58,q,p=mat3(r,0,-r,-1,2,-1,b+1.4)*vec3((P+P-R)/R.y*20.4,30);V;r=exp(-d*d/1e4)*.2;l=length(v=-vec3(90,30,10)-p);v/=l;d=1.;V;r+=50.*d/l/l;O=vec4(pow(mix(vec3(0,4,9),vec3(80,7,2),r*r)*.01,p-p+.45),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;This demo was probably one of the most challenging, but I'm pretty happy with its atmospheric vibe. It's kind of different than the usual demos for this size.&lt;/p&gt;
    &lt;p&gt;I initially tried with some voxels, but I couldn't make it work with the light under 512 characters (the initialization code was too large, not the branchless DDA stepping). It also had annoying limitations (typically the animation was unit bound), so I fell back to a classic raymarching.&lt;/p&gt;
    &lt;p&gt;The first thing I did differently was to use an L-∞ norm instead of an euclidean norm for the distance function: every solid is a cube so it's appropriate to use simpler formulas.&lt;/p&gt;
    &lt;p&gt;For the light, it's not an illusion, it's an actual light: after the first raymarch to a solid, the ray direction is reoriented toward the light and the march runs again (it's the &lt;code&gt;V&lt;/code&gt; macro). Hitting a solid or not defines if the
fragment should be lighten up or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mobile bugs&lt;/head&gt;
    &lt;p&gt;A bad surprise of this demo was uncovering two driver bugs on mobile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One with tricky for-loop compounds on Snapdragon/Adreno because I was trying hard to avoid the macros and functions.&lt;/item&gt;
      &lt;item&gt;One with chained assignments on Imagination/PowerVR (typically affect Google Pixel Pro 10).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first was worked around with the &lt;code&gt;V&lt;/code&gt; macro (actually saved 3 characters in
the process), but the 2nd one had to be unpacked and made me lose 2 characters.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isometry&lt;/head&gt;
    &lt;p&gt;Another thing I studied was how to set up the camera in a non-perspective isometric or dimetric view. I couldn't make sense of the maths from the Wikipedia page (it just didn't work), but Sympy rescued me again:&lt;/p&gt;
    &lt;code&gt;# Counter-clockwise rotation
a, ax0, ax1, ax2 = symbols('a ax0:3')
c, s = cos(a), sin(a)
k = 1-c
rot = Matrix(3,3, [
    # col 1            col 2              # col 3
    ax0*ax0*k + c,     ax0*ax1*k + ax2*s, ax0*ax2*k - ax1*s, # row 1
    ax1*ax0*k - ax2*s, ax1*ax1*k + c,     ax1*ax2*k + ax0*s, # row 2
    ax2*ax0*k + ax1*s, ax2*ax1*k - ax0*s, ax2*ax2*k + c      # row 3
])

# Rotation by 45° on the y-axis
m45 = rot.subs({a:rad(-45), ax0:0, ax1:1, ax2:0})

# Apply the 2nd rotation on the x-axis to get the transform matrices for two
# classic projections
# Note: asin(tan(rad(30))) is the same as atan(sin(rad(45)))
isometric = m45 * rot.subs({a:asin(tan(rad(30))), ax0:1, ax1:0, ax2:0})
dimetric  = m45 * rot.subs({a:         rad(30),   ax0:1, ax1:0, ax2:0})
&lt;/code&gt;
    &lt;p&gt;Inspecting the matrices and factoring out the common terms, we obtain the following transform matrices:&lt;/p&gt;
    &lt;p&gt;The ray direction is common to all fragments, so we use the central UV coordinate (0,0) as reference point. We push it forward for convenience: (0,0,1), and transform it with our matrix. This gives the central screen coordinate in world space. Since the obtained point coordinate is relative to the world origin, to go from that point to the origin, we just have to flip its sign. The ray direction formula is then:&lt;/p&gt;
    &lt;p&gt;To get the ray origin of every other pixel, the remaining question is: what is the smallest distance we need to step back the screen coordinates such that, when applying the transformation, the view wouldn't clip into the ground at y=0.&lt;/p&gt;
    &lt;p&gt;This requirement can be modeled with the following expression:&lt;/p&gt;
    &lt;p&gt;The -1 being the lowest y-screen coordinate (which we don't want into the ground). The lazy bum in me just asks Sympy to solve it for me:&lt;/p&gt;
    &lt;code&gt;x, z = symbols("x z", real=True)
u = m * Matrix([x, -1, z])
uz = solve(u[1] &amp;gt; 0, z)
&lt;/code&gt;
    &lt;p&gt;We get z&amp;gt;\sqrt{2} for isometric, and z&amp;gt;\sqrt{3} for dimetric.&lt;/p&gt;
    &lt;p&gt;With an arbitrary scale &lt;code&gt;S&lt;/code&gt; of the coordinate we end up with the following:&lt;/p&gt;
    &lt;code&gt;const float S = 50.;
vec2 u = (P+P-R)/R.y * S; // scaled screen coordinates

float A=sqrt(2.), B=sqrt(3.);

// Isometric
vec3 rd = -vec3(1)/B,
     ro = mat3(B,0,-B,-1,2,-1,A,A,A)/A/B * vec3(u, A*S + eps);

// Dimetric
vec3 rd = -vec3(B,A,B)/A/2.,
     ro = mat3(2,0,-2,-1,A*B,-1,B,A,B)/A/2. * vec3(u, B*S + eps);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;eps&lt;/code&gt; is an arbitrary small value to make sure the y-coordinate ends up
above 0.&lt;/p&gt;
    &lt;p&gt;In Entrance 3, I used a rough approximation of the isometric setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Archipelago&lt;/head&gt;
    &lt;code&gt;// Archipelago [472] by bµg
// License: CC BY-NC-SA 4.0
#define r(a)*=mat2(cos(a+vec4(0,11,33,0))),
void main(){vec3 p,q,k;for(float w,x,a,b,i,t,h,e=.1,d=e,z=.001;i++&amp;lt;50.&amp;amp;&amp;amp;d&amp;gt;z;h+=k.y,w=h-d,t+=d=min(d,h)*.8,O=vec4((w&amp;gt;z?k.zxx*e:k.zyz/20.)+i/1e2+max(1.-abs(w/e),z),1))for(p=normalize(vec3(P+P-R,R.y))*t,p.zy r(1.)p.z+=T+T,p.x+=sin(w=T*.4)*2.,p.xy r(cos(w)*e)d=p.y+=4.,h=d-2.3+abs(p.x*.2),q=p,k-=k,a=e,b=.8;a&amp;gt;z;a*=.8,b*=.5)q.xz r(.6)p.xz r(.6)k.y+=abs(dot(sin(q.xz*.4/b),R-R+b)),k.x+=w=a*exp(sin(x=p.x/a*e+T+T)),p.x-=w*cos(x),d-=w;}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;For this infinite procedurally generated Japan, I wanted to mark a rupture with my red/orange obsession. Technically speaking, it's actually fairly basic if you're familiar with Red Alp. I used the same noise for the mountains/islands, but the water uses a different noise.&lt;/p&gt;
    &lt;p&gt;The per octave noise curve is &lt;code&gt;w=exp(sin(x))&lt;/code&gt;, with the particularity of
shifting the &lt;code&gt;x&lt;/code&gt; coordinate with its derivative: &lt;code&gt;x-=w*cos(x)&lt;/code&gt;. This is some
form of domain warping that gives the nice effect here. When I say &lt;code&gt;x&lt;/code&gt;, I'm
really referring to the x-axis position. It is not needed to work with the
z-component (xz forms the flat plane) because each octave of the fbm has a
rotation that "mixes" both axis, so &lt;code&gt;z&lt;/code&gt; is actually backed in &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;I didn't come up with the formula, but found it first one this video by Acerola. I don't know if he's the original author, but I've seen the formula being replicated in various places.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cutie&lt;/head&gt;
    &lt;code&gt;// Cutie [602] by bµg
// License: CC BY-NC-SA 4.0
#define V vec3
#define L length(p
#define C(A,B,X,Y)d=min(d,-.2*log2(exp2(X-L-A)/.2)+exp2(Y-L-B)/.2)))
#define H(Z)S,k=fract(T*1.5+s),a=V(1.3,.2,Z),b=V(1,.3*max(1.-abs(3.*k-1.),z),Z*.75+3.*max(-k*S,k-1.)),q=b*S,q+=a+sqrt(1.-dot(q,q))*normalize(V(-b.y,b.x,0)),C(a,q,3.5,2.5),C(q,a-b,2.5,2.)
void main(){float i,t,k,z,s,S=.5,d=S;for(V p,q,a,b;i++&amp;lt;5e1&amp;amp;&amp;amp;d&amp;gt;.001;t+=d=min(d,s=L+V(S-2.*p.x,-1,S))-S))p=normalize(V(P+P-R,R.y))*t,p.z-=5.,p.zy*=mat2(cos(vec4(1,12,34,1))),p.xz*=mat2(cos(sin(T)+vec4(0,11,33,0))),d=1.+p.y,C(z,V(z,z,1.2),7.5,6.),s=p.x&amp;lt;z?p.x=-p.x,z:H(z),s+=H(1.);O=vec4(V(exp(-i/(s&amp;gt;d?1e2:9.))),1);}
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;See it on its official page, or play with the code on its Shadertoy portage.&lt;/p&gt;
    &lt;p&gt;Here I got cocky and thought I could manage to fit it in 512 chars. I failed, by 90 characters. I did use the smoothmin operator for the first time: every limb of the body of Cutie is composed of two spheres creating a rounded cone (two sphere of different size smoothly merged like metaballs).&lt;/p&gt;
    &lt;p&gt;Then I used simple IK kinetics for the animation. Using leg parts with a size of 1 helped simplifying the formula and make it shorter.&lt;/p&gt;
    &lt;p&gt;You may be wondering about the smooth visuals itself: I didn't use the depth map but simply the number of iterations. Due to the nature of the raymarching algorithm, when a ray passes close to a shape, it slows down significantly, increasing the number of iterations. This is super useful because it exaggerate the contour of the shapes naturally. It's wrapped into an exponential, but &lt;code&gt;i&lt;/code&gt;
defines the output color directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;I will continue making more of those, keeping my artistic ambition low because of the 512 characters constraint I'm imposing on myself.&lt;/p&gt;
    &lt;p&gt;You may be wondering why I keep this obsession about 512 characters, and many people called me out on this one. There are actually many arguments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A tiny demo has to focus on one or two very scoped aspects of computer graphics, which makes it perfect as a learning support.&lt;/item&gt;
      &lt;item&gt;It's part of the artistic performance: it's not just techniques and visuals, the wizardry of the code is part of why it's so impressive. We're in an era of visuals, people have been fed with the craziest VFX ever. But have they seen them with a few hundreds bytes of code?&lt;/item&gt;
      &lt;item&gt;The constraint helps me finish the work: when making art, there is always this question of when to stop. Here there is an intractable point where I just cannot do more and I have to move on.&lt;/item&gt;
      &lt;item&gt;Similarly, it prevents my ambition from tricking me into some colossal project I will never finish or even start. That format has a ton of limitations, and that's its strength.&lt;/item&gt;
      &lt;item&gt;Working on such a tiny piece of code for days/weeks just brings me joy. I do feel like a craftsperson, spending an unreasonable amount of time perfecting it, for the beauty of it.&lt;/item&gt;
      &lt;item&gt;I'm trying to build a portfolio, and it's important for me to keep it consistent. If the size limit was different, I would have done things differently, so I can't change it now. If I had hundreds more characters, Red Alp might have had birds, the sky opening to lit a beam of light on the mountains, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why 512 in particular? It happens to be the size of a toot on my Mastodon instance so I can fit the code there, and I found it to be a good balance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.pkh.me/p/48-a-series-of-tricks-and-techniques-i-learned-doing-tiny-glsl-demos.html"/><published>2025-12-08T16:44:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46194828</id><title>Launch HN: Nia (YC S25) – Give better context to coding agents</title><updated>2025-12-09T06:17:48.792881+00:00</updated><content/><link href="https://www.trynia.ai/"/><published>2025-12-08T17:10:14+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46195198</id><title>AI should only run as fast as we can catch up</title><updated>2025-12-09T06:17:48.707192+00:00</updated><content>&lt;doc fingerprint="1604023751650fd5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI should only run as fast as we can catch up&lt;/head&gt;07 Dec 2025&lt;head rend="h2"&gt;AI should only run as fast as we can catch up.&lt;/head&gt;&lt;head rend="h3"&gt;The story of Daniel and Eric&lt;/head&gt;&lt;p&gt;Recently I have spoke with two of my friends who all had fun playing with AI.&lt;/p&gt;&lt;p&gt;Last month, I met with Eric, a fearless PM at a medium size startup who recently got into vibe coding with Gemini.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;After getting familiarized with Gemini, Eric was genuinely amazed by how AI quickly turns prompt into playable web applications. It served great purpose as a first prototype to communicate ideas to designers and engineers. But Eric really wanted to skip those steps and directly ship it to prod. But he couldn’t really understand that Gemini actually built a single-page HTML file that merely looks like a working app. Sadly, one cannot build a reliable enterprise product out of this. And there is really no effective way for Eric to catch up on these technical details and outpace the engineering team himself.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Last week, I had coffee with Daniel, a senior staff engineer who recently grew fond of AI coding and found it to be the true force multiplier.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Daniel was skeptical of AI at first, but lately he hasn’t wrote a single line of code for months already. What he does is just precisely prompt the AI to create new components in an existing framework (involving Kafka, postgres, AuthN/Z, and k8s infra stuff) and adhering to certain preexisting paradigms. He would just spot-check the correctness of AI’s work and quickly spin up local deployments to verify it’s indeed working. Later, he pushes the changes through code review process and lands those features. All without writing a single line of code and it’s production ready just as if he wrote them himself. To Daniel, building and shipping things fast and scalable is simpler than ever.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Interpolating between the two stories&lt;/head&gt;&lt;p&gt;After speaking with Eric and Daniel, I suddenly feel that there is an overarching theme around the use of AI that we can probably interpolate out of the stories here. And after pondering for a weekend, I think I can attempt to describe it now: it’s the problem of reliable engineering - how can we make AI work reliably.&lt;/p&gt;&lt;p&gt;With the AI superpower, one can task it to do all crazy things on the internet with just typing a few lines of prompt. AI always thinks and learns faster than us, this is undeniable now. However, to make AI work actually useful (not only works, but reliable and trustworthy), we also need to catch up with what the AI does as quickly as possible.&lt;/p&gt;&lt;p&gt;It’s almost like - we need to send the AI off to learn and think as fast as possible, but we also need to catch up as soon as possible to make it all relevant. And the speed we catch up things is critical to whether AI can help us effectively do these tasks. For the case of Daniel, he can spot-check and basically just skim through AI’s work and know for sure it’s doing the right thing with a few simple tests steps to verify, hence his results are more reliable. Whereas for Eric, he needs to basically learn software development from the bottom up to comprehend what the AI has done, and that really doesn’t give him the edge to outpace engineering teams to ship features reliably by himself.&lt;/p&gt;&lt;head rend="h3"&gt;Where AI exploded: fast verification, slow learning and creation&lt;/head&gt;&lt;p&gt;To generalize the problem again, I think for all the tasks we do, we can break them down into two parts: learning/creation and verification. Basically doing the task and checking if the task is done right. Interestingly, this gives us a good perspective to our relationship with AI on performing such tasks.&lt;/p&gt;&lt;p&gt;Effort wise, if verification « learning/creation, one can very effectively check AI’s work and be confident about its reliability.&lt;/p&gt;&lt;p&gt;If verification ~= learning/creation, one spends equal amount of time checking AI’s work. It’s not a big win, maybe AI becomes a good automation script to cut down some boilerplate.&lt;/p&gt;&lt;p&gt;If verification » learning/creation, one cannot be sure about AI’s work that easily, and we are in the vibe-land.&lt;/p&gt;&lt;p&gt;A very good example of the first category is image (and video) generation. Drawing/rendering a realistic looking image is a crazily hard task. Have you tried to make a slide look nicer? It will take me literally hours to center the text boxes to make it look “good”. However, you really just need to take a look at the output of Nano Banana and you can tell if it’s a good render or a bad one based on how you feel. The verification is literally instantaneous and effortless because it’s all encoded as feeling or vibes in your brain. “Does this look right?” probably can be answered in the span of milliseconds by your vision cortex. There is also no special knowledge required - human beings have been evaluating visual images since birth, hardwired into our instincts.&lt;/p&gt;&lt;p&gt;The significant cost asymmetry can greatly explain why AI image generation exploded. If we can look for similar scenarios, we can probably identify other “killer” use cases of AI as well.&lt;/p&gt;&lt;head rend="h3"&gt;Verification debt: scarier than tech debt&lt;/head&gt;&lt;p&gt;However, if we go down into the bottom of the spectrum where verification becomes more intense - requiring domain knowledge, technical expertise, industry know-hows to tell if the AI is producing slop or not, we will enter this dark age of piling verification debt. More things are being created, but we are lagging behind to check if any of it actually works to our satisfaction.&lt;/p&gt;&lt;p&gt;If an organization keeps vibe-coding without catching up with verification, those tasks can quickly end up as “debts” that needs to be verified. When verification becomes the bottleneck, dangerous things can happen if we still want to move fast - we will risk ourselves running unverified code and having unexpected side effects that are yet to be validated. It can also apply to other fields - imagine asking AI to craft a new vaccine and you don’t want to wait for FDA to use it.&lt;/p&gt;&lt;p&gt;I’ve come across a few blog posts that talks about Verification Debt already. I think it’s genuinely a good problem for technical leaders to have in their mind in this era.&lt;/p&gt;&lt;head rend="h3"&gt;Verification Engineering is the next Context Engineering&lt;/head&gt;&lt;p&gt;AI can only reliably run as fast as we check their work. It’s almost like a complexity theory claim. But I believe it needs to be the case to ensure we can harvest the exponential warp speed of AI but also remain robust and competent, as these technologies ultimate serve human beings, and us human beings need technology to be reliable and accountable, as we humans are already flaky enough ;)&lt;/p&gt;&lt;p&gt;This brings out the topic of Verification Engineering. I believe this can be a big thing after Context Engineering (which is the big thing after Prompt Engineering). By cleverly rearranging tasks and using nice abstractions and frameworks, we can make verification of AI performed tasks easier and use AI to ship more solid products the world. No more slop.&lt;/p&gt;&lt;p&gt;I can think of a few ideas to kickoff verification engineering:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;How to craft more technicall precise prompts to guide AI to surgically do things, rather than vibing it.&lt;/item&gt;&lt;item&gt;How to train more capable technical stakeholders who can effectively verify and approve what AI has done.&lt;/item&gt;&lt;item&gt;How to find more tasks that are relatively easy to verify but rather hard to create.&lt;/item&gt;&lt;item&gt;How to push our theoretical boundaries of what things we can succinctly verify (complexity theory strikes again).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Where next&lt;/head&gt;&lt;p&gt;I believe whoever figures out ways to effectively verify more complex tasks using human brains, can gain the most benefit out of the AI boom. Maybe we need to discard traditional programming languages and start programming in abstract graph-like dataflow representations where one can easily tell if a thing is done right or wrong despite its language or implementation details.&lt;/p&gt;&lt;p&gt;Maybe our future is like the one depicted in Severance - we look at computer screens with wiggly numbers and whatever “feels right” is the right thing to do. We can harvest these effortless low latency “feelings” that nature gives us to make AI do more powerful work.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://higashi.blog/2025/12/07/ai-verification/"/><published>2025-12-08T17:38:34+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46196105</id><title>Jepsen: NATS 2.12.1</title><updated>2025-12-09T06:17:48.430930+00:00</updated><content>&lt;doc fingerprint="2790905544351bb0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;1 Background&lt;/head&gt;
    &lt;p&gt;NATS is a popular streaming system. Producers publish messages to streams, and consumers subscribe to those streams, fetching messages from them. Regular NATS streams are allowed to drop messages. However, NATS has a subsystem called JetStream, which uses the Raft consensus algorithm to replicate data among nodes. JetStream promises âat least onceâ delivery: messages may be duplicated, but acknowledged messages1 should not be lost.2 Moreover, JetStream streams are totally ordered logs.&lt;/p&gt;
    &lt;p&gt;JetStream is intended to âself-heal and always be availableâ. The documentation also states that âthe formal consistency model of NATS JetStream is Linearizableâ. At most one of these claims can be true: the CAP theorem tells us that Linearizable systems can not be totally available.3 In practice, they tend to be available so long as a majority of nodes are non-faulty and communicating. If, say, a single node loses network connectivity, operations must fail on that node. If three out of five nodes crash, all operations must fail.&lt;/p&gt;
    &lt;p&gt;Indeed, a later section of the JetStream docs acknowledges this fact, saying that streams with three replicas can tolerate the loss of one server, and those with five can tolerate the simultaneous loss of two.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Replicas=5 - Can tolerate simultaneous loss of two servers servicing the stream. Mitigates risk at the expense of performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When does NATS guarantee a message will be durable? The JetStream developer docs say that once a JetStream clientâs &lt;code&gt;publish&lt;/code&gt; request is acknowledged by the server, that message has âbeen successfully persistedâ. The clustering configuration documentation says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In order to ensure data consistency across complete restarts, a quorum of servers is required. A quorum is Â½ cluster size + 1. This is the minimum number of nodes to ensure at least one node has the most recent data and state after a catastrophic failure. So for a cluster size of 3, youâll need at least two JetStream enabled NATS servers available to store new messages. For a cluster size of 5, youâll need at least 3 NATS servers, and so forth.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With these guarantees in mind, we set out to test NATS JetStream behavior under a variety of simulated faults.&lt;/p&gt;
    &lt;head rend="h1"&gt;2 Test Design&lt;/head&gt;
    &lt;p&gt;We designed a test suite for NATS JetStream using the Jepsen testing library, using JNATS (the official Java client) at version 2.24.0. Most of our tests ran in Debian 12 containers under LXC; some tests ran in Antithesis, using the official NATS Docker images. In all our tests we created a single JetStream stream with a target replication factor of five. Per NATSâ recommendations, our clusters generally contained three or five nodes. We tested a variety of versions, but the bulk of this work focused on NATS 2.12.1.&lt;/p&gt;
    &lt;p&gt;The test harness injected a variety of faults, including process pauses, crashes, network partitions, and packet loss, as well as single-bit errors and truncation of data files. We limited file corruption to a minority of nodes. We also simulated power failureâa crash with partial amnesiaâusing the LazyFS filesystem. LazyFS allows Jepsen to drop any writes which have not yet been flushed using a call to (e.g.) &lt;code&gt;fsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Our tests did not measure Linearizability or Serializability. Instead we ran several producer processes, each bound to a single NATS client, which published globally unique values to a single JetStream stream. Each message included the process number and a sequence number within that process, so message &lt;code&gt;4-0&lt;/code&gt; denoted the first &lt;code&gt;publish&lt;/code&gt; attempted by process &lt;code&gt;4&lt;/code&gt;, message &lt;code&gt;4-1&lt;/code&gt; denoted the second, and so on. At the end of the test we ensured all nodes were running, resolved any network partitions or other faults, subscribed to the stream, and attempted to read all acknowledged messages from the the stream. Each reader called &lt;code&gt;fetch&lt;/code&gt; until it had observed (at least) the last acknowledged message published by each process, or timed out.&lt;/p&gt;
    &lt;p&gt;We measured JetStreamâs at-least-once semantics based on the union of all published and read messages. We considered a message OK if it was attempted and read. Messages were lost if they were acknowledged as published, but never read by any process. We divided lost messages into three epochs, based on the first and last OK messages written by the same process.4 We called those lost before the first OK message the lost-prefix, those lost after all the last OK message the lost-postfix, and all others the lost-middle. This helped to distinguish between lagging readers and true data loss.&lt;/p&gt;
    &lt;p&gt;In addition to verifying each acknowledged message was delivered to at least one consumer across all nodes, we also checked the set of messages read by all consumers connected to a specific node. We called it divergence, or split-brain, when an acknowledged message was missing from some nodes but not others.&lt;/p&gt;
    &lt;head rend="h1"&gt;3 Results&lt;/head&gt;
    &lt;p&gt;We begin with a belated note on total data loss in version 2.10.22, then continue with four findings related to data loss and replica divergence in version 2.12.1: two with file corruption, and two with power failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.1 Total Data Loss on Crash in 2.10.22 (#6888)&lt;/head&gt;
    &lt;p&gt;Before discussing version 2.12.1, we present a long-overdue finding from earlier work. In versions 2.10.20 through 2.10.22 (released 2024-10-17), we found that process crashes alone could cause the total loss of a JetStream stream and all its associated data. Subscription requests would return &lt;code&gt;"No matching streams for subject"&lt;/code&gt;, and &lt;code&gt;getStreamNames()&lt;/code&gt; would return an empty list. These conditions would persist for hours: in this test run, we waited 10,000 seconds for the cluster to recover, but the stream never returned.&lt;/p&gt;
    &lt;p&gt;Jepsen reported this issue to NATS as #6888, but it appears that NATS had already identified several potential causes for this problem and resolved them. In #5946, a cluster-wide crash occurring shortly after a stream was created could cause the loss of the stream. A new leader would be elected with a snapshot which preceded the creation of the stream, and replicate that empty snapshot to followers, causing everyone to delete their copy of the stream. In #5700, tests running in Antithesis found that out-of-order delivery of snapshot messages could cause streams to be deleted and re-created as well. In #6061, process crashes could cause nodes to delete their local Raft state. All of these fixes were released as a part of 2.10.23, and we no longer observed the problem in that version.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.2 Lost Writes With &lt;code&gt;.blk&lt;/code&gt; File Corruption (#7549)&lt;/head&gt;
    &lt;p&gt;NATS has several checksum mechanisms meant to detect data corruption in on-disk files. However, we found that single-bit errors or truncation of JetStreamâs &lt;code&gt;.blk&lt;/code&gt; files could cause the cluster to lose large windows of writes. This occurred even when file corruption was limited to just one or two nodes out of five. For instance, file corruption in this test run caused NATS to lose 679,153 acknowledged writes out of 1,367,069 total, including 201,286 which were missing even though later values written by the same process were later read.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In some cases, file corruption caused the quiet loss of just a single message. In others, writes vanished in large blocks. Even worse, bitflips could cause split-brain, where different nodes returned different sets of messages. In this test, NATS acknowledged a total of 1,479,661 messages. However, single-bit errors in &lt;code&gt;.blk&lt;/code&gt; files on nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n3&lt;/code&gt; caused nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n3&lt;/code&gt;, and &lt;code&gt;n5&lt;/code&gt; to lose up to 78% of those acknowledged messages. Node &lt;code&gt;n1&lt;/code&gt; lost 852,413 messages, and nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; lost 1,167,167 messages, despite &lt;code&gt;n5&lt;/code&gt;âs data files remaining intact. Messages were lost in prefix, middle, and postfix: the stream, at least on those three nodes, resembled Swiss cheese.&lt;/p&gt;
    &lt;p&gt;NATS is investigating this issue (#7549).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.3 Total Data Loss With Snapshot File Corruption (#7556)&lt;/head&gt;
    &lt;p&gt;When we truncated or introduced single-bit errors into JetStreamâs snapshot files in &lt;code&gt;data/jetstream/$SYS/_js_/&lt;/code&gt;, we found that nodes would sometimes decide that a stream had been orphaned, and delete all its data files. This happened even when only a minority of nodes in the cluster experienced file corruption. The cluster would never recover quorum, and the stream remained unavailable for the remainder of the test.&lt;/p&gt;
    &lt;p&gt;In this test run, we introduced single-bit errors into snapshots on nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt;. During the final recovery period, node &lt;code&gt;n3&lt;/code&gt; became the metadata leader for the cluster and decided to clean up &lt;code&gt;jepsen-stream&lt;/code&gt;, which stored all the testâs messages.&lt;/p&gt;
    &lt;code&gt;[1010859] 2025/11/15 20:27:02.947432 [INF]
Self is new JetStream cluster metadata leader
[1010859] 2025/11/15 20:27:14.996174 [WRN]
Detected orphaned stream 'jepsen &amp;gt;
jepsen-stream', will cleanup&lt;/code&gt;
    &lt;p&gt;Nodes &lt;code&gt;n3&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; then deleted all files in the stream directory. This might seem defensibleâafter all, some of &lt;code&gt;n3&lt;/code&gt;âs data files were corrupted. However, &lt;code&gt;n3&lt;/code&gt; managed to become the leader of the cluster despite its corrupt state! In general, leader-based consensus systems must be careful to ensure that any node which becomes a leader is aware of majority committed state. Becoming a leader, then opting to delete a stream full of committed data, is particularly troubling.&lt;/p&gt;
    &lt;p&gt;Although nodes &lt;code&gt;n1&lt;/code&gt;, &lt;code&gt;n2&lt;/code&gt;, and &lt;code&gt;n4&lt;/code&gt; retained their data files, &lt;code&gt;n1&lt;/code&gt; struggled to apply snapshots; &lt;code&gt;n4&lt;/code&gt; declared that &lt;code&gt;jepsen-stream&lt;/code&gt; had no quorum and stalled. Every attempt to subscribe to the stream threw &lt;code&gt;[SUB-90007] No matching streams for subject&lt;/code&gt;. Jepsen filed issue #7556 for this, and the NATS team is looking into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;3.4 Lazy &lt;code&gt;fsync&lt;/code&gt; by Default (#7564)&lt;/head&gt;
    &lt;p&gt;NATS JetStream promises that once a &lt;code&gt;publish&lt;/code&gt; call has been acknowledged, it is âsuccessfully persistedâ. This is not exactly true. By default, NATS calls &lt;code&gt;fsync&lt;/code&gt; to flush data to disk only once every two minutes, but acknowledges messages immediately. Consequently, recently acknowledged writes are generally not persisted, and could be lost to coordinated power failure, kernel crashes, etc. For instance, simulated power failures in this test run caused NATS to lose roughly thirty seconds of writes: 131,418 out of 930,005 messages.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Because the default flush interval is quite large, even killing a single node at a time is sufficient to cause data loss, so long as nodes fail within a few seconds of each other. In this run, a series of single-node failures in the first two minutes of the test caused NATS to delete the entire stream, along with all of its messages.&lt;/p&gt;
    &lt;p&gt;There are only two mentions of this behavior in the NATS documentation. The first is in the 2.10 release notes. The second, buried in the configuration docs, describes the &lt;code&gt;sync_interval&lt;/code&gt; option:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Change the default fsync/sync interval for page cache in the filestore. By default JetStream relies on stream replication in the cluster to guarantee data is available after an OS crash. If you run JetStream without replication or with a replication of just 2 you may want to shorten the fsync/sync interval. You can force an fsync after each messsage [sic] with&lt;/p&gt;&lt;code&gt;always&lt;/code&gt;, this will slow down the throughput to a few hundred msg/s.&lt;/quote&gt;
    &lt;p&gt;Consensus protocols often require that nodes sync to disk before acknowledging an operation. For example, the famous 2007 paper Paxos Made Live remarks:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that all writes have to be flushed to disk immediately before the system can proceed any further.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Raft thesis on which NATS is based is clear that nodes must âflush [new log entries] to their disksâ before acknowledging. Section 11.7.3 discusses the possibility of instead writing data to disk asynchronously, and concludes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The trade-off is that data loss is possible in catastrophic events. For example, if a majority of the cluster were to restart simultaneously, the cluster would have potentially lost entries and would not be able to form a new view. Raft could be extended in similar ways to support disk-less operation, but we think the risk of availability or data loss usually outweighs the benefits.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;For similar reasons, replicated systems like MongoDB, etcd, TigerBeetle, Zookeeper, Redpanda, and TiDB sync data to disk before acknowledging an operation as committed.&lt;/p&gt;
    &lt;p&gt;However, some systems do choose to &lt;code&gt;fsync&lt;/code&gt; asynchronously. YugabyteDBâs default is to acknowledge un-fsynced writes. Liskov and Cowlingâs Viewstamped Replication Revisited assumes replicas are âhighly unlikely to fail at the same timeââbut acknowledges that if they were to fail simultaneously, state would be lost. Apache Kafka makes a similar choice, but claims that it is not vulnerable to coordinated failure because Kafka âdoesnât store unflushed data in its own memory, but in the page cacheâ. This offers resilience to the Kafka process itself crashing, but not power failure.5 Jepsen remains skeptical of this approach: as Alagappan et al. argue, extensive literature on correlated failures suggests we should continue to take this risk seriously. Heat waves, grid instability, fires, lightning, tornadoes, and floods are not necessarily constrained to a single availability zone.&lt;/p&gt;
    &lt;p&gt;Jepsen suggests that NATS change the default value for &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, rather than every two minutes. Alternatively, NATS documentation should prominently disclose that JetStream may lose data when nodes experience correlated power failure, or fail in rapid succession (#7564).&lt;/p&gt;
    &lt;head rend="h2"&gt;3.5 A Single OS Crash Can Cause Split-Brain (#7567)&lt;/head&gt;
    &lt;p&gt;In response to #7564, NATS engineers noted that most production deployments run with each node in a separate availability zone, which reduces the probability of correlated failure. This raises the question: how many power failures (or hardware faults, kernel crashes, etc.) are required to cause data loss? Perhaps surprisingly, in an asynchronous network the answer is âjust oneâ.&lt;/p&gt;
    &lt;p&gt;To understand why, consider that a system which remains partly available when a minority of nodes are unavailable must allow states in which a committed operation is presentâsolely in memoryâon a bare majority of nodes. For example, in a leader-follower protocol the leader of a three-node cluster may consider a write committed as soon as a single follower has responded: it has two acknowledgements, counting itself. Under normal operation there will usually be some window of committed operations in this state.6.&lt;/p&gt;
    &lt;p&gt;Now imagine that one of those two nodes loses power and restarts. Because the write was stored only in memory, rather than on disk, the acknowledged write is no longer present on that node. There now exist two out of three nodes which do not have the write. Since the system is fault-tolerant, these two nodes must be able to form a quorum and continue processing requestsâcreating new states of the system in which the acknowledged write never happened.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, this fault requires nothing more than a single power failure (or HW fault, kernel crash, etc.) and an asynchronous networkâone which is allowed to deliver messages arbitrarily late. Whether it occurs in practice depends on the specific messages exchanged by the replication system, which node fails, how long it remains offline, the order of message delivery, and so on. However, one can reliably induce data loss by killing, pausing, or partitioning away a minority of nodes before and after a simulated OS crash.&lt;/p&gt;
    &lt;p&gt;For example, process pauses and a single simulated power failure in this test run caused JetStream to lose acknowledged writes for windows roughly on par with &lt;code&gt;sync_interval&lt;/code&gt;. Stranger still, the cluster entered a persistent split-brain which continued after all nodes were restarted and the network healed. Consider these two plots of lost writes, based on final reads performed against nodes &lt;code&gt;n1&lt;/code&gt; and &lt;code&gt;n5&lt;/code&gt; respectively:&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p&gt;Consumers talking to &lt;code&gt;n1&lt;/code&gt; failed to observe a short window of acknowledged messages written around 42 seconds into the test. Meanwhile, consumers talking to &lt;code&gt;n5&lt;/code&gt; would miss acknowledged messages written around 58 seconds. Both windows of write loss were on the order of our choice of &lt;code&gt;sync_interval = 10s&lt;/code&gt; for this run. In repeated testing, we found that any node in the cluster could lose committed writes, including the node which failed, those which received writes before the failure, and those which received writes afterwards.&lt;/p&gt;
    &lt;p&gt;The fact that a single power failure can cause data loss is not new. In 2023, RedPanda wrote a detailed blog post showing that Kafkaâs default lazy &lt;code&gt;fsync&lt;/code&gt; could lead to data loss under exactly this scenario. However, it is especially concerning that this scenario led to persistent replica divergence, not just data loss! We filed #7567 for this issue, and the NATS team is investigating.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;â&lt;/cell&gt;
        &lt;cell role="head"&gt;Summary&lt;/cell&gt;
        &lt;cell role="head"&gt;Event Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Fixed in&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#6888&lt;/cell&gt;
        &lt;cell&gt;Stream deleted on crash in 2.10.22&lt;/cell&gt;
        &lt;cell&gt;Crashes&lt;/cell&gt;
        &lt;cell&gt;2.10.23&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7549&lt;/cell&gt;
        &lt;cell&gt;Lost writes due to &lt;code&gt;.blk&lt;/code&gt; file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7556&lt;/cell&gt;
        &lt;cell&gt;Stream deleted due to snapshot file corruption&lt;/cell&gt;
        &lt;cell&gt;Minority truncation or bitflip&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;#7564&lt;/cell&gt;
        &lt;cell&gt;Write loss due to lazy &lt;code&gt;fsync&lt;/code&gt; policy&lt;/cell&gt;
        &lt;cell&gt;Coordinated OS crash&lt;/cell&gt;
        &lt;cell&gt;Documented&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;#7567&lt;/cell&gt;
        &lt;cell&gt;Write loss and split-brain&lt;/cell&gt;
        &lt;cell&gt;Single OS crash and pause&lt;/cell&gt;
        &lt;cell&gt;Unresolved&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;4 Discussion&lt;/head&gt;
    &lt;p&gt;In NATS 2.10.22, process crashes could cause JetStream to forget a stream ever existed (#6888). This issue was identified independently by NATS and resolved in version 2.10.23, released on 2024-12-10. We did not observe data loss with simple network partitions, process pauses, or crashes in version 2.12.1.&lt;/p&gt;
    &lt;p&gt;However, we found that in NATS 2.12.1, file corruption and simulated OS crashes could both lead to data loss and persistent split-brain. Bitflips or truncation of either &lt;code&gt;.blk&lt;/code&gt; (#7549) or snapshot (#7556) files, even on a minority of nodes, could cause the loss of single messages, large windows of messages, or even cause some nodes to delete their stream data altogether. Messages could be missing on some nodes and present on others. NATS has multiple checksum mechanisms designed to limit the impact of file corruption; more thorough testing of these mechanisms seems warranted.&lt;/p&gt;
    &lt;p&gt;By default, NATS only flushes data to disk every two minutes, but acknowledges operations immediately. This approach can lead to the loss of committed writes when several nodes experience a power failure, kernel crash, or hardware fault concurrentlyâor in rapid succession (#7564). In addition, a single OS crash combined with process crashes, pauses, or network partitions can cause the loss of acknowledged messages and persistent split-brain (#7567). We recommended NATS change the default value of &lt;code&gt;fsync&lt;/code&gt; to &lt;code&gt;always&lt;/code&gt;, or clearly document these hazards. NATS has added new documentation to the JetStream Concepts page.&lt;/p&gt;
    &lt;p&gt;This documentation also describes several goals for JetStream, including that â[t]he system must self-heal and always be available.â This is impossible: the CAP theorem states that Linearizable systems cannot be totally available in an asynchronous network. In our three and five-node clusters JetStream generally behaved like a typical Raft implementation. Operations proceeded on a majority of connected nodes but isolated nodes were unavailable, and if a majority failed, the system as a whole became unavailable. Jepsen suggests clarifying this part of the documentation.&lt;/p&gt;
    &lt;p&gt;As always, Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.1 LazyFS&lt;/head&gt;
    &lt;p&gt;This work demonstrates that systems which do not exhibit data loss under normal process crashes (e.g.Â &lt;code&gt;kill -9 &amp;lt;PID&amp;gt;&lt;/code&gt;) may lose data or enter split-brain under simulated OS-level crashes. Our tests relied heavily on LazyFS, a project of INESC TEC at the University of Porto.7 After killing a process, we used LazyFS to simulate the effects of a power failure by dropping writes to the filesystem which had not yet been &lt;code&gt;fsync&lt;/code&gt;ed to disk.&lt;/p&gt;
    &lt;p&gt;While this work focused purely on the loss of unflushed writes, LazyFS can also simulate linear and non-linear torn writes: an anomaly where a storage device persists part, but not all, of written data thanks to (e.g.) IO cache reordering. Our 2024 paper When Amnesia Strikes discusses these faults in more detail, highlighting bugs in PostgreSQL, Redis, ZooKeeper, etcd, LevelDB, PebblesDB, and the Lightning Network.&lt;/p&gt;
    &lt;head rend="h2"&gt;4.2 Future Work&lt;/head&gt;
    &lt;p&gt;We designed only a simple workload for NATS which checked for lost records either across all consumers, or across all consumers bound to a single node. We did not check whether single consumers could miss messages, or the order in which they were delivered. We did not check NATSâ claims of Linearizable writes or Serializable operations in general. We also did not evaluate JetStreamâs âexactly-once semanticsâ. All of these could prove fruitful avenues for further tests.&lt;/p&gt;
    &lt;p&gt;In some tests, we added and removed nodes from the cluster. This work generated some preliminary results. However, the NATS documentation for membership changes was incorrect and incomplete: it gave the wrong command for removing peers, and there appears to be an undocumented but mandatory health check step for newly-added nodes. As of this writing, Jepsen is unsure how to safely add or remove nodes to a NATS cluster. Consequently, we leave membership changes for future research.&lt;/p&gt;
    &lt;p&gt;Our thanks to INESC TEC and everyone on the LazyFS team, including Maria Ramos, JoÃ£o Azevedo, JosÃ© Pereira, TÃ¢nia Esteves, Ricardo Macedo, and JoÃ£o Paulo. Jepsen is also grateful to Silvia Botros, Kellan Elliott-McCrea, Carla Geisser, Coda Hale, and Marc Hedlund for their expertise regarding datacenter power failures, correlated kernel panics, disk faults, and other causes of OS-level crashes. Finally, our thanks to Irene Kannyo for her editorial support. This research was performed independently by Jepsen, without compensation, and conducted in accordance with the Jepsen ethics policy.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Throughout this report we use âacknowledged messageâ to describe a message whose&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;request was acknowledged successfully by some server. NATS also offers a separate notion of acknowledgement, which indicates when a message has been processed and need not be delivered again.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JetStream also promises âexactly once semanticsâ in some scenarios. We leave this for later research.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The CAP theoremâs definition of âavailabilityâ requires that all operations on non-faulty nodes must succeed.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is overly conservative: in a system with Linearizable writes, we should never observe a lost message which was acknowledged prior to the invocation of the&lt;/p&gt;&lt;code&gt;publish&lt;/code&gt;call for an OK message, regardless of process. However, early testing with NATS suggested that it might be better to test a weaker property, and come to stronger conclusions about data loss.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Redpanda argues that the situation is actually worse: a single power failure, combined with network partitions or process pauses, can cause Kafka to lose committed data.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some protocols, like Raft, consider an operation committed as soon as it is acknowledged by a majority of nodes. These systems offer lower latencies, but at any given time there are likely a few committed operations which are missing from a minority of nodes due to normal network latency. Other systems, like Kafka, require acknowledgement from all âonlineâ nodes before considering an operation committed. These systems offer worse latency in healthy clusters (since they must wait for the slowest node) but in exchange, committed operations can only be missing from some node when the fault detector decides that node is no longer online (e.g.Â due to elevated latency).â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jepsen contributed some funds, testing, and integration assistance to LazyFS, but most credit belongs to the LazyFS team.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://jepsen.io/analyses/nats-2.12.1"/><published>2025-12-08T18:51:03+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46196228</id><title>Has the cost of building software dropped 90%?</title><updated>2025-12-09T06:17:48.211001+00:00</updated><content>&lt;doc fingerprint="a35880536496fbfd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Has the cost of building software just dropped 90%?&lt;/head&gt;
    &lt;p&gt;I've been building software professionally for nearly 20 years. I've been through a lot of changes - the 'birth' of SaaS, the mass shift towards mobile apps, the outrageous hype around blockchain, and the perennial promise that low-code would make developers obsolete.&lt;/p&gt;
    &lt;p&gt;The economics have changed dramatically now with agentic coding, and it is going to totally transform the software development industry (and the wider economy). 2026 is going to catch a lot of people off guard.&lt;/p&gt;
    &lt;p&gt;In my previous post I delved into why I think evals are missing some of the big leaps, but thinking this over since then (and recent experience) has made me confident we're in the early stages of a once-in-a-generation shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;The cost of shipping&lt;/head&gt;
    &lt;p&gt;I started developing just around the time open source started to really explode - but it was clear this was one of the first big shifts in cost of building custom software. I can remember eye watering costs for SQL Server or Oracle - and as such started out really with MySQL, which did allow you to build custom networked applications without incurring five or six figures of annual database licensing costs.&lt;/p&gt;
    &lt;p&gt;Since then we've had cloud (which I would debate is a cost saving at all, but let's be generous and assume it has some initial capex savings) and lately what I feel has been the era of complexity. Software engineering has got - in my opinion, often needlessly - complicated, with people rushing to very labour intensive patterns such as TDD, microservices, super complex React frontends and Kubernetes. I definitely don't think we've seen much of a cost decrease in the past few years.&lt;/p&gt;
    &lt;p&gt;AI Agents however in my mind massively reduce the labour cost of developing software.&lt;/p&gt;
    &lt;head rend="h2"&gt;So where do the 90% savings actually come from?&lt;/head&gt;
    &lt;p&gt;At the start of 2025 I was incredibly sceptical of a lot of the AI coding tools - and a lot of them I still am. Many of the platforms felt like glorified low code tooling (Loveable, Bolt, etc), or VS Code forks with some semi-useful (but often annoying) autocomplete improvements.&lt;/p&gt;
    &lt;p&gt;Take an average project for an internal tool in a company. Let's assume the data modelling is already done to some degree, and you need to implement a web app to manage widgets.&lt;/p&gt;
    &lt;p&gt;Previously, you'd have a small team of people working on setting up CI/CD, building out data access patterns and building out the core services. Then usually a whole load of CRUD-style pages and maybe some dashboards and graphs for the user to make. Finally you'd (hopefully) add some automated unit/integration/e2e tests to make sure it was fairly solid and ship it, maybe a month later.&lt;/p&gt;
    &lt;p&gt;And that's just the direct labour. Every person on the project adds coordination overhead. Standups, ticket management, code reviews, handoffs between frontend and backend, waiting for someone to unblock you. The actual coding is often a fraction of where the time goes.&lt;/p&gt;
    &lt;p&gt;Nearly all of this can be done in a few hours with an agentic coding CLI. I've had Claude Code write an entire unit/integration test suite in a few hours (300+ tests) for a fairly complex internal tool. This would take me, or many developers I know and respect, days to write by hand.&lt;/p&gt;
    &lt;p&gt;The agentic coding tools have got extremely good at converting business logic specifications into pretty well written APIs and services.&lt;/p&gt;
    &lt;p&gt;A project that would have taken a month now takes a week. The thinking time is roughly the same - the implementation time collapsed. And with smaller teams, you get the inverse of Brooks's Law: instead of communication overhead scaling with headcount, it disappears. A handful of people can suddenly achieve an order of magnitude more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latent demand&lt;/head&gt;
    &lt;p&gt;On the face of it, this seems like incredibly bad news for the software development industry - but economics tells us otherwise.&lt;/p&gt;
    &lt;p&gt;Jevons Paradox says that when something becomes cheaper to produce, we don't just do the same amount for less money. Take electric lighting for example; while sales of candles and gas lamps fell, overall far more artificial light was generated.&lt;/p&gt;
    &lt;p&gt;If we apply this to software engineering, think of supply and demand. There is so much latent demand for software. I'm sure every organisation has hundreds if not thousands of Excel sheets tracking important business processes that would be far better off as a SaaS app. Let's say they get a quote from an agency to build one into an app for $50k - only essential ones meet the grade. At $5k (for a decent developer + AI tooling) - suddenly there is far more demand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Domain knowledge is the only moat&lt;/head&gt;
    &lt;p&gt;So where does that leave us? Right now there is still enormous value in having a human 'babysit' the agent - checking its work, suggesting the approach and shortcutting bad approaches. Pure YOLO vibe coding ends up in a total mess very quickly, but with a human in the loop I think you can build incredibly good quality software, very quickly.&lt;/p&gt;
    &lt;p&gt;This then allows developers who really master this technology to be hugely effective at solving business problems. Their domain and industry knowledge becomes a huge lever - knowing the best architectural decisions for a project, knowing which framework to use and which libraries work best.&lt;/p&gt;
    &lt;p&gt;Layer on understanding of the business domain and it does genuinely feel like the mythical 10x engineer is here. Equally, the pairing of a business domain expert with a motivated developer and these tools becomes an incredibly powerful combination, and something I think we'll see becoming quite common - instead of a 'squad' of a business specialist and a set of developers, we'll see a far tighter pairing of a couple of people.&lt;/p&gt;
    &lt;p&gt;This combination allows you to iterate incredibly quickly, and software becomes almost disposable - if the direction is bad, then throw it away and start again, using those learnings. This takes a fairly large mindset shift, but the hard work is the conceptual thinking, not the typing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don't get caught off guard&lt;/head&gt;
    &lt;p&gt;The agents and models are still improving rapidly, which I don't think is really being captured in the benchmarks. Opus 4.5 seems to be able to follow long 10-20 minute sessions without going completely off piste. We're just starting to see the results of the hundreds of billions of dollars of capex that has gone into GB200 GPUs now, and I'm sure newer models will quickly make these look completely obsolete.&lt;/p&gt;
    &lt;p&gt;However, I've spoken to so many software engineers that are really fighting this change. I've heard the same objections too many times - LLMs make too many mistakes, it can't understand &lt;code&gt;[framework]&lt;/code&gt;, or it doesn't really save any time.&lt;/p&gt;
    &lt;p&gt;These assertions are rapidly becoming completely false, and remind me a lot of the desktop engineers who dismissed the iPhone in 2007. I think we all know how that turned out - networking got better, the phones got way faster and the mobile operating systems became very capable.&lt;/p&gt;
    &lt;p&gt;Engineers need to really lean in to the change in my opinion. This won't change overnight - large corporates are still very much behind the curve in general, lost in a web of bureaucracy of vendor approvals and management structures that leave them incredibly vulnerable to smaller competitors.&lt;/p&gt;
    &lt;p&gt;But if you're working for a smaller company or team and have the power to use these tools, you should. Your job is going to change - but software has always changed. Just perhaps this time it's going to change faster than anyone anticipates. 2026 is coming.&lt;/p&gt;
    &lt;p&gt;One objection I hear a lot is that LLMs are only good at greenfield projects. I'd push back hard on this. I've spent plenty of time trying to understand 3-year-old+ codebases where everyone who wrote it has left. Agents make this dramatically easier - explaining what the code does, finding the bug(s), suggesting the fix. I'd rather inherit a repo written with an agent and a good engineer in the loop than one written by a questionable quality contractor who left three years ago, with no tests, and a spaghetti mess of classes and methods.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://martinalderson.com/posts/has-the-cost-of-software-just-dropped-90-percent/"/><published>2025-12-08T19:00:48+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46196308</id><title>Trials avoid high risk patients and underestimate drug harms</title><updated>2025-12-09T06:17:47.765843+00:00</updated><content>&lt;doc fingerprint="f9f935b219307f6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Trials Avoid High Risk Patients and Underestimate Drug Harms&lt;/head&gt;
    &lt;p&gt;The FDA does not formally regulate representativeness, but if trials under-enroll vulnerable patients, the resulting evidence may understate harm from drugs. We study the relationship between trial participation and the risk of drug-induced adverse events for cancer medications using data from the Surveillance, Epidemiology, and End Results Program linked to Medicare claims. Initiating treatment with a cancer drug increases the risk of hospitalization due to serious adverse events (SAE) by 2 percentage points per month (a 250% increase). Heterogeneity in SAE treatment effects can be predicted by patient's comorbidities, frailty, and demographic characteristics. Patients at the 90th percentile of the risk distribution experience a 2.5 times greater increase in SAEs after treatment initiation compared to patients at the 10th percentile of the risk distribution yet are 4 times less likely to enroll in trials. The predicted SAE treatment effects for the drug's target population are 15% larger than the predicted SAE treatment effects for trial enrollees, corresponding to 1 additional induced SAE hospitalization for every 25 patients per year of treatment. We formalize conditions under which regulating representativeness of SAE risk will lead to more externally valid trials, and we discuss how our results could inform regulatory requirements.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Copy CitationJason Abaluck, Leila Agha, and Sachin Shah, "Trials Avoid High Risk Patients and Underestimate Drug Harms," NBER Working Paper 34534 (2025), https://doi.org/10.3386/w34534.Download Citation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.nber.org/papers/w34534"/><published>2025-12-08T19:07:59+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46196688</id><title>Icons in Menus Everywhere – Send Help</title><updated>2025-12-09T06:17:47.573631+00:00</updated><content>&lt;doc fingerprint="422090131970f19e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Icons in Menus Everywhere — Send Help&lt;/head&gt;
    &lt;p&gt;I complained about this on the socials, but I didn’t get it all out of my system. So now I write a blog post.&lt;/p&gt;
    &lt;p&gt;I’ve never liked the philosophy of “put an icon in every menu item by default”.&lt;/p&gt;
    &lt;p&gt;Google Sheets, for example, does this. Go to “File” or “Edit” or “View” and you’ll see a menu with a list of options, every single one having an icon (same thing with the right-click context menu).&lt;/p&gt;
    &lt;p&gt;It’s extra noise to me. It’s not that I think menu items should never have icons. I think they can be incredibly useful (more on that below). It’s more that I don’t like the idea of “give each menu item an icon” being the default approach.&lt;/p&gt;
    &lt;p&gt;This posture lends itself to a practice where designers have an attitude of “I need an icon to fill up this space” instead of an attitude of “Does the addition of a icon here, and the cognitive load of parsing and understanding it, help or hurt how someone would use this menu system?”&lt;/p&gt;
    &lt;p&gt;The former doesn’t require thinking. It’s just templating — they all have icons, so we need to put something there. The latter requires care and thoughtfulness for each use case and its context.&lt;/p&gt;
    &lt;p&gt;To defend my point, one of the examples I always pointed to was macOS. For the longest time, Apple’s OS-level menus seemed to avoid this default approach of sticking icons in every menu item.&lt;/p&gt;
    &lt;p&gt;That is, until macOS Tahoe shipped.&lt;/p&gt;
    &lt;head rend="h2"&gt;Menus in macOS Tahoe&lt;/head&gt;
    &lt;p&gt;Tahoe now has icons in menus everywhere. For example, here’s the Apple menu:&lt;/p&gt;
    &lt;p&gt;Let’s look at others. As I’m writing this I have Safari open. Let’s look at the “Safari” menu:&lt;/p&gt;
    &lt;p&gt;Hmm. Interesting. Ok so we’ve got an icon for like half the menu items. I wonder why some get icons and others don’t?&lt;/p&gt;
    &lt;p&gt;For example, the “Settings” menu item (third from the top) has an icon. But the other item in its grouping “Privacy Report” does not. I wonder why? Especially when Safari has an icon for Privacy report, like if you go to customize the toolbar you’ll see it:&lt;/p&gt;
    &lt;p&gt;Hmm. Who knows? Let’s keep going.&lt;/p&gt;
    &lt;p&gt;Let’s look at the "File" menu in Safari:&lt;/p&gt;
    &lt;p&gt;Some groupings have icons and get inset, while other groupings don’t have icons and don’t get inset. Interesting…again I wonder what the rationale is here? How do you choose? It’s not clear to me.&lt;/p&gt;
    &lt;p&gt;Let’s keep going. Let’s go to the "View" menu:&lt;/p&gt;
    &lt;p&gt;Oh boy, now we’re really in it. Some of these menu items have the notion of a toggle (indicated by the checkmark) so now you’ve got all kinds of alignment things to deal with. The visual symbols are doubling-up when there’s a toggle and an icon.&lt;/p&gt;
    &lt;p&gt;The “View” menu in Mail is a similar mix of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Text&lt;/item&gt;
      &lt;item&gt;Text + toggles&lt;/item&gt;
      &lt;item&gt;Text + icons&lt;/item&gt;
      &lt;item&gt;Text + icons + toggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You know what would be a fun game? Get a bunch of people in a room, show them menus where the textual labels are gone, and see who can get the most right.&lt;/p&gt;
    &lt;p&gt;But I digress.&lt;/p&gt;
    &lt;p&gt;In so many of these cases, I honestly can’t intuit why some menus have icons and others do not. What are so many of these icons affording me at the cost of extra visual and cognitive parsing? I don’t know.&lt;/p&gt;
    &lt;p&gt;To be fair, there are some menus where these visual symbols are incredibly useful. Take this menu from Finder:&lt;/p&gt;
    &lt;p&gt;The visual depiction of how those are going to align is actually incredibly useful because it’s way easier for my brain to parse the symbol and understand where the window is going to go than it is to read the text and imagine in my head what “Top Left” or “Bottom &amp;amp; Top” or “Quarters” will mean. But a visual symbol? I instantly get it!&lt;/p&gt;
    &lt;p&gt;Those are good icons in menus. I like those.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple Abandons Its Own Guidance&lt;/head&gt;
    &lt;p&gt;What I find really interesting about this change on Apple’s part is how it seemingly goes against their own previous human interface guidelines (as pointed out to me by Peter Gassner).&lt;/p&gt;
    &lt;p&gt;They have an entire section in their 2005 guidelines (and 1992 and 2020) titled “Using Symbols in Menus”:&lt;/p&gt;
    &lt;p&gt;See what it says?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are a few standard symbols you can use to indicate additional information in menus…Don’t use other, arbitrary symbols in menus, because they add visual clutter and may confuse people.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Confused people. That’s me.&lt;/p&gt;
    &lt;p&gt;They even have an example of what not to do and guess what it looks like? A menu in macOS Tahoe.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It’s pretty obvious how I feel. I’m tired of all this visual noise in my menus.&lt;/p&gt;
    &lt;p&gt;And now that Apple has seemingly thrown in with the “stick an icon in every menu by default” crowd, it’s harder than ever for me to convince people otherwise. To persuade, “Hey, unless you can articulate a really good reason to add this, maybe our default posture should be no icons in menus?”&lt;/p&gt;
    &lt;p&gt;So I guess this is the world I live in now. Icons in menus. Icons in menus everywhere.&lt;/p&gt;
    &lt;p&gt;Send help.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.jim-nielsen.com/2025/icons-in-menus/"/><published>2025-12-08T19:44:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46198430</id><title>Show HN: I built a system for active note-taking in regular meetings like 1-1s</title><updated>2025-12-09T06:17:47.156522+00:00</updated><link href="https://withdocket.com"/><published>2025-12-08T22:21:26+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199411</id><title>Kroger acknowledges that its bet on robotics went too far</title><updated>2025-12-09T06:17:46.874319+00:00</updated><content>&lt;doc fingerprint="24a7c19bfeaa83a7"&gt;
  &lt;main&gt;
    &lt;p&gt;Kroger’s announcement on Tuesday that it will shutter three of its robotic e-commerce fulfillment facilities represents a sharp turnabout for the grocery company, which until recently had expressed confidence in its ability to leverage automation to run a profitable online grocery business.&lt;/p&gt;
    &lt;p&gt;Less than a year ago, Kroger said it planned to expand the fleet of high-tech fulfillment centers it has been developing in partnership with U.K.-based warehouse automation company Ocado. And in mid-2024, Kroger revealed that it would install new technology from Ocado to improve the efficiency of the warehouses.&lt;/p&gt;
    &lt;p&gt;When Kroger launched its partnership with Ocado, the company “believed in the relentless drive to innovate way ahead of the market in order to delight our customers and advance our position as one of America’s leading e-commerce companies,” former Kroger CEO Rodney McMullen said in a video about improvements to its equipment that the automation company announced last year.&lt;/p&gt;
    &lt;p&gt;However, Kroger’s projected confidence came even as it was questioning whether the Ocado network was living up to expectations.&lt;/p&gt;
    &lt;p&gt;Kroger revealed in September 2023 that it had decided to pause development of the Ocado project as it waited to see if sites it had already started operating would meet performance benchmarks.&lt;/p&gt;
    &lt;p&gt;In a further sign that its strategy was faltering, Kroger announced last March it would close three spoke facilities that worked in tandem with several of its robotic centers, with a spokesperson noting that the facilities “did not meet the benchmarks we set for success.”&lt;/p&gt;
    &lt;p&gt;By September 2025, it was clear that depending on automation as the foundation of a money-making grocery delivery business was probably not going to pan out for Kroger. Speaking during an earnings call, interim Kroger CEO Ron Sargent — who took over in March after McMullen’s sudden departure following an ethics probe — said the company would conduct a “full site-by-site analysis” of the Ocado network.&lt;/p&gt;
    &lt;p&gt;Sargent also said Kroger would refocus its e-commerce efforts on its fleet of more than 2,700 grocery supermarkets because it believed that its stores gave it a way to “reach new customer segments and expand rapid delivery capabilities without significant capital investments.”&lt;/p&gt;
    &lt;p&gt;Kroger said on Tuesday that its decision to close the three robotic facilities, along with other adjustments to its e-commerce operations, would provide a $400 million boost as it looks to improve e-commerce profitability. But the course-correction will be expensive, forcing Kroger to incur charges of about $2.6 billion.&lt;/p&gt;
    &lt;p&gt;Ken Fenyo, a former Kroger executive who now advises retailers on technology as managing partner of Pine Street Advisors, said the changes Kroger is making reflect the broader reality that grocery e-commerce has not reached the levels the industry had predicted when the COVID-19 pandemic supercharged digital sales five years ago.&lt;/p&gt;
    &lt;p&gt;Fenyo added that Kroger’s decision to locate the Ocado centers outside of cities turned out to be a key flaw.&lt;/p&gt;
    &lt;p&gt;“Ultimately those were hard places to make this model work,” said Fenyo. “You didn’t have enough people ordering, and you had a fair amount of distance to drive to get the orders to them. And so ultimately, these large centers were just not processing enough orders to pay for all that technology investment you had to make.”&lt;/p&gt;
    &lt;p&gt;With its automated fulfillment network, Kroger bet that consumers would be willing to trade delivery speed for sensible prices on grocery orders. That model has been highly successful for Ocado in the U.K., but U.S. consumers have shown they value speed of delivery, with companies like Instacart and DoorDash expanding rapidly in recent years and rolling out services like 30-minute delivery.&lt;/p&gt;
    &lt;p&gt;Acknowledging this reality, Kroger noted on Tuesday that it’s deepening partnerships with third-party delivery companies. The grocer also said it will pilot “capital-light, store-based automation in high-volume markets” — a seeming nod to the type of micro-fulfillment technology that grocers have tested in recent years, and that Amazon is currently piloting in a Whole Foods Market store in Pennsylvania.&lt;/p&gt;
    &lt;p&gt;Fenyo pointed out that micro-fulfillment technology has also run into significant headwinds, adding that he thinks that outside of areas with large numbers of shoppers and high online ordering volume, putting automated order-assembly systems in stores probably doesn’t justify the cost.&lt;/p&gt;
    &lt;p&gt;Kroger’s decision to reduce its commitment to automation also poses a significant setback to Ocado, which has positioned its relationship with Kroger as a key endorsement of its warehouse automation technology. Shares in the U.K.-based robotics company have fallen dramatically and are now back to their level 15 years ago, when the company went public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.grocerydive.com/news/kroger-ocado-close-automated-fulfillment-centers-robotics-grocery-ecommerce/805931/"/><published>2025-12-08T23:53:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199530</id><title>Manual: Spaces</title><updated>2025-12-09T06:17:45.750681+00:00</updated><content>&lt;doc fingerprint="d7a8147305c8adb9"&gt;
  &lt;main&gt;
    &lt;p&gt;Space (whitespace) is a whole group of glyphs, one of the most important and frequently-used. Any computer user knows space as the widest key on their keyboard, however the notion itself is much bigger and comprises multiple important typographic terms and ideas.&lt;/p&gt;
    &lt;p&gt;Space in general is a blank unprinted area, a counterform that separates letters, words, lines etc. In typography, there are several types of spaces: sinkage (space on a page above a textblock), indent (space before the paragraph), leading (vertical space), word spacing, and letter spacing. In this article, we will primarily focus on word spacing, i.e. the space as a glyph.&lt;/p&gt;
    &lt;p&gt;European languages did not use word spacing for a long time, it was not until the 7th century that word spacing entered Latin script. In the age of metal type, the space was a material, tangible object — a piece of metal that left no print. In the pre-digital era, most text blocks were justified, which required several spaces of different width. Those types of spacing were defined by the notion of em (or point size), which is height of the piece of metal &lt;lb/&gt;Diagram of a cast metal sort, c is point size used for printing a character. For example, one em in a 12-point typeface is 12 points, whereas its en (half-em) spaces’ width is 6pt, third space (of an em) equals 4pt, and so on.&lt;/p&gt;
    &lt;p&gt;Whitespace characters in Gauge. Widths and correlations between spaces differ depending on the typeface&lt;/p&gt;
    &lt;p&gt;These types of spaces are still existent in the digital age, but they are mostly used by advanced typographers. Messengers, text editors, and other programs and applications most typically use only regular space.&lt;/p&gt;
    &lt;head rend="h2"&gt;Word space&lt;/head&gt;
    &lt;p&gt;Standard space, word space, space per se, is the symbol typed using the widest key on the keyboard.&lt;/p&gt;
    &lt;p&gt;In metal type, the size of standard space varied depending on the typographic tradition, in most cases the space was rather wide.&lt;/p&gt;
    &lt;p&gt;As a standard word space, metal composition used an en space, half the height of the point size, or em-square (in Cyrillic typography), while Latin space was equal to the third of the em space. Alexandra Korolkova Living Typography (2012)&lt;/p&gt;
    &lt;p&gt;In the early digitised fonts one often sees excessively wide spaces; probably, it was an attempt to imitate en space, or three-per-em space, which were used as the main spacing material in metal type. Such a space width can affect the typesetting rhythm and would seem redundant in modern typography.&lt;/p&gt;
    &lt;p&gt;Wide spacing is both physiologically unnecessary and makes the whole typeset structure reticulate, aesthetically ruining the page’s layout. If for some reason you can’t stick to en space size in this particular line, it’s better to scale down spacing using three-per-em spaces (that equal to the third of an em), or spaces of 3, or even 2 points. M. I. Schelkunov History, Technique, Art of Printing (1926)&lt;/p&gt;
    &lt;p&gt;A wide word spacing seems weird to an eye of the modern reader, and it is way too visible in texts&lt;/p&gt;
    &lt;p&gt;Today, word space width is specified by the typeface’s designer themselves, and it is one of the defining moments in designing a typeface, along with spacing, — texture and rhythm of the typeset are heavily dependent on word space width.&lt;/p&gt;
    &lt;p&gt;Many modern typographers are seeking to subject the space width to certain rules. For example, some type designers claim that the space should be equal to the bounding box of lowercase letter i. However, this rule can’t be universal: specifically, it definitely won’t work for typefaces where letter i is of unconventional design and proportions. In super large point sizes, spacing and word spaces are often intentionally reduced, as in such cases even the bounding box of the i can be too wide.&lt;/p&gt;
    &lt;p&gt;It used to be a rule of thumb for headline settings to leave a space between words that is just wide enough to fit in a lowercase i. For comfortable reading of long lines, the space between words should be much wider. Erik Spiekermann Stop stealing sheep &amp;amp; find out how type works (1993)&lt;/p&gt;
    &lt;p&gt;Depending on whether your typeface is serif or sans serif, it makes sense to take, or not to take, in consideration sidebearings of the glyph. It can be very different depending on style, too: with wide and light weights, there will be more unprinted area than with narrow and heavy weights, and this also applies to the space width.&lt;/p&gt;
    &lt;p&gt;There is no question but that wordspaces may not be too large, or that the line must appear to be an even, well-balanced whole. What applies to letterspaces also applies to wordspaces: they too are a function of the counters of the individual letters: the smaller these are, the smaller the wordspaces; the larger the counters, the larger the wordspaces. Jost Hochuli Detail in Typography (2008)&lt;/p&gt;
    &lt;p&gt;Blank space between words should be such as to ensure that words are visibly separated from each other — if spacing is wider, there will be holes between words, if smaller, it will be difficult to tell one word from another. You can’t measure space with a ruler, as everything depends on specific design or typeface.&lt;/p&gt;
    &lt;p&gt;Word spaces as set in Kazimir Text. The space width is good: words are separated from one another, the hierarchy of white space is maintained If you increase word spacing, word spaces would conflict with leading, which makes it hard to navigate through the text If you decrease the width of word space, it will affect legibility, as the words will blend together&lt;/p&gt;
    &lt;p&gt;Using double spaces is a technique inherited from the age of typewriters. It is strongly advisable to check a document for double spaces and replace those by single spaces.&lt;/p&gt;
    &lt;p&gt;Some of the recommendations learned by the educated typist are still now acquired habits wrongly used in digital documents; for instance, the use of three spaces after a period or two after the comma. There was just one space width available in the typewriter, so words and sentences were separated by the same distance. The double space was used to differentiate sentences and improve the readability of the text. María Ramos Silva Type design for typewriters: Olivetti (2015)&lt;/p&gt;
    &lt;p&gt;Additional spacing after a period is a questionable method in terms of readability. It can be assumed that in the age of typewriters additional space could have better separated sentences from one another in the context of monowidth typeface, yet monowidth period and space already form a larger gap than any space within the sentence. Since typewriters, typesetting tools have significantly improved over time, and today nobody will typeset in a monowidth typeface, unless it is absolutely necessary. So, currently, the use of double spaces is considered mauvais ton, i.e. bad manners, regardless of typeface.&lt;/p&gt;
    &lt;p&gt;American lawyer Matthew Butterick wrote a book on typography for lawyers, writers, and anyone who works with text. In the US, it is still very common among the older generation to use double spaces, so Matthew dedicated two entire chapters of his Practical Typography to this issue. Butterick tried to convince his audience by imaginary dialogues:&lt;/p&gt;
    &lt;p&gt;“If you approve of smaller word spaces in some situations, why do you insist on only one space between sentences, where a larger gap might be useful?” Because you’re already getting a larger gap. A sentence-ending word space typically appears next to a period. A period is mostly white space. So visually, the space at the end of a sentence already appears larger than a single word space. No need to add another. Matthew Butterick Butterick’s Practical Typography (2013)&lt;/p&gt;
    &lt;head rend="h2"&gt;Non-breaking Space&lt;/head&gt;
    &lt;p&gt;Non-breaking space is a space character that prevents an automatic line break at its position. For instance, in Russian and a number of other Central and Eastern European languages, non-breaking space serves to stick together a preposition and a word next to it, numbers and units of measurements, name and surname, etc.&lt;/p&gt;
    &lt;p&gt;Non-breaking space is supported by almost any text editing program, graphic design software, or browser, along with a standard space, so one shouldn’t forget to utilise it according to the typesetting rules of any given language.&lt;/p&gt;
    &lt;p&gt;In Russian language, non-breaking space shall connect the dash and its previous word (except for direct speech), prepositions with following words, initials with surname, abbreviations (such as i.e.), numero sign with numbers, numbers and units of measurements.&lt;/p&gt;
    &lt;p&gt;In English it is considered good manners to stick together not prepositions, but pronouns and articles with the following word. However, this rule is often neglected, especially when it comes to newspapers and magazines.&lt;/p&gt;
    &lt;p&gt;Professional typesetting software have spaces of non-standard widths. In InDesign, all additional spaces — em space, en space, thin space, etc. — are non-breaking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional spaces&lt;/head&gt;
    &lt;p&gt;Standard space is used everywhere; it is supported by any word, text, or code processing app. Non-breaking space is supported almost anywhere as well. However, computer typesetting still possesses a number of spaces dating back to metal type, allowing for finer adjustment of white space if necessary.&lt;/p&gt;
    &lt;p&gt;If a font supports additional spaces, those can be fetched via glyphs palette or using clipboard. Most graphic software do not support those spaces; for example, Adobe Illustrator 2020 includes only four additional spaces: em space, en space, thin space, and hair space.&lt;/p&gt;
    &lt;p&gt;And there is a reason for that: neither Illustrator, nor Photoshop were designed for advanced typesetting and laying out books. However, in InDesign you can easily set any kind of space, and a skilled typographer will use those.&lt;/p&gt;
    &lt;head rend="h2"&gt;Em Space&lt;/head&gt;
    &lt;p&gt;A space equal to the height of the em square (point size.) In early serifs, the metal face of the capital М tended to be square — probably, thus the English name. Metal type often used em space as paragraph indent.&lt;/p&gt;
    &lt;head rend="h2"&gt;En Space&lt;/head&gt;
    &lt;p&gt;Half of the width of an em. Russian-language metal type composition considered it the main type of space, even though in word spacing, especially if the text is aligned to the left or right, it is excessively wide.&lt;/p&gt;
    &lt;head rend="h2"&gt;Three-per-em Space, Third Space&lt;/head&gt;
    &lt;p&gt;One third of an em space. Historically considered as the main space in Western European typography.&lt;/p&gt;
    &lt;p&gt;The first obligation of a good typesetter is to achieve a compact line image, something best accomplished by using three-to-em or three-space word spacing. In former times even roman was set much tighter than we do it today; the specimen sheet that contains the original of Garamond’s roman of 1592, printed in 14-point, shows a word spacing in all lines of 2 points only, which is one-seventh of an em! This means that we cannot call three-to-em word spacing particularly tight. Jan Tschichold The Form Of The Book (1975)&lt;/p&gt;
    &lt;head rend="h2"&gt;Quarter Space&lt;/head&gt;
    &lt;p&gt;One fourth of an em space. Some authors believe quarter space to be the primary word space.&lt;/p&gt;
    &lt;p&gt;For a normal text face in a normal text size, a typical value for the word space is a quarter of an em, which can be written M/4. (A quarter of an em is typically about the same as, or slightly more than, the set-width of the letter t.) Robert Bringhurst The Elements of Typographic Style (1992)&lt;/p&gt;
    &lt;head rend="h2"&gt;Thin Space&lt;/head&gt;
    &lt;p&gt;⅕ of an em space. It is common that thin space equals about half the standard one, which is why thin space is used where standard word space would be too wide. For example, thin space is often utilised for spacing a dash in cases where standard space is too wide. Thin space is also used for spacing initials, from each other and from the surname:&lt;/p&gt;
    &lt;p&gt;Standard space in Spectral is too wide to be used for spacing initials and dashes Thin spaces look more neat, better connecting initials with a surname and two parts of a sentence with each other&lt;/p&gt;
    &lt;p&gt;French typographic tradition prescribes the use of either thin or hair spaces to space any two-part symbols: exclamation mark, question mark, semicolon, etc.&lt;/p&gt;
    &lt;p&gt;Regardless of the language, such glyphs as question mark and exclamation mark typically are very visible in lowercase, but they can get lost in an all-caps typeset — in this case, one should finely space them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sixth Space&lt;/head&gt;
    &lt;p&gt;The sixth space is used when the thin space is too large.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hair Space&lt;/head&gt;
    &lt;p&gt;The narrowest of spaces. In metal type, it was equal to 1/10 of an em space, in the digital age it is mostly 1/ 24 of an em. It might be useful if a certain typeface’s punctuation marks have too tight sidebearings, but a thin space would be too wide. For example, you can use hair space to space dashes instead of thin one — everything depends on the sidebearings and the design of the particular typeface.&lt;/p&gt;
    &lt;p&gt;You should keep in mind that after you change font, selected space glyphs will remain, but their width can change, — and this will affect the texture.&lt;/p&gt;
    &lt;p&gt;Isn’t it ridiculous when a punctuation mark, relating to the entire preceding phrase, is tied to one last word of the said phrase? And, vice versa, how unfortunately it looks when there is a large gap between this mark and the previous word. As a matter of fact, it is about time type foundry workers started thinking about it and cast the punctuation mark with an extra sidebearing on its left. However, typefounders are not always, or rather rarely, that forethoughtful, and also they are used to cast all letters without generous sidebearings. During punching of matrices, the beauty of spacing punctuation marks is also barely remembered. Therefore, it is your burden and responsibility to fix this &lt;/p&gt;
    &lt;head rend="h2"&gt;Spacing in justified texts&lt;/head&gt;
    &lt;p&gt;Full justification — that is, alignment of text to its both margins, — is still commonly used in books and magazines. When the text is justified, the width of word spaces is not constant, it is changing to distribute words to the entire width of the line. In this situation, the uniformity of spacing could be even more important than the very width of these spaces: evenly large spaces in the entire page are better than large spaces in only one line. That is why, no matter how optimised the typeface’s word spacing in terms of its width is, it will not be enough for typesetting a justified text. While in metal type all spaces were set manually, and a typesetter knew what space they should add for even typesetting, nowadays it’s a computer that defines the length of spaces for justified texts. The algorithm divides the remaining space into equal parts and adds them to regular spaces. In doing so, the algorithm ignores letters, syntax, and punctuation, which is why when typesetting justified texts one should always double-check and adjust spacing manually.&lt;/p&gt;
    &lt;p&gt;In Indesign, it is possible to set minimum and maximum word spacing width for fully justified text typesetting: the width of standard space is used as a basis 100 %, maximum is normally about 120 %, minimum is about 80 %.&lt;/p&gt;
    &lt;p&gt;If the text is justified, a reasonable minimum word space is a fifth of an em (M/5), and M/4 is a good average to aim for. A reasonable maximum in justified text is M/2. If it can be held to M/3, so much the better. But for loosely fitted faces, or text set in a small size, M/3 is often a better average to aim for, and a better minimum is M/4. In a line of widely letterspaced capitals, a word space of M/2 or more may be required. Robert Bringhurst The Elements of Typographic Style (1992)&lt;/p&gt;
    &lt;p&gt;Robert Bringhurst recommends choosing appropriate spaces based on an em. However, space is a relative value, so in justified texts you should consider not the width of some abstract em, but rather the width of space in particular font.&lt;/p&gt;
    &lt;p&gt;The optimal word space width in justified texts is ephemeral and changes depending on typeface, point size, line width, line spacing, and many other factors. That is why in Indesign you can’t set maximum and minimum values once and for all cases — you will have to choose the best possible options manually.&lt;/p&gt;
    &lt;p&gt;In setting justified texts, standard word space width becomes a fluctuating value. The fixed width space and all additional spaces with constant width can help better control the setting.&lt;/p&gt;
    &lt;p&gt;The more even are the gaps between words, the better &amp;lt;…&amp;gt;. In no case shall you allow a considerable disparity in space widths, while an insignificant difference won’t ruin the beauty of typesetting. Pyotr Kolomnin A Concise Account of Typography (1899)&lt;/p&gt;
    &lt;head rend="h2"&gt;Figure Space&lt;/head&gt;
    &lt;p&gt;Figure space, or numeric space, is used for typesetting tables and sheets. If a typeface is fitted with tabular figures, its figure space will be equal to the width of tabular figures. Figure space is a non-breaking one.&lt;/p&gt;
    &lt;p&gt;Normally, figure space is significantly wider than standard space, it will be helpful when you need to even a large amount of multi-digit numbers&lt;/p&gt;
    &lt;head rend="h2"&gt;Punctuation Space&lt;/head&gt;
    &lt;p&gt;In most cases, the width of this space is equal to the glyph width of a period or a colon. May be of use in making up numbers in tables where digits are defined by a spacing element instead of period or colon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Narrow No-break Space&lt;/head&gt;
    &lt;p&gt;A thin space that prevents an automatic line break. The name of this symbol in Unicode causes additional confusion: Narrow in this case is the same thing as Thin, and Narrow Space has the same width as Thin Space does.&lt;/p&gt;
    &lt;p&gt;In some applications, such as InDesign, the simple regular thin space is non-breaking by default and is called with Thin Space. In other cases it’s a separate symbol, for example, the Web uses Narrow No-break Space.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spaces in layout&lt;/head&gt;
    &lt;p&gt;The distribution of white space in text setting is a highly important factor, responsible for the neat design and the content’s clear structure. Many designers keep in mind correlation between point size, line width, and margins, but some tend to forget that word spacing is an equivalent factor of these relations.&lt;/p&gt;
    &lt;p&gt;Body text font, designed for smaller sizes, would require smaller spacing and word spaces if used to set a large headline. The point size gets more important in determining spacing and white unprinted area in general, than whether it is a text typeface or a display one.&lt;/p&gt;
    &lt;p&gt;It is also necessary to consider spacing when you’re dealing with particular elements of the text. For instance, small-caps or all-caps fragments quite often should be additionally spaced. Manual spacing is sometimes necessary in bold or italic styles, or even if no additional styles are applied at all.&lt;/p&gt;
    &lt;p&gt;Small-caps spacing in Charter is too tight by default, more white space is needed In William, small caps are taken care of, this generous spacing doesn’t require additional adjustment A text set in a quality typeface sometimes needs manual adjustment: standard word space in Guyot is clearly not enough for the of ‘i’ combination&lt;/p&gt;
    &lt;head rend="h2"&gt;White spaces in software&lt;/head&gt;
    &lt;p&gt;Most typically, in non-professional software and web services there are only standard and non-breaking spaces available. You might be able to set additional symbols using clipboard almost anywhere where Unicode is supported. That said, you have to check everytime: for example, at the time of writing this piece, Facebook allows for inserting additional symbols in its input field, but automatically replaces them while posting.&lt;/p&gt;
    &lt;p&gt;Speaking of the Web, additional spaces are available as HTML special characters: if you use them, your source code might become a bit cluttered, but that would allow you to control the placing of each non-standard space. Please note that different browsers might render spacing differently, and not so long ago some of them even ignored additional spaces, replacing them by regular ones. You should check on the correct display of additional spaces where you use it.&lt;/p&gt;
    &lt;p&gt;Two industry standards for text formatting and typesetting, InDesign and Quark Xpress, support all kinds of spaces. Today, type designers usually include at least thin and hair spaces. Their width might vary from one typeface to another — but the typographer, at least, has more control over the word spacing.&lt;/p&gt;
    &lt;p&gt;In InDesign, an additional space not included in the typeface would still be visible, but its width would be defined by the software with no regard to what kind of typeface it is. For example, hair space in 24pt size will be 1pt — both in a display face with tight spacing and in a text face with loose spacing.&lt;/p&gt;
    &lt;p&gt;Spaces calculated this way are not always suitable for your task. Depending on the typeface, the additional space width suggested by InDesign can be insufficient or excessive. And if you export the text with such spaces from InDesign to Figma, their width will most likely change — every software may have its own algorithms for calculating these values.&lt;/p&gt;
    &lt;p&gt;Be vigilant and trust your eye: it is not mathematical values that matter, but a convincing, reasonable relationship between the black and the white.&lt;/p&gt;
    &lt;p&gt;These dashes are spaced by hair spaces provided by the typeface These dashes are spaced by hair spaces provided by the typeface The typefaces above have no hair space, therefore its width is set automatically With x-height and spacing that Arno Pro and RIA Text have, the InDesign’s hair space is good enough. Whereas in IBM Plex we perhaps should put thin space instead of a hair one&lt;/p&gt;
    &lt;p&gt;Whitespace characters are among the most important typographic elements. Alongside sidebearings, they define text rhythm and organise blocks of information. Disregard for white spaces can ruin relations between them: line and word spacing, word spacing and column-gap. In such case the reader wouldn’t be able to easily track the line and would have to put additional &lt;/p&gt;
    &lt;head rend="h2"&gt;Summary table&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Non-breaking space&lt;/cell&gt;
        &lt;cell&gt;MacOS: Alt + Space&lt;p&gt;Windows: Alt+0160&lt;/p&gt;&lt;p&gt;Unicode: U00A0&lt;/p&gt;&lt;p&gt;HTML:&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Nonbreaking Space or Alt + Cmnd + X&lt;/p&gt;&lt;p&gt;in case you need a space of non-changing width, in a justified text layout:&lt;/p&gt;&lt;p&gt;Type → Insert White Space → Nonbreaking Space (Fixed Width)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Thin space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2009&lt;p&gt;HTML: &amp;amp;ThinSpace;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Thin Space&lt;/p&gt;&lt;p&gt;or&lt;/p&gt;&lt;p&gt;Shift + Alt + Cmnd + M&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Thin non-breaking space (for Web)&lt;/cell&gt;
        &lt;cell&gt;Unicode: U202F&lt;p&gt;HTML: &amp;amp;#8239;&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Em space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2003&lt;p&gt;HTML: &amp;amp;emsp;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Em Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;En space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2002&lt;p&gt;HTML: &amp;amp;ensp;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → En Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Third space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2004&lt;p&gt;HTML: &amp;amp;emsp13;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Third Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Quarter space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2005&lt;p&gt;HTML: &amp;amp;emsp14;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Quarter Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Sixth space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2006&lt;p&gt;HTML: &amp;amp;#8198;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Sixth Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hair space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U200A&lt;p&gt;HTML: &amp;amp;hairsp;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Hair Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Figure space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2007&lt;p&gt;HTML: &amp;amp;numsp;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Figure Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Punctuation space&lt;/cell&gt;
        &lt;cell&gt;Unicode: U2008&lt;p&gt;HTML: &amp;amp;puncsp;&lt;/p&gt;&lt;p&gt;Indesign: Type → Insert White Space → Punctuation Space&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;head rend="h3"&gt;In English&lt;/head&gt;
    &lt;p&gt;Kirill Belyayev, Whitespaces and zero width characters with buttons for copying to clipboard, short mnemonics and usage comments &lt;lb/&gt; Robert Bringhurst, The Elements of Typographic Style &lt;lb/&gt; Matthew Butterick, Butterick’s Practical Typography &lt;lb/&gt; Jost Hochuli, Detail in Typography &lt;lb/&gt; Yves Peters, Adventures in Space (fontshop.com) &lt;lb/&gt; María Ramos Silva, Type design for typewriters: Olivetti &lt;lb/&gt; Erik Spiekermann, Stop stealing sheep &amp;amp; find out how type works &lt;lb/&gt; Jan Tschichold, The Form Of The Book &lt;lb/&gt; Martin Wichary, Space Yourself (smashingmagazine.com) &lt;/p&gt;
    &lt;head rend="h3"&gt;In Russian&lt;/head&gt;
    &lt;p&gt;Pyotr Kolomnin, A Concise Account of Typography &lt;lb/&gt; Alexandra Korolkova, Living Typography &lt;lb/&gt; M. I. Schelkunov, History, Technique, Art of Printing &lt;lb/&gt; Alexei Yozhikov, (Nearly) Everything You Need To Know About Whitespace (habr.com) &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://type.today/en/journal/spaces"/><published>2025-12-09T00:07:42+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199623</id><title>The universal weight subspace hypothesis</title><updated>2025-12-09T06:17:45.638717+00:00</updated><content>&lt;doc fingerprint="3c1d2c0bdef3808e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 4 Dec 2025 (v1), last revised 6 Dec 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:The Universal Weight Subspace Hypothesis&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Prakhar Kaushik [view email]&lt;p&gt;[v1] Thu, 4 Dec 2025 18:59:58 UTC (14,316 KB)&lt;/p&gt;&lt;p&gt;[v2] Sat, 6 Dec 2025 04:42:07 UTC (14,321 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2512.05117"/><published>2025-12-09T00:16:46+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199723</id><title>Horses: AI progress is steady. Human equivalence is sudden</title><updated>2025-12-09T06:17:45.395299+00:00</updated><content>&lt;doc fingerprint="5b9e92572bb22e27"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Horses&lt;/head&gt;
    &lt;p&gt;So after all these hours talking about AI, in these last five minutes I am going to talk about: horses.&lt;/p&gt;
    &lt;p&gt;Engines, steam engines, were invented in 1700.&lt;/p&gt;
    &lt;p&gt;And what followed was 200 years of steady improvement, with engines getting 20% better a decade.&lt;/p&gt;
    &lt;p&gt;For the first 120 years of that steady improvement, horses didn't notice at all.&lt;/p&gt;
    &lt;p&gt;Then, between 1930 and 1950, 90% of the horses in the US disappeared.&lt;/p&gt;
    &lt;p&gt;Progress in engines was steady. Equivalence to horses was sudden.&lt;/p&gt;
    &lt;p&gt;But enough about horses. Let's talk about chess!&lt;/p&gt;
    &lt;p&gt;Folks started tracking computer chess in 1985.&lt;/p&gt;
    &lt;p&gt;And for the next 40 years, computer chess would improve by 50 Elo per year.&lt;/p&gt;
    &lt;p&gt;That meant in 2000, a human grandmaster could expect to win 90% of their games against a computer.&lt;/p&gt;
    &lt;p&gt;But ten years later, the same human grandmaster would lose 90% of their games against a computer.&lt;/p&gt;
    &lt;p&gt;Progress in chess was steady. Equivalence to humans was sudden.&lt;/p&gt;
    &lt;p&gt;Enough about chess! Let's talk about AI.&lt;/p&gt;
    &lt;p&gt;Capital expenditure on AI has been pretty steady.&lt;/p&gt;
    &lt;p&gt;Right now we're - globally - spending the equivalent of 2% of US GDP on AI datacenters each year.&lt;/p&gt;
    &lt;p&gt;That number seems to have steadily been doubling over the past few years.&lt;/p&gt;
    &lt;p&gt;And it seems - according to the deals signed - likely to carry on doubling for the next few years.&lt;/p&gt;
    &lt;p&gt;But from my perspective, from equivalence to me, it hasn't been steady at all.&lt;/p&gt;
    &lt;p&gt;I was one of the first researchers hired at Anthropic.&lt;/p&gt;
    &lt;p&gt;This pink line, back in 2024, was a large part of my job. Answer technical questions for new hires.&lt;/p&gt;
    &lt;p&gt;Back then, me and other old-timers were answering about 4,000 new-hire questions a month.&lt;/p&gt;
    &lt;p&gt;Then in December, Claude finally got good enough to answer some of those questions for us.&lt;/p&gt;
    &lt;p&gt;In December, it was some of those questions. Six months later, 80% of the questions I'd been being asked had disappeared.&lt;/p&gt;
    &lt;p&gt;Claude, meanwhile, was now answering 30,000 questions a month; eight times as many questions as me &amp;amp; mine ever did.&lt;/p&gt;
    &lt;p&gt;Now. Answering those questions was only part of my job.&lt;/p&gt;
    &lt;p&gt;But while it took horses decades to be overcome, and chess masters years, it took me all of six months to be surpassed.&lt;/p&gt;
    &lt;p&gt;Surpassed by a system that costs one thousand times less than I do.&lt;/p&gt;
    &lt;p&gt;A system that costs less, per word thought or written, than it'd cost to hire the cheapest human labor on the face of the planet.&lt;/p&gt;
    &lt;p&gt;And so I find myself thinking a lot about horses, nowadays.&lt;/p&gt;
    &lt;p&gt;In 1920, there were 25 million horses in the United States, 25 million horses totally ambivalent to two hundred years of progress in mechanical engines.&lt;/p&gt;
    &lt;p&gt;And not very long after, 93 per cent of those horses had disappeared.&lt;/p&gt;
    &lt;p&gt;I very much hope we'll get the two decades that horses did.&lt;/p&gt;
    &lt;p&gt;But looking at how fast Claude is automating my job, I think we're getting a lot less.&lt;/p&gt;
    &lt;p&gt;This was a five-minute lightning talk given over the summer of 2025 to round out a small workshop.&lt;/p&gt;
    &lt;p&gt;All opinions are my own and not those of my employer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://andyljones.com/posts/horses.html"/><published>2025-12-09T00:26:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199935</id><title>Scientific and Technical Amateur Radio</title><updated>2025-12-09T06:17:44.776471+00:00</updated><content>&lt;doc fingerprint="b048417162f443bb"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding ESCAPADE&lt;p&gt;ESCAPADE is a twin spacecraft mission that will study the Mars magnetosphere. The science mission is led by UC Berkeley Space Sciences Laboratory and the spacecraft buses were built by Rocket Lab. It was launched on November 13 on the second Blue Origin New Glenn mission NG-2. The spacecraft will spend a year around the Earth-Sun L2 Lagrange point before falling back to Earth for a powered gravity assist that will place them on Hohmann transfer orbit to Mars as the “launch window” to Mars opens. These are the first spacecraft to fly this kind of trajectory.&lt;/p&gt;&lt;p&gt;The day after launch, I used two antennas from the Allen Telescope Array to record the X-band telemetry signals of the two spacecraft, which were approximately 200 thousand km away from Earth. In this post I will show the results of this observation, and how to decode the telemetry. I have published the recording in the dataset “Recording of ESCAPADE X-band telemetry with the Allen Telescope Array shortly after launch” in Zenodo.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;sigmf-toolkit&lt;p&gt;I have published a new Python package called sigmf-toolkit. It is intended to be a collection of Python tools to work with SigMF files. At the moment it only contains two tools, but I plan on adding more tools to this package as the needs arise. These tools are:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;gr_meta_to_sigmf&lt;/code&gt;. It converts a GNU Radio metadata file with detached headers to a SigMF file. At the moment it is really simple, and it doesn’t handle capture discontinuities.&lt;/item&gt;&lt;item&gt;sigmf_pcap_annotate. This tool parses a PCAP file using Scapy and it adds annotations to a SigMF file for each packet in the PCAP file.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I find this&lt;/p&gt;&lt;code&gt;sigmf_pcap_annotate&lt;/code&gt;tool quite useful when comparing side by side a SigMF file in Inspectrum and a PCAP file in Wireshark to debug issues with digital communications systems. In this post I showcase how this tool can be used.&lt;/item&gt;
      &lt;item&gt;Non-coherent m-FSK BER&lt;p&gt;Yesterday I posted about how to compute the well known formula for the bit error rate of FSK with non-coherent demodulation. Later I realized that the same kind of argument can be extended to cover the case of \(m\)-FSK in which the \(m\) tones are orthogonal. The formula for this is not so well known, and I don’t recall having seen it before, although surely it is somewhere in the literature. Here I show the calculations and the closed-form expression that is obtained.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Non-coherent FSK BER&lt;p&gt;It is well known that the formula for the bit error rate of 2-FSK using non-coherent demodulation is \(\exp(-\frac{E_b}{2N_0})/2\). However, I can never quickly find a source where this formula is derived, so I decided to figure this out and write down the derivation. I will use my post about m-FSK symbol error rate as a starting point.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;10 years of blogging&lt;p&gt;Today marks 10 years since I wrote the first post in this blog. It was a very basic and brief post about me decoding the European FreeDV net over a WebSDR. I mainly wrote it as a way of getting the ball rolling when I decided to start a blog back in October 2015. Over the 10 years that I have been blogging, the style, topics, length and depth of the posts have kept shifting gradually. This is no surprise, because the contents of this blog are a reflection of my interests and the work I am doing that I can share freely (usually open source work).&lt;/p&gt;&lt;p&gt;Since I started the blog, I have tried to publish at least one post every month, and I have managed. Sometimes I have forced myself to write something just to be up to the mark, but more often than not the posts have been something I really wanted to write down and release to the world regardless of a monthly tally. I plan to continue blogging in the same way, and no doubt that the contents will keep evolving over time, as we all evolve as persons during our lifetime. Who knows what the future will bring.&lt;/p&gt;&lt;p&gt;I wanted to celebrate this occasion by making a summary of the highlights throughout these 10 years. I have written 534 posts, and although Google search is often useful at finding things, for new readers that arrive to this blog it might be difficult to get a good idea of what kind of content can be found here. This summary will be useful to expose old content that can be of interest, as well as serve me to reflect on what I have been writing about.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;10ghz artemis1 astronomy astrophotography ATA ccsds ce5 contests digital modes doppler dslwp dsp eshail2 fec filters freedv frequency gmat gnss gnuradio gomx hermeslite hf jt kits lilacsat limesdr linrad lte microwaves mods moonbounce noise ofdm orbital dynamics outernet polarization radar radioastronomy rust satellites sdr signal generators tianwen vhf &amp;amp; uhf&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://destevez.net/"/><published>2025-12-09T00:50:32+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46199950</id><title>The Lost Machine Automats and Self-Service Cafeterias of NYC (2023)</title><updated>2025-12-09T06:17:44.589790+00:00</updated><content>&lt;doc fingerprint="fe209013bb5586e4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Guide to the NYBG Holiday Train Show, An Annual Love Letter to NYC&lt;/head&gt;
    &lt;p&gt;Discover which NYC buildings—both lost and extant—have been recreated out of plants!&lt;/p&gt;
    &lt;p&gt;“Automats were right up there with the Statue of Liberty and Madison Square Garden,” Kent L. Barwick, former president of the Municipal Art Society, lamented to the New York Times in 1991 when the country’s last automat closed. The automat, a precursor to today’s fast food chains, was a staple of the New York City dining scene in the first half of the 20th century. Originally conceived in Germany, the self-service restaurant featured coin-operated vending machines from which patrons could buy fresh coffee, simple meals, and desserts for an affordable price.&lt;/p&gt;
    &lt;p&gt;Along with automats, self-service cafeterias changed the way New Yorkers ate and socialized. In her book, Kibbitz and Nosh: When We All Met at Dubrow’s Cafeteria (Three Hills, May 2023), photographer Marcia Bricker Halperin revisits one of New York City’s most popular self-service cafeterias on Kings Highway in Brooklyn. Through Halperin’s photographs from the 1970s and 80s and essays by Donald Marguiles and Deborah Dash Moore, the book explores the story of Dubrow’s Cafeteria and the culture that sprang up around these New York City eateries. Check out our book talk with Halperin in our video archive!&lt;/p&gt;
    &lt;p&gt;Here, we take a look at 8 of the city’s lost automats and self-service cafeterias:&lt;/p&gt;
    &lt;p&gt;Automats are synonymous with Horn &amp;amp; Hardart. Business partners Joseph Horn and Frank Hardart opened the first automat in the United States in Philadelphia in 1902. They expanded into New York City in 1912, opening the first location in Times Square. Eventually, there would be more than forty Horn &amp;amp; Hardart locations in New York. One former Horn &amp;amp; Hardart building that still stands can be found at 2710-2714 Broadway, on the southeast corner of Broadway and 104th Street. It was occupied by the automat until 1953. A ghost sign at 146 West 48th Street marks another former location. At its height, the company had more than 150 automats and retail shops throughout Philadelphia, New York, and Baltimore.&lt;/p&gt;
    &lt;p&gt;In the beginning, automats served simple foods like buns, fish cakes, and beans. Diners could also get hot coffee, brewed fresh every twenty minutes, for just five cents. In addition to having the best cup of coffee in town, the automats were also known for their striking Art Deco decor. As the company continued to grow, its menu expanded to include lunch and dinner foods like mac and cheese. pot pies, and steaks. The company even opened up retail locations where they sold packaged “to-go” foods.&lt;/p&gt;
    &lt;p&gt;The last Horn and Hardart automat, located at 200 East 42nd Street at 3rd Avenue, closed on April 8, 1991. Automats continues to be part of New York City culture today as it was recreated as a set for the fifth and final season of Amazon’s hit series The Marvelous Mrs. Maisel. In Brooklyn, The Brooklyn Dumpling Shop is bringing back the automat format of dining with new technology.&lt;/p&gt;
    &lt;p&gt;Like automats, cafeterias were waiter-less establishments. Customers would first receive a ticket with the menu items and prices. They would then approach the food counter and make selections as the server on the other side hole-punched the ticket. Taking their tray full of food, patrons then searched for a table, which was usually shared.&lt;/p&gt;
    &lt;p&gt;Cafeterias started on Wall Street in the late 19th century as a way for busy brokers to grab a quick lunch. They soon spread throughout the city and beyond. In 1929, Belarusian immigrant Benjamin Dubrow opened Dubrow’s Pure Food, a full-service restaurant in Crown Heights at the intersection of Eastern Parkway and Utica Avenue. When the Great Depression hit, however, he needed to try a new business model. Dismissing all of his waitstaff in 1931, he converted the restaurant into a cafeteria “with refinement.” In 1939, he opened another cafeteria at 1521 Kings Highway and another in Manahttan’s Garment District in 1952. Dubrow’s Cafeteria served a wide variety of dishes including Jewish staples like blintzes with applesauce and sour cream, kugels, and gefilte fish.&lt;/p&gt;
    &lt;p&gt;The self-service cafeterias of New York City offered a unique “third place,” a place outside of work and home, where New Yorkers could comfortably socialize with their neighbors, all “for the price of a cup of coffee.” In Halperin’s book, Kibbitz and Nosh: When We All Met at Dubrow’s Cafeteria, Deborah Dash Moore writes about how while the cafeterias attracted a diverse clientele, “New York Jews particularly embraced cafeterias, less as a fast-food option than as a place to sit and schmooze.” Halperin reminisces about the people she met and photographed at Dubrow’s, writing, “I met amazing people at Dubrow’s. Most were people I ordinarily would never have had a conversation with over a cup of coffee—ex-vaudeville performers, taxi drivers, Holocaust survivors, ex-prizefighters, and bookies. Women named Gertrude, Rose, and Lillian all had sad love stories to tell and big hearts.”&lt;/p&gt;
    &lt;p&gt;The Kings Highway location of Dubrow’s Cafeteria hosted a few historic moments. John F. Kennedy held a large campaign rally outside the restaurant in 1960. Senator Robert F. Kennedy and Jimmy Carter also made appearances at the cafeteria during their own presidential campaigns. It was also where Sandy Koufax announced his decision to join the Brooklyn Dodgers. The Eastern Parkway location closed in the early 1960s while the Kings Highway cafeteria stayed open until 1978. The Manhattan location shut down in 1985.&lt;/p&gt;
    &lt;p&gt;The Garden Cafeteria was a hotspot for Jewish intellectuals and writers at 165 East Broadway, on the corner of Rutgers Street. Established by Austrian immigrant Charles Metzger in 1941, the eatery has a storied history on the Lower East Side. Located next to the offices of The Forvertz/The Jewish Daily Forward, the cafeteria was frequented by the paper’s writers. Nobel laureate Isaac Bashevis Singer and photographer Bruce Davidson were among its patrons. Singer set his short story ”The Cabalist of East Broadway” at the Garden Cafeteria.&lt;/p&gt;
    &lt;p&gt;The cafeteria closed in 1983 and became a Chinese restaurant. When construction work in 2005 revealed one of the original signs, it was given to the Museum at Eldridge Street for safe keeping. The sign has appeared on display at the Museum and in an exhibit on The Jewish Daily Forward at Museum of the City of New York.&lt;/p&gt;
    &lt;p&gt;The Belmore Cafeteria once stood at 28th Street and Park Avenue South. Opened in 1929, it was founded by Philip Siegel and run by his family until it closed in 1981. Billed as “New York’s Most Fabulous Self-Service Restaurant,” the establishment attracted some interesting characters.&lt;/p&gt;
    &lt;p&gt;Members of the notorious Murder Inc. gang reportedly ate there, but the clientele the cafeteria was known for was taxi drivers. It was a common sight to see a row of taxis lined up at the curb outside. Fittingly, the cafeteria appears as a location in the 1976 Robert DiNero film,Taxi Driver. An estimated 5,000 people a day passed under the cafeteria’s glowing red neon sign and through its turnstile each weekday. In 1981, the Siegel’s sold their building and a condominium tower was built at the site.&lt;/p&gt;
    &lt;p&gt;In a 1971 New York Times article, Garfield’s Cafeteria on Flatbush Avenue was described as a “grand old cafeteria” where you could “stop in at midnight for a nosh, or something to nibble on after leaving the Alebrmarle dance parlor or to recover from the hilarity of vaudeville at the Flatbush Theater.” Like Dubrow’s, the cafeteria served blintzes, bialys, matzoh-ball soup, and more.&lt;/p&gt;
    &lt;p&gt;Since the cafeteria was open in the morning and late at night, it attracted different crowds at different times of the day. Families and old-timers usually came for breakfast and lunch, while the nighttime brought the after-theater crowds. The Times wrote that some elderly patrons would even bring their own food and sit at the cafeteria purely for the social aspect as they nursed a cup of coffee and chatted with their neighbors for hours.&lt;/p&gt;
    &lt;p&gt;Another famous Brooklyn cafeteria was Hoffman’s Cafeteria on Pitkin and Saratoga Avenues in Brownsville. This cafeteria is often mentioned alongside Dubrow’s and Garfield’s as one of the most popular. Like Dubrow’s and Garfield’s it closed in the 1970s. Hoffman’s made news in the 1940s for a butter heist. It was discovered that two of the countermen were stealing food, mostly butter, from the establishment for a period of three months. The stolen goods amounted to $15,000!&lt;/p&gt;
    &lt;p&gt;There were multiple locations of Hector’s Cafeteria in Times Square since the 1930s. The last remaining cafeteria was inside the Claridge Hotel building on Broadway at 44th Street. It lasted until 1970.&lt;/p&gt;
    &lt;p&gt;Before Hector’s closed, it made its way into pop culture. The cafeteria is mentioned in Jack Kerouac’s novel On the Road when Dean Moriarty first arrives in New York and “looking for a place to eat,” “went right to Hector’s, and since then Hector’s Cafeteria has always been a big symbol of New York for Dean.” You can also see a bit of Hector’s in this Dennis Stock photograph of actor James Dean.&lt;/p&gt;
    &lt;p&gt;Stewart’s Cafeteria occupied the first floor of an Art Deco building at 116 Seventh Avenue South in Greenwich Village. Opened in 1933, it was part of a chain of cafeterias. Stewart’s was only open for a few years before closing and re-opening as Life Cafeteria. The building still exists today (it houses a Bank of America and CVS Pharmacy) and is regarded as an LGBTQ+ history site.&lt;/p&gt;
    &lt;p&gt;Life Cafeteria attracted a bohemian clientele including gay and lesbian patrons. Unlike most places in the city at the time where homosexuality was hidden, the large windows of Life Cafeteria put everything that happened inside on display. Crowds of tourists often formed outside the windows to peer in. Tennessee Williams and Marlon Brando were known to visit, and the scenes inside have been captured in paintings by Paul Cadmas and Vincent La Gambina.&lt;/p&gt;
    &lt;p&gt;Dubrow’s Cafeteria Book Talk&lt;/p&gt;
    &lt;p&gt;Next, check out 9 Old Fashioned Soda Fountains in NYC&lt;/p&gt;
    &lt;p&gt;Subscribe to our newsletter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.untappedcities.com/automats-cafeterias-nyc/"/><published>2025-12-09T00:51:31+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=46201381</id><title>Modern Walkmans</title><updated>2025-12-09T06:17:44.088797+00:00</updated><content>&lt;doc fingerprint="9cc2a97a4aa60702"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Modern Walkmans&lt;/head&gt;Cassette Players for the Modern Digital Age&lt;p&gt;Σ 11 models&lt;/p&gt;&lt;p&gt;(Toshiba) Wireless cassette player with Bluetooth, you can enjoy cassettes with wireless earphones. Equipped with virtual surround sound, you can enjoy realistic sound. It can play cassette tapes for about 16 hours (2*AA alkaline batteries). It can also be powered, and played from a USB port. Weight 230g. Selling primarly in Japan.&lt;/p&gt;&lt;p&gt;(-) Plastic flywheel&lt;/p&gt;&lt;p&gt;Web: https://aurex.jp/products/ax-w10c/&lt;/p&gt;&lt;p&gt;Bring out the soundtrack of past memories on Your cherished cassettes. FM/AM Radio playback. Voice Activation System. Automatic Stop System. 2AA Battery or USB power supply. KCS-315&lt;/p&gt;&lt;p&gt;(-) Bad sound quality&lt;/p&gt;&lt;p&gt;Web: https://www.byronstatics.com/byronstatics-cassette-player-black/&lt;/p&gt;&lt;head rend="h5"&gt;DIGITNOW!&lt;/head&gt;30$&lt;p&gt;The personal cassette player looks the part with its retro silver casing and comes complete with earphones for your private listening. With bluetooth function, can transmit the music to other bluetooth receivers and let everyone enjoy the music.&lt;/p&gt;&lt;p&gt;(-) Included earphones crap&lt;/p&gt;&lt;p&gt;Web: https://www.amazon.com/DIGITNOW-Bluetooth-Cassette-Headphone-Earphones/dp/B08LKF3J9Q&lt;/p&gt;&lt;head rend="h5"&gt;FiiO CP13&lt;/head&gt;120$&lt;p&gt;Achieving ultra-low Wow and Flutter. Oversized pure copper flywheel. 100% pure analog sound &amp;amp; custom balanced amplification head. Classic audiophile op-amp JRC5532. High voltage motor power supply. Dual-color all-aluminum alloy chassis and a long-lasting 13 hours of battery life.&lt;/p&gt;&lt;p&gt;(-) Pricy, no recording&lt;/p&gt;&lt;p&gt;Web: https://www.fiio.com/cp13&lt;/p&gt;&lt;head rend="h5"&gt;GPO&lt;/head&gt;27$&lt;p&gt;Battery powered and with built in speakers, just plug in your cassette and you're ready to go. The portable cassette player was an iconic piece of kit for music fans in the 80s and 90s. Play tapes or use the FM radio and listen through your headphones.&lt;/p&gt;&lt;p&gt;(-) Included earphones crap&lt;/p&gt;&lt;p&gt;Web: https://www.amazon.com/GPO-GPOPCP-Personal-Cassette-Player/dp/B07T6HW8DX&lt;/p&gt;&lt;head rend="h5"&gt;It's OK!&lt;/head&gt;63$&lt;p&gt;It is the world’s first cassette player with Bluetooth 5.0 capability that not only supports traditional 3.5mm headphones but is also compatible with Bluetooth 5.0 headphones or speakers. Whether you are alone or in an open space, you can freely enjoy the penetrating voice and warm sound from the cassette tape.&lt;/p&gt;&lt;p&gt;(-) No autoreverse, or any convenience function. No headphone.&lt;/p&gt;&lt;p&gt;Web: https://www.ninmlab.com/its-ok&lt;/p&gt;&lt;head rend="h5"&gt;Jensen&lt;/head&gt;30$&lt;p&gt;Jensen Portable Compact Lightweight Slim Design Stereo, AM/FM Radio Cassette Player. Pop in that favorite cassette or relive the magic of the mixed tape with Jensen's Portable Stereo Cassette Player AM/FM Stereo Cassette Player. When you're feeling more like the radio, tune into the AM or FM dial. You can also get up to the minute weather info with local Weather Band broadcasts. And, in the name of keeping things economical, just 2 'AA' battery has the Walkman up and running for hours on end.&lt;/p&gt;&lt;p&gt;(-) fragile, bad sound quality, no Rewind function&lt;/p&gt;&lt;p&gt;Web: https://www.amazon.com/Jensen-Scr-68C-Stereo-Cassette-Player/dp/B012M1ZL8K&lt;/p&gt;&lt;p&gt;It supports Bluetooth v5.4 , which provides high communication quality and low power consumption. Brass flywheel adopted. Reduces rotational irregularities and provides high quality sound. Bultin battery, accumulator. Playback time is around 9 hours. Weight 210g.&lt;/p&gt;&lt;p&gt;(-) Battery not removable. 2 hours to fully charge.&lt;/p&gt;&lt;p&gt;Web: https://www.maxell.jp/consumer/mxcp-p100.html&lt;/p&gt;&lt;p&gt;Affordable modern portable cassette tape player &amp;amp; recorder with 2 track, stereo playback. Good sound quality, plays all (I-IV) cassettes. Frequency response : 40Hz-11KHz (Type I), Signal-to-noise ratio 50dB, Distorsion 1%, Wow &amp;amp; Flutter 0.3%, Headphone output power: 2x2 mW into 32 ohms&lt;/p&gt;&lt;p&gt;(-) -&lt;/p&gt;&lt;p&gt;Web: https://store.recordingthemasters.com/products/b-1000ew-walkman&lt;/p&gt;&lt;head rend="h5"&gt;TOMASHI&lt;/head&gt;20$&lt;p&gt;Entry level portable cassette player. F116/F113&lt;/p&gt;&lt;p&gt;(-) Speaker too quiet, mono&lt;/p&gt;&lt;p&gt;Web: https://www.amazon.com/TOMASHI-Portable-Cassette-Microphone-Headphone/dp/B09XQCLTYF&lt;/p&gt;&lt;head rend="h5"&gt;We Are Rewind&lt;/head&gt;160$&lt;p&gt;As you will have understood, this cassette player is the best of the best! The "crème de la crème" as they say in French. An object that is both cult and essential for any self-respecting music lover.&lt;/p&gt;&lt;p&gt;(-) Expensive price point&lt;/p&gt;&lt;p&gt;Web: https://www.wearerewind.com&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://walkman.land/modern"/><published>2025-12-09T04:57:53+00:00</published></entry></feed>