<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><id>hnrss.org/frontpage</id><title>Hacker News: Front Page</title><updated>2025-10-20T16:44:00.394434+00:00</updated><link href="https://news.ycombinator.com/" rel="alternate"/><link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator><subtitle>Hacker News RSS</subtitle><entry><id>https://news.ycombinator.com/item?id=45639860</id><title>Introduction to reverse-engineering vintage synth firmware</title><updated>2025-10-20T16:44:14.841345+00:00</updated><content>&lt;doc fingerprint="a243d812e8f7a388"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Introduction to Reverse-Engineering Vintage Synth Firmware&lt;/head&gt;&lt;p&gt;In this article we're going to take a look at how to reverse-engineer vintage synthesiser firmware. The synthesiser I've chosen for us to look at is the Yamaha DX7 (See Appendix: Why Choose This Synth?). You don't need a DX7 to follow along at home, but you will need a copy of the DX7 V1.8 firmware (available here) and the Ghidra disassembler.&lt;/p&gt;&lt;p&gt;Who is this article for? This article's intended audience is people from a technical background who are new to reverse-engineering, 8-bit architectures, or embedded development. If you come from an electrical-engineering or embedded-software background, you'll probably find the content here a little basic.&lt;/p&gt;&lt;p&gt;You'll only need to know a little bit about low-level programming: A basic understanding of how binary and pointers work should be enough. You don't need to know assembly language, or understand any specific processor architecture.&lt;/p&gt;&lt;p&gt;A few years ago I decided to give myself a crash course on what goes on inside synthesisers. I ended up writing the article Yamaha DX7 Technical Analysis about what I'd learned. In order to tease out some more details about the DX7's inner-workings, I decided to disassemble the synth's firmware ROM. I didn't have any experience with reverse-engineering binaries, so I had to figure it out as I went. I'm still by no means an expert (if you see any mistakes in this article, please let me know!), but I'd like to share what I've learned.&lt;/p&gt;&lt;p&gt;All I had when I started was a copy of the firmware, a copy of the service manual, and a can-do attitude. I knew nothing about 8-bit systems, and absolutely nothing about electronics, but I was willing to give anything a shot. If this sounds like you, read on, and I hope you find this article helpful!&lt;/p&gt;&lt;p&gt;Reverse-engineering vintage synthesisers is a great introduction to embedded systems, and can be a lot of fun. In a lot of ways reverse-engineering is a bit like putting together a big jigsaw puzzle. Sometimes putting a new piece in place unlocks a lot of new progress, and like a jigsaw puzzle, the best place to start is at the edges.&lt;/p&gt;&lt;head rend="h2"&gt;Address Decoding #&lt;/head&gt;&lt;p&gt;The peripheral devices attached to the DX7's CPU, such as its LCD screen and sound chips, are memory-mapped. This means that the device has been allocated a specific address range in the system's memory, and the system communicates with the device by reading and writing data from and to these addresses.&lt;/p&gt;&lt;p&gt;Before we can start disassembling the firmware ROM, we need to know what peripheral device is mapped where. To do that we'll need to look at the DX7's address decoding logic. The first place to start is with the schematics.&lt;/p&gt;&lt;p&gt;The best version of the schematics I've seen is this version, created by the yamahamusicians.com user Miks. While you're at it, grab a copy of the service manual too. We won't be referencing it in this article, but it's a good resource to have. It explains certain details about the synth's architecture that aren't obvious from the schematics.&lt;/p&gt;&lt;p&gt;If you're new to electronics, device schematics can look very intimidating, but once you understand the basics they're not actually as scary as they look! You can find a good introductory guide to schematics here.&lt;/p&gt;&lt;head rend="h3"&gt;Background&lt;/head&gt;&lt;p&gt;But first, what does address decoding actually mean? Address decoding refers to how a specific device is mapped to a specific address. In this section we'll figure out what peripheral is mapped to what address by tracing the address decoding logic in the synth's schematics.&lt;/p&gt;&lt;p&gt;The total amount of memory addresses that a CPU can access is referred to as the CPU's 'address space'. This is limited by the width of its 'address bus'. The CPU's address bus is responsible for selecting addresses in attached memory devices, such as RAM, or peripheral devices with addressable registers. Each line in the address bus represents a single bit, with the total number of lines determining the address range the CPU can access. For example, a 16-bit address bus can address 216 unique memory locations, or 64KiB.&lt;/p&gt;&lt;p&gt;When a CPU's address lines are exposed externally in the form of pins on the chip's package, this is called an external address bus1. These lines can be physically connected to external memory devices. Together with the CPU's data bus, this allow reading and writing binary data back and forth.&lt;/p&gt;&lt;p&gt; When the CPU performs an instruction that reads or writes memory, like &lt;code&gt;LDB 0x2001&lt;/code&gt;, several things happen:
        &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; The CPU's external address pins are set to high and low logic levels according to the specified address. For address 0x2001 (&lt;code&gt;0b0010_0000_0000_0001&lt;/code&gt;), address pins 0 and 13 will be high, and all the others will be low.&lt;/item&gt;&lt;item&gt; The CPU's &lt;code&gt;RW&lt;/code&gt;pin will be set high to indicate that this is a read operation, and...&lt;/item&gt;&lt;item&gt;The CPU will prepare to accept the incoming data at 0x2001 over the data bus into the B register.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But wait... If the CPU only has one set of address and data bus lines, how do you connect multiple memory devices to the CPU? This is where the 'Chip Select' interface comes in: Each device attached to the CPU's data/address buses has a 'Chip Select' pin, controlling whether the device responds to incoming signals.&lt;/p&gt;&lt;p&gt; Consider the above (incredibly simplified) diagram: Two 8KiB 6264 RAM chips (U3 and U4) are connected to shared address and data buses on a Z80 CPU. U3's &lt;code&gt;CE1&lt;/code&gt; (Chip Enable)
          pin is connected to the CPU's A13 pin. The bar over the top of the
          label indicates that this pin is 'Active Low', meaning a low
          logic level will 'activate' its function. When the CPU selects an
          address between 0x0 and
          0x1FFF, the A13 pin will be low,
          activating the U3 chip. U4's &lt;code&gt;CE1&lt;/code&gt; pin
          is attached to the CPU's A13 pin via a &lt;code&gt;NOT&lt;/code&gt; gate, which
          inverts the signal coming from A13. When an address above
          0x1FFF is selected, A13 will be set
          high, selecting the U4 chip. This effectively maps U3 to the first
          8KiB of the system's memory, and U4 to the next.
        &lt;/p&gt;&lt;p&gt;Can you spot the problem with this example? Since any address using A13 will 'select' U4, U4 is now mapped to every 8KiB block of memory above 0x1FFF. In reality, more sophisticated logic is used to map memory devices. Let's examine the real world example of the DX7's address decoding circuitry.&lt;/p&gt;&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, and
          &lt;code&gt;NOT&lt;/code&gt; gates do is all you need. One particular type of
          component you'll encounter a lot inside vintage synthesisers are
          7400-series logic chips.
        &lt;head rend="h2"&gt;Decoding the DX7's Memory Map #&lt;/head&gt;&lt;p&gt;Nearly all of the discrete electrical components that make up a DX7 are commonly available products. They're mass-manufactured and sold by a variety of different manufacturers. The best way to understand these components is to read the datasheets made available by the manufacturer. I'll provide links to these as we go.&lt;/p&gt;&lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; gates, rather
          than the more standard
          
            ANSI, or IEC notations [pdf]. Other gates use the ANSI notation.
        &lt;head rend="h3"&gt;The Firmware ROM #&lt;/head&gt;&lt;p&gt;Let's start by taking a look at the firmware ROM, IC14.&lt;/p&gt;&lt;p&gt; In the schematic we can see that IC14's &lt;code&gt;CE1&lt;/code&gt; pin is connected to the CPU's
          &lt;code&gt;A14/A15&lt;/code&gt; lines via an &lt;code&gt;AND&lt;/code&gt; gate, and a
          &lt;code&gt;NOT&lt;/code&gt; gate. What's going on here?
        &lt;/p&gt;&lt;p&gt; The &lt;code&gt;AND&lt;/code&gt; gate ensures that the signal is only high when
          both address lines are active, and the &lt;code&gt;NOT&lt;/code&gt; gate
          inverts the signal so that it activates the active-low
          &lt;code&gt;CE1&lt;/code&gt; pin. If &lt;code&gt;A14&lt;/code&gt; and
          &lt;code&gt;A15&lt;/code&gt; being active on the CPU 'selects' the ROM chip, that
          means it's mapped to the address range
          0xC000 - 0xFFFF2.
        &lt;/p&gt;&lt;p&gt;Awesome! That wasn't so hard. Now we know where the ROM is mapped in memory. What's next?&lt;/p&gt;&lt;head rend="h3"&gt;RAM #&lt;/head&gt;&lt;p&gt;The address decoding logic for the RAM is a little more complicated.&lt;/p&gt;&lt;p&gt; The DX7 features three 4KiB 5118P RAM chips (IC19, 20, 21). These are connected to the CPU's address bus via a 74LS138 demultiplexer (IC23). This demultiplexing circuit is used to select one of 8 individual output lines based on a 3-bit input signal. These output lines are labeled as &lt;code&gt;Y0&lt;/code&gt; -
          &lt;code&gt;Y7&lt;/code&gt;, and the input lines as &lt;code&gt;DA&lt;/code&gt;, &lt;code&gt;DB&lt;/code&gt;, and &lt;code&gt;DC&lt;/code&gt;. The &lt;code&gt;OR&lt;/code&gt; gates used here are wired to the system clock
          output pin. Presumably to ensure the timing of read and write
          operations are valid.
        &lt;/p&gt;&lt;p&gt; The first RAM chip (IC19)'s chip select terminal is connected to the demultiplexer's &lt;code&gt;Y2&lt;/code&gt; line. The 'Function
          Table' from the 74LS138P datasheet shows that
          &lt;code&gt;Y2&lt;/code&gt; will be set low when
          input &lt;code&gt;DB&lt;/code&gt; (connected to the CPU's
          &lt;code&gt;A12&lt;/code&gt;) is high. Therefore, when the CPU selects address
          0x1000, the first RAM chip will be
          selected.
        &lt;/p&gt;&lt;p&gt;&lt;code&gt;Y3&lt;/code&gt;
          (connected to the second RAM chip, IC20) will be set low when inputs
          &lt;code&gt;DA&lt;/code&gt; and &lt;code&gt;DB&lt;/code&gt; (&lt;code&gt;A11&lt;/code&gt;
          and &lt;code&gt;A12&lt;/code&gt;) are high, corresponding to an address of
          0x1800. Likewise,
          &lt;code&gt;Y4&lt;/code&gt;
          (connected to IC21) corresponds to an address of
          0x2000.
        &lt;/p&gt;&lt;p&gt;By tracing this address decoding logic, we've successfully mapped the synth's RAM to 0x1000 - 0x2800.&lt;/p&gt;&lt;head rend="h3"&gt;LCD Screen #&lt;/head&gt;&lt;p&gt;The last peripheral we're going to look at right now is the synth's LCD screen. When you take your first peek inside a binary you'll be staring at an intimidating jumble of machine code. One of the few things that will stand out at a glance is ASCII strings. A good way to get a quick overview of the binary is finding out how these strings are printed to the screen, and where.&lt;/p&gt;&lt;p&gt;The best place to start doing that is understanding how the CPU interfaces with the LCD controller, and working your way backwards to the code responsible for sending string data to it. Once you've found how strings are printed to the screen, you can easily see what's printed where to get a better understanding of the code.&lt;/p&gt;&lt;p&gt;The LCD address mapping logic might look really complicated, but don't worry though. It's just more of the same logic as before.&lt;/p&gt;&lt;p&gt; IC23's &lt;code&gt;Y5&lt;/code&gt; pin is connected
          to IC24, another 74LS138 demultiplexer. From the 74LS138 function
          table we know that &lt;code&gt;Y5&lt;/code&gt; goes
          low when inputs A, and C (&lt;code&gt;A11&lt;/code&gt; and &lt;code&gt;A13&lt;/code&gt;) are
          high. So it looks like IC24 is mapped to
          0x2800.
        &lt;/p&gt;&lt;p&gt; Take a look at IC24: Inputs A, B and C are wired to &lt;code&gt;A1&lt;/code&gt;, &lt;code&gt;A2&lt;/code&gt; and &lt;code&gt;A3&lt;/code&gt;. That means that
          IC24 only maps 8 bytes.
        &lt;/p&gt;&lt;p&gt; IC24's &lt;code&gt;Y0&lt;/code&gt; and
          &lt;code&gt;Y1&lt;/code&gt; pins are connected to an
          &lt;code&gt;AND&lt;/code&gt; gate connected to the 'chip select' pin of IC12.
          What's happening here? This might seem a little confusing at first,
          but since the 74LS138P's outputs are active-low, this makes
          &lt;code&gt;LCDCS&lt;/code&gt; active when either
          &lt;code&gt;Y0&lt;/code&gt; or
          &lt;code&gt;Y1&lt;/code&gt; are active. This maps
          IC12 to the four-byte range
          0x2800 - 0x2803. Awesome. But what's
          IC12 doing?
        &lt;/p&gt;&lt;p&gt;IC12 is an Intel 8255 Programmable Peripheral Interface (PPI). It provides 24 parallel, bidirectional IO lines3.&lt;/p&gt;&lt;p&gt; The schematics show the LCD's parallel interface (&lt;code&gt;DB0 - DB7&lt;/code&gt;) is connected to the PPI's port A (&lt;code&gt;PA0 - PA7&lt;/code&gt;), and its
          control pins (&lt;code&gt;E, RW&lt;/code&gt; and &lt;code&gt;RS&lt;/code&gt;) to the PPI's
          port B (&lt;code&gt;PB0 - PB2&lt;/code&gt;).
        &lt;/p&gt;&lt;p&gt; The Hitachi LM016 LCD screen used in the DX7 features the ubiquitous Hitachi HD44780 LCD Controller. According to its datasheet (available here) it has two registers. When its &lt;code&gt;RS&lt;/code&gt; line (connected to
          the PPI's &lt;code&gt;PB0&lt;/code&gt;) is low, the instruction register is
          selected. When high, the data register is selected.
        &lt;/p&gt;&lt;p&gt;Based on the HD44780 datasheet, and the above table from the 8255's datasheet, we can tell that the LCD's data register must be mapped to 0x2800, and its control register to 0x2801. We'll go into more detail about the LCD controller itself later in the article.&lt;/p&gt;&lt;head rend="h3"&gt;Wrapping Up #&lt;/head&gt;&lt;p&gt;Now we've got a pretty good idea of what's going on where in the memory map, and how this is discovered. To save you the trouble of going through the whole schematic, here are all the memory-mapped peripheral addresses.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Address Range&lt;/cell&gt;&lt;cell role="head"&gt;Peripheral&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x1000 - 0x2800&lt;/cell&gt;&lt;cell&gt;RAM (External)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2800&lt;/cell&gt;&lt;cell&gt;LCD Data&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2801&lt;/cell&gt;&lt;cell&gt;LCD Control&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2802&lt;/cell&gt;&lt;cell&gt;Sustain/Portamento Pedals, and LCD Busy Line&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2803&lt;/cell&gt;&lt;cell&gt;8255 Peripheral Controller Control Register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2804&lt;/cell&gt;&lt;cell&gt;OPS Mode register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x2805&lt;/cell&gt;&lt;cell&gt;OPS Algorithm/Feedback register&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280A&lt;/cell&gt;&lt;cell&gt;DAC Volume&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280E&lt;/cell&gt;&lt;cell&gt;LED1&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x280F&lt;/cell&gt;&lt;cell&gt;LED2&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x3000 - 0x4000&lt;/cell&gt;&lt;cell&gt;EGS&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0x4000 - 0x5000&lt;/cell&gt;&lt;cell&gt;Cartridge Interface&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0xC000 - 0xFFFF&lt;/cell&gt;&lt;cell&gt;ROM&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;These aren't the only peripherals attached to the system, the Hitachi 6303 CPU also features 'IO Ports'. These are memory-mapped input/output lines with their own dedicated functionality. We'll touch on these later in the article.&lt;/p&gt;&lt;head rend="h2"&gt;Disassembling The Firmware #&lt;/head&gt;&lt;p&gt;Now that we know the memory map, we can start disassembling the firmware. To do this we'll use a graphical disassembler called Ghidra4. It's a relatively new player on the scene, but it's free, open source, and very powerful. A great resource to keep handy while working with Ghidra is the Ghidra Cheat Sheet.&lt;/p&gt;&lt;code&gt;6303&lt;/code&gt; directory to the
          &lt;code&gt;Ghidra/Processors&lt;/code&gt; directory inside your Ghidra
          installation. You'll need to restart Ghidra to see the new processor
          in the list.
        &lt;p&gt;Once you've installed the language definitions, open Ghidra and create a new project. The first thing we're going to need to do is to import the firmware ROM binary. Select the '6303' language, and click 'OK'.&lt;/p&gt;&lt;p&gt;Next, open up the Codebrowser. This is where all the action happens.&lt;/p&gt;&lt;p&gt;Once the initial disassembly loads, the first thing you'll be looking at is row after row of hexadecimal. This is the actual machine code as it would appear to the CPU. Don't bother with analyzing the file.&lt;/p&gt;&lt;p&gt;The first thing we're going to do is set up the memory map. Remember that thing we just did all that hard work figuring out? That's going to come in handy now. Press the 'Display Memory Map' icon in the top button bar, to open up the Memory Map dialog. By default there's only one memory block defined. This block consists of the binary we just imported, so go ahead and rename it to 'ROM'. The next thing we need to do is move this block to the correct offset 0xC000. Because all of the machine code instructions reference memory with absolute addresses, if we didn't map the ROM to the correct location none of the disassembly would work.&lt;/p&gt;&lt;p&gt;Before we finish setting up the memory map, let's take a quick look at the code. When the Hitachi 6303 processor in the DX7 powers up, it knows where to begin executing code by fetching a pointer from a specific location in the interrupt vector table.&lt;/p&gt;&lt;p&gt;In this case the 'Reset vector' is always located at the specific memory address 0xFFFE, right at the end of the address space. Press the Ctrl+End combination on your keyboard to move to the end of the binary, and select the offset 0xFFFE by clicking on it. Press the P key on your keyboard to convert the data at this address to a pointer. You should see something similar to the image below.&lt;/p&gt;&lt;p&gt;Double-click on this pointer to take you to the associated offset in the binary. Now we've found where the actual code is located, but it doesn't look like much just yet.&lt;/p&gt;&lt;p&gt;To begin disassembling the machine code into something we can work with, click on the label and press the D key on your keyboard, or right-click and select 'Disassemble' in the context menu.&lt;/p&gt;&lt;p&gt;The disassembly process will follow the flow of code through the binary, disassembling as it goes. An error will pop up here, but don't worry about it for now. This is just the disassembler mistaking a jump table for code.&lt;/p&gt;&lt;p&gt;Once the disassembly completes you should see something like the picture below.&lt;/p&gt;&lt;p&gt;Now we're looking at real code! No need to panic though. If you don't understand what you're looking at, that's okay. Assembly can look pretty intimidating at first, but with a little bit of practice you'll get the hang of it!&lt;/p&gt;&lt;p&gt;Each of the lines you're seeing here represents a single machine-code instruction translated into assembly code. The three letter mnemonics are the human-readable representation of the instructions. LDA for example, is the mnemonic for the 'Load value into register A' instruction. STA is the mnemonic for the 'Store value in register A' instruction. If you've never encountered assembly language before, that's okay! This video will give a very quick and general introduction to assembly language.&lt;/p&gt;&lt;p&gt;The HD63B03RP CPU used in the DX7 is a member of the 6800 family of processors. Its instruction set (the full set of assembly instructions) is small and easy to understand. A great resource for understanding the 6303 CPU and its instruction set is the HD6301/HD6303 Series Handbook freely available on bitsavers.org.&lt;/p&gt;&lt;p&gt;The FUN_c5e5 text you're seeing here is a label. This is a symbol placed in the disassembler's symbol table, which can be referenced elsewhere in the assembly code, usually as the target for a branching instruction. Ghidra should already have set up the reset vector as a 'function'. Select this label with your cursor and press the F key on your keyboard to edit the function and give it a more meaningful label like reset.&lt;/p&gt;&lt;p&gt;But what are all these red labels we're seeing, like DAT_2575? If you try to double click on it, Ghidra offers a helpful error message: 'Address not found in program memory: 2575'. This is because we're missing our memory map! Let's go back to the 'Memory Map' dialog, and add the missing blocks.&lt;/p&gt;&lt;p&gt;Fill in the memory map that we worked out in the last section. The completed map should look something like the screenshot below. You can choose to consolidate some of these blocks if you like. It's not super important how the blocks are divided. What matters is that the blocks cover all of the needed peripheral addresses. Note that I added memory blocks for the HD6303 CPU's internal registers, and internal RAM.&lt;/p&gt;&lt;p&gt;Now is a good time to go and fill in the individual peripheral addresses that we know. The HD6301/HD6303 Series Handbook provides a list of the HD6303RP's internal registers.&lt;/p&gt;&lt;p&gt; Press the Ctrl+Home keys on your keyboard to go to address 0x0. Press the B key on your keyboard to declare that address 0x0 specifies a byte of data. You'll see the &lt;code&gt;??&lt;/code&gt; change to &lt;code&gt;db&lt;/code&gt;, which is the
          assembler directive to define a byte of memory. Press the
          L
          key on your keyboard to give this address a useful label like
          io_port_1_dir. Go ahead and fill
          in the rest of the peripherals. When you go back to the reset handler
          you'll notice that, even with only a few pieces of the jigsaw puzzle
          in place, things will start to make a lot more sense.
        &lt;/p&gt;&lt;head rend="h3"&gt;The Reset Function #&lt;/head&gt;&lt;p&gt;The main reset handler in the DX7 is responsible for initialising the firmware. It sets up the CPU's IO ports, ensures the firmware's important variables have valid values, and sets up the CPU's timer interrupt. More on this later.&lt;/p&gt;&lt;code&gt;D == (A &amp;lt;&amp;lt; 8) | B&lt;/code&gt;.
        &lt;p&gt;A great way to visualise the 'control flow' of the program is in the 'Function Graph' view. This view shows a directed graph of the program's branching logic. You can open this view by selecting Window → Function Graph in the top window menu. You should see a view similar to the picture below. At offset 0xC605 you'll see the following instructions:&lt;/p&gt;&lt;quote&gt;LDA #0xd CMPA DAT_2328 BHI LAB_c60f&lt;/quote&gt;&lt;list rend="ol"&gt;&lt;item&gt; The LDA instruction loads the immediate value &lt;code&gt;0xD&lt;/code&gt;into the A register.&lt;/item&gt;&lt;item&gt;The CMPA instruction then compares the value in the A register with the value at the memory address DAT_2328.&lt;/item&gt;&lt;item&gt;The BHI instruction tells the CPU to branch to the label LAB_c60f if the value in the A register is greater than the value at DAT_2328.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can see in the function graph that if the memory at DAT_2328 is greater than or equal to '13', it will not branch, and the value will be cleared. The program will then continue to execute the next instruction, which would have been the original branch target. In this case, the program is checking to see that the 'pitch bend range' variable (stored in memory at location 0x2328) is within a valid range of 0-12. If not, it's reset to 0.&lt;/p&gt;&lt;p&gt;If you look down at the bottom of the graph, you'll notice something interesting: The program goes into an infinite loop. This is the firmware's 'main loop'. Tasks that need to be performed continuously happen here. Such as updating the UI based on user input, and parsing incoming MIDI messages.&lt;/p&gt;&lt;p&gt;When certain tasks not only need to be performed continuously, but also periodically, there's another way to make this happen: interrupts.&lt;/p&gt;&lt;head rend="h3"&gt;Interrupts #&lt;/head&gt;&lt;p&gt;Interrupts are signals sent to the processor by hardware or software to interrupt the current code being executed, and handle a specific event. They're commonly used in embedded-software to handle external, time-critical, or asynchronous events.&lt;/p&gt;&lt;p&gt;One of the most common types of interrupt you'll encounter is a 'timer interrupt'. The HD6303's built-in timer interrupt consists of a 16-bit 'counter' register, which is incremented every clock cycle, and a 16-bit 'output compare' register. When the value in the counter register matches the value in the output compare register, a timer interrupt will be raised. This causes the processor to halt what it was doing, push the current state of the CPU onto the stack, and jump to the appropriate interrupt handler specified in the interrupt vector table. In the 6303 a pointer to this handler is located at offset 0xFFF4. Once the firmware is done handling the interrupt, it executes the RTI instruction, which restores the CPU's state from the stack and continues executing the code from where it left off.&lt;/p&gt;&lt;p&gt;The timer interrupt handler is where all the synth's real-time functionality happens. This is any code that needs to be executed in a time-critical manner. The DX7 uses the periodic timer interrupt to process portamento and modulation, update the individual voice frequencies, and send the updated voice data to the sound chips. Feel free to declare the pointer to the timer interrupt handler just like we did for the reset handler, disassemble the handler, and take a look at what's going on.&lt;/p&gt;&lt;head rend="h3"&gt;LCD Interface #&lt;/head&gt;&lt;p&gt;One of the best places to start reverse-engineering a synth's firmware is to understand how it prints things to the LCD screen. We already know where the LCD controller is mapped in memory, let's work backwards from there and see if we can find that code.&lt;/p&gt;&lt;p&gt;Press the G key on your keyboard to open the 'Go To...' dialog, and go to address 0x2800. These are the two memory-mapped LCD registers. The list of cross-references on the right shows us where these addresses are referenced in the code. Click on the FUN_fdef label to take us to this function. This is the function called by the reset handler to initialise the LCD screen.&lt;/p&gt;&lt;p&gt;Below the function we can see something that looks like ASCII data. In fact, it looks a lot like the welcome message displayed when you boot up the DX7. Hmm. Click on offset 0xFE31, and press the ' key on the keyboard twice. Once to convert the data at this offset to character data, twice to convert it to a NULL-terminated string.&lt;/p&gt;&lt;p&gt;Notice that the welcome message location is referenced in the code at offset 0xFE2B:&lt;/p&gt;&lt;quote&gt;JSR FUN_fe52 LDX #0xfe31 JMP FUN_fea4&lt;/quote&gt;&lt;p&gt; Select the operand &lt;code&gt;#0xFe31&lt;/code&gt;, and press
          
            Alt+Ctrl+R
          
          on your keyboard to turn this into a
          memory reference. The default label looks a bit strange, so you
          might want to give it a better one like
          str_welcome_message
          by selecting the reference and pressing the
          L key.
        &lt;/p&gt;&lt;p&gt;We can see here that a pointer to the welcome message string is loaded into the X register, and then the ROM jumps to the function FUN_fea4. Could this function have something to do with printing the string? Let's find out.&lt;/p&gt;&lt;code&gt;LDA 4,x&lt;/code&gt; instruction will
          load the byte into A that is 4
          bytes from the address stored in X.
          This is useful because it allows us to reference 16-bit addresses with
          only an 8-bit operand.
        &lt;p&gt;Let's take a walk through FUN_fea4 together and see if we can figure out what it's doing:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;First, it pushes the address of the welcome message string in X to the stack.&lt;/item&gt;&lt;item&gt;Then it loads a memory address (0x261F) into X, and saves that address to a pointer in memory.&lt;/item&gt;&lt;item&gt;Then it restores the welcome message address from the stack into X.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Not very helpful yet, unfortunately. Something I find that helps make sense of so many unknown labels is to give them names that describe how they're used. Double-click on the label DAT_00fb to go to its location. Since we know this variable stores memory addresses, press the P key to convert it to a pointer. Giving it a name like unknown_lcd_pointer_00fb can help identify it at a glance later.&lt;/p&gt;&lt;p&gt;Use the Alt+← keyboard combination to navigate back to where we were before. Once you're there, click through to FUN_fe8b. We can see that lots of cross-references to this function have been found in the code already. Let's go through this function step by step and see what we can figure out:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;We already know that the X register contains a pointer to a string. So we can see that this function is loading an ASCII character into register B from the address stored in X.&lt;/item&gt;&lt;item&gt; When the LDB instruction loads a new value into B it sets the condition codes according to its value. If the most-significant bit of this byte is 1, the N(egative) condition code will be set. This will cause the BMI (Branch If MInus) instruction to branch. Valid ASCII values fall within the range 0-127, so this code looks like it's checking for an invalid character, and will branch to the exit if this is the case.&lt;lb/&gt;Note: Different instructions treat integer values as either signed, or unsigned, with the most-significant bit treated as the sign bit.&lt;/item&gt;&lt;item&gt; The value in B is then compared against &lt;code&gt;0x20&lt;/code&gt;(ASCII space). As I mentioned earlier, the CMP instruction sets condition codes according to the value in the associated accumulator, and the operand. The BCC instruction (Branch If Carry Clear) will branch if the C(arry) condition code is clear. This means that the value in B must be&lt;code&gt;0x20&lt;/code&gt;or above, otherwise the function exits.&lt;lb/&gt;You can read more about how the carry flag is used in computer arithmetic on Wikipedia.&lt;/item&gt;&lt;item&gt;If the ASCII char is valid, it calls BSR to branch to the subroutine FUN_fe9a. In this subroutine we can immediately see something interesting: Remember that pointer we labeled earlier? This subroutine writes the ASCII character in B to the location in this pointer, increments the pointer, and saves it...&lt;/item&gt;&lt;item&gt;After this, the address in X is incremented, and the function loops back to the start. Now the function repeats, with X pointing to the next character in the string.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Are you thinking what I'm thinking? This is a String Copy function! It copies characters from a string into a buffer, until either a NULL-terminator, or other unprintable ASCII character is encountered.&lt;/p&gt;&lt;p&gt;Go ahead and give this function a label like lcd_strcpy. If you like, you can apply local labels to LAB_fe94, and LAB_fe99 like .copy_character and .exit. Maybe give that buffer address we saw earlier (0x261F) a temporary label too.&lt;/p&gt;&lt;p&gt;This is where we're at so far:&lt;/p&gt;&lt;p&gt;Let's move on to that last function FUN_fe52 and see where that leads us. This function is a bit more complicated. Using the Function Graph window I showed you before might help visualise what's going on. Let's go through this function step-by-step like we did before:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;A new location in memory (0x263F) is being saved to that pointer we saw before, and the LCD buffer address we saw earlier (0x261F) is being saved to a new pointer. There's something interesting about those addresses. They're 32 bytes apart. That seems a bit conspicuous, doesn't it? Maybe this corresponds to the length of the LCD screen (2 lines of 16 characters)?&lt;/item&gt;&lt;item&gt;A constant value is loaded into B.&lt;/item&gt;&lt;item&gt;Inside the loop, we can see that B is saved to the stack. A byte is then loaded into A from the location in the pointer at 0xF9. We know from seeing the welcome message string loaded into X that this byte is ASCII string data. The pointer is then incremented and saved.&lt;/item&gt;&lt;item&gt;This byte is then compared against the byte pointed to by unknown_lcd_pointer_00fb.&lt;/item&gt;&lt;item&gt;If the character in unknown_lcd_pointer_00f9 and unknown_lcd_pointer_00fb aren't equal, then this character is used as an argument for a function call to FUN_fec7.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The function at FUN_fec7 is a bit more complicated, so I'll walk you through what's happening.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt; The constant &lt;code&gt;0x89&lt;/code&gt;is written to the 8255 PPI control register at 0x2803. The PPI datasheet refers to this as 'Control Word #5'. This sets the PPI's Port A, and B to outputs, allowing the CPU to send data to the LCD controller.&lt;/item&gt;&lt;item&gt; A value of zero is written to the LCD control register. This sets the &lt;code&gt;RS&lt;/code&gt;line low to select the Instruction Register, and the&lt;code&gt;RW&lt;/code&gt;line low to select a Write operation.&lt;/item&gt;&lt;item&gt; The &lt;code&gt;E&lt;/code&gt;line of the LCD is then driven high to instruct it to be ready to receive data over the data bus.&lt;/item&gt;&lt;item&gt; The byte in A is then written to the LCD instruction register. After this, the &lt;code&gt;E&lt;/code&gt;line is driven low, and the&lt;code&gt;RW&lt;/code&gt;line is driven high to signal the end of the data transfer.&lt;/item&gt;&lt;item&gt;The 8255 'Control Word #13' is written to the PPI control register to revert port A and C to being inputs.&lt;/item&gt;&lt;item&gt;Finally, it branches unconditionally to FUN_ff08.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Let's check out the subroutine at FUN_ff08 that our function jumps to.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt; The &lt;code&gt;E&lt;/code&gt;and&lt;code&gt;RW&lt;/code&gt;lines of the LCD controller are set high. This sets the LCD controller to read mode.&lt;/item&gt;&lt;item&gt; The PPI's port C is read into the A register, then the &lt;code&gt;E&lt;/code&gt;line of the LCD controller is set low to indicate the read operation is complete.&lt;/item&gt;&lt;item&gt; A bitwise &lt;code&gt;AND&lt;/code&gt;is performed between value of the A register and&lt;code&gt;0b1000_0000&lt;/code&gt;. This checks the status of the&lt;code&gt;PC7&lt;/code&gt;line. If the&lt;code&gt;PC7&lt;/code&gt;line is high, the function loops back to the start.&lt;/item&gt;&lt;/list&gt;&lt;p&gt; It's easy to miss, but if you look closely in the schematics you'll see that the PPI's &lt;code&gt;PC7&lt;/code&gt; line is connected to
          &lt;code&gt;PA7&lt;/code&gt;, which is connected to the LCD controller's DB7 pin.
          
            
          
          The DB7 pin serves as the LCD controller's 'Busy Flag'. This
          flag indicates whether the LCD controller is busy processing data.
          When it's clear, the LCD controller is ready to accept new data.
        &lt;/p&gt;&lt;p&gt;It looks like the purpose of this function is to poll the LCD controller, waiting for it to be ready to accept new data. Awesome! Let's give it a label like lcd_wait_ready. Okay! So putting it all together, the function at 0xFEC7 writes an instruction to the LCD controller, and then waits for it to be ready to receive data again. Go ahead and give it a name like lcd_write_instruction.&lt;/p&gt;&lt;p&gt;Reverse-engineering often involves going down a rabbit hole. Sometimes you need to fill in a few different pieces of the puzzle before you can start to see the whole picture. Let's return to the function at FUN_fe52 and see what happens next.&lt;/p&gt;&lt;p&gt; We now know the loop is writing an instruction to the LCD controller, but what did this instruction do? The original instruction value when the function started was &lt;code&gt;0x80&lt;/code&gt;, and it's incremented by
          one with each iteration of the loop. The HD44780 datasheet tells us
          that &lt;code&gt;0x80&lt;/code&gt; is the instruction to set the DDRAM (Display
          Data RAM) address in the LCD controller. This is the address in the
          LCD's memory where the next character will be written. A value of
          &lt;code&gt;0x80&lt;/code&gt; indicates the start of the screen's first line.
        &lt;/p&gt;&lt;p&gt;The next function call (FUN_fee7) looks almost identical to our lcd_write_instruction function. The only difference is that it writes to the LCD controller's data register, rather than the instruction register. This must be where the actual character data is written! You can give this function a label like lcd_write_data. Note that this function 'falls-through' to the LCD controller polling function we saw earlier.&lt;/p&gt;&lt;p&gt;Now we know what's going on here. This is our LCD printing function! Notice that after writing the character data to the LCD, at offset 0xFE77 the function writes it to the buffer at 0x263F? The incoming characters are compared against the contents of this buffer to see if they're identical, if they are then it skips printing the character. Maybe these buffers represent the 'next', and 'current' contents of the LCD screen?&lt;/p&gt;&lt;p&gt; After writing the LCD character data, the function then checks whether the LCD instruction byte is equal to &lt;code&gt;0xD0&lt;/code&gt;. Now we know
          that this is checking whether the LCD DDRAM position is at the end of
          the second line. If not, it checks whether we're at the end of the
          first line (&lt;code&gt;0x90&lt;/code&gt;). If so, the instruction byte is set to
          &lt;code&gt;0xC0&lt;/code&gt;, which sets the DDRAM address to the start of the
          second line.
        &lt;/p&gt;&lt;code&gt;0xC0&lt;/code&gt; (&lt;code&gt;0x80 + 0x40&lt;/code&gt;) is the correct
          DDRAM address for the start of the second line.
        &lt;p&gt;Awesome! Now we've discovered the LCD printing function! Go ahead and give it a name like lcd_print.&lt;/p&gt;&lt;p&gt;If you've followed along, give yourself a huge pat on the back. This was no easy feat! You've now got a pretty good understanding of how vintage synth binaries are reverse-engineered. Everything else involved in disassembling a synth's firmware is just a matter of applying these same ideas.&lt;/p&gt;&lt;head rend="h2"&gt;Going Further #&lt;/head&gt;&lt;head rend="h3"&gt;The MIDI Handling Routine #&lt;/head&gt;&lt;p&gt;After disassembling the LCD printing function, the next best way to figure out what's going on inside a synth ROM is to disassemble the function that parses incoming MIDI data. This function is an entry point to nearly every aspect of a synth's functionality. Disassembling it will allow you to trace the path of a particular MIDI message to its associated functionality. You can trace 'NOTE ON' and 'NOTE OFF' messages to find the code that handles starting and stopping individual voices; Or you can trace 'CONTROL CHANGE' messages to find the code that handles pitch bend or modulation.&lt;/p&gt;&lt;p&gt;I decided not to tackle this function in this article, as the DX7's MIDI parsing code is huge, and requires a lot of explanation. Parsing MIDI messages is always implemented via a straightforward state machine, and the code is nearly identical across different synths. Once you've seen how it works in one synth, you've seen how it works in nearly all of them.&lt;/p&gt;&lt;head rend="h3"&gt;Debugging the Firmware in an Emulator #&lt;/head&gt;&lt;p&gt;One of the best ways to understand what's going on inside a synth's firmware is to run it in an emulator. The MAME emulation framework is freely available, and already supports a wide variety of vintage synths. It features a built-in disassembler, and a debugger that can be used to step through the firmware instruction by instruction to see what's happening in detail. When I was working on my Yamaha DX9/7 project, I used MAME as a testing and development platform for the firmware.&lt;/p&gt;&lt;head rend="h3"&gt;Final Words #&lt;/head&gt;&lt;p&gt;The DX7, and its 8-bit CPU might be a bit primitive by today's standards, but the same principles apply to reverse-engineering modern devices. Instructions sets and calling-conventions might change, but whether it's a vintage 8-bit architecture like the 6800, or a cutting-edge 32-bit ARM system, the principles of how to disassemble device firmware remain the same.&lt;/p&gt;&lt;p&gt;If you have any questions about this article, please get in touch! If you have any corrections or suggestions, I'd love to hear from you. Thank you for reading!&lt;/p&gt;&lt;head rend="h2"&gt;Appendix: Why Choose This Synth? #&lt;/head&gt;&lt;head rend="h3"&gt;It Can Be Disassembled With Free Software&lt;/head&gt;&lt;p&gt;6303 binaries can be disassembled by using free and open source tools, such as Ghidra, F9DASM, and MAME's Universal Disassembler.&lt;/p&gt;&lt;head rend="h3"&gt;It's Well Documented&lt;/head&gt;&lt;p&gt;40 years on, the DX7 continues to captivate people's imaginations. As a result, lots is known about what goes on inside a DX7. Yamaha's service manuals are comprehensive, and freely available online.&lt;/p&gt;&lt;p&gt;Yamaha even released internal documentation on the DX7's architecture and sound chips, which is now available online.&lt;/p&gt;&lt;head rend="h3"&gt;Only One ROM&lt;/head&gt;&lt;p&gt;One advantage of reverse-engineering the DX7 is that there's only one ROM you need worry about. Technically there's also the sub-CPU and its mask ROM, but in this case you don't really need to worry what's going on there.&lt;/p&gt;&lt;p&gt;Some synths have important part of the firmware stored on the CPU's mask ROM, such as the Casio CZ-101. Other synths spread the synth's core functionality across multiple CPUs, each with their own ROMs, such as the Roland JX-8P. The DX7 is much simpler, having (nearly) all of its code in one place.&lt;/p&gt;&lt;head rend="h3"&gt;It Has an LCD Screen&lt;/head&gt;&lt;p&gt;Disassembling code for a system with a text-based user interface has a lot of advantages. I considered some of the early DCO-based Roland polysynths as candidates for this article, but without an LCD screen it's much harder to make headway into a ROM.&lt;/p&gt;&lt;head rend="h3"&gt;No Bank Switching&lt;/head&gt;&lt;p&gt;Unfortunately the various disassembler tools available don't handle bank switching very well. In Ghidra you can use 'Overlay' memory blocks to set up the different banks, however it's still not very intuitive in my experience.&lt;/p&gt;&lt;p&gt;I considered the Ensoniq ESQ-1 as a candidate for this article. It features a Motorola MC6809 processor, which is very well supported by lots of different debuggers. However it uses bank switching, which makes it a bit of a nuisance to disassemble.&lt;/p&gt;&lt;head rend="h4"&gt;What Is Bank Switching?&lt;/head&gt;&lt;p&gt;What happens if you need to squeeze 64KiB of firmware ROM, and 32KiB of RAM into your HD6303 chip's 16-bit address space? One solution to this problem is bank switching. Many vintage synths use bank-switching to fit their firmware into the CPU's address space.&lt;/p&gt;&lt;p&gt;Bank switching breaks a memory device's address space up into multiple 'banks' by latching one or more of its address lines to one of the CPU's I/O port lines. This allows the CPU to select which 'bank' is active by toggling the aforementioned I/O line in the software.&lt;/p&gt;&lt;p&gt; The Yamaha TX81Z features a 64KiB 27C512 EPROM chip, mapped into the CPU's address space at 0x8000 - 0xFFFF. The EPROM's A0-A14 pins are wired to the CPU's A0-A14, and the EPROM's &lt;code&gt;CE1&lt;/code&gt; pin is latched to the CPU's A15
          pin. The EPROM's A15 pin is wired to the CPU's I/O port 6 (pin
          &lt;code&gt;P63&lt;/code&gt; in the schematics). If the &lt;code&gt;P63&lt;/code&gt; I/O line
          is pulled high, the upper half of the EPROM's memory is
          selected, mapping addresses
          0x8000 - 0xFFFF into the CPU's
          address space. If it's pulled low, the EPROM's
          0x0000 to 0x7FFF memory is mapped to
          0x8000 - 0xFFFF.
        &lt;/p&gt;&lt;p&gt;To allow branching from code in one bank to code in another, a common technique is to use a 'trampoline function' located at the same address in both banks.&lt;/p&gt;&lt;head rend="h2"&gt;Appendix: Documentation #&lt;/head&gt;&lt;p&gt;Below is a list of all the important documentation referenced in the article.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;DX7 Schematic&lt;/item&gt;&lt;item&gt;DX7 Service Manual&lt;/item&gt;&lt;item&gt;Hitachi HD6303R Datasheet&lt;/item&gt;&lt;item&gt;HD6301/HD6303 Series Handbook&lt;/item&gt;&lt;item&gt;Hitachi HD44780 Datasheet&lt;/item&gt;&lt;item&gt;Intel 8255 Datasheet&lt;/item&gt;&lt;item&gt;74LS138 Datasheet&lt;/item&gt;&lt;/list&gt;&lt;list rend="ol"&gt;&lt;item&gt;The Hitachi 6303 microcontroller used in the DX7 includes both an internal, and external memory bus. The first 256 memory addresses in the 6303 point to the CPU's internal registers and on-board RAM. Many modern microcontrollers —such as the Atmel AVR, and Microchip PIC series— don't feature external address buses. It's more common for modern microcontrollers to communicate with peripheral devices over serial buses, using protocols such as SPI, or I2C. ↲&lt;/item&gt;&lt;item&gt;On boot, the 6800 CPU family fetches the reset vector from the fixed address of 0xFFFE. Knowing this, we could have just made an educated guess that the whole ROM was mapped to the high addresses. Still, it's always good to check your assumptions! ↲&lt;/item&gt;&lt;item&gt;If you're curious about why the 8255 PPI chip is used here, it's most likely because the LCD controller, cartridge, and portamento/sustain pedal interface don't feature a chip select interface. ↲&lt;/item&gt;&lt;item&gt;There are a variety of disassemblers available for the HD6303 architecture. The state-of-the-art graphical disassembler is arguably IDA Pro, but it's closed source, and prohibitively expensive for hobbyists. Non-graphical disassemblers also exist, such as F9DASM. If you're new to reverse-engineering, I'd personally recommend starting with Ghidra. It's free, open source, and easy to learn. ↲&lt;/item&gt;&lt;item&gt;I went down a bit of a rabbit-hole trying to find what year the LM016/HD44780 was first manufactured. The earliest reference I can find online is a 'preliminary' user's manual, dated March 1981. It's a shame that there's so little background information available about one of the best-known ICs in history. ↲&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://ajxs.me/blog/Introduction_to_Reverse-Engineering_Vintage_Synth_Firmware.html"/><published>2025-10-20T02:56:20+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45639995</id><title>Entire Linux Network stack diagram (2024)</title><updated>2025-10-20T16:44:10.196175+00:00</updated><content>&lt;doc fingerprint="f5ed7a37fd9ba490"&gt;
  &lt;main&gt;
    &lt;p&gt; Published November 18, 2024 | Version v7 &lt;/p&gt;
    &lt;p&gt; Poster Open &lt;/p&gt;
    &lt;head rend="h1"&gt;Entire Linux Network stack diagram&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Diagram of entire Linux Network Stack, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Virtualization and Linux containers: &lt;list rend="ul"&gt;&lt;item&gt;Emulation and Paravirtualization.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network sockets.&lt;/item&gt;
      &lt;item&gt;Network stack: &lt;list rend="ul"&gt;&lt;item&gt;Upper layer of Network stack (TCP, UDP).&lt;/item&gt;&lt;item&gt;Low layer of Network stack with GRO, RPS, RFS and GSO.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network Scheduler.&lt;/item&gt;
      &lt;item&gt;NetFilter and traffic controll: &lt;list rend="ul"&gt;&lt;item&gt;Bridge and Bond interfaces.&lt;/item&gt;&lt;item&gt;Tap interface, ...&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Device Driver: &lt;list rend="ul"&gt;&lt;item&gt;Queue.&lt;/item&gt;&lt;item&gt;NAPI.&lt;/item&gt;&lt;item&gt;IRQ handler.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network functions accelerated by NIC: &lt;list rend="ul"&gt;&lt;item&gt;Checksum offload, VLAN, VxLAN, GRE, TSO, LRO, RSS, ...&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Network card.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All (above) sections (layers) include tips for optimizations and/or statistics.&lt;/p&gt;
    &lt;p&gt;This diagram is part of the book:&lt;/p&gt;
    &lt;p&gt;Operativni sustavi i računalne mreže - Linux u primjeni&lt;/p&gt;
    &lt;p&gt;https://doi.org/10.5281/zenodo.8119310&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h3"&gt; Linux Network Stack - EN.pdf &lt;/head&gt;
    &lt;head rend="h3"&gt; Files (5.4 MB) &lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Download all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt; md5:a8f70808b4c1d2a4f33301fe7afd3ea1 &lt;/cell&gt;
        &lt;cell&gt;5.4 MB&lt;/cell&gt;
        &lt;cell&gt;Preview Download&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Additional details&lt;/head&gt;
    &lt;head rend="h3"&gt;Related works&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Is part of&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Book: 10.5281/zenodo.8119310 (DOI)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://zenodo.org/records/14179366"/><published>2025-10-20T03:33:11+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640226</id><title>Space Elevator</title><updated>2025-10-20T16:44:10.095916+00:00</updated><content/><link href="https://neal.fun/space-elevator/"/><published>2025-10-20T04:42:08+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640594</id><title>DeepSeek OCR</title><updated>2025-10-20T16:44:09.626601+00:00</updated><content>&lt;doc fingerprint="d0978f309aa0d982"&gt;
  &lt;main&gt;
    &lt;p&gt;📥 Model Download | 📄 Paper Link | 📄 Arxiv Paper Link |&lt;/p&gt;
    &lt;p&gt;Explore the boundaries of visual-text compression.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[2025/x/x]🚀🚀🚀 We release DeepSeek-OCR, a model to investigate the role of vision encoders from an LLM-centric viewpoint.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Our environment is cuda11.8+torch2.6.0.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone this repository and navigate to the DeepSeek-OCR folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/deepseek-ai/DeepSeek-OCR.git&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conda&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;conda create -n deepseek-ocr python=3.12.9 -y
conda activate deepseek-ocr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Packages&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;download the vllm-0.8.5 whl&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118
pip install vllm-0.8.5+cu118-cp38-abi3-manylinux1_x86_64.whl
pip install -r requirements.txt
pip install flash-attn==2.7.3 --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Note: if you want vLLM and transformers codes to run in the same environment, you don't need to worry about this installation error like: vllm 0.8.5+cu118 requires transformers&amp;gt;=4.51.1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VLLM:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: change the INPUT_PATH/OUTPUT_PATH and other settings in the DeepSeek-OCR-master/DeepSeek-OCR-vllm/config.py&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-vllm&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;image: streaming output&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_image.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;pdf: concurrency ~2500tokens/s(an A100-40G)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_pdf.py&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;batch eval for benchmarks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python run_dpsk_ocr_eval_batch.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformers&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from transformers import AutoModel, AutoTokenizer
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
model_name = 'deepseek-ai/DeepSeek-OCR'

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)
model = model.eval().cuda().to(torch.bfloat16)

# prompt = "&amp;lt;image&amp;gt;\nFree OCR. "
prompt = "&amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown. "
image_file = 'your_image.jpg'
output_path = 'your/output/dir'

res = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)&lt;/code&gt;
    &lt;p&gt;or you can&lt;/p&gt;
    &lt;code&gt;cd DeepSeek-OCR-master/DeepSeek-OCR-hf
python run_dpsk_ocr.py&lt;/code&gt;
    &lt;p&gt;The current open-source model supports the following modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native resolution: &lt;list rend="ul"&gt;&lt;item&gt;Tiny: 512×512 （64 vision tokens）✅&lt;/item&gt;&lt;item&gt;Small: 640×640 （100 vision tokens）✅&lt;/item&gt;&lt;item&gt;Base: 1024×1024 （256 vision tokens）✅&lt;/item&gt;&lt;item&gt;Large: 1280×1280 （400 vision tokens）✅&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Dynamic resolution &lt;list rend="ul"&gt;&lt;item&gt;Gundam: n×640×640 + 1×1024×1024 ✅&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# document: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;Convert the document to markdown.
# other image: &amp;lt;image&amp;gt;\n&amp;lt;|grounding|&amp;gt;OCR this image.
# without layouts: &amp;lt;image&amp;gt;\nFree OCR.
# figures in document: &amp;lt;image&amp;gt;\nParse the figure.
# general: &amp;lt;image&amp;gt;\nDescribe this image in detail.
# rec: &amp;lt;image&amp;gt;\nLocate &amp;lt;|ref|&amp;gt;xxxx&amp;lt;|/ref|&amp;gt; in the image.
# '先天下之忧而忧'&lt;/code&gt;
    &lt;p&gt;We would like to thank Vary, GOT-OCR2.0, MinerU, PaddleOCR, OneChart, Slow Perception for their valuable models and ideas.&lt;/p&gt;
    &lt;p&gt;We also appreciate the benchmarks: Fox, OminiDocBench.&lt;/p&gt;
    &lt;p&gt;coming soon！&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/deepseek-ai/DeepSeek-OCR"/><published>2025-10-20T06:26:33+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640838</id><title>AWS Multiple Services Down in us-east-1</title><updated>2025-10-20T16:44:08.909953+00:00</updated><link href="https://health.aws.amazon.com/health/status?ts=20251020"/><published>2025-10-20T07:22:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45640877</id><title>Docker Systems Status: Full Service Disruption</title><updated>2025-10-20T16:44:08.527140+00:00</updated><content>&lt;doc fingerprint="3bbcb2c31d7475f1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h5"&gt;Issues accessing Registry, Hub, Scout, DBC, DHIOperational&lt;/head&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Docker Hub Registry, Docker Authentication, Docker Hub Web Services, Docker Billing, Docker Hub Automated Builds, Docker Hub Security Scanning, Docker Scout, Docker Build Cloud, Testcontainers Cloud, Docker Cloud, Docker Hardened Images&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 03:05 PDT&lt;lb/&gt;October 20, 2025 10:05 UTC&lt;/p&gt;
        &lt;p&gt;[Resolved] This incident is resolved. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 02:43 PDT&lt;lb/&gt;October 20, 2025 09:43 UTC&lt;/p&gt;
        &lt;p&gt;[Monitoring] We are seeing error rates recovering across our SaaS services. We continue to monitor as we process our backlog. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 01:22 PDT&lt;lb/&gt;October 20, 2025 08:22 UTC&lt;/p&gt;
        &lt;p&gt;[Identified] We have identified the underlying issue with one of our cloud service providers. We are monitoring the situation and prepare our systems for when the issues with our service provider resolve. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;October 20, 2025 00:16 PDT&lt;lb/&gt;October 20, 2025 07:16 UTC&lt;/p&gt;
        &lt;p&gt;[Investigating] We are seeing issues accessing and using our services across many of our products. We are currently investigating and will report back as soon as possible.. &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.dockerstatus.com/pages/incident/533c6539221ae15e3f000031/68f5e1c741c825463df7486c"/><published>2025-10-20T07:31:23+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45642911</id><title>Show HN: Playwright Skill for Claude Code – Less context than playwright-MCP</title><updated>2025-10-20T16:44:08.075850+00:00</updated><content>&lt;doc fingerprint="9f451e9e0fdfe3ea"&gt;
  &lt;main&gt;
    &lt;p&gt;General-purpose browser automation as a Claude Skill&lt;/p&gt;
    &lt;p&gt;A Claude Skill that enables Claude to write and execute any Playwright automation on-the-fly - from simple page tests to complex multi-step flows. Packaged as a Claude Code Plugin for easy installation and distribution.&lt;/p&gt;
    &lt;p&gt;Claude autonomously decides when to use this skill based on your browser automation needs, loading only the minimal information required for your specific task.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any Automation Task - Claude writes custom code for your specific request, not limited to pre-built scripts&lt;/item&gt;
      &lt;item&gt;Visible Browser by Default - See automation in real-time with &lt;code&gt;headless: false&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Zero Module Resolution Errors - Universal executor ensures proper module access&lt;/item&gt;
      &lt;item&gt;Progressive Disclosure - Concise SKILL.md with full API reference loaded only when needed&lt;/item&gt;
      &lt;item&gt;Safe Cleanup - Smart temp file management without race conditions&lt;/item&gt;
      &lt;item&gt;Comprehensive Helpers - Optional utility functions for common tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This skill can be installed via the Claude Code plugin system or manually.&lt;/p&gt;
    &lt;code&gt;# Add this repository as a marketplace
/plugin marketplace add lackeyjb/playwright-skill

# Install the plugin
/plugin install playwright-skill@playwright-skill

# Navigate to the skill directory and run setup
cd ~/.claude/plugins/marketplaces/playwright-skill/skills/playwright-skill
npm run setup&lt;/code&gt;
    &lt;p&gt;Verify installation by running &lt;code&gt;/help&lt;/code&gt; to confirm the skill is available.&lt;/p&gt;
    &lt;p&gt;Install directly from GitHub to your skills directory:&lt;/p&gt;
    &lt;p&gt;Global Installation (Available Everywhere):&lt;/p&gt;
    &lt;code&gt;# Navigate to your Claude skills directory
cd ~/.claude/skills

# Clone the skill
git clone https://github.com/lackeyjb/playwright-skill.git

# Navigate into the skill directory (note the nested structure)
cd playwright-skill/skills/playwright-skill

# Install dependencies and Chromium browser
npm run setup&lt;/code&gt;
    &lt;p&gt;Project-Specific Installation:&lt;/p&gt;
    &lt;code&gt;# Install in a specific project
cd /path/to/your/project
mkdir -p .claude/skills
cd .claude/skills
git clone https://github.com/lackeyjb/playwright-skill.git
cd playwright-skill/skills/playwright-skill
npm run setup&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from GitHub Releases&lt;/item&gt;
      &lt;item&gt;Extract to: &lt;list rend="ul"&gt;&lt;item&gt;Global: &lt;code&gt;~/.claude/skills/playwright-skill&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Project: &lt;code&gt;/path/to/your/project/.claude/skills/playwright-skill&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Global: &lt;/item&gt;
      &lt;item&gt;Navigate to the skill directory and run setup: &lt;code&gt;cd playwright-skill/skills/playwright-skill npm run setup&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run &lt;code&gt;/help&lt;/code&gt; to confirm the skill is loaded, then ask Claude to perform a simple browser task like "Test if google.com loads".&lt;/p&gt;
    &lt;p&gt;After installation, simply ask Claude to test or automate any browser task. Claude will write custom Playwright code, execute it, and return results with screenshots and console output.&lt;/p&gt;
    &lt;code&gt;"Test the homepage"
"Check if the contact form works"
"Verify the signup flow"
&lt;/code&gt;
    &lt;code&gt;"Take screenshots of the dashboard in mobile and desktop"
"Test responsive design across different viewports"
&lt;/code&gt;
    &lt;code&gt;"Fill out the registration form and submit it"
"Click through the main navigation"
"Test the search functionality"
&lt;/code&gt;
    &lt;code&gt;"Check for broken links"
"Verify all images load"
"Test form validation"
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Describe what you want to test or automate&lt;/item&gt;
      &lt;item&gt;Claude writes custom Playwright code for the task&lt;/item&gt;
      &lt;item&gt;The universal executor (run.js) runs it with proper module resolution&lt;/item&gt;
      &lt;item&gt;Browser opens (visible by default) and automation executes&lt;/item&gt;
      &lt;item&gt;Results are displayed with console output and screenshots&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Headless: &lt;code&gt;false&lt;/code&gt;(browser visible unless explicitly requested otherwise)&lt;/item&gt;
      &lt;item&gt;Slow Motion: &lt;code&gt;100ms&lt;/code&gt;for visibility&lt;/item&gt;
      &lt;item&gt;Timeout: &lt;code&gt;30s&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Screenshots: Saved to &lt;code&gt;/tmp/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;playwright-skill/
├── .claude-plugin/
│   ├── plugin.json          # Plugin metadata for distribution
│   └── marketplace.json     # Marketplace configuration
├── skills/
│   └── playwright-skill/    # The actual skill (Claude discovers this)
│       ├── SKILL.md         # What Claude reads (314 lines)
│       ├── run.js           # Universal executor (proper module resolution)
│       ├── package.json     # Dependencies &amp;amp; setup scripts
│       └── lib/
│           └── helpers.js   # Optional utility functions
├── API_REFERENCE.md         # Full Playwright API reference (630 lines)
├── README.md                # This file - user documentation
├── CONTRIBUTING.md          # Contribution guidelines
└── LICENSE                  # MIT License
&lt;/code&gt;
    &lt;p&gt;Claude will automatically load &lt;code&gt;API_REFERENCE.md&lt;/code&gt; when needed for comprehensive documentation on selectors, network interception, authentication, visual regression testing, mobile emulation, performance testing, and debugging.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js &amp;gt;= 14.0.0&lt;/item&gt;
      &lt;item&gt;Playwright ^1.48.0 (installed via &lt;code&gt;npm run setup&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Chromium (installed via &lt;code&gt;npm run setup&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Playwright not installed? Navigate to the skill directory and run &lt;code&gt;npm run setup&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Module not found errors? Ensure automation runs via &lt;code&gt;run.js&lt;/code&gt;, which handles module resolution.&lt;/p&gt;
    &lt;p&gt;Browser doesn't open? Verify &lt;code&gt;headless: false&lt;/code&gt; is set. The skill defaults to visible browser unless headless mode is requested.&lt;/p&gt;
    &lt;p&gt;Install all browsers? Run &lt;code&gt;npm run install-all-browsers&lt;/code&gt; from the skill directory.&lt;/p&gt;
    &lt;p&gt;Skills are modular capabilities that extend Claude's functionality. Unlike slash commands that you invoke manually, skills are model-invoked—Claude autonomously decides when to use them based on your request.&lt;/p&gt;
    &lt;p&gt;When you ask Claude to test a webpage or automate browser interactions, Claude discovers this skill, loads the necessary instructions, executes custom Playwright code, and returns results with screenshots and console output.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome. Fork the repository, create a feature branch, make your changes, and submit a pull request. See CONTRIBUTING.md for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Skills - Official announcement from Anthropic&lt;/item&gt;
      &lt;item&gt;Claude Code Skills Documentation&lt;/item&gt;
      &lt;item&gt;Claude Code Plugins Documentation&lt;/item&gt;
      &lt;item&gt;Plugin Marketplaces&lt;/item&gt;
      &lt;item&gt;API_REFERENCE.md - Full Playwright documentation&lt;/item&gt;
      &lt;item&gt;GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/lackeyjb/playwright-skill"/><published>2025-10-20T11:58:35+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45642923</id><title>Matrix Conference 2025 Highlights</title><updated>2025-10-20T16:44:07.576503+00:00</updated><content>&lt;doc fingerprint="bdd4937c8931551f"&gt;
  &lt;main&gt;
    &lt;p&gt;The Matrix Conference 2025 was a huge success; the energy and enthusiasm was just incredible!&lt;/p&gt;
    &lt;p&gt;We were delighted to be the anchor sponsor - thanks to The Matrix Foundation and everyone else that organised the conference, those who presented, the others sponsors and of course all those who attended!&lt;/p&gt;
    &lt;p&gt;The overriding vibe of the conference was one of incredible momentum - with so many governments presenting on their Matrix-based initiatives, there was a genuine realisation that Matrix is the future of government and inter-governmental communications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Matrix Conference on-demand&lt;/head&gt;
    &lt;p&gt;All the presentations given at The Matrix Conference are available here. Each and every presentation is well worth watching.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Matrix State of the Union&lt;/head&gt;
    &lt;p&gt;Matthew Hodgson gives an excellent overview of the entire Matrix universe.&lt;/p&gt;
    &lt;p&gt;For those interested in government and public sector adoption of Matrix, don’t miss Amandine Le Pape’s How Matrix is becoming the communication standard for the public sector.&lt;/p&gt;
    &lt;head rend="h2"&gt;Element's keynote&lt;/head&gt;
    &lt;p&gt;Neil Johnson’s talk includes a power-packed review of Element’s work over the last 12 months (and some sneak previews).&lt;lb/&gt;In particular, look out for ESS Community being installed in under a minute - and then, when the time is right, it being a seamless live-upgrade to ESS Pro! The idea here is that ESS Community can be used, free of charge, for casual use (up to about 100 users) and also act as a way for organisations to run small scale evaluations. Those evaluations can then be easily upgraded to ESS Pro for increased scalability, performance, enterprise features and SLA-backed support.&lt;/p&gt;
    &lt;head rend="h2"&gt;Element product presentations&lt;/head&gt;
    &lt;p&gt;ESS - Element’s distribution for Matrix deployments, from Patrick Maier, gives more detail on Element Server Suite; covering ESS Community, ESS Pro and ESS for TI-Messenger. For those that want to get into the nitty-gritty, take a look at Gael Goinvic’s Getting Started with ESS Community workshop.&lt;/p&gt;
    &lt;p&gt;Element Pro, our new app developed specifically for the workplace, created a huge buzz - particularly in-app colour theming, and the ability to now create a whitelabelled mobile app without the expense of maintaining a fork. Catch all the details from Andreas Sisask in his session; Element X and Pro. We’d also recommend the Element X Web presentation and the session on Element Call.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real stars of the show!&lt;/head&gt;
    &lt;p&gt;The best proof points, of course, are the deployments and initiatives that are already underway. The conference was packed with presentations from governments and public sector organisations, who are the real stars of the show!&lt;/p&gt;
    &lt;p&gt;Do take a look at the outstanding work being done across governments, NGOs and public sector organisations.&lt;/p&gt;
    &lt;p&gt;An open standard based on open source software, Matrix delivers the digital sovereignty, interoperability, resilience and security that governments need to transform the way they communicate; both within their own nation and across borders. These are the key benefits driving Matrix adoption.&lt;/p&gt;
    &lt;p&gt;The discussion between sessions, and in the evenings, had a consistent theme. Governments want communications that are:&lt;/p&gt;
    &lt;p&gt;1. Digitally sovereign - meaning end-user organisations have complete autonomy over their technology stack. Crucially, that means no vendor lock-in. That so many competing vendors sponsored and attended The Matrix Conference underlines the health of the Matrix ecosystem.&lt;/p&gt;
    &lt;p&gt;2. Interoperable - to both enable digital sovereignty and ensure that separate organisations can easily communicate with each other. The interoperability delivered by the Matrix open standard is absolutely crucial in enabling large-scale federated communications between multiple organisations.&lt;/p&gt;
    &lt;p&gt;3. Resilient - a decentralised communications network provides a far more robust communications architecture than a centralised network, which is paramount for government communications (as we write this blog post, Signal, Slack, Zoom and others are down due to their centralised design and a dependency on AWS).&lt;/p&gt;
    &lt;p&gt;4. Secure - end-to-end encryption is, of course, fundamental and viewed as ‘table stakes.’&lt;/p&gt;
    &lt;p&gt;European governments are rightly determined to control their own digital destiny, and are embracing Matrix as the foundation for real time communications.&lt;/p&gt;
    &lt;p&gt;Seeing the European Commission, France, FITKO, Germany, the German healthcare system, Luxembourg, NATO, Sweden, United Nations, ZenDiS and the European Space Agency all presenting on their Matrix deployments was just mindblowing!&lt;/p&gt;
    &lt;p&gt;Knowing how many other governments were also in attendance, soaking up insights and tips for their forthcoming Matrix projects, makes us really excited for a Matrix-based future that transforms cross-border collaboration and helps support a united, digitally sovereign Europe.&lt;/p&gt;
    &lt;p&gt;👋 See you all next year!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://element.io/blog/the-matrix-conference-a-seminal-moment-for-matrix/"/><published>2025-10-20T12:00:28+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45642979</id><title>Qt Group Buys IAR Systems Group</title><updated>2025-10-20T16:44:06.916134+00:00</updated><content>&lt;doc fingerprint="b6dc8b931a21a33d"&gt;
  &lt;main&gt;
    &lt;p&gt;Oct 13, 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Qt completes the recommended public cash offer to the shareholders of I.A.R. Systems Group&lt;/head&gt;
    &lt;p&gt;Qt Group Plc Stock exchange release 13 October 2025 at 13.31 p.m. EEST&lt;/p&gt;
    &lt;p&gt;Qt completes the recommended public cash offer to the shareholders of I.A.R. Systems Group&lt;/p&gt;
    &lt;p&gt;On 4 July 2025, Qt Group Plc's ("Qt Group") wholly owned subsidiary The Qt Company Ltd ("The Qt Company" and together with Qt Group, "Qt"), announced a recommended public cash offer to the shareholders of class B shares (the "Shares" or, individually, a "Share") in I.A.R. Systems Group AB (publ) ("IAR"), to tender all their Shares at a price of SEK 180 in cash per Share (the "Offer"). The Shares in IAR are traded on Nasdaq Stockholm, Mid Cap. An offer document relating to the Offer was published on 15 August 2025.&lt;/p&gt;
    &lt;p&gt;At the end of the acceptance period on 10 October 2025, the Offer had been accepted by shareholders with a total of 12,037,848 Shares in IAR, corresponding to 94.49 per cent of the outstanding shares and votes in IAR.[1] As a result, The Qt Company controls in total 12,037,848 Shares in IAR, corresponding to 94.49 per cent of the outstanding shares and votes in IAR.[2]&lt;/p&gt;
    &lt;p&gt;The Qt Company has decided to complete the Offer. All conditions are satisfied or have been waived. Settlement for Shares tendered in the Offer during the initial acceptance period will be initiated on or around 17 October 2025.&lt;/p&gt;
    &lt;p&gt;Juha Varelius, President &amp;amp; CEO at Qt Group Plc, comments:&lt;/p&gt;
    &lt;p&gt;"We are pleased to the offer being accepted to such an extent that we can successfully complete the acquisition. We look forward to working closely with the IAR team to fully realise the potential of the combination by expanding our total addressable market, strengthening our global presence and customer offering."&lt;/p&gt;
    &lt;p&gt;To allow for those shareholders who have not yet accepted the Offer to tender their Shares to The Qt Company, the acceptance period is extended until 27 October 2025 at 3.00 p.m. CET. Settlement for Shares tendered in the Offer during the extended acceptance period is expected to be initiated on or around 3 November 2025.&lt;/p&gt;
    &lt;p&gt;Since the Offer is unconditional, shareholders who have already accepted or will accept the Offer during the extended acceptance period, have no right to withdraw their acceptances.&lt;/p&gt;
    &lt;p&gt;The Qt Company intends to initiate compulsory redemption proceedings in accordance with the Swedish Companies Act to acquire all shares not tendered in the Offer and to promote delisting of IAR’s Shares from Nasdaq Stockholm.&lt;/p&gt;
    &lt;p&gt;Information about the Offer is made available at www.nordic-software-offer.com.&lt;/p&gt;
    &lt;p&gt;Advisors&lt;/p&gt;
    &lt;p&gt;Qt has appointed Nordea Bank Abp as lead financial advisor and Stifel Nicolaus Europe Limited as financial advisor. Krogerus Attorneys, Advokatfirman Vinge and Freshfields LLP are acting as legal advisors in connection with the Offer.&lt;/p&gt;
    &lt;p&gt;Investor relations contact:&lt;/p&gt;
    &lt;p&gt;pr@qt.io&lt;/p&gt;
    &lt;p&gt;Media contact:&lt;/p&gt;
    &lt;p&gt;Sandra Uitto, Fogel &amp;amp; Partners&lt;/p&gt;
    &lt;p&gt;Tel.: +46 (0)73 892 1740&lt;/p&gt;
    &lt;p&gt;E-Mail: qtgroup@fogelpartners.se&lt;/p&gt;
    &lt;p&gt;Distribution:&lt;/p&gt;
    &lt;p&gt;Nasdaq Helsinki&lt;/p&gt;
    &lt;p&gt;Key media&lt;/p&gt;
    &lt;p&gt;www.qt.io&lt;/p&gt;
    &lt;p&gt;Important information&lt;/p&gt;
    &lt;p&gt;This stock exchange release does not constitute an offer to buy or sell Shares, nor does it constitute an invitation to offer to buy or sell Shares. Investors considering tendering their Shares in the Offer by Qt Group's subsidiary The Qt Company should rely only on information disclosed by The Qt Company as the offeror of the Offer.&lt;/p&gt;
    &lt;p&gt;The Offer is not being made to persons whose participation in the Offer requires that an additional offer document be prepared or registration effected or that any other measures be taken in addition to those required under Swedish law.&lt;/p&gt;
    &lt;p&gt;The Offer is not being made, directly or indirectly, in or into Australia, Canada, Hong Kong, New Zealand, Japan, Singapore, South Africa, South Korea, Russia, Belarus or in any other jurisdiction where such offer would be prohibited by applicable law pursuant to legislation, restrictions and regulations in the relevant jurisdiction, by use of mail or any other communication means or instrumentality (including, without limitation, facsimile transmission, electronic mail, telex, telephone and the Internet) of interstate or foreign commerce, or of any facility of national securities exchange or other trading venue, of Australia, Canada, Hong Kong, New Zealand, Japan, Singapore, South Africa, South Korea, Russia, Belarus or in any other jurisdiction where such offer would be prohibited by applicable law pursuant to legislation, restrictions and regulations in the relevant jurisdiction, and the Offer cannot be accepted by any such use or by such means, instrumentality or facility of, in or from, Australia, Canada, Hong Kong, New Zealand, Japan, Singapore, South Africa, South Korea, Russia, Belarus or in any other jurisdiction where such offer would be prohibited by applicable law pursuant to legislation, restrictions and regulations in the relevant jurisdiction. Accordingly, any documentation relating to the Offer are not being and should not be sent, mailed or otherwise distributed or forwarded in or into Australia, Canada, Hong Kong, New Zealand, Japan, Singapore, South Africa, South Korea, Russia, Belarus or in any other jurisdiction where such offer would be prohibited by applicable law pursuant to legislation, restrictions and regulations in the relevant jurisdiction.&lt;/p&gt;
    &lt;p&gt;The Offer, the information and documents relating to the Offer are not being made and have not been approved by an authorised person for the purposes of section 21 of the UK Financial Services and Markets Act 2000 (the "FSMA"). The communication of the information and documents relating to the Offer are exempt from the restriction on financial promotions under section 21 of the FSMA on the basis that they are a communication by or on behalf of a body corporate which relates to a transaction to acquire day to day control of the affairs of a body corporate; or to acquire 50 per cent or more of the voting shares in a body corporate, within article 62 of the UK Financial Services and Markets Act 2000 (Financial Promotion) Order 2005.&lt;/p&gt;
    &lt;p&gt;The initial acceptance period for the Offer commenced on 18 August 2025 and expired at 3.00 p.m. CEST on 10 October 2025. The extended acceptance period expires at 3.00 p.m. CET on 27 October 2025.&lt;/p&gt;
    &lt;p&gt;Statements in this stock exchange release relating to future status or circumstances, including statements regarding future performance, growth and other trend projections and the other benefits of the Offer, are forward-looking statements. These statements may generally, but not always, be identified by the use of words such as "anticipates", "intends", "expects", "believes", or similar expressions. By their nature, forward-looking statements involve risk and uncertainty because they relate to events and depend on circumstances that will occur in the future. There can be no assurance that actual results will not differ materially from those expressed or implied by these forward-looking statements due to many factors, many of which are outside the control of Qt Group. Any such forward-looking statements speak only as of the date on which they are made, and Qt Group has no obligation (and undertakes no such obligation) to update or revise any of them, whether as a result of new information, future events or otherwise, except for in accordance with applicable laws and regulations.&lt;/p&gt;
    &lt;p&gt;Information for shareholders in the United States&lt;/p&gt;
    &lt;p&gt;The Offer by The Qt Company described in this stock exchange release is not made by Qt Group and, as made by The Qt Company, is made for the issued and outstanding shares of IAR, a company incorporated under Swedish law, and is subject to Swedish disclosure and procedural requirements, which may be different from those of the United States. The Offer is made in the United States pursuant to Section 14(e) of the U.S. Securities Exchange Act of 1934, as amended (the "U.S. Exchange Act") and Regulation 14E thereunder, to the extent applicable and otherwise in compliance with the disclosure and procedural requirements of Swedish law, including with respect to withdrawal rights, the Offer timetable, notices of extensions, announcements of results, settlement procedures (including as regards to the time when payment of the consideration is rendered) and waivers of conditions, which may be different from requirements or customary practices in relation to U.S. domestic tender offers. The Qt Company's ability to waive the conditions to the Offer (both during and after the end of the acceptance period) and the shareholders' ability to withdraw their acceptances, are not the same under a tender offer governed by Swedish law as under a tender offer governed by U.S. law. Holders of the shares in IAR domiciled in the United States (the "U.S. Holders") are encouraged to consult with their own advisors regarding the Offer.&lt;/p&gt;
    &lt;p&gt;IAR's financial statements and all financial information included herein, or any other documents relating to the Offer, have been or will be prepared in accordance with IFRS and may not be comparable to the financial statements or financial information of companies in the United States or other companies whose financial statements are prepared in accordance with U.S. generally accepted accounting principles. The Offer is made to the U.S. Holders on the same terms and conditions as those made to all other shareholders of IAR to whom an offer is made. Any information documents, including the offer document, are being disseminated to U.S. Holders on a basis comparable to the method pursuant to which such documents are provided to IAR's other shareholders.&lt;/p&gt;
    &lt;p&gt;The Offer, which is subject to Swedish law, is being made to the U.S. Holders in accordance with the applicable U.S. securities laws, and applicable exemptions thereunder. To the extent the Offer is subject to U.S. securities laws, those laws only apply to U.S. Holders and thus will not give rise to claims on the part of any other person. The U.S. Holders should consider that the price for the Offer is being paid in SEK and that no adjustment will be made based on any changes in the exchange rate.&lt;/p&gt;
    &lt;p&gt;It may be difficult for IAR's shareholders to enforce their rights and any claims they may have arising under the U.S. federal or U.S. state securities laws in connection with the Offer, since IAR and The Qt Company are located in countries other than the United States, and some or all of their respective officers and directors may be residents of countries other than the United States. IAR's shareholders may not be able to sue IAR or The Qt Company or their respective officers or directors in a non-U.S. court for violations of U.S. securities laws. Further, it may be difficult to compel IAR or The Qt Company and/or their respective affiliates to subject themselves to the jurisdiction or judgment of a U.S. court.&lt;/p&gt;
    &lt;p&gt;To the extent permissible under applicable law and regulations and pursuant to Rule 14e-5(b) of the U.S. Exchange Act, The Qt Company and its affiliates or its brokers and its brokers' affiliates (acting as agents for The Qt Company or its affiliates, as applicable) may from time to time and during the pendency of the Offer, and other than pursuant to the Offer, directly or indirectly purchase or arrange to purchase shares of IAR, or any securities that are convertible into, exchangeable for or exercisable for such shares. These purchases may occur either in the open market at prevailing prices or in private transactions at negotiated prices, and information about such purchases will be disclosed by means of a press release or other means reasonably calculated to inform U.S. Holders of such information. In addition, the financial advisors to The Qt Company may also engage in ordinary course trading activities in securities of IAR, which may include purchases or arrangements to purchase such securities as long as such purchases or arrangements are in compliance with the applicable law. Any information about such purchases will be announced in Swedish and in a non-binding English translation available to the U.S. Holders through relevant electronic media if, and to the extent, such announcement is required under applicable Swedish or U.S. law, rules or regulations.&lt;/p&gt;
    &lt;p&gt;The receipt of cash pursuant to the Offer by a U.S. Holder may be a taxable transaction for U.S. federal income tax purposes and under applicable U.S. state and local, as well as foreign and other, tax laws. Each shareholder is urged to consult an independent professional adviser regarding the tax consequences of accepting the Offer. Neither The Qt Company nor any of its affiliates and their respective directors, officers, employees or agents or any other person acting on their behalf in connection with the Offer shall be responsible for any tax effects or liabilities resulting from acceptance of the Offer.&lt;/p&gt;
    &lt;p&gt;NEITHER THE U.S. SECURITIES AND EXCHANGE COMMISSION NOR ANY U.S. STATE SECURITIES COMMISSION HAS APPROVED OR DISAPPROVED THE OFFER, PASSED ANY COMMENTS UPON THE MERITS OR FAIRNESS OF THE OFFER, PASSED ANY COMMENT UPON THE ADEQUACY OR COMPLETENESS OF THIS STOCK EXCHANGE RELEASE OR PASSED ANY COMMENT ON WHETHER THE CONTENT IN THIS STOCK EXCHANGE RELEASE IS CORRECT OR COMPLETE. ANY REPRESENTATION TO THE CONTRARY IS A CRIMINAL OFFENCE IN THE UNITED STATES.&lt;/p&gt;
    &lt;p&gt;Disclaimer&lt;/p&gt;
    &lt;p&gt;Nordea Bank Abp ("Nordea"), which is supervised by the European Central Bank and the Finnish Financial Supervisory Authority and Stifel Nicolaus Europe Limited ("Stifel"), which is authorised and regulated by the Financial Conduct Authority are acting as financial advisors to The Qt Company and no one else, in connection with the Offer and will not regard any other person as their client in relation to the Offer and will not be responsible to anyone other than The Qt Company for providing the protection afforded to their respective clients, or for providing advice in relation to the Offer or any other matters referred to in this announcement. Neither Nordea, Stifel, nor any of their affiliates, or their or any of their affiliates' respective employees, board members, officers, vendors, advisors, members, successors, representatives or agents owes or accepts any duty, liability or responsibility whatsoever (whether direct or indirect, consequential, whether in contract, in tort, in delict, under statute or otherwise) to any person who is not a client of Nordea or Stifel, respectively, in connection with the Offer or otherwise.&lt;/p&gt;
    &lt;p&gt;[1] Corresponding to 90.05 per cent of the total number of shares and votes in IAR.&lt;/p&gt;
    &lt;p&gt;[2] Corresponding to 90.05 per cent of the total number of shares and votes in IAR.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.qt.io/stock/qt-completes-the-recommended-public-cash-offer-to-the-shareholders-of-iar-systems-group-1760351460000-3668995"/><published>2025-10-20T12:09:53+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643144</id><title>Show HN: I got tired of managing dev environments, so I built ServBay</title><updated>2025-10-20T16:44:06.230562+00:00</updated><content>&lt;doc fingerprint="8688fe123c319890"&gt;
  &lt;main&gt;&lt;p&gt;ServBay includes a variety of services required for developers in their daily work. Whether it's development languages like PHP, Node.js, Python, Golang, Java, .NET, Ruby, Rust, various databases like MySQL, MariaDB, PostgreSQL, MongoDB, Redis, or services such as DNS, email, reverse proxy/tunnel, and even the Ollama large language model, you can install them with one click and use them out of the box. With a wide range of package versions available, you can even find outdated versions like PHP 5.6 and Node.js 12. With multi-version coexistence, packages can run simultaneously without interference.&lt;/p&gt;&lt;p&gt;It's common for different projects to use different versions of development languages and databases. ServBay offers project-level runtime environment configurations, allowing you to specify different versions of development languages for each project, ensuring they do not interfere with each other, making your project development more flexible and controllable.&lt;/p&gt;&lt;p&gt;During the lifecycle of web development, domain names are consumables. ServBay comes with a built-in DNS service that allows developers to use non-existent domain names and TLDs without registering them, and issues free SSL certificates for these domain names. This capability not only saves developers significant costs but also greatly enhances security.&lt;/p&gt;&lt;p&gt;Email services are essential during development. ServBay has a built-in mail server that lets you easily use services like POP3/SMTP. With ServBay’s PKI, it supports STARTTLS and SSL/TLS with zero configuration. Want to send emails externally? The built-in SMTP Relay and SpamAssassin can help you with that. A powerful WebMail interface is also included.&lt;/p&gt;&lt;p&gt;ServBay has a built-in PKI system that allows developers to create CA and issue SSL certificates for free. In addition to domain certificates, it supports S/MIME email certificates, code signing certificates, and document signing certificates. With SSL certificates, developers can encrypt Web, database, and email services, as well as sign packages. Additionally, ServBay supports requesting SSL certificates from Let's Encrypt, ZeroSSL, and Google Trust Services via ACME and supports automatic renewal.&lt;/p&gt;&lt;p&gt;ServBay supports various mainstream web development languages, including but not limited to PHP, Node.js, Python, Golang, Java, .NET, Ruby and Rust. Each development language offers multiple versions, making it easy for developers to choose the right version based on project needs. ServBay automatically configures the corresponding runtime environment, allowing developers to start development work quickly without manual installation and setup. Switching between different language versions with one click facilitates testing compatibility across different environments.&lt;/p&gt;&lt;p&gt;ServBay integrates a variety of commonly used databases, such as MySQL, MariaDB, PostgreSQL, MongoDB, Redis, MariaDB, Memcached, SQLite, and more. Developers can choose suitable databases based on project needs and deploy them with one click. ServBay provides both a graphical interface and command-line tools, making it convenient for developers to manage databases, such as creating databases, importing and exporting data, executing SQL statements, and more. Additionally, ServBay supports multi-version coexistence of databases, allowing developers to test compatibility between different database versions.&lt;/p&gt;&lt;p&gt;ServBay supports reverse proxy features such as Ngrok, Pinggy.io, FRP, and Oray Peanut Shell. Developers can easily expose internal services of ServBay to the external network. This is very useful for debugging webhooks, connecting to intranet services, or creating secure tunnels. The reverse proxy feature in ServBay supports custom domain names, SSL certificates, and HTTP/3 protocols to ensure secure and efficient connections.&lt;/p&gt;&lt;p&gt;"We have been installing everything through BREW, managing ports is very difficult. ServBay is simply a miracle! I love ServBay!"&lt;/p&gt;&lt;p&gt;"I really enjoy using ServBay, and I believe it will gain wider application in the WordPress field."&lt;/p&gt;&lt;p&gt;"So far, I haven’t found any way to run .Net 4 on Mac without using a virtual machine other than ServBay. It saved my legacy project."&lt;/p&gt;&lt;p&gt;"I am a new member of ServBay, and I have to say, I’m very happy that this service finally exists! Cheers to the developers! You guys are doing great! #Respect! Please focus on maintaining the high quality you have already set and don’t rush to release new features. I really love the current service and hope you continue to grow and develop excellent software!"&lt;/p&gt;&lt;p&gt;"This is the easiest and most user-friendly web service software for new developers."&lt;/p&gt;&lt;p&gt;"ServBay is very easy to use and includes all the components needed to run local servers/websites, and it is very fast."&lt;/p&gt;&lt;p&gt;"It’s not just for web development! Managing my Rust development environment and backend services like databases through ServBay’s unified interface is amazing. It’s truly a multifunctional tool."&lt;/p&gt;&lt;p&gt;"Just install it, click a button, and you can launch different web servers and databases."&lt;/p&gt;&lt;p&gt;"It’s very easy to install and use, with a gentle learning curve. The software integrates easily with other software. I also love that it offers different database servers with just one click."&lt;/p&gt;&lt;p&gt;"ServBay truly allows me to focus on coding instead of endlessly tweaking and troubleshooting environment issues. Its integration and management of various services are top-notch. Highly recommend it to any developer using Mac!"&lt;/p&gt;&lt;p&gt;"Using ServBay is a fantastic experience. It helps build websites by creating web servers and MySQL databases."&lt;/p&gt;&lt;p&gt;"Most of my experience using it involves installing WordPress, and I find that using ServBay for installation is easier than some options provided by web hosts."&lt;/p&gt;&lt;p&gt;"ServBay provides an excellent graphical user interface for managing Python-based servers on Mac. I can quickly switch Python versions for software testing, and the logs provide the expected level of feedback for debugging applications."&lt;/p&gt;&lt;p&gt;"Running an older version of ASP.NET Framework applications on Mac has always been a big issue. The built-in Mono integration in ServBay perfectly solves this problem, allowing us to continue developing and maintaining critical legacy projects on modern hardware."&lt;/p&gt;&lt;p&gt;"No need to manage separate tools for Python, Go, and Node.js anymore! ServBay integrates them seamlessly with databases and even Supervisor. My entire development workflow has become clearer and more efficient."&lt;/p&gt;&lt;p&gt;"Our team standardized on ServBay for local development. New members onboard faster now, and the "it works on my machine" issues have greatly reduced. Environment consistency is finally achievable."&lt;/p&gt;&lt;p&gt;"Laragon and XAMPP drove me to despair. ServBay is the best tool of its kind that I have ever used."&lt;/p&gt;&lt;p&gt;"Finally, there is a way to handle multiple Java versions on macOS without pain! ServBay is a lifesaver for maintaining various legacy and modern projects."&lt;/p&gt;&lt;p&gt;"As a freelancer, I often switch between client projects using different tech stacks. ServBay allows me to maintain entirely independent configuration environments for each project. Context switching has become smooth and instant, significantly enhancing my productivity."&lt;/p&gt;&lt;p&gt;"I frequently need to practice penetration testing. ServBay is very convenient because it allows me to quickly set up the different environments I need."&lt;/p&gt;&lt;p&gt;"I previously used Docker Compose to manage my local Rust development environment, but it often felt heavy and had noticeable configuration overhead. ServBay provides similar isolation advantages but feels more lightweight and integrates better with my workflow. It starts faster and simplifies daily local development management."&lt;/p&gt;&lt;p&gt;Comprehensive professional features and a rich development toolkit to meet all the daily needs of individual developers.&lt;/p&gt;Download&lt;p&gt;Designed for professional developers or small startups, featuring unrestricted capabilities and various collaboration features.&lt;/p&gt;Buy Now Free Trial&lt;p&gt;Built for highly collaborative distributed development teams, empowering team leaders to coordinate daily development environments.&lt;/p&gt;Buy Now&lt;p&gt;ServBay provides web developers with an integrated, graphical, one-click installation local development environment. It includes commonly used web development services and tools, such as web servers, databases, programming languages, mail servers, queue services, SSL/PKI systems, and large language models (LLM).&lt;/p&gt;&lt;p&gt;ServBay includes services such as Caddy, NGINX, Apache, PHP, Python, Node.js, MySQL, MariaDB, PostgreSQL, MongoDB, Redis, memcached, and tools like phpMyAdmin, Adminer, and Composer. It also includes Ollama to facilitate developers in running large language models like deepseek, qwen, and llama.&lt;/p&gt;&lt;p&gt;ServBay integrates various versions of popular programming languages such as PHP, Python, and Node.js, and is continually adding support for other languages like Golang, Rust, Ruby, and Java. Developers can easily use and switch between the latest and deprecated versions of these languages in ServBay.&lt;/p&gt;&lt;p&gt;Absolutely! The original intention behind ServBay is to provide a unified and maintainable development environment for teams, reducing code discrepancies and bugs caused by inconsistent environments among team members. Through the 'unified configuration' feature, team managers can lock in different development environments for each project.&lt;/p&gt;&lt;p&gt;Compared to Docker, ServBay is specifically optimized, providing better resource consumption and making it easier to view logs and debug source code. In comparison with Homebrew, ServBay does not require compiling source code for installation, nor does it invade the operating system or pollute its environment, which could affect other applications' normal operation. Additionally, users do not need to manually edit configuration files, making it more user-friendly.&lt;/p&gt;&lt;p&gt;Unlike MAMP, XAMPP, herd, and similar tools, ServBay supports multiple hosts running simultaneously, allows the use of non-existent custom domain names, comes with free SSL certificates without application, and supports running multiple PHP versions at the same time. ServBay also offers detailed package versions, including the latest and historical versions, which can be installed and switched at any time. These capabilities are unmatched by other similar products.&lt;/p&gt;&lt;p&gt;Yes. ServBay can run multiple instances of different PHP versions concurrently, allowing you to set a different PHP version for each website and quickly switch between different versions.&lt;/p&gt;&lt;p&gt;ServBay supports PHP 5.6 - PHP 8.5, Python 2.7, 3.5 - 3.14, Node.js 12 - Node.js 23, MySQL 5.1 - MySQL 9.1, MariaDB 10.4 - MariaDB 11.5, PostgreSQL 10 - PostgreSQL 16, and MongoDB 5.0 - MongoDB 8.0.&lt;/p&gt;&lt;p&gt;Of course! You just need to xcode-select --install to install the relevant compilation tools for macOS, then set the PATH, CFLAGS, and other parameters to the path of the ServBay Development Lib to compile.&lt;/p&gt;&lt;p&gt;Absolutely! ServBay integrates Ollama, allowing you to run large language models such as deepseek, qwen, llama, solar, chatglm, etc.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.servbay.com"/><published>2025-10-20T12:29:00+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643163</id><title>Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system</title><updated>2025-10-20T16:44:05.900973+00:00</updated><content>&lt;doc fingerprint="69989c3d8f16dad1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system— up to 9x increase in output lets 213 GPUs perform like 1,192&lt;/head&gt;
    &lt;p&gt;A paper presented at SOSP 2025 details how token-level scheduling helped one GPU serve multiple LLMs, reducing demand from 1,192 to 213 H20s.&lt;/p&gt;
    &lt;p&gt;Alibaba Cloud claims its new Aegaeon pooling system reduces the number of Nvidia GPUs required to serve large language models by 82% during a multi-month beta test inside its Model Studio marketplace. The result, published in a peer-reviewed paper presented at the 2025 ACM Symposium on Operating Systems (SOSP) in Seoul, suggests that cloud providers may be able to extract significantly more inference capacity from existing silicon, especially in constrained markets like China, where the supply of Nvidia's latest H20s remains limited.&lt;/p&gt;
    &lt;p&gt;Unlike training-time breakthroughs that chase model quality or speed, Aegaeon is an inference-time scheduler designed to maximize GPU utilization across many models with bursty or unpredictable demand. Instead of pinning one accelerator to one model, Aegaeon virtualizes GPU access at the token level, allowing it to schedule tiny slices of work across a shared pool. This means one H20 could serve several different models simultaneously, with system-wide “goodput” — a measure of effective output — rising by as much as nine times compared to older serverless systems.&lt;/p&gt;
    &lt;p&gt;The system was tested in production over several months, according to the paper, which lists authors from both Peking University and Alibaba’s infrastructure division, including CTO Jingren Zhou. During that window, the number of GPUs needed to support dozens of different LLMs — ranging in size up to 72 billion parameters — fell from 1,192 to just 213.&lt;/p&gt;
    &lt;p&gt;While the paper does not break down which models contributed most to the savings, reporting by the South China Morning Post says the tests were conducted using Nvidia’s H20, one of the few accelerators still legally available to Chinese buyers under current U.S. export controls.&lt;/p&gt;
    &lt;p&gt;Alibaba says the gains came from two main techniques: Packing multiple models per GPU, and using a token-level autoscaler to dynamically allocate compute as output is generated, rather than reserving resources at the request level. In benchmarks, Aegaeon beat the goodput of ServerlessLLM and MuxServe by margins ranging from 1.5 times to 9 times.&lt;/p&gt;
    &lt;p&gt;Whether those savings translate outside Alibaba’s stack remains to be seen. Alibaba Cloud’s paper does not specify the exact network fabric used in the beta test, but we know the company offers its own eRDMA elastic RDMA network and has a record of building highly‑integrated GPU serving stacks, suggesting the results may depend on an optimized, vertically integrated environment.&lt;/p&gt;
    &lt;p&gt;Regardless, the result is likely to attract interest from other hyperscalers looking to stretch scarce accelerator fleets as inference demand continues to spike.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Luke James is a freelance writer and journalist. Although his background is in legal, he has a personal interest in all things tech, especially hardware and microelectronics, and anything regulatory.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.tomshardware.com/tech-industry/semiconductors/alibaba-says-new-pooling-system-cut-nvidia-gpu-use-by-82-percent"/><published>2025-10-20T12:31:22+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643357</id><title>Servo v0.0.1</title><updated>2025-10-20T16:44:05.437332+00:00</updated><content>&lt;doc fingerprint="946e1b5c97a1d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Servo is a prototype web browser engine written in the Rust language. It is currently developed on 64-bit macOS, 64-bit Linux, 64-bit Windows, 64-bit OpenHarmony, and Android.&lt;/p&gt;
    &lt;p&gt;Servo welcomes contribution from everyone. Check out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Servo Book for documentation&lt;/item&gt;
      &lt;item&gt;servo.org for news and guides&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Coordination of Servo development happens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here in the Github Issues&lt;/item&gt;
      &lt;item&gt;On the Servo Zulip&lt;/item&gt;
      &lt;item&gt;In video calls advertised in the Servo Project repo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more detailed build instructions, see the Servo book under Setting up your environment, Building Servo, Building for Android and Building for OpenHarmony.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download and install Xcode and &lt;code&gt;brew&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;curl&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Arch: &lt;code&gt;sudo pacman -S --needed curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Debian, Ubuntu: &lt;code&gt;sudo apt install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Fedora: &lt;code&gt;sudo dnf install curl&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Gentoo: &lt;code&gt;sudo emerge net-misc/curl&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Arch: &lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;uv&lt;/code&gt;:&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install &lt;code&gt;rustup&lt;/code&gt;:&lt;code&gt;curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;./mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;./mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download &lt;code&gt;uv&lt;/code&gt;,&lt;code&gt;choco&lt;/code&gt;, and&lt;code&gt;rustup&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Be sure to select Quick install via the Visual Studio Community installer&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;In the Visual Studio Installer, ensure the following components are installed: &lt;list rend="ul"&gt;&lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;code&gt;Microsoft.VisualStudio.Component.Windows{10, 11}SDK.{&amp;gt;=19041}&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;MSVC v143 - VS 2022 C++ x64/x86 build tools (Latest) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.Tools.x86.x64&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;C++ ATL for latest v143 build tools (x86 &amp;amp; x64) (&lt;code&gt;Microsoft.VisualStudio.Component.VC.ATL&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Windows 10/11 SDK (anything &amp;gt;= 10.0.19041.0) (&lt;/item&gt;
      &lt;item&gt;Restart your shell to make sure &lt;code&gt;cargo&lt;/code&gt;is available&lt;/item&gt;
      &lt;item&gt;Install the other dependencies: &lt;code&gt;.\mach bootstrap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Build servoshell: &lt;code&gt;.\mach build&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure that the following environment variables are set: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;ANDROID_NDK_ROOT&lt;/code&gt;:&lt;code&gt;$ANDROID_SDK_ROOT/ndk/28.2.13676358/&lt;/code&gt;&lt;code&gt;ANDROID_SDK_ROOT&lt;/code&gt;can be any directory (such as&lt;code&gt;~/android-sdk&lt;/code&gt;). All of the Android build dependencies will be installed there.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Install the latest version of the Android command-line tools to &lt;code&gt;$ANDROID_SDK_ROOT/cmdline-tools/latest&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Run the following command to install the necessary components: &lt;quote&gt;sudo $ANDROID_SDK_ROOT/cmdline-tools/latest/bin/sdkmanager --install \ "build-tools;34.0.0" \ "emulator" \ "ndk;28.2.13676358" \ "platform-tools" \ "platforms;android-33" \ "system-images;android-33;google_apis;x86_64"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow the instructions above for the platform you are building on to prepare the environment.&lt;/item&gt;
      &lt;item&gt;Depending on the target distribution (e.g. &lt;code&gt;HarmonyOS NEXT&lt;/code&gt;vs pure&lt;code&gt;OpenHarmony&lt;/code&gt;) the build configuration will differ slightly.&lt;/item&gt;
      &lt;item&gt;Ensure that the following environment variables are set &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;DEVECO_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;HarmonyOS NEXT&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_BASE_SDK_HOME&lt;/code&gt;(Required when targeting&lt;code&gt;OpenHarmony&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;OHOS_SDK_NATIVE&lt;/code&gt;(e.g.&lt;code&gt;${DEVECO_SDK_HOME}/default/openharmony/native&lt;/code&gt;or&lt;code&gt;${OHOS_BASE_SDK_HOME}/${API_VERSION}/native&lt;/code&gt;)&lt;/item&gt;&lt;item&gt;&lt;code&gt;SERVO_OHOS_SIGNING_CONFIG&lt;/code&gt;: Path to json file containing a valid signing configuration for the demo app.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Review the detailed instructions at Building for OpenHarmony.&lt;/item&gt;
      &lt;item&gt;The target distribution can be modified by passing &lt;code&gt;--flavor=&amp;lt;default|harmonyos&amp;gt;&lt;/code&gt;to&lt;code&gt;mach &amp;lt;build|package|install&amp;gt;&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://github.com/servo/servo"/><published>2025-10-20T12:55:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45643976</id><title>Modeling Others' Minds as Code</title><updated>2025-10-20T16:44:05.187889+00:00</updated><content>&lt;doc fingerprint="e99e25080df784af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 29 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Modeling Others' Minds as Code&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Accurate prediction of human behavior is essential for robust and safe human-AI collaboration. However, existing approaches for modeling people are often data-hungry and brittle because they either make unrealistic assumptions about rationality or are too computationally demanding to adapt rapidly. Our key insight is that many everyday social interactions may follow predictable patterns; efficient "scripts" that minimize cognitive load for actors and observers, e.g., "wait for the green light, then go." We propose modeling these routines as behavioral programs instantiated in computer code rather than policies conditioned on beliefs and desires. We introduce ROTE, a novel algorithm that leverages both large language models (LLMs) for synthesizing a hypothesis space of behavioral programs, and probabilistic inference for reasoning about uncertainty over that space. We test ROTE in a suite of gridworld tasks and a large-scale embodied household simulator. ROTE predicts human and AI behaviors from sparse observations, outperforming competitive baselines -- including behavior cloning and LLM-based methods -- by as much as 50% in terms of in-sample accuracy and out-of-sample generalization. By treating action understanding as a program synthesis problem, ROTE opens a path for AI systems to efficiently and effectively predict human behavior in the real-world.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://arxiv.org/abs/2510.01272"/><published>2025-10-20T13:54:38+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644276</id><title>How Soon Will the Seas Rise?</title><updated>2025-10-20T16:44:04.876235+00:00</updated><content>&lt;doc fingerprint="7f6c0127e0a10421"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Soon Will the Seas Rise?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In May 2014, NASA announced at a press conference that a portion of the West Antarctic Ice Sheet appeared to have reached a point of irreversible retreat. Glaciers flowing toward the sea at the periphery of the 2-kilometer-thick sheet of ice were losing ice faster than snowfall could replenish them, causing their edges to recede inland. With that, the question was no longer whether the West Antarctic Ice Sheet would disappear, but when. When those glaciers go, sea levels will rise by more than a meter, inundating land currently inhabited by 230 million people. And that would be just the first act before the collapse of the entire ice sheet, which could raise seas 5 meters and redraw the world’s coastlines.&lt;/p&gt;
    &lt;p&gt;At the time, scientists assumed that the loss of those glaciers would unfold over centuries. But in 2016, a bombshell study in Nature concluded that crumbling ice cliffs could trigger a runaway process of retreat, dramatically hastening the timeline. The Intergovernmental Panel on Climate Change (IPCC) took notice, establishing a sobering new worst-case scenario: By 2100, meltwater from Antarctica, Greenland and mountain glaciers combined with the thermal expansion of seawater could raise global sea levels by over 2 meters. And that would only be the beginning. If greenhouse gas emissions continue unabated, seas would rise a staggering 15 meters by 2300.&lt;/p&gt;
    &lt;p&gt;However, not all scientists are convinced by the runaway scenario. Thus, a tension has emerged over how long we have until West Antarctica’s huge glaciers vanish. If their retreat unfolds over centuries, humanity may have time to adapt. But if rapid destabilization begins in the coming decades through the controversial runaway process, the consequences could outpace our ability to respond. Scientists warn that major population centers — New York City, New Orleans, Miami and Houston — may not be ready.&lt;/p&gt;
    &lt;p&gt;“We’ve definitely not ruled this out,” said Karen Alley, a glaciologist at the University of Manitoba whose research supports the possibility of the runaway process. “But I’m not ready to say it’s going to happen soon. I’m also not going to say it can’t happen, either.”&lt;/p&gt;
    &lt;p&gt;For millennia, humanity has flourished along the shore, unaware that we were living in a geological fluke — an unusual spell of low seas. The oceans will return, but how soon? What does the science say about how ice sheets retreat, and therefore, about the future of our ports, our homes, and the billions who live near the coast?&lt;/p&gt;
    &lt;head rend="h2"&gt;Grounded by the Sea&lt;/head&gt;
    &lt;p&gt;In 1978, John Mercer, an eccentric glaciologist at Ohio State University who allegedly conducted fieldwork nude, was among the first to predict that global warming threatened the West Antarctic Ice Sheet. He based his theory on the ice sheet’s uniquely precarious relationship with the sea.&lt;/p&gt;
    &lt;p&gt;Bigger than Alaska and Texas combined, West Antarctica is split from the eastern half of the continent by the Transantarctic Mountains, whose peaks are buried to their chins in ice. Unlike in East Antarctica (and Greenland), where most ice rests on land high above the water, in West Antarctica the ice sheet has settled into a bowl-shaped depression deep below sea level, with seawater lapping at its edges. This makes West Antarctica’s ice sheet the most vulnerable to collapse.&lt;/p&gt;
    &lt;p&gt;A heaping dome of ice, the ice sheet flows outward under its own weight through tentaclelike glaciers. But the glaciers don’t stop at the shoreline; instead, colossal floating plates of ice hundreds of meters thick extend over the sea. These “ice shelves” float like giant rafts, tethered by drag forces and contact with underwater rises and ridges. They buttress the glaciers against an inexorable gravitational draw toward the sea.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;The critical frontline of the ice sheet’s vulnerability is the “grounding line,” where ice transitions from resting on the seafloor to floating as an ice shelf. As the relatively warm sea works its way below the protective shelves, it thins them from below, shifting the grounding line inland. The floating shelves fragment and break away. The upstream glaciers, now without their buttressing support, flow faster toward the sea. Meanwhile, seawater intrudes like an advancing army toward thicker ice, which rests on bedrock that slopes inward toward the bowl-like center of the continent.&lt;/p&gt;
    &lt;p&gt;“There’s a very serious message here,” said Hilmar Gudmundsson, a glaciologist at Northumbria University: As the grounding line marches inland toward ever-thicker ice in a process called marine ice sheet instability, “you will have a very sharp increase in global sea level, and it will happen very quickly.”&lt;/p&gt;
    &lt;p&gt;In 2002, scientists got a live view of how that process may play out. The Larsen B ice shelf, a floating mass off the Antarctic Peninsula roughly the size of Rhode Island, broke apart in just over a month, stunning scientists. Pooling surface meltwater had forced open cracks — a process called hydrofracturing — which splintered the shelf, the only barrier for the glaciers behind it. The glaciers began flowing seaward up to eight times faster. One of these, Crane Glacier, lost its cliff edge in a series of collapses over the course of 2003, causing it to shrink rapidly. What if something similar happened to far larger glaciers on the coast of West Antarctica, like Thwaites and Pine Island?&lt;/p&gt;
    &lt;p&gt;NASA Earth Observatory&lt;/p&gt;
    &lt;p&gt;In the years that followed, studies of ancient shorelines revealed a stunning sensitivity in the Earth system: It appeared that epochs only slightly warmer than today featured seas 6 to 9 meters above present-day levels.&lt;/p&gt;
    &lt;p&gt;In response, glaciologists Robert DeConto and David Pollard developed a bold new theory of ice sheet collapse. They created a computer simulation based on Larsen B’s breakup and Greenland’s calving glaciers that was also calibrated to the geologic past — projecting future melt that matched expectations derived from ancient sea levels.&lt;/p&gt;
    &lt;p&gt;Their 2016 study outlined a scenario of almost unimaginably quick ice loss and sea-level rise. In a process called marine ice cliff instability (MICI), cliffs taller than 90 meters at the edges of glaciers become unstable and collapse, exposing ever-thicker ice in a chain reaction that accelerates retreat. The model suggested that ice from Antarctica alone — before any additions from Greenland, mountain glaciers or thermal expansion — could raise the seas by more than a meter by 2100.&lt;/p&gt;
    &lt;p&gt;In a 2021 update that incorporated additional factors into the simulations, DeConto and colleagues revised that estimate sharply downward, projecting less than 40 centimeters of sea-level rise by the century’s end under high-emission scenarios. Yet even as the numbers have shifted, DeConto remains convinced of the MICI concept. “It’s founded on super basic physical and glaciological principles that are pretty undeniable,” he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mechanisms to Slow Retreat&lt;/head&gt;
    &lt;p&gt;After the 2016 study, the scientific community set out to test whether towering ice cliffs really could undergo runaway collapse. Many soon found reasons for doubt.&lt;/p&gt;
    &lt;p&gt;Few dispute the basic physics: If ice shelves like Larsen B collapse quickly and expose tall-enough cliffs on the glaciers behind them, those cliffs will indeed buckle under their own weight. “There’s a reason why skyscrapers are only so tall,” said Jeremy Bassis, a glaciologist and expert in fracture mechanics at the University of Michigan. However, critics argue that runaway cliff collapse hasn’t been seen in nature, and there might be good reasons why not.&lt;/p&gt;
    &lt;p&gt;“Yes, ice breaks off if you expose tall cliffs, but you have two stabilizing factors,” said Mathieu Morlighem, a glaciologist at Dartmouth College who led a 2024 study that identified these factors. First, as newly exposed glacier cliffs topple, the ice behind stretches and thins. As this happens, rapidly, “your ice cliff is going to be less of a tall cliff,” Morlighem said. Second, the flowing glacier brings more ice forward to replace what breaks off, slowing the cliff’s inland retreat and making a chain reaction of cliff toppling less likely.&lt;/p&gt;
    &lt;p&gt;NASA&lt;/p&gt;
    &lt;p&gt;Another study challenging the MICI scenario noted that breaking ice also tends to form a mélange, a dense, jumbled slurry of icebergs and sea ice. This frozen slurry can act as a retaining wall, at least temporarily stabilizing the cliffs against collapse.&lt;/p&gt;
    &lt;p&gt;The bedrock beneath the ice might also be a key player. “The solid Earth is having much bigger impacts on our understanding of sea-level change than we ever expected,” said Frederick Richards, a geodynamicist at Imperial College London. Scientists have long recognized that when glaciers melt, the land rebounds like a mattress relieved of weight. But this rebound has been mostly dismissed as too sluggish to matter for several centuries. Now, high-precision GPS and other geophysical data reveal rebound occurring over decades, even years.&lt;/p&gt;
    &lt;p&gt;Whether that’s good or bad depends on how quickly ice retreats. If it goes at a modest clip, the bedrock lifts the ice, reducing the amount of water that can lap away at it. But if retreat happens quickly enough through something like runaway cliff collapse, the Earth can’t keep up. A 2024 study showed that the bedrock still rises, but in that scenario it pushes meltwater into the ocean. “You’re actually getting more sea-level rise,” Richards said. “You’re pushing all this water out of a bowl underneath West Antarctica and into the global ocean system.”&lt;/p&gt;
    &lt;p&gt;Earth’s restlessness also affects models of ancient sea-level rise. In a 2023 study, Richards and colleagues found that Australia’s 3-million-year-old Pliocene shorelines had ridden the slow heave and sigh of Earth’s mantle, and that accounting for that vertical motion resulted in lower estimates for ancient sea levels. This matters, according to Richards, because the revised record is a better match for more conservative ice retreat models. “Hold on, guys,” he said. “We have to be a little bit careful. [Ancient] sea-level estimates might be overestimates, and therefore we might be overestimating how sensitive the ice sheets are.”&lt;/p&gt;
    &lt;p&gt;DeConto points to the Larsen B breakup and the crumbling of Greenland’s Jakobshavn Glacier as evidence to the contrary. Once Larsen B stopped holding back the Crane Glacier, he says, ice began breaking away faster than the glacier could replenish it. That is “really strong evidence that fracture can outpace flow.”&lt;/p&gt;
    &lt;head rend="h2"&gt;From Past to Future&lt;/head&gt;
    &lt;p&gt;“When I started my career, the question was whether Antarctica was growing or shrinking,” said Ted Scambos, a glaciologist at the University of Colorado, Boulder. The IPCC long held that the ice sheet would remain relatively stable through the 21st century, on the logic that rising temperatures would bring more snow, offsetting melt.&lt;/p&gt;
    &lt;p&gt;That assumption collapsed along with Larsen B in the early 2000s, and scientists soon came to a consensus that ice loss was well underway. Satellite observations revealed that glaciers along the Amundsen Sea, including Pine Island and Thwaites, were flowing faster than in previous decades. The ice sheet was not in balance. By the time NASA called the 2014 press conference, it was clear that many of West Antarctica’s enormous glaciers had been retreating steadily since the 1990s.&lt;/p&gt;
    &lt;p&gt;National Guard/Alamy&lt;/p&gt;
    &lt;p&gt;“It was the first time we had enough observations to say, hey, look, these grounding lines have been retreating year after year,” said Morlighem, a co-author on one of the studies presented at the press conference. This steady loss signaled that the glaciers would inevitably disappear. “In theory, if we turn off melt, we can stop it,” he noted. “But there’s absolutely zero chance we can do that.”&lt;/p&gt;
    &lt;p&gt;While the conversation has centered on how the sea will lap away at the ice shelves, some scientists are increasingly concerned about what’s happening up top, as warming air melts the ice sheet’s surface. Nicholas Golledge, a glaciologist at Victoria University of Wellington, sees West Antarctica today as transitioning to the status of Greenland: Most of Greenland’s marine-vulnerable ice has already vanished, and surface melt dominates. That process, Golledge believes, may soon play a bigger role in Antarctica than most models assume.&lt;/p&gt;
    &lt;p&gt;Pooling meltwater, for example, contributed to the Larsen B collapse. As the water trickles into crevasses, it lubricates the bedrock and sediments below, making everything more slippery. The Columbia University glaciologist Jonny Kingslake says these processes are oversimplified or omitted in numerical simulations. “If you ignore hydrology change, you are underestimating retreat,” he said.&lt;/p&gt;
    &lt;p&gt;Indeed, a 2020 study found that meltwater trickling into Antarctica’s ice shelves could infiltrate cracks and force them open, a precursor to marine ice cliff instability that DeConto and colleagues envisioned.&lt;/p&gt;
    &lt;p&gt;Depending on future emissions, the IPCC now projects an average sea-level rise of half a meter to 1 meter by 2100, a total that includes all melt sources and the expansion of warming water. The MICI process, if correct, could accelerate Antarctica’s contribution enough to double that overall rise. “There’s deep uncertainty around some of these processes,” said Robert Kopp, a climate scientist and science policy expert at Rutgers University. “The one thing we do know is that the more carbon dioxide we put into the atmosphere, the greater the risk.”&lt;/p&gt;
    &lt;p&gt;In Bassis’ view, “Whether it’s with marine ice cliff instability or marine ice sheet instability, it’s a bit of a distraction. By 2100, we will be talking about a coastline radically different than what I grew up with.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.quantamagazine.org/how-soon-will-the-seas-rise-20251020/"/><published>2025-10-20T14:27:17+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644328</id><title>BERT Is Just a Single Text Diffusion Step</title><updated>2025-10-20T16:44:04.646308+00:00</updated><content>&lt;doc fingerprint="35106edd82bc1c52"&gt;
  &lt;main&gt;
    &lt;p&gt;A while back, Google DeepMind unveiled Gemini Diffusion, an experimental language model that generates text using diffusion. Unlike traditional GPT-style models that generate one word at a time, Gemini Diffusion creates whole blocks of text by refining random noise step-by-step.&lt;/p&gt;
    &lt;p&gt;I read the paper Large Language Diffusion Models and was surprised to find that discrete language diffusion is just a generalization of masked language modeling (MLM), something we’ve been doing since 2018. The first thought I had was, “can we finetune a BERT-like model to do text generation?” I decided to try a quick proof of concept out of curiosity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: After I wrote the article I stumbled upon the paper DiffusionBERT which does essentially the same thing but with more rigorous testing! Check it out if this post interested you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;A Short History of Transformers#&lt;/head&gt;
    &lt;p&gt;The original Transformer architecture, introduced in 2017, was an encoder-decoder model. In 2018, researchers realized that the encoder and decoder components of the model could be separated (with the advent of BERT and GPT), and two distinct families of models were created:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Encoder-only models (BERT-style, bidirectional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Encoder models used masked language modeling (MLM) as a training objective: randomly mask out a subset of tokens of each input and train the encoder to reconstruct the missing tokens (fill in the blanks). The model sees the entire (partially masked) context at once and learns bidirectional representations. This architecture excelled at tasks requiring a full‐sentence (or paragraph) representation (e.g., classification and retrieval).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoder-only models (GPT-style, autoregressive)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Decoder models used next‐token prediction as a training objective: at each position $t$, predict the token at position $t + 1$ given all tokens up to $t$ as context. Only the left context is used to predict future values (unidirectional). This architecture excelled at generative tasks where you produce text one token at a time, such as open‐ended generation, summarization, and translation.&lt;/p&gt;
    &lt;p&gt;Originally, BERT saw immediate use in tasks such as classification, whereas GPT-style models didn’t become popular until later (due to initial limited capabilities). Eventually, the generation capabilities of autoregressive (decoder) transformers vastly improved. The general training objective of “next token prediction” means a much larger space of use cases when compared to encoder models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discrete Language Diffusion Models#&lt;/head&gt;
    &lt;p&gt;Diffusion models were first popularized in image generation. In image generation, diffusion models gradually add Gaussian noise to an image (forward process) and then train a neural network to iteratively denoise it (reverse process). A high‐level summary of continuous diffusion with images is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Forward process: Start from a clean image x₀, then add small amounts of (usually Gaussian) noise at each timestep until you end up with near‐pure noise.&lt;/item&gt;
      &lt;item&gt;Reverse process: Train a model (often a U‐Net) to predict the noise at each timestep, gradually recovering the original image in discrete denoising steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this idea to language means we need a way to add noise to text and then remove it in stages. The simplest way to do this is a masking‐based noise process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Forward (masking) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;At timestep t = 0, you have a fully uncorrupted text sequence.&lt;/item&gt;
          &lt;item&gt;At each subsequent timestep t &amp;gt; 0, randomly replace a fraction of tokens with a special &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;token according to a pre‐defined schedule (e.g., gradually increasing the masked proportion from 0% to 100%).&lt;/item&gt;
          &lt;item&gt;By the final timestep T, the entire sequence may be masked (all tokens are &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt;).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reverse (denoising) process:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Train a model (often a standard Transformer encoder) to predict the original token IDs given a partially masked sequence at timestep t.&lt;/item&gt;
          &lt;item&gt;This is akin to performing masked language modeling at varying mask rates: at early timesteps, only a few tokens are masked (easy to predict); at later timesteps, many tokens are masked (harder).&lt;/item&gt;
          &lt;item&gt;By chaining together predictions from high‐mask‐rate back down to zero, you can recover (or generate) a full sequence.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this discrete text diffusion framework, the model learns a likelihood bound on the data distribution by optimizing a sum of denoising losses over all timesteps, rather than a single MLM objective at a fixed mask probability.&lt;/p&gt;
    &lt;p&gt;As we can see, BERT’s masked language modeling objective is the same training objective as text diffusion, but just for a subset of masking rates. By introducing variable masking rates (from 0 to 1) and a scheduled sequence of denoising steps (inspired by diffusion theory), we can transform BERT’s masked language modeling objective into a full generative procedure.&lt;/p&gt;
    &lt;head rend="h2"&gt;RoBERTa Diffusion#&lt;/head&gt;
    &lt;p&gt;In 2019, RoBERTa was released. It was essentially just an enhancement of the original BERT model, with better hyperparameters, data training size, and a more simple training objective (MLM only, removed next sentence prediction).&lt;/p&gt;
    &lt;p&gt;Here we use the HuggingFace &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;dataset&lt;/code&gt; libraries to pull in the original RoBERTa weights, tokenizer, and the Trainer class to easily finetune the model on the WikiText dataset.
The main code (full code here) looks like this below:&lt;/p&gt;
    &lt;code&gt;# Load and tokenize dataset and instantiate the model
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

# Create the training args and Trainer instance
training_args = TrainingArguments(
    output_dir="finetuned-roberta-diffusion",
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    save_strategy="epoch",
    save_total_limit=1,
    logging_steps=200,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["validation"],
    data_collator=diffusion_collator, # custom implementation
    tokenizer=tokenizer,
)

# Train &amp;amp; save
trainer.train()
trainer.save_model("finetuned-roberta-diffusion")&lt;/code&gt;
    &lt;p&gt;Currently we have 10 diffusion steps, so we randomly sample a percentage $p$ out of &lt;code&gt;mask_probs&lt;/code&gt; (1.0, 0.9, 0.9, &amp;amp;mldr;, 0.1) and mask that percent of the tokens each batch.
The custom &lt;code&gt;diffusion_collator&lt;/code&gt; function (see code here) samples one mask-probability &lt;code&gt;p&lt;/code&gt; from &lt;code&gt;mask_probs&lt;/code&gt; per batch and sets each token to &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; with &lt;code&gt;p&lt;/code&gt; probability.&lt;/p&gt;
    &lt;p&gt;To be able to condition the generation on a “prompt”, we currently never mask the first 16 tokens. That means that during training, each step will always have the first 16 tokens as context for generation.&lt;/p&gt;
    &lt;p&gt;Simplified code for the &lt;code&gt;diffusion_collator&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;  def diffusion_collator(examples):
      batch = tokenizer.pad(examples, return_tensors="pt")

      # Randomly select masking probability for this batch
      mask_prob = random.choice([1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])

      # Never mask the first PREFIX_LEN tokens (preserved context)
      maskable_positions = batch.input_ids[:, PREFIX_LEN:]

      # Create random mask for the chosen probability
      mask = torch.rand(maskable_positions.shape) &amp;lt; mask_prob

      # Apply masking
      batch.input_ids[:, PREFIX_LEN:][mask] = tokenizer.mask_token_id
      batch.labels = batch.input_ids.clone()

      return batch&lt;/code&gt;
    &lt;p&gt;For inference, we start with an input which is a tensor of size 256 (since we are generating blocks of 256 tokens). The first 16 positions are the token ids that correspond to the prompt, and the last 240 are just &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens. We iterate through the denoising schedule and each step, we generate a prediction and then remask the sequence again. The process looks like this:&lt;/p&gt;
    &lt;code&gt;Step 0: [PREFIX] &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; ...     (100% masked)
Step 1: [PREFIX] will &amp;lt;mask&amp;gt; over &amp;lt;mask&amp;gt; control ...        (90% masked)
Step 2: [PREFIX] will begin &amp;lt;mask&amp;gt; greater control ...      (80% masked)
...
Step 10: [PREFIX] will begin to assert greater control ...  (0% masked - DONE)&lt;/code&gt;
    &lt;p&gt;Simplified code for generation looks like:&lt;/p&gt;
    &lt;code&gt;# Generate text through iterative denoising
for step, mask_prob in enumerate(mask_probs):
    # Forward pass: predict masked tokens
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = outputs.logits  # shape: (1, MAX_LEN, vocab_size)

    # For each masked position, sample from top-k/top-p filtered distribution
    for pos in range(PREFIX_LEN, MAX_LEN):
        if input_ids[0, pos] == tokenizer.mask_token_id:
            logits = predictions[0, pos, :]
            # Apply top-k and top-p filtering
            filtered_logits = top_k_top_p_filtering(logits, top_k=TOP_K, top_p=TOP_P)
            probs = F.softmax(filtered_logits, dim=-1)
            # Sample token
            sampled_token = torch.multinomial(probs, 1)
            input_ids[0, pos] = sampled_token

    # Re-mask a portion of non-prefix tokens for next iteration
    if mask_prob &amp;gt; 0:
        mask_indices = torch.rand(MAX_LEN - PREFIX_LEN) &amp;lt; mask_prob
        input_ids[0, PREFIX_LEN:][mask_indices] = tokenizer.mask_token_id&lt;/code&gt;
    &lt;p&gt;Here is an example output generation of the fine-tuned model after training on an H200 for 30 minutes (the first line is the initial prompt):&lt;/p&gt;
    &lt;code&gt;Following their victory in the French and Indian War, Britain began to assert
greater...

...dominion over Europe beginning about the early 19th. There conflict took
place on the island, between British and Irish Ireland. British officials 
administered British Ireland, a Celtic empire under the control of the Irish 
nationalist authorities, defined as a dominion of Britain. As the newly Fortic 
states acquired independent and powerful status, many former English colonies
played their part in this new, British @-@ controlled colonial system. Following
this period the Non @-@ Parliamentaryist Party won its influence in Britain in 
1890, led by the support of settlers from the Irish colonies. Looking inwards, 
Sinclair, Lewis questioned, and debated the need to describe " The New Britain "&lt;/code&gt;
    &lt;p&gt;The output looks surprisingly coherent! Most of the quirks present are actually just quirks from the formatting of WikiText (spaces around punctuation &lt;code&gt;"&lt;/code&gt;, turning hyphens &lt;code&gt;-&lt;/code&gt; into &lt;code&gt;@-@&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;Below is a comparison between our diffusion model and GPT-2:&lt;/p&gt;
    &lt;p&gt;We see GPT-2’s output is more coherent and slightly faster (~9 seconds vs ~13) but I’m pleasantly surprised with how good my simple implementation was. It is a good proof of concept, and with new approaches like AR-Diffusion and Skip-Step Diffusion (and a more optimized implementation), the quality and speed can be drastically improved.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;We’ve seen that masked language models like RoBERTa, originally designed for fill-in-the-blank tasks, can be repurposed into fully generative engines by interpreting variable-rate masking as a discrete diffusion process. By gradually corrupting text with &lt;code&gt;&amp;lt;MASK&amp;gt;&lt;/code&gt; tokens and training the model to iteratively denoise at increasing mask intensities, we effectively turn the standard MLM objective into a step-by-step generation procedure.&lt;/p&gt;
    &lt;p&gt;Even without architectural changes, a fine-tuned RoBERTa can generate coherent looking text after slightly modifying the training objective, validating the idea that BERT-style models are essentially just text diffusion models trained on one masking rate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://nathan.rs/posts/roberta-diffusion/"/><published>2025-10-20T14:31:16+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644654</id><title>Commodore 64 Ultimate</title><updated>2025-10-20T16:44:01.705404+00:00</updated><content>&lt;doc fingerprint="f780496976f87956"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Commodore 64 Ultimate: BASIC Beige&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Your childhood just leveled up.&lt;/p&gt;
      &lt;p&gt;The first official Commodore 64 in over 30 years is here - a faithful recreation of the original motherboard on FPGA hardware.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Play 10,000+ original games with more RAM &amp;amp; 48Mhz Turbo mode!&lt;/item&gt;
      &lt;item&gt;Plug in dusty old cartridges, CRT TVs, datasettes, or disk drives - it all works.‡&lt;/item&gt;
      &lt;item&gt;No disk drive needed: Load games using the File Browser from the supplied cassette-style USB thumbdrive, packed with 50+ titles - full games, licensed classics, music, demos, and Commodore's exclusive new sequel to our first ever game - Jupiter Lander. Or add your own!&lt;/item&gt;
      &lt;item&gt;Not just reborn but upgraded: More memory &amp;amp; faster speed options, multiple SID sound chips (add 2 original ones or use 8 x virtual SIDs!), keyboard macros, even modern printer support!&lt;/item&gt;
      &lt;item&gt;Enjoy HDMI clarity, Wi-Fi game transfer, USB convenience.&lt;/item&gt;
      &lt;item&gt;Hidden inside: the autographs/names of Commodore 64 creators past and present - including Albert 'Father of the Commodore 64' Charpentier - etched forever in the motherboard copper.*&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want something even rarer and more collectible? Inspired by the famous 1 millionth gold C64, the Gold Label Founders Edition adds 24k gold-plated badges, satin gold keys, a translucent amber case, commemorative Commodore gold seal 'share' certificate, "I Rebooted C=" tee, and a gold label holographic serial number sticker starting at 00000001 - that boosts the mission to reboot Commodore.&lt;/p&gt;
    &lt;p&gt;This isn’t tech that controls you. It invites you to play, learn, and create - just like it used to.&lt;/p&gt;
    &lt;p&gt;C64U. Retro gaming. Modern power.&lt;/p&gt;
    &lt;p&gt;‡ At least 99% compatible with all 80s/90s games, cartridges, and peripherals.&lt;/p&gt;
    &lt;p&gt;* Simulated render shown. Final product may vary. Specs may change. Autograph placeholders shown; names may replace some or all.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Commodore 64 Ultimate Computer&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Spiral-bound User Guide&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;64GB USB Cassette Drive “The Very Second” with 50+ full games including licensed classics, plus music, and demos (rated PEGI 3 provisional)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Quick Start Guide&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Stickers&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;HDMI Cable&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Power Supply (12V, 100–240V with worldwide adapters)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;All in the glossy original-style box&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Buy 1 get 2nd 10% off&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.commodore.net/product-page/commodore-64-ultimate-basic-beige-batch1"/><published>2025-10-20T14:55:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45644777</id><title>How much Anthropic and Cursor spend on Amazon Web Services</title><updated>2025-10-20T16:44:01.432788+00:00</updated><content>&lt;doc fingerprint="f4c935865aeafd0"&gt;
  &lt;main&gt;
    &lt;p&gt;So, I originally planned for this to be on my premium newsletter, but decided it was better to publish on my free one so that you could all enjoy it. If you liked it, please consider subscribing to support my work. Here’s $10 off the first year of annual.&lt;/p&gt;
    &lt;p&gt;I’ve also recorded an episode about this on my podcast Better Offline (RSS feed, Apple, Spotify, iHeartRadio), it’s a little different but both handle the same information, just subscribe and it'll pop up.&lt;/p&gt;
    &lt;p&gt;Over the last two years I have written again and again about the ruinous costs of running generative AI services, and today I’m coming to you with real proof.&lt;/p&gt;
    &lt;p&gt;Based on discussions with sources with direct knowledge of their AWS billing, I am able to disclose the amounts that AI firms are spending, specifically Anthropic and AI coding company Cursor, its largest customer.&lt;/p&gt;
    &lt;p&gt;I can exclusively reveal today Anthropic’s spending on Amazon Web Services for the entirety of 2024, and for every month in 2025 up until September, and that that Anthropic’s spend on compute far exceeds that previously reported.&lt;/p&gt;
    &lt;p&gt;Furthermore, I can confirm that through September, Anthropic has spent more than 100% of its estimated revenue (based on reporting in the last year) on Amazon Web Services, spending $2.66 billion on compute on an estimated $2.55 billion in revenue.&lt;/p&gt;
    &lt;p&gt;Additionally, Cursor’s Amazon Web Services bills more than doubled from $6.2 million in May 2025 to $12.6 million in June 2025, exacerbating a cash crunch that began when Anthropic introduced Priority Service Tiers, an aggressive rent-seeking measure that begun what I call the Subprime AI Crisis, where model providers begin jacking up the prices on their previously subsidized rates.&lt;/p&gt;
    &lt;p&gt;Although Cursor obtains the majority of its compute from Anthropic — with AWS contributing a relatively small amount, and likely also taking care of other parts of its business — the data seen reveals an overall direction of travel, where the costs of compute only keep on going up.&lt;/p&gt;
    &lt;p&gt;Let’s get to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Initial Important Details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I do not have all the answers! I am going to do my best to go through the information I’ve obtained and give you a thorough review and analysis. This information provides a revealing — though incomplete — insight into the costs of running Anthropic and Cursor, but does not include other costs, like salaries and compute obtained from other providers. I cannot tell you (and do not have insight into) Anthropic’s actual private moves. Any conclusions or speculation I make in this article will be based on my interpretations of the information I’ve received, as well as other publicly-available information.&lt;/item&gt;
      &lt;item&gt;I have used estimates of Anthropic’s revenue based on reporting across the last ten months. Any estimates I make are detailed and they are brief.&lt;/item&gt;
      &lt;item&gt;These costs are inclusive of every product bought on Amazon Web Services, including EC2, storage and database services (as well as literally everything else they pay for).&lt;/item&gt;
      &lt;item&gt;Anthropic works with both Amazon Web Services and Google Cloud for compute. I do not have any information about its Google Cloud spend.&lt;list rend="ul"&gt;&lt;item&gt;The reason I bring this up is that Anthropic’s revenue is already being eaten up by its AWS spend. It’s likely billions more in the hole from Google Cloud and other operational expenses.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;I have confirmed with sources that every single number I give around Anthropic and Cursor’s AWS spend is the final cash paid to Amazon after any discounts or credits.&lt;/item&gt;
      &lt;item&gt;While I cannot disclose the identity of my source, I am 100% confident in these numbers, and have verified their veracity with other sources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Anthropic’s Compute Costs Are Likely Much Higher Than Reported — $1.35 Billion in 2024 on AWS Alone&lt;/head&gt;
    &lt;p&gt;In February of this year, The information reported that Anthropic burned $5.6 billion in 2024, and made somewhere between $400 million and $600 million in revenue:&lt;/p&gt;
    &lt;quote&gt;It’s not publicly known how much revenue Anthropic generated in 2024, although its monthly revenue rose to about $80 million by the end of the year, compared to around $8 million at the start. That suggests full-year revenue in the $400 million to $600 million range.&lt;lb/&gt;…Anthropic told investors it expects to burn $3 billion this year, substantially less than last year, when it burned $5.6 billion. Last year’s cash burn was nearly $3 billion more than Anthropic had previously projected. That’s likely due to the fact that more than half of the cash burn came from a one-off payment to access the data centers that power its technology, according to one of the people who viewed the pitch.&lt;/quote&gt;
    &lt;p&gt;While I don’t know about prepayment for services, I can confirm from a source with direct knowledge of billing that Anthropic spent $1.35 billion on Amazon Web Services in 2024, and has already spent $2.66 billion on Amazon Web Services through the end of September.&lt;/p&gt;
    &lt;p&gt;Assuming that Anthropic made $600 million in revenue, this means that Anthropic spent $6.2 billion in 2024, leaving $4.85 billion in costs unaccounted for.&lt;/p&gt;
    &lt;p&gt;The Information’s piece also brings up another point:&lt;/p&gt;
    &lt;quote&gt;The costs to develop AI models accounted for a major portion of Anthropic’s expenses last year. The company spent $1.5 billion on servers for training AI models. OpenAI was on track to spend as much as $3 billion on training costs last year, though that figure includes additional expenses like paying for data.&lt;/quote&gt;
    &lt;p&gt;Before I go any further, I want to be clear that The Information’s reporting is sound, and I trust that their source (I have no idea who they are or what information was provided) was operating in good faith with good data.&lt;/p&gt;
    &lt;p&gt;However, Anthropic is telling people it spent $1.5 billion on just training when it has an Amazon Web Services bill of $1.35 billion, which heavily suggests that its actual compute costs are significantly higher than we thought, because, to quote SemiAnalysis, “a large share of Anthropic’s spending is going to Google Cloud.”&lt;/p&gt;
    &lt;p&gt;I am guessing, because I do not know, but with $4.85 billion of other expenses to account for, it’s reasonable to believe Anthropic spent an amount similar to its AWS spend on Google Cloud. I do not have any information to confirm this, but given the discrepancies mentioned above, this is an explanation that makes sense.&lt;/p&gt;
    &lt;p&gt;I also will add that there is some sort of undisclosed cut that Amazon gets of Anthropic’s revenue, though it’s unclear how much. According to The Information, “Anthropic previously told some investors it paid a substantially higher percentage to Amazon [than OpenAI’s 20% revenue share with Microsoft] when companies purchase Anthropic models through Amazon.”&lt;/p&gt;
    &lt;p&gt;I cannot confirm whether a similar revenue share agreement exists between Anthropic and Google.&lt;/p&gt;
    &lt;p&gt;This also makes me wonder exactly where Anthropic’s money is going.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Is Anthropic’s Money Going?&lt;/head&gt;
    &lt;p&gt;Anthropic has, based on what I can find, raised $32 billion in the last two years, starting out 2023 with a $4 billion investment from Amazon from September 2023 (bringing the total to $37.5 billion), where Amazon was named its “primary cloud provider” nearly eight months after Anthropic announced Google was Anthropic’s “cloud provider.,” which Google responded to a month later by investing another $2 billion on October 27 2023, “involving a $500 million upfront investment and an additional $1.5 billion to be invested over time,” bringing its total funding from 2023 to $6 billion.&lt;/p&gt;
    &lt;p&gt;In 2024, it would raise several more rounds — one in January for $750 million, another in March for $884.1 million, another in May for $452.3 million, and another $4 billion from Amazon in November 2024, which also saw it name AWS as Anthropic’s “primary cloud and training partner,” bringing its 2024 funding total to $6 billion.&lt;/p&gt;
    &lt;p&gt;In 2025 so far, it’s raised a $1 billion round from Google, a $3.5 billion venture round in March, opened a $2.5 billion credit facility in May, and completed a $13 billion venture round in September, valuing the company at $183 billion. This brings its total 2025 funding to $20 billion.&lt;/p&gt;
    &lt;p&gt;While I do not have Anthropic’s 2023 numbers, its spend on AWS in 2024 — around $1.35 billion — leaves (as I’ve mentioned) $4.85 billion in costs that are unaccounted for. The Information reports that costs for Anthropic’s 521 research and development staff reached $160 million in 2024, leaving 394 other employees unaccounted for (for 915 employees total), and also adding that Anthropic expects its headcount to increase to 1900 people by the end of 2025.&lt;/p&gt;
    &lt;p&gt;The Information also adds that Anthropic “expects to stop burning cash in 2027.”&lt;/p&gt;
    &lt;p&gt;This leaves two unanswered questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where is the rest of Anthropic’s money going?&lt;/item&gt;
      &lt;item&gt;How will it “stop burning cash” when its operational costs explode as its revenue increases?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An optimist might argue that Anthropic is just growing its pile of cash so it’s got a warchest to burn through in the future, but I have my doubts. In a memo revealed by WIRED, Anthropic CEO Dario Amodei stated that “if [Anthropic wanted] to stay on the frontier, [it would] gain a very large benefit from having access to this capital,” with “this capital” referring to money from the Middle East.&lt;/p&gt;
    &lt;p&gt;Anthropic and Amodei’s sudden willingness to take large swaths of capital from the Gulf States does not suggest that it’s not at least a little desperate for capital, especially given Anthropic has, according to Bloomberg, “recently held early funding talks with Abu Dhabi-based investment firm MGX” a month after raising $13 billion.&lt;/p&gt;
    &lt;p&gt;In my opinion — and this is just my gut instinct — I believe that it is either significantly more expensive to run Anthropic than we know, or Anthropic’s leaked (and stated) revenue numbers are worse than we believe. I do not know one way or another, and will only report what I know.&lt;/p&gt;
    &lt;head rend="h1"&gt;How Much Did Anthropic and Cursor Spend On Amazon Web Services In 2025?&lt;/head&gt;
    &lt;p&gt;So, I’m going to do this a little differently than you’d expect, in that I’m going to lay out how much these companies spent, and draw throughlines from that spend to its reported revenue numbers and product announcements or events that may have caused its compute costs to increase.&lt;/p&gt;
    &lt;p&gt;I’ve only got Cursor’s numbers from January through September 2025, but I have Anthropic’s AWS spend for both the entirety of 2024 and through September 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Does “Annualized” Mean?&lt;/head&gt;
    &lt;p&gt;So, this term is one of the most abused terms in the world of software, but in this case, I am sticking to the idea that it means “month times 12.” So, if a company made $10m in January, you would say that its annualized revenue is $120m. Obviously, there’s a lot of (when you think about it, really obvious) problems with this kind of reporting — and thus, you only ever see it when it comes to pre-IPO firms — but that’s besides the point.&lt;/p&gt;
    &lt;p&gt;I give you this explanation because, when contrasting Anthropic’s AWS spend with its revenues, I’ve had to work back from whatever annualized revenues were reported for that month.&lt;/p&gt;
    &lt;head rend="h1"&gt;Anthropic’s Amazon Web Services Spend In 2024 - $1.359 Billion - Estimated Revenue $400 Million to $600 Million&lt;/head&gt;
    &lt;p&gt;Anthropic’s 2024 revenues are a little bit of a mystery, but, as mentioned above, The Information says it might be between $400 million and $600 million.&lt;/p&gt;
    &lt;p&gt;Here’s its monthly AWS spend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 2024 - $52.9 million&lt;/item&gt;
      &lt;item&gt;February 2024 - $60.9 million&lt;/item&gt;
      &lt;item&gt;March 2024 - $74.3 million&lt;/item&gt;
      &lt;item&gt;April 2024 - $101.1 million&lt;/item&gt;
      &lt;item&gt;May 2024 - $100.1 million&lt;/item&gt;
      &lt;item&gt;June 2024 - $101.8 million&lt;/item&gt;
      &lt;item&gt;July 2024 - $118.9 million&lt;/item&gt;
      &lt;item&gt;August 2024 - $128.8 million&lt;/item&gt;
      &lt;item&gt;September 2024 - $127.8 million&lt;/item&gt;
      &lt;item&gt;October 2024 - $169.6 million&lt;/item&gt;
      &lt;item&gt;November 2024 - $146.5 million&lt;/item&gt;
      &lt;item&gt;December 2024 - $176.1 million&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Analysis: Anthropic Spent At Least 200% of Its 2024 Revenue On Amazon Web Services In 2024&lt;/head&gt;
    &lt;p&gt;I’m gonna be nice here and say that Anthropic made $600 million in 2024 — the higher end of The Information’s reporting — meaning that it spent around 226% of its revenue ($1.359 billion) on Amazon Web Services.&lt;/p&gt;
    &lt;p&gt;[Editor's note: this copy originally had incorrect maths on the %. Fixed now.]&lt;/p&gt;
    &lt;head rend="h1"&gt;Anthropic’s Amazon Web Services Spend In 2025 Through September 2025 - $2.66 Billion - Estimated Revenue Through September $2.55 Billion - 104% Of Revenue Spent on AWS&lt;/head&gt;
    &lt;p&gt;Thanks to my own analysis and reporting from outlets like The Information and Reuters, we have a pretty good idea of Anthropic’s revenues for much of the year. That said, July, August, and September get a little weirder, because we’re relying on “almosts” and “approachings,” as I’ll explain as we go.&lt;/p&gt;
    &lt;p&gt;I’m also gonna do an analysis on a month-by-month basis, because it’s necessary to evaluate these numbers in context.&lt;/p&gt;
    &lt;head rend="h3"&gt;January 2025 - $188.5 million In AWS Spend, $72.91 or $83 Million In Revenue - 227% Of Revenue Spent on AWS&lt;/head&gt;
    &lt;p&gt;In this month, Anthropic’s reported revenue was somewhere from $875 million to $1 billion annualized, meaning either $72.91 million or $83 million for the month of January.&lt;/p&gt;
    &lt;head rend="h3"&gt;February 2025 - $181.2 million in AWS Spend, $116 Million In Revenue - 156% Of Revenue Spent On AWS - 181% Of Revenue Spent On AWS&lt;/head&gt;
    &lt;p&gt;In February, as reported by The Information, Anthropic hit $1.4 billion annualized revenue, or around $116 million each month.&lt;/p&gt;
    &lt;head rend="h3"&gt;March 2025 - $240.3 million in AWS Spend - $166 Million In Revenue - 144% Of Revenue Spent On AWS - Launch of Claude Sonnet 3.7 &amp;amp; Claude Code Research Preview (February 24)&lt;/head&gt;
    &lt;p&gt;In March, as reported by Reuters, Anthropic hit $2 billion in annualized revenue, or $166 million in revenue.&lt;/p&gt;
    &lt;p&gt;Because February is a short month, and the launch took place on February 24 2025, I’m considering the launches of Claude 3.7 Sonnet and Claude Code’s research preview to be a cost burden in the month of March.&lt;/p&gt;
    &lt;p&gt;And man, what a burden! Costs increased by $59.1 million, primarily across compute categories, but with a large ($2 million since January) increase in monthly costs for S3 storage.&lt;/p&gt;
    &lt;head rend="h3"&gt;April 2025 - $221.6 million in AWS Spend - $204 Million In Revenue - 108% Of Revenue Spent On AWS&lt;/head&gt;
    &lt;p&gt;I estimate, based on a 22.4% compound growth rate, that Anthropic hit around $2.44 billion in annualized revenue in April, or $204 million in revenue.&lt;/p&gt;
    &lt;p&gt;Interestingly, this was the month where Anthropic launched its $100 and $200 dollar a month “Max” plans, and it doesn’t seem to have dramatically increased its costs. Then again, Max is also the gateway to things like Claude Code, which I’ll get to shortly.&lt;/p&gt;
    &lt;head rend="h3"&gt;May 2025 - $286.7 million in AWS Spend - $250 Million In Revenue - 114% Of Revenue Spent On AWS - Sonnet 4, Opus 4, General Availability Of Claude Code (May 22) Service Tiers (May 30)&lt;/head&gt;
    &lt;p&gt;In May, as reported by CNBC, Anthropic hit $3 billion in annualized revenue, or $250 million in monthly average revenue.&lt;/p&gt;
    &lt;p&gt;This was a big month for Anthropic, with two huge launches on May 22 2025 — its new, “more powerful” models Claude Sonnet and Opus 4, as well as the general availability of its AI coding environment Claude Code.&lt;/p&gt;
    &lt;p&gt;Eight days later, on May 30 2025, a page on Anthropic's API documentation appeared for the first time: "Service Tiers":&lt;/p&gt;
    &lt;quote&gt;Different tiers of service allow you to balance availability, performance, and predictable costs based on your application’s needs.&lt;lb/&gt;We offer three service tiers:&lt;lb/&gt;- Priority Tier: Best for workflows deployed in production where time, availability, and predictable pricing are important&lt;lb/&gt;Standard: Best for bursty traffic, or for when you’re trying a new idea&lt;lb/&gt;Batch: Best for asynchronous workflows which can wait or benefit from being outside your normal capacity&lt;/quote&gt;
    &lt;p&gt;Accessing the priority tier requires you to make an up-front commitment to Anthropic, and said commitment is based on a number of months (1, 3, 6 or 12) and the number of input and output tokens you estimate you will use each minute.&lt;/p&gt;
    &lt;head rend="h4"&gt;What’s a Priority Tier? Why Is It Significant?&lt;/head&gt;
    &lt;p&gt;As I’ll get into in my June analysis, Anthropic’s Service Tiers exist specifically for it to “guarantee” your company won’t face rate limits or any other service interruptions, requiring a minimum spend, minimum token throughput, and for you to pay higher rates when writing to the cache — which is, as I’ll explain, a big part of running an AI coding product like Cursor.&lt;/p&gt;
    &lt;p&gt;Now, the jump in costs — $65.1 million or so between April and May — likely comes as a result of the final training for Sonnet and Opus 4, as well as, I imagine, some sort of testing to make sure Claude Code was ready to go.&lt;/p&gt;
    &lt;head rend="h3"&gt;June 2025 - $321.4 million in AWS Spend - $333 Million In Revenue - 96.5% Of Revenue Spent On AWS - Anthropic Cashes In On Service Tier Tolls That Add An Increased Charge For Prompt Caching, Directly Targeting Companies Like Cursor&lt;/head&gt;
    &lt;p&gt;In June, as reported by The Information, Anthropic hit $4 billion in annualized revenue, or $333 million.&lt;/p&gt;
    &lt;p&gt;Anthropic’s revenue spiked by $83 million this month, and so did its costs by $34.7 million.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anthropic Started The Subprime AI Crisis In June 2025, Increasing Costs On Its Largest Customer, Doubling Its AWS Spend In A Month&lt;/head&gt;
    &lt;p&gt;I have, for a while, talked about the Subprime AI Crisis, where big tech and companies like Anthropic, after offering subsidized pricing to entice in customers, raise the rates on their customers to start covering more of their costs, leading to a cascade where businesses are forced to raise their prices to handle their new, exploding costs.&lt;/p&gt;
    &lt;p&gt;And I was god damn right. Or, at least, it sure looks like I am. I’m hedging, forgive me. I cannot say for certain, but I see a pattern.&lt;/p&gt;
    &lt;p&gt;It’s likely the June 2025 spike in revenue came from the introduction of service tiers, which specifically target prompt caching, increasing the amount of tokens you’re charged for as an enterprise customer based on the term of the contract, and your forecast usage.&lt;/p&gt;
    &lt;quote&gt;You see, Anthropic specifically notes on its "service tiers" page that requests at the priority tier are "prioritized over all other requests to Anthropic," a rent-seeking measure that effectively means a company must either:&lt;lb/&gt;- Commit to at least a month, though likely 3-12 months of specific levels of input and output tokens a minute, based on what they believe they will use in the future, regardless of whether they do.&lt;lb/&gt;- Accept that access to Anthropic models will be slower at some point, in some way that Anthropic can't guarantee.Furthermore, the way that Anthropic is charging almost feels intentionally built to fuck over any coding startup that would use its service. Per the service tier page, Anthropic charges 1.25 for every time you write a token to the cache with a 5 minute TTL — or 2 tokens if you have a 1 hour TTL — and a longer cache is effectively essential for any background task where an agent will be working for more than 5 minutes, such as restructuring a particularly complex series of code, you know, the exact things that Cursor is well-known and marketed to do.&lt;lb/&gt;Furthermore, the longer something is in the cache, the better autocomplete suggestions for your code will be. It's also important to remember you're, at some point, caching the prompts themselves — so the instructions of what you want Cursor to do, meaning that the more complex the operation, the more expensive it'll now be for Cursor to provide the service with reasonable uptime.&lt;/quote&gt;
    &lt;p&gt;Cursor, as Anthropic’s largest client (the second largest being Github Copilot), represents a material part of its revenue, and its surging popularity meant it was sending more and more revenue Anthropic’s way. Anysphere, the company that develops Cursor, hit $500 million annualized revenue ($41.6 million) by the end of May, which Anthropic chose to celebrate by increasing its costs.&lt;/p&gt;
    &lt;p&gt;On June 16 2025, Cursor launched a $200-a-month “Ultra” plan, as well as dramatic changes to its $20-a-month Pro pricing that, instead of offering 500 “fast” responses using models from Anthropic and OpenAI, now effectively provided you with “at least” whatever you paid a month (so $20-a-month got at least $20 of credit), massively increasing the costs for users, with one calling the changes a “rug pull” after spending $71 in a single day.&lt;/p&gt;
    &lt;p&gt;As I’ll get to later in the piece, Cursor’s costs exploded from $6.19 million in May 2025 to $12.67 million in June 2025, and I believe this is a direct result of Anthropic’s sudden and aggressive cost increases.&lt;/p&gt;
    &lt;p&gt;Similarly, Replit, another AI coding startup, moved to “Effort-Based Pricing” on June 18 2025. I have not got any information around its AWS spend.&lt;/p&gt;
    &lt;p&gt;I’ll get into this a bit later, but I find this whole situation disgusting.&lt;/p&gt;
    &lt;head rend="h3"&gt;July 2025 $323.2 million in AWS Spend - $416 Million In Revenue - 77.7% Of Revenue Spent On AWS&lt;/head&gt;
    &lt;p&gt;In July, as reported by Bloomberg, Anthropic hit $5 billion in annualized revenue, or $416 million.&lt;/p&gt;
    &lt;p&gt;While July wasn’t a huge month for announcements, it was allegedly the month that Claude Code was generating “nearly $400 million in annualized revenue,” or $33.3 million (according to The Information, who says Anthropic was “approaching” $5 billion in annualized revenue - which likely means LESS than that - but I’m going to go with the full $5 billion annualized for sake of fairness.&lt;/p&gt;
    &lt;p&gt;There’s roughly an $83 million bump in Anthropic’s revenue between June and July 2025, and I think Claude Code and its new rates are a big part of it. What’s fascinating is that cloud costs didn’t increase too much — by only $1.8 million, to be specific.&lt;/p&gt;
    &lt;head rend="h3"&gt;August 2025 - $383.7 million in AWS Spend - $416 Million In Revenue - 92% Of Revenue Spent On AWS&lt;/head&gt;
    &lt;p&gt;In August, according to Anthropic, its run-rate “reached over $5 billion,” or in or around $416 million. I am not giving it anything more than $5 billion, especially considering in July Bloomberg’s reporting said “about $5 billion.”&lt;/p&gt;
    &lt;p&gt;Costs grew by $60.5 this month, potentially due to the launch of Claude Opus 4.1, Anthropic’s more aggressively expensive model, though revenues do not appear to have grown much along the way.&lt;/p&gt;
    &lt;p&gt;Yet what’s very interesting is that Anthropic — starting August 28 — launched weekly rate limits on its Claude Pro and Max plans. I wonder why?&lt;/p&gt;
    &lt;head rend="h3"&gt;September 2025 - $518.9 million in AWS Spend - $583 Million In Revenue - 88.9% Of Revenue Spent On AWS&lt;/head&gt;
    &lt;p&gt;Oh fuck! Look at that massive cost explosion!&lt;/p&gt;
    &lt;p&gt;Anyway, according to Reuters, Anthropic’s run rate is “approaching $7 billion” in October, and for the sake of fairness, I am going to just say it has $7 billion annualized, though I believe this number to be lower. “Approaching” can mean a lot of different things — $6.1 billion, $6.5 billion — and because I already anticipate a lot of accusations of “FUD,” I’m going to err on the side of generosity.&lt;/p&gt;
    &lt;p&gt;If we assume a $6.5 billion annualized rate, that would make this month’s revenue $541.6 million, or 95.8% of its AWS spend.&lt;/p&gt;
    &lt;p&gt;Nevertheless, Anthropic’s costs exploded in the space of a month by $135.2 million (35%) - likely due to the fact that users, as I reported in mid-July, were costing it thousands or tens of thousands of dollars in compute, a problem it still faces to this day, with VibeRank showing a user currently spending $51,291 in a calendar month on a $200-a-month subscription.&lt;/p&gt;
    &lt;p&gt;If there were other costs, they likely had something to do with the training runs for the launches of Sonnet 4.5 on September 29 2025 and Haiku 4.5 in October 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anthropic’s Monthly AWS Costs Have Increased By 174% Since January - And With Its Potential Google Cloud Spend and Massive Staff, Anthropic Is Burning Billions In 2025&lt;/head&gt;
    &lt;p&gt;While these costs only speak to one part of its cloud stack — Anthropic has an unknowable amount of cloud spend on Google Cloud, and the data I have only covers AWS — it is simply remarkable how much this company spends on AWS, and how rapidly its costs seem to escalate as it grows.&lt;/p&gt;
    &lt;p&gt;Though things improved slightly over time — in that Anthropic is no longer burning over 200% of its revenue on AWS alone — these costs have still dramatically escalated, and done so in an aggressive and arbitrary manner.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anthropic’s AWS Costs Increase Linearly With Revenue, Consuming The Majority Of Each Dollar Anthropic Makes - As A Reminder, It Also Spends Hundreds Of Millions Or Billions On Google Cloud Too&lt;/head&gt;
    &lt;p&gt;So, I wanted to visualize this part of the story, because I think it’s important to see the various different scenarios.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Estimate of Anthropic’s Potential Cloud Compute Spend Through September&lt;/head&gt;
    &lt;p&gt;THE NUMBERS I AM USING ARE ESTIMATES CALCULATED BASED ON 25%, 50% and 100% OF THE AMOUNTS THAT ANTHROPIC HAS SPENT ON AMAZON WEB SERVICES THROUGH SEPTEMBER.&lt;/p&gt;
    &lt;p&gt;I apologize for all the noise, I just want it to be crystal clear what you see next.&lt;/p&gt;
    &lt;p&gt;As you can see, all it takes is for Anthropic to spend (I am estimating) around 25% of its Amazon Web Services bills (for a total of around $3.33 billion in compute costs through the end of September) to savage any and all revenue ($2.55 billion) it’s making.&lt;/p&gt;
    &lt;p&gt;Assuming Anthropic spends half of its AWS spend on Google Cloud, this number climbs to $3.99 billion, and if you assume - and to be clear, this is an estimate - that it spends around the same on both Google Cloud and AWS, Anthropic has spent $5.3 billion on compute through the end of September.&lt;/p&gt;
    &lt;p&gt;I can’t tell you which it is, just that we know for certain that Anthropic is spending money on Google Cloud, and because Google owns 14% of the company — rivalling estimates saying Amazon owns around 15-19% — it’s fair to assume that there’s a significant spend.&lt;/p&gt;
    &lt;head rend="h1"&gt;Anthropic’s Costs Are Out Of Control, Consistently And Aggressively Outpacing Revenue - And Amazon’s Revenue from Anthropic Of $2.66 Billion Is 2.5% Of Its 2025 Capex&lt;/head&gt;
    &lt;p&gt;I have sat with these numbers for a great deal of time, and I can’t find any evidence that Anthropic has any path to profitability outside of aggressively increasing the prices on their customers to the point that its services will become untenable for consumers and enterprise customers alike.&lt;/p&gt;
    &lt;p&gt;As you can see from these estimated and reported revenues, Anthropic’s AWS costs appear to increase in a near-linear fashion with its revenues, meaning that the current pricing — including rent-seeking measures like Priority Service Tiers — isn’t working to meet the burden of its costs.&lt;/p&gt;
    &lt;p&gt;We do not know its Google Cloud spend, but I’d be shocked if it was anything less than 50% of its AWS bill. If that’s the case, Anthropic is in real trouble - the cost of the services underlying its business increase the more money they make.&lt;/p&gt;
    &lt;p&gt;It’s becoming increasingly apparent that Large Language Models are not a profitable business. While I cannot speak to Amazon Web Services’ actual costs, it’s making $2.66 billion from Anthropic, which is the second largest foundation model company in the world.&lt;/p&gt;
    &lt;p&gt;Is that really worth $105 billion in capital expenditures? Is that really worth building a giant 1200 acre data center in Indiana with 2.2GW of electricity?&lt;/p&gt;
    &lt;p&gt;What’s the plan, exactly? Let Anthropic burn money for the foreseeable future until it dies, and then pick up the pieces? Wait until Wall Street gets mad at you and then pull the plug?&lt;/p&gt;
    &lt;p&gt;Who knows.&lt;/p&gt;
    &lt;p&gt;But let’s change gears and talk about Cursor — Anthropic’s largest client and, at this point, a victim of circumstance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Cursor’s Amazon Web Services Spend In 2025 Through September 2025 - $69.99 Million&lt;/head&gt;
    &lt;head rend="h3"&gt;An Important Note About Cursor’s Compute Spend&lt;/head&gt;
    &lt;p&gt;Amazon sells Anthropic’s models through Amazon Bedrock, and I believe that AI startups are compelled to spend some of their AI model compute costs through Amazon Web Services. Cursor also sends money directly to Anthropic and OpenAI, meaning that these costs are only one piece of its overall compute costs. In any case, it’s very clear that Cursor buys some degree of its Anthropic model spend through Amazon.&lt;/p&gt;
    &lt;p&gt;I’ll also add that Tom Dotan of Newcomer reported a few months ago that an investor told him that “Cursor is spending 100% of its revenue on Anthropic.”&lt;/p&gt;
    &lt;p&gt;Unlike Anthropic, we lack thorough reporting of the month-by-month breakdown of Cursor’s revenues. I will, however, mention them in the month I have them.&lt;/p&gt;
    &lt;p&gt;For the sake of readability — and because we really don’t have much information on Cursor’s revenues beyond a few months — I’m going to stick to a bullet point list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another Note About Cursor’s AWS Spend - It Likely Funnels Some Model Spend Through AWS, But The Majority Goes Directly To Providers Like Anthropic&lt;/head&gt;
    &lt;p&gt;As discussed above, Cursor announced (along with their price change and $200-a-month plan) several multi-year partnerships with xAI, Anthropic, OpenAI and Google, suggesting that it has direct agreements with Anthropic itself versus one with AWS to guarantee “this volume of compute at a predictable price.”&lt;/p&gt;
    &lt;p&gt;Based on its spend with AWS, I do not see a strong “minimum” spend that would suggest that they have a similar deal with Amazon — likely because Amazon handles more than its infrastructure than just compute, but incentivizes it to spend on Anthropic’s models through AWS by offering discounts, something I’ve confirmed with a source.&lt;/p&gt;
    &lt;p&gt;In any case, here’s what Cursor spent on AWS.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 2025 - $1.459 million&lt;list rend="ul"&gt;&lt;item&gt;This, apparently, is the month that Cursor hit $100 million annualized revenue — or $8.3 million, meaning it spent 17.5% of its revenue on AWS.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;February 2025 - $2.47 million&lt;/item&gt;
      &lt;item&gt;March 2025 - $4.39 million&lt;/item&gt;
      &lt;item&gt;April 2025 - $4.74 million&lt;list rend="ul"&gt;&lt;item&gt;Cursor hit $200 million annualized ($16.6 million) at the end of March 2025, according to The Information, working out to spending 28% of its revenue on AWS.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;May 2025 - $6.19 million&lt;/item&gt;
      &lt;item&gt;June 2025 - $12.67 million&lt;list rend="ul"&gt;&lt;item&gt;So, Bloomberg reported that Cursor hit $500 million on June 5 2025, along with raising a $900 million funding round. Great news! Turns out it’d need to start handing a lot of that to Anthropic.&lt;/item&gt;&lt;item&gt;This was, as I’ve discussed above, the month when Anthropic forced it to adopt “Service Tiers”. I go into detail about the situation here, but the long and short of it is that Anthropic increased the amount of tokens you burned by writing stuff to the cache (think of it like RAM in a computer), and AI coding startups are very cache heavy, meaning that Cursor immediately took on what I believed would be massive new costs. As I discuss in what I just linked, this led Cursor to aggressively change its product, thereby vastly increasing its customers’ costs if they wanted to use the same service.&lt;/item&gt;&lt;item&gt;That same month, Cursor’s AWS costs — which I believe are the minority of its cloud compute costs — exploded by 104% (or by $6.48 million), and never returned to their previous levels.&lt;/item&gt;&lt;item&gt;It’s conceivable that this surge is due to the compute-heavy nature of the latest Claude 4 models released that month — or, perhaps, Cursor sending more of its users to other models that it runs on Bedrock.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;July 2025 - $15.5 million&lt;list rend="ul"&gt;&lt;item&gt;As you can see, Cursor’s costs continue to balloon in July, and I am guessing it’s because of the Service Tiers situation — which, I believe, indirectly resulted in Cursor pushing more users to models that it runs on Amazon’s infrastructure.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;August 2025 - $9.67 million&lt;list rend="ul"&gt;&lt;item&gt;So, I can only guess as to why there was a drop here. User churn? It could be the launch of GPT-5 on Cursor, which gave users a week of free access to OpenAI’s new models.&lt;/item&gt;&lt;item&gt;What’s also interesting is that this was the month when Cursor announced that its previously free “auto” model (where Cursor would select the best available premium model or its own model) would now bill at “competitive token rates,” by which I mean it went from charging nothing to $1.25 per million input and $6 per million output tokens. This change would take effect on September 15 2025.&lt;/item&gt;&lt;item&gt;On August 10 2025, Tom Dotan of Newcomer reported that Cursor was “well above” $500 million in annualized revenue based on commentary from two sources.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;September 2025 - $12.91 million&lt;list rend="ul"&gt;&lt;item&gt;Per the above, this is the month when Cursor started charging for its “auto” model.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;What Anthropic May Have Done To Cursor Is Disgusting - And Is A Preview Of What’s To Come For AI Startups&lt;/head&gt;
    &lt;p&gt;When I wrote that Anthropic and OpenAI had begun the Subprime AI Crisis back in July, I assumed that the increase in costs was burdensome, but having the information from its AWS bills, it seems that Anthropic’s actions directly caused Cursor’s costs to explode by over 100%.&lt;/p&gt;
    &lt;p&gt;While I can’t definitively say “this is exactly what did it,” the timelines match up exactly, the costs have never come down, Amazon offers provisioned throughput, and, more than likely, Cursor needs to keep a standard of uptime similar to that of Anthropic’s own direct API access.&lt;/p&gt;
    &lt;p&gt;If this is what happened, it’s deeply shameful.&lt;/p&gt;
    &lt;p&gt;Cursor, Anthropic’s largest customer, in the very same month it hit $500 million in annualized revenue, immediately had its AWS and Anthropic-related costs explode to the point that it had to dramatically reduce the value of its product just as it hit the apex of its revenue growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anthropic Timed Its Rent-Seeking Service Tier Price Increases on Cursor With The Launch Of A Competitive Product - Which Is What’s Coming To Any AI Startup That Builds On Top Of Its Products&lt;/head&gt;
    &lt;p&gt;It’s very difficult to see Service Tiers as anything other than an aggressive rent-seeking maneuver.&lt;/p&gt;
    &lt;p&gt;Yet another undiscussed part of the story is that the launch of Claude 4 Opus and Sonnet — and the subsequent launch of Service Tiers — coincided with the launch of Claude Code, a product that directly competes with Cursor, without the burden of having to pay itself for the cost of models or, indeed, having to deal with its own “Service Tiers.”&lt;/p&gt;
    &lt;p&gt;Anthropic may have increased the prices on its largest client at the time it was launching a competitor, and I believe that this is what awaits any product built on top of OpenAI or Anthropic’s models.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Subprime AI Crisis Is Real, And It Can Hurt You&lt;/head&gt;
    &lt;p&gt;I realize this has been a long, number-stuffed article, but the long-and-short of it is simple: Anthropic is burning all of its revenue on compute, and Anthropic will willingly increase the prices on its customers if it’ll help it burn less money, even though that doesn’t seem to be working.&lt;/p&gt;
    &lt;p&gt;What I believe happened to Cursor will likely happen to every AI-native company, because in a very real sense, Anthropic’s products are a wrapper for its own models, except it only has to pay the (unprofitable) costs of running them on Amazon Web Services and Google Cloud.&lt;/p&gt;
    &lt;p&gt;As a result, both OpenAI and Anthropic can (and may very well!) devour the market of any company that builds on top of their models.&lt;/p&gt;
    &lt;p&gt;OpenAI may have given Cursor free access to its GPT-5 models in August, but a month later on September 15 2025 it debuted massive upgrades to its competitive “Codex” platform.&lt;/p&gt;
    &lt;p&gt;Any product built on top of an AI model that shows any kind of success can be cloned immediately by OpenAI and Anthropic, and I believe that we’re going to see multiple price increases on AI-native companies in the next few months. After all, OpenAI already has its own priority processing product, which it launched shortly after Anthropic’s in June.&lt;/p&gt;
    &lt;p&gt;The ultimate problem is that there really are no winners in this situation. If Anthropic kills Cursor through aggressive rent-seeking, that directly eats into its own revenues. If Anthropic lets Cursor succeed, that’s revenue, but it’s also clearly unprofitable revenue. Everybody loses, but nobody loses more than Cursor’s (and other AI companies’) customers.&lt;/p&gt;
    &lt;head rend="h1"&gt;Anthropic Is In Real Trouble - And The Current Cost Of Doing Business Is Unsustainable, Meaning Prices Must Increase&lt;/head&gt;
    &lt;p&gt;I’ve come away from this piece with a feeling of dread.&lt;/p&gt;
    &lt;p&gt;Anthropic’s costs are out of control, and as things get more desperate, it appears to be lashing out at its customers, both companies like Cursor and Claude Code customers facing weekly rate limits on their more-powerful models who are chided for using a product they pay for. Again, I cannot say for certain, but the spike in costs is clear, and it feels like more than a coincidence to me.&lt;/p&gt;
    &lt;p&gt;There is no period of time that I can see in the just under two years of data I’ve been party to that suggests that Anthropic has any means of — or any success doing — cost-cutting, and the only thing this company seems capable of doing is increasing the amount of money it burns on a monthly basis.&lt;/p&gt;
    &lt;p&gt;Based on what I have been party to, the more successful Anthropic becomes, the more its services cost. The cost of inference is clearly increasing for customers, but based on its escalating monthly costs, the cost of inference appears to be high for Anthropic too, though it’s impossible to tell how much of its compute is based on training versus running inference.&lt;/p&gt;
    &lt;p&gt;In any case, these costs seem to increase with the amount of money Anthropic makes, meaning that the current pricing of both subscriptions and API access seems unprofitable, and must increase dramatically — from my calculations, a 100% price increase might work, but good luck retaining every single customer and their customers too! — for this company to ever become sustainable.&lt;/p&gt;
    &lt;p&gt;I don’t think that people would pay those prices. If anything, I think what we’re seeing in these numbers is a company bleeding out from costs that escalate the more that its user base grows. This is just my opinion, of course.&lt;/p&gt;
    &lt;p&gt;I’m tired of watching these companies burn billions of dollars to destroy our environment and steal from everybody. I’m tired that so many people have tried to pretend there’s a justification for burning billions of dollars every year, clinging to empty tropes about how this is just like Uber or Amazon Web Services, when Anthropic has built something far more mediocre.&lt;/p&gt;
    &lt;p&gt;Mr. Amodei, I am sure you will read this piece, and I can make time to chat in person on my show Better Offline. Perhaps this Friday? I even have some studio time on the books.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://www.wheresyoured.at/costs/"/><published>2025-10-20T15:05:30+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645349</id><title>Production RAG: what I learned from processing 5M+ documents</title><updated>2025-10-20T16:44:01.080291+00:00</updated><content>&lt;doc fingerprint="9bf95f134d9a6771"&gt;
  &lt;main&gt;
    &lt;p&gt;I've spent the last 8 months in the RAG trenches, I want to share what actually worked vs. wasted our time. We built RAG for Usul AI (9M pages) and an unnamed legal AI enterprise (4M pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Langchain + Llamaindex&lt;/head&gt;
    &lt;p&gt;We started out with youtube tutorials. First Langchain → Llamaindex. Got to a working prototype in a couple of days and were optimistic with the progress. We run tests on subset of the data (100 documents) and the results looked great. We spent the next few days running the pipeline on the production dataset and got everything working in a week — incredible.&lt;/p&gt;
    &lt;p&gt;Except it wasn't, the results were subpar and only the end users could tell. We spent the following few months rewriting pieces of the system, one at a time, until the performance was at the level we wanted. Here are things we did ranked by ROI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What moved the needle&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query Generation: not all context can be captured by the user's last query. We had an LLM review the thread and generate a number of semantic + keyword queries. We processed all of those queries in parallel, and passed them to a reranker. This made us cover a larger surface area and not be dependent on a computed score for hybrid search.&lt;/item&gt;
      &lt;item&gt;Reranking: the highest value 5 lines of code you'll add. The chunk ranking shifted a lot. More than you'd expect. Reranking can many times make up for a bad setup if you pass in enough chunks. We found the ideal reranker set-up to be 50 chunk input -&amp;gt; 15 output.&lt;/item&gt;
      &lt;item&gt;Chunking Strategy: this takes a lot of effort, you'll probably be spending most of your time on it. We built a custom flow for both enterprises, make sure to understand the data, review the chunks, and check that a) chunks are not getting cut mid-word or sentence b) ~each chunk is a logical unit and captures information on its own&lt;/item&gt;
      &lt;item&gt;Metadata to LLM: we started by passing the chunk text to the LLM, we ran an experiment and found that injecting relevant metadata as well (title, author, etc.) improves context and answers by a lot.&lt;/item&gt;
      &lt;item&gt;Query routing: many users asked questions that can't be answered by RAG (e.g. summarize the article, who wrote this). We created a small router that detects these questions and answers them using an API call + LLM instead of the full-blown RAG set-ups.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our stack&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vector database: Azure -&amp;gt; Pinecone -&amp;gt; Turbopuffer (cheap, supports keyword search natively)&lt;/item&gt;
      &lt;item&gt;Document Extraction: Custom&lt;/item&gt;
      &lt;item&gt;Chunking: Unstructured.io by default, custom for enterprises (heard that Chonkie is good)&lt;/item&gt;
      &lt;item&gt;Embedding: text-embedding-large-3, haven't tested others&lt;/item&gt;
      &lt;item&gt;Reranker: None -&amp;gt; Cohere 3.5 -&amp;gt; Zerank (less known but actually good)&lt;/item&gt;
      &lt;item&gt;LLM: GPT 4.1 -&amp;gt; GPT 5 -&amp;gt; GPT 4.1, covered by Azure credits&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Going Open-source&lt;/head&gt;
    &lt;p&gt;We put all our learning into an open-source project: agentset-ai/agentset under an MIT license. Feel free to reach out if you have any questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://blog.abdellatif.io/production-rag-processing-5m-documents"/><published>2025-10-20T15:55:36+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645510</id><title>What I Self Host</title><updated>2025-10-20T16:44:00.780964+00:00</updated><content>&lt;doc fingerprint="bc4990790f05bf90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What I self host&lt;/head&gt;
    &lt;p&gt;I’ve always liked reading blogs, and have used several feed readers in the past (Feedly, for example). For a long time I was thinking it would be fun to write my own RSS reader, but instead of diving into the challenge, I did the next best thing, which was finding a decent one, and learning how to self host it.&lt;/p&gt;
    &lt;p&gt;In this post I will tell about the self hosting I do, and end by sketching the setup.&lt;/p&gt;
    &lt;head rend="h2"&gt;RSS reader - Miniflux&lt;/head&gt;
    &lt;p&gt;Miniflux is a “minimalist and opinionated feed reader”. I host my own instance at https://rss.fredrikmeyer.net/ It is very easy to set up using Docker, see the documentation.&lt;/p&gt;
    &lt;p&gt;I do have lots of unread blog posts 🤨.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grafana, Strava Integration&lt;/head&gt;
    &lt;p&gt;I host a Grafana instance, also using Docker. What first triggered me to make this instance was an old project (that I want to revive one day): I had a Raspberry Pi with some sensors measuring gas and dust at my previous apartment, and a Grafana dashboard showing the data. It was interesting seeing how making food at home had a measurable impact on volatile gas levels.&lt;/p&gt;
    &lt;p&gt;Later I discovered the Strava datasource plugin for Grafana. It is a plugin that lets Grafana connect to the Strava API, and gives you summaries of your Strava activities. Below is an example of how it looks for me:&lt;/p&gt;
    &lt;p&gt;One gets several other dashboards included in the plugin.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spotify&lt;/head&gt;
    &lt;p&gt;One day YourSpotify was mentioned on HackerNews. It is an application that connects to the Spotify API, and gives you aggregated statistics of artists and albums you’ve listened to over time (why they chose to store the data in MongoDB I have no idea of!).&lt;/p&gt;
    &lt;p&gt;It is interesting to note that I have listened to less and less music over the years (I have noticed that the more experience I have at work, the less actual programming I do).&lt;/p&gt;
    &lt;p&gt;Because I didn’t bother setting up DNS, this one is only exposed locally, so I use Tailscale to be able to access YourSpotify. This works by having Tailscale installed on the host, and connecting to the Tailnet. It lets me access the application by writing &lt;code&gt;http://forgottensuperhero:3001/&lt;/code&gt; in the browser.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bookmark manager&lt;/head&gt;
    &lt;p&gt;I have a problem with closing tabs, and a tendency to hoard information (don’t get me started on the number of unread PDF books on my Remarkable!). So I found Linkding, a bookmark manager, which I access at https://links.fredrikmeyer.net/bookmarks/shared.&lt;/p&gt;
    &lt;p&gt;In practice it is a grave yard for interesting things I never have the time to read, but it gives me peace some kind of peace of mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;How&lt;/head&gt;
    &lt;p&gt;I have an ambition of making the hosting “production grade”, but at the moment this setup is a mix of practices of varying levels of quality.&lt;/p&gt;
    &lt;p&gt;I pay for a cheap droplet at DigitalOcean, about $5 per month, and an additional dollar for backup. The domain name and DNS is from Domeneshop. SSL certificates from Let’s Encrypt.&lt;/p&gt;
    &lt;p&gt;All the apps run in different Docker containers, with ports exposed. These ports are then listened to by Nginx, which redirects to HTTPS.&lt;/p&gt;
    &lt;p&gt;I manage most of the configuration using Ansible. Here I must give thanks to Jeff Geerling’s book Ansible for DevOps, which was really good. So if I change my Nginx configuration, I edit it on my laptop, and run&lt;/p&gt;
    &lt;code&gt;ansible-playbook -i inventory.ini docker.yml  --ask-become-pass
&lt;/code&gt;
    &lt;p&gt;to let Ansible do its magic. In this case, “most” means the Nginx configuration and Grafana.&lt;/p&gt;
    &lt;p&gt;Miniflux and YourSpotify are managed by simply doing &lt;code&gt;scp spotify_stats.yml droplet:~&lt;/code&gt; and running &lt;code&gt;sudo docker-compose -f ./spotify_stats.yml up -d&lt;/code&gt; on the host.&lt;/p&gt;
    &lt;p&gt;Ideally, I would like to have a 100% “infrastructure as code” approach, but hey, who has time for that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideas for the future&lt;/head&gt;
    &lt;p&gt;It would be nice to combine AI and user manuals of house appliances to make an application that lets you ask questions like “what does the red light on my oven mean?”. Or write my own Jira, or… Lots of rabbit holes in this list on Github.&lt;/p&gt;
    &lt;p&gt;Until next time!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://fredrikmeyer.net/2025/10/18/what-i-self-host.html"/><published>2025-10-20T16:07:52+00:00</published></entry><entry><id>https://news.ycombinator.com/item?id=45645567</id><title>Kohler launches smart toilet camera</title><updated>2025-10-20T16:44:00.702521+00:00</updated><content>&lt;doc fingerprint="b65cd443519451f5"&gt;
  &lt;main&gt;
    &lt;p&gt;Home goods company Kohler recently unveiled a new device called the Dekoda — a $599 camera that can be attached to your toilet bowl and take pictures of what’s inside.&lt;/p&gt;
    &lt;p&gt;CNET reports that the Dekoda analyzes these images in order to provide updates on your gut health and hydration, and to potentially detect blood. It also comes with a rechargeable battery, a USB connection, and a fingerprint sensor to identify who’s using the toilet.&lt;/p&gt;
    &lt;p&gt;The Dekoda is currently available for pre-order, with shipments scheduled to begin on October 21. In addition to the hardware purchase fee, customers will need to pay between $70 and $156 per year for a subscription.&lt;/p&gt;
    &lt;p&gt;If you’re uneasy about the privacy implications of putting a camera right below your private parts, the company says, “Dekoda’s sensors see down into your toilet and nowhere else.” It also notes that the resulting data is secured via end-to-end encryption.&lt;/p&gt;
    &lt;p&gt;Kohler isn’t the only company seeking to take pictures of your poop — we’ve also written about the toilet camera offered by a startup called Throne.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</content><link href="https://techcrunch.com/2025/10/19/kohler-unveils-a-camera-for-your-toilet/"/><published>2025-10-20T16:12:32+00:00</published></entry></feed>